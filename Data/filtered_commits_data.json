{
    "build/main.py": [
        {
            "commit": "16a01cf914066656381d461f822ba3cccc2a3171",
            "timestamp": "2024-10-22T15:10:44+02:00",
            "author": "alexander.schranner",
            "diff": {
                "added": {
                    "1": "",
                    "2": "import matplotlib.pyplot as plt",
                    "3": "# from pydriller import Repository",
                    "4": "# import numpy as np",
                    "5": "# import requests",
                    "6": "# import json",
                    "7": "# from flask import Response",
                    "8": "#",
                    "9": "# commits_monthly = list()",
                    "10": "# curr_month = 0",
                    "11": "# curr_year = 0",
                    "12": "# i = 0",
                    "13": "# month_del = 0",
                    "14": "# month_add = 0",
                    "15": "# month_commits = 0",
                    "16": "# filecounter = 0",
                    "17": "#",
                    "18": "#",
                    "19": "# for commit in Repository('https://github.com/dani-garcia/vaultwarden').traverse_commits():",
                    "20": "# #for commit in Repository('~/Developer/Ecogenium/Atava').traverse_commits():",
                    "21": "#     if (commit.committer_date.month != curr_month):",
                    "22": "#         label = str(curr_month) + \"-\" + str(curr_year)",
                    "23": "#         commits_monthly.append([label, filecounter, month_commits, month_add, month_del])",
                    "24": "#         curr_month = commit.committer_date.month",
                    "25": "#         curr_year = commit.committer_date.year",
                    "26": "#         i += 1",
                    "27": "#         filecounter = 0",
                    "28": "#         month_del = 0",
                    "29": "#         month_add = 0",
                    "30": "#         month_commits = 0",
                    "31": "#",
                    "32": "#     month_commits += 1",
                    "33": "#     for file in commit.modified_files:",
                    "34": "#         month_add += file.added_lines",
                    "35": "#         month_del += file.deleted_lines",
                    "36": "#         filecounter += 1",
                    "37": "#",
                    "38": "# commits = []",
                    "39": "# additions = []",
                    "40": "# deletions = []",
                    "41": "# files = []",
                    "42": "# loc = []",
                    "43": "# issues = []",
                    "44": "#",
                    "45": "# for i in range(0, len(commits_monthly)):",
                    "46": "#     commits.append(commits_monthly[i][1])",
                    "47": "#     files.append(commits_monthly[i][2])",
                    "48": "#     additions.append(commits_monthly[i][3])",
                    "49": "#     deletions.append(commits_monthly[i][4])",
                    "50": "#     if (i == 0):",
                    "51": "#         loc.append(additions[i] - deletions[i])",
                    "52": "#     else:",
                    "53": "#         loc.append(loc[i-1] + additions[i] - deletions[i])",
                    "54": "#",
                    "55": "# api_url = \"https://api.github.com/repos/dani-garcia/vaultwarden/issues\"",
                    "56": "# for i in range (1, 50):",
                    "57": "#     params = {",
                    "58": "#         \"state\": \"all\",",
                    "59": "#         \"direction\": \"asc\",",
                    "60": "#         \"per_page\": 100,",
                    "61": "#         \"page\" : i",
                    "62": "#     }",
                    "63": "#     response = requests.get(api_url, params=params)",
                    "64": "#     issues_response = response.json()",
                    "65": "#     for issue in issues_response:",
                    "66": "#         issue_data = []",
                    "67": "#         issue_data.append(issue['number'])",
                    "68": "#         issue_data.append(issue['created_at'])",
                    "69": "#         issue_data.append(issue['closed_at'])",
                    "70": "#         issues.append(issue_data)",
                    "71": "#",
                    "72": "# m = 0",
                    "73": "# issues_monthly = [0]",
                    "74": "# for i in range(len(issues)):",
                    "75": "#     if (issues[i][1][2:5] != issues[i+1][1][2:5]):",
                    "76": "#         m += 1",
                    "77": "#         issues_monthly.append(0)",
                    "78": "#     issues_monthly[m] += 1",
                    "79": "#",
                    "80": "# fig, ax1 = plt.subplots()",
                    "81": "# ax1.plot(additions, color='g', label='Additions')",
                    "82": "# ax1.plot(deletions, color='r', label='Deletions')",
                    "83": "# ax1.plot(loc, color='0', label='LOC')",
                    "84": "# ax1.tick_params(axis='y', labelcolor='0.5')",
                    "85": "#",
                    "86": "# ax2 = ax1.twinx()",
                    "87": "# ax2.plot(commits, color='y', label='Commits')",
                    "88": "# ax2.plot(files, color='b', label='Files')",
                    "89": "# ax2.plot(issues_monthly, color='0.3', label='Created Issues')",
                    "90": "# ax2.tick_params(axis='y', labelcolor='0.8')",
                    "91": "#",
                    "92": "# fig.tight_layout()",
                    "93": "# ax1.legend()",
                    "94": "# ax2.legend()",
                    "95": "# plt.savefig(\"GitVisualisation.pdf\", format=\"pdf\")",
                    "96": "# plt.show()",
                    "97": "#",
                    "98": "import matplotlib.pyplot as plt",
                    "99": "from pydriller import Repository",
                    "100": "import requests",
                    "101": "",
                    "102": "def analyze_commit_data(repo_url):",
                    "103": "    # Initialize tracking variables",
                    "104": "    monthly_commit_data = []",
                    "105": "    current_month = 0",
                    "106": "    current_year = 0",
                    "107": "    monthly_additions = 0",
                    "108": "    monthly_deletions = 0",
                    "109": "    monthly_commit_count = 0",
                    "110": "    modified_file_count = 0",
                    "111": "",
                    "112": "    # Traverse through all commits in the repository",
                    "113": "    for commit in Repository(repo_url).traverse_commits():",
                    "114": "        # If we encounter a new month, save the previous month's data",
                    "115": "        if commit.committer_date.month != current_month:",
                    "116": "            label = f\"{current_month}-{current_year}\"",
                    "117": "            monthly_commit_data.append([label, monthly_commit_count, modified_file_count, monthly_additions, monthly_deletions])",
                    "118": "            # Reset counters for the new month",
                    "119": "            current_month = commit.committer_date.month",
                    "120": "            current_year = commit.committer_date.year",
                    "121": "            monthly_additions = 0",
                    "122": "            monthly_deletions = 0",
                    "123": "            monthly_commit_count = 0",
                    "124": "            modified_file_count = 0",
                    "125": "",
                    "126": "        # Update current month's data",
                    "127": "        monthly_commit_count += 1",
                    "128": "        for file in commit.modified_files:",
                    "129": "            monthly_additions += file.added_lines",
                    "130": "            monthly_deletions += file.deleted_lines",
                    "131": "            modified_file_count += 1",
                    "132": "",
                    "133": "    return monthly_commit_data",
                    "134": "",
                    "135": "def analyze_issues_data(repo_url, max_pages):",
                    "136": "    issues_data = []",
                    "137": "    api_url = f\"https://api.github.com/repos/{repo_url}/issues\"",
                    "138": "",
                    "139": "    # Fetch issues data from GitHub using pagination",
                    "140": "    for page in range(1, max_pages + 1):",
                    "141": "        params = {",
                    "142": "            \"state\": \"all\",",
                    "143": "            \"direction\": \"asc\",",
                    "144": "            \"per_page\": 100,",
                    "145": "            \"page\": page",
                    "146": "        }",
                    "147": "        response = requests.get(api_url, params=params)",
                    "148": "        if response.status_code != 200:",
                    "149": "            print(f\"Failed to fetch issues data. Status code: {response.status_code}\")",
                    "150": "            break",
                    "151": "",
                    "152": "        issues_response = response.json()",
                    "153": "        # Stop if no more issues are returned",
                    "154": "        if not issues_response:",
                    "155": "            break",
                    "156": "",
                    "157": "        # Extract necessary issue data",
                    "158": "        for issue in issues_response:",
                    "159": "            issues_data.append([issue['number'], issue['created_at'], issue['closed_at']])",
                    "160": "",
                    "161": "    return issues_data",
                    "162": "",
                    "163": "def count_issues_monthly(issues_data):",
                    "164": "    issues_per_month = [0,0,0,0]",
                    "165": "    current_month = issues_data[0][1][5:7] if issues_data else None",
                    "166": "    monthly_issue_count = 0",
                    "167": "",
                    "168": "    # Count issues per month",
                    "169": "    for i in range(len(issues_data) - 1):",
                    "170": "        issue_month = issues_data[i][1][5:7]",
                    "171": "        next_issue_month = issues_data[i + 1][1][5:7]",
                    "172": "        if issue_month != next_issue_month:",
                    "173": "            issues_per_month.append(monthly_issue_count)",
                    "174": "            print(\"mic: \", monthly_issue_count, issues_data[i][1][2:7])",
                    "175": "            monthly_issue_count = 0",
                    "176": "            current_month = next_issue_month",
                    "177": "        monthly_issue_count += 1",
                    "178": "",
                    "179": "    # Add the last month's count",
                    "180": "    issues_per_month.append(monthly_issue_count)",
                    "181": "    return issues_per_month",
                    "182": "",
                    "183": "def calculate_loc(monthly_commit_data):",
                    "184": "    # Calculate lines of code (LOC) changes over time",
                    "185": "    loc_over_time = []",
                    "186": "    total_loc = 0",
                    "187": "",
                    "188": "    for month_data in monthly_commit_data:",
                    "189": "        additions = month_data[3]",
                    "190": "        deletions = month_data[4]",
                    "191": "        total_loc += additions - deletions",
                    "192": "        loc_over_time.append(total_loc)",
                    "193": "",
                    "194": "    return loc_over_time",
                    "195": "",
                    "196": "def plot_data(monthly_commit_data, loc_over_time, issues_per_month):",
                    "197": "    # Extract data for plotting",
                    "198": "    monthly_labels = [data[0] for data in monthly_commit_data]",
                    "199": "    monthly_commits = [data[1] for data in monthly_commit_data]",
                    "200": "    modified_files = [data[2] for data in monthly_commit_data]",
                    "201": "    monthly_additions = [data[3] for data in monthly_commit_data]",
                    "202": "    monthly_deletions = [data[4] for data in monthly_commit_data]",
                    "203": "",
                    "204": "    for i in range(len(monthly_labels)):",
                    "205": "        label = monthly_labels[i].split('-')",
                    "206": "        if (len(label[0]) == 1):",
                    "207": "            label[0] = \"0\" + label[0]",
                    "208": "        label[1] = label[1][2:]",
                    "209": "        monthly_labels[i] = label[1] + \"-\" + label[0]",
                    "210": "",
                    "211": "    # Plotting",
                    "212": "    fig, ax1 = plt.subplots()",
                    "213": "",
                    "214": "    # Plot lines for additions, deletions, and LOC",
                    "215": "    ax1.plot(monthly_labels, monthly_additions, color='g', label='Additions')",
                    "216": "    ax1.plot(monthly_labels, monthly_deletions, color='r', label='Deletions')",
                    "217": "    ax1.plot(monthly_labels, loc_over_time, color='k', label='LOC')",
                    "218": "    ax1.tick_params(axis='y', labelcolor='black')",
                    "219": "",
                    "220": "    # Secondary Y-axis for commits, modified files, and issues",
                    "221": "    ax2 = ax1.twinx()",
                    "222": "    ax2.plot(monthly_labels, monthly_commits, color='y', label='Commits')",
                    "223": "    ax2.plot(monthly_labels, modified_files, color='b', label='Modified Files')",
                    "224": "    ax2.plot(monthly_labels[:len(issues_per_month)], issues_per_month, color='grey', label='Created Issues')",
                    "225": "    ax2.tick_params(axis='y', labelcolor='grey')",
                    "226": "",
                    "227": "    # Finalize and show the plot",
                    "228": "    fig.tight_layout()",
                    "229": "    ax1.legend(loc='upper left')",
                    "230": "    ax2.legend(loc='upper right')",
                    "231": "    plt.xticks(rotation=45)",
                    "232": "    plt.title(\"Repository Analysis\")",
                    "233": "    plt.savefig(\"GitVisualisation.pdf\", format=\"pdf\")",
                    "234": "    plt.show()",
                    "235": "",
                    "236": "def main():",
                    "237": "    repo_url = 'dani-garcia/vaultwarden'",
                    "238": "    monthly_commit_data = analyze_commit_data(f'https://github.com/{repo_url}')",
                    "239": "    issues_data = analyze_issues_data(repo_url, 50)",
                    "240": "    issues_per_month = count_issues_monthly(issues_data)",
                    "241": "    loc_over_time = calculate_loc(monthly_commit_data)",
                    "242": "    plot_data(monthly_commit_data, loc_over_time, issues_per_month)",
                    "243": "",
                    "244": "if __name__ == \"__main__\":",
                    "245": "    main()"
                },
                "deleted": {}
            },
            "source_code": {
                "1": "",
                "2": "import matplotlib.pyplot as plt",
                "3": "# from pydriller import Repository",
                "4": "# import numpy as np",
                "5": "# import requests",
                "6": "# import json",
                "7": "# from flask import Response",
                "8": "# ",
                "9": "# commits_monthly = list()",
                "10": "# curr_month = 0",
                "11": "# curr_year = 0",
                "12": "# i = 0",
                "13": "# month_del = 0",
                "14": "# month_add = 0",
                "15": "# month_commits = 0",
                "16": "# filecounter = 0",
                "17": "# ",
                "18": "# ",
                "19": "# for commit in Repository('https://github.com/dani-garcia/vaultwarden').traverse_commits():",
                "20": "# #for commit in Repository('~/Developer/Ecogenium/Atava').traverse_commits():",
                "21": "#     if (commit.committer_date.month != curr_month):",
                "22": "#         label = str(curr_month) + \"-\" + str(curr_year)",
                "23": "#         commits_monthly.append([label, filecounter, month_commits, month_add, month_del])",
                "24": "#         curr_month = commit.committer_date.month",
                "25": "#         curr_year = commit.committer_date.year",
                "26": "#         i += 1",
                "27": "#         filecounter = 0",
                "28": "#         month_del = 0",
                "29": "#         month_add = 0",
                "30": "#         month_commits = 0",
                "31": "# ",
                "32": "#     month_commits += 1",
                "33": "#     for file in commit.modified_files:",
                "34": "#         month_add += file.added_lines",
                "35": "#         month_del += file.deleted_lines",
                "36": "#         filecounter += 1",
                "37": "#     ",
                "38": "# commits = []",
                "39": "# additions = []",
                "40": "# deletions = []",
                "41": "# files = []",
                "42": "# loc = []",
                "43": "# issues = []",
                "44": "# ",
                "45": "# for i in range(0, len(commits_monthly)):",
                "46": "#     commits.append(commits_monthly[i][1])",
                "47": "#     files.append(commits_monthly[i][2])",
                "48": "#     additions.append(commits_monthly[i][3])",
                "49": "#     deletions.append(commits_monthly[i][4])",
                "50": "#     if (i == 0):",
                "51": "#         loc.append(additions[i] - deletions[i])",
                "52": "#     else: ",
                "53": "#         loc.append(loc[i-1] + additions[i] - deletions[i])",
                "54": "# ",
                "55": "# api_url = \"https://api.github.com/repos/dani-garcia/vaultwarden/issues\"",
                "56": "# for i in range (1, 50):",
                "57": "#     params = {",
                "58": "#         \"state\": \"all\",",
                "59": "#         \"direction\": \"asc\",",
                "60": "#         \"per_page\": 100,",
                "61": "#         \"page\" : i",
                "62": "#     }",
                "63": "#     response = requests.get(api_url, params=params)",
                "64": "#     issues_response = response.json()",
                "65": "#     for issue in issues_response:",
                "66": "#         issue_data = []",
                "67": "#         issue_data.append(issue['number'])",
                "68": "#         issue_data.append(issue['created_at'])",
                "69": "#         issue_data.append(issue['closed_at'])",
                "70": "#         issues.append(issue_data)",
                "71": "# ",
                "72": "# m = 0",
                "73": "# issues_monthly = [0]",
                "74": "# for i in range(len(issues)):",
                "75": "#     if (issues[i][1][2:5] != issues[i+1][1][2:5]):",
                "76": "#         m += 1",
                "77": "#         issues_monthly.append(0)",
                "78": "#     issues_monthly[m] += 1",
                "79": "# ",
                "80": "# fig, ax1 = plt.subplots()",
                "81": "# ax1.plot(additions, color='g', label='Additions')",
                "82": "# ax1.plot(deletions, color='r', label='Deletions')",
                "83": "# ax1.plot(loc, color='0', label='LOC')",
                "84": "# ax1.tick_params(axis='y', labelcolor='0.5')",
                "85": "# ",
                "86": "# ax2 = ax1.twinx()",
                "87": "# ax2.plot(commits, color='y', label='Commits')",
                "88": "# ax2.plot(files, color='b', label='Files')",
                "89": "# ax2.plot(issues_monthly, color='0.3', label='Created Issues')",
                "90": "# ax2.tick_params(axis='y', labelcolor='0.8')",
                "91": "# ",
                "92": "# fig.tight_layout()",
                "93": "# ax1.legend()",
                "94": "# ax2.legend()",
                "95": "# plt.savefig(\"GitVisualisation.pdf\", format=\"pdf\")",
                "96": "# plt.show()",
                "97": "# ",
                "98": "import matplotlib.pyplot as plt",
                "99": "from pydriller import Repository",
                "100": "import requests",
                "101": "",
                "102": "def analyze_commit_data(repo_url):",
                "103": "    # Initialize tracking variables",
                "104": "    monthly_commit_data = []",
                "105": "    current_month = 0",
                "106": "    current_year = 0",
                "107": "    monthly_additions = 0",
                "108": "    monthly_deletions = 0",
                "109": "    monthly_commit_count = 0",
                "110": "    modified_file_count = 0",
                "111": "",
                "112": "    # Traverse through all commits in the repository",
                "113": "    for commit in Repository(repo_url).traverse_commits():",
                "114": "        # If we encounter a new month, save the previous month's data",
                "115": "        if commit.committer_date.month != current_month:",
                "116": "            label = f\"{current_month}-{current_year}\"",
                "117": "            monthly_commit_data.append([label, monthly_commit_count, modified_file_count, monthly_additions, monthly_deletions])",
                "118": "            # Reset counters for the new month",
                "119": "            current_month = commit.committer_date.month",
                "120": "            current_year = commit.committer_date.year",
                "121": "            monthly_additions = 0",
                "122": "            monthly_deletions = 0",
                "123": "            monthly_commit_count = 0",
                "124": "            modified_file_count = 0",
                "125": "",
                "126": "        # Update current month's data",
                "127": "        monthly_commit_count += 1",
                "128": "        for file in commit.modified_files:",
                "129": "            monthly_additions += file.added_lines",
                "130": "            monthly_deletions += file.deleted_lines",
                "131": "            modified_file_count += 1",
                "132": "",
                "133": "    return monthly_commit_data",
                "134": "",
                "135": "def analyze_issues_data(repo_url, max_pages):",
                "136": "    issues_data = []",
                "137": "    api_url = f\"https://api.github.com/repos/{repo_url}/issues\"",
                "138": "",
                "139": "    # Fetch issues data from GitHub using pagination",
                "140": "    for page in range(1, max_pages + 1):",
                "141": "        params = {",
                "142": "            \"state\": \"all\",",
                "143": "            \"direction\": \"asc\",",
                "144": "            \"per_page\": 100,",
                "145": "            \"page\": page",
                "146": "        }",
                "147": "        response = requests.get(api_url, params=params)",
                "148": "        if response.status_code != 200:",
                "149": "            print(f\"Failed to fetch issues data. Status code: {response.status_code}\")",
                "150": "            break",
                "151": "",
                "152": "        issues_response = response.json()",
                "153": "        # Stop if no more issues are returned",
                "154": "        if not issues_response:",
                "155": "            break",
                "156": "",
                "157": "        # Extract necessary issue data",
                "158": "        for issue in issues_response:",
                "159": "            issues_data.append([issue['number'], issue['created_at'], issue['closed_at']])",
                "160": "",
                "161": "    return issues_data",
                "162": "",
                "163": "def count_issues_monthly(issues_data):",
                "164": "    issues_per_month = [0,0,0,0]",
                "165": "    current_month = issues_data[0][1][5:7] if issues_data else None",
                "166": "    monthly_issue_count = 0",
                "167": "",
                "168": "    # Count issues per month",
                "169": "    for i in range(len(issues_data) - 1):",
                "170": "        issue_month = issues_data[i][1][5:7]",
                "171": "        next_issue_month = issues_data[i + 1][1][5:7]",
                "172": "        if issue_month != next_issue_month:",
                "173": "            issues_per_month.append(monthly_issue_count)",
                "174": "            print(\"mic: \", monthly_issue_count, issues_data[i][1][2:7])",
                "175": "            monthly_issue_count = 0",
                "176": "            current_month = next_issue_month",
                "177": "        monthly_issue_count += 1",
                "178": "",
                "179": "    # Add the last month's count",
                "180": "    issues_per_month.append(monthly_issue_count)",
                "181": "    return issues_per_month",
                "182": "",
                "183": "def calculate_loc(monthly_commit_data):",
                "184": "    # Calculate lines of code (LOC) changes over time",
                "185": "    loc_over_time = []",
                "186": "    total_loc = 0",
                "187": "",
                "188": "    for month_data in monthly_commit_data:",
                "189": "        additions = month_data[3]",
                "190": "        deletions = month_data[4]",
                "191": "        total_loc += additions - deletions",
                "192": "        loc_over_time.append(total_loc)",
                "193": "",
                "194": "    return loc_over_time",
                "195": "",
                "196": "def plot_data(monthly_commit_data, loc_over_time, issues_per_month):",
                "197": "    # Extract data for plotting",
                "198": "    monthly_labels = [data[0] for data in monthly_commit_data]",
                "199": "    monthly_commits = [data[1] for data in monthly_commit_data]",
                "200": "    modified_files = [data[2] for data in monthly_commit_data]",
                "201": "    monthly_additions = [data[3] for data in monthly_commit_data]",
                "202": "    monthly_deletions = [data[4] for data in monthly_commit_data]",
                "203": "",
                "204": "    for i in range(len(monthly_labels)):",
                "205": "        label = monthly_labels[i].split('-')",
                "206": "        if (len(label[0]) == 1):",
                "207": "            label[0] = \"0\" + label[0]",
                "208": "        label[1] = label[1][2:]",
                "209": "        monthly_labels[i] = label[1] + \"-\" + label[0]",
                "210": "",
                "211": "    # Plotting",
                "212": "    fig, ax1 = plt.subplots()",
                "213": "",
                "214": "    # Plot lines for additions, deletions, and LOC",
                "215": "    ax1.plot(monthly_labels, monthly_additions, color='g', label='Additions')",
                "216": "    ax1.plot(monthly_labels, monthly_deletions, color='r', label='Deletions')",
                "217": "    ax1.plot(monthly_labels, loc_over_time, color='k', label='LOC')",
                "218": "    ax1.tick_params(axis='y', labelcolor='black')",
                "219": "",
                "220": "    # Secondary Y-axis for commits, modified files, and issues",
                "221": "    ax2 = ax1.twinx()",
                "222": "    ax2.plot(monthly_labels, monthly_commits, color='y', label='Commits')",
                "223": "    ax2.plot(monthly_labels, modified_files, color='b', label='Modified Files')",
                "224": "    ax2.plot(monthly_labels[:len(issues_per_month)], issues_per_month, color='grey', label='Created Issues')",
                "225": "    ax2.tick_params(axis='y', labelcolor='grey')",
                "226": "",
                "227": "    # Finalize and show the plot",
                "228": "    fig.tight_layout()",
                "229": "    ax1.legend(loc='upper left')",
                "230": "    ax2.legend(loc='upper right')",
                "231": "    plt.xticks(rotation=45)",
                "232": "    plt.title(\"Repository Analysis\")",
                "233": "    plt.savefig(\"GitVisualisation.pdf\", format=\"pdf\")",
                "234": "    plt.show()",
                "235": "",
                "236": "def main():",
                "237": "    repo_url = 'dani-garcia/vaultwarden'",
                "238": "    monthly_commit_data = analyze_commit_data(f'https://github.com/{repo_url}')",
                "239": "    issues_data = analyze_issues_data(repo_url, 50)",
                "240": "    issues_per_month = count_issues_monthly(issues_data)",
                "241": "    loc_over_time = calculate_loc(monthly_commit_data)",
                "242": "    plot_data(monthly_commit_data, loc_over_time, issues_per_month)",
                "243": "",
                "244": "if __name__ == \"__main__\":",
                "245": "    main()"
            },
            "comments": [
                {
                    "line": 3,
                    "comment": "# from pydriller import Repository",
                    "char_position_in_line": 0,
                    "type": [
                        "commented-out",
                        "block"
                    ]
                },
                {
                    "line": 4,
                    "comment": "# import numpy as np",
                    "char_position_in_line": 0,
                    "type": [
                        "commented-out",
                        "block"
                    ]
                },
                {
                    "line": 5,
                    "comment": "# import requests",
                    "char_position_in_line": 0,
                    "type": [
                        "commented-out",
                        "block"
                    ]
                },
                {
                    "line": 6,
                    "comment": "# import json",
                    "char_position_in_line": 0,
                    "type": [
                        "commented-out",
                        "block"
                    ]
                },
                {
                    "line": 7,
                    "comment": "# from flask import Response",
                    "char_position_in_line": 0,
                    "type": [
                        "commented-out",
                        "block"
                    ]
                },
                {
                    "line": 8,
                    "comment": "# ",
                    "char_position_in_line": 0,
                    "type": [
                        "commented-out",
                        "block"
                    ]
                },
                {
                    "line": 9,
                    "comment": "# commits_monthly = list()",
                    "char_position_in_line": 0,
                    "type": [
                        "commented-out",
                        "block"
                    ]
                },
                {
                    "line": 10,
                    "comment": "# curr_month = 0",
                    "char_position_in_line": 0,
                    "type": [
                        "commented-out",
                        "block"
                    ]
                },
                {
                    "line": 11,
                    "comment": "# curr_year = 0",
                    "char_position_in_line": 0,
                    "type": [
                        "commented-out",
                        "block"
                    ]
                },
                {
                    "line": 12,
                    "comment": "# i = 0",
                    "char_position_in_line": 0,
                    "type": [
                        "commented-out",
                        "block"
                    ]
                },
                {
                    "line": 13,
                    "comment": "# month_del = 0",
                    "char_position_in_line": 0,
                    "type": [
                        "commented-out",
                        "block"
                    ]
                },
                {
                    "line": 14,
                    "comment": "# month_add = 0",
                    "char_position_in_line": 0,
                    "type": [
                        "commented-out",
                        "block"
                    ]
                },
                {
                    "line": 15,
                    "comment": "# month_commits = 0",
                    "char_position_in_line": 0,
                    "type": [
                        "commented-out",
                        "block"
                    ]
                },
                {
                    "line": 16,
                    "comment": "# filecounter = 0",
                    "char_position_in_line": 0,
                    "type": [
                        "commented-out",
                        "block"
                    ]
                },
                {
                    "line": 17,
                    "comment": "# ",
                    "char_position_in_line": 0,
                    "type": [
                        "commented-out",
                        "block"
                    ]
                },
                {
                    "line": 18,
                    "comment": "# ",
                    "char_position_in_line": 0,
                    "type": [
                        "commented-out",
                        "block"
                    ]
                },
                {
                    "line": 19,
                    "comment": "# for commit in Repository('https://github.com/dani-garcia/vaultwarden').traverse_commits():",
                    "char_position_in_line": 0,
                    "type": [
                        "commented-out",
                        "block"
                    ]
                },
                {
                    "line": 20,
                    "comment": "# #for commit in Repository('~/Developer/Ecogenium/Atava').traverse_commits():",
                    "char_position_in_line": 0,
                    "type": [
                        "commented-out",
                        "block"
                    ]
                },
                {
                    "line": 21,
                    "comment": "#     if (commit.committer_date.month != curr_month):",
                    "char_position_in_line": 0,
                    "type": [
                        "commented-out",
                        "block"
                    ]
                },
                {
                    "line": 22,
                    "comment": "#         label = str(curr_month) + \"-\" + str(curr_year)",
                    "char_position_in_line": 0,
                    "type": [
                        "commented-out",
                        "block"
                    ]
                },
                {
                    "line": 23,
                    "comment": "#         commits_monthly.append([label, filecounter, month_commits, month_add, month_del])",
                    "char_position_in_line": 0,
                    "type": [
                        "commented-out",
                        "block"
                    ]
                },
                {
                    "line": 24,
                    "comment": "#         curr_month = commit.committer_date.month",
                    "char_position_in_line": 0,
                    "type": [
                        "commented-out",
                        "block"
                    ]
                },
                {
                    "line": 25,
                    "comment": "#         curr_year = commit.committer_date.year",
                    "char_position_in_line": 0,
                    "type": [
                        "commented-out",
                        "block"
                    ]
                },
                {
                    "line": 26,
                    "comment": "#         i += 1",
                    "char_position_in_line": 0,
                    "type": [
                        "commented-out",
                        "block"
                    ]
                },
                {
                    "line": 27,
                    "comment": "#         filecounter = 0",
                    "char_position_in_line": 0,
                    "type": [
                        "commented-out",
                        "block"
                    ]
                },
                {
                    "line": 28,
                    "comment": "#         month_del = 0",
                    "char_position_in_line": 0,
                    "type": [
                        "commented-out",
                        "block"
                    ]
                },
                {
                    "line": 29,
                    "comment": "#         month_add = 0",
                    "char_position_in_line": 0,
                    "type": [
                        "commented-out",
                        "block"
                    ]
                },
                {
                    "line": 30,
                    "comment": "#         month_commits = 0",
                    "char_position_in_line": 0,
                    "type": [
                        "commented-out",
                        "block"
                    ]
                },
                {
                    "line": 31,
                    "comment": "# ",
                    "char_position_in_line": 0,
                    "type": [
                        "commented-out",
                        "block"
                    ]
                },
                {
                    "line": 32,
                    "comment": "#     month_commits += 1",
                    "char_position_in_line": 0,
                    "type": [
                        "commented-out",
                        "block"
                    ]
                },
                {
                    "line": 33,
                    "comment": "#     for file in commit.modified_files:",
                    "char_position_in_line": 0,
                    "type": [
                        "commented-out",
                        "block"
                    ]
                },
                {
                    "line": 34,
                    "comment": "#         month_add += file.added_lines",
                    "char_position_in_line": 0,
                    "type": [
                        "commented-out",
                        "block"
                    ]
                },
                {
                    "line": 35,
                    "comment": "#         month_del += file.deleted_lines",
                    "char_position_in_line": 0,
                    "type": [
                        "commented-out",
                        "block"
                    ]
                },
                {
                    "line": 36,
                    "comment": "#         filecounter += 1",
                    "char_position_in_line": 0,
                    "type": [
                        "commented-out",
                        "block"
                    ]
                },
                {
                    "line": 37,
                    "comment": "#     ",
                    "char_position_in_line": 0,
                    "type": [
                        "commented-out",
                        "block"
                    ]
                },
                {
                    "line": 38,
                    "comment": "# commits = []",
                    "char_position_in_line": 0,
                    "type": [
                        "commented-out",
                        "block"
                    ]
                },
                {
                    "line": 39,
                    "comment": "# additions = []",
                    "char_position_in_line": 0,
                    "type": [
                        "commented-out",
                        "block"
                    ]
                },
                {
                    "line": 40,
                    "comment": "# deletions = []",
                    "char_position_in_line": 0,
                    "type": [
                        "commented-out",
                        "block"
                    ]
                },
                {
                    "line": 41,
                    "comment": "# files = []",
                    "char_position_in_line": 0,
                    "type": [
                        "commented-out",
                        "block"
                    ]
                },
                {
                    "line": 42,
                    "comment": "# loc = []",
                    "char_position_in_line": 0,
                    "type": [
                        "commented-out",
                        "block"
                    ]
                },
                {
                    "line": 43,
                    "comment": "# issues = []",
                    "char_position_in_line": 0,
                    "type": [
                        "commented-out",
                        "block"
                    ]
                },
                {
                    "line": 44,
                    "comment": "# ",
                    "char_position_in_line": 0,
                    "type": [
                        "commented-out",
                        "block"
                    ]
                },
                {
                    "line": 45,
                    "comment": "# for i in range(0, len(commits_monthly)):",
                    "char_position_in_line": 0,
                    "type": [
                        "commented-out",
                        "block"
                    ]
                },
                {
                    "line": 46,
                    "comment": "#     commits.append(commits_monthly[i][1])",
                    "char_position_in_line": 0,
                    "type": [
                        "commented-out",
                        "block"
                    ]
                },
                {
                    "line": 47,
                    "comment": "#     files.append(commits_monthly[i][2])",
                    "char_position_in_line": 0,
                    "type": [
                        "commented-out",
                        "block"
                    ]
                },
                {
                    "line": 48,
                    "comment": "#     additions.append(commits_monthly[i][3])",
                    "char_position_in_line": 0,
                    "type": [
                        "commented-out",
                        "block"
                    ]
                },
                {
                    "line": 49,
                    "comment": "#     deletions.append(commits_monthly[i][4])",
                    "char_position_in_line": 0,
                    "type": [
                        "commented-out",
                        "block"
                    ]
                },
                {
                    "line": 50,
                    "comment": "#     if (i == 0):",
                    "char_position_in_line": 0,
                    "type": [
                        "commented-out",
                        "block"
                    ]
                },
                {
                    "line": 51,
                    "comment": "#         loc.append(additions[i] - deletions[i])",
                    "char_position_in_line": 0,
                    "type": [
                        "commented-out",
                        "block"
                    ]
                },
                {
                    "line": 52,
                    "comment": "#     else: ",
                    "char_position_in_line": 0,
                    "type": [
                        "commented-out",
                        "block"
                    ]
                },
                {
                    "line": 53,
                    "comment": "#         loc.append(loc[i-1] + additions[i] - deletions[i])",
                    "char_position_in_line": 0,
                    "type": [
                        "commented-out",
                        "block"
                    ]
                },
                {
                    "line": 54,
                    "comment": "# ",
                    "char_position_in_line": 0,
                    "type": [
                        "commented-out",
                        "block"
                    ]
                },
                {
                    "line": 55,
                    "comment": "# api_url = \"https://api.github.com/repos/dani-garcia/vaultwarden/issues\"",
                    "char_position_in_line": 0,
                    "type": [
                        "commented-out",
                        "block"
                    ]
                },
                {
                    "line": 56,
                    "comment": "# for i in range (1, 50):",
                    "char_position_in_line": 0,
                    "type": [
                        "commented-out",
                        "block"
                    ]
                },
                {
                    "line": 57,
                    "comment": "#     params = {",
                    "char_position_in_line": 0,
                    "type": [
                        "commented-out",
                        "block"
                    ]
                },
                {
                    "line": 58,
                    "comment": "#         \"state\": \"all\",",
                    "char_position_in_line": 0,
                    "type": [
                        "commented-out",
                        "block"
                    ]
                },
                {
                    "line": 59,
                    "comment": "#         \"direction\": \"asc\",",
                    "char_position_in_line": 0,
                    "type": [
                        "commented-out",
                        "block"
                    ]
                },
                {
                    "line": 60,
                    "comment": "#         \"per_page\": 100,",
                    "char_position_in_line": 0,
                    "type": [
                        "commented-out",
                        "block"
                    ]
                },
                {
                    "line": 61,
                    "comment": "#         \"page\" : i",
                    "char_position_in_line": 0,
                    "type": [
                        "commented-out",
                        "block"
                    ]
                },
                {
                    "line": 62,
                    "comment": "#     }",
                    "char_position_in_line": 0,
                    "type": [
                        "commented-out",
                        "block"
                    ]
                },
                {
                    "line": 63,
                    "comment": "#     response = requests.get(api_url, params=params)",
                    "char_position_in_line": 0,
                    "type": [
                        "commented-out",
                        "block"
                    ]
                },
                {
                    "line": 64,
                    "comment": "#     issues_response = response.json()",
                    "char_position_in_line": 0,
                    "type": [
                        "commented-out",
                        "block"
                    ]
                },
                {
                    "line": 65,
                    "comment": "#     for issue in issues_response:",
                    "char_position_in_line": 0,
                    "type": [
                        "commented-out",
                        "block"
                    ]
                },
                {
                    "line": 66,
                    "comment": "#         issue_data = []",
                    "char_position_in_line": 0,
                    "type": [
                        "commented-out",
                        "block"
                    ]
                },
                {
                    "line": 67,
                    "comment": "#         issue_data.append(issue['number'])",
                    "char_position_in_line": 0,
                    "type": [
                        "commented-out",
                        "block"
                    ]
                },
                {
                    "line": 68,
                    "comment": "#         issue_data.append(issue['created_at'])",
                    "char_position_in_line": 0,
                    "type": [
                        "commented-out",
                        "block"
                    ]
                },
                {
                    "line": 69,
                    "comment": "#         issue_data.append(issue['closed_at'])",
                    "char_position_in_line": 0,
                    "type": [
                        "commented-out",
                        "block"
                    ]
                },
                {
                    "line": 70,
                    "comment": "#         issues.append(issue_data)",
                    "char_position_in_line": 0,
                    "type": [
                        "commented-out",
                        "block"
                    ]
                },
                {
                    "line": 71,
                    "comment": "# ",
                    "char_position_in_line": 0,
                    "type": [
                        "commented-out",
                        "block"
                    ]
                },
                {
                    "line": 72,
                    "comment": "# m = 0",
                    "char_position_in_line": 0,
                    "type": [
                        "commented-out",
                        "block"
                    ]
                },
                {
                    "line": 73,
                    "comment": "# issues_monthly = [0]",
                    "char_position_in_line": 0,
                    "type": [
                        "commented-out",
                        "block"
                    ]
                },
                {
                    "line": 74,
                    "comment": "# for i in range(len(issues)):",
                    "char_position_in_line": 0,
                    "type": [
                        "commented-out",
                        "block"
                    ]
                },
                {
                    "line": 75,
                    "comment": "#     if (issues[i][1][2:5] != issues[i+1][1][2:5]):",
                    "char_position_in_line": 0,
                    "type": [
                        "commented-out",
                        "block"
                    ]
                },
                {
                    "line": 76,
                    "comment": "#         m += 1",
                    "char_position_in_line": 0,
                    "type": [
                        "commented-out",
                        "block"
                    ]
                },
                {
                    "line": 77,
                    "comment": "#         issues_monthly.append(0)",
                    "char_position_in_line": 0,
                    "type": [
                        "commented-out",
                        "block"
                    ]
                },
                {
                    "line": 78,
                    "comment": "#     issues_monthly[m] += 1",
                    "char_position_in_line": 0,
                    "type": [
                        "commented-out",
                        "block"
                    ]
                },
                {
                    "line": 79,
                    "comment": "# ",
                    "char_position_in_line": 0,
                    "type": [
                        "commented-out",
                        "block"
                    ]
                },
                {
                    "line": 80,
                    "comment": "# fig, ax1 = plt.subplots()",
                    "char_position_in_line": 0,
                    "type": [
                        "commented-out",
                        "block"
                    ]
                },
                {
                    "line": 81,
                    "comment": "# ax1.plot(additions, color='g', label='Additions')",
                    "char_position_in_line": 0,
                    "type": [
                        "commented-out",
                        "block"
                    ]
                },
                {
                    "line": 82,
                    "comment": "# ax1.plot(deletions, color='r', label='Deletions')",
                    "char_position_in_line": 0,
                    "type": [
                        "commented-out",
                        "block"
                    ]
                },
                {
                    "line": 83,
                    "comment": "# ax1.plot(loc, color='0', label='LOC')",
                    "char_position_in_line": 0,
                    "type": [
                        "commented-out",
                        "block"
                    ]
                },
                {
                    "line": 84,
                    "comment": "# ax1.tick_params(axis='y', labelcolor='0.5')",
                    "char_position_in_line": 0,
                    "type": [
                        "commented-out",
                        "block"
                    ]
                },
                {
                    "line": 85,
                    "comment": "# ",
                    "char_position_in_line": 0,
                    "type": [
                        "commented-out",
                        "block"
                    ]
                },
                {
                    "line": 86,
                    "comment": "# ax2 = ax1.twinx()",
                    "char_position_in_line": 0,
                    "type": [
                        "commented-out",
                        "block"
                    ]
                },
                {
                    "line": 87,
                    "comment": "# ax2.plot(commits, color='y', label='Commits')",
                    "char_position_in_line": 0,
                    "type": [
                        "commented-out",
                        "block"
                    ]
                },
                {
                    "line": 88,
                    "comment": "# ax2.plot(files, color='b', label='Files')",
                    "char_position_in_line": 0,
                    "type": [
                        "commented-out",
                        "block"
                    ]
                },
                {
                    "line": 89,
                    "comment": "# ax2.plot(issues_monthly, color='0.3', label='Created Issues')",
                    "char_position_in_line": 0,
                    "type": [
                        "commented-out",
                        "block"
                    ]
                },
                {
                    "line": 90,
                    "comment": "# ax2.tick_params(axis='y', labelcolor='0.8')",
                    "char_position_in_line": 0,
                    "type": [
                        "commented-out",
                        "block"
                    ]
                },
                {
                    "line": 91,
                    "comment": "# ",
                    "char_position_in_line": 0,
                    "type": [
                        "commented-out",
                        "block"
                    ]
                },
                {
                    "line": 92,
                    "comment": "# fig.tight_layout()",
                    "char_position_in_line": 0,
                    "type": [
                        "commented-out",
                        "block"
                    ]
                },
                {
                    "line": 93,
                    "comment": "# ax1.legend()",
                    "char_position_in_line": 0,
                    "type": [
                        "commented-out",
                        "block"
                    ]
                },
                {
                    "line": 94,
                    "comment": "# ax2.legend()",
                    "char_position_in_line": 0,
                    "type": [
                        "commented-out",
                        "block"
                    ]
                },
                {
                    "line": 95,
                    "comment": "# plt.savefig(\"GitVisualisation.pdf\", format=\"pdf\")",
                    "char_position_in_line": 0,
                    "type": [
                        "commented-out",
                        "block"
                    ]
                },
                {
                    "line": 96,
                    "comment": "# plt.show()",
                    "char_position_in_line": 0,
                    "type": [
                        "commented-out",
                        "block"
                    ]
                },
                {
                    "line": 97,
                    "comment": "# ",
                    "char_position_in_line": 0,
                    "type": [
                        "commented-out",
                        "block"
                    ]
                },
                {
                    "line": 103,
                    "comment": "# Initialize tracking variables",
                    "char_position_in_line": 4,
                    "type": [
                        "normal"
                    ]
                },
                {
                    "line": 112,
                    "comment": "# Traverse through all commits in the repository",
                    "char_position_in_line": 4,
                    "type": [
                        "normal"
                    ]
                },
                {
                    "line": 114,
                    "comment": "# If we encounter a new month, save the previous month's data",
                    "char_position_in_line": 8,
                    "type": [
                        "normal"
                    ]
                },
                {
                    "line": 118,
                    "comment": "# Reset counters for the new month",
                    "char_position_in_line": 12,
                    "type": [
                        "normal"
                    ]
                },
                {
                    "line": 126,
                    "comment": "# Update current month's data",
                    "char_position_in_line": 8,
                    "type": [
                        "normal"
                    ]
                },
                {
                    "line": 139,
                    "comment": "# Fetch issues data from GitHub using pagination",
                    "char_position_in_line": 4,
                    "type": [
                        "normal"
                    ]
                },
                {
                    "line": 153,
                    "comment": "# Stop if no more issues are returned",
                    "char_position_in_line": 8,
                    "type": [
                        "normal"
                    ]
                },
                {
                    "line": 157,
                    "comment": "# Extract necessary issue data",
                    "char_position_in_line": 8,
                    "type": [
                        "normal"
                    ]
                },
                {
                    "line": 168,
                    "comment": "# Count issues per month",
                    "char_position_in_line": 4,
                    "type": [
                        "normal"
                    ]
                },
                {
                    "line": 179,
                    "comment": "# Add the last month's count",
                    "char_position_in_line": 4,
                    "type": [
                        "normal"
                    ]
                },
                {
                    "line": 184,
                    "comment": "# Calculate lines of code (LOC) changes over time",
                    "char_position_in_line": 4,
                    "type": [
                        "normal"
                    ]
                },
                {
                    "line": 197,
                    "comment": "# Extract data for plotting",
                    "char_position_in_line": 4,
                    "type": [
                        "normal"
                    ]
                },
                {
                    "line": 211,
                    "comment": "# Plotting",
                    "char_position_in_line": 4,
                    "type": [
                        "commented-out"
                    ]
                },
                {
                    "line": 214,
                    "comment": "# Plot lines for additions, deletions, and LOC",
                    "char_position_in_line": 4,
                    "type": [
                        "normal"
                    ]
                },
                {
                    "line": 220,
                    "comment": "# Secondary Y-axis for commits, modified files, and issues",
                    "char_position_in_line": 4,
                    "type": [
                        "normal"
                    ]
                },
                {
                    "line": 227,
                    "comment": "# Finalize and show the plot",
                    "char_position_in_line": 4,
                    "type": [
                        "normal"
                    ]
                }
            ]
        }
    ],
    "build/XESConversion.py": [
        {
            "commit": "a1ad5c2cb35d621f2b187166af65a2b2ee3ea45e",
            "timestamp": "2024-10-24T15:08:06+02:00",
            "author": "alexander.schranner",
            "diff": {
                "added": {
                    "1": "from pydriller import Repository",
                    "2": "import json",
                    "3": "import pm4py",
                    "4": "from pm4py.objects.log.obj import EventLog, Trace, Event",
                    "5": "from pm4py.objects.log.exporter.xes import exporter as xes_exporter",
                    "6": "",
                    "7": "def analyze_commits(repo_url):",
                    "8": "    # This will hold the data for each file and its changes across commits",
                    "9": "    commits_data = []",
                    "10": "",
                    "11": "    # Traverse through the commits in the repository",
                    "12": "    for commit in Repository(repo_url).traverse_commits():",
                    "13": "        commit_data = {",
                    "14": "            \"timestamp\": commit.committer_date.isoformat(),",
                    "15": "            \"author\": commit.author.name,",
                    "16": "            \"files\": []",
                    "17": "        }",
                    "18": "",
                    "19": "        # Analyze each file modified in the commit",
                    "20": "        for modified_file in commit.modified_files:",
                    "21": "            file_data = {",
                    "22": "                \"filename\": modified_file.filename,",
                    "23": "                \"additions\": modified_file.added_lines,",
                    "24": "                \"deletions\": modified_file.deleted_lines,",
                    "25": "                \"change_type\": modified_file.change_type.name,",
                    "26": "                \"commit_message\": commit.msg",
                    "27": "            }",
                    "28": "",
                    "29": "            # Use commit message keywords to determine activity type",
                    "30": "            if \"bug\" in commit.msg.lower() or \"fix\" in commit.msg.lower():",
                    "31": "                file_data[\"activity\"] = \"Bug Fix\"",
                    "32": "            elif \"feature\" in commit.msg.lower() or \"add\" in commit.msg.lower():",
                    "33": "                file_data[\"activity\"] = \"Feature Development\"",
                    "34": "            elif \"refactor\" in commit.msg.lower():",
                    "35": "                file_data[\"activity\"] = \"Refactoring\"",
                    "36": "            else:",
                    "37": "                file_data[\"activity\"] = \"Other\"",
                    "38": "",
                    "39": "            # Generate effect/meaning keywords based on the commit message and type of changes",
                    "40": "            file_data[\"effect_keywords\"] = extract_keywords(commit.msg, modified_file)",
                    "41": "",
                    "42": "            commit_data[\"files\"].append(file_data)",
                    "43": "",
                    "44": "        # Store the processed commit data",
                    "45": "        commits_data.append(commit_data)",
                    "46": "",
                    "47": "    return commits_data",
                    "48": "",
                    "49": "def extract_keywords(commit_message, modified_file):",
                    "50": "    # This function can use NLP techniques or simple keyword extraction",
                    "51": "    # Here, a simplified approach is used: basic keywords based on the commit message",
                    "52": "    keywords = []",
                    "53": "    if \"performance\" in commit_message.lower():",
                    "54": "        keywords.append(\"performance\")",
                    "55": "    if \"security\" in commit_message.lower():",
                    "56": "        keywords.append(\"security\")",
                    "57": "    if modified_file.added_lines > modified_file.deleted_lines:",
                    "58": "        keywords.append(\"expansion\")",
                    "59": "    else:",
                    "60": "        keywords.append(\"optimization\")",
                    "61": "",
                    "62": "    return keywords",
                    "63": "",
                    "64": "def save_to_json(commits_data, filename):",
                    "65": "    # Save the processed commit data to a JSON file",
                    "66": "    with open(filename, 'w') as json_file:",
                    "67": "        json.dump(commits_data, json_file, indent=4)",
                    "68": "",
                    "69": "",
                    "70": "",
                    "71": "def create_xes_log(commits_data):",
                    "72": "    # Create a new EventLog object",
                    "73": "    log = EventLog()",
                    "74": "",
                    "75": "    # Iterate over each commit entry in the data",
                    "76": "    for commit_data in commits_data:",
                    "77": "        # For each file affected in the commit, create a trace",
                    "78": "        for file_data in commit_data['files']:",
                    "79": "            # Check if a trace for this file already exists, if not, create one",
                    "80": "            trace_name = file_data['filename']",
                    "81": "            trace = next((t for t in log if t.attributes.get(\"concept:name\") == trace_name), None)",
                    "82": "",
                    "83": "            if trace is None:",
                    "84": "                trace = Trace()",
                    "85": "                trace.attributes[\"concept:name\"] = trace_name",
                    "86": "                log.append(trace)",
                    "87": "",
                    "88": "            # Create an event for the current commit affecting this file",
                    "89": "            event = Event()",
                    "90": "            event[\"concept:name\"] = file_data['activity']",
                    "91": "            event[\"time:timestamp\"] = commit_data['timestamp']",
                    "92": "            event[\"org:resource\"] = commit_data['author']",
                    "93": "",
                    "94": "            # Add custom attributes for the event",
                    "95": "            event[\"additions\"] = file_data['additions']",
                    "96": "            event[\"deletions\"] = file_data['deletions']",
                    "97": "            event[\"change_type\"] = file_data['change_type']",
                    "98": "            event[\"commit_message\"] = file_data['commit_message']",
                    "99": "            event[\"effect_keywords\"] = ', '.join(file_data['effect_keywords'])",
                    "100": "",
                    "101": "            # Append the event to the trace",
                    "102": "            trace.append(event)",
                    "103": "",
                    "104": "    return log",
                    "105": "",
                    "106": "def save_xes_log(log, filename):",
                    "107": "    # Export the log to an XES file",
                    "108": "    xes_exporter.apply(log, filename)",
                    "109": "",
                    "110": "if __name__ == \"__main__\":",
                    "111": "    repo_url = \"https://github.com/dani-garcia/vaultwarden\"  # Example repository URL",
                    "112": "    commits_data = analyze_commits(repo_url)",
                    "113": "    save_to_json(commits_data, \"commits_data.json\")",
                    "114": "    print(\"Commit data has been saved to commits_data.json\")",
                    "115": "     # Load the previously saved commit data JSON file",
                    "116": "    with open(\"commits_data.json\", \"r\") as json_file:",
                    "117": "        commits_data = json.load(json_file)",
                    "118": "",
                    "119": "    # Create the XES log from the commit data",
                    "120": "    xes_log = create_xes_log(commits_data)",
                    "121": "",
                    "122": "    # Save the XES log to a file",
                    "123": "    save_xes_log(xes_log, \"commits_data.xes\")",
                    "124": "",
                    "125": "    print(\"XES log has been saved to commits_data.xes\")"
                },
                "deleted": {}
            },
            "source_code": {
                "1": "from pydriller import Repository",
                "2": "import json",
                "3": "import pm4py",
                "4": "from pm4py.objects.log.obj import EventLog, Trace, Event",
                "5": "from pm4py.objects.log.exporter.xes import exporter as xes_exporter",
                "6": "",
                "7": "def analyze_commits(repo_url):",
                "8": "    # This will hold the data for each file and its changes across commits",
                "9": "    commits_data = []",
                "10": "",
                "11": "    # Traverse through the commits in the repository",
                "12": "    for commit in Repository(repo_url).traverse_commits():",
                "13": "        commit_data = {",
                "14": "            \"timestamp\": commit.committer_date.isoformat(),",
                "15": "            \"author\": commit.author.name,",
                "16": "            \"files\": []",
                "17": "        }",
                "18": "",
                "19": "        # Analyze each file modified in the commit",
                "20": "        for modified_file in commit.modified_files:",
                "21": "            file_data = {",
                "22": "                \"filename\": modified_file.filename,",
                "23": "                \"additions\": modified_file.added_lines,",
                "24": "                \"deletions\": modified_file.deleted_lines,",
                "25": "                \"change_type\": modified_file.change_type.name,",
                "26": "                \"commit_message\": commit.msg",
                "27": "            }",
                "28": "",
                "29": "            # Use commit message keywords to determine activity type",
                "30": "            if \"bug\" in commit.msg.lower() or \"fix\" in commit.msg.lower():",
                "31": "                file_data[\"activity\"] = \"Bug Fix\"",
                "32": "            elif \"feature\" in commit.msg.lower() or \"add\" in commit.msg.lower():",
                "33": "                file_data[\"activity\"] = \"Feature Development\"",
                "34": "            elif \"refactor\" in commit.msg.lower():",
                "35": "                file_data[\"activity\"] = \"Refactoring\"",
                "36": "            else:",
                "37": "                file_data[\"activity\"] = \"Other\"",
                "38": "",
                "39": "            # Generate effect/meaning keywords based on the commit message and type of changes",
                "40": "            file_data[\"effect_keywords\"] = extract_keywords(commit.msg, modified_file)",
                "41": "",
                "42": "            commit_data[\"files\"].append(file_data)",
                "43": "",
                "44": "        # Store the processed commit data",
                "45": "        commits_data.append(commit_data)",
                "46": "",
                "47": "    return commits_data",
                "48": "",
                "49": "def extract_keywords(commit_message, modified_file):",
                "50": "    # This function can use NLP techniques or simple keyword extraction",
                "51": "    # Here, a simplified approach is used: basic keywords based on the commit message",
                "52": "    keywords = []",
                "53": "    if \"performance\" in commit_message.lower():",
                "54": "        keywords.append(\"performance\")",
                "55": "    if \"security\" in commit_message.lower():",
                "56": "        keywords.append(\"security\")",
                "57": "    if modified_file.added_lines > modified_file.deleted_lines:",
                "58": "        keywords.append(\"expansion\")",
                "59": "    else:",
                "60": "        keywords.append(\"optimization\")",
                "61": "",
                "62": "    return keywords",
                "63": "",
                "64": "def save_to_json(commits_data, filename):",
                "65": "    # Save the processed commit data to a JSON file",
                "66": "    with open(filename, 'w') as json_file:",
                "67": "        json.dump(commits_data, json_file, indent=4)",
                "68": "",
                "69": "",
                "70": "",
                "71": "def create_xes_log(commits_data):",
                "72": "    # Create a new EventLog object",
                "73": "    log = EventLog()",
                "74": "",
                "75": "    # Iterate over each commit entry in the data",
                "76": "    for commit_data in commits_data:",
                "77": "        # For each file affected in the commit, create a trace",
                "78": "        for file_data in commit_data['files']:",
                "79": "            # Check if a trace for this file already exists, if not, create one",
                "80": "            trace_name = file_data['filename']",
                "81": "            trace = next((t for t in log if t.attributes.get(\"concept:name\") == trace_name), None)",
                "82": "            ",
                "83": "            if trace is None:",
                "84": "                trace = Trace()",
                "85": "                trace.attributes[\"concept:name\"] = trace_name",
                "86": "                log.append(trace)",
                "87": "",
                "88": "            # Create an event for the current commit affecting this file",
                "89": "            event = Event()",
                "90": "            event[\"concept:name\"] = file_data['activity']",
                "91": "            event[\"time:timestamp\"] = commit_data['timestamp']",
                "92": "            event[\"org:resource\"] = commit_data['author']",
                "93": "",
                "94": "            # Add custom attributes for the event",
                "95": "            event[\"additions\"] = file_data['additions']",
                "96": "            event[\"deletions\"] = file_data['deletions']",
                "97": "            event[\"change_type\"] = file_data['change_type']",
                "98": "            event[\"commit_message\"] = file_data['commit_message']",
                "99": "            event[\"effect_keywords\"] = ', '.join(file_data['effect_keywords'])",
                "100": "",
                "101": "            # Append the event to the trace",
                "102": "            trace.append(event)",
                "103": "",
                "104": "    return log",
                "105": "",
                "106": "def save_xes_log(log, filename):",
                "107": "    # Export the log to an XES file",
                "108": "    xes_exporter.apply(log, filename)",
                "109": "",
                "110": "if __name__ == \"__main__\":",
                "111": "    repo_url = \"https://github.com/dani-garcia/vaultwarden\"  # Example repository URL",
                "112": "    commits_data = analyze_commits(repo_url)",
                "113": "    save_to_json(commits_data, \"commits_data.json\")",
                "114": "    print(\"Commit data has been saved to commits_data.json\")",
                "115": "     # Load the previously saved commit data JSON file",
                "116": "    with open(\"commits_data.json\", \"r\") as json_file:",
                "117": "        commits_data = json.load(json_file)",
                "118": "",
                "119": "    # Create the XES log from the commit data",
                "120": "    xes_log = create_xes_log(commits_data)",
                "121": "",
                "122": "    # Save the XES log to a file",
                "123": "    save_xes_log(xes_log, \"commits_data.xes\")",
                "124": "",
                "125": "    print(\"XES log has been saved to commits_data.xes\")",
                "126": ""
            },
            "comments": [
                {
                    "line": 8,
                    "comment": "# This will hold the data for each file and its changes across commits",
                    "char_position_in_line": 4,
                    "type": [
                        "normal"
                    ]
                },
                {
                    "line": 11,
                    "comment": "# Traverse through the commits in the repository",
                    "char_position_in_line": 4,
                    "type": [
                        "normal"
                    ]
                },
                {
                    "line": 19,
                    "comment": "# Analyze each file modified in the commit",
                    "char_position_in_line": 8,
                    "type": [
                        "normal"
                    ]
                },
                {
                    "line": 29,
                    "comment": "# Use commit message keywords to determine activity type",
                    "char_position_in_line": 12,
                    "type": [
                        "normal"
                    ]
                },
                {
                    "line": 39,
                    "comment": "# Generate effect/meaning keywords based on the commit message and type of changes",
                    "char_position_in_line": 12,
                    "type": [
                        "normal"
                    ]
                },
                {
                    "line": 44,
                    "comment": "# Store the processed commit data",
                    "char_position_in_line": 8,
                    "type": [
                        "normal"
                    ]
                },
                {
                    "line": 50,
                    "comment": "# This function can use NLP techniques or simple keyword extraction",
                    "char_position_in_line": 4,
                    "type": [
                        "block"
                    ]
                },
                {
                    "line": 51,
                    "comment": "# Here, a simplified approach is used: basic keywords based on the commit message",
                    "char_position_in_line": 4,
                    "type": [
                        "block"
                    ]
                },
                {
                    "line": 65,
                    "comment": "# Save the processed commit data to a JSON file",
                    "char_position_in_line": 4,
                    "type": [
                        "normal"
                    ]
                },
                {
                    "line": 72,
                    "comment": "# Create a new EventLog object",
                    "char_position_in_line": 4,
                    "type": [
                        "normal"
                    ]
                },
                {
                    "line": 75,
                    "comment": "# Iterate over each commit entry in the data",
                    "char_position_in_line": 4,
                    "type": [
                        "normal"
                    ]
                },
                {
                    "line": 77,
                    "comment": "# For each file affected in the commit, create a trace",
                    "char_position_in_line": 8,
                    "type": [
                        "normal"
                    ]
                },
                {
                    "line": 79,
                    "comment": "# Check if a trace for this file already exists, if not, create one",
                    "char_position_in_line": 12,
                    "type": [
                        "normal"
                    ]
                },
                {
                    "line": 88,
                    "comment": "# Create an event for the current commit affecting this file",
                    "char_position_in_line": 12,
                    "type": [
                        "normal"
                    ]
                },
                {
                    "line": 94,
                    "comment": "# Add custom attributes for the event",
                    "char_position_in_line": 12,
                    "type": [
                        "normal"
                    ]
                },
                {
                    "line": 101,
                    "comment": "# Append the event to the trace",
                    "char_position_in_line": 12,
                    "type": [
                        "normal"
                    ]
                },
                {
                    "line": 107,
                    "comment": "# Export the log to an XES file",
                    "char_position_in_line": 4,
                    "type": [
                        "normal"
                    ]
                },
                {
                    "line": 111,
                    "comment": "# Example repository URL",
                    "char_position_in_line": 61,
                    "type": [
                        "normal"
                    ]
                },
                {
                    "line": 115,
                    "comment": "# Load the previously saved commit data JSON file",
                    "char_position_in_line": 5,
                    "type": [
                        "normal"
                    ]
                },
                {
                    "line": 119,
                    "comment": "# Create the XES log from the commit data",
                    "char_position_in_line": 4,
                    "type": [
                        "normal"
                    ]
                },
                {
                    "line": 122,
                    "comment": "# Save the XES log to a file",
                    "char_position_in_line": 4,
                    "type": [
                        "normal"
                    ]
                }
            ]
        },
        {
            "commit": "1061293a43b0788f9db921ae6fc61734ccdf1b8d",
            "timestamp": "2024-11-16T19:39:45+01:00",
            "author": "alexander.schranner",
            "diff": {
                "added": {
                    "4": "import datetime",
                    "8": "def analyze_commits(repo_url, comment_symbol, language_file_extension):",
                    "10": "    files_data = {}",
                    "12": "    # Analysis range",
                    "13": "    dt1 = datetime.datetime(2022, 10, 8, 17, 0, 0)",
                    "14": "    dt2 = datetime.datetime(2023, 10, 8, 17, 59, 0)",
                    "16": "    # Traverse through the commits in the repository",
                    "17": "    # Only save commits, that contain at least one file of the format {language_file_extension}",
                    "18": "    for commit in Repository(repo_url,",
                    "19": "    only_modifications_with_file_types=[f\".{language_file_extension}\"],",
                    "20": "    since=dt1,",
                    "21": "    to=dt2).traverse_commits():",
                    "24": "            # only store file data for Rust files",
                    "25": "            if modified_file.filename not in files_data:",
                    "26": "                files_data[modified_file.filename] = []",
                    "27": "            if len(modified_file.filename.split(\".\")) == 2 and modified_file.filename.split(\".\")[1] == language_file_extension:",
                    "28": "                file_data = {",
                    "29": "                    \"commit\": commit.hash,",
                    "30": "                    \"timestamp\": commit.committer_date.isoformat(),",
                    "31": "                    \"author\": commit.author.name,",
                    "32": "                    \"commit_message\": commit.msg,",
                    "33": "                    \"additions\": modified_file.added_lines,",
                    "34": "                    \"deletions\": modified_file.deleted_lines,",
                    "35": "                    \"change_type\": modified_file.change_type.name,",
                    "36": "                    \"diff\": modified_file.diff",
                    "37": "                }",
                    "38": "                diff_added = {}",
                    "39": "                diff_deleted = {}",
                    "40": "                diff_modified = {}",
                    "41": "                for line in modified_file.diff_parsed[\"added\"]:",
                    "42": "                    if line[1].find(comment_symbol) != -1:",
                    "43": "                        diff_added[line[0]] = line[1]",
                    "44": "                file_data[\"comment_added_diff\"] = diff_added",
                    "45": "                for line in modified_file.diff_parsed[\"deleted\"]:",
                    "46": "                    if line[1].find(comment_symbol) != -1:",
                    "47": "                        diff_deleted[line[0]] = line[1]",
                    "48": "                    if line[0] in diff_added.keys():",
                    "49": "                        diff_modified[line[0]] = line[1]",
                    "50": "                file_data[\"comment_deleted_diff\"] = diff_deleted",
                    "51": "                file_data[\"comment_modified_diff\"] = diff_modified",
                    "52": "                # Generate keywords based on the commit message and type of changes",
                    "53": "                # file_data[\"keywords\"] = extract_keywords(commit.msg, modified_file)",
                    "54": "                # Extract type of commit from commit message",
                    "55": "                # file_data[\"activity\"] = extract_activity(commit.msg)",
                    "56": "                if len(diff_added) + len(diff_deleted) != 0:",
                    "57": "                    files_data[modified_file.filename].append(file_data)",
                    "58": "    return files_data",
                    "74": "def extract_activity(commit_message):",
                    "75": "    # Use commit message keywords to determine activity type",
                    "76": "    activity = \"\"",
                    "77": "    if \"bug\" in commit.msg.lower() or \"fix\" in commit.msg.lower():",
                    "78": "        activity = \"Bug Fix\"",
                    "79": "    elif \"feature\" in commit.msg.lower() or \"add\" in commit.msg.lower():",
                    "80": "        activity = \"Feature Development\"",
                    "81": "    elif \"refactor\" in commit.msg.lower():",
                    "82": "        activity = \"Refactoring\"",
                    "83": "    else:",
                    "84": "        activity = \"Other\"",
                    "85": "    return activity",
                    "86": "",
                    "92": "def create_xes_log(data):",
                    "97": "    for file in data:",
                    "120": "            #event[\"effect_keywords\"] = ', '.join(file_data['effect_keywords'])",
                    "121": "            event[\"diff\"] = file_data[\"diff\"]",
                    "133": "    repo_url = \"https://github.com/numpy/numpy\"  # Example repository URL",
                    "134": "    # commits_data = analyze_commits(repo_url, \"#\", \"py\")",
                    "135": "    # save_to_json(commits_data, \"Data/commits_data.json\")",
                    "136": "    # print(\"Commit data has been saved to commits_data.json\")",
                    "137": "    # Load the previously saved commit data JSON file",
                    "138": "    with open(\"Data/commits_data.json\", \"r\") as json_file:",
                    "139": "       commits_data = json.load(json_file)",
                    "145": "    save_xes_log(xes_log, \"Data/commits_data.xes\")"
                },
                "deleted": {
                    "7": "def analyze_commits(repo_url):",
                    "9": "    commits_data = []",
                    "11": "    # Traverse through the commits in the repository",
                    "12": "    for commit in Repository(repo_url).traverse_commits():",
                    "13": "        commit_data = {",
                    "14": "            \"timestamp\": commit.committer_date.isoformat(),",
                    "15": "            \"author\": commit.author.name,",
                    "16": "            \"files\": []",
                    "17": "        }",
                    "21": "            file_data = {",
                    "22": "                \"filename\": modified_file.filename,",
                    "23": "                \"additions\": modified_file.added_lines,",
                    "24": "                \"deletions\": modified_file.deleted_lines,",
                    "25": "                \"change_type\": modified_file.change_type.name,",
                    "26": "                \"commit_message\": commit.msg",
                    "27": "            }",
                    "28": "",
                    "29": "            # Use commit message keywords to determine activity type",
                    "30": "            if \"bug\" in commit.msg.lower() or \"fix\" in commit.msg.lower():",
                    "31": "                file_data[\"activity\"] = \"Bug Fix\"",
                    "32": "            elif \"feature\" in commit.msg.lower() or \"add\" in commit.msg.lower():",
                    "33": "                file_data[\"activity\"] = \"Feature Development\"",
                    "34": "            elif \"refactor\" in commit.msg.lower():",
                    "35": "                file_data[\"activity\"] = \"Refactoring\"",
                    "36": "            else:",
                    "37": "                file_data[\"activity\"] = \"Other\"",
                    "38": "",
                    "39": "            # Generate effect/meaning keywords based on the commit message and type of changes",
                    "40": "            file_data[\"effect_keywords\"] = extract_keywords(commit.msg, modified_file)",
                    "41": "",
                    "42": "            commit_data[\"files\"].append(file_data)",
                    "43": "",
                    "44": "        # Store the processed commit data",
                    "45": "        commits_data.append(commit_data)",
                    "46": "",
                    "47": "    return commits_data",
                    "61": "",
                    "69": "",
                    "70": "",
                    "71": "def create_xes_log(commits_data):",
                    "76": "    for commit_data in commits_data:",
                    "99": "            event[\"effect_keywords\"] = ', '.join(file_data['effect_keywords'])",
                    "111": "    repo_url = \"https://github.com/dani-garcia/vaultwarden\"  # Example repository URL",
                    "112": "    commits_data = analyze_commits(repo_url)",
                    "113": "    save_to_json(commits_data, \"commits_data.json\")",
                    "114": "    print(\"Commit data has been saved to commits_data.json\")",
                    "115": "     # Load the previously saved commit data JSON file",
                    "116": "    with open(\"commits_data.json\", \"r\") as json_file:",
                    "117": "        commits_data = json.load(json_file)",
                    "123": "    save_xes_log(xes_log, \"commits_data.xes\")",
                    "124": ""
                }
            },
            "source_code": {
                "1": "from pydriller import Repository",
                "2": "import json",
                "3": "import pm4py",
                "4": "import datetime",
                "5": "from pm4py.objects.log.obj import EventLog, Trace, Event",
                "6": "from pm4py.objects.log.exporter.xes import exporter as xes_exporter",
                "7": "",
                "8": "def analyze_commits(repo_url, comment_symbol, language_file_extension):",
                "9": "    # This will hold the data for each file and its changes across commits",
                "10": "    files_data = {}",
                "11": "",
                "12": "    # Analysis range",
                "13": "    dt1 = datetime.datetime(2022, 10, 8, 17, 0, 0)",
                "14": "    dt2 = datetime.datetime(2023, 10, 8, 17, 59, 0)",
                "15": "",
                "16": "    # Traverse through the commits in the repository",
                "17": "    # Only save commits, that contain at least one file of the format {language_file_extension}",
                "18": "    for commit in Repository(repo_url, ",
                "19": "    only_modifications_with_file_types=[f\".{language_file_extension}\"],",
                "20": "    since=dt1,",
                "21": "    to=dt2).traverse_commits():",
                "22": "        # Analyze each file modified in the commit",
                "23": "        for modified_file in commit.modified_files:",
                "24": "            # only store file data for Rust files",
                "25": "            if modified_file.filename not in files_data:",
                "26": "                files_data[modified_file.filename] = []",
                "27": "            if len(modified_file.filename.split(\".\")) == 2 and modified_file.filename.split(\".\")[1] == language_file_extension:",
                "28": "                file_data = {",
                "29": "                    \"commit\": commit.hash,",
                "30": "                    \"timestamp\": commit.committer_date.isoformat(),",
                "31": "                    \"author\": commit.author.name,",
                "32": "                    \"commit_message\": commit.msg,",
                "33": "                    \"additions\": modified_file.added_lines,",
                "34": "                    \"deletions\": modified_file.deleted_lines,",
                "35": "                    \"change_type\": modified_file.change_type.name,",
                "36": "                    \"diff\": modified_file.diff",
                "37": "                }",
                "38": "                diff_added = {}",
                "39": "                diff_deleted = {}",
                "40": "                diff_modified = {}",
                "41": "                for line in modified_file.diff_parsed[\"added\"]:",
                "42": "                    if line[1].find(comment_symbol) != -1:",
                "43": "                        diff_added[line[0]] = line[1]",
                "44": "                file_data[\"comment_added_diff\"] = diff_added",
                "45": "                for line in modified_file.diff_parsed[\"deleted\"]:",
                "46": "                    if line[1].find(comment_symbol) != -1:",
                "47": "                        diff_deleted[line[0]] = line[1]",
                "48": "                    if line[0] in diff_added.keys():",
                "49": "                        diff_modified[line[0]] = line[1]",
                "50": "                file_data[\"comment_deleted_diff\"] = diff_deleted",
                "51": "                file_data[\"comment_modified_diff\"] = diff_modified",
                "52": "                # Generate keywords based on the commit message and type of changes",
                "53": "                # file_data[\"keywords\"] = extract_keywords(commit.msg, modified_file)",
                "54": "                # Extract type of commit from commit message",
                "55": "                # file_data[\"activity\"] = extract_activity(commit.msg)",
                "56": "                if len(diff_added) + len(diff_deleted) != 0:",
                "57": "                    files_data[modified_file.filename].append(file_data)",
                "58": "    return files_data",
                "59": "",
                "60": "def extract_keywords(commit_message, modified_file):",
                "61": "    # This function can use NLP techniques or simple keyword extraction",
                "62": "    # Here, a simplified approach is used: basic keywords based on the commit message",
                "63": "    keywords = []",
                "64": "    if \"performance\" in commit_message.lower():",
                "65": "        keywords.append(\"performance\")",
                "66": "    if \"security\" in commit_message.lower():",
                "67": "        keywords.append(\"security\")",
                "68": "    if modified_file.added_lines > modified_file.deleted_lines:",
                "69": "        keywords.append(\"expansion\")",
                "70": "    else:",
                "71": "        keywords.append(\"optimization\")",
                "72": "    return keywords",
                "73": "",
                "74": "def extract_activity(commit_message):",
                "75": "    # Use commit message keywords to determine activity type",
                "76": "    activity = \"\"",
                "77": "    if \"bug\" in commit.msg.lower() or \"fix\" in commit.msg.lower():",
                "78": "        activity = \"Bug Fix\"",
                "79": "    elif \"feature\" in commit.msg.lower() or \"add\" in commit.msg.lower():",
                "80": "        activity = \"Feature Development\"",
                "81": "    elif \"refactor\" in commit.msg.lower():",
                "82": "        activity = \"Refactoring\"",
                "83": "    else:",
                "84": "        activity = \"Other\"",
                "85": "    return activity",
                "86": "",
                "87": "def save_to_json(commits_data, filename):",
                "88": "    # Save the processed commit data to a JSON file",
                "89": "    with open(filename, 'w') as json_file:",
                "90": "        json.dump(commits_data, json_file, indent=4)",
                "91": "",
                "92": "def create_xes_log(data):",
                "93": "    # Create a new EventLog object",
                "94": "    log = EventLog()",
                "95": "",
                "96": "    # Iterate over each commit entry in the data",
                "97": "    for file in data:",
                "98": "        # For each file affected in the commit, create a trace",
                "99": "        for file_data in commit_data['files']:",
                "100": "            # Check if a trace for this file already exists, if not, create one",
                "101": "            trace_name = file_data['filename']",
                "102": "            trace = next((t for t in log if t.attributes.get(\"concept:name\") == trace_name), None)",
                "103": "            ",
                "104": "            if trace is None:",
                "105": "                trace = Trace()",
                "106": "                trace.attributes[\"concept:name\"] = trace_name",
                "107": "                log.append(trace)",
                "108": "",
                "109": "            # Create an event for the current commit affecting this file",
                "110": "            event = Event()",
                "111": "            event[\"concept:name\"] = file_data['activity']",
                "112": "            event[\"time:timestamp\"] = commit_data['timestamp']",
                "113": "            event[\"org:resource\"] = commit_data['author']",
                "114": "",
                "115": "            # Add custom attributes for the event",
                "116": "            event[\"additions\"] = file_data['additions']",
                "117": "            event[\"deletions\"] = file_data['deletions']",
                "118": "            event[\"change_type\"] = file_data['change_type']",
                "119": "            event[\"commit_message\"] = file_data['commit_message']",
                "120": "            #event[\"effect_keywords\"] = ', '.join(file_data['effect_keywords'])",
                "121": "            event[\"diff\"] = file_data[\"diff\"]",
                "122": "",
                "123": "            # Append the event to the trace",
                "124": "            trace.append(event)",
                "125": "",
                "126": "    return log",
                "127": "",
                "128": "def save_xes_log(log, filename):",
                "129": "    # Export the log to an XES file",
                "130": "    xes_exporter.apply(log, filename)",
                "131": "",
                "132": "if __name__ == \"__main__\":",
                "133": "    repo_url = \"https://github.com/numpy/numpy\"  # Example repository URL",
                "134": "    # commits_data = analyze_commits(repo_url, \"#\", \"py\")",
                "135": "    # save_to_json(commits_data, \"Data/commits_data.json\")",
                "136": "    # print(\"Commit data has been saved to commits_data.json\")",
                "137": "    # Load the previously saved commit data JSON file",
                "138": "    with open(\"Data/commits_data.json\", \"r\") as json_file:",
                "139": "       commits_data = json.load(json_file)",
                "140": "",
                "141": "    # Create the XES log from the commit data",
                "142": "    xes_log = create_xes_log(commits_data)",
                "143": "",
                "144": "    # Save the XES log to a file",
                "145": "    save_xes_log(xes_log, \"Data/commits_data.xes\")",
                "146": "    print(\"XES log has been saved to commits_data.xes\")",
                "147": ""
            },
            "comments": [
                {
                    "line": 9,
                    "comment": "# This will hold the data for each file and its changes across commits",
                    "char_position_in_line": 4,
                    "type": [
                        "normal"
                    ]
                },
                {
                    "line": 12,
                    "comment": "# Analysis range",
                    "char_position_in_line": 4,
                    "type": [
                        "normal"
                    ]
                },
                {
                    "line": 16,
                    "comment": "# Traverse through the commits in the repository",
                    "char_position_in_line": 4,
                    "type": [
                        "block"
                    ]
                },
                {
                    "line": 17,
                    "comment": "# Only save commits, that contain at least one file of the format {language_file_extension}",
                    "char_position_in_line": 4,
                    "type": [
                        "block"
                    ]
                },
                {
                    "line": 22,
                    "comment": "# Analyze each file modified in the commit",
                    "char_position_in_line": 8,
                    "type": [
                        "normal"
                    ]
                },
                {
                    "line": 24,
                    "comment": "# only store file data for Rust files",
                    "char_position_in_line": 12,
                    "type": [
                        "normal"
                    ]
                },
                {
                    "line": 52,
                    "comment": "# Generate keywords based on the commit message and type of changes",
                    "char_position_in_line": 16,
                    "type": [
                        "block"
                    ]
                },
                {
                    "line": 53,
                    "comment": "# file_data[\"keywords\"] = extract_keywords(commit.msg, modified_file)",
                    "char_position_in_line": 16,
                    "type": [
                        "block"
                    ]
                },
                {
                    "line": 54,
                    "comment": "# Extract type of commit from commit message",
                    "char_position_in_line": 16,
                    "type": [
                        "block"
                    ]
                },
                {
                    "line": 55,
                    "comment": "# file_data[\"activity\"] = extract_activity(commit.msg)",
                    "char_position_in_line": 16,
                    "type": [
                        "block"
                    ]
                },
                {
                    "line": 61,
                    "comment": "# This function can use NLP techniques or simple keyword extraction",
                    "char_position_in_line": 4,
                    "type": [
                        "block"
                    ]
                },
                {
                    "line": 62,
                    "comment": "# Here, a simplified approach is used: basic keywords based on the commit message",
                    "char_position_in_line": 4,
                    "type": [
                        "block"
                    ]
                },
                {
                    "line": 75,
                    "comment": "# Use commit message keywords to determine activity type",
                    "char_position_in_line": 4,
                    "type": [
                        "normal"
                    ]
                },
                {
                    "line": 88,
                    "comment": "# Save the processed commit data to a JSON file",
                    "char_position_in_line": 4,
                    "type": [
                        "normal"
                    ]
                },
                {
                    "line": 93,
                    "comment": "# Create a new EventLog object",
                    "char_position_in_line": 4,
                    "type": [
                        "normal"
                    ]
                },
                {
                    "line": 96,
                    "comment": "# Iterate over each commit entry in the data",
                    "char_position_in_line": 4,
                    "type": [
                        "normal"
                    ]
                },
                {
                    "line": 98,
                    "comment": "# For each file affected in the commit, create a trace",
                    "char_position_in_line": 8,
                    "type": [
                        "normal"
                    ]
                },
                {
                    "line": 100,
                    "comment": "# Check if a trace for this file already exists, if not, create one",
                    "char_position_in_line": 12,
                    "type": [
                        "normal"
                    ]
                },
                {
                    "line": 109,
                    "comment": "# Create an event for the current commit affecting this file",
                    "char_position_in_line": 12,
                    "type": [
                        "normal"
                    ]
                },
                {
                    "line": 115,
                    "comment": "# Add custom attributes for the event",
                    "char_position_in_line": 12,
                    "type": [
                        "normal"
                    ]
                },
                {
                    "line": 120,
                    "comment": "#event[\"effect_keywords\"] = ', '.join(file_data['effect_keywords'])",
                    "char_position_in_line": 12,
                    "type": [
                        "commented-out"
                    ]
                },
                {
                    "line": 123,
                    "comment": "# Append the event to the trace",
                    "char_position_in_line": 12,
                    "type": [
                        "normal"
                    ]
                },
                {
                    "line": 129,
                    "comment": "# Export the log to an XES file",
                    "char_position_in_line": 4,
                    "type": [
                        "normal"
                    ]
                },
                {
                    "line": 133,
                    "comment": "# Example repository URL",
                    "char_position_in_line": 49,
                    "type": [
                        "normal"
                    ]
                },
                {
                    "line": 134,
                    "comment": "# commits_data = analyze_commits(repo_url, \"#\", \"py\")",
                    "char_position_in_line": 4,
                    "type": [
                        "commented-out",
                        "block"
                    ]
                },
                {
                    "line": 135,
                    "comment": "# save_to_json(commits_data, \"Data/commits_data.json\")",
                    "char_position_in_line": 4,
                    "type": [
                        "commented-out",
                        "block"
                    ]
                },
                {
                    "line": 136,
                    "comment": "# print(\"Commit data has been saved to commits_data.json\")",
                    "char_position_in_line": 4,
                    "type": [
                        "commented-out",
                        "block"
                    ]
                },
                {
                    "line": 137,
                    "comment": "# Load the previously saved commit data JSON file",
                    "char_position_in_line": 4,
                    "type": [
                        "commented-out",
                        "block"
                    ]
                },
                {
                    "line": 141,
                    "comment": "# Create the XES log from the commit data",
                    "char_position_in_line": 4,
                    "type": [
                        "normal"
                    ]
                },
                {
                    "line": 144,
                    "comment": "# Save the XES log to a file",
                    "char_position_in_line": 4,
                    "type": [
                        "normal"
                    ]
                }
            ]
        },
        {
            "commit": "f73512c4aa778287e31d18e9d218502acf7479ee",
            "timestamp": "2024-11-16T20:51:56+01:00",
            "author": "alexander.schranner",
            "diff": {
                "added": {
                    "97": "    for file, commits in data.items():",
                    "98": "        # Create a trace for the file",
                    "99": "        trace = Trace()",
                    "100": "        trace.attributes[\"file\"] = file",
                    "101": "",
                    "102": "        for commit in commits:",
                    "103": "            # Extract event attributes",
                    "105": "            event[\"timestamp\"] = commit.get(\"timestamp\")",
                    "106": "            event[\"author\"] = commit.get(\"author\")",
                    "107": "            event[\"change_type\"] = commit.get(\"change_type\")",
                    "108": "            event[\"commit_message\"] = commit.get(\"commit_message\")",
                    "109": "            event[\"additions\"] = commit.get(\"additions\")",
                    "110": "            event[\"deletions\"] = commit.get(\"deletions\")",
                    "111": "            event[\"diff\"] = commit.get(\"diff\")",
                    "112": "            if commit.get(\"comment_added_diff\"):",
                    "113": "                event[\"comment_change\"] = \"True\"",
                    "114": "            else:",
                    "115": "                event[\"comment_change\"] = \"False\"",
                    "116": "",
                    "117": "            # Add the event to the trace",
                    "120": "        # Add the trace to the log",
                    "121": "        log.append(trace)",
                    "122": "",
                    "131": "    commits_data = analyze_commits(repo_url, \"#\", \"py\")",
                    "132": "    save_to_json(commits_data, \"Data/commits_data.json\")",
                    "133": "    print(\"Commit data has been saved to commits_data.json\")"
                },
                "deleted": {
                    "97": "    for file in data:",
                    "98": "        # For each file affected in the commit, create a trace",
                    "99": "        for file_data in commit_data['files']:",
                    "100": "            # Check if a trace for this file already exists, if not, create one",
                    "101": "            trace_name = file_data['filename']",
                    "102": "            trace = next((t for t in log if t.attributes.get(\"concept:name\") == trace_name), None)",
                    "103": "",
                    "104": "            if trace is None:",
                    "105": "                trace = Trace()",
                    "106": "                trace.attributes[\"concept:name\"] = trace_name",
                    "107": "                log.append(trace)",
                    "108": "",
                    "109": "            # Create an event for the current commit affecting this file",
                    "111": "            event[\"concept:name\"] = file_data['activity']",
                    "112": "            event[\"time:timestamp\"] = commit_data['timestamp']",
                    "113": "            event[\"org:resource\"] = commit_data['author']",
                    "114": "",
                    "115": "            # Add custom attributes for the event",
                    "116": "            event[\"additions\"] = file_data['additions']",
                    "117": "            event[\"deletions\"] = file_data['deletions']",
                    "118": "            event[\"change_type\"] = file_data['change_type']",
                    "119": "            event[\"commit_message\"] = file_data['commit_message']",
                    "120": "            #event[\"effect_keywords\"] = ', '.join(file_data['effect_keywords'])",
                    "121": "            event[\"diff\"] = file_data[\"diff\"]",
                    "122": "",
                    "123": "            # Append the event to the trace",
                    "134": "    # commits_data = analyze_commits(repo_url, \"#\", \"py\")",
                    "135": "    # save_to_json(commits_data, \"Data/commits_data.json\")",
                    "136": "    # print(\"Commit data has been saved to commits_data.json\")"
                }
            },
            "source_code": {
                "1": "from pydriller import Repository",
                "2": "import json",
                "3": "import pm4py",
                "4": "import datetime",
                "5": "from pm4py.objects.log.obj import EventLog, Trace, Event",
                "6": "from pm4py.objects.log.exporter.xes import exporter as xes_exporter",
                "7": "",
                "8": "def analyze_commits(repo_url, comment_symbol, language_file_extension):",
                "9": "    # This will hold the data for each file and its changes across commits",
                "10": "    files_data = {}",
                "11": "",
                "12": "    # Analysis range",
                "13": "    dt1 = datetime.datetime(2022, 10, 8, 17, 0, 0)",
                "14": "    dt2 = datetime.datetime(2023, 10, 8, 17, 59, 0)",
                "15": "",
                "16": "    # Traverse through the commits in the repository",
                "17": "    # Only save commits, that contain at least one file of the format {language_file_extension}",
                "18": "    for commit in Repository(repo_url, ",
                "19": "    only_modifications_with_file_types=[f\".{language_file_extension}\"],",
                "20": "    since=dt1,",
                "21": "    to=dt2).traverse_commits():",
                "22": "        # Analyze each file modified in the commit",
                "23": "        for modified_file in commit.modified_files:",
                "24": "            # only store file data for Rust files",
                "25": "            if modified_file.filename not in files_data:",
                "26": "                files_data[modified_file.filename] = []",
                "27": "            if len(modified_file.filename.split(\".\")) == 2 and modified_file.filename.split(\".\")[1] == language_file_extension:",
                "28": "                file_data = {",
                "29": "                    \"commit\": commit.hash,",
                "30": "                    \"timestamp\": commit.committer_date.isoformat(),",
                "31": "                    \"author\": commit.author.name,",
                "32": "                    \"commit_message\": commit.msg,",
                "33": "                    \"additions\": modified_file.added_lines,",
                "34": "                    \"deletions\": modified_file.deleted_lines,",
                "35": "                    \"change_type\": modified_file.change_type.name,",
                "36": "                    \"diff\": modified_file.diff",
                "37": "                }",
                "38": "                diff_added = {}",
                "39": "                diff_deleted = {}",
                "40": "                diff_modified = {}",
                "41": "                for line in modified_file.diff_parsed[\"added\"]:",
                "42": "                    if line[1].find(comment_symbol) != -1:",
                "43": "                        diff_added[line[0]] = line[1]",
                "44": "                file_data[\"comment_added_diff\"] = diff_added",
                "45": "                for line in modified_file.diff_parsed[\"deleted\"]:",
                "46": "                    if line[1].find(comment_symbol) != -1:",
                "47": "                        diff_deleted[line[0]] = line[1]",
                "48": "                    if line[0] in diff_added.keys():",
                "49": "                        diff_modified[line[0]] = line[1]",
                "50": "                file_data[\"comment_deleted_diff\"] = diff_deleted",
                "51": "                file_data[\"comment_modified_diff\"] = diff_modified",
                "52": "                # Generate keywords based on the commit message and type of changes",
                "53": "                # file_data[\"keywords\"] = extract_keywords(commit.msg, modified_file)",
                "54": "                # Extract type of commit from commit message",
                "55": "                # file_data[\"activity\"] = extract_activity(commit.msg)",
                "56": "                if len(diff_added) + len(diff_deleted) != 0:",
                "57": "                    files_data[modified_file.filename].append(file_data)",
                "58": "    return files_data",
                "59": "",
                "60": "def extract_keywords(commit_message, modified_file):",
                "61": "    # This function can use NLP techniques or simple keyword extraction",
                "62": "    # Here, a simplified approach is used: basic keywords based on the commit message",
                "63": "    keywords = []",
                "64": "    if \"performance\" in commit_message.lower():",
                "65": "        keywords.append(\"performance\")",
                "66": "    if \"security\" in commit_message.lower():",
                "67": "        keywords.append(\"security\")",
                "68": "    if modified_file.added_lines > modified_file.deleted_lines:",
                "69": "        keywords.append(\"expansion\")",
                "70": "    else:",
                "71": "        keywords.append(\"optimization\")",
                "72": "    return keywords",
                "73": "",
                "74": "def extract_activity(commit_message):",
                "75": "    # Use commit message keywords to determine activity type",
                "76": "    activity = \"\"",
                "77": "    if \"bug\" in commit.msg.lower() or \"fix\" in commit.msg.lower():",
                "78": "        activity = \"Bug Fix\"",
                "79": "    elif \"feature\" in commit.msg.lower() or \"add\" in commit.msg.lower():",
                "80": "        activity = \"Feature Development\"",
                "81": "    elif \"refactor\" in commit.msg.lower():",
                "82": "        activity = \"Refactoring\"",
                "83": "    else:",
                "84": "        activity = \"Other\"",
                "85": "    return activity",
                "86": "",
                "87": "def save_to_json(commits_data, filename):",
                "88": "    # Save the processed commit data to a JSON file",
                "89": "    with open(filename, 'w') as json_file:",
                "90": "        json.dump(commits_data, json_file, indent=4)",
                "91": "",
                "92": "def create_xes_log(data):",
                "93": "    # Create a new EventLog object",
                "94": "    log = EventLog()",
                "95": "",
                "96": "    # Iterate over each commit entry in the data",
                "97": "    for file, commits in data.items():",
                "98": "        # Create a trace for the file",
                "99": "        trace = Trace()",
                "100": "        trace.attributes[\"file\"] = file",
                "101": "",
                "102": "        for commit in commits:",
                "103": "            # Extract event attributes",
                "104": "            event = Event()",
                "105": "            event[\"timestamp\"] = commit.get(\"timestamp\")",
                "106": "            event[\"author\"] = commit.get(\"author\")",
                "107": "            event[\"change_type\"] = commit.get(\"change_type\")",
                "108": "            event[\"commit_message\"] = commit.get(\"commit_message\")",
                "109": "            event[\"additions\"] = commit.get(\"additions\")",
                "110": "            event[\"deletions\"] = commit.get(\"deletions\")",
                "111": "            event[\"diff\"] = commit.get(\"diff\")",
                "112": "            if commit.get(\"comment_added_diff\"):",
                "113": "                event[\"comment_change\"] = \"True\"",
                "114": "            else:",
                "115": "                event[\"comment_change\"] = \"False\"",
                "116": "",
                "117": "            # Add the event to the trace",
                "118": "            trace.append(event)",
                "119": "",
                "120": "        # Add the trace to the log",
                "121": "        log.append(trace)",
                "122": "",
                "123": "    return log",
                "124": "",
                "125": "def save_xes_log(log, filename):",
                "126": "    # Export the log to an XES file",
                "127": "    xes_exporter.apply(log, filename)",
                "128": "",
                "129": "if __name__ == \"__main__\":",
                "130": "    repo_url = \"https://github.com/numpy/numpy\"  # Example repository URL",
                "131": "    commits_data = analyze_commits(repo_url, \"#\", \"py\")",
                "132": "    save_to_json(commits_data, \"Data/commits_data.json\")",
                "133": "    print(\"Commit data has been saved to commits_data.json\")",
                "134": "    # Load the previously saved commit data JSON file",
                "135": "    with open(\"Data/commits_data.json\", \"r\") as json_file:",
                "136": "       commits_data = json.load(json_file)",
                "137": "",
                "138": "    # Create the XES log from the commit data",
                "139": "    xes_log = create_xes_log(commits_data)",
                "140": "",
                "141": "    # Save the XES log to a file",
                "142": "    save_xes_log(xes_log, \"Data/commits_data.xes\")",
                "143": "    print(\"XES log has been saved to commits_data.xes\")",
                "144": ""
            },
            "comments": [
                {
                    "line": 9,
                    "comment": "# This will hold the data for each file and its changes across commits",
                    "char_position_in_line": 4,
                    "type": [
                        "normal"
                    ]
                },
                {
                    "line": 12,
                    "comment": "# Analysis range",
                    "char_position_in_line": 4,
                    "type": [
                        "normal"
                    ]
                },
                {
                    "line": 16,
                    "comment": "# Traverse through the commits in the repository",
                    "char_position_in_line": 4,
                    "type": [
                        "block"
                    ]
                },
                {
                    "line": 17,
                    "comment": "# Only save commits, that contain at least one file of the format {language_file_extension}",
                    "char_position_in_line": 4,
                    "type": [
                        "block"
                    ]
                },
                {
                    "line": 22,
                    "comment": "# Analyze each file modified in the commit",
                    "char_position_in_line": 8,
                    "type": [
                        "normal"
                    ]
                },
                {
                    "line": 24,
                    "comment": "# only store file data for Rust files",
                    "char_position_in_line": 12,
                    "type": [
                        "normal"
                    ]
                },
                {
                    "line": 52,
                    "comment": "# Generate keywords based on the commit message and type of changes",
                    "char_position_in_line": 16,
                    "type": [
                        "block"
                    ]
                },
                {
                    "line": 53,
                    "comment": "# file_data[\"keywords\"] = extract_keywords(commit.msg, modified_file)",
                    "char_position_in_line": 16,
                    "type": [
                        "block"
                    ]
                },
                {
                    "line": 54,
                    "comment": "# Extract type of commit from commit message",
                    "char_position_in_line": 16,
                    "type": [
                        "block"
                    ]
                },
                {
                    "line": 55,
                    "comment": "# file_data[\"activity\"] = extract_activity(commit.msg)",
                    "char_position_in_line": 16,
                    "type": [
                        "block"
                    ]
                },
                {
                    "line": 61,
                    "comment": "# This function can use NLP techniques or simple keyword extraction",
                    "char_position_in_line": 4,
                    "type": [
                        "block"
                    ]
                },
                {
                    "line": 62,
                    "comment": "# Here, a simplified approach is used: basic keywords based on the commit message",
                    "char_position_in_line": 4,
                    "type": [
                        "block"
                    ]
                },
                {
                    "line": 75,
                    "comment": "# Use commit message keywords to determine activity type",
                    "char_position_in_line": 4,
                    "type": [
                        "normal"
                    ]
                },
                {
                    "line": 88,
                    "comment": "# Save the processed commit data to a JSON file",
                    "char_position_in_line": 4,
                    "type": [
                        "normal"
                    ]
                },
                {
                    "line": 93,
                    "comment": "# Create a new EventLog object",
                    "char_position_in_line": 4,
                    "type": [
                        "normal"
                    ]
                },
                {
                    "line": 96,
                    "comment": "# Iterate over each commit entry in the data",
                    "char_position_in_line": 4,
                    "type": [
                        "normal"
                    ]
                },
                {
                    "line": 98,
                    "comment": "# Create a trace for the file",
                    "char_position_in_line": 8,
                    "type": [
                        "normal"
                    ]
                },
                {
                    "line": 103,
                    "comment": "# Extract event attributes",
                    "char_position_in_line": 12,
                    "type": [
                        "normal"
                    ]
                },
                {
                    "line": 117,
                    "comment": "# Add the event to the trace",
                    "char_position_in_line": 12,
                    "type": [
                        "normal"
                    ]
                },
                {
                    "line": 120,
                    "comment": "# Add the trace to the log",
                    "char_position_in_line": 8,
                    "type": [
                        "normal"
                    ]
                },
                {
                    "line": 126,
                    "comment": "# Export the log to an XES file",
                    "char_position_in_line": 4,
                    "type": [
                        "normal"
                    ]
                },
                {
                    "line": 130,
                    "comment": "# Example repository URL",
                    "char_position_in_line": 49,
                    "type": [
                        "normal"
                    ]
                },
                {
                    "line": 134,
                    "comment": "# Load the previously saved commit data JSON file",
                    "char_position_in_line": 4,
                    "type": [
                        "normal"
                    ]
                },
                {
                    "line": 138,
                    "comment": "# Create the XES log from the commit data",
                    "char_position_in_line": 4,
                    "type": [
                        "normal"
                    ]
                },
                {
                    "line": 141,
                    "comment": "# Save the XES log to a file",
                    "char_position_in_line": 4,
                    "type": [
                        "normal"
                    ]
                }
            ]
        },
        {
            "commit": "1277d634d38557e2d5981481fd55f1753da2314b",
            "timestamp": "2024-11-17T16:33:10+01:00",
            "author": "alexander.schranner",
            "diff": {
                "added": {
                    "48": "                        if line[0] in diff_added.keys():",
                    "49": "                            diff_modified[line[0]] = line[1]",
                    "130": "    repo_url = \"https://github.com/espressif/arduino-esp32\"  # Example repository URL",
                    "131": "    commits_data = analyze_commits(repo_url, \"// \", \"cpp\")"
                },
                "deleted": {
                    "48": "                    if line[0] in diff_added.keys():",
                    "49": "                        diff_modified[line[0]] = line[1]",
                    "130": "    repo_url = \"https://github.com/numpy/numpy\"  # Example repository URL",
                    "131": "    commits_data = analyze_commits(repo_url, \"#\", \"py\")"
                }
            },
            "source_code": {
                "1": "from pydriller import Repository",
                "2": "import json",
                "3": "import pm4py",
                "4": "import datetime",
                "5": "from pm4py.objects.log.obj import EventLog, Trace, Event",
                "6": "from pm4py.objects.log.exporter.xes import exporter as xes_exporter",
                "7": "",
                "8": "def analyze_commits(repo_url, comment_symbol, language_file_extension):",
                "9": "    # This will hold the data for each file and its changes across commits",
                "10": "    files_data = {}",
                "11": "",
                "12": "    # Analysis range",
                "13": "    dt1 = datetime.datetime(2022, 10, 8, 17, 0, 0)",
                "14": "    dt2 = datetime.datetime(2023, 10, 8, 17, 59, 0)",
                "15": "",
                "16": "    # Traverse through the commits in the repository",
                "17": "    # Only save commits, that contain at least one file of the format {language_file_extension}",
                "18": "    for commit in Repository(repo_url, ",
                "19": "    only_modifications_with_file_types=[f\".{language_file_extension}\"],",
                "20": "    since=dt1,",
                "21": "    to=dt2).traverse_commits():",
                "22": "        # Analyze each file modified in the commit",
                "23": "        for modified_file in commit.modified_files:",
                "24": "            # only store file data for Rust files",
                "25": "            if modified_file.filename not in files_data:",
                "26": "                files_data[modified_file.filename] = []",
                "27": "            if len(modified_file.filename.split(\".\")) == 2 and modified_file.filename.split(\".\")[1] == language_file_extension:",
                "28": "                file_data = {",
                "29": "                    \"commit\": commit.hash,",
                "30": "                    \"timestamp\": commit.committer_date.isoformat(),",
                "31": "                    \"author\": commit.author.name,",
                "32": "                    \"commit_message\": commit.msg,",
                "33": "                    \"additions\": modified_file.added_lines,",
                "34": "                    \"deletions\": modified_file.deleted_lines,",
                "35": "                    \"change_type\": modified_file.change_type.name,",
                "36": "                    \"diff\": modified_file.diff",
                "37": "                }",
                "38": "                diff_added = {}",
                "39": "                diff_deleted = {}",
                "40": "                diff_modified = {}",
                "41": "                for line in modified_file.diff_parsed[\"added\"]:",
                "42": "                    if line[1].find(comment_symbol) != -1:",
                "43": "                        diff_added[line[0]] = line[1]",
                "44": "                file_data[\"comment_added_diff\"] = diff_added",
                "45": "                for line in modified_file.diff_parsed[\"deleted\"]:",
                "46": "                    if line[1].find(comment_symbol) != -1:",
                "47": "                        diff_deleted[line[0]] = line[1]",
                "48": "                        if line[0] in diff_added.keys():",
                "49": "                            diff_modified[line[0]] = line[1]",
                "50": "                file_data[\"comment_deleted_diff\"] = diff_deleted",
                "51": "                file_data[\"comment_modified_diff\"] = diff_modified",
                "52": "                # Generate keywords based on the commit message and type of changes",
                "53": "                # file_data[\"keywords\"] = extract_keywords(commit.msg, modified_file)",
                "54": "                # Extract type of commit from commit message",
                "55": "                # file_data[\"activity\"] = extract_activity(commit.msg)",
                "56": "                if len(diff_added) + len(diff_deleted) != 0:",
                "57": "                    files_data[modified_file.filename].append(file_data)",
                "58": "    return files_data",
                "59": "",
                "60": "def extract_keywords(commit_message, modified_file):",
                "61": "    # This function can use NLP techniques or simple keyword extraction",
                "62": "    # Here, a simplified approach is used: basic keywords based on the commit message",
                "63": "    keywords = []",
                "64": "    if \"performance\" in commit_message.lower():",
                "65": "        keywords.append(\"performance\")",
                "66": "    if \"security\" in commit_message.lower():",
                "67": "        keywords.append(\"security\")",
                "68": "    if modified_file.added_lines > modified_file.deleted_lines:",
                "69": "        keywords.append(\"expansion\")",
                "70": "    else:",
                "71": "        keywords.append(\"optimization\")",
                "72": "    return keywords",
                "73": "",
                "74": "def extract_activity(commit_message):",
                "75": "    # Use commit message keywords to determine activity type",
                "76": "    activity = \"\"",
                "77": "    if \"bug\" in commit.msg.lower() or \"fix\" in commit.msg.lower():",
                "78": "        activity = \"Bug Fix\"",
                "79": "    elif \"feature\" in commit.msg.lower() or \"add\" in commit.msg.lower():",
                "80": "        activity = \"Feature Development\"",
                "81": "    elif \"refactor\" in commit.msg.lower():",
                "82": "        activity = \"Refactoring\"",
                "83": "    else:",
                "84": "        activity = \"Other\"",
                "85": "    return activity",
                "86": "",
                "87": "def save_to_json(commits_data, filename):",
                "88": "    # Save the processed commit data to a JSON file",
                "89": "    with open(filename, 'w') as json_file:",
                "90": "        json.dump(commits_data, json_file, indent=4)",
                "91": "",
                "92": "def create_xes_log(data):",
                "93": "    # Create a new EventLog object",
                "94": "    log = EventLog()",
                "95": "",
                "96": "    # Iterate over each commit entry in the data",
                "97": "    for file, commits in data.items():",
                "98": "        # Create a trace for the file",
                "99": "        trace = Trace()",
                "100": "        trace.attributes[\"file\"] = file",
                "101": "",
                "102": "        for commit in commits:",
                "103": "            # Extract event attributes",
                "104": "            event = Event()",
                "105": "            event[\"timestamp\"] = commit.get(\"timestamp\")",
                "106": "            event[\"author\"] = commit.get(\"author\")",
                "107": "            event[\"change_type\"] = commit.get(\"change_type\")",
                "108": "            event[\"commit_message\"] = commit.get(\"commit_message\")",
                "109": "            event[\"additions\"] = commit.get(\"additions\")",
                "110": "            event[\"deletions\"] = commit.get(\"deletions\")",
                "111": "            event[\"diff\"] = commit.get(\"diff\")",
                "112": "            if commit.get(\"comment_added_diff\"):",
                "113": "                event[\"comment_change\"] = \"True\"",
                "114": "            else:",
                "115": "                event[\"comment_change\"] = \"False\"",
                "116": "",
                "117": "            # Add the event to the trace",
                "118": "            trace.append(event)",
                "119": "",
                "120": "        # Add the trace to the log",
                "121": "        log.append(trace)",
                "122": "",
                "123": "    return log",
                "124": "",
                "125": "def save_xes_log(log, filename):",
                "126": "    # Export the log to an XES file",
                "127": "    xes_exporter.apply(log, filename)",
                "128": "",
                "129": "if __name__ == \"__main__\":",
                "130": "    repo_url = \"https://github.com/espressif/arduino-esp32\"  # Example repository URL",
                "131": "    commits_data = analyze_commits(repo_url, \"// \", \"cpp\")",
                "132": "    save_to_json(commits_data, \"Data/commits_data.json\")",
                "133": "    print(\"Commit data has been saved to commits_data.json\")",
                "134": "    # Load the previously saved commit data JSON file",
                "135": "    with open(\"Data/commits_data.json\", \"r\") as json_file:",
                "136": "       commits_data = json.load(json_file)",
                "137": "",
                "138": "    # Create the XES log from the commit data",
                "139": "    xes_log = create_xes_log(commits_data)",
                "140": "",
                "141": "    # Save the XES log to a file",
                "142": "    save_xes_log(xes_log, \"Data/commits_data.xes\")",
                "143": "    print(\"XES log has been saved to commits_data.xes\")",
                "144": ""
            },
            "comments": [
                {
                    "line": 9,
                    "comment": "# This will hold the data for each file and its changes across commits",
                    "char_position_in_line": 4,
                    "type": [
                        "normal"
                    ]
                },
                {
                    "line": 12,
                    "comment": "# Analysis range",
                    "char_position_in_line": 4,
                    "type": [
                        "normal"
                    ]
                },
                {
                    "line": 16,
                    "comment": "# Traverse through the commits in the repository",
                    "char_position_in_line": 4,
                    "type": [
                        "block"
                    ]
                },
                {
                    "line": 17,
                    "comment": "# Only save commits, that contain at least one file of the format {language_file_extension}",
                    "char_position_in_line": 4,
                    "type": [
                        "block"
                    ]
                },
                {
                    "line": 22,
                    "comment": "# Analyze each file modified in the commit",
                    "char_position_in_line": 8,
                    "type": [
                        "normal"
                    ]
                },
                {
                    "line": 24,
                    "comment": "# only store file data for Rust files",
                    "char_position_in_line": 12,
                    "type": [
                        "normal"
                    ]
                },
                {
                    "line": 52,
                    "comment": "# Generate keywords based on the commit message and type of changes",
                    "char_position_in_line": 16,
                    "type": [
                        "block"
                    ]
                },
                {
                    "line": 53,
                    "comment": "# file_data[\"keywords\"] = extract_keywords(commit.msg, modified_file)",
                    "char_position_in_line": 16,
                    "type": [
                        "block"
                    ]
                },
                {
                    "line": 54,
                    "comment": "# Extract type of commit from commit message",
                    "char_position_in_line": 16,
                    "type": [
                        "block"
                    ]
                },
                {
                    "line": 55,
                    "comment": "# file_data[\"activity\"] = extract_activity(commit.msg)",
                    "char_position_in_line": 16,
                    "type": [
                        "block"
                    ]
                },
                {
                    "line": 61,
                    "comment": "# This function can use NLP techniques or simple keyword extraction",
                    "char_position_in_line": 4,
                    "type": [
                        "block"
                    ]
                },
                {
                    "line": 62,
                    "comment": "# Here, a simplified approach is used: basic keywords based on the commit message",
                    "char_position_in_line": 4,
                    "type": [
                        "block"
                    ]
                },
                {
                    "line": 75,
                    "comment": "# Use commit message keywords to determine activity type",
                    "char_position_in_line": 4,
                    "type": [
                        "normal"
                    ]
                },
                {
                    "line": 88,
                    "comment": "# Save the processed commit data to a JSON file",
                    "char_position_in_line": 4,
                    "type": [
                        "normal"
                    ]
                },
                {
                    "line": 93,
                    "comment": "# Create a new EventLog object",
                    "char_position_in_line": 4,
                    "type": [
                        "normal"
                    ]
                },
                {
                    "line": 96,
                    "comment": "# Iterate over each commit entry in the data",
                    "char_position_in_line": 4,
                    "type": [
                        "normal"
                    ]
                },
                {
                    "line": 98,
                    "comment": "# Create a trace for the file",
                    "char_position_in_line": 8,
                    "type": [
                        "normal"
                    ]
                },
                {
                    "line": 103,
                    "comment": "# Extract event attributes",
                    "char_position_in_line": 12,
                    "type": [
                        "normal"
                    ]
                },
                {
                    "line": 117,
                    "comment": "# Add the event to the trace",
                    "char_position_in_line": 12,
                    "type": [
                        "normal"
                    ]
                },
                {
                    "line": 120,
                    "comment": "# Add the trace to the log",
                    "char_position_in_line": 8,
                    "type": [
                        "normal"
                    ]
                },
                {
                    "line": 126,
                    "comment": "# Export the log to an XES file",
                    "char_position_in_line": 4,
                    "type": [
                        "normal"
                    ]
                },
                {
                    "line": 130,
                    "comment": "# Example repository URL",
                    "char_position_in_line": 61,
                    "type": [
                        "normal"
                    ]
                },
                {
                    "line": 134,
                    "comment": "# Load the previously saved commit data JSON file",
                    "char_position_in_line": 4,
                    "type": [
                        "normal"
                    ]
                },
                {
                    "line": 138,
                    "comment": "# Create the XES log from the commit data",
                    "char_position_in_line": 4,
                    "type": [
                        "normal"
                    ]
                },
                {
                    "line": 141,
                    "comment": "# Save the XES log to a file",
                    "char_position_in_line": 4,
                    "type": [
                        "normal"
                    ]
                }
            ]
        },
        {
            "commit": "a8af330e0eb1c2574ba424b69e0958c938502a67",
            "timestamp": "2024-11-18T10:51:21+01:00",
            "author": "alexander.schranner",
            "diff": {
                "added": {
                    "13": "    # dt1 = datetime.datetime(2022, 10, 8, 17, 0, 0)",
                    "14": "    dt2 = datetime.datetime(2020, 10, 8, 17, 59, 0)",
                    "20": "    # since=dt1,",
                    "36": "                    \"diff\": modified_file.diff_parsed",
                    "43": "                        print(int(line[0]))",
                    "88": "def analyze_diff(commits_data, type):",
                    "89": "    for file, commits in commits_data.items():",
                    "90": "        if len(file) > 0:",
                    "91": "            for commit in commits:",
                    "92": "                diff_edited = []",
                    "93": "                for i in range(len(commit[\"diff\"][type])):",
                    "94": "                    if commit[\"diff\"][type][i][1].find(\"// \") == -1:",
                    "95": "                        if len(diff_edited) > 0:",
                    "96": "                            # In case of no comment add lines to existing dict if line number directly follows",
                    "97": "                            if commit[\"diff\"][type][i][0] == diff_edited[-1][\"line_numbers\"][-1] + 1:",
                    "98": "                                diff_edited[-1][\"line_numbers\"].append(commit[\"diff\"][type][i][0])",
                    "99": "                                diff_edited[-1][\"lines\"].append(commit[\"diff\"][type][i][1])",
                    "100": "                            else:",
                    "101": "                                # or create new one",
                    "102": "                                diff_edited.append({",
                    "103": "                                    \"line_numbers\": [commit[\"diff\"][type][i][0]],",
                    "104": "                                    \"comments\": [],",
                    "105": "                                    \"lines\": [commit[\"diff\"][type][i][1]]})",
                    "106": "                    else:",
                    "107": "                        if len(diff_edited) > 0:",
                    "108": "                            # In case of comment add them to existing dict if they directly follow",
                    "109": "                            if commit[\"diff\"][type][i][0] == diff_edited[-1][\"line_numbers\"][-1] + 1:",
                    "110": "                                diff_edited[-1][\"line_numbers\"].append(commit[\"diff\"][type][i][0])",
                    "111": "                                diff_edited[-1][\"comments\"].append(commit[\"diff\"][type][i][1])",
                    "112": "                        else:",
                    "113": "                            # or create new one",
                    "114": "                            diff_edited.append({",
                    "115": "                                \"line_numbers\": [commit[\"diff\"][type][i][0]],",
                    "116": "                                \"comments\": [commit[\"diff\"][type][i][1]],",
                    "117": "                                \"lines\": []})",
                    "118": "                commit[\"diff\"][type] = diff_edited",
                    "119": "    return commits_data",
                    "120": "",
                    "121": "def save_to_json(commits_data, path):",
                    "123": "    with open(path, 'w') as json_file:",
                    "125": "    print(\"Commit data has been saved to \", path)",
                    "164": "def save_to_xes(log, path):",
                    "165": "    # Create the XES log from the commit data",
                    "166": "    xes_log = create_xes_log(log)",
                    "167": "",
                    "168": "    # Save the XES log to a file",
                    "169": "    save_xes_log(xes_log, path)",
                    "170": "    print(\"XES log has been saved to \", path)",
                    "171": "",
                    "176": "    # save_to_xes(commits_data, \"Data/commits_data.xes\")",
                    "179": "    analyzed_data = analyze_diff(commits_data, \"added\")",
                    "180": "    save_to_json(analyzed_data, \"Exports/analyzed_data.json\")",
                    "181": "    analyzed_data = analyze_diff(commits_data, \"deleted\")",
                    "182": "    save_to_json(analyzed_data, \"Exports/analyzed_data.json\")"
                },
                "deleted": {
                    "13": "    dt1 = datetime.datetime(2022, 10, 8, 17, 0, 0)",
                    "14": "    dt2 = datetime.datetime(2023, 10, 8, 17, 59, 0)",
                    "20": "    since=dt1,",
                    "36": "                    \"diff\": modified_file.diff",
                    "87": "def save_to_json(commits_data, filename):",
                    "89": "    with open(filename, 'w') as json_file:",
                    "133": "    print(\"Commit data has been saved to commits_data.json\")",
                    "134": "    # Load the previously saved commit data JSON file",
                    "138": "    # Create the XES log from the commit data",
                    "139": "    xes_log = create_xes_log(commits_data)",
                    "141": "    # Save the XES log to a file",
                    "142": "    save_xes_log(xes_log, \"Data/commits_data.xes\")",
                    "143": "    print(\"XES log has been saved to commits_data.xes\")"
                }
            },
            "source_code": {
                "1": "from pydriller import Repository",
                "2": "import json",
                "3": "import pm4py",
                "4": "import datetime",
                "5": "from pm4py.objects.log.obj import EventLog, Trace, Event",
                "6": "from pm4py.objects.log.exporter.xes import exporter as xes_exporter",
                "7": "",
                "8": "def analyze_commits(repo_url, comment_symbol, language_file_extension):",
                "9": "    # This will hold the data for each file and its changes across commits",
                "10": "    files_data = {}",
                "11": "",
                "12": "    # Analysis range",
                "13": "    # dt1 = datetime.datetime(2022, 10, 8, 17, 0, 0)",
                "14": "    dt2 = datetime.datetime(2020, 10, 8, 17, 59, 0)",
                "15": "",
                "16": "    # Traverse through the commits in the repository",
                "17": "    # Only save commits, that contain at least one file of the format {language_file_extension}",
                "18": "    for commit in Repository(repo_url, ",
                "19": "    only_modifications_with_file_types=[f\".{language_file_extension}\"],",
                "20": "    # since=dt1,",
                "21": "    to=dt2).traverse_commits():",
                "22": "        # Analyze each file modified in the commit",
                "23": "        for modified_file in commit.modified_files:",
                "24": "            # only store file data for Rust files",
                "25": "            if modified_file.filename not in files_data:",
                "26": "                files_data[modified_file.filename] = []",
                "27": "            if len(modified_file.filename.split(\".\")) == 2 and modified_file.filename.split(\".\")[1] == language_file_extension:",
                "28": "                file_data = {",
                "29": "                    \"commit\": commit.hash,",
                "30": "                    \"timestamp\": commit.committer_date.isoformat(),",
                "31": "                    \"author\": commit.author.name,",
                "32": "                    \"commit_message\": commit.msg,",
                "33": "                    \"additions\": modified_file.added_lines,",
                "34": "                    \"deletions\": modified_file.deleted_lines,",
                "35": "                    \"change_type\": modified_file.change_type.name,",
                "36": "                    \"diff\": modified_file.diff_parsed",
                "37": "                }",
                "38": "                diff_added = {}",
                "39": "                diff_deleted = {}",
                "40": "                diff_modified = {}",
                "41": "                for line in modified_file.diff_parsed[\"added\"]:",
                "42": "                    if line[1].find(comment_symbol) != -1:",
                "43": "                        print(int(line[0]))",
                "44": "                        diff_added[line[0]] = line[1]",
                "45": "                file_data[\"comment_added_diff\"] = diff_added",
                "46": "                for line in modified_file.diff_parsed[\"deleted\"]:",
                "47": "                    if line[1].find(comment_symbol) != -1:",
                "48": "                        diff_deleted[line[0]] = line[1]",
                "49": "                        if line[0] in diff_added.keys():",
                "50": "                            diff_modified[line[0]] = line[1]",
                "51": "                file_data[\"comment_deleted_diff\"] = diff_deleted",
                "52": "                file_data[\"comment_modified_diff\"] = diff_modified",
                "53": "                # Generate keywords based on the commit message and type of changes",
                "54": "                # file_data[\"keywords\"] = extract_keywords(commit.msg, modified_file)",
                "55": "                # Extract type of commit from commit message",
                "56": "                # file_data[\"activity\"] = extract_activity(commit.msg)",
                "57": "                if len(diff_added) + len(diff_deleted) != 0:",
                "58": "                    files_data[modified_file.filename].append(file_data)",
                "59": "    return files_data",
                "60": "",
                "61": "def extract_keywords(commit_message, modified_file):",
                "62": "    # This function can use NLP techniques or simple keyword extraction",
                "63": "    # Here, a simplified approach is used: basic keywords based on the commit message",
                "64": "    keywords = []",
                "65": "    if \"performance\" in commit_message.lower():",
                "66": "        keywords.append(\"performance\")",
                "67": "    if \"security\" in commit_message.lower():",
                "68": "        keywords.append(\"security\")",
                "69": "    if modified_file.added_lines > modified_file.deleted_lines:",
                "70": "        keywords.append(\"expansion\")",
                "71": "    else:",
                "72": "        keywords.append(\"optimization\")",
                "73": "    return keywords",
                "74": "",
                "75": "def extract_activity(commit_message):",
                "76": "    # Use commit message keywords to determine activity type",
                "77": "    activity = \"\"",
                "78": "    if \"bug\" in commit.msg.lower() or \"fix\" in commit.msg.lower():",
                "79": "        activity = \"Bug Fix\"",
                "80": "    elif \"feature\" in commit.msg.lower() or \"add\" in commit.msg.lower():",
                "81": "        activity = \"Feature Development\"",
                "82": "    elif \"refactor\" in commit.msg.lower():",
                "83": "        activity = \"Refactoring\"",
                "84": "    else:",
                "85": "        activity = \"Other\"",
                "86": "    return activity",
                "87": "",
                "88": "def analyze_diff(commits_data, type):",
                "89": "    for file, commits in commits_data.items():",
                "90": "        if len(file) > 0:",
                "91": "            for commit in commits:",
                "92": "                diff_edited = []",
                "93": "                for i in range(len(commit[\"diff\"][type])):",
                "94": "                    if commit[\"diff\"][type][i][1].find(\"// \") == -1:",
                "95": "                        if len(diff_edited) > 0:",
                "96": "                            # In case of no comment add lines to existing dict if line number directly follows",
                "97": "                            if commit[\"diff\"][type][i][0] == diff_edited[-1][\"line_numbers\"][-1] + 1:",
                "98": "                                diff_edited[-1][\"line_numbers\"].append(commit[\"diff\"][type][i][0])",
                "99": "                                diff_edited[-1][\"lines\"].append(commit[\"diff\"][type][i][1])",
                "100": "                            else:",
                "101": "                                # or create new one",
                "102": "                                diff_edited.append({",
                "103": "                                    \"line_numbers\": [commit[\"diff\"][type][i][0]],",
                "104": "                                    \"comments\": [],",
                "105": "                                    \"lines\": [commit[\"diff\"][type][i][1]]})",
                "106": "                    else:",
                "107": "                        if len(diff_edited) > 0:",
                "108": "                            # In case of comment add them to existing dict if they directly follow",
                "109": "                            if commit[\"diff\"][type][i][0] == diff_edited[-1][\"line_numbers\"][-1] + 1:",
                "110": "                                diff_edited[-1][\"line_numbers\"].append(commit[\"diff\"][type][i][0])",
                "111": "                                diff_edited[-1][\"comments\"].append(commit[\"diff\"][type][i][1])",
                "112": "                        else:",
                "113": "                            # or create new one",
                "114": "                            diff_edited.append({",
                "115": "                                \"line_numbers\": [commit[\"diff\"][type][i][0]],",
                "116": "                                \"comments\": [commit[\"diff\"][type][i][1]],",
                "117": "                                \"lines\": []})",
                "118": "                commit[\"diff\"][type] = diff_edited",
                "119": "    return commits_data",
                "120": "",
                "121": "def save_to_json(commits_data, path):",
                "122": "    # Save the processed commit data to a JSON file",
                "123": "    with open(path, 'w') as json_file:",
                "124": "        json.dump(commits_data, json_file, indent=4)",
                "125": "    print(\"Commit data has been saved to \", path)",
                "126": "",
                "127": "def create_xes_log(data):",
                "128": "    # Create a new EventLog object",
                "129": "    log = EventLog()",
                "130": "",
                "131": "    # Iterate over each commit entry in the data",
                "132": "    for file, commits in data.items():",
                "133": "        # Create a trace for the file",
                "134": "        trace = Trace()",
                "135": "        trace.attributes[\"file\"] = file",
                "136": "",
                "137": "        for commit in commits:",
                "138": "            # Extract event attributes",
                "139": "            event = Event()",
                "140": "            event[\"timestamp\"] = commit.get(\"timestamp\")",
                "141": "            event[\"author\"] = commit.get(\"author\")",
                "142": "            event[\"change_type\"] = commit.get(\"change_type\")",
                "143": "            event[\"commit_message\"] = commit.get(\"commit_message\")",
                "144": "            event[\"additions\"] = commit.get(\"additions\")",
                "145": "            event[\"deletions\"] = commit.get(\"deletions\")",
                "146": "            event[\"diff\"] = commit.get(\"diff\")",
                "147": "            if commit.get(\"comment_added_diff\"):",
                "148": "                event[\"comment_change\"] = \"True\"",
                "149": "            else:",
                "150": "                event[\"comment_change\"] = \"False\"",
                "151": "",
                "152": "            # Add the event to the trace",
                "153": "            trace.append(event)",
                "154": "",
                "155": "        # Add the trace to the log",
                "156": "        log.append(trace)",
                "157": "",
                "158": "    return log",
                "159": "",
                "160": "def save_xes_log(log, filename):",
                "161": "    # Export the log to an XES file",
                "162": "    xes_exporter.apply(log, filename)",
                "163": "",
                "164": "def save_to_xes(log, path):",
                "165": "    # Create the XES log from the commit data",
                "166": "    xes_log = create_xes_log(log)",
                "167": "",
                "168": "    # Save the XES log to a file",
                "169": "    save_xes_log(xes_log, path)",
                "170": "    print(\"XES log has been saved to \", path)",
                "171": "",
                "172": "if __name__ == \"__main__\":",
                "173": "    repo_url = \"https://github.com/espressif/arduino-esp32\"  # Example repository URL",
                "174": "    commits_data = analyze_commits(repo_url, \"// \", \"cpp\")",
                "175": "    save_to_json(commits_data, \"Data/commits_data.json\")",
                "176": "    # save_to_xes(commits_data, \"Data/commits_data.xes\")",
                "177": "    with open(\"Data/commits_data.json\", \"r\") as json_file:",
                "178": "       commits_data = json.load(json_file)",
                "179": "    analyzed_data = analyze_diff(commits_data, \"added\")",
                "180": "    save_to_json(analyzed_data, \"Exports/analyzed_data.json\")",
                "181": "    analyzed_data = analyze_diff(commits_data, \"deleted\")",
                "182": "    save_to_json(analyzed_data, \"Exports/analyzed_data.json\")",
                "183": "",
                "184": "",
                "185": ""
            },
            "comments": [
                {
                    "line": 9,
                    "comment": "# This will hold the data for each file and its changes across commits",
                    "char_position_in_line": 4,
                    "type": [
                        "normal"
                    ]
                },
                {
                    "line": 12,
                    "comment": "# Analysis range",
                    "char_position_in_line": 4,
                    "type": [
                        "block"
                    ]
                },
                {
                    "line": 13,
                    "comment": "# dt1 = datetime.datetime(2022, 10, 8, 17, 0, 0)",
                    "char_position_in_line": 4,
                    "type": [
                        "block"
                    ]
                },
                {
                    "line": 16,
                    "comment": "# Traverse through the commits in the repository",
                    "char_position_in_line": 4,
                    "type": [
                        "block"
                    ]
                },
                {
                    "line": 17,
                    "comment": "# Only save commits, that contain at least one file of the format {language_file_extension}",
                    "char_position_in_line": 4,
                    "type": [
                        "block"
                    ]
                },
                {
                    "line": 20,
                    "comment": "# since=dt1,",
                    "char_position_in_line": 4,
                    "type": [
                        "commented-out"
                    ]
                },
                {
                    "line": 22,
                    "comment": "# Analyze each file modified in the commit",
                    "char_position_in_line": 8,
                    "type": [
                        "normal"
                    ]
                },
                {
                    "line": 24,
                    "comment": "# only store file data for Rust files",
                    "char_position_in_line": 12,
                    "type": [
                        "normal"
                    ]
                },
                {
                    "line": 53,
                    "comment": "# Generate keywords based on the commit message and type of changes",
                    "char_position_in_line": 16,
                    "type": [
                        "block"
                    ]
                },
                {
                    "line": 54,
                    "comment": "# file_data[\"keywords\"] = extract_keywords(commit.msg, modified_file)",
                    "char_position_in_line": 16,
                    "type": [
                        "block"
                    ]
                },
                {
                    "line": 55,
                    "comment": "# Extract type of commit from commit message",
                    "char_position_in_line": 16,
                    "type": [
                        "block"
                    ]
                },
                {
                    "line": 56,
                    "comment": "# file_data[\"activity\"] = extract_activity(commit.msg)",
                    "char_position_in_line": 16,
                    "type": [
                        "block"
                    ]
                },
                {
                    "line": 62,
                    "comment": "# This function can use NLP techniques or simple keyword extraction",
                    "char_position_in_line": 4,
                    "type": [
                        "block"
                    ]
                },
                {
                    "line": 63,
                    "comment": "# Here, a simplified approach is used: basic keywords based on the commit message",
                    "char_position_in_line": 4,
                    "type": [
                        "block"
                    ]
                },
                {
                    "line": 76,
                    "comment": "# Use commit message keywords to determine activity type",
                    "char_position_in_line": 4,
                    "type": [
                        "normal"
                    ]
                },
                {
                    "line": 96,
                    "comment": "# In case of no comment add lines to existing dict if line number directly follows",
                    "char_position_in_line": 28,
                    "type": [
                        "normal"
                    ]
                },
                {
                    "line": 101,
                    "comment": "# or create new one",
                    "char_position_in_line": 32,
                    "type": [
                        "normal"
                    ]
                },
                {
                    "line": 108,
                    "comment": "# In case of comment add them to existing dict if they directly follow",
                    "char_position_in_line": 28,
                    "type": [
                        "normal"
                    ]
                },
                {
                    "line": 113,
                    "comment": "# or create new one",
                    "char_position_in_line": 28,
                    "type": [
                        "normal"
                    ]
                },
                {
                    "line": 122,
                    "comment": "# Save the processed commit data to a JSON file",
                    "char_position_in_line": 4,
                    "type": [
                        "normal"
                    ]
                },
                {
                    "line": 128,
                    "comment": "# Create a new EventLog object",
                    "char_position_in_line": 4,
                    "type": [
                        "normal"
                    ]
                },
                {
                    "line": 131,
                    "comment": "# Iterate over each commit entry in the data",
                    "char_position_in_line": 4,
                    "type": [
                        "normal"
                    ]
                },
                {
                    "line": 133,
                    "comment": "# Create a trace for the file",
                    "char_position_in_line": 8,
                    "type": [
                        "normal"
                    ]
                },
                {
                    "line": 138,
                    "comment": "# Extract event attributes",
                    "char_position_in_line": 12,
                    "type": [
                        "normal"
                    ]
                },
                {
                    "line": 152,
                    "comment": "# Add the event to the trace",
                    "char_position_in_line": 12,
                    "type": [
                        "normal"
                    ]
                },
                {
                    "line": 155,
                    "comment": "# Add the trace to the log",
                    "char_position_in_line": 8,
                    "type": [
                        "normal"
                    ]
                },
                {
                    "line": 161,
                    "comment": "# Export the log to an XES file",
                    "char_position_in_line": 4,
                    "type": [
                        "normal"
                    ]
                },
                {
                    "line": 165,
                    "comment": "# Create the XES log from the commit data",
                    "char_position_in_line": 4,
                    "type": [
                        "normal"
                    ]
                },
                {
                    "line": 168,
                    "comment": "# Save the XES log to a file",
                    "char_position_in_line": 4,
                    "type": [
                        "normal"
                    ]
                },
                {
                    "line": 173,
                    "comment": "# Example repository URL",
                    "char_position_in_line": 61,
                    "type": [
                        "normal"
                    ]
                },
                {
                    "line": 176,
                    "comment": "# save_to_xes(commits_data, \"Data/commits_data.xes\")",
                    "char_position_in_line": 4,
                    "type": [
                        "commented-out"
                    ]
                }
            ]
        },
        {
            "commit": "3bffee0480ea6e1007c2b96cafa58305afd4a9f2",
            "timestamp": "2024-11-18T15:19:19+01:00",
            "author": "alexander.schranner",
            "diff": {
                "added": {
                    "4": "from datetime import datetime",
                    "13": "    # dt1 = datetime(2022, 10, 8, 17, 0, 0)",
                    "14": "    dt2 = datetime(2022, 10, 8, 17, 59, 0)",
                    "87": "def pretty_diff(commits_data, type):",
                    "120": "def analyze_diffs(data):",
                    "121": "    analysis_results = []",
                    "122": "",
                    "123": "    for file, commits in data.items():",
                    "124": "        # Store last modified timestamps for each line",
                    "125": "        last_modified = {}",
                    "126": "",
                    "127": "        for commit in commits:",
                    "128": "            print(\"Starting to analyse commit: \", commit[\"commit\"])",
                    "129": "            commit_time = datetime.fromisoformat(commit[\"timestamp\"])",
                    "130": "",
                    "131": "            # Track modified lines",
                    "132": "            for block in commit[\"diff\"][\"added\"]:",
                    "133": "                for line in block[\"line_numbers\"]:",
                    "134": "                    line_number = line",
                    "135": "                    last_modified[line_number] = commit_time",
                    "136": "",
                    "137": "            print(last_modified)",
                    "138": "",
                    "139": "            # Compare with comments",
                    "140": "            for line in commit[\"comment_added_diff\"]:",
                    "141": "                comment_time = datetime.fromisoformat(commit[\"timestamp\"])",
                    "142": "                last_modified_lines = list(last_modified.keys())",
                    "143": "                # print(\"Check if \", line, \" is in keys \", last_modified_lines)",
                    "144": "                if int(line) in last_modified_lines:",
                    "145": "                    # print(\"Check if \", comment_time, \" is larger than \", last_modified[int(line)])",
                    "146": "                    if(comment_time > last_modified[int(line)]):",
                    "147": "                        analysis_results.append({",
                    "148": "                            \"file\": file,",
                    "149": "                            \"line\": int(line),",
                    "150": "                            \"comment\": commit[\"comment_added_diff\"][line],",
                    "151": "                            \"comment_time\": str(comment_time),",
                    "152": "                            \"last_code_change_time\": str(last_modified[int(line)])",
                    "153": "                        })",
                    "154": "            # print(\"Finsihed with commit\")",
                    "155": "            # print(\"Current state analysis results: \", analysis_results, \"\\n\")",
                    "156": "",
                    "157": "    return analysis_results",
                    "158": "",
                    "163": "    print(\"Data has been saved to\", path)",
                    "208": "    print(\"XES log has been saved to\", path)",
                    "212": "    # commits_data = analyze_commits(repo_url, \"// \", \"cpp\")",
                    "213": "    # save_to_json(commits_data, \"Data/commits_data.json\")",
                    "215": "    # with open(\"Data/commits_data.json\", \"r\") as json_file:",
                    "216": "      #  commits_data = json.load(json_file)",
                    "217": "    # analyzed_data = pretty_diff(commits_data, \"added\")",
                    "218": "    # save_to_json(analyzed_data, \"Exports/analyzed_data.json\")",
                    "219": "    # analyzed_data = pretty_diff(commits_data, \"deleted\")",
                    "220": "    # save_to_json(analyzed_data, \"Exports/analyzed_data.json\")",
                    "221": "",
                    "222": "    # Test case",
                    "223": "    with open(\"Exports/analyzed_data.json\", \"r\") as json_file:",
                    "224": "        data = json.load(json_file)",
                    "225": "    print(\"\\n\")",
                    "226": "    analyzed_data = analyze_diffs(data)",
                    "227": "    for result in analyzed_data:",
                    "228": "        print(f\"In {result['file']}, line {result['line']} was commented on {result['comment_time']} \"",
                    "229": "            f\"after being changed on {result['last_code_change_time']}.\")",
                    "230": "    save_to_json(analyzed_data, \"Exports/analysis_results.json\")"
                },
                "deleted": {
                    "4": "import datetime",
                    "13": "    # dt1 = datetime.datetime(2022, 10, 8, 17, 0, 0)",
                    "14": "    dt2 = datetime.datetime(2020, 10, 8, 17, 59, 0)",
                    "43": "                        print(int(line[0]))",
                    "88": "def analyze_diff(commits_data, type):",
                    "125": "    print(\"Commit data has been saved to \", path)",
                    "170": "    print(\"XES log has been saved to \", path)",
                    "174": "    commits_data = analyze_commits(repo_url, \"// \", \"cpp\")",
                    "175": "    save_to_json(commits_data, \"Data/commits_data.json\")",
                    "177": "    with open(\"Data/commits_data.json\", \"r\") as json_file:",
                    "178": "       commits_data = json.load(json_file)",
                    "179": "    analyzed_data = analyze_diff(commits_data, \"added\")",
                    "180": "    save_to_json(analyzed_data, \"Exports/analyzed_data.json\")",
                    "181": "    analyzed_data = analyze_diff(commits_data, \"deleted\")",
                    "182": "    save_to_json(analyzed_data, \"Exports/analyzed_data.json\")",
                    "183": "",
                    "184": ""
                }
            },
            "source_code": {
                "1": "from pydriller import Repository",
                "2": "import json",
                "3": "import pm4py",
                "4": "from datetime import datetime",
                "5": "from pm4py.objects.log.obj import EventLog, Trace, Event",
                "6": "from pm4py.objects.log.exporter.xes import exporter as xes_exporter",
                "7": "",
                "8": "def analyze_commits(repo_url, comment_symbol, language_file_extension):",
                "9": "    # This will hold the data for each file and its changes across commits",
                "10": "    files_data = {}",
                "11": "",
                "12": "    # Analysis range",
                "13": "    # dt1 = datetime(2022, 10, 8, 17, 0, 0)",
                "14": "    dt2 = datetime(2022, 10, 8, 17, 59, 0)",
                "15": "",
                "16": "    # Traverse through the commits in the repository",
                "17": "    # Only save commits, that contain at least one file of the format {language_file_extension}",
                "18": "    for commit in Repository(repo_url, ",
                "19": "    only_modifications_with_file_types=[f\".{language_file_extension}\"],",
                "20": "    # since=dt1,",
                "21": "    to=dt2).traverse_commits():",
                "22": "        # Analyze each file modified in the commit",
                "23": "        for modified_file in commit.modified_files:",
                "24": "            # only store file data for Rust files",
                "25": "            if modified_file.filename not in files_data:",
                "26": "                files_data[modified_file.filename] = []",
                "27": "            if len(modified_file.filename.split(\".\")) == 2 and modified_file.filename.split(\".\")[1] == language_file_extension:",
                "28": "                file_data = {",
                "29": "                    \"commit\": commit.hash,",
                "30": "                    \"timestamp\": commit.committer_date.isoformat(),",
                "31": "                    \"author\": commit.author.name,",
                "32": "                    \"commit_message\": commit.msg,",
                "33": "                    \"additions\": modified_file.added_lines,",
                "34": "                    \"deletions\": modified_file.deleted_lines,",
                "35": "                    \"change_type\": modified_file.change_type.name,",
                "36": "                    \"diff\": modified_file.diff_parsed",
                "37": "                }",
                "38": "                diff_added = {}",
                "39": "                diff_deleted = {}",
                "40": "                diff_modified = {}",
                "41": "                for line in modified_file.diff_parsed[\"added\"]:",
                "42": "                    if line[1].find(comment_symbol) != -1:",
                "43": "                        diff_added[line[0]] = line[1]",
                "44": "                file_data[\"comment_added_diff\"] = diff_added",
                "45": "                for line in modified_file.diff_parsed[\"deleted\"]:",
                "46": "                    if line[1].find(comment_symbol) != -1:",
                "47": "                        diff_deleted[line[0]] = line[1]",
                "48": "                        if line[0] in diff_added.keys():",
                "49": "                            diff_modified[line[0]] = line[1]",
                "50": "                file_data[\"comment_deleted_diff\"] = diff_deleted",
                "51": "                file_data[\"comment_modified_diff\"] = diff_modified",
                "52": "                # Generate keywords based on the commit message and type of changes",
                "53": "                # file_data[\"keywords\"] = extract_keywords(commit.msg, modified_file)",
                "54": "                # Extract type of commit from commit message",
                "55": "                # file_data[\"activity\"] = extract_activity(commit.msg)",
                "56": "                if len(diff_added) + len(diff_deleted) != 0:",
                "57": "                    files_data[modified_file.filename].append(file_data)",
                "58": "    return files_data",
                "59": "",
                "60": "def extract_keywords(commit_message, modified_file):",
                "61": "    # This function can use NLP techniques or simple keyword extraction",
                "62": "    # Here, a simplified approach is used: basic keywords based on the commit message",
                "63": "    keywords = []",
                "64": "    if \"performance\" in commit_message.lower():",
                "65": "        keywords.append(\"performance\")",
                "66": "    if \"security\" in commit_message.lower():",
                "67": "        keywords.append(\"security\")",
                "68": "    if modified_file.added_lines > modified_file.deleted_lines:",
                "69": "        keywords.append(\"expansion\")",
                "70": "    else:",
                "71": "        keywords.append(\"optimization\")",
                "72": "    return keywords",
                "73": "",
                "74": "def extract_activity(commit_message):",
                "75": "    # Use commit message keywords to determine activity type",
                "76": "    activity = \"\"",
                "77": "    if \"bug\" in commit.msg.lower() or \"fix\" in commit.msg.lower():",
                "78": "        activity = \"Bug Fix\"",
                "79": "    elif \"feature\" in commit.msg.lower() or \"add\" in commit.msg.lower():",
                "80": "        activity = \"Feature Development\"",
                "81": "    elif \"refactor\" in commit.msg.lower():",
                "82": "        activity = \"Refactoring\"",
                "83": "    else:",
                "84": "        activity = \"Other\"",
                "85": "    return activity",
                "86": "",
                "87": "def pretty_diff(commits_data, type):",
                "88": "    for file, commits in commits_data.items():",
                "89": "        if len(file) > 0:",
                "90": "            for commit in commits:",
                "91": "                diff_edited = []",
                "92": "                for i in range(len(commit[\"diff\"][type])):",
                "93": "                    if commit[\"diff\"][type][i][1].find(\"// \") == -1:",
                "94": "                        if len(diff_edited) > 0:",
                "95": "                            # In case of no comment add lines to existing dict if line number directly follows",
                "96": "                            if commit[\"diff\"][type][i][0] == diff_edited[-1][\"line_numbers\"][-1] + 1:",
                "97": "                                diff_edited[-1][\"line_numbers\"].append(commit[\"diff\"][type][i][0])",
                "98": "                                diff_edited[-1][\"lines\"].append(commit[\"diff\"][type][i][1])",
                "99": "                            else:",
                "100": "                                # or create new one",
                "101": "                                diff_edited.append({",
                "102": "                                    \"line_numbers\": [commit[\"diff\"][type][i][0]],",
                "103": "                                    \"comments\": [],",
                "104": "                                    \"lines\": [commit[\"diff\"][type][i][1]]})",
                "105": "                    else:",
                "106": "                        if len(diff_edited) > 0:",
                "107": "                            # In case of comment add them to existing dict if they directly follow",
                "108": "                            if commit[\"diff\"][type][i][0] == diff_edited[-1][\"line_numbers\"][-1] + 1:",
                "109": "                                diff_edited[-1][\"line_numbers\"].append(commit[\"diff\"][type][i][0])",
                "110": "                                diff_edited[-1][\"comments\"].append(commit[\"diff\"][type][i][1])",
                "111": "                        else:",
                "112": "                            # or create new one",
                "113": "                            diff_edited.append({",
                "114": "                                \"line_numbers\": [commit[\"diff\"][type][i][0]],",
                "115": "                                \"comments\": [commit[\"diff\"][type][i][1]],",
                "116": "                                \"lines\": []})",
                "117": "                commit[\"diff\"][type] = diff_edited",
                "118": "    return commits_data",
                "119": "",
                "120": "def analyze_diffs(data):",
                "121": "    analysis_results = []",
                "122": "",
                "123": "    for file, commits in data.items():",
                "124": "        # Store last modified timestamps for each line",
                "125": "        last_modified = {}",
                "126": "",
                "127": "        for commit in commits:",
                "128": "            print(\"Starting to analyse commit: \", commit[\"commit\"])",
                "129": "            commit_time = datetime.fromisoformat(commit[\"timestamp\"])",
                "130": "            ",
                "131": "            # Track modified lines",
                "132": "            for block in commit[\"diff\"][\"added\"]:",
                "133": "                for line in block[\"line_numbers\"]:",
                "134": "                    line_number = line",
                "135": "                    last_modified[line_number] = commit_time",
                "136": "",
                "137": "            print(last_modified)",
                "138": "",
                "139": "            # Compare with comments",
                "140": "            for line in commit[\"comment_added_diff\"]:",
                "141": "                comment_time = datetime.fromisoformat(commit[\"timestamp\"])",
                "142": "                last_modified_lines = list(last_modified.keys())",
                "143": "                # print(\"Check if \", line, \" is in keys \", last_modified_lines)",
                "144": "                if int(line) in last_modified_lines:",
                "145": "                    # print(\"Check if \", comment_time, \" is larger than \", last_modified[int(line)])",
                "146": "                    if(comment_time > last_modified[int(line)]):",
                "147": "                        analysis_results.append({",
                "148": "                            \"file\": file,",
                "149": "                            \"line\": int(line),",
                "150": "                            \"comment\": commit[\"comment_added_diff\"][line],",
                "151": "                            \"comment_time\": str(comment_time),",
                "152": "                            \"last_code_change_time\": str(last_modified[int(line)])",
                "153": "                        })",
                "154": "            # print(\"Finsihed with commit\")",
                "155": "            # print(\"Current state analysis results: \", analysis_results, \"\\n\")",
                "156": "    ",
                "157": "    return analysis_results",
                "158": "",
                "159": "def save_to_json(commits_data, path):",
                "160": "    # Save the processed commit data to a JSON file",
                "161": "    with open(path, 'w') as json_file:",
                "162": "        json.dump(commits_data, json_file, indent=4)",
                "163": "    print(\"Data has been saved to\", path)",
                "164": "",
                "165": "def create_xes_log(data):",
                "166": "    # Create a new EventLog object",
                "167": "    log = EventLog()",
                "168": "",
                "169": "    # Iterate over each commit entry in the data",
                "170": "    for file, commits in data.items():",
                "171": "        # Create a trace for the file",
                "172": "        trace = Trace()",
                "173": "        trace.attributes[\"file\"] = file",
                "174": "",
                "175": "        for commit in commits:",
                "176": "            # Extract event attributes",
                "177": "            event = Event()",
                "178": "            event[\"timestamp\"] = commit.get(\"timestamp\")",
                "179": "            event[\"author\"] = commit.get(\"author\")",
                "180": "            event[\"change_type\"] = commit.get(\"change_type\")",
                "181": "            event[\"commit_message\"] = commit.get(\"commit_message\")",
                "182": "            event[\"additions\"] = commit.get(\"additions\")",
                "183": "            event[\"deletions\"] = commit.get(\"deletions\")",
                "184": "            event[\"diff\"] = commit.get(\"diff\")",
                "185": "            if commit.get(\"comment_added_diff\"):",
                "186": "                event[\"comment_change\"] = \"True\"",
                "187": "            else:",
                "188": "                event[\"comment_change\"] = \"False\"",
                "189": "",
                "190": "            # Add the event to the trace",
                "191": "            trace.append(event)",
                "192": "",
                "193": "        # Add the trace to the log",
                "194": "        log.append(trace)",
                "195": "",
                "196": "    return log",
                "197": "",
                "198": "def save_xes_log(log, filename):",
                "199": "    # Export the log to an XES file",
                "200": "    xes_exporter.apply(log, filename)",
                "201": "",
                "202": "def save_to_xes(log, path):",
                "203": "    # Create the XES log from the commit data",
                "204": "    xes_log = create_xes_log(log)",
                "205": "",
                "206": "    # Save the XES log to a file",
                "207": "    save_xes_log(xes_log, path)",
                "208": "    print(\"XES log has been saved to\", path)",
                "209": "",
                "210": "if __name__ == \"__main__\":",
                "211": "    repo_url = \"https://github.com/espressif/arduino-esp32\"  # Example repository URL",
                "212": "    # commits_data = analyze_commits(repo_url, \"// \", \"cpp\")",
                "213": "    # save_to_json(commits_data, \"Data/commits_data.json\")",
                "214": "    # save_to_xes(commits_data, \"Data/commits_data.xes\")",
                "215": "    # with open(\"Data/commits_data.json\", \"r\") as json_file:",
                "216": "      #  commits_data = json.load(json_file)",
                "217": "    # analyzed_data = pretty_diff(commits_data, \"added\")",
                "218": "    # save_to_json(analyzed_data, \"Exports/analyzed_data.json\")",
                "219": "    # analyzed_data = pretty_diff(commits_data, \"deleted\")",
                "220": "    # save_to_json(analyzed_data, \"Exports/analyzed_data.json\")",
                "221": "    ",
                "222": "    # Test case",
                "223": "    with open(\"Exports/analyzed_data.json\", \"r\") as json_file:",
                "224": "        data = json.load(json_file)",
                "225": "    print(\"\\n\")",
                "226": "    analyzed_data = analyze_diffs(data) ",
                "227": "    for result in analyzed_data:",
                "228": "        print(f\"In {result['file']}, line {result['line']} was commented on {result['comment_time']} \"",
                "229": "            f\"after being changed on {result['last_code_change_time']}.\")",
                "230": "    save_to_json(analyzed_data, \"Exports/analysis_results.json\")",
                "231": ""
            },
            "comments": [
                {
                    "line": 9,
                    "comment": "# This will hold the data for each file and its changes across commits",
                    "char_position_in_line": 4,
                    "type": [
                        "normal"
                    ]
                },
                {
                    "line": 12,
                    "comment": "# Analysis range",
                    "char_position_in_line": 4,
                    "type": [
                        "block"
                    ]
                },
                {
                    "line": 13,
                    "comment": "# dt1 = datetime(2022, 10, 8, 17, 0, 0)",
                    "char_position_in_line": 4,
                    "type": [
                        "block"
                    ]
                },
                {
                    "line": 16,
                    "comment": "# Traverse through the commits in the repository",
                    "char_position_in_line": 4,
                    "type": [
                        "block"
                    ]
                },
                {
                    "line": 17,
                    "comment": "# Only save commits, that contain at least one file of the format {language_file_extension}",
                    "char_position_in_line": 4,
                    "type": [
                        "block"
                    ]
                },
                {
                    "line": 20,
                    "comment": "# since=dt1,",
                    "char_position_in_line": 4,
                    "type": [
                        "commented-out"
                    ]
                },
                {
                    "line": 22,
                    "comment": "# Analyze each file modified in the commit",
                    "char_position_in_line": 8,
                    "type": [
                        "normal"
                    ]
                },
                {
                    "line": 24,
                    "comment": "# only store file data for Rust files",
                    "char_position_in_line": 12,
                    "type": [
                        "normal"
                    ]
                },
                {
                    "line": 52,
                    "comment": "# Generate keywords based on the commit message and type of changes",
                    "char_position_in_line": 16,
                    "type": [
                        "block"
                    ]
                },
                {
                    "line": 53,
                    "comment": "# file_data[\"keywords\"] = extract_keywords(commit.msg, modified_file)",
                    "char_position_in_line": 16,
                    "type": [
                        "block"
                    ]
                },
                {
                    "line": 54,
                    "comment": "# Extract type of commit from commit message",
                    "char_position_in_line": 16,
                    "type": [
                        "block"
                    ]
                },
                {
                    "line": 55,
                    "comment": "# file_data[\"activity\"] = extract_activity(commit.msg)",
                    "char_position_in_line": 16,
                    "type": [
                        "block"
                    ]
                },
                {
                    "line": 61,
                    "comment": "# This function can use NLP techniques or simple keyword extraction",
                    "char_position_in_line": 4,
                    "type": [
                        "block"
                    ]
                },
                {
                    "line": 62,
                    "comment": "# Here, a simplified approach is used: basic keywords based on the commit message",
                    "char_position_in_line": 4,
                    "type": [
                        "block"
                    ]
                },
                {
                    "line": 75,
                    "comment": "# Use commit message keywords to determine activity type",
                    "char_position_in_line": 4,
                    "type": [
                        "normal"
                    ]
                },
                {
                    "line": 95,
                    "comment": "# In case of no comment add lines to existing dict if line number directly follows",
                    "char_position_in_line": 28,
                    "type": [
                        "normal"
                    ]
                },
                {
                    "line": 100,
                    "comment": "# or create new one",
                    "char_position_in_line": 32,
                    "type": [
                        "normal"
                    ]
                },
                {
                    "line": 107,
                    "comment": "# In case of comment add them to existing dict if they directly follow",
                    "char_position_in_line": 28,
                    "type": [
                        "normal"
                    ]
                },
                {
                    "line": 112,
                    "comment": "# or create new one",
                    "char_position_in_line": 28,
                    "type": [
                        "normal"
                    ]
                },
                {
                    "line": 124,
                    "comment": "# Store last modified timestamps for each line",
                    "char_position_in_line": 8,
                    "type": [
                        "normal"
                    ]
                },
                {
                    "line": 131,
                    "comment": "# Track modified lines",
                    "char_position_in_line": 12,
                    "type": [
                        "normal"
                    ]
                },
                {
                    "line": 139,
                    "comment": "# Compare with comments",
                    "char_position_in_line": 12,
                    "type": [
                        "normal"
                    ]
                },
                {
                    "line": 143,
                    "comment": "# print(\"Check if \", line, \" is in keys \", last_modified_lines)",
                    "char_position_in_line": 16,
                    "type": [
                        "commented-out"
                    ]
                },
                {
                    "line": 145,
                    "comment": "# print(\"Check if \", comment_time, \" is larger than \", last_modified[int(line)])",
                    "char_position_in_line": 20,
                    "type": [
                        "commented-out"
                    ]
                },
                {
                    "line": 154,
                    "comment": "# print(\"Finsihed with commit\")",
                    "char_position_in_line": 12,
                    "type": [
                        "commented-out",
                        "block"
                    ]
                },
                {
                    "line": 155,
                    "comment": "# print(\"Current state analysis results: \", analysis_results, \"\\n\")",
                    "char_position_in_line": 12,
                    "type": [
                        "commented-out",
                        "block"
                    ]
                },
                {
                    "line": 160,
                    "comment": "# Save the processed commit data to a JSON file",
                    "char_position_in_line": 4,
                    "type": [
                        "normal"
                    ]
                },
                {
                    "line": 166,
                    "comment": "# Create a new EventLog object",
                    "char_position_in_line": 4,
                    "type": [
                        "normal"
                    ]
                },
                {
                    "line": 169,
                    "comment": "# Iterate over each commit entry in the data",
                    "char_position_in_line": 4,
                    "type": [
                        "normal"
                    ]
                },
                {
                    "line": 171,
                    "comment": "# Create a trace for the file",
                    "char_position_in_line": 8,
                    "type": [
                        "normal"
                    ]
                },
                {
                    "line": 176,
                    "comment": "# Extract event attributes",
                    "char_position_in_line": 12,
                    "type": [
                        "normal"
                    ]
                },
                {
                    "line": 190,
                    "comment": "# Add the event to the trace",
                    "char_position_in_line": 12,
                    "type": [
                        "normal"
                    ]
                },
                {
                    "line": 193,
                    "comment": "# Add the trace to the log",
                    "char_position_in_line": 8,
                    "type": [
                        "normal"
                    ]
                },
                {
                    "line": 199,
                    "comment": "# Export the log to an XES file",
                    "char_position_in_line": 4,
                    "type": [
                        "normal"
                    ]
                },
                {
                    "line": 203,
                    "comment": "# Create the XES log from the commit data",
                    "char_position_in_line": 4,
                    "type": [
                        "normal"
                    ]
                },
                {
                    "line": 206,
                    "comment": "# Save the XES log to a file",
                    "char_position_in_line": 4,
                    "type": [
                        "normal"
                    ]
                },
                {
                    "line": 211,
                    "comment": "# Example repository URL",
                    "char_position_in_line": 61,
                    "type": [
                        "normal"
                    ]
                },
                {
                    "line": 212,
                    "comment": "# commits_data = analyze_commits(repo_url, \"// \", \"cpp\")",
                    "char_position_in_line": 4,
                    "type": [
                        "commented-out",
                        "block"
                    ]
                },
                {
                    "line": 213,
                    "comment": "# save_to_json(commits_data, \"Data/commits_data.json\")",
                    "char_position_in_line": 4,
                    "type": [
                        "commented-out",
                        "block"
                    ]
                },
                {
                    "line": 214,
                    "comment": "# save_to_xes(commits_data, \"Data/commits_data.xes\")",
                    "char_position_in_line": 4,
                    "type": [
                        "commented-out",
                        "block"
                    ]
                },
                {
                    "line": 215,
                    "comment": "# with open(\"Data/commits_data.json\", \"r\") as json_file:",
                    "char_position_in_line": 4,
                    "type": [
                        "commented-out",
                        "block"
                    ]
                },
                {
                    "line": 216,
                    "comment": "#  commits_data = json.load(json_file)",
                    "char_position_in_line": 6,
                    "type": [
                        "commented-out"
                    ]
                },
                {
                    "line": 217,
                    "comment": "# analyzed_data = pretty_diff(commits_data, \"added\")",
                    "char_position_in_line": 4,
                    "type": [
                        "commented-out",
                        "block"
                    ]
                },
                {
                    "line": 218,
                    "comment": "# save_to_json(analyzed_data, \"Exports/analyzed_data.json\")",
                    "char_position_in_line": 4,
                    "type": [
                        "commented-out",
                        "block"
                    ]
                },
                {
                    "line": 219,
                    "comment": "# analyzed_data = pretty_diff(commits_data, \"deleted\")",
                    "char_position_in_line": 4,
                    "type": [
                        "commented-out",
                        "block"
                    ]
                },
                {
                    "line": 220,
                    "comment": "# save_to_json(analyzed_data, \"Exports/analyzed_data.json\")",
                    "char_position_in_line": 4,
                    "type": [
                        "commented-out",
                        "block"
                    ]
                },
                {
                    "line": 222,
                    "comment": "# Test case",
                    "char_position_in_line": 4,
                    "type": [
                        "normal"
                    ]
                }
            ]
        },
        {
            "commit": "e226da877d3d9f98cab4990bb1fbaa02e6adbb7b",
            "timestamp": "2024-11-19T15:32:58+01:00",
            "author": "alexander.schranner",
            "diff": {
                "added": {
                    "14": "    dt2 = datetime(2010, 10, 8, 17, 59, 0)",
                    "93": "                    curr_line = commit[\"diff\"][type][i][0]",
                    "94": "                    curr_content = commit[\"diff\"][type][i][1]",
                    "95": "                    if curr_content.find(\"//\") == 0 or curr_content.find(\"// \") != -1:",
                    "98": "                            if len(diff_edited[-1][\"line_numbers\"]) == 0 or curr_line == diff_edited[-1][\"line_numbers\"][-1] + 1:",
                    "99": "                                diff_edited[-1][\"comments\"][curr_line] = curr_content",
                    "100": "                            # else:",
                    "101": "                            #     diff_edited.append({",
                    "102": "                            #         \"line_numbers\": [],",
                    "103": "                            #         \"comments\": {curr_line: curr_content},",
                    "104": "                            #         \"lines\": []})",
                    "106": "                        #     if i < 5:",
                    "107": "                        #     # or create new one",
                    "108": "                        #         print(\"I: \", i, \"comment is first part of block: \", curr_line, \": \", curr_content)",
                    "111": "                                \"line_numbers\": [],",
                    "112": "                                \"comments\": {curr_line: curr_content},",
                    "114": "                    else:",
                    "115": "                        if len(diff_edited) > 0:",
                    "116": "                            # In case of no comment add lines to existing dict if line number directly follows",
                    "117": "                            if len(diff_edited[-1][\"line_numbers\"]) == 0 or curr_line == diff_edited[-1][\"line_numbers\"][-1] + 1:",
                    "118": "                                diff_edited[-1][\"line_numbers\"].append(curr_line)",
                    "119": "                                diff_edited[-1][\"lines\"].append(curr_content)",
                    "120": "                            else:",
                    "121": "                                # or create new one",
                    "122": "                                diff_edited.append({",
                    "123": "                                    \"line_numbers\": [curr_line],",
                    "124": "                                    \"comments\": {},",
                    "125": "                                    \"lines\": [curr_content]})",
                    "137": "            # print(\"Starting to analyse commit: \", commit[\"commit\"])",
                    "146": "            # print(last_modified)",
                    "153": "                    for block in commit[\"diff\"][\"added\"]:",
                    "154": "                        if line in block[\"comments\"] and len(block[\"line_numbers\"]) == 0:",
                    "155": "                            if(comment_time > last_modified[int(line)]):",
                    "156": "                                analysis_results.append({",
                    "157": "                                    \"file\": file,",
                    "158": "                                    \"line\": int(line),",
                    "159": "                                    \"comment\": commit[\"comment_added_diff\"][line],",
                    "160": "                                    \"comment_time\": str(comment_time),",
                    "161": "                                    \"last_code_change_time\": str(last_modified[int(line)])",
                    "162": "                                })",
                    "218": "    repo_url = \"https://github.com/nodejs/node\"  # Example repository URL",
                    "219": "    # commits_data = analyze_commits(repo_url, \"//\", \"js\")",
                    "222": "    with open(\"Data/commits_data.json\", \"r\") as json_file:",
                    "223": "       commits_data = json.load(json_file)",
                    "224": "    analyzed_data = pretty_diff(commits_data, \"added\")",
                    "225": "    save_to_json(analyzed_data, \"Exports/analyzed_data.json\")",
                    "226": "    analyzed_data = pretty_diff(commits_data, \"deleted\")",
                    "227": "    save_to_json(analyzed_data, \"Exports/analyzed_data.json\")",
                    "232": "    analyzed_data = analyze_diffs(data)",
                    "233": ""
                },
                "deleted": {
                    "14": "    dt2 = datetime(2022, 10, 8, 17, 59, 0)",
                    "93": "                    if commit[\"diff\"][type][i][1].find(\"// \") == -1:",
                    "94": "                        if len(diff_edited) > 0:",
                    "95": "                            # In case of no comment add lines to existing dict if line number directly follows",
                    "96": "                            if commit[\"diff\"][type][i][0] == diff_edited[-1][\"line_numbers\"][-1] + 1:",
                    "97": "                                diff_edited[-1][\"line_numbers\"].append(commit[\"diff\"][type][i][0])",
                    "98": "                                diff_edited[-1][\"lines\"].append(commit[\"diff\"][type][i][1])",
                    "99": "                            else:",
                    "100": "                                # or create new one",
                    "101": "                                diff_edited.append({",
                    "102": "                                    \"line_numbers\": [commit[\"diff\"][type][i][0]],",
                    "103": "                                    \"comments\": [],",
                    "104": "                                    \"lines\": [commit[\"diff\"][type][i][1]]})",
                    "105": "                    else:",
                    "108": "                            if commit[\"diff\"][type][i][0] == diff_edited[-1][\"line_numbers\"][-1] + 1:",
                    "109": "                                diff_edited[-1][\"line_numbers\"].append(commit[\"diff\"][type][i][0])",
                    "110": "                                diff_edited[-1][\"comments\"].append(commit[\"diff\"][type][i][1])",
                    "114": "                                \"line_numbers\": [commit[\"diff\"][type][i][0]],",
                    "115": "                                \"comments\": [commit[\"diff\"][type][i][1]],",
                    "128": "            print(\"Starting to analyse commit: \", commit[\"commit\"])",
                    "137": "            print(last_modified)",
                    "143": "                # print(\"Check if \", line, \" is in keys \", last_modified_lines)",
                    "145": "                    # print(\"Check if \", comment_time, \" is larger than \", last_modified[int(line)])",
                    "146": "                    if(comment_time > last_modified[int(line)]):",
                    "147": "                        analysis_results.append({",
                    "148": "                            \"file\": file,",
                    "149": "                            \"line\": int(line),",
                    "150": "                            \"comment\": commit[\"comment_added_diff\"][line],",
                    "151": "                            \"comment_time\": str(comment_time),",
                    "152": "                            \"last_code_change_time\": str(last_modified[int(line)])",
                    "153": "                        })",
                    "154": "            # print(\"Finsihed with commit\")",
                    "155": "            # print(\"Current state analysis results: \", analysis_results, \"\\n\")",
                    "211": "    repo_url = \"https://github.com/espressif/arduino-esp32\"  # Example repository URL",
                    "212": "    # commits_data = analyze_commits(repo_url, \"// \", \"cpp\")",
                    "215": "    # with open(\"Data/commits_data.json\", \"r\") as json_file:",
                    "216": "      #  commits_data = json.load(json_file)",
                    "217": "    # analyzed_data = pretty_diff(commits_data, \"added\")",
                    "218": "    # save_to_json(analyzed_data, \"Exports/analyzed_data.json\")",
                    "219": "    # analyzed_data = pretty_diff(commits_data, \"deleted\")",
                    "220": "    # save_to_json(analyzed_data, \"Exports/analyzed_data.json\")",
                    "225": "    print(\"\\n\")",
                    "226": "    analyzed_data = analyze_diffs(data)",
                    "227": "    for result in analyzed_data:",
                    "228": "        print(f\"In {result['file']}, line {result['line']} was commented on {result['comment_time']} \"",
                    "229": "            f\"after being changed on {result['last_code_change_time']}.\")"
                }
            },
            "source_code": {
                "1": "from pydriller import Repository",
                "2": "import json",
                "3": "import pm4py",
                "4": "from datetime import datetime",
                "5": "from pm4py.objects.log.obj import EventLog, Trace, Event",
                "6": "from pm4py.objects.log.exporter.xes import exporter as xes_exporter",
                "7": "",
                "8": "def analyze_commits(repo_url, comment_symbol, language_file_extension):",
                "9": "    # This will hold the data for each file and its changes across commits",
                "10": "    files_data = {}",
                "11": "",
                "12": "    # Analysis range",
                "13": "    # dt1 = datetime(2022, 10, 8, 17, 0, 0)",
                "14": "    dt2 = datetime(2010, 10, 8, 17, 59, 0)",
                "15": "",
                "16": "    # Traverse through the commits in the repository",
                "17": "    # Only save commits, that contain at least one file of the format {language_file_extension}",
                "18": "    for commit in Repository(repo_url, ",
                "19": "    only_modifications_with_file_types=[f\".{language_file_extension}\"],",
                "20": "    # since=dt1,",
                "21": "    to=dt2).traverse_commits():",
                "22": "        # Analyze each file modified in the commit",
                "23": "        for modified_file in commit.modified_files:",
                "24": "            # only store file data for Rust files",
                "25": "            if modified_file.filename not in files_data:",
                "26": "                files_data[modified_file.filename] = []",
                "27": "            if len(modified_file.filename.split(\".\")) == 2 and modified_file.filename.split(\".\")[1] == language_file_extension:",
                "28": "                file_data = {",
                "29": "                    \"commit\": commit.hash,",
                "30": "                    \"timestamp\": commit.committer_date.isoformat(),",
                "31": "                    \"author\": commit.author.name,",
                "32": "                    \"commit_message\": commit.msg,",
                "33": "                    \"additions\": modified_file.added_lines,",
                "34": "                    \"deletions\": modified_file.deleted_lines,",
                "35": "                    \"change_type\": modified_file.change_type.name,",
                "36": "                    \"diff\": modified_file.diff_parsed",
                "37": "                }",
                "38": "                diff_added = {}",
                "39": "                diff_deleted = {}",
                "40": "                diff_modified = {}",
                "41": "                for line in modified_file.diff_parsed[\"added\"]:",
                "42": "                    if line[1].find(comment_symbol) != -1:",
                "43": "                        diff_added[line[0]] = line[1]",
                "44": "                file_data[\"comment_added_diff\"] = diff_added",
                "45": "                for line in modified_file.diff_parsed[\"deleted\"]:",
                "46": "                    if line[1].find(comment_symbol) != -1:",
                "47": "                        diff_deleted[line[0]] = line[1]",
                "48": "                        if line[0] in diff_added.keys():",
                "49": "                            diff_modified[line[0]] = line[1]",
                "50": "                file_data[\"comment_deleted_diff\"] = diff_deleted",
                "51": "                file_data[\"comment_modified_diff\"] = diff_modified",
                "52": "                # Generate keywords based on the commit message and type of changes",
                "53": "                # file_data[\"keywords\"] = extract_keywords(commit.msg, modified_file)",
                "54": "                # Extract type of commit from commit message",
                "55": "                # file_data[\"activity\"] = extract_activity(commit.msg)",
                "56": "                if len(diff_added) + len(diff_deleted) != 0:",
                "57": "                    files_data[modified_file.filename].append(file_data)",
                "58": "    return files_data",
                "59": "",
                "60": "def extract_keywords(commit_message, modified_file):",
                "61": "    # This function can use NLP techniques or simple keyword extraction",
                "62": "    # Here, a simplified approach is used: basic keywords based on the commit message",
                "63": "    keywords = []",
                "64": "    if \"performance\" in commit_message.lower():",
                "65": "        keywords.append(\"performance\")",
                "66": "    if \"security\" in commit_message.lower():",
                "67": "        keywords.append(\"security\")",
                "68": "    if modified_file.added_lines > modified_file.deleted_lines:",
                "69": "        keywords.append(\"expansion\")",
                "70": "    else:",
                "71": "        keywords.append(\"optimization\")",
                "72": "    return keywords",
                "73": "",
                "74": "def extract_activity(commit_message):",
                "75": "    # Use commit message keywords to determine activity type",
                "76": "    activity = \"\"",
                "77": "    if \"bug\" in commit.msg.lower() or \"fix\" in commit.msg.lower():",
                "78": "        activity = \"Bug Fix\"",
                "79": "    elif \"feature\" in commit.msg.lower() or \"add\" in commit.msg.lower():",
                "80": "        activity = \"Feature Development\"",
                "81": "    elif \"refactor\" in commit.msg.lower():",
                "82": "        activity = \"Refactoring\"",
                "83": "    else:",
                "84": "        activity = \"Other\"",
                "85": "    return activity",
                "86": "",
                "87": "def pretty_diff(commits_data, type):",
                "88": "    for file, commits in commits_data.items():",
                "89": "        if len(file) > 0:",
                "90": "            for commit in commits:",
                "91": "                diff_edited = []",
                "92": "                for i in range(len(commit[\"diff\"][type])):",
                "93": "                    curr_line = commit[\"diff\"][type][i][0]",
                "94": "                    curr_content = commit[\"diff\"][type][i][1]",
                "95": "                    if curr_content.find(\"//\") == 0 or curr_content.find(\"// \") != -1:",
                "96": "                        if len(diff_edited) > 0:",
                "97": "                            # In case of comment add them to existing dict if they directly follow",
                "98": "                            if len(diff_edited[-1][\"line_numbers\"]) == 0 or curr_line == diff_edited[-1][\"line_numbers\"][-1] + 1:",
                "99": "                                diff_edited[-1][\"comments\"][curr_line] = curr_content",
                "100": "                            # else: ",
                "101": "                            #     diff_edited.append({",
                "102": "                            #         \"line_numbers\": [],",
                "103": "                            #         \"comments\": {curr_line: curr_content},",
                "104": "                            #         \"lines\": []})",
                "105": "                        else:",
                "106": "                        #     if i < 5:",
                "107": "                        #     # or create new one",
                "108": "                        #         print(\"I: \", i, \"comment is first part of block: \", curr_line, \": \", curr_content)",
                "109": "                            # or create new one",
                "110": "                            diff_edited.append({",
                "111": "                                \"line_numbers\": [],",
                "112": "                                \"comments\": {curr_line: curr_content},",
                "113": "                                \"lines\": []})",
                "114": "                    else:",
                "115": "                        if len(diff_edited) > 0:",
                "116": "                            # In case of no comment add lines to existing dict if line number directly follows",
                "117": "                            if len(diff_edited[-1][\"line_numbers\"]) == 0 or curr_line == diff_edited[-1][\"line_numbers\"][-1] + 1:",
                "118": "                                diff_edited[-1][\"line_numbers\"].append(curr_line)",
                "119": "                                diff_edited[-1][\"lines\"].append(curr_content)",
                "120": "                            else:",
                "121": "                                # or create new one",
                "122": "                                diff_edited.append({",
                "123": "                                    \"line_numbers\": [curr_line],",
                "124": "                                    \"comments\": {},",
                "125": "                                    \"lines\": [curr_content]})       ",
                "126": "                commit[\"diff\"][type] = diff_edited",
                "127": "    return commits_data",
                "128": "",
                "129": "def analyze_diffs(data):",
                "130": "    analysis_results = []",
                "131": "",
                "132": "    for file, commits in data.items():",
                "133": "        # Store last modified timestamps for each line",
                "134": "        last_modified = {}",
                "135": "",
                "136": "        for commit in commits:",
                "137": "            # print(\"Starting to analyse commit: \", commit[\"commit\"])",
                "138": "            commit_time = datetime.fromisoformat(commit[\"timestamp\"])",
                "139": "            ",
                "140": "            # Track modified lines",
                "141": "            for block in commit[\"diff\"][\"added\"]:",
                "142": "                for line in block[\"line_numbers\"]:",
                "143": "                    line_number = line",
                "144": "                    last_modified[line_number] = commit_time",
                "145": "",
                "146": "            # print(last_modified)",
                "147": "",
                "148": "            # Compare with comments",
                "149": "            for line in commit[\"comment_added_diff\"]:",
                "150": "                comment_time = datetime.fromisoformat(commit[\"timestamp\"])",
                "151": "                last_modified_lines = list(last_modified.keys())",
                "152": "                if int(line) in last_modified_lines:",
                "153": "                    for block in commit[\"diff\"][\"added\"]:",
                "154": "                        if line in block[\"comments\"] and len(block[\"line_numbers\"]) == 0:",
                "155": "                            if(comment_time > last_modified[int(line)]):",
                "156": "                                analysis_results.append({",
                "157": "                                    \"file\": file,",
                "158": "                                    \"line\": int(line),",
                "159": "                                    \"comment\": commit[\"comment_added_diff\"][line],",
                "160": "                                    \"comment_time\": str(comment_time),",
                "161": "                                    \"last_code_change_time\": str(last_modified[int(line)])",
                "162": "                                })",
                "163": "    ",
                "164": "    return analysis_results",
                "165": "",
                "166": "def save_to_json(commits_data, path):",
                "167": "    # Save the processed commit data to a JSON file",
                "168": "    with open(path, 'w') as json_file:",
                "169": "        json.dump(commits_data, json_file, indent=4)",
                "170": "    print(\"Data has been saved to\", path)",
                "171": "",
                "172": "def create_xes_log(data):",
                "173": "    # Create a new EventLog object",
                "174": "    log = EventLog()",
                "175": "",
                "176": "    # Iterate over each commit entry in the data",
                "177": "    for file, commits in data.items():",
                "178": "        # Create a trace for the file",
                "179": "        trace = Trace()",
                "180": "        trace.attributes[\"file\"] = file",
                "181": "",
                "182": "        for commit in commits:",
                "183": "            # Extract event attributes",
                "184": "            event = Event()",
                "185": "            event[\"timestamp\"] = commit.get(\"timestamp\")",
                "186": "            event[\"author\"] = commit.get(\"author\")",
                "187": "            event[\"change_type\"] = commit.get(\"change_type\")",
                "188": "            event[\"commit_message\"] = commit.get(\"commit_message\")",
                "189": "            event[\"additions\"] = commit.get(\"additions\")",
                "190": "            event[\"deletions\"] = commit.get(\"deletions\")",
                "191": "            event[\"diff\"] = commit.get(\"diff\")",
                "192": "            if commit.get(\"comment_added_diff\"):",
                "193": "                event[\"comment_change\"] = \"True\"",
                "194": "            else:",
                "195": "                event[\"comment_change\"] = \"False\"",
                "196": "",
                "197": "            # Add the event to the trace",
                "198": "            trace.append(event)",
                "199": "",
                "200": "        # Add the trace to the log",
                "201": "        log.append(trace)",
                "202": "",
                "203": "    return log",
                "204": "",
                "205": "def save_xes_log(log, filename):",
                "206": "    # Export the log to an XES file",
                "207": "    xes_exporter.apply(log, filename)",
                "208": "",
                "209": "def save_to_xes(log, path):",
                "210": "    # Create the XES log from the commit data",
                "211": "    xes_log = create_xes_log(log)",
                "212": "",
                "213": "    # Save the XES log to a file",
                "214": "    save_xes_log(xes_log, path)",
                "215": "    print(\"XES log has been saved to\", path)",
                "216": "",
                "217": "if __name__ == \"__main__\":",
                "218": "    repo_url = \"https://github.com/nodejs/node\"  # Example repository URL",
                "219": "    # commits_data = analyze_commits(repo_url, \"//\", \"js\")",
                "220": "    # save_to_json(commits_data, \"Data/commits_data.json\")",
                "221": "    # save_to_xes(commits_data, \"Data/commits_data.xes\")",
                "222": "    with open(\"Data/commits_data.json\", \"r\") as json_file:",
                "223": "       commits_data = json.load(json_file)",
                "224": "    analyzed_data = pretty_diff(commits_data, \"added\")",
                "225": "    save_to_json(analyzed_data, \"Exports/analyzed_data.json\")",
                "226": "    analyzed_data = pretty_diff(commits_data, \"deleted\")",
                "227": "    save_to_json(analyzed_data, \"Exports/analyzed_data.json\")",
                "228": "    ",
                "229": "    # Test case",
                "230": "    with open(\"Exports/analyzed_data.json\", \"r\") as json_file:",
                "231": "        data = json.load(json_file)",
                "232": "    analyzed_data = analyze_diffs(data)",
                "233": "",
                "234": "    save_to_json(analyzed_data, \"Exports/analysis_results.json\")",
                "235": ""
            },
            "comments": [
                {
                    "line": 9,
                    "comment": "# This will hold the data for each file and its changes across commits",
                    "char_position_in_line": 4,
                    "type": [
                        "normal"
                    ]
                },
                {
                    "line": 12,
                    "comment": "# Analysis range",
                    "char_position_in_line": 4,
                    "type": [
                        "block"
                    ]
                },
                {
                    "line": 13,
                    "comment": "# dt1 = datetime(2022, 10, 8, 17, 0, 0)",
                    "char_position_in_line": 4,
                    "type": [
                        "block"
                    ]
                },
                {
                    "line": 16,
                    "comment": "# Traverse through the commits in the repository",
                    "char_position_in_line": 4,
                    "type": [
                        "block"
                    ]
                },
                {
                    "line": 17,
                    "comment": "# Only save commits, that contain at least one file of the format {language_file_extension}",
                    "char_position_in_line": 4,
                    "type": [
                        "block"
                    ]
                },
                {
                    "line": 20,
                    "comment": "# since=dt1,",
                    "char_position_in_line": 4,
                    "type": [
                        "commented-out"
                    ]
                },
                {
                    "line": 22,
                    "comment": "# Analyze each file modified in the commit",
                    "char_position_in_line": 8,
                    "type": [
                        "normal"
                    ]
                },
                {
                    "line": 24,
                    "comment": "# only store file data for Rust files",
                    "char_position_in_line": 12,
                    "type": [
                        "normal"
                    ]
                },
                {
                    "line": 52,
                    "comment": "# Generate keywords based on the commit message and type of changes",
                    "char_position_in_line": 16,
                    "type": [
                        "block"
                    ]
                },
                {
                    "line": 53,
                    "comment": "# file_data[\"keywords\"] = extract_keywords(commit.msg, modified_file)",
                    "char_position_in_line": 16,
                    "type": [
                        "block"
                    ]
                },
                {
                    "line": 54,
                    "comment": "# Extract type of commit from commit message",
                    "char_position_in_line": 16,
                    "type": [
                        "block"
                    ]
                },
                {
                    "line": 55,
                    "comment": "# file_data[\"activity\"] = extract_activity(commit.msg)",
                    "char_position_in_line": 16,
                    "type": [
                        "block"
                    ]
                },
                {
                    "line": 61,
                    "comment": "# This function can use NLP techniques or simple keyword extraction",
                    "char_position_in_line": 4,
                    "type": [
                        "block"
                    ]
                },
                {
                    "line": 62,
                    "comment": "# Here, a simplified approach is used: basic keywords based on the commit message",
                    "char_position_in_line": 4,
                    "type": [
                        "block"
                    ]
                },
                {
                    "line": 75,
                    "comment": "# Use commit message keywords to determine activity type",
                    "char_position_in_line": 4,
                    "type": [
                        "normal"
                    ]
                },
                {
                    "line": 97,
                    "comment": "# In case of comment add them to existing dict if they directly follow",
                    "char_position_in_line": 28,
                    "type": [
                        "normal"
                    ]
                },
                {
                    "line": 100,
                    "comment": "# else: ",
                    "char_position_in_line": 28,
                    "type": [
                        "block"
                    ]
                },
                {
                    "line": 101,
                    "comment": "#     diff_edited.append({",
                    "char_position_in_line": 28,
                    "type": [
                        "block"
                    ]
                },
                {
                    "line": 102,
                    "comment": "#         \"line_numbers\": [],",
                    "char_position_in_line": 28,
                    "type": [
                        "block"
                    ]
                },
                {
                    "line": 103,
                    "comment": "#         \"comments\": {curr_line: curr_content},",
                    "char_position_in_line": 28,
                    "type": [
                        "block"
                    ]
                },
                {
                    "line": 104,
                    "comment": "#         \"lines\": []})",
                    "char_position_in_line": 28,
                    "type": [
                        "block"
                    ]
                },
                {
                    "line": 106,
                    "comment": "#     if i < 5:",
                    "char_position_in_line": 24,
                    "type": [
                        "block"
                    ]
                },
                {
                    "line": 107,
                    "comment": "#     # or create new one",
                    "char_position_in_line": 24,
                    "type": [
                        "block"
                    ]
                },
                {
                    "line": 108,
                    "comment": "#         print(\"I: \", i, \"comment is first part of block: \", curr_line, \": \", curr_content)",
                    "char_position_in_line": 24,
                    "type": [
                        "block"
                    ]
                },
                {
                    "line": 109,
                    "comment": "# or create new one",
                    "char_position_in_line": 28,
                    "type": [
                        "normal"
                    ]
                },
                {
                    "line": 116,
                    "comment": "# In case of no comment add lines to existing dict if line number directly follows",
                    "char_position_in_line": 28,
                    "type": [
                        "normal"
                    ]
                },
                {
                    "line": 121,
                    "comment": "# or create new one",
                    "char_position_in_line": 32,
                    "type": [
                        "normal"
                    ]
                },
                {
                    "line": 133,
                    "comment": "# Store last modified timestamps for each line",
                    "char_position_in_line": 8,
                    "type": [
                        "normal"
                    ]
                },
                {
                    "line": 137,
                    "comment": "# print(\"Starting to analyse commit: \", commit[\"commit\"])",
                    "char_position_in_line": 12,
                    "type": [
                        "commented-out"
                    ]
                },
                {
                    "line": 140,
                    "comment": "# Track modified lines",
                    "char_position_in_line": 12,
                    "type": [
                        "normal"
                    ]
                },
                {
                    "line": 146,
                    "comment": "# print(last_modified)",
                    "char_position_in_line": 12,
                    "type": [
                        "commented-out"
                    ]
                },
                {
                    "line": 148,
                    "comment": "# Compare with comments",
                    "char_position_in_line": 12,
                    "type": [
                        "normal"
                    ]
                },
                {
                    "line": 167,
                    "comment": "# Save the processed commit data to a JSON file",
                    "char_position_in_line": 4,
                    "type": [
                        "normal"
                    ]
                },
                {
                    "line": 173,
                    "comment": "# Create a new EventLog object",
                    "char_position_in_line": 4,
                    "type": [
                        "normal"
                    ]
                },
                {
                    "line": 176,
                    "comment": "# Iterate over each commit entry in the data",
                    "char_position_in_line": 4,
                    "type": [
                        "normal"
                    ]
                },
                {
                    "line": 178,
                    "comment": "# Create a trace for the file",
                    "char_position_in_line": 8,
                    "type": [
                        "normal"
                    ]
                },
                {
                    "line": 183,
                    "comment": "# Extract event attributes",
                    "char_position_in_line": 12,
                    "type": [
                        "normal"
                    ]
                },
                {
                    "line": 197,
                    "comment": "# Add the event to the trace",
                    "char_position_in_line": 12,
                    "type": [
                        "normal"
                    ]
                },
                {
                    "line": 200,
                    "comment": "# Add the trace to the log",
                    "char_position_in_line": 8,
                    "type": [
                        "normal"
                    ]
                },
                {
                    "line": 206,
                    "comment": "# Export the log to an XES file",
                    "char_position_in_line": 4,
                    "type": [
                        "normal"
                    ]
                },
                {
                    "line": 210,
                    "comment": "# Create the XES log from the commit data",
                    "char_position_in_line": 4,
                    "type": [
                        "normal"
                    ]
                },
                {
                    "line": 213,
                    "comment": "# Save the XES log to a file",
                    "char_position_in_line": 4,
                    "type": [
                        "normal"
                    ]
                },
                {
                    "line": 218,
                    "comment": "# Example repository URL",
                    "char_position_in_line": 49,
                    "type": [
                        "normal"
                    ]
                },
                {
                    "line": 219,
                    "comment": "# commits_data = analyze_commits(repo_url, \"//\", \"js\")",
                    "char_position_in_line": 4,
                    "type": [
                        "commented-out",
                        "block"
                    ]
                },
                {
                    "line": 220,
                    "comment": "# save_to_json(commits_data, \"Data/commits_data.json\")",
                    "char_position_in_line": 4,
                    "type": [
                        "commented-out",
                        "block"
                    ]
                },
                {
                    "line": 221,
                    "comment": "# save_to_xes(commits_data, \"Data/commits_data.xes\")",
                    "char_position_in_line": 4,
                    "type": [
                        "commented-out",
                        "block"
                    ]
                },
                {
                    "line": 229,
                    "comment": "# Test case",
                    "char_position_in_line": 4,
                    "type": [
                        "normal"
                    ]
                }
            ]
        },
        {
            "commit": "367493d87df2ab980b261f32c54beb20f1bc5c4e",
            "timestamp": "2024-11-19T17:09:41+01:00",
            "author": "alexander.schranner",
            "diff": {
                "added": {
                    "8": "def analyze_commits(repo_url, language_file_extension, dt1, dt2, single_comment_symbol, multi_comment_symbols=[]):",
                    "16": "    since=dt1,",
                    "18": "        if len(multi_comment_symbols) >= 2:",
                    "19": "            multi_comments_enabled = True",
                    "20": "        else:",
                    "21": "            multi_comments_enabled = False",
                    "41": "                following_multi_comment = False",
                    "43": "                    if line[1].find(single_comment_symbol) != -1 or following_multi_comment:",
                    "44": "                        diff_added[line[0]] = line[1]",
                    "45": "                    if multi_comments_enabled and line[1].find(multi_comment_symbols[0]) != -1:",
                    "46": "                        diff_added[line[0]] = line[1]",
                    "47": "                        following_multi_comment = True",
                    "48": "                    if multi_comments_enabled and line[1].find(multi_comment_symbols[1]) != -1:",
                    "50": "                        following_multi_comment = False",
                    "53": "                    if line[1].find(single_comment_symbol) != -1 or following_multi_comment:",
                    "57": "                    if multi_comments_enabled and line[1].find(multi_comment_symbols[0]) != -1:",
                    "58": "                        diff_added[line[0]] = line[1]",
                    "59": "                        following_multi_comment = True",
                    "60": "                        if line[0] in diff_added.keys():",
                    "61": "                            diff_modified[line[0]] = line[1]",
                    "62": "                    if multi_comments_enabled and line[1].find(multi_comment_symbols[1]) != -1:",
                    "63": "                        diff_added[line[0]] = line[1]",
                    "64": "                        if line[0] in diff_added.keys():",
                    "65": "                            diff_modified[line[0]] = line[1]",
                    "103": "def pretty_diff(commits_data, type, single_comment_symbol, multi_comment_symbols=[]):",
                    "104": "    following_multi_comment = False",
                    "105": "    if len(multi_comment_symbols) >= 2:",
                    "106": "        multi_comments_enabled = True",
                    "107": "    else:",
                    "108": "        multi_comments_enabled = False",
                    "116": "                    if curr_content == \"/*<replacement>*/\":",
                    "117": "                        print()",
                    "118": "                    if curr_content.find(multi_comment_symbols[0]) != -1:",
                    "119": "                            following_multi_comment = True",
                    "120": "                    if curr_content.find(single_comment_symbol) == 0 or curr_content.find(single_comment_symbol + \" \") != -1 or following_multi_comment:",
                    "142": "                                    \"lines\": [curr_content]})",
                    "143": "                    if multi_comments_enabled and curr_content.find(multi_comment_symbols[1]) != -1:",
                    "144": "                        following_multi_comment = False",
                    "238": "    commits_data = analyze_commits(repo_url, \"js\", datetime(2015,2,1), datetime(2015,8,1), \"//\", [\"/*\", \"*/\"])",
                    "239": "    save_to_json(commits_data, \"Data/commits_data.json\")",
                    "243": "    analyzed_data = pretty_diff(commits_data, \"added\", \"//\", [\"/*\", \"*/\"])",
                    "245": "    analyzed_data = pretty_diff(commits_data, \"deleted\", \"//\", [\"/*\", \"*/\"])"
                },
                "deleted": {
                    "8": "def analyze_commits(repo_url, comment_symbol, language_file_extension):",
                    "12": "    # Analysis range",
                    "13": "    # dt1 = datetime(2022, 10, 8, 17, 0, 0)",
                    "14": "    dt2 = datetime(2010, 10, 8, 17, 59, 0)",
                    "15": "",
                    "20": "    # since=dt1,",
                    "42": "                    if line[1].find(comment_symbol) != -1:",
                    "46": "                    if line[1].find(comment_symbol) != -1:",
                    "87": "def pretty_diff(commits_data, type):",
                    "95": "                    if curr_content.find(\"//\") == 0 or curr_content.find(\"// \") != -1:",
                    "100": "                            # else:",
                    "101": "                            #     diff_edited.append({",
                    "102": "                            #         \"line_numbers\": [],",
                    "103": "                            #         \"comments\": {curr_line: curr_content},",
                    "104": "                            #         \"lines\": []})",
                    "106": "                        #     if i < 5:",
                    "107": "                        #     # or create new one",
                    "108": "                        #         print(\"I: \", i, \"comment is first part of block: \", curr_line, \": \", curr_content)",
                    "125": "                                    \"lines\": [curr_content]})",
                    "219": "    # commits_data = analyze_commits(repo_url, \"//\", \"js\")",
                    "220": "    # save_to_json(commits_data, \"Data/commits_data.json\")",
                    "224": "    analyzed_data = pretty_diff(commits_data, \"added\")",
                    "226": "    analyzed_data = pretty_diff(commits_data, \"deleted\")"
                }
            },
            "source_code": {
                "1": "from pydriller import Repository",
                "2": "import json",
                "3": "import pm4py",
                "4": "from datetime import datetime",
                "5": "from pm4py.objects.log.obj import EventLog, Trace, Event",
                "6": "from pm4py.objects.log.exporter.xes import exporter as xes_exporter",
                "7": "",
                "8": "def analyze_commits(repo_url, language_file_extension, dt1, dt2, single_comment_symbol, multi_comment_symbols=[]):",
                "9": "    # This will hold the data for each file and its changes across commits",
                "10": "    files_data = {}",
                "11": "",
                "12": "    # Traverse through the commits in the repository",
                "13": "    # Only save commits, that contain at least one file of the format {language_file_extension}",
                "14": "    for commit in Repository(repo_url, ",
                "15": "    only_modifications_with_file_types=[f\".{language_file_extension}\"],",
                "16": "    since=dt1,",
                "17": "    to=dt2).traverse_commits():",
                "18": "        if len(multi_comment_symbols) >= 2:",
                "19": "            multi_comments_enabled = True",
                "20": "        else:",
                "21": "            multi_comments_enabled = False",
                "22": "        # Analyze each file modified in the commit",
                "23": "        for modified_file in commit.modified_files:",
                "24": "            # only store file data for Rust files",
                "25": "            if modified_file.filename not in files_data:",
                "26": "                files_data[modified_file.filename] = []",
                "27": "            if len(modified_file.filename.split(\".\")) == 2 and modified_file.filename.split(\".\")[1] == language_file_extension:",
                "28": "                file_data = {",
                "29": "                    \"commit\": commit.hash,",
                "30": "                    \"timestamp\": commit.committer_date.isoformat(),",
                "31": "                    \"author\": commit.author.name,",
                "32": "                    \"commit_message\": commit.msg,",
                "33": "                    \"additions\": modified_file.added_lines,",
                "34": "                    \"deletions\": modified_file.deleted_lines,",
                "35": "                    \"change_type\": modified_file.change_type.name,",
                "36": "                    \"diff\": modified_file.diff_parsed",
                "37": "                }",
                "38": "                diff_added = {}",
                "39": "                diff_deleted = {}",
                "40": "                diff_modified = {}",
                "41": "                following_multi_comment = False",
                "42": "                for line in modified_file.diff_parsed[\"added\"]:",
                "43": "                    if line[1].find(single_comment_symbol) != -1 or following_multi_comment:",
                "44": "                        diff_added[line[0]] = line[1]",
                "45": "                    if multi_comments_enabled and line[1].find(multi_comment_symbols[0]) != -1:",
                "46": "                        diff_added[line[0]] = line[1]",
                "47": "                        following_multi_comment = True",
                "48": "                    if multi_comments_enabled and line[1].find(multi_comment_symbols[1]) != -1:",
                "49": "                        diff_added[line[0]] = line[1]",
                "50": "                        following_multi_comment = False",
                "51": "                file_data[\"comment_added_diff\"] = diff_added",
                "52": "                for line in modified_file.diff_parsed[\"deleted\"]:",
                "53": "                    if line[1].find(single_comment_symbol) != -1 or following_multi_comment:",
                "54": "                        diff_deleted[line[0]] = line[1]",
                "55": "                        if line[0] in diff_added.keys():",
                "56": "                            diff_modified[line[0]] = line[1]",
                "57": "                    if multi_comments_enabled and line[1].find(multi_comment_symbols[0]) != -1:",
                "58": "                        diff_added[line[0]] = line[1]",
                "59": "                        following_multi_comment = True",
                "60": "                        if line[0] in diff_added.keys():",
                "61": "                            diff_modified[line[0]] = line[1]",
                "62": "                    if multi_comments_enabled and line[1].find(multi_comment_symbols[1]) != -1:",
                "63": "                        diff_added[line[0]] = line[1]",
                "64": "                        if line[0] in diff_added.keys():",
                "65": "                            diff_modified[line[0]] = line[1]",
                "66": "                file_data[\"comment_deleted_diff\"] = diff_deleted",
                "67": "                file_data[\"comment_modified_diff\"] = diff_modified",
                "68": "                # Generate keywords based on the commit message and type of changes",
                "69": "                # file_data[\"keywords\"] = extract_keywords(commit.msg, modified_file)",
                "70": "                # Extract type of commit from commit message",
                "71": "                # file_data[\"activity\"] = extract_activity(commit.msg)",
                "72": "                if len(diff_added) + len(diff_deleted) != 0:",
                "73": "                    files_data[modified_file.filename].append(file_data)",
                "74": "    return files_data",
                "75": "",
                "76": "def extract_keywords(commit_message, modified_file):",
                "77": "    # This function can use NLP techniques or simple keyword extraction",
                "78": "    # Here, a simplified approach is used: basic keywords based on the commit message",
                "79": "    keywords = []",
                "80": "    if \"performance\" in commit_message.lower():",
                "81": "        keywords.append(\"performance\")",
                "82": "    if \"security\" in commit_message.lower():",
                "83": "        keywords.append(\"security\")",
                "84": "    if modified_file.added_lines > modified_file.deleted_lines:",
                "85": "        keywords.append(\"expansion\")",
                "86": "    else:",
                "87": "        keywords.append(\"optimization\")",
                "88": "    return keywords",
                "89": "",
                "90": "def extract_activity(commit_message):",
                "91": "    # Use commit message keywords to determine activity type",
                "92": "    activity = \"\"",
                "93": "    if \"bug\" in commit.msg.lower() or \"fix\" in commit.msg.lower():",
                "94": "        activity = \"Bug Fix\"",
                "95": "    elif \"feature\" in commit.msg.lower() or \"add\" in commit.msg.lower():",
                "96": "        activity = \"Feature Development\"",
                "97": "    elif \"refactor\" in commit.msg.lower():",
                "98": "        activity = \"Refactoring\"",
                "99": "    else:",
                "100": "        activity = \"Other\"",
                "101": "    return activity",
                "102": "",
                "103": "def pretty_diff(commits_data, type, single_comment_symbol, multi_comment_symbols=[]):",
                "104": "    following_multi_comment = False",
                "105": "    if len(multi_comment_symbols) >= 2:",
                "106": "        multi_comments_enabled = True",
                "107": "    else:",
                "108": "        multi_comments_enabled = False",
                "109": "    for file, commits in commits_data.items():",
                "110": "        if len(file) > 0:",
                "111": "            for commit in commits:",
                "112": "                diff_edited = []",
                "113": "                for i in range(len(commit[\"diff\"][type])):",
                "114": "                    curr_line = commit[\"diff\"][type][i][0]",
                "115": "                    curr_content = commit[\"diff\"][type][i][1]",
                "116": "                    if curr_content == \"/*<replacement>*/\": ",
                "117": "                        print()",
                "118": "                    if curr_content.find(multi_comment_symbols[0]) != -1:",
                "119": "                            following_multi_comment = True",
                "120": "                    if curr_content.find(single_comment_symbol) == 0 or curr_content.find(single_comment_symbol + \" \") != -1 or following_multi_comment:",
                "121": "                        if len(diff_edited) > 0:",
                "122": "                            # In case of comment add them to existing dict if they directly follow",
                "123": "                            if len(diff_edited[-1][\"line_numbers\"]) == 0 or curr_line == diff_edited[-1][\"line_numbers\"][-1] + 1:",
                "124": "                                diff_edited[-1][\"comments\"][curr_line] = curr_content",
                "125": "                        else:",
                "126": "                            # or create new one",
                "127": "                            diff_edited.append({",
                "128": "                                \"line_numbers\": [],",
                "129": "                                \"comments\": {curr_line: curr_content},",
                "130": "                                \"lines\": []})",
                "131": "                    else:",
                "132": "                        if len(diff_edited) > 0:",
                "133": "                            # In case of no comment add lines to existing dict if line number directly follows",
                "134": "                            if len(diff_edited[-1][\"line_numbers\"]) == 0 or curr_line == diff_edited[-1][\"line_numbers\"][-1] + 1:",
                "135": "                                diff_edited[-1][\"line_numbers\"].append(curr_line)",
                "136": "                                diff_edited[-1][\"lines\"].append(curr_content)",
                "137": "                            else:",
                "138": "                                # or create new one",
                "139": "                                diff_edited.append({",
                "140": "                                    \"line_numbers\": [curr_line],",
                "141": "                                    \"comments\": {},",
                "142": "                                    \"lines\": [curr_content]}) ",
                "143": "                    if multi_comments_enabled and curr_content.find(multi_comment_symbols[1]) != -1:",
                "144": "                        following_multi_comment = False",
                "145": "                commit[\"diff\"][type] = diff_edited",
                "146": "    return commits_data",
                "147": "",
                "148": "def analyze_diffs(data):",
                "149": "    analysis_results = []",
                "150": "",
                "151": "    for file, commits in data.items():",
                "152": "        # Store last modified timestamps for each line",
                "153": "        last_modified = {}",
                "154": "",
                "155": "        for commit in commits:",
                "156": "            # print(\"Starting to analyse commit: \", commit[\"commit\"])",
                "157": "            commit_time = datetime.fromisoformat(commit[\"timestamp\"])",
                "158": "            ",
                "159": "            # Track modified lines",
                "160": "            for block in commit[\"diff\"][\"added\"]:",
                "161": "                for line in block[\"line_numbers\"]:",
                "162": "                    line_number = line",
                "163": "                    last_modified[line_number] = commit_time",
                "164": "",
                "165": "            # print(last_modified)",
                "166": "",
                "167": "            # Compare with comments",
                "168": "            for line in commit[\"comment_added_diff\"]:",
                "169": "                comment_time = datetime.fromisoformat(commit[\"timestamp\"])",
                "170": "                last_modified_lines = list(last_modified.keys())",
                "171": "                if int(line) in last_modified_lines:",
                "172": "                    for block in commit[\"diff\"][\"added\"]:",
                "173": "                        if line in block[\"comments\"] and len(block[\"line_numbers\"]) == 0:",
                "174": "                            if(comment_time > last_modified[int(line)]):",
                "175": "                                analysis_results.append({",
                "176": "                                    \"file\": file,",
                "177": "                                    \"line\": int(line),",
                "178": "                                    \"comment\": commit[\"comment_added_diff\"][line],",
                "179": "                                    \"comment_time\": str(comment_time),",
                "180": "                                    \"last_code_change_time\": str(last_modified[int(line)])",
                "181": "                                })",
                "182": "    ",
                "183": "    return analysis_results",
                "184": "",
                "185": "def save_to_json(commits_data, path):",
                "186": "    # Save the processed commit data to a JSON file",
                "187": "    with open(path, 'w') as json_file:",
                "188": "        json.dump(commits_data, json_file, indent=4)",
                "189": "    print(\"Data has been saved to\", path)",
                "190": "",
                "191": "def create_xes_log(data):",
                "192": "    # Create a new EventLog object",
                "193": "    log = EventLog()",
                "194": "",
                "195": "    # Iterate over each commit entry in the data",
                "196": "    for file, commits in data.items():",
                "197": "        # Create a trace for the file",
                "198": "        trace = Trace()",
                "199": "        trace.attributes[\"file\"] = file",
                "200": "",
                "201": "        for commit in commits:",
                "202": "            # Extract event attributes",
                "203": "            event = Event()",
                "204": "            event[\"timestamp\"] = commit.get(\"timestamp\")",
                "205": "            event[\"author\"] = commit.get(\"author\")",
                "206": "            event[\"change_type\"] = commit.get(\"change_type\")",
                "207": "            event[\"commit_message\"] = commit.get(\"commit_message\")",
                "208": "            event[\"additions\"] = commit.get(\"additions\")",
                "209": "            event[\"deletions\"] = commit.get(\"deletions\")",
                "210": "            event[\"diff\"] = commit.get(\"diff\")",
                "211": "            if commit.get(\"comment_added_diff\"):",
                "212": "                event[\"comment_change\"] = \"True\"",
                "213": "            else:",
                "214": "                event[\"comment_change\"] = \"False\"",
                "215": "",
                "216": "            # Add the event to the trace",
                "217": "            trace.append(event)",
                "218": "",
                "219": "        # Add the trace to the log",
                "220": "        log.append(trace)",
                "221": "",
                "222": "    return log",
                "223": "",
                "224": "def save_xes_log(log, filename):",
                "225": "    # Export the log to an XES file",
                "226": "    xes_exporter.apply(log, filename)",
                "227": "",
                "228": "def save_to_xes(log, path):",
                "229": "    # Create the XES log from the commit data",
                "230": "    xes_log = create_xes_log(log)",
                "231": "",
                "232": "    # Save the XES log to a file",
                "233": "    save_xes_log(xes_log, path)",
                "234": "    print(\"XES log has been saved to\", path)",
                "235": "",
                "236": "if __name__ == \"__main__\":",
                "237": "    repo_url = \"https://github.com/nodejs/node\"  # Example repository URL",
                "238": "    commits_data = analyze_commits(repo_url, \"js\", datetime(2015,2,1), datetime(2015,8,1), \"//\", [\"/*\", \"*/\"])",
                "239": "    save_to_json(commits_data, \"Data/commits_data.json\")",
                "240": "    # save_to_xes(commits_data, \"Data/commits_data.xes\")",
                "241": "    with open(\"Data/commits_data.json\", \"r\") as json_file:",
                "242": "       commits_data = json.load(json_file)",
                "243": "    analyzed_data = pretty_diff(commits_data, \"added\", \"//\", [\"/*\", \"*/\"])",
                "244": "    save_to_json(analyzed_data, \"Exports/analyzed_data.json\")",
                "245": "    analyzed_data = pretty_diff(commits_data, \"deleted\", \"//\", [\"/*\", \"*/\"])",
                "246": "    save_to_json(analyzed_data, \"Exports/analyzed_data.json\")",
                "247": "    ",
                "248": "    # Test case",
                "249": "    with open(\"Exports/analyzed_data.json\", \"r\") as json_file:",
                "250": "        data = json.load(json_file)",
                "251": "    analyzed_data = analyze_diffs(data)",
                "252": "",
                "253": "    save_to_json(analyzed_data, \"Exports/analysis_results.json\")",
                "254": ""
            },
            "comments": [
                {
                    "line": 9,
                    "comment": "# This will hold the data for each file and its changes across commits",
                    "char_position_in_line": 4,
                    "type": [
                        "normal"
                    ]
                },
                {
                    "line": 12,
                    "comment": "# Traverse through the commits in the repository",
                    "char_position_in_line": 4,
                    "type": [
                        "block"
                    ]
                },
                {
                    "line": 13,
                    "comment": "# Only save commits, that contain at least one file of the format {language_file_extension}",
                    "char_position_in_line": 4,
                    "type": [
                        "block"
                    ]
                },
                {
                    "line": 22,
                    "comment": "# Analyze each file modified in the commit",
                    "char_position_in_line": 8,
                    "type": [
                        "normal"
                    ]
                },
                {
                    "line": 24,
                    "comment": "# only store file data for Rust files",
                    "char_position_in_line": 12,
                    "type": [
                        "normal"
                    ]
                },
                {
                    "line": 68,
                    "comment": "# Generate keywords based on the commit message and type of changes",
                    "char_position_in_line": 16,
                    "type": [
                        "block"
                    ]
                },
                {
                    "line": 69,
                    "comment": "# file_data[\"keywords\"] = extract_keywords(commit.msg, modified_file)",
                    "char_position_in_line": 16,
                    "type": [
                        "block"
                    ]
                },
                {
                    "line": 70,
                    "comment": "# Extract type of commit from commit message",
                    "char_position_in_line": 16,
                    "type": [
                        "block"
                    ]
                },
                {
                    "line": 71,
                    "comment": "# file_data[\"activity\"] = extract_activity(commit.msg)",
                    "char_position_in_line": 16,
                    "type": [
                        "block"
                    ]
                },
                {
                    "line": 77,
                    "comment": "# This function can use NLP techniques or simple keyword extraction",
                    "char_position_in_line": 4,
                    "type": [
                        "block"
                    ]
                },
                {
                    "line": 78,
                    "comment": "# Here, a simplified approach is used: basic keywords based on the commit message",
                    "char_position_in_line": 4,
                    "type": [
                        "block"
                    ]
                },
                {
                    "line": 91,
                    "comment": "# Use commit message keywords to determine activity type",
                    "char_position_in_line": 4,
                    "type": [
                        "normal"
                    ]
                },
                {
                    "line": 122,
                    "comment": "# In case of comment add them to existing dict if they directly follow",
                    "char_position_in_line": 28,
                    "type": [
                        "normal"
                    ]
                },
                {
                    "line": 126,
                    "comment": "# or create new one",
                    "char_position_in_line": 28,
                    "type": [
                        "normal"
                    ]
                },
                {
                    "line": 133,
                    "comment": "# In case of no comment add lines to existing dict if line number directly follows",
                    "char_position_in_line": 28,
                    "type": [
                        "normal"
                    ]
                },
                {
                    "line": 138,
                    "comment": "# or create new one",
                    "char_position_in_line": 32,
                    "type": [
                        "normal"
                    ]
                },
                {
                    "line": 152,
                    "comment": "# Store last modified timestamps for each line",
                    "char_position_in_line": 8,
                    "type": [
                        "normal"
                    ]
                },
                {
                    "line": 156,
                    "comment": "# print(\"Starting to analyse commit: \", commit[\"commit\"])",
                    "char_position_in_line": 12,
                    "type": [
                        "commented-out"
                    ]
                },
                {
                    "line": 159,
                    "comment": "# Track modified lines",
                    "char_position_in_line": 12,
                    "type": [
                        "normal"
                    ]
                },
                {
                    "line": 165,
                    "comment": "# print(last_modified)",
                    "char_position_in_line": 12,
                    "type": [
                        "commented-out"
                    ]
                },
                {
                    "line": 167,
                    "comment": "# Compare with comments",
                    "char_position_in_line": 12,
                    "type": [
                        "normal"
                    ]
                },
                {
                    "line": 186,
                    "comment": "# Save the processed commit data to a JSON file",
                    "char_position_in_line": 4,
                    "type": [
                        "normal"
                    ]
                },
                {
                    "line": 192,
                    "comment": "# Create a new EventLog object",
                    "char_position_in_line": 4,
                    "type": [
                        "normal"
                    ]
                },
                {
                    "line": 195,
                    "comment": "# Iterate over each commit entry in the data",
                    "char_position_in_line": 4,
                    "type": [
                        "normal"
                    ]
                },
                {
                    "line": 197,
                    "comment": "# Create a trace for the file",
                    "char_position_in_line": 8,
                    "type": [
                        "normal"
                    ]
                },
                {
                    "line": 202,
                    "comment": "# Extract event attributes",
                    "char_position_in_line": 12,
                    "type": [
                        "normal"
                    ]
                },
                {
                    "line": 216,
                    "comment": "# Add the event to the trace",
                    "char_position_in_line": 12,
                    "type": [
                        "normal"
                    ]
                },
                {
                    "line": 219,
                    "comment": "# Add the trace to the log",
                    "char_position_in_line": 8,
                    "type": [
                        "normal"
                    ]
                },
                {
                    "line": 225,
                    "comment": "# Export the log to an XES file",
                    "char_position_in_line": 4,
                    "type": [
                        "normal"
                    ]
                },
                {
                    "line": 229,
                    "comment": "# Create the XES log from the commit data",
                    "char_position_in_line": 4,
                    "type": [
                        "normal"
                    ]
                },
                {
                    "line": 232,
                    "comment": "# Save the XES log to a file",
                    "char_position_in_line": 4,
                    "type": [
                        "normal"
                    ]
                },
                {
                    "line": 237,
                    "comment": "# Example repository URL",
                    "char_position_in_line": 49,
                    "type": [
                        "normal"
                    ]
                },
                {
                    "line": 240,
                    "comment": "# save_to_xes(commits_data, \"Data/commits_data.xes\")",
                    "char_position_in_line": 4,
                    "type": [
                        "commented-out"
                    ]
                },
                {
                    "line": 248,
                    "comment": "# Test case",
                    "char_position_in_line": 4,
                    "type": [
                        "normal"
                    ]
                }
            ]
        },
        {
            "commit": "bdfb1cc02e0a08cad8109e142080e326cd09f189",
            "timestamp": "2024-11-19T17:28:53+01:00",
            "author": "alexander.schranner",
            "diff": {
                "added": {
                    "40": "                # For added diff ispect lines filter out comments",
                    "51": "                # For deleted diff ispect lines filter out comments",
                    "77": "    # Determine basic keywords based on the commit message",
                    "112": "                # Set current line for each analysis",
                    "116": "                    # In case of a starting multiline comment start adding future lines without comment symbol",
                    "119": "                    # In case of comment add them to existing dict if they directly follow",
                    "125": "                    # or create new one",
                    "130": "                    # In case of no comment add lines to existing dict if line number directly follows",
                    "131": "                    else:",
                    "137": "                    # Or create new one",
                    "141": "                                    \"lines\": [curr_content]})",
                    "142": "                    # Disable multiline comments when symbol found"
                },
                "deleted": {
                    "9": "    # This will hold the data for each file and its changes across commits",
                    "11": "",
                    "77": "    # This function can use NLP techniques or simple keyword extraction",
                    "78": "    # Here, a simplified approach is used: basic keywords based on the commit message",
                    "116": "                    if curr_content == \"/*<replacement>*/\":",
                    "117": "                        print()",
                    "122": "                            # In case of comment add them to existing dict if they directly follow",
                    "126": "                            # or create new one",
                    "131": "                    else:",
                    "133": "                            # In case of no comment add lines to existing dict if line number directly follows",
                    "138": "                                # or create new one",
                    "142": "                                    \"lines\": [curr_content]})",
                    "154": "",
                    "158": "",
                    "164": "",
                    "165": "            # print(last_modified)",
                    "166": "",
                    "182": ""
                }
            },
            "source_code": {
                "1": "from pydriller import Repository",
                "2": "import json",
                "3": "import pm4py",
                "4": "from datetime import datetime",
                "5": "from pm4py.objects.log.obj import EventLog, Trace, Event",
                "6": "from pm4py.objects.log.exporter.xes import exporter as xes_exporter",
                "7": "",
                "8": "def analyze_commits(repo_url, language_file_extension, dt1, dt2, single_comment_symbol, multi_comment_symbols=[]):",
                "9": "    files_data = {}",
                "10": "    # Traverse through the commits in the repository",
                "11": "    # Only save commits, that contain at least one file of the format {language_file_extension}",
                "12": "    for commit in Repository(repo_url, ",
                "13": "    only_modifications_with_file_types=[f\".{language_file_extension}\"],",
                "14": "    since=dt1,",
                "15": "    to=dt2).traverse_commits():",
                "16": "        if len(multi_comment_symbols) >= 2:",
                "17": "            multi_comments_enabled = True",
                "18": "        else:",
                "19": "            multi_comments_enabled = False",
                "20": "        # Analyze each file modified in the commit",
                "21": "        for modified_file in commit.modified_files:",
                "22": "            # only store file data for Rust files",
                "23": "            if modified_file.filename not in files_data:",
                "24": "                files_data[modified_file.filename] = []",
                "25": "            if len(modified_file.filename.split(\".\")) == 2 and modified_file.filename.split(\".\")[1] == language_file_extension:",
                "26": "                file_data = {",
                "27": "                    \"commit\": commit.hash,",
                "28": "                    \"timestamp\": commit.committer_date.isoformat(),",
                "29": "                    \"author\": commit.author.name,",
                "30": "                    \"commit_message\": commit.msg,",
                "31": "                    \"additions\": modified_file.added_lines,",
                "32": "                    \"deletions\": modified_file.deleted_lines,",
                "33": "                    \"change_type\": modified_file.change_type.name,",
                "34": "                    \"diff\": modified_file.diff_parsed",
                "35": "                }",
                "36": "                diff_added = {}",
                "37": "                diff_deleted = {}",
                "38": "                diff_modified = {}",
                "39": "                following_multi_comment = False",
                "40": "                # For added diff ispect lines filter out comments",
                "41": "                for line in modified_file.diff_parsed[\"added\"]:",
                "42": "                    if line[1].find(single_comment_symbol) != -1 or following_multi_comment:",
                "43": "                        diff_added[line[0]] = line[1]",
                "44": "                    if multi_comments_enabled and line[1].find(multi_comment_symbols[0]) != -1:",
                "45": "                        diff_added[line[0]] = line[1]",
                "46": "                        following_multi_comment = True",
                "47": "                    if multi_comments_enabled and line[1].find(multi_comment_symbols[1]) != -1:",
                "48": "                        diff_added[line[0]] = line[1]",
                "49": "                        following_multi_comment = False",
                "50": "                file_data[\"comment_added_diff\"] = diff_added",
                "51": "                # For deleted diff ispect lines filter out comments",
                "52": "                for line in modified_file.diff_parsed[\"deleted\"]:",
                "53": "                    if line[1].find(single_comment_symbol) != -1 or following_multi_comment:",
                "54": "                        diff_deleted[line[0]] = line[1]",
                "55": "                        if line[0] in diff_added.keys():",
                "56": "                            diff_modified[line[0]] = line[1]",
                "57": "                    if multi_comments_enabled and line[1].find(multi_comment_symbols[0]) != -1:",
                "58": "                        diff_added[line[0]] = line[1]",
                "59": "                        following_multi_comment = True",
                "60": "                        if line[0] in diff_added.keys():",
                "61": "                            diff_modified[line[0]] = line[1]",
                "62": "                    if multi_comments_enabled and line[1].find(multi_comment_symbols[1]) != -1:",
                "63": "                        diff_added[line[0]] = line[1]",
                "64": "                        if line[0] in diff_added.keys():",
                "65": "                            diff_modified[line[0]] = line[1]",
                "66": "                file_data[\"comment_deleted_diff\"] = diff_deleted",
                "67": "                file_data[\"comment_modified_diff\"] = diff_modified",
                "68": "                # Generate keywords based on the commit message and type of changes",
                "69": "                # file_data[\"keywords\"] = extract_keywords(commit.msg, modified_file)",
                "70": "                # Extract type of commit from commit message",
                "71": "                # file_data[\"activity\"] = extract_activity(commit.msg)",
                "72": "                if len(diff_added) + len(diff_deleted) != 0:",
                "73": "                    files_data[modified_file.filename].append(file_data)",
                "74": "    return files_data",
                "75": "",
                "76": "def extract_keywords(commit_message, modified_file):",
                "77": "    # Determine basic keywords based on the commit message",
                "78": "    keywords = []",
                "79": "    if \"performance\" in commit_message.lower():",
                "80": "        keywords.append(\"performance\")",
                "81": "    if \"security\" in commit_message.lower():",
                "82": "        keywords.append(\"security\")",
                "83": "    if modified_file.added_lines > modified_file.deleted_lines:",
                "84": "        keywords.append(\"expansion\")",
                "85": "    else:",
                "86": "        keywords.append(\"optimization\")",
                "87": "    return keywords",
                "88": "",
                "89": "def extract_activity(commit_message):",
                "90": "    # Use commit message keywords to determine activity type",
                "91": "    activity = \"\"",
                "92": "    if \"bug\" in commit.msg.lower() or \"fix\" in commit.msg.lower():",
                "93": "        activity = \"Bug Fix\"",
                "94": "    elif \"feature\" in commit.msg.lower() or \"add\" in commit.msg.lower():",
                "95": "        activity = \"Feature Development\"",
                "96": "    elif \"refactor\" in commit.msg.lower():",
                "97": "        activity = \"Refactoring\"",
                "98": "    else:",
                "99": "        activity = \"Other\"",
                "100": "    return activity",
                "101": "",
                "102": "def pretty_diff(commits_data, type, single_comment_symbol, multi_comment_symbols=[]):",
                "103": "    following_multi_comment = False",
                "104": "    if len(multi_comment_symbols) >= 2:",
                "105": "        multi_comments_enabled = True",
                "106": "    else:",
                "107": "        multi_comments_enabled = False",
                "108": "    for file, commits in commits_data.items():",
                "109": "        if len(file) > 0:",
                "110": "            for commit in commits:",
                "111": "                diff_edited = []",
                "112": "                # Set current line for each analysis",
                "113": "                for i in range(len(commit[\"diff\"][type])):",
                "114": "                    curr_line = commit[\"diff\"][type][i][0]",
                "115": "                    curr_content = commit[\"diff\"][type][i][1]",
                "116": "                    # In case of a starting multiline comment start adding future lines without comment symbol ",
                "117": "                    if curr_content.find(multi_comment_symbols[0]) != -1:",
                "118": "                            following_multi_comment = True",
                "119": "                    # In case of comment add them to existing dict if they directly follow",
                "120": "                    if curr_content.find(single_comment_symbol) == 0 or curr_content.find(single_comment_symbol + \" \") != -1 or following_multi_comment:",
                "121": "                        if len(diff_edited) > 0:",
                "122": "                            if len(diff_edited[-1][\"line_numbers\"]) == 0 or curr_line == diff_edited[-1][\"line_numbers\"][-1] + 1:",
                "123": "                                diff_edited[-1][\"comments\"][curr_line] = curr_content",
                "124": "                        else:",
                "125": "                    # or create new one",
                "126": "                            diff_edited.append({",
                "127": "                                \"line_numbers\": [],",
                "128": "                                \"comments\": {curr_line: curr_content},",
                "129": "                                \"lines\": []})",
                "130": "                    # In case of no comment add lines to existing dict if line number directly follows",
                "131": "                    else:    ",
                "132": "                        if len(diff_edited) > 0:",
                "133": "                            if len(diff_edited[-1][\"line_numbers\"]) == 0 or curr_line == diff_edited[-1][\"line_numbers\"][-1] + 1:",
                "134": "                                diff_edited[-1][\"line_numbers\"].append(curr_line)",
                "135": "                                diff_edited[-1][\"lines\"].append(curr_content)",
                "136": "                            else:",
                "137": "                    # Or create new one",
                "138": "                                diff_edited.append({",
                "139": "                                    \"line_numbers\": [curr_line],",
                "140": "                                    \"comments\": {},",
                "141": "                                    \"lines\": [curr_content]})",
                "142": "                    # Disable multiline comments when symbol found",
                "143": "                    if multi_comments_enabled and curr_content.find(multi_comment_symbols[1]) != -1:",
                "144": "                        following_multi_comment = False",
                "145": "                commit[\"diff\"][type] = diff_edited",
                "146": "    return commits_data",
                "147": "",
                "148": "def analyze_diffs(data):",
                "149": "    analysis_results = []",
                "150": "",
                "151": "    for file, commits in data.items():",
                "152": "        # Store last modified timestamps for each line",
                "153": "        last_modified = {}",
                "154": "        for commit in commits:",
                "155": "            # print(\"Starting to analyse commit: \", commit[\"commit\"])",
                "156": "            commit_time = datetime.fromisoformat(commit[\"timestamp\"])",
                "157": "            # Track modified lines",
                "158": "            for block in commit[\"diff\"][\"added\"]:",
                "159": "                for line in block[\"line_numbers\"]:",
                "160": "                    line_number = line",
                "161": "                    last_modified[line_number] = commit_time",
                "162": "            # Compare with comments",
                "163": "            for line in commit[\"comment_added_diff\"]:",
                "164": "                comment_time = datetime.fromisoformat(commit[\"timestamp\"])",
                "165": "                last_modified_lines = list(last_modified.keys())",
                "166": "                if int(line) in last_modified_lines:",
                "167": "                    for block in commit[\"diff\"][\"added\"]:",
                "168": "                        if line in block[\"comments\"] and len(block[\"line_numbers\"]) == 0:",
                "169": "                            if(comment_time > last_modified[int(line)]):",
                "170": "                                analysis_results.append({",
                "171": "                                    \"file\": file,",
                "172": "                                    \"line\": int(line),",
                "173": "                                    \"comment\": commit[\"comment_added_diff\"][line],",
                "174": "                                    \"comment_time\": str(comment_time),",
                "175": "                                    \"last_code_change_time\": str(last_modified[int(line)])",
                "176": "                                })",
                "177": "    return analysis_results",
                "178": "",
                "179": "def save_to_json(commits_data, path):",
                "180": "    # Save the processed commit data to a JSON file",
                "181": "    with open(path, 'w') as json_file:",
                "182": "        json.dump(commits_data, json_file, indent=4)",
                "183": "    print(\"Data has been saved to\", path)",
                "184": "",
                "185": "def create_xes_log(data):",
                "186": "    # Create a new EventLog object",
                "187": "    log = EventLog()",
                "188": "",
                "189": "    # Iterate over each commit entry in the data",
                "190": "    for file, commits in data.items():",
                "191": "        # Create a trace for the file",
                "192": "        trace = Trace()",
                "193": "        trace.attributes[\"file\"] = file",
                "194": "",
                "195": "        for commit in commits:",
                "196": "            # Extract event attributes",
                "197": "            event = Event()",
                "198": "            event[\"timestamp\"] = commit.get(\"timestamp\")",
                "199": "            event[\"author\"] = commit.get(\"author\")",
                "200": "            event[\"change_type\"] = commit.get(\"change_type\")",
                "201": "            event[\"commit_message\"] = commit.get(\"commit_message\")",
                "202": "            event[\"additions\"] = commit.get(\"additions\")",
                "203": "            event[\"deletions\"] = commit.get(\"deletions\")",
                "204": "            event[\"diff\"] = commit.get(\"diff\")",
                "205": "            if commit.get(\"comment_added_diff\"):",
                "206": "                event[\"comment_change\"] = \"True\"",
                "207": "            else:",
                "208": "                event[\"comment_change\"] = \"False\"",
                "209": "",
                "210": "            # Add the event to the trace",
                "211": "            trace.append(event)",
                "212": "",
                "213": "        # Add the trace to the log",
                "214": "        log.append(trace)",
                "215": "",
                "216": "    return log",
                "217": "",
                "218": "def save_xes_log(log, filename):",
                "219": "    # Export the log to an XES file",
                "220": "    xes_exporter.apply(log, filename)",
                "221": "",
                "222": "def save_to_xes(log, path):",
                "223": "    # Create the XES log from the commit data",
                "224": "    xes_log = create_xes_log(log)",
                "225": "",
                "226": "    # Save the XES log to a file",
                "227": "    save_xes_log(xes_log, path)",
                "228": "    print(\"XES log has been saved to\", path)",
                "229": "",
                "230": "if __name__ == \"__main__\":",
                "231": "    repo_url = \"https://github.com/nodejs/node\"  # Example repository URL",
                "232": "    commits_data = analyze_commits(repo_url, \"js\", datetime(2015,2,1), datetime(2015,8,1), \"//\", [\"/*\", \"*/\"])",
                "233": "    save_to_json(commits_data, \"Data/commits_data.json\")",
                "234": "    # save_to_xes(commits_data, \"Data/commits_data.xes\")",
                "235": "    with open(\"Data/commits_data.json\", \"r\") as json_file:",
                "236": "       commits_data = json.load(json_file)",
                "237": "    analyzed_data = pretty_diff(commits_data, \"added\", \"//\", [\"/*\", \"*/\"])",
                "238": "    save_to_json(analyzed_data, \"Exports/analyzed_data.json\")",
                "239": "    analyzed_data = pretty_diff(commits_data, \"deleted\", \"//\", [\"/*\", \"*/\"])",
                "240": "    save_to_json(analyzed_data, \"Exports/analyzed_data.json\")",
                "241": "    ",
                "242": "    # Test case",
                "243": "    with open(\"Exports/analyzed_data.json\", \"r\") as json_file:",
                "244": "        data = json.load(json_file)",
                "245": "    analyzed_data = analyze_diffs(data)",
                "246": "",
                "247": "    save_to_json(analyzed_data, \"Exports/analysis_results.json\")",
                "248": ""
            },
            "comments": [
                {
                    "line": 10,
                    "comment": "# Traverse through the commits in the repository",
                    "char_position_in_line": 4,
                    "type": [
                        "block"
                    ]
                },
                {
                    "line": 11,
                    "comment": "# Only save commits, that contain at least one file of the format {language_file_extension}",
                    "char_position_in_line": 4,
                    "type": [
                        "block"
                    ]
                },
                {
                    "line": 20,
                    "comment": "# Analyze each file modified in the commit",
                    "char_position_in_line": 8,
                    "type": [
                        "normal"
                    ]
                },
                {
                    "line": 22,
                    "comment": "# only store file data for Rust files",
                    "char_position_in_line": 12,
                    "type": [
                        "normal"
                    ]
                },
                {
                    "line": 40,
                    "comment": "# For added diff ispect lines filter out comments",
                    "char_position_in_line": 16,
                    "type": [
                        "normal"
                    ]
                },
                {
                    "line": 51,
                    "comment": "# For deleted diff ispect lines filter out comments",
                    "char_position_in_line": 16,
                    "type": [
                        "normal"
                    ]
                },
                {
                    "line": 68,
                    "comment": "# Generate keywords based on the commit message and type of changes",
                    "char_position_in_line": 16,
                    "type": [
                        "block"
                    ]
                },
                {
                    "line": 69,
                    "comment": "# file_data[\"keywords\"] = extract_keywords(commit.msg, modified_file)",
                    "char_position_in_line": 16,
                    "type": [
                        "block"
                    ]
                },
                {
                    "line": 70,
                    "comment": "# Extract type of commit from commit message",
                    "char_position_in_line": 16,
                    "type": [
                        "block"
                    ]
                },
                {
                    "line": 71,
                    "comment": "# file_data[\"activity\"] = extract_activity(commit.msg)",
                    "char_position_in_line": 16,
                    "type": [
                        "block"
                    ]
                },
                {
                    "line": 77,
                    "comment": "# Determine basic keywords based on the commit message",
                    "char_position_in_line": 4,
                    "type": [
                        "normal"
                    ]
                },
                {
                    "line": 90,
                    "comment": "# Use commit message keywords to determine activity type",
                    "char_position_in_line": 4,
                    "type": [
                        "normal"
                    ]
                },
                {
                    "line": 112,
                    "comment": "# Set current line for each analysis",
                    "char_position_in_line": 16,
                    "type": [
                        "normal"
                    ]
                },
                {
                    "line": 116,
                    "comment": "# In case of a starting multiline comment start adding future lines without comment symbol ",
                    "char_position_in_line": 20,
                    "type": [
                        "normal"
                    ]
                },
                {
                    "line": 119,
                    "comment": "# In case of comment add them to existing dict if they directly follow",
                    "char_position_in_line": 20,
                    "type": [
                        "normal"
                    ]
                },
                {
                    "line": 125,
                    "comment": "# or create new one",
                    "char_position_in_line": 20,
                    "type": [
                        "normal"
                    ]
                },
                {
                    "line": 130,
                    "comment": "# In case of no comment add lines to existing dict if line number directly follows",
                    "char_position_in_line": 20,
                    "type": [
                        "normal"
                    ]
                },
                {
                    "line": 137,
                    "comment": "# Or create new one",
                    "char_position_in_line": 20,
                    "type": [
                        "normal"
                    ]
                },
                {
                    "line": 142,
                    "comment": "# Disable multiline comments when symbol found",
                    "char_position_in_line": 20,
                    "type": [
                        "normal"
                    ]
                },
                {
                    "line": 152,
                    "comment": "# Store last modified timestamps for each line",
                    "char_position_in_line": 8,
                    "type": [
                        "normal"
                    ]
                },
                {
                    "line": 155,
                    "comment": "# print(\"Starting to analyse commit: \", commit[\"commit\"])",
                    "char_position_in_line": 12,
                    "type": [
                        "commented-out"
                    ]
                },
                {
                    "line": 157,
                    "comment": "# Track modified lines",
                    "char_position_in_line": 12,
                    "type": [
                        "normal"
                    ]
                },
                {
                    "line": 162,
                    "comment": "# Compare with comments",
                    "char_position_in_line": 12,
                    "type": [
                        "normal"
                    ]
                },
                {
                    "line": 180,
                    "comment": "# Save the processed commit data to a JSON file",
                    "char_position_in_line": 4,
                    "type": [
                        "normal"
                    ]
                },
                {
                    "line": 186,
                    "comment": "# Create a new EventLog object",
                    "char_position_in_line": 4,
                    "type": [
                        "normal"
                    ]
                },
                {
                    "line": 189,
                    "comment": "# Iterate over each commit entry in the data",
                    "char_position_in_line": 4,
                    "type": [
                        "normal"
                    ]
                },
                {
                    "line": 191,
                    "comment": "# Create a trace for the file",
                    "char_position_in_line": 8,
                    "type": [
                        "normal"
                    ]
                },
                {
                    "line": 196,
                    "comment": "# Extract event attributes",
                    "char_position_in_line": 12,
                    "type": [
                        "normal"
                    ]
                },
                {
                    "line": 210,
                    "comment": "# Add the event to the trace",
                    "char_position_in_line": 12,
                    "type": [
                        "normal"
                    ]
                },
                {
                    "line": 213,
                    "comment": "# Add the trace to the log",
                    "char_position_in_line": 8,
                    "type": [
                        "normal"
                    ]
                },
                {
                    "line": 219,
                    "comment": "# Export the log to an XES file",
                    "char_position_in_line": 4,
                    "type": [
                        "normal"
                    ]
                },
                {
                    "line": 223,
                    "comment": "# Create the XES log from the commit data",
                    "char_position_in_line": 4,
                    "type": [
                        "normal"
                    ]
                },
                {
                    "line": 226,
                    "comment": "# Save the XES log to a file",
                    "char_position_in_line": 4,
                    "type": [
                        "normal"
                    ]
                },
                {
                    "line": 231,
                    "comment": "# Example repository URL",
                    "char_position_in_line": 49,
                    "type": [
                        "normal"
                    ]
                },
                {
                    "line": 234,
                    "comment": "# save_to_xes(commits_data, \"Data/commits_data.xes\")",
                    "char_position_in_line": 4,
                    "type": [
                        "commented-out"
                    ]
                },
                {
                    "line": 242,
                    "comment": "# Test case",
                    "char_position_in_line": 4,
                    "type": [
                        "normal"
                    ]
                }
            ]
        },
        {
            "commit": "2bfdaaeca6f4bc1b69403beb494198e8364ba925",
            "timestamp": "2024-11-19T18:23:46+01:00",
            "author": "alexander.schranner",
            "diff": {
                "added": {
                    "117": "                    if multi_comments_enabled and curr_content.find(multi_comment_symbols[0]) != -1:",
                    "123": "                                if len(diff_edited[-1][\"comments\"].keys()) > 0 and list(diff_edited[-1][\"comments\"].keys())[-1] + 1 == curr_line:",
                    "124": "                                    diff_edited[-1][\"comments\"][curr_line] = curr_content",
                    "125": "                                else:",
                    "126": "                                    diff_edited.append({",
                    "127": "                                        \"line_numbers\": [],",
                    "128": "                                        \"comments\": {curr_line: curr_content},",
                    "129": "                                        \"lines\": []})",
                    "237": "    repo_url = \"https://github.com/AlexS-1/Bachelor-Code\"  # Example repository URL",
                    "238": "    # commits_data = analyze_commits(repo_url, \"py\", datetime(2015,2,1), datetime.today(), \"#\")",
                    "239": "    # save_to_json(commits_data, \"Data/commits_data.json\")",
                    "243": "    analyzed_data = pretty_diff(commits_data, \"added\", \"#\")",
                    "245": "    analyzed_data = pretty_diff(commits_data, \"deleted\", \"#\")"
                },
                "deleted": {
                    "117": "                    if curr_content.find(multi_comment_symbols[0]) != -1:",
                    "123": "                                diff_edited[-1][\"comments\"][curr_line] = curr_content",
                    "231": "    repo_url = \"https://github.com/nodejs/node\"  # Example repository URL",
                    "232": "    commits_data = analyze_commits(repo_url, \"js\", datetime(2015,2,1), datetime(2015,8,1), \"//\", [\"/*\", \"*/\"])",
                    "233": "    save_to_json(commits_data, \"Data/commits_data.json\")",
                    "237": "    analyzed_data = pretty_diff(commits_data, \"added\", \"//\", [\"/*\", \"*/\"])",
                    "239": "    analyzed_data = pretty_diff(commits_data, \"deleted\", \"//\", [\"/*\", \"*/\"])"
                }
            },
            "source_code": {
                "1": "from pydriller import Repository",
                "2": "import json",
                "3": "import pm4py",
                "4": "from datetime import datetime",
                "5": "from pm4py.objects.log.obj import EventLog, Trace, Event",
                "6": "from pm4py.objects.log.exporter.xes import exporter as xes_exporter",
                "7": "",
                "8": "def analyze_commits(repo_url, language_file_extension, dt1, dt2, single_comment_symbol, multi_comment_symbols=[]):",
                "9": "    files_data = {}",
                "10": "    # Traverse through the commits in the repository",
                "11": "    # Only save commits, that contain at least one file of the format {language_file_extension}",
                "12": "    for commit in Repository(repo_url, ",
                "13": "    only_modifications_with_file_types=[f\".{language_file_extension}\"],",
                "14": "    since=dt1,",
                "15": "    to=dt2).traverse_commits():",
                "16": "        if len(multi_comment_symbols) >= 2:",
                "17": "            multi_comments_enabled = True",
                "18": "        else:",
                "19": "            multi_comments_enabled = False",
                "20": "        # Analyze each file modified in the commit",
                "21": "        for modified_file in commit.modified_files:",
                "22": "            # only store file data for Rust files",
                "23": "            if modified_file.filename not in files_data:",
                "24": "                files_data[modified_file.filename] = []",
                "25": "            if len(modified_file.filename.split(\".\")) == 2 and modified_file.filename.split(\".\")[1] == language_file_extension:",
                "26": "                file_data = {",
                "27": "                    \"commit\": commit.hash,",
                "28": "                    \"timestamp\": commit.committer_date.isoformat(),",
                "29": "                    \"author\": commit.author.name,",
                "30": "                    \"commit_message\": commit.msg,",
                "31": "                    \"additions\": modified_file.added_lines,",
                "32": "                    \"deletions\": modified_file.deleted_lines,",
                "33": "                    \"change_type\": modified_file.change_type.name,",
                "34": "                    \"diff\": modified_file.diff_parsed",
                "35": "                }",
                "36": "                diff_added = {}",
                "37": "                diff_deleted = {}",
                "38": "                diff_modified = {}",
                "39": "                following_multi_comment = False",
                "40": "                # For added diff ispect lines filter out comments",
                "41": "                for line in modified_file.diff_parsed[\"added\"]:",
                "42": "                    if line[1].find(single_comment_symbol) != -1 or following_multi_comment:",
                "43": "                        diff_added[line[0]] = line[1]",
                "44": "                    if multi_comments_enabled and line[1].find(multi_comment_symbols[0]) != -1:",
                "45": "                        diff_added[line[0]] = line[1]",
                "46": "                        following_multi_comment = True",
                "47": "                    if multi_comments_enabled and line[1].find(multi_comment_symbols[1]) != -1:",
                "48": "                        diff_added[line[0]] = line[1]",
                "49": "                        following_multi_comment = False",
                "50": "                file_data[\"comment_added_diff\"] = diff_added",
                "51": "                # For deleted diff ispect lines filter out comments",
                "52": "                for line in modified_file.diff_parsed[\"deleted\"]:",
                "53": "                    if line[1].find(single_comment_symbol) != -1 or following_multi_comment:",
                "54": "                        diff_deleted[line[0]] = line[1]",
                "55": "                        if line[0] in diff_added.keys():",
                "56": "                            diff_modified[line[0]] = line[1]",
                "57": "                    if multi_comments_enabled and line[1].find(multi_comment_symbols[0]) != -1:",
                "58": "                        diff_added[line[0]] = line[1]",
                "59": "                        following_multi_comment = True",
                "60": "                        if line[0] in diff_added.keys():",
                "61": "                            diff_modified[line[0]] = line[1]",
                "62": "                    if multi_comments_enabled and line[1].find(multi_comment_symbols[1]) != -1:",
                "63": "                        diff_added[line[0]] = line[1]",
                "64": "                        if line[0] in diff_added.keys():",
                "65": "                            diff_modified[line[0]] = line[1]",
                "66": "                file_data[\"comment_deleted_diff\"] = diff_deleted",
                "67": "                file_data[\"comment_modified_diff\"] = diff_modified",
                "68": "                # Generate keywords based on the commit message and type of changes",
                "69": "                # file_data[\"keywords\"] = extract_keywords(commit.msg, modified_file)",
                "70": "                # Extract type of commit from commit message",
                "71": "                # file_data[\"activity\"] = extract_activity(commit.msg)",
                "72": "                if len(diff_added) + len(diff_deleted) != 0:",
                "73": "                    files_data[modified_file.filename].append(file_data)",
                "74": "    return files_data",
                "75": "",
                "76": "def extract_keywords(commit_message, modified_file):",
                "77": "    # Determine basic keywords based on the commit message",
                "78": "    keywords = []",
                "79": "    if \"performance\" in commit_message.lower():",
                "80": "        keywords.append(\"performance\")",
                "81": "    if \"security\" in commit_message.lower():",
                "82": "        keywords.append(\"security\")",
                "83": "    if modified_file.added_lines > modified_file.deleted_lines:",
                "84": "        keywords.append(\"expansion\")",
                "85": "    else:",
                "86": "        keywords.append(\"optimization\")",
                "87": "    return keywords",
                "88": "",
                "89": "def extract_activity(commit_message):",
                "90": "    # Use commit message keywords to determine activity type",
                "91": "    activity = \"\"",
                "92": "    if \"bug\" in commit.msg.lower() or \"fix\" in commit.msg.lower():",
                "93": "        activity = \"Bug Fix\"",
                "94": "    elif \"feature\" in commit.msg.lower() or \"add\" in commit.msg.lower():",
                "95": "        activity = \"Feature Development\"",
                "96": "    elif \"refactor\" in commit.msg.lower():",
                "97": "        activity = \"Refactoring\"",
                "98": "    else:",
                "99": "        activity = \"Other\"",
                "100": "    return activity",
                "101": "",
                "102": "def pretty_diff(commits_data, type, single_comment_symbol, multi_comment_symbols=[]):",
                "103": "    following_multi_comment = False",
                "104": "    if len(multi_comment_symbols) >= 2:",
                "105": "        multi_comments_enabled = True",
                "106": "    else:",
                "107": "        multi_comments_enabled = False",
                "108": "    for file, commits in commits_data.items():",
                "109": "        if len(file) > 0:",
                "110": "            for commit in commits:",
                "111": "                diff_edited = []",
                "112": "                # Set current line for each analysis",
                "113": "                for i in range(len(commit[\"diff\"][type])):",
                "114": "                    curr_line = commit[\"diff\"][type][i][0]",
                "115": "                    curr_content = commit[\"diff\"][type][i][1]",
                "116": "                    # In case of a starting multiline comment start adding future lines without comment symbol ",
                "117": "                    if multi_comments_enabled and curr_content.find(multi_comment_symbols[0]) != -1:",
                "118": "                            following_multi_comment = True",
                "119": "                    # In case of comment add them to existing dict if they directly follow",
                "120": "                    if curr_content.find(single_comment_symbol) == 0 or curr_content.find(single_comment_symbol + \" \") != -1 or following_multi_comment:",
                "121": "                        if len(diff_edited) > 0:",
                "122": "                            if len(diff_edited[-1][\"line_numbers\"]) == 0 or curr_line == diff_edited[-1][\"line_numbers\"][-1] + 1:",
                "123": "                                if len(diff_edited[-1][\"comments\"].keys()) > 0 and list(diff_edited[-1][\"comments\"].keys())[-1] + 1 == curr_line:",
                "124": "                                    diff_edited[-1][\"comments\"][curr_line] = curr_content",
                "125": "                                else:",
                "126": "                                    diff_edited.append({",
                "127": "                                        \"line_numbers\": [],",
                "128": "                                        \"comments\": {curr_line: curr_content},",
                "129": "                                        \"lines\": []})",
                "130": "                        else:",
                "131": "                    # or create new one",
                "132": "                            diff_edited.append({",
                "133": "                                \"line_numbers\": [],",
                "134": "                                \"comments\": {curr_line: curr_content},",
                "135": "                                \"lines\": []})",
                "136": "                    # In case of no comment add lines to existing dict if line number directly follows",
                "137": "                    else:    ",
                "138": "                        if len(diff_edited) > 0:",
                "139": "                            if len(diff_edited[-1][\"line_numbers\"]) == 0 or curr_line == diff_edited[-1][\"line_numbers\"][-1] + 1:",
                "140": "                                diff_edited[-1][\"line_numbers\"].append(curr_line)",
                "141": "                                diff_edited[-1][\"lines\"].append(curr_content)",
                "142": "                            else:",
                "143": "                    # Or create new one",
                "144": "                                diff_edited.append({",
                "145": "                                    \"line_numbers\": [curr_line],",
                "146": "                                    \"comments\": {},",
                "147": "                                    \"lines\": [curr_content]})",
                "148": "                    # Disable multiline comments when symbol found",
                "149": "                    if multi_comments_enabled and curr_content.find(multi_comment_symbols[1]) != -1:",
                "150": "                        following_multi_comment = False",
                "151": "                commit[\"diff\"][type] = diff_edited",
                "152": "    return commits_data",
                "153": "",
                "154": "def analyze_diffs(data):",
                "155": "    analysis_results = []",
                "156": "",
                "157": "    for file, commits in data.items():",
                "158": "        # Store last modified timestamps for each line",
                "159": "        last_modified = {}",
                "160": "        for commit in commits:",
                "161": "            # print(\"Starting to analyse commit: \", commit[\"commit\"])",
                "162": "            commit_time = datetime.fromisoformat(commit[\"timestamp\"])",
                "163": "            # Track modified lines",
                "164": "            for block in commit[\"diff\"][\"added\"]:",
                "165": "                for line in block[\"line_numbers\"]:",
                "166": "                    line_number = line",
                "167": "                    last_modified[line_number] = commit_time",
                "168": "            # Compare with comments",
                "169": "            for line in commit[\"comment_added_diff\"]:",
                "170": "                comment_time = datetime.fromisoformat(commit[\"timestamp\"])",
                "171": "                last_modified_lines = list(last_modified.keys())",
                "172": "                if int(line) in last_modified_lines:",
                "173": "                    for block in commit[\"diff\"][\"added\"]:",
                "174": "                        if line in block[\"comments\"] and len(block[\"line_numbers\"]) == 0:",
                "175": "                            if(comment_time > last_modified[int(line)]):",
                "176": "                                analysis_results.append({",
                "177": "                                    \"file\": file,",
                "178": "                                    \"line\": int(line),",
                "179": "                                    \"comment\": commit[\"comment_added_diff\"][line],",
                "180": "                                    \"comment_time\": str(comment_time),",
                "181": "                                    \"last_code_change_time\": str(last_modified[int(line)])",
                "182": "                                })",
                "183": "    return analysis_results",
                "184": "",
                "185": "def save_to_json(commits_data, path):",
                "186": "    # Save the processed commit data to a JSON file",
                "187": "    with open(path, 'w') as json_file:",
                "188": "        json.dump(commits_data, json_file, indent=4)",
                "189": "    print(\"Data has been saved to\", path)",
                "190": "",
                "191": "def create_xes_log(data):",
                "192": "    # Create a new EventLog object",
                "193": "    log = EventLog()",
                "194": "",
                "195": "    # Iterate over each commit entry in the data",
                "196": "    for file, commits in data.items():",
                "197": "        # Create a trace for the file",
                "198": "        trace = Trace()",
                "199": "        trace.attributes[\"file\"] = file",
                "200": "",
                "201": "        for commit in commits:",
                "202": "            # Extract event attributes",
                "203": "            event = Event()",
                "204": "            event[\"timestamp\"] = commit.get(\"timestamp\")",
                "205": "            event[\"author\"] = commit.get(\"author\")",
                "206": "            event[\"change_type\"] = commit.get(\"change_type\")",
                "207": "            event[\"commit_message\"] = commit.get(\"commit_message\")",
                "208": "            event[\"additions\"] = commit.get(\"additions\")",
                "209": "            event[\"deletions\"] = commit.get(\"deletions\")",
                "210": "            event[\"diff\"] = commit.get(\"diff\")",
                "211": "            if commit.get(\"comment_added_diff\"):",
                "212": "                event[\"comment_change\"] = \"True\"",
                "213": "            else:",
                "214": "                event[\"comment_change\"] = \"False\"",
                "215": "",
                "216": "            # Add the event to the trace",
                "217": "            trace.append(event)",
                "218": "",
                "219": "        # Add the trace to the log",
                "220": "        log.append(trace)",
                "221": "",
                "222": "    return log",
                "223": "",
                "224": "def save_xes_log(log, filename):",
                "225": "    # Export the log to an XES file",
                "226": "    xes_exporter.apply(log, filename)",
                "227": "",
                "228": "def save_to_xes(log, path):",
                "229": "    # Create the XES log from the commit data",
                "230": "    xes_log = create_xes_log(log)",
                "231": "",
                "232": "    # Save the XES log to a file",
                "233": "    save_xes_log(xes_log, path)",
                "234": "    print(\"XES log has been saved to\", path)",
                "235": "",
                "236": "if __name__ == \"__main__\":",
                "237": "    repo_url = \"https://github.com/AlexS-1/Bachelor-Code\"  # Example repository URL",
                "238": "    # commits_data = analyze_commits(repo_url, \"py\", datetime(2015,2,1), datetime.today(), \"#\")",
                "239": "    # save_to_json(commits_data, \"Data/commits_data.json\")",
                "240": "    # save_to_xes(commits_data, \"Data/commits_data.xes\")",
                "241": "    with open(\"Data/commits_data.json\", \"r\") as json_file:",
                "242": "       commits_data = json.load(json_file)",
                "243": "    analyzed_data = pretty_diff(commits_data, \"added\", \"#\")",
                "244": "    save_to_json(analyzed_data, \"Exports/analyzed_data.json\")",
                "245": "    analyzed_data = pretty_diff(commits_data, \"deleted\", \"#\")",
                "246": "    save_to_json(analyzed_data, \"Exports/analyzed_data.json\")",
                "247": "    ",
                "248": "    # Test case",
                "249": "    with open(\"Exports/analyzed_data.json\", \"r\") as json_file:",
                "250": "        data = json.load(json_file)",
                "251": "    analyzed_data = analyze_diffs(data)",
                "252": "",
                "253": "    save_to_json(analyzed_data, \"Exports/analysis_results.json\")",
                "254": ""
            },
            "comments": [
                {
                    "line": 10,
                    "comment": "# Traverse through the commits in the repository",
                    "char_position_in_line": 4,
                    "type": [
                        "block"
                    ]
                },
                {
                    "line": 11,
                    "comment": "# Only save commits, that contain at least one file of the format {language_file_extension}",
                    "char_position_in_line": 4,
                    "type": [
                        "block"
                    ]
                },
                {
                    "line": 20,
                    "comment": "# Analyze each file modified in the commit",
                    "char_position_in_line": 8,
                    "type": [
                        "normal"
                    ]
                },
                {
                    "line": 22,
                    "comment": "# only store file data for Rust files",
                    "char_position_in_line": 12,
                    "type": [
                        "normal"
                    ]
                },
                {
                    "line": 40,
                    "comment": "# For added diff ispect lines filter out comments",
                    "char_position_in_line": 16,
                    "type": [
                        "normal"
                    ]
                },
                {
                    "line": 51,
                    "comment": "# For deleted diff ispect lines filter out comments",
                    "char_position_in_line": 16,
                    "type": [
                        "normal"
                    ]
                },
                {
                    "line": 68,
                    "comment": "# Generate keywords based on the commit message and type of changes",
                    "char_position_in_line": 16,
                    "type": [
                        "block"
                    ]
                },
                {
                    "line": 69,
                    "comment": "# file_data[\"keywords\"] = extract_keywords(commit.msg, modified_file)",
                    "char_position_in_line": 16,
                    "type": [
                        "block"
                    ]
                },
                {
                    "line": 70,
                    "comment": "# Extract type of commit from commit message",
                    "char_position_in_line": 16,
                    "type": [
                        "block"
                    ]
                },
                {
                    "line": 71,
                    "comment": "# file_data[\"activity\"] = extract_activity(commit.msg)",
                    "char_position_in_line": 16,
                    "type": [
                        "block"
                    ]
                },
                {
                    "line": 77,
                    "comment": "# Determine basic keywords based on the commit message",
                    "char_position_in_line": 4,
                    "type": [
                        "normal"
                    ]
                },
                {
                    "line": 90,
                    "comment": "# Use commit message keywords to determine activity type",
                    "char_position_in_line": 4,
                    "type": [
                        "normal"
                    ]
                },
                {
                    "line": 112,
                    "comment": "# Set current line for each analysis",
                    "char_position_in_line": 16,
                    "type": [
                        "normal"
                    ]
                },
                {
                    "line": 116,
                    "comment": "# In case of a starting multiline comment start adding future lines without comment symbol ",
                    "char_position_in_line": 20,
                    "type": [
                        "normal"
                    ]
                },
                {
                    "line": 119,
                    "comment": "# In case of comment add them to existing dict if they directly follow",
                    "char_position_in_line": 20,
                    "type": [
                        "normal"
                    ]
                },
                {
                    "line": 131,
                    "comment": "# or create new one",
                    "char_position_in_line": 20,
                    "type": [
                        "normal"
                    ]
                },
                {
                    "line": 136,
                    "comment": "# In case of no comment add lines to existing dict if line number directly follows",
                    "char_position_in_line": 20,
                    "type": [
                        "normal"
                    ]
                },
                {
                    "line": 143,
                    "comment": "# Or create new one",
                    "char_position_in_line": 20,
                    "type": [
                        "normal"
                    ]
                },
                {
                    "line": 148,
                    "comment": "# Disable multiline comments when symbol found",
                    "char_position_in_line": 20,
                    "type": [
                        "normal"
                    ]
                },
                {
                    "line": 158,
                    "comment": "# Store last modified timestamps for each line",
                    "char_position_in_line": 8,
                    "type": [
                        "normal"
                    ]
                },
                {
                    "line": 161,
                    "comment": "# print(\"Starting to analyse commit: \", commit[\"commit\"])",
                    "char_position_in_line": 12,
                    "type": [
                        "commented-out"
                    ]
                },
                {
                    "line": 163,
                    "comment": "# Track modified lines",
                    "char_position_in_line": 12,
                    "type": [
                        "normal"
                    ]
                },
                {
                    "line": 168,
                    "comment": "# Compare with comments",
                    "char_position_in_line": 12,
                    "type": [
                        "normal"
                    ]
                },
                {
                    "line": 186,
                    "comment": "# Save the processed commit data to a JSON file",
                    "char_position_in_line": 4,
                    "type": [
                        "normal"
                    ]
                },
                {
                    "line": 192,
                    "comment": "# Create a new EventLog object",
                    "char_position_in_line": 4,
                    "type": [
                        "normal"
                    ]
                },
                {
                    "line": 195,
                    "comment": "# Iterate over each commit entry in the data",
                    "char_position_in_line": 4,
                    "type": [
                        "normal"
                    ]
                },
                {
                    "line": 197,
                    "comment": "# Create a trace for the file",
                    "char_position_in_line": 8,
                    "type": [
                        "normal"
                    ]
                },
                {
                    "line": 202,
                    "comment": "# Extract event attributes",
                    "char_position_in_line": 12,
                    "type": [
                        "normal"
                    ]
                },
                {
                    "line": 216,
                    "comment": "# Add the event to the trace",
                    "char_position_in_line": 12,
                    "type": [
                        "normal"
                    ]
                },
                {
                    "line": 219,
                    "comment": "# Add the trace to the log",
                    "char_position_in_line": 8,
                    "type": [
                        "normal"
                    ]
                },
                {
                    "line": 225,
                    "comment": "# Export the log to an XES file",
                    "char_position_in_line": 4,
                    "type": [
                        "normal"
                    ]
                },
                {
                    "line": 229,
                    "comment": "# Create the XES log from the commit data",
                    "char_position_in_line": 4,
                    "type": [
                        "normal"
                    ]
                },
                {
                    "line": 232,
                    "comment": "# Save the XES log to a file",
                    "char_position_in_line": 4,
                    "type": [
                        "normal"
                    ]
                },
                {
                    "line": 237,
                    "comment": "# Example repository URL",
                    "char_position_in_line": 59,
                    "type": [
                        "normal"
                    ]
                },
                {
                    "line": 238,
                    "comment": "# commits_data = analyze_commits(repo_url, \"py\", datetime(2015,2,1), datetime.today(), \"#\")",
                    "char_position_in_line": 4,
                    "type": [
                        "commented-out",
                        "block"
                    ]
                },
                {
                    "line": 239,
                    "comment": "# save_to_json(commits_data, \"Data/commits_data.json\")",
                    "char_position_in_line": 4,
                    "type": [
                        "commented-out",
                        "block"
                    ]
                },
                {
                    "line": 240,
                    "comment": "# save_to_xes(commits_data, \"Data/commits_data.xes\")",
                    "char_position_in_line": 4,
                    "type": [
                        "commented-out",
                        "block"
                    ]
                },
                {
                    "line": 248,
                    "comment": "# Test case",
                    "char_position_in_line": 4,
                    "type": [
                        "normal"
                    ]
                }
            ]
        },
        {
            "commit": "96c6276dc1ea0aad50b96cf844406a8641fa1d37",
            "timestamp": "2024-11-21T12:33:35+01:00",
            "author": "alexander.schranner",
            "diff": {
                "added": {
                    "5": "from dateutil.relativedelta import relativedelta",
                    "238": "    repo_url = \"https://github.com/apache/accumulo.git\"  # Example repository URL",
                    "239": "    commits_data = analyze_commits(repo_url, \"java\", datetime.today() - relativedelta(years=1), datetime.today(), \"//\", [\"/*\", \"*/\"])",
                    "240": "    save_to_json(commits_data, \"Data/commits_data.json\")",
                    "242": "    with open(\"Data/commits_data.json\", \"r\") as json_file:",
                    "244": "    analyzed_data = pretty_diff(commits_data, \"added\", \"//\", [\"/*\", \"*/\"])"
                },
                "deleted": {
                    "237": "    repo_url = \"https://github.com/AlexS-1/Bachelor-Code\"  # Example repository URL",
                    "238": "    # commits_data = analyze_commits(repo_url, \"py\", datetime(2015,2,1), datetime.today(), \"#\")",
                    "239": "    # save_to_json(commits_data, \"Data/commits_data.json\")",
                    "241": "    with open(\"Data/commits_data.json\", \"r\") as json_file:",
                    "243": "    analyzed_data = pretty_diff(commits_data, \"added\", \"#\")"
                }
            },
            "source_code": {
                "1": "from pydriller import Repository",
                "2": "import json",
                "3": "import pm4py",
                "4": "from datetime import datetime",
                "5": "from dateutil.relativedelta import relativedelta",
                "6": "from pm4py.objects.log.obj import EventLog, Trace, Event",
                "7": "from pm4py.objects.log.exporter.xes import exporter as xes_exporter",
                "8": "",
                "9": "def analyze_commits(repo_url, language_file_extension, dt1, dt2, single_comment_symbol, multi_comment_symbols=[]):",
                "10": "    files_data = {}",
                "11": "    # Traverse through the commits in the repository",
                "12": "    # Only save commits, that contain at least one file of the format {language_file_extension}",
                "13": "    for commit in Repository(repo_url, ",
                "14": "    only_modifications_with_file_types=[f\".{language_file_extension}\"],",
                "15": "    since=dt1,",
                "16": "    to=dt2).traverse_commits():",
                "17": "        if len(multi_comment_symbols) >= 2:",
                "18": "            multi_comments_enabled = True",
                "19": "        else:",
                "20": "            multi_comments_enabled = False",
                "21": "        # Analyze each file modified in the commit",
                "22": "        for modified_file in commit.modified_files:",
                "23": "            # only store file data for Rust files",
                "24": "            if modified_file.filename not in files_data:",
                "25": "                files_data[modified_file.filename] = []",
                "26": "            if len(modified_file.filename.split(\".\")) == 2 and modified_file.filename.split(\".\")[1] == language_file_extension:",
                "27": "                file_data = {",
                "28": "                    \"commit\": commit.hash,",
                "29": "                    \"timestamp\": commit.committer_date.isoformat(),",
                "30": "                    \"author\": commit.author.name,",
                "31": "                    \"commit_message\": commit.msg,",
                "32": "                    \"additions\": modified_file.added_lines,",
                "33": "                    \"deletions\": modified_file.deleted_lines,",
                "34": "                    \"change_type\": modified_file.change_type.name,",
                "35": "                    \"diff\": modified_file.diff_parsed",
                "36": "                }",
                "37": "                diff_added = {}",
                "38": "                diff_deleted = {}",
                "39": "                diff_modified = {}",
                "40": "                following_multi_comment = False",
                "41": "                # For added diff ispect lines filter out comments",
                "42": "                for line in modified_file.diff_parsed[\"added\"]:",
                "43": "                    if line[1].find(single_comment_symbol) != -1 or following_multi_comment:",
                "44": "                        diff_added[line[0]] = line[1]",
                "45": "                    if multi_comments_enabled and line[1].find(multi_comment_symbols[0]) != -1:",
                "46": "                        diff_added[line[0]] = line[1]",
                "47": "                        following_multi_comment = True",
                "48": "                    if multi_comments_enabled and line[1].find(multi_comment_symbols[1]) != -1:",
                "49": "                        diff_added[line[0]] = line[1]",
                "50": "                        following_multi_comment = False",
                "51": "                file_data[\"comment_added_diff\"] = diff_added",
                "52": "                # For deleted diff ispect lines filter out comments",
                "53": "                for line in modified_file.diff_parsed[\"deleted\"]:",
                "54": "                    if line[1].find(single_comment_symbol) != -1 or following_multi_comment:",
                "55": "                        diff_deleted[line[0]] = line[1]",
                "56": "                        if line[0] in diff_added.keys():",
                "57": "                            diff_modified[line[0]] = line[1]",
                "58": "                    if multi_comments_enabled and line[1].find(multi_comment_symbols[0]) != -1:",
                "59": "                        diff_added[line[0]] = line[1]",
                "60": "                        following_multi_comment = True",
                "61": "                        if line[0] in diff_added.keys():",
                "62": "                            diff_modified[line[0]] = line[1]",
                "63": "                    if multi_comments_enabled and line[1].find(multi_comment_symbols[1]) != -1:",
                "64": "                        diff_added[line[0]] = line[1]",
                "65": "                        if line[0] in diff_added.keys():",
                "66": "                            diff_modified[line[0]] = line[1]",
                "67": "                file_data[\"comment_deleted_diff\"] = diff_deleted",
                "68": "                file_data[\"comment_modified_diff\"] = diff_modified",
                "69": "                # Generate keywords based on the commit message and type of changes",
                "70": "                # file_data[\"keywords\"] = extract_keywords(commit.msg, modified_file)",
                "71": "                # Extract type of commit from commit message",
                "72": "                # file_data[\"activity\"] = extract_activity(commit.msg)",
                "73": "                if len(diff_added) + len(diff_deleted) != 0:",
                "74": "                    files_data[modified_file.filename].append(file_data)",
                "75": "    return files_data",
                "76": "",
                "77": "def extract_keywords(commit_message, modified_file):",
                "78": "    # Determine basic keywords based on the commit message",
                "79": "    keywords = []",
                "80": "    if \"performance\" in commit_message.lower():",
                "81": "        keywords.append(\"performance\")",
                "82": "    if \"security\" in commit_message.lower():",
                "83": "        keywords.append(\"security\")",
                "84": "    if modified_file.added_lines > modified_file.deleted_lines:",
                "85": "        keywords.append(\"expansion\")",
                "86": "    else:",
                "87": "        keywords.append(\"optimization\")",
                "88": "    return keywords",
                "89": "",
                "90": "def extract_activity(commit_message):",
                "91": "    # Use commit message keywords to determine activity type",
                "92": "    activity = \"\"",
                "93": "    if \"bug\" in commit.msg.lower() or \"fix\" in commit.msg.lower():",
                "94": "        activity = \"Bug Fix\"",
                "95": "    elif \"feature\" in commit.msg.lower() or \"add\" in commit.msg.lower():",
                "96": "        activity = \"Feature Development\"",
                "97": "    elif \"refactor\" in commit.msg.lower():",
                "98": "        activity = \"Refactoring\"",
                "99": "    else:",
                "100": "        activity = \"Other\"",
                "101": "    return activity",
                "102": "",
                "103": "def pretty_diff(commits_data, type, single_comment_symbol, multi_comment_symbols=[]):",
                "104": "    following_multi_comment = False",
                "105": "    if len(multi_comment_symbols) >= 2:",
                "106": "        multi_comments_enabled = True",
                "107": "    else:",
                "108": "        multi_comments_enabled = False",
                "109": "    for file, commits in commits_data.items():",
                "110": "        if len(file) > 0:",
                "111": "            for commit in commits:",
                "112": "                diff_edited = []",
                "113": "                # Set current line for each analysis",
                "114": "                for i in range(len(commit[\"diff\"][type])):",
                "115": "                    curr_line = commit[\"diff\"][type][i][0]",
                "116": "                    curr_content = commit[\"diff\"][type][i][1]",
                "117": "                    # In case of a starting multiline comment start adding future lines without comment symbol ",
                "118": "                    if multi_comments_enabled and curr_content.find(multi_comment_symbols[0]) != -1:",
                "119": "                            following_multi_comment = True",
                "120": "                    # In case of comment add them to existing dict if they directly follow",
                "121": "                    if curr_content.find(single_comment_symbol) == 0 or curr_content.find(single_comment_symbol + \" \") != -1 or following_multi_comment:",
                "122": "                        if len(diff_edited) > 0:",
                "123": "                            if len(diff_edited[-1][\"line_numbers\"]) == 0 or curr_line == diff_edited[-1][\"line_numbers\"][-1] + 1:",
                "124": "                                if len(diff_edited[-1][\"comments\"].keys()) > 0 and list(diff_edited[-1][\"comments\"].keys())[-1] + 1 == curr_line:",
                "125": "                                    diff_edited[-1][\"comments\"][curr_line] = curr_content",
                "126": "                                else:",
                "127": "                                    diff_edited.append({",
                "128": "                                        \"line_numbers\": [],",
                "129": "                                        \"comments\": {curr_line: curr_content},",
                "130": "                                        \"lines\": []})",
                "131": "                        else:",
                "132": "                    # or create new one",
                "133": "                            diff_edited.append({",
                "134": "                                \"line_numbers\": [],",
                "135": "                                \"comments\": {curr_line: curr_content},",
                "136": "                                \"lines\": []})",
                "137": "                    # In case of no comment add lines to existing dict if line number directly follows",
                "138": "                    else:    ",
                "139": "                        if len(diff_edited) > 0:",
                "140": "                            if len(diff_edited[-1][\"line_numbers\"]) == 0 or curr_line == diff_edited[-1][\"line_numbers\"][-1] + 1:",
                "141": "                                diff_edited[-1][\"line_numbers\"].append(curr_line)",
                "142": "                                diff_edited[-1][\"lines\"].append(curr_content)",
                "143": "                            else:",
                "144": "                    # Or create new one",
                "145": "                                diff_edited.append({",
                "146": "                                    \"line_numbers\": [curr_line],",
                "147": "                                    \"comments\": {},",
                "148": "                                    \"lines\": [curr_content]})",
                "149": "                    # Disable multiline comments when symbol found",
                "150": "                    if multi_comments_enabled and curr_content.find(multi_comment_symbols[1]) != -1:",
                "151": "                        following_multi_comment = False",
                "152": "                commit[\"diff\"][type] = diff_edited",
                "153": "    return commits_data",
                "154": "",
                "155": "def analyze_diffs(data):",
                "156": "    analysis_results = []",
                "157": "",
                "158": "    for file, commits in data.items():",
                "159": "        # Store last modified timestamps for each line",
                "160": "        last_modified = {}",
                "161": "        for commit in commits:",
                "162": "            # print(\"Starting to analyse commit: \", commit[\"commit\"])",
                "163": "            commit_time = datetime.fromisoformat(commit[\"timestamp\"])",
                "164": "            # Track modified lines",
                "165": "            for block in commit[\"diff\"][\"added\"]:",
                "166": "                for line in block[\"line_numbers\"]:",
                "167": "                    line_number = line",
                "168": "                    last_modified[line_number] = commit_time",
                "169": "            # Compare with comments",
                "170": "            for line in commit[\"comment_added_diff\"]:",
                "171": "                comment_time = datetime.fromisoformat(commit[\"timestamp\"])",
                "172": "                last_modified_lines = list(last_modified.keys())",
                "173": "                if int(line) in last_modified_lines:",
                "174": "                    for block in commit[\"diff\"][\"added\"]:",
                "175": "                        if line in block[\"comments\"] and len(block[\"line_numbers\"]) == 0:",
                "176": "                            if(comment_time > last_modified[int(line)]):",
                "177": "                                analysis_results.append({",
                "178": "                                    \"file\": file,",
                "179": "                                    \"line\": int(line),",
                "180": "                                    \"comment\": commit[\"comment_added_diff\"][line],",
                "181": "                                    \"comment_time\": str(comment_time),",
                "182": "                                    \"last_code_change_time\": str(last_modified[int(line)])",
                "183": "                                })",
                "184": "    return analysis_results",
                "185": "",
                "186": "def save_to_json(commits_data, path):",
                "187": "    # Save the processed commit data to a JSON file",
                "188": "    with open(path, 'w') as json_file:",
                "189": "        json.dump(commits_data, json_file, indent=4)",
                "190": "    print(\"Data has been saved to\", path)",
                "191": "",
                "192": "def create_xes_log(data):",
                "193": "    # Create a new EventLog object",
                "194": "    log = EventLog()",
                "195": "",
                "196": "    # Iterate over each commit entry in the data",
                "197": "    for file, commits in data.items():",
                "198": "        # Create a trace for the file",
                "199": "        trace = Trace()",
                "200": "        trace.attributes[\"file\"] = file",
                "201": "",
                "202": "        for commit in commits:",
                "203": "            # Extract event attributes",
                "204": "            event = Event()",
                "205": "            event[\"timestamp\"] = commit.get(\"timestamp\")",
                "206": "            event[\"author\"] = commit.get(\"author\")",
                "207": "            event[\"change_type\"] = commit.get(\"change_type\")",
                "208": "            event[\"commit_message\"] = commit.get(\"commit_message\")",
                "209": "            event[\"additions\"] = commit.get(\"additions\")",
                "210": "            event[\"deletions\"] = commit.get(\"deletions\")",
                "211": "            event[\"diff\"] = commit.get(\"diff\")",
                "212": "            if commit.get(\"comment_added_diff\"):",
                "213": "                event[\"comment_change\"] = \"True\"",
                "214": "            else:",
                "215": "                event[\"comment_change\"] = \"False\"",
                "216": "",
                "217": "            # Add the event to the trace",
                "218": "            trace.append(event)",
                "219": "",
                "220": "        # Add the trace to the log",
                "221": "        log.append(trace)",
                "222": "",
                "223": "    return log",
                "224": "",
                "225": "def save_xes_log(log, filename):",
                "226": "    # Export the log to an XES file",
                "227": "    xes_exporter.apply(log, filename)",
                "228": "",
                "229": "def save_to_xes(log, path):",
                "230": "    # Create the XES log from the commit data",
                "231": "    xes_log = create_xes_log(log)",
                "232": "",
                "233": "    # Save the XES log to a file",
                "234": "    save_xes_log(xes_log, path)",
                "235": "    print(\"XES log has been saved to\", path)",
                "236": "",
                "237": "if __name__ == \"__main__\":",
                "238": "    repo_url = \"https://github.com/apache/accumulo.git\"  # Example repository URL",
                "239": "    commits_data = analyze_commits(repo_url, \"java\", datetime.today() - relativedelta(years=1), datetime.today(), \"//\", [\"/*\", \"*/\"])",
                "240": "    save_to_json(commits_data, \"Data/commits_data.json\")",
                "241": "    # save_to_xes(commits_data, \"Data/commits_data.xes\")",
                "242": "    with open(\"Data/commits_data.json\", \"r\") as json_file: ",
                "243": "       commits_data = json.load(json_file)",
                "244": "    analyzed_data = pretty_diff(commits_data, \"added\", \"//\", [\"/*\", \"*/\"])",
                "245": "    save_to_json(analyzed_data, \"Exports/analyzed_data.json\")",
                "246": "    analyzed_data = pretty_diff(commits_data, \"deleted\", \"#\")",
                "247": "    save_to_json(analyzed_data, \"Exports/analyzed_data.json\")",
                "248": "    ",
                "249": "    # Test case",
                "250": "    with open(\"Exports/analyzed_data.json\", \"r\") as json_file:",
                "251": "        data = json.load(json_file)",
                "252": "    analyzed_data = analyze_diffs(data)",
                "253": "",
                "254": "    save_to_json(analyzed_data, \"Exports/analysis_results.json\")",
                "255": ""
            },
            "comments": [
                {
                    "line": 11,
                    "comment": "# Traverse through the commits in the repository",
                    "char_position_in_line": 4,
                    "type": [
                        "block"
                    ]
                },
                {
                    "line": 12,
                    "comment": "# Only save commits, that contain at least one file of the format {language_file_extension}",
                    "char_position_in_line": 4,
                    "type": [
                        "block"
                    ]
                },
                {
                    "line": 21,
                    "comment": "# Analyze each file modified in the commit",
                    "char_position_in_line": 8,
                    "type": [
                        "normal"
                    ]
                },
                {
                    "line": 23,
                    "comment": "# only store file data for Rust files",
                    "char_position_in_line": 12,
                    "type": [
                        "normal"
                    ]
                },
                {
                    "line": 41,
                    "comment": "# For added diff ispect lines filter out comments",
                    "char_position_in_line": 16,
                    "type": [
                        "normal"
                    ]
                },
                {
                    "line": 52,
                    "comment": "# For deleted diff ispect lines filter out comments",
                    "char_position_in_line": 16,
                    "type": [
                        "normal"
                    ]
                },
                {
                    "line": 69,
                    "comment": "# Generate keywords based on the commit message and type of changes",
                    "char_position_in_line": 16,
                    "type": [
                        "block"
                    ]
                },
                {
                    "line": 70,
                    "comment": "# file_data[\"keywords\"] = extract_keywords(commit.msg, modified_file)",
                    "char_position_in_line": 16,
                    "type": [
                        "block"
                    ]
                },
                {
                    "line": 71,
                    "comment": "# Extract type of commit from commit message",
                    "char_position_in_line": 16,
                    "type": [
                        "block"
                    ]
                },
                {
                    "line": 72,
                    "comment": "# file_data[\"activity\"] = extract_activity(commit.msg)",
                    "char_position_in_line": 16,
                    "type": [
                        "block"
                    ]
                },
                {
                    "line": 78,
                    "comment": "# Determine basic keywords based on the commit message",
                    "char_position_in_line": 4,
                    "type": [
                        "normal"
                    ]
                },
                {
                    "line": 91,
                    "comment": "# Use commit message keywords to determine activity type",
                    "char_position_in_line": 4,
                    "type": [
                        "normal"
                    ]
                },
                {
                    "line": 113,
                    "comment": "# Set current line for each analysis",
                    "char_position_in_line": 16,
                    "type": [
                        "normal"
                    ]
                },
                {
                    "line": 117,
                    "comment": "# In case of a starting multiline comment start adding future lines without comment symbol ",
                    "char_position_in_line": 20,
                    "type": [
                        "normal"
                    ]
                },
                {
                    "line": 120,
                    "comment": "# In case of comment add them to existing dict if they directly follow",
                    "char_position_in_line": 20,
                    "type": [
                        "normal"
                    ]
                },
                {
                    "line": 132,
                    "comment": "# or create new one",
                    "char_position_in_line": 20,
                    "type": [
                        "normal"
                    ]
                },
                {
                    "line": 137,
                    "comment": "# In case of no comment add lines to existing dict if line number directly follows",
                    "char_position_in_line": 20,
                    "type": [
                        "normal"
                    ]
                },
                {
                    "line": 144,
                    "comment": "# Or create new one",
                    "char_position_in_line": 20,
                    "type": [
                        "normal"
                    ]
                },
                {
                    "line": 149,
                    "comment": "# Disable multiline comments when symbol found",
                    "char_position_in_line": 20,
                    "type": [
                        "normal"
                    ]
                },
                {
                    "line": 159,
                    "comment": "# Store last modified timestamps for each line",
                    "char_position_in_line": 8,
                    "type": [
                        "normal"
                    ]
                },
                {
                    "line": 162,
                    "comment": "# print(\"Starting to analyse commit: \", commit[\"commit\"])",
                    "char_position_in_line": 12,
                    "type": [
                        "commented-out"
                    ]
                },
                {
                    "line": 164,
                    "comment": "# Track modified lines",
                    "char_position_in_line": 12,
                    "type": [
                        "normal"
                    ]
                },
                {
                    "line": 169,
                    "comment": "# Compare with comments",
                    "char_position_in_line": 12,
                    "type": [
                        "normal"
                    ]
                },
                {
                    "line": 187,
                    "comment": "# Save the processed commit data to a JSON file",
                    "char_position_in_line": 4,
                    "type": [
                        "normal"
                    ]
                },
                {
                    "line": 193,
                    "comment": "# Create a new EventLog object",
                    "char_position_in_line": 4,
                    "type": [
                        "normal"
                    ]
                },
                {
                    "line": 196,
                    "comment": "# Iterate over each commit entry in the data",
                    "char_position_in_line": 4,
                    "type": [
                        "normal"
                    ]
                },
                {
                    "line": 198,
                    "comment": "# Create a trace for the file",
                    "char_position_in_line": 8,
                    "type": [
                        "normal"
                    ]
                },
                {
                    "line": 203,
                    "comment": "# Extract event attributes",
                    "char_position_in_line": 12,
                    "type": [
                        "normal"
                    ]
                },
                {
                    "line": 217,
                    "comment": "# Add the event to the trace",
                    "char_position_in_line": 12,
                    "type": [
                        "normal"
                    ]
                },
                {
                    "line": 220,
                    "comment": "# Add the trace to the log",
                    "char_position_in_line": 8,
                    "type": [
                        "normal"
                    ]
                },
                {
                    "line": 226,
                    "comment": "# Export the log to an XES file",
                    "char_position_in_line": 4,
                    "type": [
                        "normal"
                    ]
                },
                {
                    "line": 230,
                    "comment": "# Create the XES log from the commit data",
                    "char_position_in_line": 4,
                    "type": [
                        "normal"
                    ]
                },
                {
                    "line": 233,
                    "comment": "# Save the XES log to a file",
                    "char_position_in_line": 4,
                    "type": [
                        "normal"
                    ]
                },
                {
                    "line": 238,
                    "comment": "# Example repository URL",
                    "char_position_in_line": 57,
                    "type": [
                        "normal"
                    ]
                },
                {
                    "line": 241,
                    "comment": "# save_to_xes(commits_data, \"Data/commits_data.xes\")",
                    "char_position_in_line": 4,
                    "type": [
                        "commented-out"
                    ]
                },
                {
                    "line": 249,
                    "comment": "# Test case",
                    "char_position_in_line": 4,
                    "type": [
                        "normal"
                    ]
                }
            ]
        }
    ],
    "null": [
        {
            "commit": "1061293a43b0788f9db921ae6fc61734ccdf1b8d",
            "timestamp": "2024-11-16T19:39:45+01:00",
            "author": "alexander.schranner",
            "diff": {
                "added": {},
                "deleted": {
                    "1": "",
                    "2": "import matplotlib.pyplot as plt",
                    "3": "# from pydriller import Repository",
                    "4": "# import numpy as np",
                    "5": "# import requests",
                    "6": "# import json",
                    "7": "# from flask import Response",
                    "8": "#",
                    "9": "# commits_monthly = list()",
                    "10": "# curr_month = 0",
                    "11": "# curr_year = 0",
                    "12": "# i = 0",
                    "13": "# month_del = 0",
                    "14": "# month_add = 0",
                    "15": "# month_commits = 0",
                    "16": "# filecounter = 0",
                    "17": "#",
                    "18": "#",
                    "19": "# for commit in Repository('https://github.com/dani-garcia/vaultwarden').traverse_commits():",
                    "20": "# #for commit in Repository('~/Developer/Ecogenium/Atava').traverse_commits():",
                    "21": "#     if (commit.committer_date.month != curr_month):",
                    "22": "#         label = str(curr_month) + \"-\" + str(curr_year)",
                    "23": "#         commits_monthly.append([label, filecounter, month_commits, month_add, month_del])",
                    "24": "#         curr_month = commit.committer_date.month",
                    "25": "#         curr_year = commit.committer_date.year",
                    "26": "#         i += 1",
                    "27": "#         filecounter = 0",
                    "28": "#         month_del = 0",
                    "29": "#         month_add = 0",
                    "30": "#         month_commits = 0",
                    "31": "#",
                    "32": "#     month_commits += 1",
                    "33": "#     for file in commit.modified_files:",
                    "34": "#         month_add += file.added_lines",
                    "35": "#         month_del += file.deleted_lines",
                    "36": "#         filecounter += 1",
                    "37": "#",
                    "38": "# commits = []",
                    "39": "# additions = []",
                    "40": "# deletions = []",
                    "41": "# files = []",
                    "42": "# loc = []",
                    "43": "# issues = []",
                    "44": "#",
                    "45": "# for i in range(0, len(commits_monthly)):",
                    "46": "#     commits.append(commits_monthly[i][1])",
                    "47": "#     files.append(commits_monthly[i][2])",
                    "48": "#     additions.append(commits_monthly[i][3])",
                    "49": "#     deletions.append(commits_monthly[i][4])",
                    "50": "#     if (i == 0):",
                    "51": "#         loc.append(additions[i] - deletions[i])",
                    "52": "#     else:",
                    "53": "#         loc.append(loc[i-1] + additions[i] - deletions[i])",
                    "54": "#",
                    "55": "# api_url = \"https://api.github.com/repos/dani-garcia/vaultwarden/issues\"",
                    "56": "# for i in range (1, 50):",
                    "57": "#     params = {",
                    "58": "#         \"state\": \"all\",",
                    "59": "#         \"direction\": \"asc\",",
                    "60": "#         \"per_page\": 100,",
                    "61": "#         \"page\" : i",
                    "62": "#     }",
                    "63": "#     response = requests.get(api_url, params=params)",
                    "64": "#     issues_response = response.json()",
                    "65": "#     for issue in issues_response:",
                    "66": "#         issue_data = []",
                    "67": "#         issue_data.append(issue['number'])",
                    "68": "#         issue_data.append(issue['created_at'])",
                    "69": "#         issue_data.append(issue['closed_at'])",
                    "70": "#         issues.append(issue_data)",
                    "71": "#",
                    "72": "# m = 0",
                    "73": "# issues_monthly = [0]",
                    "74": "# for i in range(len(issues)):",
                    "75": "#     if (issues[i][1][2:5] != issues[i+1][1][2:5]):",
                    "76": "#         m += 1",
                    "77": "#         issues_monthly.append(0)",
                    "78": "#     issues_monthly[m] += 1",
                    "79": "#",
                    "80": "# fig, ax1 = plt.subplots()",
                    "81": "# ax1.plot(additions, color='g', label='Additions')",
                    "82": "# ax1.plot(deletions, color='r', label='Deletions')",
                    "83": "# ax1.plot(loc, color='0', label='LOC')",
                    "84": "# ax1.tick_params(axis='y', labelcolor='0.5')",
                    "85": "#",
                    "86": "# ax2 = ax1.twinx()",
                    "87": "# ax2.plot(commits, color='y', label='Commits')",
                    "88": "# ax2.plot(files, color='b', label='Files')",
                    "89": "# ax2.plot(issues_monthly, color='0.3', label='Created Issues')",
                    "90": "# ax2.tick_params(axis='y', labelcolor='0.8')",
                    "91": "#",
                    "92": "# fig.tight_layout()",
                    "93": "# ax1.legend()",
                    "94": "# ax2.legend()",
                    "95": "# plt.savefig(\"GitVisualisation.pdf\", format=\"pdf\")",
                    "96": "# plt.show()",
                    "97": "#",
                    "98": "import matplotlib.pyplot as plt",
                    "99": "from pydriller import Repository",
                    "100": "import requests",
                    "101": "",
                    "102": "def analyze_commit_data(repo_url):",
                    "103": "    # Initialize tracking variables",
                    "104": "    monthly_commit_data = []",
                    "105": "    current_month = 0",
                    "106": "    current_year = 0",
                    "107": "    monthly_additions = 0",
                    "108": "    monthly_deletions = 0",
                    "109": "    monthly_commit_count = 0",
                    "110": "    modified_file_count = 0",
                    "111": "",
                    "112": "    # Traverse through all commits in the repository",
                    "113": "    for commit in Repository(repo_url).traverse_commits():",
                    "114": "        # If we encounter a new month, save the previous month's data",
                    "115": "        if commit.committer_date.month != current_month:",
                    "116": "            label = f\"{current_month}-{current_year}\"",
                    "117": "            monthly_commit_data.append([label, monthly_commit_count, modified_file_count, monthly_additions, monthly_deletions])",
                    "118": "            # Reset counters for the new month",
                    "119": "            current_month = commit.committer_date.month",
                    "120": "            current_year = commit.committer_date.year",
                    "121": "            monthly_additions = 0",
                    "122": "            monthly_deletions = 0",
                    "123": "            monthly_commit_count = 0",
                    "124": "            modified_file_count = 0",
                    "125": "",
                    "126": "        # Update current month's data",
                    "127": "        monthly_commit_count += 1",
                    "128": "        for file in commit.modified_files:",
                    "129": "            monthly_additions += file.added_lines",
                    "130": "            monthly_deletions += file.deleted_lines",
                    "131": "            modified_file_count += 1",
                    "132": "",
                    "133": "    return monthly_commit_data",
                    "134": "",
                    "135": "def analyze_issues_data(repo_url, max_pages):",
                    "136": "    issues_data = []",
                    "137": "    api_url = f\"https://api.github.com/repos/{repo_url}/issues\"",
                    "138": "",
                    "139": "    # Fetch issues data from GitHub using pagination",
                    "140": "    for page in range(1, max_pages + 1):",
                    "141": "        params = {",
                    "142": "            \"state\": \"all\",",
                    "143": "            \"direction\": \"asc\",",
                    "144": "            \"per_page\": 100,",
                    "145": "            \"page\": page",
                    "146": "        }",
                    "147": "        response = requests.get(api_url, params=params)",
                    "148": "        if response.status_code != 200:",
                    "149": "            print(f\"Failed to fetch issues data. Status code: {response.status_code}\")",
                    "150": "            break",
                    "151": "",
                    "152": "        issues_response = response.json()",
                    "153": "        # Stop if no more issues are returned",
                    "154": "        if not issues_response:",
                    "155": "            break",
                    "156": "",
                    "157": "        # Extract necessary issue data",
                    "158": "        for issue in issues_response:",
                    "159": "            issues_data.append([issue['number'], issue['created_at'], issue['closed_at']])",
                    "160": "",
                    "161": "    return issues_data",
                    "162": "",
                    "163": "def count_issues_monthly(issues_data):",
                    "164": "    issues_per_month = [0,0,0,0]",
                    "165": "    current_month = issues_data[0][1][5:7] if issues_data else None",
                    "166": "    monthly_issue_count = 0",
                    "167": "",
                    "168": "    # Count issues per month",
                    "169": "    for i in range(len(issues_data) - 1):",
                    "170": "        issue_month = issues_data[i][1][5:7]",
                    "171": "        next_issue_month = issues_data[i + 1][1][5:7]",
                    "172": "        if issue_month != next_issue_month:",
                    "173": "            issues_per_month.append(monthly_issue_count)",
                    "174": "            print(\"mic: \", monthly_issue_count, issues_data[i][1][2:7])",
                    "175": "            monthly_issue_count = 0",
                    "176": "            current_month = next_issue_month",
                    "177": "        monthly_issue_count += 1",
                    "178": "",
                    "179": "    # Add the last month's count",
                    "180": "    issues_per_month.append(monthly_issue_count)",
                    "181": "    return issues_per_month",
                    "182": "",
                    "183": "def calculate_loc(monthly_commit_data):",
                    "184": "    # Calculate lines of code (LOC) changes over time",
                    "185": "    loc_over_time = []",
                    "186": "    total_loc = 0",
                    "187": "",
                    "188": "    for month_data in monthly_commit_data:",
                    "189": "        additions = month_data[3]",
                    "190": "        deletions = month_data[4]",
                    "191": "        total_loc += additions - deletions",
                    "192": "        loc_over_time.append(total_loc)",
                    "193": "",
                    "194": "    return loc_over_time",
                    "195": "",
                    "196": "def plot_data(monthly_commit_data, loc_over_time, issues_per_month):",
                    "197": "    # Extract data for plotting",
                    "198": "    monthly_labels = [data[0] for data in monthly_commit_data]",
                    "199": "    monthly_commits = [data[1] for data in monthly_commit_data]",
                    "200": "    modified_files = [data[2] for data in monthly_commit_data]",
                    "201": "    monthly_additions = [data[3] for data in monthly_commit_data]",
                    "202": "    monthly_deletions = [data[4] for data in monthly_commit_data]",
                    "203": "",
                    "204": "    for i in range(len(monthly_labels)):",
                    "205": "        label = monthly_labels[i].split('-')",
                    "206": "        if (len(label[0]) == 1):",
                    "207": "            label[0] = \"0\" + label[0]",
                    "208": "        label[1] = label[1][2:]",
                    "209": "        monthly_labels[i] = label[1] + \"-\" + label[0]",
                    "210": "",
                    "211": "    # Plotting",
                    "212": "    fig, ax1 = plt.subplots()",
                    "213": "",
                    "214": "    # Plot lines for additions, deletions, and LOC",
                    "215": "    ax1.plot(monthly_labels, monthly_additions, color='g', label='Additions')",
                    "216": "    ax1.plot(monthly_labels, monthly_deletions, color='r', label='Deletions')",
                    "217": "    ax1.plot(monthly_labels, loc_over_time, color='k', label='LOC')",
                    "218": "    ax1.tick_params(axis='y', labelcolor='black')",
                    "219": "",
                    "220": "    # Secondary Y-axis for commits, modified files, and issues",
                    "221": "    ax2 = ax1.twinx()",
                    "222": "    ax2.plot(monthly_labels, monthly_commits, color='y', label='Commits')",
                    "223": "    ax2.plot(monthly_labels, modified_files, color='b', label='Modified Files')",
                    "224": "    ax2.plot(monthly_labels[:len(issues_per_month)], issues_per_month, color='grey', label='Created Issues')",
                    "225": "    ax2.tick_params(axis='y', labelcolor='grey')",
                    "226": "",
                    "227": "    # Finalize and show the plot",
                    "228": "    fig.tight_layout()",
                    "229": "    ax1.legend(loc='upper left')",
                    "230": "    ax2.legend(loc='upper right')",
                    "231": "    plt.xticks(rotation=45)",
                    "232": "    plt.title(\"Repository Analysis\")",
                    "233": "    plt.savefig(\"GitVisualisation.pdf\", format=\"pdf\")",
                    "234": "    plt.show()",
                    "235": "",
                    "236": "def main():",
                    "237": "    repo_url = 'dani-garcia/vaultwarden'",
                    "238": "    monthly_commit_data = analyze_commit_data(f'https://github.com/{repo_url}')",
                    "239": "    issues_data = analyze_issues_data(repo_url, 50)",
                    "240": "    issues_per_month = count_issues_monthly(issues_data)",
                    "241": "    loc_over_time = calculate_loc(monthly_commit_data)",
                    "242": "    plot_data(monthly_commit_data, loc_over_time, issues_per_month)",
                    "243": "",
                    "244": "if __name__ == \"__main__\":",
                    "245": "    main()"
                }
            },
            "source_code": {},
            "comments": {}
        }
    ],
    "build/__init__.py": [
        {
            "commit": "4210b185c8b05071e5e33b726c1d537cba181c6c",
            "timestamp": "2024-11-21T21:30:11+01:00",
            "author": "alexander.schranner",
            "diff": {
                "added": {},
                "deleted": {}
            },
            "source_code": {},
            "comments": []
        }
    ],
    "build/comment_lister.py": [
        {
            "commit": "4210b185c8b05071e5e33b726c1d537cba181c6c",
            "timestamp": "2024-11-21T21:30:11+01:00",
            "author": "alexander.schranner",
            "diff": {
                "added": {
                    "1": "import subprocess",
                    "2": "import json",
                    "3": "import os",
                    "4": "import shutil",
                    "5": "from datetime import datetime, timezone",
                    "6": "",
                    "7": "def run_comment_lister(repo_path, jar_path, tag=\"-target=HEAD\"):",
                    "8": "    try:",
                    "9": "        result = subprocess.run(",
                    "10": "            ['java', '-jar', jar_path, repo_path, tag],",
                    "11": "            stdout=subprocess.PIPE,",
                    "12": "            stderr=subprocess.PIPE,",
                    "13": "            text=True,",
                    "14": "            check=True",
                    "15": "        )",
                    "16": "        return result.stdout",
                    "17": "    except subprocess.CalledProcessError as e:",
                    "18": "        print(f\"Error running CommentLister: {e.stderr}\")",
                    "19": "        return None",
                    "20": "",
                    "21": "def filter_comments_by_time(commit_data, start_time, end_time):",
                    "22": "    filtered_comments = []",
                    "23": "    commit_time = datetime.fromisoformat(commit_data[\"CommitTime\"])",
                    "24": "    if start_time <= end_time:",
                    "25": "        for filename, contents in commit_data[\"Files\"].items():",
                    "26": "            i = 0",
                    "27": "            error = False",
                    "28": "            while not error:",
                    "29": "                try:",
                    "30": "                    comment_data = {",
                    "31": "                        \"line\": contents[str(i)][\"Line\"],",
                    "32": "                        \"comment\": contents[str(i)][\"Text\"]",
                    "33": "                    }",
                    "34": "                except KeyError as e:",
                    "35": "                    error = True",
                    "36": "                if not error:",
                    "37": "                    filtered_comments.append(comment_data)",
                    "38": "                i += 1",
                    "39": "    return filtered_comments"
                },
                "deleted": {}
            },
            "source_code": {
                "1": "import subprocess",
                "2": "import json",
                "3": "import os",
                "4": "import shutil",
                "5": "from datetime import datetime, timezone",
                "6": "",
                "7": "def run_comment_lister(repo_path, jar_path, tag=\"-target=HEAD\"):",
                "8": "    try:",
                "9": "        result = subprocess.run(",
                "10": "            ['java', '-jar', jar_path, repo_path, tag],",
                "11": "            stdout=subprocess.PIPE,",
                "12": "            stderr=subprocess.PIPE,",
                "13": "            text=True,",
                "14": "            check=True",
                "15": "        )",
                "16": "        return result.stdout",
                "17": "    except subprocess.CalledProcessError as e:",
                "18": "        print(f\"Error running CommentLister: {e.stderr}\")",
                "19": "        return None",
                "20": "",
                "21": "def filter_comments_by_time(commit_data, start_time, end_time):",
                "22": "    filtered_comments = []",
                "23": "    commit_time = datetime.fromisoformat(commit_data[\"CommitTime\"])",
                "24": "    if start_time <= end_time:",
                "25": "        for filename, contents in commit_data[\"Files\"].items():",
                "26": "            i = 0",
                "27": "            error = False",
                "28": "            while not error:",
                "29": "                try:",
                "30": "                    comment_data = {",
                "31": "                        \"line\": contents[str(i)][\"Line\"],",
                "32": "                        \"comment\": contents[str(i)][\"Text\"]",
                "33": "                    }",
                "34": "                except KeyError as e:",
                "35": "                    error = True",
                "36": "                if not error:",
                "37": "                    filtered_comments.append(comment_data)",
                "38": "                i += 1",
                "39": "    return filtered_comments"
            },
            "comments": []
        },
        {
            "commit": "1013981f1c0e06dc3def7ea036c0da903f2e38f6",
            "timestamp": "2024-11-22T16:53:22+01:00",
            "author": "alexander.schranner",
            "diff": {
                "added": {
                    "23": "    commit_time = datetime.fromisoformat(commit_data[\"CommitTime\"]).replace(tzinfo=None)",
                    "24": "    if start_time <= commit_time <= end_time:",
                    "32": "                        \"comment\": contents[str(i)][\"Text\"],"
                },
                "deleted": {
                    "23": "    commit_time = datetime.fromisoformat(commit_data[\"CommitTime\"])",
                    "24": "    if start_time <= end_time:",
                    "32": "                        \"comment\": contents[str(i)][\"Text\"]"
                }
            },
            "source_code": {
                "1": "import subprocess",
                "2": "import json",
                "3": "import os",
                "4": "import shutil",
                "5": "from datetime import datetime, timezone",
                "6": "",
                "7": "def run_comment_lister(repo_path, jar_path, tag=\"-target=HEAD\"):",
                "8": "    try:",
                "9": "        result = subprocess.run(",
                "10": "            ['java', '-jar', jar_path, repo_path, tag],",
                "11": "            stdout=subprocess.PIPE,",
                "12": "            stderr=subprocess.PIPE,",
                "13": "            text=True,",
                "14": "            check=True",
                "15": "        )",
                "16": "        return result.stdout",
                "17": "    except subprocess.CalledProcessError as e:",
                "18": "        print(f\"Error running CommentLister: {e.stderr}\")",
                "19": "        return None",
                "20": "",
                "21": "def filter_comments_by_time(commit_data, start_time, end_time):",
                "22": "    filtered_comments = []",
                "23": "    commit_time = datetime.fromisoformat(commit_data[\"CommitTime\"]).replace(tzinfo=None)",
                "24": "    if start_time <= commit_time <= end_time:",
                "25": "        for filename, contents in commit_data[\"Files\"].items():",
                "26": "            i = 0",
                "27": "            error = False",
                "28": "            while not error:",
                "29": "                try:",
                "30": "                    comment_data = {",
                "31": "                        \"line\": contents[str(i)][\"Line\"],",
                "32": "                        \"comment\": contents[str(i)][\"Text\"],",
                "33": "                    }",
                "34": "                except KeyError as e:",
                "35": "                    error = True",
                "36": "                if not error:",
                "37": "                    filtered_comments.append(comment_data)",
                "38": "                i += 1",
                "39": "    return filtered_comments"
            },
            "comments": []
        },
        {
            "commit": "b10794953bf2b307859821a8354d3429d710e31b",
            "timestamp": "2024-11-24T18:37:49+01:00",
            "author": "alexander.schranner",
            "diff": {
                "added": {
                    "27": "            i = 0",
                    "30": "                    split_comment_lines = contents[str(i)][\"Text\"].split(\"\\n\")",
                    "31": "                    # print(\"Have to split comments:\", split_comment_lines)",
                    "32": "                    if len(split_comment_lines) > 1:",
                    "33": "                        initial_line = contents[str(i)][\"Line\"]",
                    "34": "                        j = 0",
                    "35": "                        for comment in split_comment_lines:",
                    "36": "                            # Assumption: All multi line comments are formatted in one block, i.e. vertically in one collum",
                    "37": "                            comment_data = {",
                    "38": "                                \"line\": initial_line + j,",
                    "39": "                                \"comment\": comment,",
                    "40": "                                \"char_position_in_line\": contents[str(i)][\"CharPositionInLine\"]",
                    "41": "                            }",
                    "42": "                            j += 1",
                    "43": "                            filtered_comments.append(comment_data)",
                    "44": "                    else:",
                    "45": "                        comment_data = {",
                    "46": "                            \"line\": contents[str(i)][\"Line\"],",
                    "47": "                            \"comment\": contents[str(i)][\"Text\"],",
                    "48": "                            \"char_position_in_line\": contents[str(i)][\"CharPositionInLine\"]",
                    "49": "                        }",
                    "50": "                        filtered_comments.append(comment_data)",
                    "54": "                    i += 1",
                    "55": "    else:",
                    "56": "        print(\"Comments not in specified date range\")",
                    "57": "    return commit_data[\"ObjectId\"], filtered_comments"
                },
                "deleted": {
                    "26": "            i = 0",
                    "30": "                    comment_data = {",
                    "31": "                        \"line\": contents[str(i)][\"Line\"],",
                    "32": "                        \"comment\": contents[str(i)][\"Text\"],",
                    "33": "                    }",
                    "37": "                    filtered_comments.append(comment_data)",
                    "38": "                i += 1",
                    "39": "    return filtered_comments"
                }
            },
            "source_code": {
                "1": "import subprocess",
                "2": "import json",
                "3": "import os",
                "4": "import shutil",
                "5": "from datetime import datetime, timezone",
                "6": "",
                "7": "def run_comment_lister(repo_path, jar_path, tag=\"-target=HEAD\"):",
                "8": "    try:",
                "9": "        result = subprocess.run(",
                "10": "            ['java', '-jar', jar_path, repo_path, tag],",
                "11": "            stdout=subprocess.PIPE,",
                "12": "            stderr=subprocess.PIPE,",
                "13": "            text=True,",
                "14": "            check=True",
                "15": "        )",
                "16": "        return result.stdout",
                "17": "    except subprocess.CalledProcessError as e:",
                "18": "        print(f\"Error running CommentLister: {e.stderr}\")",
                "19": "        return None",
                "20": "",
                "21": "def filter_comments_by_time(commit_data, start_time, end_time):",
                "22": "    filtered_comments = []",
                "23": "    commit_time = datetime.fromisoformat(commit_data[\"CommitTime\"]).replace(tzinfo=None)",
                "24": "    if start_time <= commit_time <= end_time:",
                "25": "        for filename, contents in commit_data[\"Files\"].items():",
                "26": "            error = False",
                "27": "            i = 0",
                "28": "            while not error:",
                "29": "                try:",
                "30": "                    split_comment_lines = contents[str(i)][\"Text\"].split(\"\\n\")",
                "31": "                    # print(\"Have to split comments:\", split_comment_lines)",
                "32": "                    if len(split_comment_lines) > 1:",
                "33": "                        initial_line = contents[str(i)][\"Line\"]",
                "34": "                        j = 0",
                "35": "                        for comment in split_comment_lines:",
                "36": "                            # Assumption: All multi line comments are formatted in one block, i.e. vertically in one collum",
                "37": "                            comment_data = {",
                "38": "                                \"line\": initial_line + j,",
                "39": "                                \"comment\": comment,",
                "40": "                                \"char_position_in_line\": contents[str(i)][\"CharPositionInLine\"]",
                "41": "                            }",
                "42": "                            j += 1",
                "43": "                            filtered_comments.append(comment_data)",
                "44": "                    else:",
                "45": "                        comment_data = {",
                "46": "                            \"line\": contents[str(i)][\"Line\"],",
                "47": "                            \"comment\": contents[str(i)][\"Text\"],",
                "48": "                            \"char_position_in_line\": contents[str(i)][\"CharPositionInLine\"]",
                "49": "                        }",
                "50": "                        filtered_comments.append(comment_data)",
                "51": "                except KeyError as e:",
                "52": "                    error = True",
                "53": "                if not error:",
                "54": "                    i += 1",
                "55": "    else:",
                "56": "        print(\"Comments not in specified date range\")",
                "57": "    return commit_data[\"ObjectId\"], filtered_comments"
            },
            "comments": [
                {
                    "line": 31,
                    "comment": "# print(\"Have to split comments:\", split_comment_lines)",
                    "char_position_in_line": 20,
                    "type": [
                        "commented-out"
                    ]
                },
                {
                    "line": 36,
                    "comment": "# Assumption: All multi line comments are formatted in one block, i.e. vertically in one collum",
                    "char_position_in_line": 28,
                    "type": [
                        "normal"
                    ]
                }
            ]
        },
        {
            "commit": "202468fb39d473251ab81eb3037227cf7af47344",
            "timestamp": "2024-12-04T00:35:22+01:00",
            "author": "alexander.schranner",
            "diff": {
                "added": {
                    "22": "    filtered_comments = {}",
                    "26": "            filtered_comments[filename] = []",
                    "44": "                            filtered_comments[filename].append(comment_data)",
                    "51": "                        filtered_comments[filename].append(comment_data)"
                },
                "deleted": {
                    "22": "    filtered_comments = []",
                    "43": "                            filtered_comments.append(comment_data)",
                    "50": "                        filtered_comments.append(comment_data)"
                }
            },
            "source_code": {
                "1": "import subprocess",
                "2": "import json",
                "3": "import os",
                "4": "import shutil",
                "5": "from datetime import datetime, timezone",
                "6": "",
                "7": "def run_comment_lister(repo_path, jar_path, tag=\"-target=HEAD\"):",
                "8": "    try:",
                "9": "        result = subprocess.run(",
                "10": "            ['java', '-jar', jar_path, repo_path, tag],",
                "11": "            stdout=subprocess.PIPE,",
                "12": "            stderr=subprocess.PIPE,",
                "13": "            text=True,",
                "14": "            check=True",
                "15": "        )",
                "16": "        return result.stdout",
                "17": "    except subprocess.CalledProcessError as e:",
                "18": "        print(f\"Error running CommentLister: {e.stderr}\")",
                "19": "        return None",
                "20": "",
                "21": "def filter_comments_by_time(commit_data, start_time, end_time):",
                "22": "    filtered_comments = {}",
                "23": "    commit_time = datetime.fromisoformat(commit_data[\"CommitTime\"]).replace(tzinfo=None)",
                "24": "    if start_time <= commit_time <= end_time:",
                "25": "        for filename, contents in commit_data[\"Files\"].items():",
                "26": "            filtered_comments[filename] = []",
                "27": "            error = False",
                "28": "            i = 0",
                "29": "            while not error:",
                "30": "                try:",
                "31": "                    split_comment_lines = contents[str(i)][\"Text\"].split(\"\\n\")",
                "32": "                    # print(\"Have to split comments:\", split_comment_lines)",
                "33": "                    if len(split_comment_lines) > 1:",
                "34": "                        initial_line = contents[str(i)][\"Line\"]",
                "35": "                        j = 0",
                "36": "                        for comment in split_comment_lines:",
                "37": "                            # Assumption: All multi line comments are formatted in one block, i.e. vertically in one collum",
                "38": "                            comment_data = {",
                "39": "                                \"line\": initial_line + j,",
                "40": "                                \"comment\": comment,",
                "41": "                                \"char_position_in_line\": contents[str(i)][\"CharPositionInLine\"]",
                "42": "                            }",
                "43": "                            j += 1",
                "44": "                            filtered_comments[filename].append(comment_data)",
                "45": "                    else:",
                "46": "                        comment_data = {",
                "47": "                            \"line\": contents[str(i)][\"Line\"],",
                "48": "                            \"comment\": contents[str(i)][\"Text\"],",
                "49": "                            \"char_position_in_line\": contents[str(i)][\"CharPositionInLine\"]",
                "50": "                        }",
                "51": "                        filtered_comments[filename].append(comment_data)",
                "52": "                except KeyError as e:",
                "53": "                    error = True",
                "54": "                if not error:",
                "55": "                    i += 1",
                "56": "    else:",
                "57": "        print(\"Comments not in specified date range\")",
                "58": "    return commit_data[\"ObjectId\"], filtered_comments"
            },
            "comments": [
                {
                    "line": 32,
                    "comment": "# print(\"Have to split comments:\", split_comment_lines)",
                    "char_position_in_line": 20,
                    "type": [
                        "commented-out"
                    ]
                },
                {
                    "line": 37,
                    "comment": "# Assumption: All multi line comments are formatted in one block, i.e. vertically in one collum",
                    "char_position_in_line": 28,
                    "type": [
                        "normal"
                    ]
                }
            ]
        },
        {
            "commit": "76c0e457bfe78effef6b104d334eed6a7fa3e4e4",
            "timestamp": "2024-12-04T23:01:11+01:00",
            "author": "alexander.schranner",
            "diff": {
                "added": {
                    "6": "from build.analysis import classify_comments",
                    "32": "                    type = classify_comments(contents[str(i)][\"Text\"])",
                    "43": "                                \"char_position_in_line\": contents[str(i)][\"CharPositionInLine\"],",
                    "44": "                                \"type\": type",
                    "52": "                            \"char_position_in_line\": contents[str(i)][\"CharPositionInLine\"],",
                    "53": "                            \"type\": type"
                },
                "deleted": {
                    "41": "                                \"char_position_in_line\": contents[str(i)][\"CharPositionInLine\"]",
                    "49": "                            \"char_position_in_line\": contents[str(i)][\"CharPositionInLine\"]"
                }
            },
            "source_code": {
                "1": "import subprocess",
                "2": "import json",
                "3": "import os",
                "4": "import shutil",
                "5": "from datetime import datetime, timezone",
                "6": "from build.analysis import classify_comments",
                "7": "",
                "8": "def run_comment_lister(repo_path, jar_path, tag=\"-target=HEAD\"):",
                "9": "    try:",
                "10": "        result = subprocess.run(",
                "11": "            ['java', '-jar', jar_path, repo_path, tag],",
                "12": "            stdout=subprocess.PIPE,",
                "13": "            stderr=subprocess.PIPE,",
                "14": "            text=True,",
                "15": "            check=True",
                "16": "        )",
                "17": "        return result.stdout",
                "18": "    except subprocess.CalledProcessError as e:",
                "19": "        print(f\"Error running CommentLister: {e.stderr}\")",
                "20": "        return None",
                "21": "",
                "22": "def filter_comments_by_time(commit_data, start_time, end_time):",
                "23": "    filtered_comments = {}",
                "24": "    commit_time = datetime.fromisoformat(commit_data[\"CommitTime\"]).replace(tzinfo=None)",
                "25": "    if start_time <= commit_time <= end_time:",
                "26": "        for filename, contents in commit_data[\"Files\"].items():",
                "27": "            filtered_comments[filename] = []",
                "28": "            error = False",
                "29": "            i = 0",
                "30": "            while not error:",
                "31": "                try:",
                "32": "                    type = classify_comments(contents[str(i)][\"Text\"])",
                "33": "                    split_comment_lines = contents[str(i)][\"Text\"].split(\"\\n\")",
                "34": "                    # print(\"Have to split comments:\", split_comment_lines)",
                "35": "                    if len(split_comment_lines) > 1:",
                "36": "                        initial_line = contents[str(i)][\"Line\"]",
                "37": "                        j = 0",
                "38": "                        for comment in split_comment_lines:",
                "39": "                            # Assumption: All multi line comments are formatted in one block, i.e. vertically in one collum",
                "40": "                            comment_data = {",
                "41": "                                \"line\": initial_line + j,",
                "42": "                                \"comment\": comment,",
                "43": "                                \"char_position_in_line\": contents[str(i)][\"CharPositionInLine\"],",
                "44": "                                \"type\": type",
                "45": "                            }",
                "46": "                            j += 1",
                "47": "                            filtered_comments[filename].append(comment_data)",
                "48": "                    else:",
                "49": "                        comment_data = {",
                "50": "                            \"line\": contents[str(i)][\"Line\"],",
                "51": "                            \"comment\": contents[str(i)][\"Text\"],",
                "52": "                            \"char_position_in_line\": contents[str(i)][\"CharPositionInLine\"],",
                "53": "                            \"type\": type",
                "54": "                        }",
                "55": "                        filtered_comments[filename].append(comment_data)",
                "56": "                except KeyError as e:",
                "57": "                    error = True",
                "58": "                if not error:",
                "59": "                    i += 1",
                "60": "    else:",
                "61": "        print(\"Comments not in specified date range\")",
                "62": "    return commit_data[\"ObjectId\"], filtered_comments"
            },
            "comments": [
                {
                    "line": 34,
                    "comment": "# print(\"Have to split comments:\", split_comment_lines)",
                    "char_position_in_line": 20,
                    "type": [
                        "commented-out"
                    ]
                },
                {
                    "line": 39,
                    "comment": "# Assumption: All multi line comments are formatted in one block, i.e. vertically in one collum",
                    "char_position_in_line": 28,
                    "type": [
                        "normal"
                    ]
                }
            ]
        }
    ],
    "build/pydriller.py": [
        {
            "commit": "4210b185c8b05071e5e33b726c1d537cba181c6c",
            "timestamp": "2024-11-21T21:30:11+01:00",
            "author": "alexander.schranner",
            "diff": {
                "added": {
                    "9": "def get_commits_data(repo_path, from_date, to_date):",
                    "10": "    files_data = {}",
                    "11": "    for commit in Repository(repo_path, since=from_date, to=to_date).traverse_commits():",
                    "12": "        for file in commit.modified_files:",
                    "13": "            if file not in files_data:",
                    "14": "                files_data[file.filename] = []",
                    "15": "            file_data = {",
                    "16": "                \"commit\": commit.hash,",
                    "17": "                \"timestamp\": commit.committer_date.isoformat(),",
                    "18": "                \"author\": commit.author.name,",
                    "19": "                \"diff\": file.diff_parsed",
                    "20": "            }",
                    "21": "            if len(file.diff_parsed) != 0:",
                    "22": "                files_data[file.filename].append(file_data)",
                    "23": "    return files_data",
                    "24": ""
                },
                "deleted": {}
            },
            "source_code": {
                "1": "from pydriller import Repository",
                "2": "import json",
                "3": "import pm4py",
                "4": "from datetime import datetime",
                "5": "from dateutil.relativedelta import relativedelta",
                "6": "from pm4py.objects.log.obj import EventLog, Trace, Event",
                "7": "from pm4py.objects.log.exporter.xes import exporter as xes_exporter",
                "8": "",
                "9": "def get_commits_data(repo_path, from_date, to_date):",
                "10": "    files_data = {}",
                "11": "    for commit in Repository(repo_path, since=from_date, to=to_date).traverse_commits():",
                "12": "        for file in commit.modified_files:",
                "13": "            if file not in files_data:",
                "14": "                files_data[file.filename] = []",
                "15": "            file_data = {",
                "16": "                \"commit\": commit.hash,",
                "17": "                \"timestamp\": commit.committer_date.isoformat(),",
                "18": "                \"author\": commit.author.name,",
                "19": "                \"diff\": file.diff_parsed",
                "20": "            }",
                "21": "            if len(file.diff_parsed) != 0:",
                "22": "                files_data[file.filename].append(file_data)",
                "23": "    return files_data",
                "24": "",
                "25": "def analyze_commits(repo_url, language_file_extension, dt1, dt2, single_comment_symbol, multi_comment_symbols=[]):",
                "26": "    files_data = {}",
                "27": "    # Traverse through the commits in the repository",
                "28": "    # Only save commits, that contain at least one file of the format {language_file_extension}",
                "29": "    for commit in Repository(repo_url, ",
                "30": "    only_modifications_with_file_types=[f\".{language_file_extension}\"],",
                "31": "    since=dt1,",
                "32": "    to=dt2).traverse_commits():",
                "33": "        if len(multi_comment_symbols) >= 2:",
                "34": "            multi_comments_enabled = True",
                "35": "        else:",
                "36": "            multi_comments_enabled = False",
                "37": "        # Analyze each file modified in the commit",
                "38": "        for modified_file in commit.modified_files:",
                "39": "            # only store file data for Rust files",
                "40": "            if modified_file.filename not in files_data:",
                "41": "                files_data[modified_file.filename] = []",
                "42": "            if len(modified_file.filename.split(\".\")) == 2 and modified_file.filename.split(\".\")[1] == language_file_extension:",
                "43": "                file_data = {",
                "44": "                    \"commit\": commit.hash,",
                "45": "                    \"timestamp\": commit.committer_date.isoformat(),",
                "46": "                    \"author\": commit.author.name,",
                "47": "                    \"commit_message\": commit.msg,",
                "48": "                    \"additions\": modified_file.added_lines,",
                "49": "                    \"deletions\": modified_file.deleted_lines,",
                "50": "                    \"change_type\": modified_file.change_type.name,",
                "51": "                    \"diff\": modified_file.diff_parsed",
                "52": "                }",
                "53": "                diff_added = {}",
                "54": "                diff_deleted = {}",
                "55": "                diff_modified = {}",
                "56": "                following_multi_comment = False",
                "57": "                # For added diff ispect lines filter out comments",
                "58": "                for line in modified_file.diff_parsed[\"added\"]:",
                "59": "                    if line[1].find(single_comment_symbol) != -1 or following_multi_comment:",
                "60": "                        diff_added[line[0]] = line[1]",
                "61": "                    if multi_comments_enabled and line[1].find(multi_comment_symbols[0]) != -1:",
                "62": "                        diff_added[line[0]] = line[1]",
                "63": "                        following_multi_comment = True",
                "64": "                    if multi_comments_enabled and line[1].find(multi_comment_symbols[1]) != -1:",
                "65": "                        diff_added[line[0]] = line[1]",
                "66": "                        following_multi_comment = False",
                "67": "                file_data[\"comment_added_diff\"] = diff_added",
                "68": "                # For deleted diff ispect lines filter out comments",
                "69": "                for line in modified_file.diff_parsed[\"deleted\"]:",
                "70": "                    if line[1].find(single_comment_symbol) != -1 or following_multi_comment:",
                "71": "                        diff_deleted[line[0]] = line[1]",
                "72": "                        if line[0] in diff_added.keys():",
                "73": "                            diff_modified[line[0]] = line[1]",
                "74": "                    if multi_comments_enabled and line[1].find(multi_comment_symbols[0]) != -1:",
                "75": "                        diff_added[line[0]] = line[1]",
                "76": "                        following_multi_comment = True",
                "77": "                        if line[0] in diff_added.keys():",
                "78": "                            diff_modified[line[0]] = line[1]",
                "79": "                    if multi_comments_enabled and line[1].find(multi_comment_symbols[1]) != -1:",
                "80": "                        diff_added[line[0]] = line[1]",
                "81": "                        if line[0] in diff_added.keys():",
                "82": "                            diff_modified[line[0]] = line[1]",
                "83": "                file_data[\"comment_deleted_diff\"] = diff_deleted",
                "84": "                file_data[\"comment_modified_diff\"] = diff_modified",
                "85": "                # Generate keywords based on the commit message and type of changes",
                "86": "                # file_data[\"keywords\"] = extract_keywords(commit.msg, modified_file)",
                "87": "                # Extract type of commit from commit message",
                "88": "                # file_data[\"activity\"] = extract_activity(commit.msg)",
                "89": "                if len(diff_added) + len(diff_deleted) != 0:",
                "90": "                    files_data[modified_file.filename].append(file_data)",
                "91": "    return files_data",
                "92": "",
                "93": "def extract_keywords(commit_message, modified_file):",
                "94": "    # Determine basic keywords based on the commit message",
                "95": "    keywords = []",
                "96": "    if \"performance\" in commit_message.lower():",
                "97": "        keywords.append(\"performance\")",
                "98": "    if \"security\" in commit_message.lower():",
                "99": "        keywords.append(\"security\")",
                "100": "    if modified_file.added_lines > modified_file.deleted_lines:",
                "101": "        keywords.append(\"expansion\")",
                "102": "    else:",
                "103": "        keywords.append(\"optimization\")",
                "104": "    return keywords",
                "105": "",
                "106": "def extract_activity(commit_message):",
                "107": "    # Use commit message keywords to determine activity type",
                "108": "    activity = \"\"",
                "109": "    if \"bug\" in commit.msg.lower() or \"fix\" in commit.msg.lower():",
                "110": "        activity = \"Bug Fix\"",
                "111": "    elif \"feature\" in commit.msg.lower() or \"add\" in commit.msg.lower():",
                "112": "        activity = \"Feature Development\"",
                "113": "    elif \"refactor\" in commit.msg.lower():",
                "114": "        activity = \"Refactoring\"",
                "115": "    else:",
                "116": "        activity = \"Other\"",
                "117": "    return activity",
                "118": "",
                "119": "def pretty_diff(commits_data, type, single_comment_symbol, multi_comment_symbols=[]):",
                "120": "    following_multi_comment = False",
                "121": "    if len(multi_comment_symbols) >= 2:",
                "122": "        multi_comments_enabled = True",
                "123": "    else:",
                "124": "        multi_comments_enabled = False",
                "125": "    for file, commits in commits_data.items():",
                "126": "        if len(file) > 0:",
                "127": "            for commit in commits:",
                "128": "                diff_edited = []",
                "129": "                # Set current line for each analysis",
                "130": "                for i in range(len(commit[\"diff\"][type])):",
                "131": "                    curr_line = commit[\"diff\"][type][i][0]",
                "132": "                    curr_content = commit[\"diff\"][type][i][1]",
                "133": "                    # In case of a starting multiline comment start adding future lines without comment symbol ",
                "134": "                    if multi_comments_enabled and curr_content.find(multi_comment_symbols[0]) != -1:",
                "135": "                            following_multi_comment = True",
                "136": "                    # In case of comment add them to existing dict if they directly follow",
                "137": "                    if curr_content.find(single_comment_symbol) == 0 or curr_content.find(single_comment_symbol + \" \") != -1 or following_multi_comment:",
                "138": "                        if len(diff_edited) > 0:",
                "139": "                            if len(diff_edited[-1][\"line_numbers\"]) == 0 or curr_line == diff_edited[-1][\"line_numbers\"][-1] + 1:",
                "140": "                                if len(diff_edited[-1][\"comments\"].keys()) > 0 and list(diff_edited[-1][\"comments\"].keys())[-1] + 1 == curr_line:",
                "141": "                                    diff_edited[-1][\"comments\"][curr_line] = curr_content",
                "142": "                                else:",
                "143": "                                    diff_edited.append({",
                "144": "                                        \"line_numbers\": [],",
                "145": "                                        \"comments\": {curr_line: curr_content},",
                "146": "                                        \"lines\": []})",
                "147": "                        else:",
                "148": "                    # or create new one",
                "149": "                            diff_edited.append({",
                "150": "                                \"line_numbers\": [],",
                "151": "                                \"comments\": {curr_line: curr_content},",
                "152": "                                \"lines\": []})",
                "153": "                    # In case of no comment add lines to existing dict if line number directly follows",
                "154": "                    else:    ",
                "155": "                        if len(diff_edited) > 0:",
                "156": "                            if len(diff_edited[-1][\"line_numbers\"]) == 0 or curr_line == diff_edited[-1][\"line_numbers\"][-1] + 1:",
                "157": "                                diff_edited[-1][\"line_numbers\"].append(curr_line)",
                "158": "                                diff_edited[-1][\"lines\"].append(curr_content)",
                "159": "                            else:",
                "160": "                    # Or create new one",
                "161": "                                diff_edited.append({",
                "162": "                                    \"line_numbers\": [curr_line],",
                "163": "                                    \"comments\": {},",
                "164": "                                    \"lines\": [curr_content]})",
                "165": "                    # Disable multiline comments when symbol found",
                "166": "                    if multi_comments_enabled and curr_content.find(multi_comment_symbols[1]) != -1:",
                "167": "                        following_multi_comment = False",
                "168": "                commit[\"diff\"][type] = diff_edited",
                "169": "    return commits_data",
                "170": "",
                "171": "def analyze_diffs(data):",
                "172": "    analysis_results = []",
                "173": "",
                "174": "    for file, commits in data.items():",
                "175": "        # Store last modified timestamps for each line",
                "176": "        last_modified = {}",
                "177": "        for commit in commits:",
                "178": "            # print(\"Starting to analyse commit: \", commit[\"commit\"])",
                "179": "            commit_time = datetime.fromisoformat(commit[\"timestamp\"])",
                "180": "            # Track modified lines",
                "181": "            for block in commit[\"diff\"][\"added\"]:",
                "182": "                for line in block[\"line_numbers\"]:",
                "183": "                    line_number = line",
                "184": "                    last_modified[line_number] = commit_time",
                "185": "            # Compare with comments",
                "186": "            for line in commit[\"comment_added_diff\"]:",
                "187": "                comment_time = datetime.fromisoformat(commit[\"timestamp\"])",
                "188": "                last_modified_lines = list(last_modified.keys())",
                "189": "                if int(line) in last_modified_lines:",
                "190": "                    for block in commit[\"diff\"][\"added\"]:",
                "191": "                        if line in block[\"comments\"] and len(block[\"line_numbers\"]) == 0:",
                "192": "                            if(comment_time > last_modified[int(line)]):",
                "193": "                                analysis_results.append({",
                "194": "                                    \"file\": file,",
                "195": "                                    \"line\": int(line),",
                "196": "                                    \"comment\": commit[\"comment_added_diff\"][line],",
                "197": "                                    \"comment_time\": str(comment_time),",
                "198": "                                    \"last_code_change_time\": str(last_modified[int(line)])",
                "199": "                                })",
                "200": "    return analysis_results",
                "201": "",
                "202": "def save_to_json(commits_data, path):",
                "203": "    # Save the processed commit data to a JSON file",
                "204": "    with open(path, 'w') as json_file:",
                "205": "        json.dump(commits_data, json_file, indent=4)",
                "206": "    print(\"Data has been saved to\", path)",
                "207": "",
                "208": "def create_xes_log(data):",
                "209": "    # Create a new EventLog object",
                "210": "    log = EventLog()",
                "211": "",
                "212": "    # Iterate over each commit entry in the data",
                "213": "    for file, commits in data.items():",
                "214": "        # Create a trace for the file",
                "215": "        trace = Trace()",
                "216": "        trace.attributes[\"file\"] = file",
                "217": "",
                "218": "        for commit in commits:",
                "219": "            # Extract event attributes",
                "220": "            event = Event()",
                "221": "            event[\"timestamp\"] = commit.get(\"timestamp\")",
                "222": "            event[\"author\"] = commit.get(\"author\")",
                "223": "            event[\"change_type\"] = commit.get(\"change_type\")",
                "224": "            event[\"commit_message\"] = commit.get(\"commit_message\")",
                "225": "            event[\"additions\"] = commit.get(\"additions\")",
                "226": "            event[\"deletions\"] = commit.get(\"deletions\")",
                "227": "            event[\"diff\"] = commit.get(\"diff\")",
                "228": "            if commit.get(\"comment_added_diff\"):",
                "229": "                event[\"comment_change\"] = \"True\"",
                "230": "            else:",
                "231": "                event[\"comment_change\"] = \"False\"",
                "232": "",
                "233": "            # Add the event to the trace",
                "234": "            trace.append(event)",
                "235": "",
                "236": "        # Add the trace to the log",
                "237": "        log.append(trace)",
                "238": "",
                "239": "    return log",
                "240": "",
                "241": "def save_xes_log(log, filename):",
                "242": "    # Export the log to an XES file",
                "243": "    xes_exporter.apply(log, filename)",
                "244": "",
                "245": "def save_to_xes(log, path):",
                "246": "    # Create the XES log from the commit data",
                "247": "    xes_log = create_xes_log(log)",
                "248": "",
                "249": "    # Save the XES log to a file",
                "250": "    save_xes_log(xes_log, path)",
                "251": "    print(\"XES log has been saved to\", path)",
                "252": "",
                "253": "if __name__ == \"__main__\":",
                "254": "    repo_url = \"https://github.com/apache/accumulo.git\"  # Example repository URL",
                "255": "    commits_data = analyze_commits(repo_url, \"java\", datetime.today() - relativedelta(years=1), datetime.today(), \"//\", [\"/*\", \"*/\"])",
                "256": "    save_to_json(commits_data, \"Data/commits_data.json\")",
                "257": "    # save_to_xes(commits_data, \"Data/commits_data.xes\")",
                "258": "    with open(\"Data/commits_data.json\", \"r\") as json_file: ",
                "259": "       commits_data = json.load(json_file)",
                "260": "    analyzed_data = pretty_diff(commits_data, \"added\", \"//\", [\"/*\", \"*/\"])",
                "261": "    save_to_json(analyzed_data, \"Exports/analyzed_data.json\")",
                "262": "    analyzed_data = pretty_diff(commits_data, \"deleted\", \"#\")",
                "263": "    save_to_json(analyzed_data, \"Exports/analyzed_data.json\")",
                "264": "    ",
                "265": "    # Test case",
                "266": "    with open(\"Exports/analyzed_data.json\", \"r\") as json_file:",
                "267": "        data = json.load(json_file)",
                "268": "    analyzed_data = analyze_diffs(data)",
                "269": "",
                "270": "    save_to_json(analyzed_data, \"Exports/analysis_results.json\")",
                "271": ""
            },
            "comments": [
                {
                    "line": 27,
                    "comment": "# Traverse through the commits in the repository",
                    "char_position_in_line": 4,
                    "type": [
                        "block"
                    ]
                },
                {
                    "line": 28,
                    "comment": "# Only save commits, that contain at least one file of the format {language_file_extension}",
                    "char_position_in_line": 4,
                    "type": [
                        "block"
                    ]
                },
                {
                    "line": 37,
                    "comment": "# Analyze each file modified in the commit",
                    "char_position_in_line": 8,
                    "type": [
                        "normal"
                    ]
                },
                {
                    "line": 39,
                    "comment": "# only store file data for Rust files",
                    "char_position_in_line": 12,
                    "type": [
                        "normal"
                    ]
                },
                {
                    "line": 57,
                    "comment": "# For added diff ispect lines filter out comments",
                    "char_position_in_line": 16,
                    "type": [
                        "normal"
                    ]
                },
                {
                    "line": 68,
                    "comment": "# For deleted diff ispect lines filter out comments",
                    "char_position_in_line": 16,
                    "type": [
                        "normal"
                    ]
                },
                {
                    "line": 85,
                    "comment": "# Generate keywords based on the commit message and type of changes",
                    "char_position_in_line": 16,
                    "type": [
                        "block"
                    ]
                },
                {
                    "line": 86,
                    "comment": "# file_data[\"keywords\"] = extract_keywords(commit.msg, modified_file)",
                    "char_position_in_line": 16,
                    "type": [
                        "block"
                    ]
                },
                {
                    "line": 87,
                    "comment": "# Extract type of commit from commit message",
                    "char_position_in_line": 16,
                    "type": [
                        "block"
                    ]
                },
                {
                    "line": 88,
                    "comment": "# file_data[\"activity\"] = extract_activity(commit.msg)",
                    "char_position_in_line": 16,
                    "type": [
                        "block"
                    ]
                },
                {
                    "line": 94,
                    "comment": "# Determine basic keywords based on the commit message",
                    "char_position_in_line": 4,
                    "type": [
                        "normal"
                    ]
                },
                {
                    "line": 107,
                    "comment": "# Use commit message keywords to determine activity type",
                    "char_position_in_line": 4,
                    "type": [
                        "normal"
                    ]
                },
                {
                    "line": 129,
                    "comment": "# Set current line for each analysis",
                    "char_position_in_line": 16,
                    "type": [
                        "normal"
                    ]
                },
                {
                    "line": 133,
                    "comment": "# In case of a starting multiline comment start adding future lines without comment symbol ",
                    "char_position_in_line": 20,
                    "type": [
                        "normal"
                    ]
                },
                {
                    "line": 136,
                    "comment": "# In case of comment add them to existing dict if they directly follow",
                    "char_position_in_line": 20,
                    "type": [
                        "normal"
                    ]
                },
                {
                    "line": 148,
                    "comment": "# or create new one",
                    "char_position_in_line": 20,
                    "type": [
                        "normal"
                    ]
                },
                {
                    "line": 153,
                    "comment": "# In case of no comment add lines to existing dict if line number directly follows",
                    "char_position_in_line": 20,
                    "type": [
                        "normal"
                    ]
                },
                {
                    "line": 160,
                    "comment": "# Or create new one",
                    "char_position_in_line": 20,
                    "type": [
                        "normal"
                    ]
                },
                {
                    "line": 165,
                    "comment": "# Disable multiline comments when symbol found",
                    "char_position_in_line": 20,
                    "type": [
                        "normal"
                    ]
                },
                {
                    "line": 175,
                    "comment": "# Store last modified timestamps for each line",
                    "char_position_in_line": 8,
                    "type": [
                        "normal"
                    ]
                },
                {
                    "line": 178,
                    "comment": "# print(\"Starting to analyse commit: \", commit[\"commit\"])",
                    "char_position_in_line": 12,
                    "type": [
                        "commented-out"
                    ]
                },
                {
                    "line": 180,
                    "comment": "# Track modified lines",
                    "char_position_in_line": 12,
                    "type": [
                        "normal"
                    ]
                },
                {
                    "line": 185,
                    "comment": "# Compare with comments",
                    "char_position_in_line": 12,
                    "type": [
                        "normal"
                    ]
                },
                {
                    "line": 203,
                    "comment": "# Save the processed commit data to a JSON file",
                    "char_position_in_line": 4,
                    "type": [
                        "normal"
                    ]
                },
                {
                    "line": 209,
                    "comment": "# Create a new EventLog object",
                    "char_position_in_line": 4,
                    "type": [
                        "normal"
                    ]
                },
                {
                    "line": 212,
                    "comment": "# Iterate over each commit entry in the data",
                    "char_position_in_line": 4,
                    "type": [
                        "normal"
                    ]
                },
                {
                    "line": 214,
                    "comment": "# Create a trace for the file",
                    "char_position_in_line": 8,
                    "type": [
                        "normal"
                    ]
                },
                {
                    "line": 219,
                    "comment": "# Extract event attributes",
                    "char_position_in_line": 12,
                    "type": [
                        "normal"
                    ]
                },
                {
                    "line": 233,
                    "comment": "# Add the event to the trace",
                    "char_position_in_line": 12,
                    "type": [
                        "normal"
                    ]
                },
                {
                    "line": 236,
                    "comment": "# Add the trace to the log",
                    "char_position_in_line": 8,
                    "type": [
                        "normal"
                    ]
                },
                {
                    "line": 242,
                    "comment": "# Export the log to an XES file",
                    "char_position_in_line": 4,
                    "type": [
                        "normal"
                    ]
                },
                {
                    "line": 246,
                    "comment": "# Create the XES log from the commit data",
                    "char_position_in_line": 4,
                    "type": [
                        "normal"
                    ]
                },
                {
                    "line": 249,
                    "comment": "# Save the XES log to a file",
                    "char_position_in_line": 4,
                    "type": [
                        "normal"
                    ]
                },
                {
                    "line": 254,
                    "comment": "# Example repository URL",
                    "char_position_in_line": 57,
                    "type": [
                        "normal"
                    ]
                },
                {
                    "line": 257,
                    "comment": "# save_to_xes(commits_data, \"Data/commits_data.xes\")",
                    "char_position_in_line": 4,
                    "type": [
                        "commented-out"
                    ]
                },
                {
                    "line": 265,
                    "comment": "# Test case",
                    "char_position_in_line": 4,
                    "type": [
                        "normal"
                    ]
                }
            ]
        },
        {
            "commit": "1013981f1c0e06dc3def7ea036c0da903f2e38f6",
            "timestamp": "2024-11-22T16:53:22+01:00",
            "author": "alexander.schranner",
            "diff": {
                "added": {
                    "7": "def get_commits_data(repo_path, from_date, to_date, file_types):",
                    "9": "    for commit in Repository(   repo_path,",
                    "10": "                                since=from_date,",
                    "11": "                                to=to_date,",
                    "12": "                                only_modifications_with_file_types=file_types).traverse_commits():",
                    "13": "            for file in commit.modified_files:",
                    "14": "                if file.filename not in files_data and len(file.filename.split(\".\")) == 2 and \".\" + file.filename.split(\".\")[1] in file_types:",
                    "15": "                    files_data[file.filename] = []",
                    "16": "                if len(file.filename.split(\".\")) == 2 and \".\" + file.filename.split(\".\")[1] in file_types:",
                    "17": "                    file_data = {",
                    "18": "                        \"commit\": commit.hash,",
                    "19": "                        \"timestamp\": commit.committer_date.isoformat(),",
                    "20": "                        \"author\": commit.author.name,",
                    "21": "                        \"diff\": file.diff_parsed",
                    "22": "                    }",
                    "23": "                    if len(file.diff_parsed) != 0:",
                    "24": "                        files_data[file.filename].append(file_data)",
                    "202": "    return analysis_results"
                },
                "deleted": {
                    "6": "from pm4py.objects.log.obj import EventLog, Trace, Event",
                    "7": "from pm4py.objects.log.exporter.xes import exporter as xes_exporter",
                    "9": "def get_commits_data(repo_path, from_date, to_date):",
                    "11": "    for commit in Repository(repo_path, since=from_date, to=to_date).traverse_commits():",
                    "12": "        for file in commit.modified_files:",
                    "13": "            if file not in files_data:",
                    "14": "                files_data[file.filename] = []",
                    "15": "            file_data = {",
                    "16": "                \"commit\": commit.hash,",
                    "17": "                \"timestamp\": commit.committer_date.isoformat(),",
                    "18": "                \"author\": commit.author.name,",
                    "19": "                \"diff\": file.diff_parsed",
                    "20": "            }",
                    "21": "            if len(file.diff_parsed) != 0:",
                    "22": "                files_data[file.filename].append(file_data)",
                    "200": "    return analysis_results",
                    "201": "",
                    "202": "def save_to_json(commits_data, path):",
                    "203": "    # Save the processed commit data to a JSON file",
                    "204": "    with open(path, 'w') as json_file:",
                    "205": "        json.dump(commits_data, json_file, indent=4)",
                    "206": "    print(\"Data has been saved to\", path)",
                    "207": "",
                    "208": "def create_xes_log(data):",
                    "209": "    # Create a new EventLog object",
                    "210": "    log = EventLog()",
                    "211": "",
                    "212": "    # Iterate over each commit entry in the data",
                    "213": "    for file, commits in data.items():",
                    "214": "        # Create a trace for the file",
                    "215": "        trace = Trace()",
                    "216": "        trace.attributes[\"file\"] = file",
                    "217": "",
                    "218": "        for commit in commits:",
                    "219": "            # Extract event attributes",
                    "220": "            event = Event()",
                    "221": "            event[\"timestamp\"] = commit.get(\"timestamp\")",
                    "222": "            event[\"author\"] = commit.get(\"author\")",
                    "223": "            event[\"change_type\"] = commit.get(\"change_type\")",
                    "224": "            event[\"commit_message\"] = commit.get(\"commit_message\")",
                    "225": "            event[\"additions\"] = commit.get(\"additions\")",
                    "226": "            event[\"deletions\"] = commit.get(\"deletions\")",
                    "227": "            event[\"diff\"] = commit.get(\"diff\")",
                    "228": "            if commit.get(\"comment_added_diff\"):",
                    "229": "                event[\"comment_change\"] = \"True\"",
                    "230": "            else:",
                    "231": "                event[\"comment_change\"] = \"False\"",
                    "232": "",
                    "233": "            # Add the event to the trace",
                    "234": "            trace.append(event)",
                    "235": "",
                    "236": "        # Add the trace to the log",
                    "237": "        log.append(trace)",
                    "238": "",
                    "239": "    return log",
                    "240": "",
                    "241": "def save_xes_log(log, filename):",
                    "242": "    # Export the log to an XES file",
                    "243": "    xes_exporter.apply(log, filename)",
                    "244": "",
                    "245": "def save_to_xes(log, path):",
                    "246": "    # Create the XES log from the commit data",
                    "247": "    xes_log = create_xes_log(log)",
                    "248": "",
                    "249": "    # Save the XES log to a file",
                    "250": "    save_xes_log(xes_log, path)",
                    "251": "    print(\"XES log has been saved to\", path)",
                    "252": "",
                    "253": "if __name__ == \"__main__\":",
                    "254": "    repo_url = \"https://github.com/apache/accumulo.git\"  # Example repository URL",
                    "255": "    commits_data = analyze_commits(repo_url, \"java\", datetime.today() - relativedelta(years=1), datetime.today(), \"//\", [\"/*\", \"*/\"])",
                    "256": "    save_to_json(commits_data, \"Data/commits_data.json\")",
                    "257": "    # save_to_xes(commits_data, \"Data/commits_data.xes\")",
                    "258": "    with open(\"Data/commits_data.json\", \"r\") as json_file:",
                    "259": "       commits_data = json.load(json_file)",
                    "260": "    analyzed_data = pretty_diff(commits_data, \"added\", \"//\", [\"/*\", \"*/\"])",
                    "261": "    save_to_json(analyzed_data, \"Exports/analyzed_data.json\")",
                    "262": "    analyzed_data = pretty_diff(commits_data, \"deleted\", \"#\")",
                    "263": "    save_to_json(analyzed_data, \"Exports/analyzed_data.json\")",
                    "264": "",
                    "265": "    # Test case",
                    "266": "    with open(\"Exports/analyzed_data.json\", \"r\") as json_file:",
                    "267": "        data = json.load(json_file)",
                    "268": "    analyzed_data = analyze_diffs(data)",
                    "269": "",
                    "270": "    save_to_json(analyzed_data, \"Exports/analysis_results.json\")"
                }
            },
            "source_code": {
                "1": "from pydriller import Repository",
                "2": "import json",
                "3": "import pm4py",
                "4": "from datetime import datetime",
                "5": "from dateutil.relativedelta import relativedelta",
                "6": "",
                "7": "def get_commits_data(repo_path, from_date, to_date, file_types):",
                "8": "    files_data = {}",
                "9": "    for commit in Repository(   repo_path, ",
                "10": "                                since=from_date, ",
                "11": "                                to=to_date, ",
                "12": "                                only_modifications_with_file_types=file_types).traverse_commits():",
                "13": "            for file in commit.modified_files:",
                "14": "                if file.filename not in files_data and len(file.filename.split(\".\")) == 2 and \".\" + file.filename.split(\".\")[1] in file_types:",
                "15": "                    files_data[file.filename] = []",
                "16": "                if len(file.filename.split(\".\")) == 2 and \".\" + file.filename.split(\".\")[1] in file_types:",
                "17": "                    file_data = {",
                "18": "                        \"commit\": commit.hash,",
                "19": "                        \"timestamp\": commit.committer_date.isoformat(),",
                "20": "                        \"author\": commit.author.name,",
                "21": "                        \"diff\": file.diff_parsed",
                "22": "                    }",
                "23": "                    if len(file.diff_parsed) != 0:",
                "24": "                        files_data[file.filename].append(file_data)",
                "25": "    return files_data",
                "26": "",
                "27": "def analyze_commits(repo_url, language_file_extension, dt1, dt2, single_comment_symbol, multi_comment_symbols=[]):",
                "28": "    files_data = {}",
                "29": "    # Traverse through the commits in the repository",
                "30": "    # Only save commits, that contain at least one file of the format {language_file_extension}",
                "31": "    for commit in Repository(repo_url, ",
                "32": "    only_modifications_with_file_types=[f\".{language_file_extension}\"],",
                "33": "    since=dt1,",
                "34": "    to=dt2).traverse_commits():",
                "35": "        if len(multi_comment_symbols) >= 2:",
                "36": "            multi_comments_enabled = True",
                "37": "        else:",
                "38": "            multi_comments_enabled = False",
                "39": "        # Analyze each file modified in the commit",
                "40": "        for modified_file in commit.modified_files:",
                "41": "            # only store file data for Rust files",
                "42": "            if modified_file.filename not in files_data:",
                "43": "                files_data[modified_file.filename] = []",
                "44": "            if len(modified_file.filename.split(\".\")) == 2 and modified_file.filename.split(\".\")[1] == language_file_extension:",
                "45": "                file_data = {",
                "46": "                    \"commit\": commit.hash,",
                "47": "                    \"timestamp\": commit.committer_date.isoformat(),",
                "48": "                    \"author\": commit.author.name,",
                "49": "                    \"commit_message\": commit.msg,",
                "50": "                    \"additions\": modified_file.added_lines,",
                "51": "                    \"deletions\": modified_file.deleted_lines,",
                "52": "                    \"change_type\": modified_file.change_type.name,",
                "53": "                    \"diff\": modified_file.diff_parsed",
                "54": "                }",
                "55": "                diff_added = {}",
                "56": "                diff_deleted = {}",
                "57": "                diff_modified = {}",
                "58": "                following_multi_comment = False",
                "59": "                # For added diff ispect lines filter out comments",
                "60": "                for line in modified_file.diff_parsed[\"added\"]:",
                "61": "                    if line[1].find(single_comment_symbol) != -1 or following_multi_comment:",
                "62": "                        diff_added[line[0]] = line[1]",
                "63": "                    if multi_comments_enabled and line[1].find(multi_comment_symbols[0]) != -1:",
                "64": "                        diff_added[line[0]] = line[1]",
                "65": "                        following_multi_comment = True",
                "66": "                    if multi_comments_enabled and line[1].find(multi_comment_symbols[1]) != -1:",
                "67": "                        diff_added[line[0]] = line[1]",
                "68": "                        following_multi_comment = False",
                "69": "                file_data[\"comment_added_diff\"] = diff_added",
                "70": "                # For deleted diff ispect lines filter out comments",
                "71": "                for line in modified_file.diff_parsed[\"deleted\"]:",
                "72": "                    if line[1].find(single_comment_symbol) != -1 or following_multi_comment:",
                "73": "                        diff_deleted[line[0]] = line[1]",
                "74": "                        if line[0] in diff_added.keys():",
                "75": "                            diff_modified[line[0]] = line[1]",
                "76": "                    if multi_comments_enabled and line[1].find(multi_comment_symbols[0]) != -1:",
                "77": "                        diff_added[line[0]] = line[1]",
                "78": "                        following_multi_comment = True",
                "79": "                        if line[0] in diff_added.keys():",
                "80": "                            diff_modified[line[0]] = line[1]",
                "81": "                    if multi_comments_enabled and line[1].find(multi_comment_symbols[1]) != -1:",
                "82": "                        diff_added[line[0]] = line[1]",
                "83": "                        if line[0] in diff_added.keys():",
                "84": "                            diff_modified[line[0]] = line[1]",
                "85": "                file_data[\"comment_deleted_diff\"] = diff_deleted",
                "86": "                file_data[\"comment_modified_diff\"] = diff_modified",
                "87": "                # Generate keywords based on the commit message and type of changes",
                "88": "                # file_data[\"keywords\"] = extract_keywords(commit.msg, modified_file)",
                "89": "                # Extract type of commit from commit message",
                "90": "                # file_data[\"activity\"] = extract_activity(commit.msg)",
                "91": "                if len(diff_added) + len(diff_deleted) != 0:",
                "92": "                    files_data[modified_file.filename].append(file_data)",
                "93": "    return files_data",
                "94": "",
                "95": "def extract_keywords(commit_message, modified_file):",
                "96": "    # Determine basic keywords based on the commit message",
                "97": "    keywords = []",
                "98": "    if \"performance\" in commit_message.lower():",
                "99": "        keywords.append(\"performance\")",
                "100": "    if \"security\" in commit_message.lower():",
                "101": "        keywords.append(\"security\")",
                "102": "    if modified_file.added_lines > modified_file.deleted_lines:",
                "103": "        keywords.append(\"expansion\")",
                "104": "    else:",
                "105": "        keywords.append(\"optimization\")",
                "106": "    return keywords",
                "107": "",
                "108": "def extract_activity(commit_message):",
                "109": "    # Use commit message keywords to determine activity type",
                "110": "    activity = \"\"",
                "111": "    if \"bug\" in commit.msg.lower() or \"fix\" in commit.msg.lower():",
                "112": "        activity = \"Bug Fix\"",
                "113": "    elif \"feature\" in commit.msg.lower() or \"add\" in commit.msg.lower():",
                "114": "        activity = \"Feature Development\"",
                "115": "    elif \"refactor\" in commit.msg.lower():",
                "116": "        activity = \"Refactoring\"",
                "117": "    else:",
                "118": "        activity = \"Other\"",
                "119": "    return activity",
                "120": "",
                "121": "def pretty_diff(commits_data, type, single_comment_symbol, multi_comment_symbols=[]):",
                "122": "    following_multi_comment = False",
                "123": "    if len(multi_comment_symbols) >= 2:",
                "124": "        multi_comments_enabled = True",
                "125": "    else:",
                "126": "        multi_comments_enabled = False",
                "127": "    for file, commits in commits_data.items():",
                "128": "        if len(file) > 0:",
                "129": "            for commit in commits:",
                "130": "                diff_edited = []",
                "131": "                # Set current line for each analysis",
                "132": "                for i in range(len(commit[\"diff\"][type])):",
                "133": "                    curr_line = commit[\"diff\"][type][i][0]",
                "134": "                    curr_content = commit[\"diff\"][type][i][1]",
                "135": "                    # In case of a starting multiline comment start adding future lines without comment symbol ",
                "136": "                    if multi_comments_enabled and curr_content.find(multi_comment_symbols[0]) != -1:",
                "137": "                            following_multi_comment = True",
                "138": "                    # In case of comment add them to existing dict if they directly follow",
                "139": "                    if curr_content.find(single_comment_symbol) == 0 or curr_content.find(single_comment_symbol + \" \") != -1 or following_multi_comment:",
                "140": "                        if len(diff_edited) > 0:",
                "141": "                            if len(diff_edited[-1][\"line_numbers\"]) == 0 or curr_line == diff_edited[-1][\"line_numbers\"][-1] + 1:",
                "142": "                                if len(diff_edited[-1][\"comments\"].keys()) > 0 and list(diff_edited[-1][\"comments\"].keys())[-1] + 1 == curr_line:",
                "143": "                                    diff_edited[-1][\"comments\"][curr_line] = curr_content",
                "144": "                                else:",
                "145": "                                    diff_edited.append({",
                "146": "                                        \"line_numbers\": [],",
                "147": "                                        \"comments\": {curr_line: curr_content},",
                "148": "                                        \"lines\": []})",
                "149": "                        else:",
                "150": "                    # or create new one",
                "151": "                            diff_edited.append({",
                "152": "                                \"line_numbers\": [],",
                "153": "                                \"comments\": {curr_line: curr_content},",
                "154": "                                \"lines\": []})",
                "155": "                    # In case of no comment add lines to existing dict if line number directly follows",
                "156": "                    else:    ",
                "157": "                        if len(diff_edited) > 0:",
                "158": "                            if len(diff_edited[-1][\"line_numbers\"]) == 0 or curr_line == diff_edited[-1][\"line_numbers\"][-1] + 1:",
                "159": "                                diff_edited[-1][\"line_numbers\"].append(curr_line)",
                "160": "                                diff_edited[-1][\"lines\"].append(curr_content)",
                "161": "                            else:",
                "162": "                    # Or create new one",
                "163": "                                diff_edited.append({",
                "164": "                                    \"line_numbers\": [curr_line],",
                "165": "                                    \"comments\": {},",
                "166": "                                    \"lines\": [curr_content]})",
                "167": "                    # Disable multiline comments when symbol found",
                "168": "                    if multi_comments_enabled and curr_content.find(multi_comment_symbols[1]) != -1:",
                "169": "                        following_multi_comment = False",
                "170": "                commit[\"diff\"][type] = diff_edited",
                "171": "    return commits_data",
                "172": "",
                "173": "def analyze_diffs(data):",
                "174": "    analysis_results = []",
                "175": "",
                "176": "    for file, commits in data.items():",
                "177": "        # Store last modified timestamps for each line",
                "178": "        last_modified = {}",
                "179": "        for commit in commits:",
                "180": "            # print(\"Starting to analyse commit: \", commit[\"commit\"])",
                "181": "            commit_time = datetime.fromisoformat(commit[\"timestamp\"])",
                "182": "            # Track modified lines",
                "183": "            for block in commit[\"diff\"][\"added\"]:",
                "184": "                for line in block[\"line_numbers\"]:",
                "185": "                    line_number = line",
                "186": "                    last_modified[line_number] = commit_time",
                "187": "            # Compare with comments",
                "188": "            for line in commit[\"comment_added_diff\"]:",
                "189": "                comment_time = datetime.fromisoformat(commit[\"timestamp\"])",
                "190": "                last_modified_lines = list(last_modified.keys())",
                "191": "                if int(line) in last_modified_lines:",
                "192": "                    for block in commit[\"diff\"][\"added\"]:",
                "193": "                        if line in block[\"comments\"] and len(block[\"line_numbers\"]) == 0:",
                "194": "                            if(comment_time > last_modified[int(line)]):",
                "195": "                                analysis_results.append({",
                "196": "                                    \"file\": file,",
                "197": "                                    \"line\": int(line),",
                "198": "                                    \"comment\": commit[\"comment_added_diff\"][line],",
                "199": "                                    \"comment_time\": str(comment_time),",
                "200": "                                    \"last_code_change_time\": str(last_modified[int(line)])",
                "201": "                                })",
                "202": "    return analysis_results"
            },
            "comments": [
                {
                    "line": 29,
                    "comment": "# Traverse through the commits in the repository",
                    "char_position_in_line": 4,
                    "type": [
                        "block"
                    ]
                },
                {
                    "line": 30,
                    "comment": "# Only save commits, that contain at least one file of the format {language_file_extension}",
                    "char_position_in_line": 4,
                    "type": [
                        "block"
                    ]
                },
                {
                    "line": 39,
                    "comment": "# Analyze each file modified in the commit",
                    "char_position_in_line": 8,
                    "type": [
                        "normal"
                    ]
                },
                {
                    "line": 41,
                    "comment": "# only store file data for Rust files",
                    "char_position_in_line": 12,
                    "type": [
                        "normal"
                    ]
                },
                {
                    "line": 59,
                    "comment": "# For added diff ispect lines filter out comments",
                    "char_position_in_line": 16,
                    "type": [
                        "normal"
                    ]
                },
                {
                    "line": 70,
                    "comment": "# For deleted diff ispect lines filter out comments",
                    "char_position_in_line": 16,
                    "type": [
                        "normal"
                    ]
                },
                {
                    "line": 87,
                    "comment": "# Generate keywords based on the commit message and type of changes",
                    "char_position_in_line": 16,
                    "type": [
                        "block"
                    ]
                },
                {
                    "line": 88,
                    "comment": "# file_data[\"keywords\"] = extract_keywords(commit.msg, modified_file)",
                    "char_position_in_line": 16,
                    "type": [
                        "block"
                    ]
                },
                {
                    "line": 89,
                    "comment": "# Extract type of commit from commit message",
                    "char_position_in_line": 16,
                    "type": [
                        "block"
                    ]
                },
                {
                    "line": 90,
                    "comment": "# file_data[\"activity\"] = extract_activity(commit.msg)",
                    "char_position_in_line": 16,
                    "type": [
                        "block"
                    ]
                },
                {
                    "line": 96,
                    "comment": "# Determine basic keywords based on the commit message",
                    "char_position_in_line": 4,
                    "type": [
                        "normal"
                    ]
                },
                {
                    "line": 109,
                    "comment": "# Use commit message keywords to determine activity type",
                    "char_position_in_line": 4,
                    "type": [
                        "normal"
                    ]
                },
                {
                    "line": 131,
                    "comment": "# Set current line for each analysis",
                    "char_position_in_line": 16,
                    "type": [
                        "normal"
                    ]
                },
                {
                    "line": 135,
                    "comment": "# In case of a starting multiline comment start adding future lines without comment symbol ",
                    "char_position_in_line": 20,
                    "type": [
                        "normal"
                    ]
                },
                {
                    "line": 138,
                    "comment": "# In case of comment add them to existing dict if they directly follow",
                    "char_position_in_line": 20,
                    "type": [
                        "normal"
                    ]
                },
                {
                    "line": 150,
                    "comment": "# or create new one",
                    "char_position_in_line": 20,
                    "type": [
                        "normal"
                    ]
                },
                {
                    "line": 155,
                    "comment": "# In case of no comment add lines to existing dict if line number directly follows",
                    "char_position_in_line": 20,
                    "type": [
                        "normal"
                    ]
                },
                {
                    "line": 162,
                    "comment": "# Or create new one",
                    "char_position_in_line": 20,
                    "type": [
                        "normal"
                    ]
                },
                {
                    "line": 167,
                    "comment": "# Disable multiline comments when symbol found",
                    "char_position_in_line": 20,
                    "type": [
                        "normal"
                    ]
                },
                {
                    "line": 177,
                    "comment": "# Store last modified timestamps for each line",
                    "char_position_in_line": 8,
                    "type": [
                        "normal"
                    ]
                },
                {
                    "line": 180,
                    "comment": "# print(\"Starting to analyse commit: \", commit[\"commit\"])",
                    "char_position_in_line": 12,
                    "type": [
                        "commented-out"
                    ]
                },
                {
                    "line": 182,
                    "comment": "# Track modified lines",
                    "char_position_in_line": 12,
                    "type": [
                        "normal"
                    ]
                },
                {
                    "line": 187,
                    "comment": "# Compare with comments",
                    "char_position_in_line": 12,
                    "type": [
                        "normal"
                    ]
                }
            ]
        },
        {
            "commit": "e20d03792161ba1b90725e6912b40275f06bf2da",
            "timestamp": "2024-11-25T02:54:47+01:00",
            "author": "alexander.schranner",
            "diff": {
                "added": {
                    "21": "                        \"diff\": diff_to_dict(file.diff_parsed)",
                    "27": "def diff_to_dict(diff):",
                    "28": "    dict_added = {}",
                    "29": "    for line in diff[\"added\"]:",
                    "30": "        dict_added[line[0]] = line[1]",
                    "31": "    diff[\"added\"] = dict_added",
                    "32": "    dict_deleted = {}",
                    "33": "    for line in diff[\"deleted\"]:",
                    "34": "        dict_deleted[line[0]] = line[1]",
                    "35": "    diff[\"deleted\"] = dict_deleted",
                    "36": "    return diff",
                    "37": "",
                    "38": ""
                },
                "deleted": {
                    "21": "                        \"diff\": file.diff_parsed"
                }
            },
            "source_code": {
                "1": "from pydriller import Repository",
                "2": "import json",
                "3": "import pm4py",
                "4": "from datetime import datetime",
                "5": "from dateutil.relativedelta import relativedelta",
                "6": "",
                "7": "def get_commits_data(repo_path, from_date, to_date, file_types):",
                "8": "    files_data = {}",
                "9": "    for commit in Repository(   repo_path, ",
                "10": "                                since=from_date, ",
                "11": "                                to=to_date, ",
                "12": "                                only_modifications_with_file_types=file_types).traverse_commits():",
                "13": "            for file in commit.modified_files:",
                "14": "                if file.filename not in files_data and len(file.filename.split(\".\")) == 2 and \".\" + file.filename.split(\".\")[1] in file_types:",
                "15": "                    files_data[file.filename] = []",
                "16": "                if len(file.filename.split(\".\")) == 2 and \".\" + file.filename.split(\".\")[1] in file_types:",
                "17": "                    file_data = {",
                "18": "                        \"commit\": commit.hash,",
                "19": "                        \"timestamp\": commit.committer_date.isoformat(),",
                "20": "                        \"author\": commit.author.name,",
                "21": "                        \"diff\": diff_to_dict(file.diff_parsed)",
                "22": "                    }",
                "23": "                    if len(file.diff_parsed) != 0:",
                "24": "                        files_data[file.filename].append(file_data)",
                "25": "    return files_data",
                "26": "",
                "27": "def diff_to_dict(diff):",
                "28": "    dict_added = {}",
                "29": "    for line in diff[\"added\"]:",
                "30": "        dict_added[line[0]] = line[1]",
                "31": "    diff[\"added\"] = dict_added",
                "32": "    dict_deleted = {}",
                "33": "    for line in diff[\"deleted\"]:",
                "34": "        dict_deleted[line[0]] = line[1]",
                "35": "    diff[\"deleted\"] = dict_deleted",
                "36": "    return diff",
                "37": "",
                "38": "",
                "39": "def analyze_commits(repo_url, language_file_extension, dt1, dt2, single_comment_symbol, multi_comment_symbols=[]):",
                "40": "    files_data = {}",
                "41": "    # Traverse through the commits in the repository",
                "42": "    # Only save commits, that contain at least one file of the format {language_file_extension}",
                "43": "    for commit in Repository(repo_url, ",
                "44": "    only_modifications_with_file_types=[f\".{language_file_extension}\"],",
                "45": "    since=dt1,",
                "46": "    to=dt2).traverse_commits():",
                "47": "        if len(multi_comment_symbols) >= 2:",
                "48": "            multi_comments_enabled = True",
                "49": "        else:",
                "50": "            multi_comments_enabled = False",
                "51": "        # Analyze each file modified in the commit",
                "52": "        for modified_file in commit.modified_files:",
                "53": "            # only store file data for Rust files",
                "54": "            if modified_file.filename not in files_data:",
                "55": "                files_data[modified_file.filename] = []",
                "56": "            if len(modified_file.filename.split(\".\")) == 2 and modified_file.filename.split(\".\")[1] == language_file_extension:",
                "57": "                file_data = {",
                "58": "                    \"commit\": commit.hash,",
                "59": "                    \"timestamp\": commit.committer_date.isoformat(),",
                "60": "                    \"author\": commit.author.name,",
                "61": "                    \"commit_message\": commit.msg,",
                "62": "                    \"additions\": modified_file.added_lines,",
                "63": "                    \"deletions\": modified_file.deleted_lines,",
                "64": "                    \"change_type\": modified_file.change_type.name,",
                "65": "                    \"diff\": modified_file.diff_parsed",
                "66": "                }",
                "67": "                diff_added = {}",
                "68": "                diff_deleted = {}",
                "69": "                diff_modified = {}",
                "70": "                following_multi_comment = False",
                "71": "                # For added diff ispect lines filter out comments",
                "72": "                for line in modified_file.diff_parsed[\"added\"]:",
                "73": "                    if line[1].find(single_comment_symbol) != -1 or following_multi_comment:",
                "74": "                        diff_added[line[0]] = line[1]",
                "75": "                    if multi_comments_enabled and line[1].find(multi_comment_symbols[0]) != -1:",
                "76": "                        diff_added[line[0]] = line[1]",
                "77": "                        following_multi_comment = True",
                "78": "                    if multi_comments_enabled and line[1].find(multi_comment_symbols[1]) != -1:",
                "79": "                        diff_added[line[0]] = line[1]",
                "80": "                        following_multi_comment = False",
                "81": "                file_data[\"comment_added_diff\"] = diff_added",
                "82": "                # For deleted diff ispect lines filter out comments",
                "83": "                for line in modified_file.diff_parsed[\"deleted\"]:",
                "84": "                    if line[1].find(single_comment_symbol) != -1 or following_multi_comment:",
                "85": "                        diff_deleted[line[0]] = line[1]",
                "86": "                        if line[0] in diff_added.keys():",
                "87": "                            diff_modified[line[0]] = line[1]",
                "88": "                    if multi_comments_enabled and line[1].find(multi_comment_symbols[0]) != -1:",
                "89": "                        diff_added[line[0]] = line[1]",
                "90": "                        following_multi_comment = True",
                "91": "                        if line[0] in diff_added.keys():",
                "92": "                            diff_modified[line[0]] = line[1]",
                "93": "                    if multi_comments_enabled and line[1].find(multi_comment_symbols[1]) != -1:",
                "94": "                        diff_added[line[0]] = line[1]",
                "95": "                        if line[0] in diff_added.keys():",
                "96": "                            diff_modified[line[0]] = line[1]",
                "97": "                file_data[\"comment_deleted_diff\"] = diff_deleted",
                "98": "                file_data[\"comment_modified_diff\"] = diff_modified",
                "99": "                # Generate keywords based on the commit message and type of changes",
                "100": "                # file_data[\"keywords\"] = extract_keywords(commit.msg, modified_file)",
                "101": "                # Extract type of commit from commit message",
                "102": "                # file_data[\"activity\"] = extract_activity(commit.msg)",
                "103": "                if len(diff_added) + len(diff_deleted) != 0:",
                "104": "                    files_data[modified_file.filename].append(file_data)",
                "105": "    return files_data",
                "106": "",
                "107": "def extract_keywords(commit_message, modified_file):",
                "108": "    # Determine basic keywords based on the commit message",
                "109": "    keywords = []",
                "110": "    if \"performance\" in commit_message.lower():",
                "111": "        keywords.append(\"performance\")",
                "112": "    if \"security\" in commit_message.lower():",
                "113": "        keywords.append(\"security\")",
                "114": "    if modified_file.added_lines > modified_file.deleted_lines:",
                "115": "        keywords.append(\"expansion\")",
                "116": "    else:",
                "117": "        keywords.append(\"optimization\")",
                "118": "    return keywords",
                "119": "",
                "120": "def extract_activity(commit_message):",
                "121": "    # Use commit message keywords to determine activity type",
                "122": "    activity = \"\"",
                "123": "    if \"bug\" in commit.msg.lower() or \"fix\" in commit.msg.lower():",
                "124": "        activity = \"Bug Fix\"",
                "125": "    elif \"feature\" in commit.msg.lower() or \"add\" in commit.msg.lower():",
                "126": "        activity = \"Feature Development\"",
                "127": "    elif \"refactor\" in commit.msg.lower():",
                "128": "        activity = \"Refactoring\"",
                "129": "    else:",
                "130": "        activity = \"Other\"",
                "131": "    return activity",
                "132": "",
                "133": "def pretty_diff(commits_data, type, single_comment_symbol, multi_comment_symbols=[]):",
                "134": "    following_multi_comment = False",
                "135": "    if len(multi_comment_symbols) >= 2:",
                "136": "        multi_comments_enabled = True",
                "137": "    else:",
                "138": "        multi_comments_enabled = False",
                "139": "    for file, commits in commits_data.items():",
                "140": "        if len(file) > 0:",
                "141": "            for commit in commits:",
                "142": "                diff_edited = []",
                "143": "                # Set current line for each analysis",
                "144": "                for i in range(len(commit[\"diff\"][type])):",
                "145": "                    curr_line = commit[\"diff\"][type][i][0]",
                "146": "                    curr_content = commit[\"diff\"][type][i][1]",
                "147": "                    # In case of a starting multiline comment start adding future lines without comment symbol ",
                "148": "                    if multi_comments_enabled and curr_content.find(multi_comment_symbols[0]) != -1:",
                "149": "                            following_multi_comment = True",
                "150": "                    # In case of comment add them to existing dict if they directly follow",
                "151": "                    if curr_content.find(single_comment_symbol) == 0 or curr_content.find(single_comment_symbol + \" \") != -1 or following_multi_comment:",
                "152": "                        if len(diff_edited) > 0:",
                "153": "                            if len(diff_edited[-1][\"line_numbers\"]) == 0 or curr_line == diff_edited[-1][\"line_numbers\"][-1] + 1:",
                "154": "                                if len(diff_edited[-1][\"comments\"].keys()) > 0 and list(diff_edited[-1][\"comments\"].keys())[-1] + 1 == curr_line:",
                "155": "                                    diff_edited[-1][\"comments\"][curr_line] = curr_content",
                "156": "                                else:",
                "157": "                                    diff_edited.append({",
                "158": "                                        \"line_numbers\": [],",
                "159": "                                        \"comments\": {curr_line: curr_content},",
                "160": "                                        \"lines\": []})",
                "161": "                        else:",
                "162": "                    # or create new one",
                "163": "                            diff_edited.append({",
                "164": "                                \"line_numbers\": [],",
                "165": "                                \"comments\": {curr_line: curr_content},",
                "166": "                                \"lines\": []})",
                "167": "                    # In case of no comment add lines to existing dict if line number directly follows",
                "168": "                    else:    ",
                "169": "                        if len(diff_edited) > 0:",
                "170": "                            if len(diff_edited[-1][\"line_numbers\"]) == 0 or curr_line == diff_edited[-1][\"line_numbers\"][-1] + 1:",
                "171": "                                diff_edited[-1][\"line_numbers\"].append(curr_line)",
                "172": "                                diff_edited[-1][\"lines\"].append(curr_content)",
                "173": "                            else:",
                "174": "                    # Or create new one",
                "175": "                                diff_edited.append({",
                "176": "                                    \"line_numbers\": [curr_line],",
                "177": "                                    \"comments\": {},",
                "178": "                                    \"lines\": [curr_content]})",
                "179": "                    # Disable multiline comments when symbol found",
                "180": "                    if multi_comments_enabled and curr_content.find(multi_comment_symbols[1]) != -1:",
                "181": "                        following_multi_comment = False",
                "182": "                commit[\"diff\"][type] = diff_edited",
                "183": "    return commits_data",
                "184": "",
                "185": "def analyze_diffs(data):",
                "186": "    analysis_results = []",
                "187": "",
                "188": "    for file, commits in data.items():",
                "189": "        # Store last modified timestamps for each line",
                "190": "        last_modified = {}",
                "191": "        for commit in commits:",
                "192": "            # print(\"Starting to analyse commit: \", commit[\"commit\"])",
                "193": "            commit_time = datetime.fromisoformat(commit[\"timestamp\"])",
                "194": "            # Track modified lines",
                "195": "            for block in commit[\"diff\"][\"added\"]:",
                "196": "                for line in block[\"line_numbers\"]:",
                "197": "                    line_number = line",
                "198": "                    last_modified[line_number] = commit_time",
                "199": "            # Compare with comments",
                "200": "            for line in commit[\"comment_added_diff\"]:",
                "201": "                comment_time = datetime.fromisoformat(commit[\"timestamp\"])",
                "202": "                last_modified_lines = list(last_modified.keys())",
                "203": "                if int(line) in last_modified_lines:",
                "204": "                    for block in commit[\"diff\"][\"added\"]:",
                "205": "                        if line in block[\"comments\"] and len(block[\"line_numbers\"]) == 0:",
                "206": "                            if(comment_time > last_modified[int(line)]):",
                "207": "                                analysis_results.append({",
                "208": "                                    \"file\": file,",
                "209": "                                    \"line\": int(line),",
                "210": "                                    \"comment\": commit[\"comment_added_diff\"][line],",
                "211": "                                    \"comment_time\": str(comment_time),",
                "212": "                                    \"last_code_change_time\": str(last_modified[int(line)])",
                "213": "                                })",
                "214": "    return analysis_results"
            },
            "comments": [
                {
                    "line": 41,
                    "comment": "# Traverse through the commits in the repository",
                    "char_position_in_line": 4,
                    "type": [
                        "block"
                    ]
                },
                {
                    "line": 42,
                    "comment": "# Only save commits, that contain at least one file of the format {language_file_extension}",
                    "char_position_in_line": 4,
                    "type": [
                        "block"
                    ]
                },
                {
                    "line": 51,
                    "comment": "# Analyze each file modified in the commit",
                    "char_position_in_line": 8,
                    "type": [
                        "normal"
                    ]
                },
                {
                    "line": 53,
                    "comment": "# only store file data for Rust files",
                    "char_position_in_line": 12,
                    "type": [
                        "normal"
                    ]
                },
                {
                    "line": 71,
                    "comment": "# For added diff ispect lines filter out comments",
                    "char_position_in_line": 16,
                    "type": [
                        "normal"
                    ]
                },
                {
                    "line": 82,
                    "comment": "# For deleted diff ispect lines filter out comments",
                    "char_position_in_line": 16,
                    "type": [
                        "normal"
                    ]
                },
                {
                    "line": 99,
                    "comment": "# Generate keywords based on the commit message and type of changes",
                    "char_position_in_line": 16,
                    "type": [
                        "block"
                    ]
                },
                {
                    "line": 100,
                    "comment": "# file_data[\"keywords\"] = extract_keywords(commit.msg, modified_file)",
                    "char_position_in_line": 16,
                    "type": [
                        "block"
                    ]
                },
                {
                    "line": 101,
                    "comment": "# Extract type of commit from commit message",
                    "char_position_in_line": 16,
                    "type": [
                        "block"
                    ]
                },
                {
                    "line": 102,
                    "comment": "# file_data[\"activity\"] = extract_activity(commit.msg)",
                    "char_position_in_line": 16,
                    "type": [
                        "block"
                    ]
                },
                {
                    "line": 108,
                    "comment": "# Determine basic keywords based on the commit message",
                    "char_position_in_line": 4,
                    "type": [
                        "normal"
                    ]
                },
                {
                    "line": 121,
                    "comment": "# Use commit message keywords to determine activity type",
                    "char_position_in_line": 4,
                    "type": [
                        "normal"
                    ]
                },
                {
                    "line": 143,
                    "comment": "# Set current line for each analysis",
                    "char_position_in_line": 16,
                    "type": [
                        "normal"
                    ]
                },
                {
                    "line": 147,
                    "comment": "# In case of a starting multiline comment start adding future lines without comment symbol ",
                    "char_position_in_line": 20,
                    "type": [
                        "normal"
                    ]
                },
                {
                    "line": 150,
                    "comment": "# In case of comment add them to existing dict if they directly follow",
                    "char_position_in_line": 20,
                    "type": [
                        "normal"
                    ]
                },
                {
                    "line": 162,
                    "comment": "# or create new one",
                    "char_position_in_line": 20,
                    "type": [
                        "normal"
                    ]
                },
                {
                    "line": 167,
                    "comment": "# In case of no comment add lines to existing dict if line number directly follows",
                    "char_position_in_line": 20,
                    "type": [
                        "normal"
                    ]
                },
                {
                    "line": 174,
                    "comment": "# Or create new one",
                    "char_position_in_line": 20,
                    "type": [
                        "normal"
                    ]
                },
                {
                    "line": 179,
                    "comment": "# Disable multiline comments when symbol found",
                    "char_position_in_line": 20,
                    "type": [
                        "normal"
                    ]
                },
                {
                    "line": 189,
                    "comment": "# Store last modified timestamps for each line",
                    "char_position_in_line": 8,
                    "type": [
                        "normal"
                    ]
                },
                {
                    "line": 192,
                    "comment": "# print(\"Starting to analyse commit: \", commit[\"commit\"])",
                    "char_position_in_line": 12,
                    "type": [
                        "commented-out"
                    ]
                },
                {
                    "line": 194,
                    "comment": "# Track modified lines",
                    "char_position_in_line": 12,
                    "type": [
                        "normal"
                    ]
                },
                {
                    "line": 199,
                    "comment": "# Compare with comments",
                    "char_position_in_line": 12,
                    "type": [
                        "normal"
                    ]
                }
            ]
        },
        {
            "commit": "ca64e3c56b1e147cafb1af0d143d4f60f04cfbe2",
            "timestamp": "2024-11-28T10:16:59+01:00",
            "author": "alexander.schranner",
            "diff": {
                "added": {
                    "62": "    return activity"
                },
                "deleted": {
                    "38": "",
                    "39": "def analyze_commits(repo_url, language_file_extension, dt1, dt2, single_comment_symbol, multi_comment_symbols=[]):",
                    "40": "    files_data = {}",
                    "41": "    # Traverse through the commits in the repository",
                    "42": "    # Only save commits, that contain at least one file of the format {language_file_extension}",
                    "43": "    for commit in Repository(repo_url,",
                    "44": "    only_modifications_with_file_types=[f\".{language_file_extension}\"],",
                    "45": "    since=dt1,",
                    "46": "    to=dt2).traverse_commits():",
                    "47": "        if len(multi_comment_symbols) >= 2:",
                    "48": "            multi_comments_enabled = True",
                    "49": "        else:",
                    "50": "            multi_comments_enabled = False",
                    "51": "        # Analyze each file modified in the commit",
                    "52": "        for modified_file in commit.modified_files:",
                    "53": "            # only store file data for Rust files",
                    "54": "            if modified_file.filename not in files_data:",
                    "55": "                files_data[modified_file.filename] = []",
                    "56": "            if len(modified_file.filename.split(\".\")) == 2 and modified_file.filename.split(\".\")[1] == language_file_extension:",
                    "57": "                file_data = {",
                    "58": "                    \"commit\": commit.hash,",
                    "59": "                    \"timestamp\": commit.committer_date.isoformat(),",
                    "60": "                    \"author\": commit.author.name,",
                    "61": "                    \"commit_message\": commit.msg,",
                    "62": "                    \"additions\": modified_file.added_lines,",
                    "63": "                    \"deletions\": modified_file.deleted_lines,",
                    "64": "                    \"change_type\": modified_file.change_type.name,",
                    "65": "                    \"diff\": modified_file.diff_parsed",
                    "66": "                }",
                    "67": "                diff_added = {}",
                    "68": "                diff_deleted = {}",
                    "69": "                diff_modified = {}",
                    "70": "                following_multi_comment = False",
                    "71": "                # For added diff ispect lines filter out comments",
                    "72": "                for line in modified_file.diff_parsed[\"added\"]:",
                    "73": "                    if line[1].find(single_comment_symbol) != -1 or following_multi_comment:",
                    "74": "                        diff_added[line[0]] = line[1]",
                    "75": "                    if multi_comments_enabled and line[1].find(multi_comment_symbols[0]) != -1:",
                    "76": "                        diff_added[line[0]] = line[1]",
                    "77": "                        following_multi_comment = True",
                    "78": "                    if multi_comments_enabled and line[1].find(multi_comment_symbols[1]) != -1:",
                    "79": "                        diff_added[line[0]] = line[1]",
                    "80": "                        following_multi_comment = False",
                    "81": "                file_data[\"comment_added_diff\"] = diff_added",
                    "82": "                # For deleted diff ispect lines filter out comments",
                    "83": "                for line in modified_file.diff_parsed[\"deleted\"]:",
                    "84": "                    if line[1].find(single_comment_symbol) != -1 or following_multi_comment:",
                    "85": "                        diff_deleted[line[0]] = line[1]",
                    "86": "                        if line[0] in diff_added.keys():",
                    "87": "                            diff_modified[line[0]] = line[1]",
                    "88": "                    if multi_comments_enabled and line[1].find(multi_comment_symbols[0]) != -1:",
                    "89": "                        diff_added[line[0]] = line[1]",
                    "90": "                        following_multi_comment = True",
                    "91": "                        if line[0] in diff_added.keys():",
                    "92": "                            diff_modified[line[0]] = line[1]",
                    "93": "                    if multi_comments_enabled and line[1].find(multi_comment_symbols[1]) != -1:",
                    "94": "                        diff_added[line[0]] = line[1]",
                    "95": "                        if line[0] in diff_added.keys():",
                    "96": "                            diff_modified[line[0]] = line[1]",
                    "97": "                file_data[\"comment_deleted_diff\"] = diff_deleted",
                    "98": "                file_data[\"comment_modified_diff\"] = diff_modified",
                    "99": "                # Generate keywords based on the commit message and type of changes",
                    "100": "                # file_data[\"keywords\"] = extract_keywords(commit.msg, modified_file)",
                    "101": "                # Extract type of commit from commit message",
                    "102": "                # file_data[\"activity\"] = extract_activity(commit.msg)",
                    "103": "                if len(diff_added) + len(diff_deleted) != 0:",
                    "104": "                    files_data[modified_file.filename].append(file_data)",
                    "105": "    return files_data",
                    "106": "",
                    "131": "    return activity",
                    "132": "",
                    "133": "def pretty_diff(commits_data, type, single_comment_symbol, multi_comment_symbols=[]):",
                    "134": "    following_multi_comment = False",
                    "135": "    if len(multi_comment_symbols) >= 2:",
                    "136": "        multi_comments_enabled = True",
                    "137": "    else:",
                    "138": "        multi_comments_enabled = False",
                    "139": "    for file, commits in commits_data.items():",
                    "140": "        if len(file) > 0:",
                    "141": "            for commit in commits:",
                    "142": "                diff_edited = []",
                    "143": "                # Set current line for each analysis",
                    "144": "                for i in range(len(commit[\"diff\"][type])):",
                    "145": "                    curr_line = commit[\"diff\"][type][i][0]",
                    "146": "                    curr_content = commit[\"diff\"][type][i][1]",
                    "147": "                    # In case of a starting multiline comment start adding future lines without comment symbol",
                    "148": "                    if multi_comments_enabled and curr_content.find(multi_comment_symbols[0]) != -1:",
                    "149": "                            following_multi_comment = True",
                    "150": "                    # In case of comment add them to existing dict if they directly follow",
                    "151": "                    if curr_content.find(single_comment_symbol) == 0 or curr_content.find(single_comment_symbol + \" \") != -1 or following_multi_comment:",
                    "152": "                        if len(diff_edited) > 0:",
                    "153": "                            if len(diff_edited[-1][\"line_numbers\"]) == 0 or curr_line == diff_edited[-1][\"line_numbers\"][-1] + 1:",
                    "154": "                                if len(diff_edited[-1][\"comments\"].keys()) > 0 and list(diff_edited[-1][\"comments\"].keys())[-1] + 1 == curr_line:",
                    "155": "                                    diff_edited[-1][\"comments\"][curr_line] = curr_content",
                    "156": "                                else:",
                    "157": "                                    diff_edited.append({",
                    "158": "                                        \"line_numbers\": [],",
                    "159": "                                        \"comments\": {curr_line: curr_content},",
                    "160": "                                        \"lines\": []})",
                    "161": "                        else:",
                    "162": "                    # or create new one",
                    "163": "                            diff_edited.append({",
                    "164": "                                \"line_numbers\": [],",
                    "165": "                                \"comments\": {curr_line: curr_content},",
                    "166": "                                \"lines\": []})",
                    "167": "                    # In case of no comment add lines to existing dict if line number directly follows",
                    "168": "                    else:",
                    "169": "                        if len(diff_edited) > 0:",
                    "170": "                            if len(diff_edited[-1][\"line_numbers\"]) == 0 or curr_line == diff_edited[-1][\"line_numbers\"][-1] + 1:",
                    "171": "                                diff_edited[-1][\"line_numbers\"].append(curr_line)",
                    "172": "                                diff_edited[-1][\"lines\"].append(curr_content)",
                    "173": "                            else:",
                    "174": "                    # Or create new one",
                    "175": "                                diff_edited.append({",
                    "176": "                                    \"line_numbers\": [curr_line],",
                    "177": "                                    \"comments\": {},",
                    "178": "                                    \"lines\": [curr_content]})",
                    "179": "                    # Disable multiline comments when symbol found",
                    "180": "                    if multi_comments_enabled and curr_content.find(multi_comment_symbols[1]) != -1:",
                    "181": "                        following_multi_comment = False",
                    "182": "                commit[\"diff\"][type] = diff_edited",
                    "183": "    return commits_data",
                    "184": "",
                    "185": "def analyze_diffs(data):",
                    "186": "    analysis_results = []",
                    "187": "",
                    "188": "    for file, commits in data.items():",
                    "189": "        # Store last modified timestamps for each line",
                    "190": "        last_modified = {}",
                    "191": "        for commit in commits:",
                    "192": "            # print(\"Starting to analyse commit: \", commit[\"commit\"])",
                    "193": "            commit_time = datetime.fromisoformat(commit[\"timestamp\"])",
                    "194": "            # Track modified lines",
                    "195": "            for block in commit[\"diff\"][\"added\"]:",
                    "196": "                for line in block[\"line_numbers\"]:",
                    "197": "                    line_number = line",
                    "198": "                    last_modified[line_number] = commit_time",
                    "199": "            # Compare with comments",
                    "200": "            for line in commit[\"comment_added_diff\"]:",
                    "201": "                comment_time = datetime.fromisoformat(commit[\"timestamp\"])",
                    "202": "                last_modified_lines = list(last_modified.keys())",
                    "203": "                if int(line) in last_modified_lines:",
                    "204": "                    for block in commit[\"diff\"][\"added\"]:",
                    "205": "                        if line in block[\"comments\"] and len(block[\"line_numbers\"]) == 0:",
                    "206": "                            if(comment_time > last_modified[int(line)]):",
                    "207": "                                analysis_results.append({",
                    "208": "                                    \"file\": file,",
                    "209": "                                    \"line\": int(line),",
                    "210": "                                    \"comment\": commit[\"comment_added_diff\"][line],",
                    "211": "                                    \"comment_time\": str(comment_time),",
                    "212": "                                    \"last_code_change_time\": str(last_modified[int(line)])",
                    "213": "                                })",
                    "214": "    return analysis_results"
                }
            },
            "source_code": {
                "1": "from pydriller import Repository",
                "2": "import json",
                "3": "import pm4py",
                "4": "from datetime import datetime",
                "5": "from dateutil.relativedelta import relativedelta",
                "6": "",
                "7": "def get_commits_data(repo_path, from_date, to_date, file_types):",
                "8": "    files_data = {}",
                "9": "    for commit in Repository(   repo_path, ",
                "10": "                                since=from_date, ",
                "11": "                                to=to_date, ",
                "12": "                                only_modifications_with_file_types=file_types).traverse_commits():",
                "13": "            for file in commit.modified_files:",
                "14": "                if file.filename not in files_data and len(file.filename.split(\".\")) == 2 and \".\" + file.filename.split(\".\")[1] in file_types:",
                "15": "                    files_data[file.filename] = []",
                "16": "                if len(file.filename.split(\".\")) == 2 and \".\" + file.filename.split(\".\")[1] in file_types:",
                "17": "                    file_data = {",
                "18": "                        \"commit\": commit.hash,",
                "19": "                        \"timestamp\": commit.committer_date.isoformat(),",
                "20": "                        \"author\": commit.author.name,",
                "21": "                        \"diff\": diff_to_dict(file.diff_parsed)",
                "22": "                    }",
                "23": "                    if len(file.diff_parsed) != 0:",
                "24": "                        files_data[file.filename].append(file_data)",
                "25": "    return files_data",
                "26": "",
                "27": "def diff_to_dict(diff):",
                "28": "    dict_added = {}",
                "29": "    for line in diff[\"added\"]:",
                "30": "        dict_added[line[0]] = line[1]",
                "31": "    diff[\"added\"] = dict_added",
                "32": "    dict_deleted = {}",
                "33": "    for line in diff[\"deleted\"]:",
                "34": "        dict_deleted[line[0]] = line[1]",
                "35": "    diff[\"deleted\"] = dict_deleted",
                "36": "    return diff",
                "37": "",
                "38": "def extract_keywords(commit_message, modified_file):",
                "39": "    # Determine basic keywords based on the commit message",
                "40": "    keywords = []",
                "41": "    if \"performance\" in commit_message.lower():",
                "42": "        keywords.append(\"performance\")",
                "43": "    if \"security\" in commit_message.lower():",
                "44": "        keywords.append(\"security\")",
                "45": "    if modified_file.added_lines > modified_file.deleted_lines:",
                "46": "        keywords.append(\"expansion\")",
                "47": "    else:",
                "48": "        keywords.append(\"optimization\")",
                "49": "    return keywords",
                "50": "",
                "51": "def extract_activity(commit_message):",
                "52": "    # Use commit message keywords to determine activity type",
                "53": "    activity = \"\"",
                "54": "    if \"bug\" in commit.msg.lower() or \"fix\" in commit.msg.lower():",
                "55": "        activity = \"Bug Fix\"",
                "56": "    elif \"feature\" in commit.msg.lower() or \"add\" in commit.msg.lower():",
                "57": "        activity = \"Feature Development\"",
                "58": "    elif \"refactor\" in commit.msg.lower():",
                "59": "        activity = \"Refactoring\"",
                "60": "    else:",
                "61": "        activity = \"Other\"",
                "62": "    return activity"
            },
            "comments": [
                {
                    "line": 39,
                    "comment": "# Determine basic keywords based on the commit message",
                    "char_position_in_line": 4,
                    "type": [
                        "normal"
                    ]
                },
                {
                    "line": 52,
                    "comment": "# Use commit message keywords to determine activity type",
                    "char_position_in_line": 4,
                    "type": [
                        "normal"
                    ]
                }
            ]
        },
        {
            "commit": "202468fb39d473251ab81eb3037227cf7af47344",
            "timestamp": "2024-12-04T00:35:22+01:00",
            "author": "alexander.schranner",
            "diff": {
                "added": {
                    "6": "from build.utils import list_to_dict",
                    "15": "                if file.new_path not in files_data and len(file.filename.split(\".\")) == 2 and \".\" + file.filename.split(\".\")[1] in file_types:",
                    "16": "                    files_data[file.new_path] = []",
                    "18": "                    if file.source_code:",
                    "19": "                        source = list_to_dict(file.source_code.split(\"\\n\"))",
                    "20": "                    else:",
                    "21": "                        source = {}",
                    "26": "                        \"diff\": diff_to_dict(file.diff_parsed),",
                    "27": "                        \"source_code\": source",
                    "30": "                        files_data[file.new_path].append(file_data)"
                },
                "deleted": {
                    "14": "                if file.filename not in files_data and len(file.filename.split(\".\")) == 2 and \".\" + file.filename.split(\".\")[1] in file_types:",
                    "15": "                    files_data[file.filename] = []",
                    "21": "                        \"diff\": diff_to_dict(file.diff_parsed)",
                    "24": "                        files_data[file.filename].append(file_data)"
                }
            },
            "source_code": {
                "1": "from pydriller import Repository",
                "2": "import json",
                "3": "import pm4py",
                "4": "from datetime import datetime",
                "5": "from dateutil.relativedelta import relativedelta",
                "6": "from build.utils import list_to_dict",
                "7": "",
                "8": "def get_commits_data(repo_path, from_date, to_date, file_types):",
                "9": "    files_data = {}",
                "10": "    for commit in Repository(   repo_path, ",
                "11": "                                since=from_date, ",
                "12": "                                to=to_date, ",
                "13": "                                only_modifications_with_file_types=file_types).traverse_commits():",
                "14": "            for file in commit.modified_files:",
                "15": "                if file.new_path not in files_data and len(file.filename.split(\".\")) == 2 and \".\" + file.filename.split(\".\")[1] in file_types:",
                "16": "                    files_data[file.new_path] = []",
                "17": "                if len(file.filename.split(\".\")) == 2 and \".\" + file.filename.split(\".\")[1] in file_types:",
                "18": "                    if file.source_code:",
                "19": "                        source = list_to_dict(file.source_code.split(\"\\n\"))",
                "20": "                    else:",
                "21": "                        source = {}",
                "22": "                    file_data = {",
                "23": "                        \"commit\": commit.hash,",
                "24": "                        \"timestamp\": commit.committer_date.isoformat(),",
                "25": "                        \"author\": commit.author.name,",
                "26": "                        \"diff\": diff_to_dict(file.diff_parsed),",
                "27": "                        \"source_code\": source",
                "28": "                    }",
                "29": "                    if len(file.diff_parsed) != 0:",
                "30": "                        files_data[file.new_path].append(file_data)",
                "31": "    return files_data",
                "32": "",
                "33": "def diff_to_dict(diff):",
                "34": "    dict_added = {}",
                "35": "    for line in diff[\"added\"]:",
                "36": "        dict_added[line[0]] = line[1]",
                "37": "    diff[\"added\"] = dict_added",
                "38": "    dict_deleted = {}",
                "39": "    for line in diff[\"deleted\"]:",
                "40": "        dict_deleted[line[0]] = line[1]",
                "41": "    diff[\"deleted\"] = dict_deleted",
                "42": "    return diff",
                "43": "",
                "44": "def extract_keywords(commit_message, modified_file):",
                "45": "    # Determine basic keywords based on the commit message",
                "46": "    keywords = []",
                "47": "    if \"performance\" in commit_message.lower():",
                "48": "        keywords.append(\"performance\")",
                "49": "    if \"security\" in commit_message.lower():",
                "50": "        keywords.append(\"security\")",
                "51": "    if modified_file.added_lines > modified_file.deleted_lines:",
                "52": "        keywords.append(\"expansion\")",
                "53": "    else:",
                "54": "        keywords.append(\"optimization\")",
                "55": "    return keywords",
                "56": "",
                "57": "def extract_activity(commit_message):",
                "58": "    # Use commit message keywords to determine activity type",
                "59": "    activity = \"\"",
                "60": "    if \"bug\" in commit.msg.lower() or \"fix\" in commit.msg.lower():",
                "61": "        activity = \"Bug Fix\"",
                "62": "    elif \"feature\" in commit.msg.lower() or \"add\" in commit.msg.lower():",
                "63": "        activity = \"Feature Development\"",
                "64": "    elif \"refactor\" in commit.msg.lower():",
                "65": "        activity = \"Refactoring\"",
                "66": "    else:",
                "67": "        activity = \"Other\"",
                "68": "    return activity"
            },
            "comments": [
                {
                    "line": 45,
                    "comment": "# Determine basic keywords based on the commit message",
                    "char_position_in_line": 4,
                    "type": [
                        "normal"
                    ]
                },
                {
                    "line": 58,
                    "comment": "# Use commit message keywords to determine activity type",
                    "char_position_in_line": 4,
                    "type": [
                        "normal"
                    ]
                }
            ]
        }
    ],
    "main.py": [
        {
            "commit": "4210b185c8b05071e5e33b726c1d537cba181c6c",
            "timestamp": "2024-11-21T21:30:11+01:00",
            "author": "alexander.schranner",
            "diff": {
                "added": {
                    "1": "# Import modules",
                    "2": "from build.pydriller import get_commits_data",
                    "3": "from build.comment_lister import run_comment_lister, filter_comments_by_time",
                    "4": "",
                    "5": "# Import packages",
                    "6": "import os",
                    "7": "import json",
                    "8": "import subprocess",
                    "9": "import shutil",
                    "10": "from datetime import datetime, timezone",
                    "11": "",
                    "12": "def main():",
                    "13": "     # Convert repo URL to path by cloning repo",
                    "14": "    repo_url = \"https://github.com/AlexS-1/Bachelor-Code.git\"",
                    "15": "",
                    "16": "    repo_name = os.path.basename(repo_url).replace(\".git\", \"\")",
                    "17": "    temp_dir = \"/Users/as/Library/Mobile Documents/com~apple~CloudDocs/Dokumente/Studium/Bachelor-Thesis/tmp\"",
                    "18": "    clone_path = os.path.join(temp_dir, repo_name)",
                    "19": "",
                    "20": "    subprocess.run(['git', 'clone', repo_url, clone_path], check=True)",
                    "21": "",
                    "22": "    # Paths",
                    "23": "    repo_path = clone_path",
                    "24": "    jar_path = \"/Users/as/Library/Mobile Documents/com~apple~CloudDocs/Dokumente/Studium/Bachelor-Thesis/CommentLister/target/CommentLister.jar\"",
                    "25": "",
                    "26": "    # Setting different timeperiod",
                    "27": "    start_time = datetime.today().replace(year = datetime.today().year - 1, tzinfo=None, microsecond=0)",
                    "28": "    end_time = datetime.today().replace(microsecond=0)",
                    "29": "",
                    "30": "    commits_data = get_commits_data(repo_path, start_time, end_time)",
                    "31": "",
                    "32": "    for file, commits in commits_data.items():",
                    "33": "        for commit in commits:",
                    "34": "            tag = \"-target=\" + commit[\"commit\"]",
                    "35": "            output = run_comment_lister(repo_path, jar_path, tag)",
                    "36": "            if output is None:",
                    "37": "                return",
                    "38": "",
                    "39": "            # Parse output as JSON",
                    "40": "            try:",
                    "41": "                comment_data = json.loads(output)",
                    "42": "            except json.JSONDecodeError as e:",
                    "43": "                print(f\"Failed to parse CommentLister output: {e}\")",
                    "44": "                return",
                    "45": "",
                    "46": "            # Filter comments by time",
                    "47": "            filtered_comments = filter_comments_by_time(comment_data, start_time, end_time)",
                    "48": "            commit[\"comments\"] = filtered_comments",
                    "49": "",
                    "50": "    # Save filtered comments",
                    "51": "    with open('Data/filtered_commits_data.json', 'w') as f:",
                    "52": "        json.dump(commits, f, indent=4)",
                    "53": "",
                    "54": "",
                    "55": "",
                    "56": "    shutil.rmtree(clone_path)",
                    "57": "",
                    "58": "if __name__ == \"__main__\":",
                    "59": "    main()"
                },
                "deleted": {}
            },
            "source_code": {
                "1": "# Import modules",
                "2": "from build.pydriller import get_commits_data",
                "3": "from build.comment_lister import run_comment_lister, filter_comments_by_time",
                "4": "",
                "5": "# Import packages",
                "6": "import os",
                "7": "import json",
                "8": "import subprocess",
                "9": "import shutil",
                "10": "from datetime import datetime, timezone",
                "11": "",
                "12": "def main():",
                "13": "     # Convert repo URL to path by cloning repo",
                "14": "    repo_url = \"https://github.com/AlexS-1/Bachelor-Code.git\"",
                "15": "",
                "16": "    repo_name = os.path.basename(repo_url).replace(\".git\", \"\")",
                "17": "    temp_dir = \"/Users/as/Library/Mobile Documents/com~apple~CloudDocs/Dokumente/Studium/Bachelor-Thesis/tmp\"",
                "18": "    clone_path = os.path.join(temp_dir, repo_name)",
                "19": "",
                "20": "    subprocess.run(['git', 'clone', repo_url, clone_path], check=True)",
                "21": "",
                "22": "    # Paths",
                "23": "    repo_path = clone_path",
                "24": "    jar_path = \"/Users/as/Library/Mobile Documents/com~apple~CloudDocs/Dokumente/Studium/Bachelor-Thesis/CommentLister/target/CommentLister.jar\"",
                "25": "    ",
                "26": "    # Setting different timeperiod",
                "27": "    start_time = datetime.today().replace(year = datetime.today().year - 1, tzinfo=None, microsecond=0)",
                "28": "    end_time = datetime.today().replace(microsecond=0)",
                "29": "",
                "30": "    commits_data = get_commits_data(repo_path, start_time, end_time)",
                "31": "",
                "32": "    for file, commits in commits_data.items():",
                "33": "        for commit in commits:",
                "34": "            tag = \"-target=\" + commit[\"commit\"]",
                "35": "            output = run_comment_lister(repo_path, jar_path, tag)",
                "36": "            if output is None:",
                "37": "                return",
                "38": "",
                "39": "            # Parse output as JSON",
                "40": "            try:",
                "41": "                comment_data = json.loads(output)",
                "42": "            except json.JSONDecodeError as e:",
                "43": "                print(f\"Failed to parse CommentLister output: {e}\")",
                "44": "                return",
                "45": "",
                "46": "            # Filter comments by time",
                "47": "            filtered_comments = filter_comments_by_time(comment_data, start_time, end_time)",
                "48": "            commit[\"comments\"] = filtered_comments",
                "49": "            ",
                "50": "    # Save filtered comments",
                "51": "    with open('Data/filtered_commits_data.json', 'w') as f:",
                "52": "        json.dump(commits, f, indent=4)",
                "53": "",
                "54": "    ",
                "55": "    ",
                "56": "    shutil.rmtree(clone_path)",
                "57": "",
                "58": "if __name__ == \"__main__\":",
                "59": "    main()"
            },
            "comments": [
                {
                    "line": 1,
                    "comment": "# Import modules",
                    "char_position_in_line": 0,
                    "type": [
                        "normal"
                    ]
                },
                {
                    "line": 5,
                    "comment": "# Import packages",
                    "char_position_in_line": 0,
                    "type": [
                        "normal"
                    ]
                },
                {
                    "line": 13,
                    "comment": "# Convert repo URL to path by cloning repo",
                    "char_position_in_line": 5,
                    "type": [
                        "normal"
                    ]
                },
                {
                    "line": 22,
                    "comment": "# Paths",
                    "char_position_in_line": 4,
                    "type": [
                        "commented-out"
                    ]
                },
                {
                    "line": 26,
                    "comment": "# Setting different timeperiod",
                    "char_position_in_line": 4,
                    "type": [
                        "normal"
                    ]
                },
                {
                    "line": 39,
                    "comment": "# Parse output as JSON",
                    "char_position_in_line": 12,
                    "type": [
                        "normal"
                    ]
                },
                {
                    "line": 46,
                    "comment": "# Filter comments by time",
                    "char_position_in_line": 12,
                    "type": [
                        "normal"
                    ]
                },
                {
                    "line": 50,
                    "comment": "# Save filtered comments",
                    "char_position_in_line": 4,
                    "type": [
                        "normal"
                    ]
                }
            ]
        },
        {
            "commit": "1013981f1c0e06dc3def7ea036c0da903f2e38f6",
            "timestamp": "2024-11-22T16:53:22+01:00",
            "author": "alexander.schranner",
            "diff": {
                "added": {
                    "4": "from build.utils import save_to_json",
                    "31": "    file_types = [\".c\", \".c\", \".cc\", \".cp\", \".cpp\", \".cx\", \".cxx\", \".c+\", \".c++\", \".h\", \".hh\", \".hxx\", \".h+\", \".h++\", \".hp\", \".hpp\", \".java\", \".js\", \".cs\", \".py\", \".php\", \".rb\"]",
                    "32": "",
                    "33": "    commits_data = get_commits_data(repo_path, start_time, end_time, file_types)",
                    "34": "    save_to_json(commits_data, \"Data/commits_data.json\")",
                    "40": "            commit_hash = commit[\"commit\"]",
                    "41": "            save_to_json(output, f\"Data/{commit_hash}.json\")",
                    "55": "            save_to_json(filtered_comments, f\"Exports/{commit_hash}.json\")",
                    "56": "",
                    "57": "    # Save filtered comments on your system",
                    "58": "    save_to_json(commits_data, \"Data/filtered_commits_data.json\")",
                    "61": "",
                    "62": ""
                },
                "deleted": {
                    "30": "    commits_data = get_commits_data(repo_path, start_time, end_time)",
                    "49": "",
                    "50": "    # Save filtered comments",
                    "51": "    with open('Data/filtered_commits_data.json', 'w') as f:",
                    "52": "        json.dump(commits, f, indent=4)",
                    "53": "",
                    "54": "",
                    "55": ""
                }
            },
            "source_code": {
                "1": "# Import modules",
                "2": "from build.pydriller import get_commits_data",
                "3": "from build.comment_lister import run_comment_lister, filter_comments_by_time",
                "4": "from build.utils import save_to_json",
                "5": "",
                "6": "# Import packages",
                "7": "import os",
                "8": "import json",
                "9": "import subprocess",
                "10": "import shutil",
                "11": "from datetime import datetime, timezone",
                "12": "",
                "13": "def main():",
                "14": "     # Convert repo URL to path by cloning repo",
                "15": "    repo_url = \"https://github.com/AlexS-1/Bachelor-Code.git\"",
                "16": "",
                "17": "    repo_name = os.path.basename(repo_url).replace(\".git\", \"\")",
                "18": "    temp_dir = \"/Users/as/Library/Mobile Documents/com~apple~CloudDocs/Dokumente/Studium/Bachelor-Thesis/tmp\"",
                "19": "    clone_path = os.path.join(temp_dir, repo_name)",
                "20": "",
                "21": "    subprocess.run(['git', 'clone', repo_url, clone_path], check=True)",
                "22": "",
                "23": "    # Paths",
                "24": "    repo_path = clone_path",
                "25": "    jar_path = \"/Users/as/Library/Mobile Documents/com~apple~CloudDocs/Dokumente/Studium/Bachelor-Thesis/CommentLister/target/CommentLister.jar\"",
                "26": "    ",
                "27": "    # Setting different timeperiod",
                "28": "    start_time = datetime.today().replace(year = datetime.today().year - 1, tzinfo=None, microsecond=0)",
                "29": "    end_time = datetime.today().replace(microsecond=0)",
                "30": "",
                "31": "    file_types = [\".c\", \".c\", \".cc\", \".cp\", \".cpp\", \".cx\", \".cxx\", \".c+\", \".c++\", \".h\", \".hh\", \".hxx\", \".h+\", \".h++\", \".hp\", \".hpp\", \".java\", \".js\", \".cs\", \".py\", \".php\", \".rb\"]",
                "32": "",
                "33": "    commits_data = get_commits_data(repo_path, start_time, end_time, file_types)",
                "34": "    save_to_json(commits_data, \"Data/commits_data.json\")",
                "35": "",
                "36": "    for file, commits in commits_data.items():",
                "37": "        for commit in commits:",
                "38": "            tag = \"-target=\" + commit[\"commit\"]",
                "39": "            output = run_comment_lister(repo_path, jar_path, tag)",
                "40": "            commit_hash = commit[\"commit\"]",
                "41": "            save_to_json(output, f\"Data/{commit_hash}.json\")",
                "42": "            if output is None:",
                "43": "                return",
                "44": "",
                "45": "            # Parse output as JSON",
                "46": "            try:",
                "47": "                comment_data = json.loads(output)",
                "48": "            except json.JSONDecodeError as e:",
                "49": "                print(f\"Failed to parse CommentLister output: {e}\")",
                "50": "                return",
                "51": "",
                "52": "            # Filter comments by time",
                "53": "            filtered_comments = filter_comments_by_time(comment_data, start_time, end_time)",
                "54": "            commit[\"comments\"] = filtered_comments",
                "55": "            save_to_json(filtered_comments, f\"Exports/{commit_hash}.json\")",
                "56": "        ",
                "57": "    # Save filtered comments on your system",
                "58": "    save_to_json(commits_data, \"Data/filtered_commits_data.json\")",
                "59": "    shutil.rmtree(clone_path)",
                "60": "",
                "61": "",
                "62": "",
                "63": "if __name__ == \"__main__\":",
                "64": "    main()"
            },
            "comments": [
                {
                    "line": 1,
                    "comment": "# Import modules",
                    "char_position_in_line": 0,
                    "type": [
                        "normal"
                    ]
                },
                {
                    "line": 6,
                    "comment": "# Import packages",
                    "char_position_in_line": 0,
                    "type": [
                        "normal"
                    ]
                },
                {
                    "line": 14,
                    "comment": "# Convert repo URL to path by cloning repo",
                    "char_position_in_line": 5,
                    "type": [
                        "normal"
                    ]
                },
                {
                    "line": 23,
                    "comment": "# Paths",
                    "char_position_in_line": 4,
                    "type": [
                        "commented-out"
                    ]
                },
                {
                    "line": 27,
                    "comment": "# Setting different timeperiod",
                    "char_position_in_line": 4,
                    "type": [
                        "normal"
                    ]
                },
                {
                    "line": 45,
                    "comment": "# Parse output as JSON",
                    "char_position_in_line": 12,
                    "type": [
                        "normal"
                    ]
                },
                {
                    "line": 52,
                    "comment": "# Filter comments by time",
                    "char_position_in_line": 12,
                    "type": [
                        "normal"
                    ]
                },
                {
                    "line": 57,
                    "comment": "# Save filtered comments on your system",
                    "char_position_in_line": 4,
                    "type": [
                        "normal"
                    ]
                }
            ]
        },
        {
            "commit": "5c23ad83451ad8bade1900f2f5ca41afdd3b4c71",
            "timestamp": "2024-11-23T21:32:59+01:00",
            "author": "alexander.schranner",
            "diff": {
                "added": {
                    "5": "from build.analysis import analyse_diff_comments",
                    "15": "    # Convert repo URL to path by cloning repo",
                    "24": "    # # Paths",
                    "28": "    # # Setting different timeperiod",
                    "36": "",
                    "37": "    with open (\"Data/commits_data.json\", \"r\") as json_file:",
                    "38": "        commits_data = json.load(json_file)",
                    "56": "    with open(\"Data/filtered_commits_data.json\", \"r\") as json_file:",
                    "57": "        data = json.load(json_file)",
                    "58": "    analyse_diff_comments(data)",
                    "59": "    save_to_json(data, \"Exports/filtered_comments_data.json\")"
                },
                "deleted": {
                    "14": "     # Convert repo URL to path by cloning repo",
                    "23": "    # Paths",
                    "27": "    # Setting different timeperiod",
                    "40": "            commit_hash = commit[\"commit\"]",
                    "41": "            save_to_json(output, f\"Data/{commit_hash}.json\")",
                    "42": "            if output is None:",
                    "43": "                return",
                    "44": "",
                    "51": "",
                    "55": "            save_to_json(filtered_comments, f\"Exports/{commit_hash}.json\")",
                    "56": "",
                    "60": "",
                    "61": ""
                }
            },
            "source_code": {
                "1": "# Import modules",
                "2": "from build.pydriller import get_commits_data",
                "3": "from build.comment_lister import run_comment_lister, filter_comments_by_time",
                "4": "from build.utils import save_to_json",
                "5": "from build.analysis import analyse_diff_comments",
                "6": "",
                "7": "# Import packages",
                "8": "import os",
                "9": "import json",
                "10": "import subprocess",
                "11": "import shutil",
                "12": "from datetime import datetime, timezone",
                "13": "",
                "14": "def main():",
                "15": "    # Convert repo URL to path by cloning repo",
                "16": "    repo_url = \"https://github.com/AlexS-1/Bachelor-Code.git\"",
                "17": "",
                "18": "    repo_name = os.path.basename(repo_url).replace(\".git\", \"\")",
                "19": "    temp_dir = \"/Users/as/Library/Mobile Documents/com~apple~CloudDocs/Dokumente/Studium/Bachelor-Thesis/tmp\"",
                "20": "    clone_path = os.path.join(temp_dir, repo_name)",
                "21": "",
                "22": "    subprocess.run(['git', 'clone', repo_url, clone_path], check=True)",
                "23": "",
                "24": "    # # Paths",
                "25": "    repo_path = clone_path",
                "26": "    jar_path = \"/Users/as/Library/Mobile Documents/com~apple~CloudDocs/Dokumente/Studium/Bachelor-Thesis/CommentLister/target/CommentLister.jar\"",
                "27": "    ",
                "28": "    # # Setting different timeperiod",
                "29": "    start_time = datetime.today().replace(year = datetime.today().year - 1, tzinfo=None, microsecond=0)",
                "30": "    end_time = datetime.today().replace(microsecond=0)",
                "31": "",
                "32": "    file_types = [\".c\", \".c\", \".cc\", \".cp\", \".cpp\", \".cx\", \".cxx\", \".c+\", \".c++\", \".h\", \".hh\", \".hxx\", \".h+\", \".h++\", \".hp\", \".hpp\", \".java\", \".js\", \".cs\", \".py\", \".php\", \".rb\"]",
                "33": "",
                "34": "    commits_data = get_commits_data(repo_path, start_time, end_time, file_types)",
                "35": "    save_to_json(commits_data, \"Data/commits_data.json\")",
                "36": "    ",
                "37": "    with open (\"Data/commits_data.json\", \"r\") as json_file:",
                "38": "        commits_data = json.load(json_file)",
                "39": "",
                "40": "    for file, commits in commits_data.items():",
                "41": "        for commit in commits:",
                "42": "            tag = \"-target=\" + commit[\"commit\"]",
                "43": "            output = run_comment_lister(repo_path, jar_path, tag)",
                "44": "            # Parse output as JSON",
                "45": "            try:",
                "46": "                comment_data = json.loads(output)",
                "47": "            except json.JSONDecodeError as e:",
                "48": "                print(f\"Failed to parse CommentLister output: {e}\")",
                "49": "                return",
                "50": "            # Filter comments by time",
                "51": "            filtered_comments = filter_comments_by_time(comment_data, start_time, end_time)",
                "52": "            commit[\"comments\"] = filtered_comments",
                "53": "    # Save filtered comments on your system",
                "54": "    save_to_json(commits_data, \"Data/filtered_commits_data.json\")",
                "55": "    shutil.rmtree(clone_path)",
                "56": "    with open(\"Data/filtered_commits_data.json\", \"r\") as json_file:",
                "57": "        data = json.load(json_file)",
                "58": "    analyse_diff_comments(data)",
                "59": "    save_to_json(data, \"Exports/filtered_comments_data.json\")",
                "60": "",
                "61": "if __name__ == \"__main__\":",
                "62": "    main()"
            },
            "comments": [
                {
                    "line": 1,
                    "comment": "# Import modules",
                    "char_position_in_line": 0,
                    "type": [
                        "normal"
                    ]
                },
                {
                    "line": 7,
                    "comment": "# Import packages",
                    "char_position_in_line": 0,
                    "type": [
                        "normal"
                    ]
                },
                {
                    "line": 15,
                    "comment": "# Convert repo URL to path by cloning repo",
                    "char_position_in_line": 4,
                    "type": [
                        "normal"
                    ]
                },
                {
                    "line": 24,
                    "comment": "# # Paths",
                    "char_position_in_line": 4,
                    "type": [
                        "commented-out"
                    ]
                },
                {
                    "line": 28,
                    "comment": "# # Setting different timeperiod",
                    "char_position_in_line": 4,
                    "type": [
                        "commented-out"
                    ]
                },
                {
                    "line": 44,
                    "comment": "# Parse output as JSON",
                    "char_position_in_line": 12,
                    "type": [
                        "normal"
                    ]
                },
                {
                    "line": 50,
                    "comment": "# Filter comments by time",
                    "char_position_in_line": 12,
                    "type": [
                        "normal"
                    ]
                },
                {
                    "line": 53,
                    "comment": "# Save filtered comments on your system",
                    "char_position_in_line": 4,
                    "type": [
                        "normal"
                    ]
                }
            ]
        },
        {
            "commit": "b10794953bf2b307859821a8354d3429d710e31b",
            "timestamp": "2024-11-24T18:37:49+01:00",
            "author": "alexander.schranner",
            "diff": {
                "added": {
                    "5": "from build.analysis import analyse_diff_comments, blockify_comments",
                    "51": "            commit_hash, filtered_comments = filter_comments_by_time(comment_data, start_time, end_time)",
                    "52": "            if commit[\"commit\"] == commit_hash:",
                    "53": "                commit[\"comments\"] = filtered_comments",
                    "54": "            else:",
                    "55": "                print(\"mismatch in commit and comment data\")",
                    "62": "    blockify_comments(data)",
                    "63": "    save_to_json(data, \"Exports/blockified_comments_data.json\")"
                },
                "deleted": {
                    "5": "from build.analysis import analyse_diff_comments",
                    "51": "            filtered_comments = filter_comments_by_time(comment_data, start_time, end_time)",
                    "52": "            commit[\"comments\"] = filtered_comments",
                    "59": "    save_to_json(data, \"Exports/filtered_comments_data.json\")"
                }
            },
            "source_code": {
                "1": "# Import modules",
                "2": "from build.pydriller import get_commits_data",
                "3": "from build.comment_lister import run_comment_lister, filter_comments_by_time",
                "4": "from build.utils import save_to_json",
                "5": "from build.analysis import analyse_diff_comments, blockify_comments",
                "6": "",
                "7": "# Import packages",
                "8": "import os",
                "9": "import json",
                "10": "import subprocess",
                "11": "import shutil",
                "12": "from datetime import datetime, timezone",
                "13": "",
                "14": "def main():",
                "15": "    # Convert repo URL to path by cloning repo",
                "16": "    repo_url = \"https://github.com/AlexS-1/Bachelor-Code.git\"",
                "17": "",
                "18": "    repo_name = os.path.basename(repo_url).replace(\".git\", \"\")",
                "19": "    temp_dir = \"/Users/as/Library/Mobile Documents/com~apple~CloudDocs/Dokumente/Studium/Bachelor-Thesis/tmp\"",
                "20": "    clone_path = os.path.join(temp_dir, repo_name)",
                "21": "",
                "22": "    subprocess.run(['git', 'clone', repo_url, clone_path], check=True)",
                "23": "",
                "24": "    # # Paths",
                "25": "    repo_path = clone_path",
                "26": "    jar_path = \"/Users/as/Library/Mobile Documents/com~apple~CloudDocs/Dokumente/Studium/Bachelor-Thesis/CommentLister/target/CommentLister.jar\"",
                "27": "    ",
                "28": "    # # Setting different timeperiod",
                "29": "    start_time = datetime.today().replace(year = datetime.today().year - 1, tzinfo=None, microsecond=0)",
                "30": "    end_time = datetime.today().replace(microsecond=0)",
                "31": "",
                "32": "    file_types = [\".c\", \".c\", \".cc\", \".cp\", \".cpp\", \".cx\", \".cxx\", \".c+\", \".c++\", \".h\", \".hh\", \".hxx\", \".h+\", \".h++\", \".hp\", \".hpp\", \".java\", \".js\", \".cs\", \".py\", \".php\", \".rb\"]",
                "33": "",
                "34": "    commits_data = get_commits_data(repo_path, start_time, end_time, file_types)",
                "35": "    save_to_json(commits_data, \"Data/commits_data.json\")",
                "36": "    ",
                "37": "    with open (\"Data/commits_data.json\", \"r\") as json_file:",
                "38": "        commits_data = json.load(json_file)",
                "39": "",
                "40": "    for file, commits in commits_data.items():",
                "41": "        for commit in commits:",
                "42": "            tag = \"-target=\" + commit[\"commit\"]",
                "43": "            output = run_comment_lister(repo_path, jar_path, tag)",
                "44": "            # Parse output as JSON",
                "45": "            try:",
                "46": "                comment_data = json.loads(output)",
                "47": "            except json.JSONDecodeError as e:",
                "48": "                print(f\"Failed to parse CommentLister output: {e}\")",
                "49": "                return",
                "50": "            # Filter comments by time",
                "51": "            commit_hash, filtered_comments = filter_comments_by_time(comment_data, start_time, end_time)",
                "52": "            if commit[\"commit\"] == commit_hash:",
                "53": "                commit[\"comments\"] = filtered_comments",
                "54": "            else:",
                "55": "                print(\"mismatch in commit and comment data\")",
                "56": "    # Save filtered comments on your system",
                "57": "    save_to_json(commits_data, \"Data/filtered_commits_data.json\")",
                "58": "    shutil.rmtree(clone_path)",
                "59": "    with open(\"Data/filtered_commits_data.json\", \"r\") as json_file:",
                "60": "        data = json.load(json_file)",
                "61": "    analyse_diff_comments(data)",
                "62": "    blockify_comments(data)",
                "63": "    save_to_json(data, \"Exports/blockified_comments_data.json\")",
                "64": "",
                "65": "if __name__ == \"__main__\":",
                "66": "    main()"
            },
            "comments": [
                {
                    "line": 1,
                    "comment": "# Import modules",
                    "char_position_in_line": 0,
                    "type": [
                        "normal"
                    ]
                },
                {
                    "line": 7,
                    "comment": "# Import packages",
                    "char_position_in_line": 0,
                    "type": [
                        "normal"
                    ]
                },
                {
                    "line": 15,
                    "comment": "# Convert repo URL to path by cloning repo",
                    "char_position_in_line": 4,
                    "type": [
                        "normal"
                    ]
                },
                {
                    "line": 24,
                    "comment": "# # Paths",
                    "char_position_in_line": 4,
                    "type": [
                        "commented-out"
                    ]
                },
                {
                    "line": 28,
                    "comment": "# # Setting different timeperiod",
                    "char_position_in_line": 4,
                    "type": [
                        "commented-out"
                    ]
                },
                {
                    "line": 44,
                    "comment": "# Parse output as JSON",
                    "char_position_in_line": 12,
                    "type": [
                        "normal"
                    ]
                },
                {
                    "line": 50,
                    "comment": "# Filter comments by time",
                    "char_position_in_line": 12,
                    "type": [
                        "normal"
                    ]
                },
                {
                    "line": 56,
                    "comment": "# Save filtered comments on your system",
                    "char_position_in_line": 4,
                    "type": [
                        "normal"
                    ]
                }
            ]
        },
        {
            "commit": "e20d03792161ba1b90725e6912b40275f06bf2da",
            "timestamp": "2024-11-25T02:54:47+01:00",
            "author": "alexander.schranner",
            "diff": {
                "added": {
                    "5": "from build.analysis import analyse_diff_comments, blockify_comments, blockify_comments2, extract_later_modified_comments",
                    "64": "    with open(\"Exports/blockified_comments_data.json\", \"r\") as json_file:",
                    "65": "        data = json.load(json_file)",
                    "66": "    blockify_comments2(data)",
                    "67": "    save_to_json(data, \"Exports/blockified_comments2_data.json\")",
                    "68": "    with open(\"Exports/blockified_comments2_data.json\", \"r\") as json_file:",
                    "69": "        data = json.load(json_file)",
                    "70": "    d = extract_later_modified_comments(data)",
                    "71": "    save_to_json(d, \"Exports/analysis_results.json\")"
                },
                "deleted": {
                    "5": "from build.analysis import analyse_diff_comments, blockify_comments",
                    "64": ""
                }
            },
            "source_code": {
                "1": "# Import modules",
                "2": "from build.pydriller import get_commits_data",
                "3": "from build.comment_lister import run_comment_lister, filter_comments_by_time",
                "4": "from build.utils import save_to_json",
                "5": "from build.analysis import analyse_diff_comments, blockify_comments, blockify_comments2, extract_later_modified_comments",
                "6": "",
                "7": "# Import packages",
                "8": "import os",
                "9": "import json",
                "10": "import subprocess",
                "11": "import shutil",
                "12": "from datetime import datetime, timezone",
                "13": "",
                "14": "def main():",
                "15": "    # Convert repo URL to path by cloning repo",
                "16": "    repo_url = \"https://github.com/AlexS-1/Bachelor-Code.git\"",
                "17": "",
                "18": "    repo_name = os.path.basename(repo_url).replace(\".git\", \"\")",
                "19": "    temp_dir = \"/Users/as/Library/Mobile Documents/com~apple~CloudDocs/Dokumente/Studium/Bachelor-Thesis/tmp\"",
                "20": "    clone_path = os.path.join(temp_dir, repo_name)",
                "21": "",
                "22": "    subprocess.run(['git', 'clone', repo_url, clone_path], check=True)",
                "23": "",
                "24": "    # # Paths",
                "25": "    repo_path = clone_path",
                "26": "    jar_path = \"/Users/as/Library/Mobile Documents/com~apple~CloudDocs/Dokumente/Studium/Bachelor-Thesis/CommentLister/target/CommentLister.jar\"",
                "27": "    ",
                "28": "    # # Setting different timeperiod",
                "29": "    start_time = datetime.today().replace(year = datetime.today().year - 1, tzinfo=None, microsecond=0)",
                "30": "    end_time = datetime.today().replace(microsecond=0)",
                "31": "",
                "32": "    file_types = [\".c\", \".c\", \".cc\", \".cp\", \".cpp\", \".cx\", \".cxx\", \".c+\", \".c++\", \".h\", \".hh\", \".hxx\", \".h+\", \".h++\", \".hp\", \".hpp\", \".java\", \".js\", \".cs\", \".py\", \".php\", \".rb\"]",
                "33": "",
                "34": "    commits_data = get_commits_data(repo_path, start_time, end_time, file_types)",
                "35": "    save_to_json(commits_data, \"Data/commits_data.json\")",
                "36": "    ",
                "37": "    with open (\"Data/commits_data.json\", \"r\") as json_file:",
                "38": "        commits_data = json.load(json_file)",
                "39": "",
                "40": "    for file, commits in commits_data.items():",
                "41": "        for commit in commits:",
                "42": "            tag = \"-target=\" + commit[\"commit\"]",
                "43": "            output = run_comment_lister(repo_path, jar_path, tag)",
                "44": "            # Parse output as JSON",
                "45": "            try:",
                "46": "                comment_data = json.loads(output)",
                "47": "            except json.JSONDecodeError as e:",
                "48": "                print(f\"Failed to parse CommentLister output: {e}\")",
                "49": "                return",
                "50": "            # Filter comments by time",
                "51": "            commit_hash, filtered_comments = filter_comments_by_time(comment_data, start_time, end_time)",
                "52": "            if commit[\"commit\"] == commit_hash:",
                "53": "                commit[\"comments\"] = filtered_comments",
                "54": "            else:",
                "55": "                print(\"mismatch in commit and comment data\")",
                "56": "    # Save filtered comments on your system",
                "57": "    save_to_json(commits_data, \"Data/filtered_commits_data.json\")",
                "58": "    shutil.rmtree(clone_path)",
                "59": "    with open(\"Data/filtered_commits_data.json\", \"r\") as json_file:",
                "60": "        data = json.load(json_file)",
                "61": "    analyse_diff_comments(data)",
                "62": "    blockify_comments(data)",
                "63": "    save_to_json(data, \"Exports/blockified_comments_data.json\")",
                "64": "    with open(\"Exports/blockified_comments_data.json\", \"r\") as json_file:",
                "65": "        data = json.load(json_file)",
                "66": "    blockify_comments2(data)",
                "67": "    save_to_json(data, \"Exports/blockified_comments2_data.json\")",
                "68": "    with open(\"Exports/blockified_comments2_data.json\", \"r\") as json_file:",
                "69": "        data = json.load(json_file)",
                "70": "    d = extract_later_modified_comments(data)",
                "71": "    save_to_json(d, \"Exports/analysis_results.json\")",
                "72": "if __name__ == \"__main__\":",
                "73": "    main()"
            },
            "comments": [
                {
                    "line": 1,
                    "comment": "# Import modules",
                    "char_position_in_line": 0,
                    "type": [
                        "normal"
                    ]
                },
                {
                    "line": 7,
                    "comment": "# Import packages",
                    "char_position_in_line": 0,
                    "type": [
                        "normal"
                    ]
                },
                {
                    "line": 15,
                    "comment": "# Convert repo URL to path by cloning repo",
                    "char_position_in_line": 4,
                    "type": [
                        "normal"
                    ]
                },
                {
                    "line": 24,
                    "comment": "# # Paths",
                    "char_position_in_line": 4,
                    "type": [
                        "commented-out"
                    ]
                },
                {
                    "line": 28,
                    "comment": "# # Setting different timeperiod",
                    "char_position_in_line": 4,
                    "type": [
                        "commented-out"
                    ]
                },
                {
                    "line": 44,
                    "comment": "# Parse output as JSON",
                    "char_position_in_line": 12,
                    "type": [
                        "normal"
                    ]
                },
                {
                    "line": 50,
                    "comment": "# Filter comments by time",
                    "char_position_in_line": 12,
                    "type": [
                        "normal"
                    ]
                },
                {
                    "line": 56,
                    "comment": "# Save filtered comments on your system",
                    "char_position_in_line": 4,
                    "type": [
                        "normal"
                    ]
                }
            ]
        },
        {
            "commit": "6f599e9f229a41cfe89bc628bb60c7565944a0ad",
            "timestamp": "2024-11-25T11:26:40+01:00",
            "author": "alexander.schranner",
            "diff": {
                "added": {
                    "5": "from build.analysis import analyse_diff_comments, blockify_comments, blockify_comments2, extract_later_modified_comments, clean",
                    "16": "    repo_url = \"https://github.com/dani-garcia/vaultwarden\"",
                    "72": "    with open(\"Exports/analysis_results.json\", \"r\") as json_file:",
                    "73": "        data = json.load(json_file)",
                    "74": "    d = clean(data)",
                    "75": "    save_to_json(d, \"Exports/analysis_results.json\")"
                },
                "deleted": {
                    "5": "from build.analysis import analyse_diff_comments, blockify_comments, blockify_comments2, extract_later_modified_comments",
                    "16": "    repo_url = \"https://github.com/AlexS-1/Bachelor-Code.git\""
                }
            },
            "source_code": {
                "1": "# Import modules",
                "2": "from build.pydriller import get_commits_data",
                "3": "from build.comment_lister import run_comment_lister, filter_comments_by_time",
                "4": "from build.utils import save_to_json",
                "5": "from build.analysis import analyse_diff_comments, blockify_comments, blockify_comments2, extract_later_modified_comments, clean",
                "6": "",
                "7": "# Import packages",
                "8": "import os",
                "9": "import json",
                "10": "import subprocess",
                "11": "import shutil",
                "12": "from datetime import datetime, timezone",
                "13": "",
                "14": "def main():",
                "15": "    # Convert repo URL to path by cloning repo",
                "16": "    repo_url = \"https://github.com/dani-garcia/vaultwarden\"",
                "17": "",
                "18": "    repo_name = os.path.basename(repo_url).replace(\".git\", \"\")",
                "19": "    temp_dir = \"/Users/as/Library/Mobile Documents/com~apple~CloudDocs/Dokumente/Studium/Bachelor-Thesis/tmp\"",
                "20": "    clone_path = os.path.join(temp_dir, repo_name)",
                "21": "",
                "22": "    subprocess.run(['git', 'clone', repo_url, clone_path], check=True)",
                "23": "",
                "24": "    # # Paths",
                "25": "    repo_path = clone_path",
                "26": "    jar_path = \"/Users/as/Library/Mobile Documents/com~apple~CloudDocs/Dokumente/Studium/Bachelor-Thesis/CommentLister/target/CommentLister.jar\"",
                "27": "    ",
                "28": "    # # Setting different timeperiod",
                "29": "    start_time = datetime.today().replace(year = datetime.today().year - 1, tzinfo=None, microsecond=0)",
                "30": "    end_time = datetime.today().replace(microsecond=0)",
                "31": "",
                "32": "    file_types = [\".c\", \".c\", \".cc\", \".cp\", \".cpp\", \".cx\", \".cxx\", \".c+\", \".c++\", \".h\", \".hh\", \".hxx\", \".h+\", \".h++\", \".hp\", \".hpp\", \".java\", \".js\", \".cs\", \".py\", \".php\", \".rb\"]",
                "33": "",
                "34": "    commits_data = get_commits_data(repo_path, start_time, end_time, file_types)",
                "35": "    save_to_json(commits_data, \"Data/commits_data.json\")",
                "36": "    ",
                "37": "    with open (\"Data/commits_data.json\", \"r\") as json_file:",
                "38": "        commits_data = json.load(json_file)",
                "39": "",
                "40": "    for file, commits in commits_data.items():",
                "41": "        for commit in commits:",
                "42": "            tag = \"-target=\" + commit[\"commit\"]",
                "43": "            output = run_comment_lister(repo_path, jar_path, tag)",
                "44": "            # Parse output as JSON",
                "45": "            try:",
                "46": "                comment_data = json.loads(output)",
                "47": "            except json.JSONDecodeError as e:",
                "48": "                print(f\"Failed to parse CommentLister output: {e}\")",
                "49": "                return",
                "50": "            # Filter comments by time",
                "51": "            commit_hash, filtered_comments = filter_comments_by_time(comment_data, start_time, end_time)",
                "52": "            if commit[\"commit\"] == commit_hash:",
                "53": "                commit[\"comments\"] = filtered_comments",
                "54": "            else:",
                "55": "                print(\"mismatch in commit and comment data\")",
                "56": "    # Save filtered comments on your system",
                "57": "    save_to_json(commits_data, \"Data/filtered_commits_data.json\")",
                "58": "    shutil.rmtree(clone_path)",
                "59": "    with open(\"Data/filtered_commits_data.json\", \"r\") as json_file:",
                "60": "        data = json.load(json_file)",
                "61": "    analyse_diff_comments(data)",
                "62": "    blockify_comments(data)",
                "63": "    save_to_json(data, \"Exports/blockified_comments_data.json\")",
                "64": "    with open(\"Exports/blockified_comments_data.json\", \"r\") as json_file:",
                "65": "        data = json.load(json_file)",
                "66": "    blockify_comments2(data)",
                "67": "    save_to_json(data, \"Exports/blockified_comments2_data.json\")",
                "68": "    with open(\"Exports/blockified_comments2_data.json\", \"r\") as json_file:",
                "69": "        data = json.load(json_file)",
                "70": "    d = extract_later_modified_comments(data)",
                "71": "    save_to_json(d, \"Exports/analysis_results.json\")",
                "72": "    with open(\"Exports/analysis_results.json\", \"r\") as json_file:",
                "73": "        data = json.load(json_file)",
                "74": "    d = clean(data)",
                "75": "    save_to_json(d, \"Exports/analysis_results.json\")",
                "76": "if __name__ == \"__main__\":",
                "77": "    main()"
            },
            "comments": [
                {
                    "line": 1,
                    "comment": "# Import modules",
                    "char_position_in_line": 0,
                    "type": [
                        "normal"
                    ]
                },
                {
                    "line": 7,
                    "comment": "# Import packages",
                    "char_position_in_line": 0,
                    "type": [
                        "normal"
                    ]
                },
                {
                    "line": 15,
                    "comment": "# Convert repo URL to path by cloning repo",
                    "char_position_in_line": 4,
                    "type": [
                        "normal"
                    ]
                },
                {
                    "line": 24,
                    "comment": "# # Paths",
                    "char_position_in_line": 4,
                    "type": [
                        "commented-out"
                    ]
                },
                {
                    "line": 28,
                    "comment": "# # Setting different timeperiod",
                    "char_position_in_line": 4,
                    "type": [
                        "commented-out"
                    ]
                },
                {
                    "line": 44,
                    "comment": "# Parse output as JSON",
                    "char_position_in_line": 12,
                    "type": [
                        "normal"
                    ]
                },
                {
                    "line": 50,
                    "comment": "# Filter comments by time",
                    "char_position_in_line": 12,
                    "type": [
                        "normal"
                    ]
                },
                {
                    "line": 56,
                    "comment": "# Save filtered comments on your system",
                    "char_position_in_line": 4,
                    "type": [
                        "normal"
                    ]
                }
            ]
        },
        {
            "commit": "ca64e3c56b1e147cafb1af0d143d4f60f04cfbe2",
            "timestamp": "2024-11-28T10:16:59+01:00",
            "author": "alexander.schranner",
            "diff": {
                "added": {
                    "5": "from build.analysis import analyse_diff_comments, blockify_comments, blockify_comments2, extract_later_modified_comments, clean, average_comment_update_time",
                    "76": "    print(\"Average duration:\", average_comment_update_time(d))",
                    "77": ""
                },
                "deleted": {
                    "5": "from build.analysis import analyse_diff_comments, blockify_comments, blockify_comments2, extract_later_modified_comments, clean"
                }
            },
            "source_code": {
                "1": "# Import modules",
                "2": "from build.pydriller import get_commits_data",
                "3": "from build.comment_lister import run_comment_lister, filter_comments_by_time",
                "4": "from build.utils import save_to_json",
                "5": "from build.analysis import analyse_diff_comments, blockify_comments, blockify_comments2, extract_later_modified_comments, clean, average_comment_update_time",
                "6": "",
                "7": "# Import packages",
                "8": "import os",
                "9": "import json",
                "10": "import subprocess",
                "11": "import shutil",
                "12": "from datetime import datetime, timezone",
                "13": "",
                "14": "def main():",
                "15": "    # Convert repo URL to path by cloning repo",
                "16": "    repo_url = \"https://github.com/dani-garcia/vaultwarden\"",
                "17": "",
                "18": "    repo_name = os.path.basename(repo_url).replace(\".git\", \"\")",
                "19": "    temp_dir = \"/Users/as/Library/Mobile Documents/com~apple~CloudDocs/Dokumente/Studium/Bachelor-Thesis/tmp\"",
                "20": "    clone_path = os.path.join(temp_dir, repo_name)",
                "21": "",
                "22": "    subprocess.run(['git', 'clone', repo_url, clone_path], check=True)",
                "23": "",
                "24": "    # # Paths",
                "25": "    repo_path = clone_path",
                "26": "    jar_path = \"/Users/as/Library/Mobile Documents/com~apple~CloudDocs/Dokumente/Studium/Bachelor-Thesis/CommentLister/target/CommentLister.jar\"",
                "27": "    ",
                "28": "    # # Setting different timeperiod",
                "29": "    start_time = datetime.today().replace(year = datetime.today().year - 1, tzinfo=None, microsecond=0)",
                "30": "    end_time = datetime.today().replace(microsecond=0)",
                "31": "",
                "32": "    file_types = [\".c\", \".c\", \".cc\", \".cp\", \".cpp\", \".cx\", \".cxx\", \".c+\", \".c++\", \".h\", \".hh\", \".hxx\", \".h+\", \".h++\", \".hp\", \".hpp\", \".java\", \".js\", \".cs\", \".py\", \".php\", \".rb\"]",
                "33": "",
                "34": "    commits_data = get_commits_data(repo_path, start_time, end_time, file_types)",
                "35": "    save_to_json(commits_data, \"Data/commits_data.json\")",
                "36": "    ",
                "37": "    with open (\"Data/commits_data.json\", \"r\") as json_file:",
                "38": "        commits_data = json.load(json_file)",
                "39": "",
                "40": "    for file, commits in commits_data.items():",
                "41": "        for commit in commits:",
                "42": "            tag = \"-target=\" + commit[\"commit\"]",
                "43": "            output = run_comment_lister(repo_path, jar_path, tag)",
                "44": "            # Parse output as JSON",
                "45": "            try:",
                "46": "                comment_data = json.loads(output)",
                "47": "            except json.JSONDecodeError as e:",
                "48": "                print(f\"Failed to parse CommentLister output: {e}\")",
                "49": "                return",
                "50": "            # Filter comments by time",
                "51": "            commit_hash, filtered_comments = filter_comments_by_time(comment_data, start_time, end_time)",
                "52": "            if commit[\"commit\"] == commit_hash:",
                "53": "                commit[\"comments\"] = filtered_comments",
                "54": "            else:",
                "55": "                print(\"mismatch in commit and comment data\")",
                "56": "    # Save filtered comments on your system",
                "57": "    save_to_json(commits_data, \"Data/filtered_commits_data.json\")",
                "58": "    shutil.rmtree(clone_path)",
                "59": "    with open(\"Data/filtered_commits_data.json\", \"r\") as json_file:",
                "60": "        data = json.load(json_file)",
                "61": "    analyse_diff_comments(data)",
                "62": "    blockify_comments(data)",
                "63": "    save_to_json(data, \"Exports/blockified_comments_data.json\")",
                "64": "    with open(\"Exports/blockified_comments_data.json\", \"r\") as json_file:",
                "65": "        data = json.load(json_file)",
                "66": "    blockify_comments2(data)",
                "67": "    save_to_json(data, \"Exports/blockified_comments2_data.json\")",
                "68": "    with open(\"Exports/blockified_comments2_data.json\", \"r\") as json_file:",
                "69": "        data = json.load(json_file)",
                "70": "    d = extract_later_modified_comments(data)",
                "71": "    save_to_json(d, \"Exports/analysis_results.json\")",
                "72": "    with open(\"Exports/analysis_results.json\", \"r\") as json_file:",
                "73": "        data = json.load(json_file)",
                "74": "    d = clean(data)",
                "75": "    save_to_json(d, \"Exports/analysis_results.json\")",
                "76": "    print(\"Average duration:\", average_comment_update_time(d))",
                "77": "",
                "78": "if __name__ == \"__main__\":",
                "79": "    main()"
            },
            "comments": [
                {
                    "line": 1,
                    "comment": "# Import modules",
                    "char_position_in_line": 0,
                    "type": [
                        "normal"
                    ]
                },
                {
                    "line": 7,
                    "comment": "# Import packages",
                    "char_position_in_line": 0,
                    "type": [
                        "normal"
                    ]
                },
                {
                    "line": 15,
                    "comment": "# Convert repo URL to path by cloning repo",
                    "char_position_in_line": 4,
                    "type": [
                        "normal"
                    ]
                },
                {
                    "line": 24,
                    "comment": "# # Paths",
                    "char_position_in_line": 4,
                    "type": [
                        "commented-out"
                    ]
                },
                {
                    "line": 28,
                    "comment": "# # Setting different timeperiod",
                    "char_position_in_line": 4,
                    "type": [
                        "commented-out"
                    ]
                },
                {
                    "line": 44,
                    "comment": "# Parse output as JSON",
                    "char_position_in_line": 12,
                    "type": [
                        "normal"
                    ]
                },
                {
                    "line": 50,
                    "comment": "# Filter comments by time",
                    "char_position_in_line": 12,
                    "type": [
                        "normal"
                    ]
                },
                {
                    "line": 56,
                    "comment": "# Save filtered comments on your system",
                    "char_position_in_line": 4,
                    "type": [
                        "normal"
                    ]
                }
            ]
        },
        {
            "commit": "39a855de8da8bde2a00bb031978e8a5bda2178ac",
            "timestamp": "2024-12-03T15:33:45+01:00",
            "author": "alexander.schranner",
            "diff": {
                "added": {
                    "6": "from build.xes_conversion import convert_json_to_xes",
                    "78": "    convert_json_to_xes(d, 'output.xes')"
                },
                "deleted": {}
            },
            "source_code": {
                "1": "# Import modules",
                "2": "from build.pydriller import get_commits_data",
                "3": "from build.comment_lister import run_comment_lister, filter_comments_by_time",
                "4": "from build.utils import save_to_json",
                "5": "from build.analysis import analyse_diff_comments, blockify_comments, blockify_comments2, extract_later_modified_comments, clean, average_comment_update_time",
                "6": "from build.xes_conversion import convert_json_to_xes",
                "7": "",
                "8": "# Import packages",
                "9": "import os",
                "10": "import json",
                "11": "import subprocess",
                "12": "import shutil",
                "13": "from datetime import datetime, timezone",
                "14": "",
                "15": "def main():",
                "16": "    # Convert repo URL to path by cloning repo",
                "17": "    repo_url = \"https://github.com/dani-garcia/vaultwarden\"",
                "18": "",
                "19": "    repo_name = os.path.basename(repo_url).replace(\".git\", \"\")",
                "20": "    temp_dir = \"/Users/as/Library/Mobile Documents/com~apple~CloudDocs/Dokumente/Studium/Bachelor-Thesis/tmp\"",
                "21": "    clone_path = os.path.join(temp_dir, repo_name)",
                "22": "",
                "23": "    subprocess.run(['git', 'clone', repo_url, clone_path], check=True)",
                "24": "",
                "25": "    # # Paths",
                "26": "    repo_path = clone_path",
                "27": "    jar_path = \"/Users/as/Library/Mobile Documents/com~apple~CloudDocs/Dokumente/Studium/Bachelor-Thesis/CommentLister/target/CommentLister.jar\"",
                "28": "    ",
                "29": "    # # Setting different timeperiod",
                "30": "    start_time = datetime.today().replace(year = datetime.today().year - 1, tzinfo=None, microsecond=0)",
                "31": "    end_time = datetime.today().replace(microsecond=0)",
                "32": "",
                "33": "    file_types = [\".c\", \".c\", \".cc\", \".cp\", \".cpp\", \".cx\", \".cxx\", \".c+\", \".c++\", \".h\", \".hh\", \".hxx\", \".h+\", \".h++\", \".hp\", \".hpp\", \".java\", \".js\", \".cs\", \".py\", \".php\", \".rb\"]",
                "34": "",
                "35": "    commits_data = get_commits_data(repo_path, start_time, end_time, file_types)",
                "36": "    save_to_json(commits_data, \"Data/commits_data.json\")",
                "37": "    ",
                "38": "    with open (\"Data/commits_data.json\", \"r\") as json_file:",
                "39": "        commits_data = json.load(json_file)",
                "40": "",
                "41": "    for file, commits in commits_data.items():",
                "42": "        for commit in commits:",
                "43": "            tag = \"-target=\" + commit[\"commit\"]",
                "44": "            output = run_comment_lister(repo_path, jar_path, tag)",
                "45": "            # Parse output as JSON",
                "46": "            try:",
                "47": "                comment_data = json.loads(output)",
                "48": "            except json.JSONDecodeError as e:",
                "49": "                print(f\"Failed to parse CommentLister output: {e}\")",
                "50": "                return",
                "51": "            # Filter comments by time",
                "52": "            commit_hash, filtered_comments = filter_comments_by_time(comment_data, start_time, end_time)",
                "53": "            if commit[\"commit\"] == commit_hash:",
                "54": "                commit[\"comments\"] = filtered_comments",
                "55": "            else:",
                "56": "                print(\"mismatch in commit and comment data\")",
                "57": "    # Save filtered comments on your system",
                "58": "    save_to_json(commits_data, \"Data/filtered_commits_data.json\")",
                "59": "    shutil.rmtree(clone_path)",
                "60": "    with open(\"Data/filtered_commits_data.json\", \"r\") as json_file:",
                "61": "        data = json.load(json_file)",
                "62": "    analyse_diff_comments(data)",
                "63": "    blockify_comments(data)",
                "64": "    save_to_json(data, \"Exports/blockified_comments_data.json\")",
                "65": "    with open(\"Exports/blockified_comments_data.json\", \"r\") as json_file:",
                "66": "        data = json.load(json_file)",
                "67": "    blockify_comments2(data)",
                "68": "    save_to_json(data, \"Exports/blockified_comments2_data.json\")",
                "69": "    with open(\"Exports/blockified_comments2_data.json\", \"r\") as json_file:",
                "70": "        data = json.load(json_file)",
                "71": "    d = extract_later_modified_comments(data)",
                "72": "    save_to_json(d, \"Exports/analysis_results.json\")",
                "73": "    with open(\"Exports/analysis_results.json\", \"r\") as json_file:",
                "74": "        data = json.load(json_file)",
                "75": "    d = clean(data)",
                "76": "    save_to_json(d, \"Exports/analysis_results.json\")",
                "77": "    print(\"Average duration:\", average_comment_update_time(d))",
                "78": "    convert_json_to_xes(d, 'output.xes')",
                "79": "",
                "80": "if __name__ == \"__main__\":",
                "81": "    main()"
            },
            "comments": [
                {
                    "line": 1,
                    "comment": "# Import modules",
                    "char_position_in_line": 0,
                    "type": [
                        "normal"
                    ]
                },
                {
                    "line": 8,
                    "comment": "# Import packages",
                    "char_position_in_line": 0,
                    "type": [
                        "normal"
                    ]
                },
                {
                    "line": 16,
                    "comment": "# Convert repo URL to path by cloning repo",
                    "char_position_in_line": 4,
                    "type": [
                        "normal"
                    ]
                },
                {
                    "line": 25,
                    "comment": "# # Paths",
                    "char_position_in_line": 4,
                    "type": [
                        "commented-out"
                    ]
                },
                {
                    "line": 29,
                    "comment": "# # Setting different timeperiod",
                    "char_position_in_line": 4,
                    "type": [
                        "commented-out"
                    ]
                },
                {
                    "line": 45,
                    "comment": "# Parse output as JSON",
                    "char_position_in_line": 12,
                    "type": [
                        "normal"
                    ]
                },
                {
                    "line": 51,
                    "comment": "# Filter comments by time",
                    "char_position_in_line": 12,
                    "type": [
                        "normal"
                    ]
                },
                {
                    "line": 57,
                    "comment": "# Save filtered comments on your system",
                    "char_position_in_line": 4,
                    "type": [
                        "normal"
                    ]
                }
            ]
        },
        {
            "commit": "b47cc911aa41b4d19ef8edb2623063f5a615a0b1",
            "timestamp": "2024-12-03T15:46:08+01:00",
            "author": "alexander.schranner",
            "diff": {
                "added": {
                    "78": "    convert_json_to_xes(d, 'Exports/output.xes')"
                },
                "deleted": {
                    "78": "    convert_json_to_xes(d, 'output.xes')"
                }
            },
            "source_code": {
                "1": "# Import modules",
                "2": "from build.pydriller import get_commits_data",
                "3": "from build.comment_lister import run_comment_lister, filter_comments_by_time",
                "4": "from build.utils import save_to_json",
                "5": "from build.analysis import analyse_diff_comments, blockify_comments, blockify_comments2, extract_later_modified_comments, clean, average_comment_update_time",
                "6": "from build.xes_conversion import convert_json_to_xes",
                "7": "",
                "8": "# Import packages",
                "9": "import os",
                "10": "import json",
                "11": "import subprocess",
                "12": "import shutil",
                "13": "from datetime import datetime, timezone",
                "14": "",
                "15": "def main():",
                "16": "    # Convert repo URL to path by cloning repo",
                "17": "    repo_url = \"https://github.com/dani-garcia/vaultwarden\"",
                "18": "",
                "19": "    repo_name = os.path.basename(repo_url).replace(\".git\", \"\")",
                "20": "    temp_dir = \"/Users/as/Library/Mobile Documents/com~apple~CloudDocs/Dokumente/Studium/Bachelor-Thesis/tmp\"",
                "21": "    clone_path = os.path.join(temp_dir, repo_name)",
                "22": "",
                "23": "    subprocess.run(['git', 'clone', repo_url, clone_path], check=True)",
                "24": "",
                "25": "    # # Paths",
                "26": "    repo_path = clone_path",
                "27": "    jar_path = \"/Users/as/Library/Mobile Documents/com~apple~CloudDocs/Dokumente/Studium/Bachelor-Thesis/CommentLister/target/CommentLister.jar\"",
                "28": "    ",
                "29": "    # # Setting different timeperiod",
                "30": "    start_time = datetime.today().replace(year = datetime.today().year - 1, tzinfo=None, microsecond=0)",
                "31": "    end_time = datetime.today().replace(microsecond=0)",
                "32": "",
                "33": "    file_types = [\".c\", \".c\", \".cc\", \".cp\", \".cpp\", \".cx\", \".cxx\", \".c+\", \".c++\", \".h\", \".hh\", \".hxx\", \".h+\", \".h++\", \".hp\", \".hpp\", \".java\", \".js\", \".cs\", \".py\", \".php\", \".rb\"]",
                "34": "",
                "35": "    commits_data = get_commits_data(repo_path, start_time, end_time, file_types)",
                "36": "    save_to_json(commits_data, \"Data/commits_data.json\")",
                "37": "    ",
                "38": "    with open (\"Data/commits_data.json\", \"r\") as json_file:",
                "39": "        commits_data = json.load(json_file)",
                "40": "",
                "41": "    for file, commits in commits_data.items():",
                "42": "        for commit in commits:",
                "43": "            tag = \"-target=\" + commit[\"commit\"]",
                "44": "            output = run_comment_lister(repo_path, jar_path, tag)",
                "45": "            # Parse output as JSON",
                "46": "            try:",
                "47": "                comment_data = json.loads(output)",
                "48": "            except json.JSONDecodeError as e:",
                "49": "                print(f\"Failed to parse CommentLister output: {e}\")",
                "50": "                return",
                "51": "            # Filter comments by time",
                "52": "            commit_hash, filtered_comments = filter_comments_by_time(comment_data, start_time, end_time)",
                "53": "            if commit[\"commit\"] == commit_hash:",
                "54": "                commit[\"comments\"] = filtered_comments",
                "55": "            else:",
                "56": "                print(\"mismatch in commit and comment data\")",
                "57": "    # Save filtered comments on your system",
                "58": "    save_to_json(commits_data, \"Data/filtered_commits_data.json\")",
                "59": "    shutil.rmtree(clone_path)",
                "60": "    with open(\"Data/filtered_commits_data.json\", \"r\") as json_file:",
                "61": "        data = json.load(json_file)",
                "62": "    analyse_diff_comments(data)",
                "63": "    blockify_comments(data)",
                "64": "    save_to_json(data, \"Exports/blockified_comments_data.json\")",
                "65": "    with open(\"Exports/blockified_comments_data.json\", \"r\") as json_file:",
                "66": "        data = json.load(json_file)",
                "67": "    blockify_comments2(data)",
                "68": "    save_to_json(data, \"Exports/blockified_comments2_data.json\")",
                "69": "    with open(\"Exports/blockified_comments2_data.json\", \"r\") as json_file:",
                "70": "        data = json.load(json_file)",
                "71": "    d = extract_later_modified_comments(data)",
                "72": "    save_to_json(d, \"Exports/analysis_results.json\")",
                "73": "    with open(\"Exports/analysis_results.json\", \"r\") as json_file:",
                "74": "        data = json.load(json_file)",
                "75": "    d = clean(data)",
                "76": "    save_to_json(d, \"Exports/analysis_results.json\")",
                "77": "    print(\"Average duration:\", average_comment_update_time(d))",
                "78": "    convert_json_to_xes(d, 'Exports/output.xes')",
                "79": "",
                "80": "if __name__ == \"__main__\":",
                "81": "    main()"
            },
            "comments": [
                {
                    "line": 1,
                    "comment": "# Import modules",
                    "char_position_in_line": 0,
                    "type": [
                        "normal"
                    ]
                },
                {
                    "line": 8,
                    "comment": "# Import packages",
                    "char_position_in_line": 0,
                    "type": [
                        "normal"
                    ]
                },
                {
                    "line": 16,
                    "comment": "# Convert repo URL to path by cloning repo",
                    "char_position_in_line": 4,
                    "type": [
                        "normal"
                    ]
                },
                {
                    "line": 25,
                    "comment": "# # Paths",
                    "char_position_in_line": 4,
                    "type": [
                        "commented-out"
                    ]
                },
                {
                    "line": 29,
                    "comment": "# # Setting different timeperiod",
                    "char_position_in_line": 4,
                    "type": [
                        "commented-out"
                    ]
                },
                {
                    "line": 45,
                    "comment": "# Parse output as JSON",
                    "char_position_in_line": 12,
                    "type": [
                        "normal"
                    ]
                },
                {
                    "line": 51,
                    "comment": "# Filter comments by time",
                    "char_position_in_line": 12,
                    "type": [
                        "normal"
                    ]
                },
                {
                    "line": 57,
                    "comment": "# Save filtered comments on your system",
                    "char_position_in_line": 4,
                    "type": [
                        "normal"
                    ]
                }
            ]
        },
        {
            "commit": "202468fb39d473251ab81eb3037227cf7af47344",
            "timestamp": "2024-12-04T00:35:22+01:00",
            "author": "alexander.schranner",
            "diff": {
                "added": {
                    "5": "from build.analysis import analyse_diff_comments, blockify_comments, blockify_comments2, extract_later_modified_comments, clean, average_comment_update_time, classify_comments",
                    "16": "    # # Convert repo URL to path by cloning repo",
                    "17": "    # repo_url = \"https://github.com/AlexS-1/Bachelor-Code\"",
                    "19": "    # repo_name = os.path.basename(repo_url).replace(\".git\", \"\")",
                    "20": "    # temp_dir = \"/Users/as/Library/Mobile Documents/com~apple~CloudDocs/Dokumente/Studium/Bachelor-Thesis/tmp\"",
                    "21": "    # clone_path = os.path.join(temp_dir, repo_name)",
                    "23": "    # subprocess.run(['git', 'clone', repo_url, clone_path], check=True)",
                    "25": "    # # # Paths",
                    "26": "    # repo_path = clone_path",
                    "27": "    # jar_path = \"/Users/as/Library/Mobile Documents/com~apple~CloudDocs/Dokumente/Studium/Bachelor-Thesis/CommentLister/target/CommentLister.jar\"",
                    "29": "    # # # Setting different timeperiod",
                    "30": "    # start_time = datetime.today().replace(year = datetime.today().year - 1, tzinfo=None, microsecond=0)",
                    "31": "    # end_time = datetime.today().replace(microsecond=0)",
                    "33": "    # file_types = [\".c\", \".c\", \".cc\", \".cp\", \".cpp\", \".cx\", \".cxx\", \".c+\", \".c++\", \".h\", \".hh\", \".hxx\", \".h+\", \".h++\", \".hp\", \".hpp\", \".java\", \".js\", \".cs\", \".py\", \".php\", \".rb\"]",
                    "35": "    # commits_data = get_commits_data(repo_path, start_time, end_time, file_types)",
                    "36": "    # save_to_json(commits_data, \"Data/commits_data.json\")",
                    "38": "    # with open (\"Data/commits_data.json\", \"r\") as json_file:",
                    "39": "    #     commits_data = json.load(json_file)",
                    "41": "    # for file, commits in commits_data.items():",
                    "42": "    #     for commit in commits:",
                    "43": "    #         tag = \"-target=\" + commit[\"commit\"]",
                    "44": "    #         output = run_comment_lister(repo_path, jar_path, tag)",
                    "45": "    #         # Parse output as JSON",
                    "46": "    #         try:",
                    "47": "    #             comment_data = json.loads(output)",
                    "48": "    #         except json.JSONDecodeError as e:",
                    "49": "    #             print(f\"Failed to parse CommentLister output: {e}\")",
                    "50": "    #             return",
                    "51": "    #         # Filter comments by time",
                    "52": "    #         commit_hash, filtered_comments = filter_comments_by_time(comment_data, start_time, end_time)",
                    "53": "    #         if commit[\"commit\"] == commit_hash and file in filtered_comments.keys():",
                    "54": "    #             commit[\"comments\"] = filtered_comments[file]",
                    "55": "    #         else:",
                    "56": "    #             print(\"mismatch in commit and comment data or no comments in this commit for investigatet file\")",
                    "57": "    #             print(\"file could have been deleted\")",
                    "58": "    #             commit[\"comments\"] = {}",
                    "59": "    # # Save filtered comments on your system",
                    "60": "    # save_to_json(commits_data, \"Data/filtered_commits_data.json\")",
                    "61": "    # shutil.rmtree(clone_path)",
                    "64": "    # analyse_diff_comments(data)",
                    "78": "    save_to_json(d, \"Exports/clean_analysis_results.json\")",
                    "79": "    with open(\"Exports/clean_analysis_results.json\", \"r\") as json_file:",
                    "80": "        data = json.load(json_file)",
                    "81": "    d = classify_comments(data)",
                    "82": "    save_to_json(d, \"Exports/clean_analysis_results2.json\")"
                },
                "deleted": {
                    "5": "from build.analysis import analyse_diff_comments, blockify_comments, blockify_comments2, extract_later_modified_comments, clean, average_comment_update_time",
                    "16": "    # Convert repo URL to path by cloning repo",
                    "17": "    repo_url = \"https://github.com/dani-garcia/vaultwarden\"",
                    "19": "    repo_name = os.path.basename(repo_url).replace(\".git\", \"\")",
                    "20": "    temp_dir = \"/Users/as/Library/Mobile Documents/com~apple~CloudDocs/Dokumente/Studium/Bachelor-Thesis/tmp\"",
                    "21": "    clone_path = os.path.join(temp_dir, repo_name)",
                    "23": "    subprocess.run(['git', 'clone', repo_url, clone_path], check=True)",
                    "25": "    # # Paths",
                    "26": "    repo_path = clone_path",
                    "27": "    jar_path = \"/Users/as/Library/Mobile Documents/com~apple~CloudDocs/Dokumente/Studium/Bachelor-Thesis/CommentLister/target/CommentLister.jar\"",
                    "29": "    # # Setting different timeperiod",
                    "30": "    start_time = datetime.today().replace(year = datetime.today().year - 1, tzinfo=None, microsecond=0)",
                    "31": "    end_time = datetime.today().replace(microsecond=0)",
                    "33": "    file_types = [\".c\", \".c\", \".cc\", \".cp\", \".cpp\", \".cx\", \".cxx\", \".c+\", \".c++\", \".h\", \".hh\", \".hxx\", \".h+\", \".h++\", \".hp\", \".hpp\", \".java\", \".js\", \".cs\", \".py\", \".php\", \".rb\"]",
                    "35": "    commits_data = get_commits_data(repo_path, start_time, end_time, file_types)",
                    "36": "    save_to_json(commits_data, \"Data/commits_data.json\")",
                    "38": "    with open (\"Data/commits_data.json\", \"r\") as json_file:",
                    "39": "        commits_data = json.load(json_file)",
                    "41": "    for file, commits in commits_data.items():",
                    "42": "        for commit in commits:",
                    "43": "            tag = \"-target=\" + commit[\"commit\"]",
                    "44": "            output = run_comment_lister(repo_path, jar_path, tag)",
                    "45": "            # Parse output as JSON",
                    "46": "            try:",
                    "47": "                comment_data = json.loads(output)",
                    "48": "            except json.JSONDecodeError as e:",
                    "49": "                print(f\"Failed to parse CommentLister output: {e}\")",
                    "50": "                return",
                    "51": "            # Filter comments by time",
                    "52": "            commit_hash, filtered_comments = filter_comments_by_time(comment_data, start_time, end_time)",
                    "53": "            if commit[\"commit\"] == commit_hash:",
                    "54": "                commit[\"comments\"] = filtered_comments",
                    "55": "            else:",
                    "56": "                print(\"mismatch in commit and comment data\")",
                    "57": "    # Save filtered comments on your system",
                    "58": "    save_to_json(commits_data, \"Data/filtered_commits_data.json\")",
                    "59": "    shutil.rmtree(clone_path)",
                    "62": "    analyse_diff_comments(data)",
                    "76": "    save_to_json(d, \"Exports/analysis_results.json\")"
                }
            },
            "source_code": {
                "1": "# Import modules",
                "2": "from build.pydriller import get_commits_data",
                "3": "from build.comment_lister import run_comment_lister, filter_comments_by_time",
                "4": "from build.utils import save_to_json",
                "5": "from build.analysis import analyse_diff_comments, blockify_comments, blockify_comments2, extract_later_modified_comments, clean, average_comment_update_time, classify_comments",
                "6": "from build.xes_conversion import convert_json_to_xes",
                "7": "",
                "8": "# Import packages",
                "9": "import os",
                "10": "import json",
                "11": "import subprocess",
                "12": "import shutil",
                "13": "from datetime import datetime, timezone",
                "14": "",
                "15": "def main():",
                "16": "    # # Convert repo URL to path by cloning repo",
                "17": "    # repo_url = \"https://github.com/AlexS-1/Bachelor-Code\"",
                "18": "",
                "19": "    # repo_name = os.path.basename(repo_url).replace(\".git\", \"\")",
                "20": "    # temp_dir = \"/Users/as/Library/Mobile Documents/com~apple~CloudDocs/Dokumente/Studium/Bachelor-Thesis/tmp\"",
                "21": "    # clone_path = os.path.join(temp_dir, repo_name)",
                "22": "",
                "23": "    # subprocess.run(['git', 'clone', repo_url, clone_path], check=True)",
                "24": "",
                "25": "    # # # Paths",
                "26": "    # repo_path = clone_path",
                "27": "    # jar_path = \"/Users/as/Library/Mobile Documents/com~apple~CloudDocs/Dokumente/Studium/Bachelor-Thesis/CommentLister/target/CommentLister.jar\"",
                "28": "    ",
                "29": "    # # # Setting different timeperiod",
                "30": "    # start_time = datetime.today().replace(year = datetime.today().year - 1, tzinfo=None, microsecond=0)",
                "31": "    # end_time = datetime.today().replace(microsecond=0)",
                "32": "",
                "33": "    # file_types = [\".c\", \".c\", \".cc\", \".cp\", \".cpp\", \".cx\", \".cxx\", \".c+\", \".c++\", \".h\", \".hh\", \".hxx\", \".h+\", \".h++\", \".hp\", \".hpp\", \".java\", \".js\", \".cs\", \".py\", \".php\", \".rb\"]",
                "34": "",
                "35": "    # commits_data = get_commits_data(repo_path, start_time, end_time, file_types)",
                "36": "    # save_to_json(commits_data, \"Data/commits_data.json\")",
                "37": "    ",
                "38": "    # with open (\"Data/commits_data.json\", \"r\") as json_file:",
                "39": "    #     commits_data = json.load(json_file)",
                "40": "",
                "41": "    # for file, commits in commits_data.items():",
                "42": "    #     for commit in commits:",
                "43": "    #         tag = \"-target=\" + commit[\"commit\"]",
                "44": "    #         output = run_comment_lister(repo_path, jar_path, tag)",
                "45": "    #         # Parse output as JSON",
                "46": "    #         try:",
                "47": "    #             comment_data = json.loads(output)",
                "48": "    #         except json.JSONDecodeError as e:",
                "49": "    #             print(f\"Failed to parse CommentLister output: {e}\")",
                "50": "    #             return",
                "51": "    #         # Filter comments by time",
                "52": "    #         commit_hash, filtered_comments = filter_comments_by_time(comment_data, start_time, end_time)",
                "53": "    #         if commit[\"commit\"] == commit_hash and file in filtered_comments.keys():",
                "54": "    #             commit[\"comments\"] = filtered_comments[file]",
                "55": "    #         else:",
                "56": "    #             print(\"mismatch in commit and comment data or no comments in this commit for investigatet file\")",
                "57": "    #             print(\"file could have been deleted\")",
                "58": "    #             commit[\"comments\"] = {}",
                "59": "    # # Save filtered comments on your system",
                "60": "    # save_to_json(commits_data, \"Data/filtered_commits_data.json\")",
                "61": "    # shutil.rmtree(clone_path)",
                "62": "    with open(\"Data/filtered_commits_data.json\", \"r\") as json_file:",
                "63": "        data = json.load(json_file)",
                "64": "    # analyse_diff_comments(data)",
                "65": "    blockify_comments(data)",
                "66": "    save_to_json(data, \"Exports/blockified_comments_data.json\")",
                "67": "    with open(\"Exports/blockified_comments_data.json\", \"r\") as json_file:",
                "68": "        data = json.load(json_file)",
                "69": "    blockify_comments2(data)",
                "70": "    save_to_json(data, \"Exports/blockified_comments2_data.json\")",
                "71": "    with open(\"Exports/blockified_comments2_data.json\", \"r\") as json_file:",
                "72": "        data = json.load(json_file)",
                "73": "    d = extract_later_modified_comments(data)",
                "74": "    save_to_json(d, \"Exports/analysis_results.json\")",
                "75": "    with open(\"Exports/analysis_results.json\", \"r\") as json_file:",
                "76": "        data = json.load(json_file)",
                "77": "    d = clean(data)",
                "78": "    save_to_json(d, \"Exports/clean_analysis_results.json\")",
                "79": "    with open(\"Exports/clean_analysis_results.json\", \"r\") as json_file:",
                "80": "        data = json.load(json_file)",
                "81": "    d = classify_comments(data)",
                "82": "    save_to_json(d, \"Exports/clean_analysis_results2.json\")",
                "83": "    print(\"Average duration:\", average_comment_update_time(d))",
                "84": "    convert_json_to_xes(d, 'Exports/output.xes')",
                "85": "",
                "86": "if __name__ == \"__main__\":",
                "87": "    main()"
            },
            "comments": [
                {
                    "line": 1,
                    "comment": "# Import modules",
                    "char_position_in_line": 0,
                    "type": [
                        "normal"
                    ]
                },
                {
                    "line": 8,
                    "comment": "# Import packages",
                    "char_position_in_line": 0,
                    "type": [
                        "normal"
                    ]
                },
                {
                    "line": 16,
                    "comment": "# # Convert repo URL to path by cloning repo",
                    "char_position_in_line": 4,
                    "type": [
                        "commented-out",
                        "block"
                    ]
                },
                {
                    "line": 17,
                    "comment": "# repo_url = \"https://github.com/AlexS-1/Bachelor-Code\"",
                    "char_position_in_line": 4,
                    "type": [
                        "commented-out",
                        "block"
                    ]
                },
                {
                    "line": 19,
                    "comment": "# repo_name = os.path.basename(repo_url).replace(\".git\", \"\")",
                    "char_position_in_line": 4,
                    "type": [
                        "commented-out",
                        "block"
                    ]
                },
                {
                    "line": 20,
                    "comment": "# temp_dir = \"/Users/as/Library/Mobile Documents/com~apple~CloudDocs/Dokumente/Studium/Bachelor-Thesis/tmp\"",
                    "char_position_in_line": 4,
                    "type": [
                        "commented-out",
                        "block"
                    ]
                },
                {
                    "line": 21,
                    "comment": "# clone_path = os.path.join(temp_dir, repo_name)",
                    "char_position_in_line": 4,
                    "type": [
                        "commented-out",
                        "block"
                    ]
                },
                {
                    "line": 23,
                    "comment": "# subprocess.run(['git', 'clone', repo_url, clone_path], check=True)",
                    "char_position_in_line": 4,
                    "type": [
                        "commented-out"
                    ]
                },
                {
                    "line": 25,
                    "comment": "# # # Paths",
                    "char_position_in_line": 4,
                    "type": [
                        "commented-out",
                        "block"
                    ]
                },
                {
                    "line": 26,
                    "comment": "# repo_path = clone_path",
                    "char_position_in_line": 4,
                    "type": [
                        "commented-out",
                        "block"
                    ]
                },
                {
                    "line": 27,
                    "comment": "# jar_path = \"/Users/as/Library/Mobile Documents/com~apple~CloudDocs/Dokumente/Studium/Bachelor-Thesis/CommentLister/target/CommentLister.jar\"",
                    "char_position_in_line": 4,
                    "type": [
                        "commented-out",
                        "block"
                    ]
                },
                {
                    "line": 29,
                    "comment": "# # # Setting different timeperiod",
                    "char_position_in_line": 4,
                    "type": [
                        "commented-out",
                        "block"
                    ]
                },
                {
                    "line": 30,
                    "comment": "# start_time = datetime.today().replace(year = datetime.today().year - 1, tzinfo=None, microsecond=0)",
                    "char_position_in_line": 4,
                    "type": [
                        "commented-out",
                        "block"
                    ]
                },
                {
                    "line": 31,
                    "comment": "# end_time = datetime.today().replace(microsecond=0)",
                    "char_position_in_line": 4,
                    "type": [
                        "commented-out",
                        "block"
                    ]
                },
                {
                    "line": 33,
                    "comment": "# file_types = [\".c\", \".c\", \".cc\", \".cp\", \".cpp\", \".cx\", \".cxx\", \".c+\", \".c++\", \".h\", \".hh\", \".hxx\", \".h+\", \".h++\", \".hp\", \".hpp\", \".java\", \".js\", \".cs\", \".py\", \".php\", \".rb\"]",
                    "char_position_in_line": 4,
                    "type": [
                        "commented-out"
                    ]
                },
                {
                    "line": 35,
                    "comment": "# commits_data = get_commits_data(repo_path, start_time, end_time, file_types)",
                    "char_position_in_line": 4,
                    "type": [
                        "commented-out",
                        "block"
                    ]
                },
                {
                    "line": 36,
                    "comment": "# save_to_json(commits_data, \"Data/commits_data.json\")",
                    "char_position_in_line": 4,
                    "type": [
                        "commented-out",
                        "block"
                    ]
                },
                {
                    "line": 38,
                    "comment": "# with open (\"Data/commits_data.json\", \"r\") as json_file:",
                    "char_position_in_line": 4,
                    "type": [
                        "block"
                    ]
                },
                {
                    "line": 39,
                    "comment": "#     commits_data = json.load(json_file)",
                    "char_position_in_line": 4,
                    "type": [
                        "block"
                    ]
                },
                {
                    "line": 41,
                    "comment": "# for file, commits in commits_data.items():",
                    "char_position_in_line": 4,
                    "type": [
                        "block"
                    ]
                },
                {
                    "line": 42,
                    "comment": "#     for commit in commits:",
                    "char_position_in_line": 4,
                    "type": [
                        "block"
                    ]
                },
                {
                    "line": 43,
                    "comment": "#         tag = \"-target=\" + commit[\"commit\"]",
                    "char_position_in_line": 4,
                    "type": [
                        "block"
                    ]
                },
                {
                    "line": 44,
                    "comment": "#         output = run_comment_lister(repo_path, jar_path, tag)",
                    "char_position_in_line": 4,
                    "type": [
                        "block"
                    ]
                },
                {
                    "line": 45,
                    "comment": "#         # Parse output as JSON",
                    "char_position_in_line": 4,
                    "type": [
                        "block"
                    ]
                },
                {
                    "line": 46,
                    "comment": "#         try:",
                    "char_position_in_line": 4,
                    "type": [
                        "block"
                    ]
                },
                {
                    "line": 47,
                    "comment": "#             comment_data = json.loads(output)",
                    "char_position_in_line": 4,
                    "type": [
                        "block"
                    ]
                },
                {
                    "line": 48,
                    "comment": "#         except json.JSONDecodeError as e:",
                    "char_position_in_line": 4,
                    "type": [
                        "block"
                    ]
                },
                {
                    "line": 49,
                    "comment": "#             print(f\"Failed to parse CommentLister output: {e}\")",
                    "char_position_in_line": 4,
                    "type": [
                        "block"
                    ]
                },
                {
                    "line": 50,
                    "comment": "#             return",
                    "char_position_in_line": 4,
                    "type": [
                        "block"
                    ]
                },
                {
                    "line": 51,
                    "comment": "#         # Filter comments by time",
                    "char_position_in_line": 4,
                    "type": [
                        "block"
                    ]
                },
                {
                    "line": 52,
                    "comment": "#         commit_hash, filtered_comments = filter_comments_by_time(comment_data, start_time, end_time)",
                    "char_position_in_line": 4,
                    "type": [
                        "block"
                    ]
                },
                {
                    "line": 53,
                    "comment": "#         if commit[\"commit\"] == commit_hash and file in filtered_comments.keys():",
                    "char_position_in_line": 4,
                    "type": [
                        "block"
                    ]
                },
                {
                    "line": 54,
                    "comment": "#             commit[\"comments\"] = filtered_comments[file]",
                    "char_position_in_line": 4,
                    "type": [
                        "block"
                    ]
                },
                {
                    "line": 55,
                    "comment": "#         else:",
                    "char_position_in_line": 4,
                    "type": [
                        "block"
                    ]
                },
                {
                    "line": 56,
                    "comment": "#             print(\"mismatch in commit and comment data or no comments in this commit for investigatet file\")",
                    "char_position_in_line": 4,
                    "type": [
                        "block"
                    ]
                },
                {
                    "line": 57,
                    "comment": "#             print(\"file could have been deleted\")",
                    "char_position_in_line": 4,
                    "type": [
                        "block"
                    ]
                },
                {
                    "line": 58,
                    "comment": "#             commit[\"comments\"] = {}",
                    "char_position_in_line": 4,
                    "type": [
                        "block"
                    ]
                },
                {
                    "line": 59,
                    "comment": "# # Save filtered comments on your system",
                    "char_position_in_line": 4,
                    "type": [
                        "block"
                    ]
                },
                {
                    "line": 60,
                    "comment": "# save_to_json(commits_data, \"Data/filtered_commits_data.json\")",
                    "char_position_in_line": 4,
                    "type": [
                        "block"
                    ]
                },
                {
                    "line": 61,
                    "comment": "# shutil.rmtree(clone_path)",
                    "char_position_in_line": 4,
                    "type": [
                        "block"
                    ]
                },
                {
                    "line": 64,
                    "comment": "# analyse_diff_comments(data)",
                    "char_position_in_line": 4,
                    "type": [
                        "commented-out"
                    ]
                }
            ]
        },
        {
            "commit": "22aa658dddb9457b7fae9f52febd52afdf2a413e",
            "timestamp": "2024-12-04T00:55:39+01:00",
            "author": "alexander.schranner",
            "diff": {
                "added": {
                    "16": "    # Convert repo URL to path by cloning repo",
                    "17": "    repo_url = \"https://github.com/AlexS-1/Bachelor-Code\"",
                    "19": "    repo_name = os.path.basename(repo_url).replace(\".git\", \"\")",
                    "20": "    temp_dir = \"/Users/as/Library/Mobile Documents/com~apple~CloudDocs/Dokumente/Studium/Bachelor-Thesis/tmp\"",
                    "21": "    clone_path = os.path.join(temp_dir, repo_name)",
                    "23": "    subprocess.run(['git', 'clone', repo_url, clone_path], check=True)",
                    "25": "    # # Paths",
                    "26": "    repo_path = clone_path",
                    "27": "    jar_path = \"/Users/as/Library/Mobile Documents/com~apple~CloudDocs/Dokumente/Studium/Bachelor-Thesis/CommentLister/target/CommentLister.jar\"",
                    "29": "    # # Setting different timeperiod",
                    "30": "    start_time = datetime.today().replace(year = datetime.today().year - 1, tzinfo=None, microsecond=0)",
                    "31": "    end_time = datetime.today().replace(microsecond=0)",
                    "33": "    file_types = [\".c\", \".c\", \".cc\", \".cp\", \".cpp\", \".cx\", \".cxx\", \".c+\", \".c++\", \".h\", \".hh\", \".hxx\", \".h+\", \".h++\", \".hp\", \".hpp\", \".java\", \".js\", \".cs\", \".py\", \".php\", \".rb\"]",
                    "35": "    commits_data = get_commits_data(repo_path, start_time, end_time, file_types)",
                    "36": "    save_to_json(commits_data, \"Data/commits_data.json\")",
                    "38": "    with open (\"Data/commits_data.json\", \"r\") as json_file:",
                    "39": "        commits_data = json.load(json_file)",
                    "41": "    for file, commits in commits_data.items():",
                    "42": "        for commit in commits:",
                    "43": "            tag = \"-target=\" + commit[\"commit\"]",
                    "44": "            output = run_comment_lister(repo_path, jar_path, tag)",
                    "45": "            # Parse output as JSON",
                    "46": "            try:",
                    "47": "                comment_data = json.loads(output)",
                    "48": "            except json.JSONDecodeError as e:",
                    "49": "                print(f\"Failed to parse CommentLister output: {e}\")",
                    "50": "                return",
                    "51": "            # Filter comments by time",
                    "52": "            commit_hash, filtered_comments = filter_comments_by_time(comment_data, start_time, end_time)",
                    "53": "            if commit[\"commit\"] == commit_hash and file in filtered_comments.keys():",
                    "54": "                commit[\"comments\"] = filtered_comments[file]",
                    "55": "            else:",
                    "56": "                print(\"mismatch in commit and comment data or no comments in this commit for investigatet file\")",
                    "57": "                print(\"file could have been deleted\")",
                    "58": "                commit[\"comments\"] = {}",
                    "59": "    # Save filtered comments on your system",
                    "60": "    save_to_json(commits_data, \"Data/filtered_commits_data.json\")",
                    "61": "    shutil.rmtree(clone_path)"
                },
                "deleted": {
                    "16": "    # # Convert repo URL to path by cloning repo",
                    "17": "    # repo_url = \"https://github.com/AlexS-1/Bachelor-Code\"",
                    "19": "    # repo_name = os.path.basename(repo_url).replace(\".git\", \"\")",
                    "20": "    # temp_dir = \"/Users/as/Library/Mobile Documents/com~apple~CloudDocs/Dokumente/Studium/Bachelor-Thesis/tmp\"",
                    "21": "    # clone_path = os.path.join(temp_dir, repo_name)",
                    "23": "    # subprocess.run(['git', 'clone', repo_url, clone_path], check=True)",
                    "25": "    # # # Paths",
                    "26": "    # repo_path = clone_path",
                    "27": "    # jar_path = \"/Users/as/Library/Mobile Documents/com~apple~CloudDocs/Dokumente/Studium/Bachelor-Thesis/CommentLister/target/CommentLister.jar\"",
                    "29": "    # # # Setting different timeperiod",
                    "30": "    # start_time = datetime.today().replace(year = datetime.today().year - 1, tzinfo=None, microsecond=0)",
                    "31": "    # end_time = datetime.today().replace(microsecond=0)",
                    "33": "    # file_types = [\".c\", \".c\", \".cc\", \".cp\", \".cpp\", \".cx\", \".cxx\", \".c+\", \".c++\", \".h\", \".hh\", \".hxx\", \".h+\", \".h++\", \".hp\", \".hpp\", \".java\", \".js\", \".cs\", \".py\", \".php\", \".rb\"]",
                    "35": "    # commits_data = get_commits_data(repo_path, start_time, end_time, file_types)",
                    "36": "    # save_to_json(commits_data, \"Data/commits_data.json\")",
                    "38": "    # with open (\"Data/commits_data.json\", \"r\") as json_file:",
                    "39": "    #     commits_data = json.load(json_file)",
                    "41": "    # for file, commits in commits_data.items():",
                    "42": "    #     for commit in commits:",
                    "43": "    #         tag = \"-target=\" + commit[\"commit\"]",
                    "44": "    #         output = run_comment_lister(repo_path, jar_path, tag)",
                    "45": "    #         # Parse output as JSON",
                    "46": "    #         try:",
                    "47": "    #             comment_data = json.loads(output)",
                    "48": "    #         except json.JSONDecodeError as e:",
                    "49": "    #             print(f\"Failed to parse CommentLister output: {e}\")",
                    "50": "    #             return",
                    "51": "    #         # Filter comments by time",
                    "52": "    #         commit_hash, filtered_comments = filter_comments_by_time(comment_data, start_time, end_time)",
                    "53": "    #         if commit[\"commit\"] == commit_hash and file in filtered_comments.keys():",
                    "54": "    #             commit[\"comments\"] = filtered_comments[file]",
                    "55": "    #         else:",
                    "56": "    #             print(\"mismatch in commit and comment data or no comments in this commit for investigatet file\")",
                    "57": "    #             print(\"file could have been deleted\")",
                    "58": "    #             commit[\"comments\"] = {}",
                    "59": "    # # Save filtered comments on your system",
                    "60": "    # save_to_json(commits_data, \"Data/filtered_commits_data.json\")",
                    "61": "    # shutil.rmtree(clone_path)"
                }
            },
            "source_code": {
                "1": "# Import modules",
                "2": "from build.pydriller import get_commits_data",
                "3": "from build.comment_lister import run_comment_lister, filter_comments_by_time",
                "4": "from build.utils import save_to_json",
                "5": "from build.analysis import analyse_diff_comments, blockify_comments, blockify_comments2, extract_later_modified_comments, clean, average_comment_update_time, classify_comments",
                "6": "from build.xes_conversion import convert_json_to_xes",
                "7": "",
                "8": "# Import packages",
                "9": "import os",
                "10": "import json",
                "11": "import subprocess",
                "12": "import shutil",
                "13": "from datetime import datetime, timezone",
                "14": "",
                "15": "def main():",
                "16": "    # Convert repo URL to path by cloning repo",
                "17": "    repo_url = \"https://github.com/AlexS-1/Bachelor-Code\"",
                "18": "",
                "19": "    repo_name = os.path.basename(repo_url).replace(\".git\", \"\")",
                "20": "    temp_dir = \"/Users/as/Library/Mobile Documents/com~apple~CloudDocs/Dokumente/Studium/Bachelor-Thesis/tmp\"",
                "21": "    clone_path = os.path.join(temp_dir, repo_name)",
                "22": "",
                "23": "    subprocess.run(['git', 'clone', repo_url, clone_path], check=True)",
                "24": "",
                "25": "    # # Paths",
                "26": "    repo_path = clone_path",
                "27": "    jar_path = \"/Users/as/Library/Mobile Documents/com~apple~CloudDocs/Dokumente/Studium/Bachelor-Thesis/CommentLister/target/CommentLister.jar\"",
                "28": "    ",
                "29": "    # # Setting different timeperiod",
                "30": "    start_time = datetime.today().replace(year = datetime.today().year - 1, tzinfo=None, microsecond=0)",
                "31": "    end_time = datetime.today().replace(microsecond=0)",
                "32": "",
                "33": "    file_types = [\".c\", \".c\", \".cc\", \".cp\", \".cpp\", \".cx\", \".cxx\", \".c+\", \".c++\", \".h\", \".hh\", \".hxx\", \".h+\", \".h++\", \".hp\", \".hpp\", \".java\", \".js\", \".cs\", \".py\", \".php\", \".rb\"]",
                "34": "",
                "35": "    commits_data = get_commits_data(repo_path, start_time, end_time, file_types)",
                "36": "    save_to_json(commits_data, \"Data/commits_data.json\")",
                "37": "    ",
                "38": "    with open (\"Data/commits_data.json\", \"r\") as json_file:",
                "39": "        commits_data = json.load(json_file)",
                "40": "",
                "41": "    for file, commits in commits_data.items():",
                "42": "        for commit in commits:",
                "43": "            tag = \"-target=\" + commit[\"commit\"]",
                "44": "            output = run_comment_lister(repo_path, jar_path, tag)",
                "45": "            # Parse output as JSON",
                "46": "            try:",
                "47": "                comment_data = json.loads(output)",
                "48": "            except json.JSONDecodeError as e:",
                "49": "                print(f\"Failed to parse CommentLister output: {e}\")",
                "50": "                return",
                "51": "            # Filter comments by time",
                "52": "            commit_hash, filtered_comments = filter_comments_by_time(comment_data, start_time, end_time)",
                "53": "            if commit[\"commit\"] == commit_hash and file in filtered_comments.keys():",
                "54": "                commit[\"comments\"] = filtered_comments[file]",
                "55": "            else:",
                "56": "                print(\"mismatch in commit and comment data or no comments in this commit for investigatet file\")",
                "57": "                print(\"file could have been deleted\")",
                "58": "                commit[\"comments\"] = {}",
                "59": "    # Save filtered comments on your system",
                "60": "    save_to_json(commits_data, \"Data/filtered_commits_data.json\")",
                "61": "    shutil.rmtree(clone_path)",
                "62": "    with open(\"Data/filtered_commits_data.json\", \"r\") as json_file:",
                "63": "        data = json.load(json_file)",
                "64": "    # analyse_diff_comments(data)",
                "65": "    blockify_comments(data)",
                "66": "    save_to_json(data, \"Exports/blockified_comments_data.json\")",
                "67": "    with open(\"Exports/blockified_comments_data.json\", \"r\") as json_file:",
                "68": "        data = json.load(json_file)",
                "69": "    blockify_comments2(data)",
                "70": "    save_to_json(data, \"Exports/blockified_comments2_data.json\")",
                "71": "    with open(\"Exports/blockified_comments2_data.json\", \"r\") as json_file:",
                "72": "        data = json.load(json_file)",
                "73": "    d = extract_later_modified_comments(data)",
                "74": "    save_to_json(d, \"Exports/analysis_results.json\")",
                "75": "    with open(\"Exports/analysis_results.json\", \"r\") as json_file:",
                "76": "        data = json.load(json_file)",
                "77": "    d = clean(data)",
                "78": "    save_to_json(d, \"Exports/clean_analysis_results.json\")",
                "79": "    with open(\"Exports/clean_analysis_results.json\", \"r\") as json_file:",
                "80": "        data = json.load(json_file)",
                "81": "    d = classify_comments(data)",
                "82": "    save_to_json(d, \"Exports/clean_analysis_results2.json\")",
                "83": "    print(\"Average duration:\", average_comment_update_time(d))",
                "84": "    convert_json_to_xes(d, 'Exports/output.xes')",
                "85": "",
                "86": "if __name__ == \"__main__\":",
                "87": "    main()"
            },
            "comments": [
                {
                    "line": 1,
                    "comment": "# Import modules",
                    "char_position_in_line": 0,
                    "type": [
                        "normal"
                    ]
                },
                {
                    "line": 8,
                    "comment": "# Import packages",
                    "char_position_in_line": 0,
                    "type": [
                        "normal"
                    ]
                },
                {
                    "line": 16,
                    "comment": "# Convert repo URL to path by cloning repo",
                    "char_position_in_line": 4,
                    "type": [
                        "normal"
                    ]
                },
                {
                    "line": 25,
                    "comment": "# # Paths",
                    "char_position_in_line": 4,
                    "type": [
                        "commented-out"
                    ]
                },
                {
                    "line": 29,
                    "comment": "# # Setting different timeperiod",
                    "char_position_in_line": 4,
                    "type": [
                        "commented-out"
                    ]
                },
                {
                    "line": 45,
                    "comment": "# Parse output as JSON",
                    "char_position_in_line": 12,
                    "type": [
                        "normal"
                    ]
                },
                {
                    "line": 51,
                    "comment": "# Filter comments by time",
                    "char_position_in_line": 12,
                    "type": [
                        "normal"
                    ]
                },
                {
                    "line": 59,
                    "comment": "# Save filtered comments on your system",
                    "char_position_in_line": 4,
                    "type": [
                        "normal"
                    ]
                },
                {
                    "line": 64,
                    "comment": "# analyse_diff_comments(data)",
                    "char_position_in_line": 4,
                    "type": [
                        "commented-out"
                    ]
                }
            ]
        },
        {
            "commit": "76c0e457bfe78effef6b104d334eed6a7fa3e4e4",
            "timestamp": "2024-12-04T23:01:11+01:00",
            "author": "alexander.schranner",
            "diff": {
                "added": {
                    "5": "from build.analysis import analyse_diff_comments, blockify_comments, blockify_comments2, extract_later_modified_comments, clean, average_comment_update_time, classify_comments, classify_content",
                    "81": "    d = classify_content(data)"
                },
                "deleted": {
                    "5": "from build.analysis import analyse_diff_comments, blockify_comments, blockify_comments2, extract_later_modified_comments, clean, average_comment_update_time, classify_comments",
                    "81": "    d = classify_comments(data)"
                }
            },
            "source_code": {
                "1": "# Import modules",
                "2": "from build.pydriller import get_commits_data",
                "3": "from build.comment_lister import run_comment_lister, filter_comments_by_time",
                "4": "from build.utils import save_to_json",
                "5": "from build.analysis import analyse_diff_comments, blockify_comments, blockify_comments2, extract_later_modified_comments, clean, average_comment_update_time, classify_comments, classify_content",
                "6": "from build.xes_conversion import convert_json_to_xes",
                "7": "",
                "8": "# Import packages",
                "9": "import os",
                "10": "import json",
                "11": "import subprocess",
                "12": "import shutil",
                "13": "from datetime import datetime, timezone",
                "14": "",
                "15": "def main():",
                "16": "    # Convert repo URL to path by cloning repo",
                "17": "    repo_url = \"https://github.com/AlexS-1/Bachelor-Code\"",
                "18": "",
                "19": "    repo_name = os.path.basename(repo_url).replace(\".git\", \"\")",
                "20": "    temp_dir = \"/Users/as/Library/Mobile Documents/com~apple~CloudDocs/Dokumente/Studium/Bachelor-Thesis/tmp\"",
                "21": "    clone_path = os.path.join(temp_dir, repo_name)",
                "22": "",
                "23": "    subprocess.run(['git', 'clone', repo_url, clone_path], check=True)",
                "24": "",
                "25": "    # # Paths",
                "26": "    repo_path = clone_path",
                "27": "    jar_path = \"/Users/as/Library/Mobile Documents/com~apple~CloudDocs/Dokumente/Studium/Bachelor-Thesis/CommentLister/target/CommentLister.jar\"",
                "28": "    ",
                "29": "    # # Setting different timeperiod",
                "30": "    start_time = datetime.today().replace(year = datetime.today().year - 1, tzinfo=None, microsecond=0)",
                "31": "    end_time = datetime.today().replace(microsecond=0)",
                "32": "",
                "33": "    file_types = [\".c\", \".c\", \".cc\", \".cp\", \".cpp\", \".cx\", \".cxx\", \".c+\", \".c++\", \".h\", \".hh\", \".hxx\", \".h+\", \".h++\", \".hp\", \".hpp\", \".java\", \".js\", \".cs\", \".py\", \".php\", \".rb\"]",
                "34": "",
                "35": "    commits_data = get_commits_data(repo_path, start_time, end_time, file_types)",
                "36": "    save_to_json(commits_data, \"Data/commits_data.json\")",
                "37": "    ",
                "38": "    with open (\"Data/commits_data.json\", \"r\") as json_file:",
                "39": "        commits_data = json.load(json_file)",
                "40": "",
                "41": "    for file, commits in commits_data.items():",
                "42": "        for commit in commits:",
                "43": "            tag = \"-target=\" + commit[\"commit\"]",
                "44": "            output = run_comment_lister(repo_path, jar_path, tag)",
                "45": "            # Parse output as JSON",
                "46": "            try:",
                "47": "                comment_data = json.loads(output)",
                "48": "            except json.JSONDecodeError as e:",
                "49": "                print(f\"Failed to parse CommentLister output: {e}\")",
                "50": "                return",
                "51": "            # Filter comments by time",
                "52": "            commit_hash, filtered_comments = filter_comments_by_time(comment_data, start_time, end_time)",
                "53": "            if commit[\"commit\"] == commit_hash and file in filtered_comments.keys():",
                "54": "                commit[\"comments\"] = filtered_comments[file]",
                "55": "            else:",
                "56": "                print(\"mismatch in commit and comment data or no comments in this commit for investigatet file\")",
                "57": "                print(\"file could have been deleted\")",
                "58": "                commit[\"comments\"] = {}",
                "59": "    # Save filtered comments on your system",
                "60": "    save_to_json(commits_data, \"Data/filtered_commits_data.json\")",
                "61": "    shutil.rmtree(clone_path)",
                "62": "    with open(\"Data/filtered_commits_data.json\", \"r\") as json_file:",
                "63": "        data = json.load(json_file)",
                "64": "    # analyse_diff_comments(data)",
                "65": "    blockify_comments(data)",
                "66": "    save_to_json(data, \"Exports/blockified_comments_data.json\")",
                "67": "    with open(\"Exports/blockified_comments_data.json\", \"r\") as json_file:",
                "68": "        data = json.load(json_file)",
                "69": "    blockify_comments2(data)",
                "70": "    save_to_json(data, \"Exports/blockified_comments2_data.json\")",
                "71": "    with open(\"Exports/blockified_comments2_data.json\", \"r\") as json_file:",
                "72": "        data = json.load(json_file)",
                "73": "    d = extract_later_modified_comments(data)",
                "74": "    save_to_json(d, \"Exports/analysis_results.json\")",
                "75": "    with open(\"Exports/analysis_results.json\", \"r\") as json_file:",
                "76": "        data = json.load(json_file)",
                "77": "    d = clean(data)",
                "78": "    save_to_json(d, \"Exports/clean_analysis_results.json\")",
                "79": "    with open(\"Exports/clean_analysis_results.json\", \"r\") as json_file:",
                "80": "        data = json.load(json_file)",
                "81": "    d = classify_content(data)",
                "82": "    save_to_json(d, \"Exports/clean_analysis_results2.json\")",
                "83": "    print(\"Average duration:\", average_comment_update_time(d))",
                "84": "    convert_json_to_xes(d, 'Exports/output.xes')",
                "85": "",
                "86": "if __name__ == \"__main__\":",
                "87": "    main()"
            },
            "comments": [
                {
                    "line": 1,
                    "comment": "# Import modules",
                    "char_position_in_line": 0,
                    "type": [
                        "normal"
                    ]
                },
                {
                    "line": 8,
                    "comment": "# Import packages",
                    "char_position_in_line": 0,
                    "type": [
                        "normal"
                    ]
                },
                {
                    "line": 16,
                    "comment": "# Convert repo URL to path by cloning repo",
                    "char_position_in_line": 4,
                    "type": [
                        "normal"
                    ]
                },
                {
                    "line": 25,
                    "comment": "# # Paths",
                    "char_position_in_line": 4,
                    "type": [
                        "commented-out"
                    ]
                },
                {
                    "line": 29,
                    "comment": "# # Setting different timeperiod",
                    "char_position_in_line": 4,
                    "type": [
                        "commented-out"
                    ]
                },
                {
                    "line": 45,
                    "comment": "# Parse output as JSON",
                    "char_position_in_line": 12,
                    "type": [
                        "normal"
                    ]
                },
                {
                    "line": 51,
                    "comment": "# Filter comments by time",
                    "char_position_in_line": 12,
                    "type": [
                        "normal"
                    ]
                },
                {
                    "line": 59,
                    "comment": "# Save filtered comments on your system",
                    "char_position_in_line": 4,
                    "type": [
                        "normal"
                    ]
                },
                {
                    "line": 64,
                    "comment": "# analyse_diff_comments(data)",
                    "char_position_in_line": 4,
                    "type": [
                        "commented-out"
                    ]
                }
            ]
        }
    ],
    "build/utils.py": [
        {
            "commit": "1013981f1c0e06dc3def7ea036c0da903f2e38f6",
            "timestamp": "2024-11-22T16:53:22+01:00",
            "author": "alexander.schranner",
            "diff": {
                "added": {
                    "1": "from pm4py.objects.log.obj import EventLog, Trace, Event",
                    "2": "from pm4py.objects.log.exporter.xes import exporter as xes_exporter",
                    "3": "import json",
                    "4": "",
                    "5": "def save_to_json(data, path):",
                    "6": "    with open(path, 'w') as json_file:",
                    "7": "        json.dump(data, json_file, indent=4)",
                    "8": "",
                    "9": "def save_to_xes(data, path):",
                    "10": "    log = EventLog()",
                    "11": "    # Iterate over each element in the data",
                    "12": "    for file, commits in data.items():",
                    "13": "        # Create a trace for the file",
                    "14": "        trace = Trace()",
                    "15": "        trace.attributes[\"file\"] = file",
                    "16": "        for commit in commits:",
                    "17": "            # Extract event attributes",
                    "18": "            event = Event()",
                    "19": "            event[\"timestamp\"] = commit.get(\"timestamp\")",
                    "20": "            event[\"author\"] = commit.get(\"author\")",
                    "21": "            event[\"change_type\"] = commit.get(\"change_type\")",
                    "22": "            # Add the event to the trace",
                    "23": "            trace.append(event)",
                    "24": "        # Add the trace to the log",
                    "25": "        log.append(trace)",
                    "26": "    xes_exporter.apply(log, path)"
                },
                "deleted": {}
            },
            "source_code": {
                "1": "from pm4py.objects.log.obj import EventLog, Trace, Event",
                "2": "from pm4py.objects.log.exporter.xes import exporter as xes_exporter",
                "3": "import json",
                "4": "",
                "5": "def save_to_json(data, path):",
                "6": "    with open(path, 'w') as json_file:",
                "7": "        json.dump(data, json_file, indent=4)",
                "8": "",
                "9": "def save_to_xes(data, path):",
                "10": "    log = EventLog()",
                "11": "    # Iterate over each element in the data",
                "12": "    for file, commits in data.items():",
                "13": "        # Create a trace for the file",
                "14": "        trace = Trace()",
                "15": "        trace.attributes[\"file\"] = file",
                "16": "        for commit in commits:",
                "17": "            # Extract event attributes",
                "18": "            event = Event()",
                "19": "            event[\"timestamp\"] = commit.get(\"timestamp\")",
                "20": "            event[\"author\"] = commit.get(\"author\")",
                "21": "            event[\"change_type\"] = commit.get(\"change_type\")",
                "22": "            # Add the event to the trace",
                "23": "            trace.append(event)",
                "24": "        # Add the trace to the log",
                "25": "        log.append(trace)",
                "26": "    xes_exporter.apply(log, path)"
            },
            "comments": [
                {
                    "line": 11,
                    "comment": "# Iterate over each element in the data",
                    "char_position_in_line": 4,
                    "type": [
                        "normal"
                    ]
                },
                {
                    "line": 13,
                    "comment": "# Create a trace for the file",
                    "char_position_in_line": 8,
                    "type": [
                        "normal"
                    ]
                },
                {
                    "line": 17,
                    "comment": "# Extract event attributes",
                    "char_position_in_line": 12,
                    "type": [
                        "normal"
                    ]
                },
                {
                    "line": 22,
                    "comment": "# Add the event to the trace",
                    "char_position_in_line": 12,
                    "type": [
                        "normal"
                    ]
                },
                {
                    "line": 24,
                    "comment": "# Add the trace to the log",
                    "char_position_in_line": 8,
                    "type": [
                        "normal"
                    ]
                }
            ]
        },
        {
            "commit": "202468fb39d473251ab81eb3037227cf7af47344",
            "timestamp": "2024-12-04T00:35:22+01:00",
            "author": "alexander.schranner",
            "diff": {
                "added": {
                    "26": "    xes_exporter.apply(log, path)",
                    "27": "",
                    "28": "def list_to_dict(list):",
                    "29": "    dict = {}",
                    "30": "    for i in range(len(list)):",
                    "31": "        dict[i+1] = list[i]",
                    "32": "    return dict"
                },
                "deleted": {
                    "26": "    xes_exporter.apply(log, path)"
                }
            },
            "source_code": {
                "1": "from pm4py.objects.log.obj import EventLog, Trace, Event",
                "2": "from pm4py.objects.log.exporter.xes import exporter as xes_exporter",
                "3": "import json",
                "4": "",
                "5": "def save_to_json(data, path):",
                "6": "    with open(path, 'w') as json_file:",
                "7": "        json.dump(data, json_file, indent=4)",
                "8": "",
                "9": "def save_to_xes(data, path):",
                "10": "    log = EventLog()",
                "11": "    # Iterate over each element in the data",
                "12": "    for file, commits in data.items():",
                "13": "        # Create a trace for the file",
                "14": "        trace = Trace()",
                "15": "        trace.attributes[\"file\"] = file",
                "16": "        for commit in commits:",
                "17": "            # Extract event attributes",
                "18": "            event = Event()",
                "19": "            event[\"timestamp\"] = commit.get(\"timestamp\")",
                "20": "            event[\"author\"] = commit.get(\"author\")",
                "21": "            event[\"change_type\"] = commit.get(\"change_type\")",
                "22": "            # Add the event to the trace",
                "23": "            trace.append(event)",
                "24": "        # Add the trace to the log",
                "25": "        log.append(trace)",
                "26": "    xes_exporter.apply(log, path)",
                "27": "",
                "28": "def list_to_dict(list):",
                "29": "    dict = {}",
                "30": "    for i in range(len(list)):",
                "31": "        dict[i+1] = list[i]",
                "32": "    return dict"
            },
            "comments": [
                {
                    "line": 11,
                    "comment": "# Iterate over each element in the data",
                    "char_position_in_line": 4,
                    "type": [
                        "normal"
                    ]
                },
                {
                    "line": 13,
                    "comment": "# Create a trace for the file",
                    "char_position_in_line": 8,
                    "type": [
                        "normal"
                    ]
                },
                {
                    "line": 17,
                    "comment": "# Extract event attributes",
                    "char_position_in_line": 12,
                    "type": [
                        "normal"
                    ]
                },
                {
                    "line": 22,
                    "comment": "# Add the event to the trace",
                    "char_position_in_line": 12,
                    "type": [
                        "normal"
                    ]
                },
                {
                    "line": 24,
                    "comment": "# Add the trace to the log",
                    "char_position_in_line": 8,
                    "type": [
                        "normal"
                    ]
                }
            ]
        }
    ],
    "build/analysis.py": [
        {
            "commit": "5c23ad83451ad8bade1900f2f5ca41afdd3b4c71",
            "timestamp": "2024-11-23T21:32:59+01:00",
            "author": "alexander.schranner",
            "diff": {
                "added": {
                    "1": "def analyse_diff_comments(data):",
                    "2": "    for file, commits in data.items():",
                    "3": "        for commit in commits:",
                    "4": "            no_change_comments = []",
                    "5": "            for i in range(len(commit[\"comments\"])):",
                    "6": "                if commit[\"comments\"][i][\"line\"] in [item[0] for item in commit[\"diff\"][\"added\"]]:",
                    "7": "                    commit[\"comments\"][i][\"edit\"] = \"added\"",
                    "8": "                if commit[\"comments\"][i][\"line\"] in [item[0] for item in commit[\"diff\"][\"deleted\"]]:",
                    "9": "                    if \"edit\" in list(commit[\"comments\"][i].keys()):",
                    "10": "                        commit[\"comments\"][i][\"edit\"] = \"modified\"",
                    "11": "                        continue",
                    "12": "                    else:",
                    "13": "                        commit[\"comments\"][i][\"edit\"] = \"deleted\"",
                    "14": "                        continue",
                    "15": "                no_change_comments.append(i)",
                    "16": "            # Ensure the gaps of deleted elements are artificially filled by increasing the shift",
                    "17": "            shift = 0",
                    "18": "            for j in no_change_comments:",
                    "19": "                del commit[\"comments\"][j-shift]",
                    "20": "                shift += 1"
                },
                "deleted": {}
            },
            "source_code": {
                "1": "def analyse_diff_comments(data):",
                "2": "    for file, commits in data.items():",
                "3": "        for commit in commits:",
                "4": "            no_change_comments = []",
                "5": "            for i in range(len(commit[\"comments\"])):",
                "6": "                if commit[\"comments\"][i][\"line\"] in [item[0] for item in commit[\"diff\"][\"added\"]]:",
                "7": "                    commit[\"comments\"][i][\"edit\"] = \"added\"",
                "8": "                if commit[\"comments\"][i][\"line\"] in [item[0] for item in commit[\"diff\"][\"deleted\"]]:",
                "9": "                    if \"edit\" in list(commit[\"comments\"][i].keys()):",
                "10": "                        commit[\"comments\"][i][\"edit\"] = \"modified\"",
                "11": "                        continue",
                "12": "                    else:",
                "13": "                        commit[\"comments\"][i][\"edit\"] = \"deleted\"",
                "14": "                        continue",
                "15": "                no_change_comments.append(i)",
                "16": "            # Ensure the gaps of deleted elements are artificially filled by increasing the shift",
                "17": "            shift = 0",
                "18": "            for j in no_change_comments:",
                "19": "                del commit[\"comments\"][j-shift]",
                "20": "                shift += 1"
            },
            "comments": [
                {
                    "line": 16,
                    "comment": "# Ensure the gaps of deleted elements are artificially filled by increasing the shift",
                    "char_position_in_line": 12,
                    "type": [
                        "normal"
                    ]
                }
            ]
        },
        {
            "commit": "b10794953bf2b307859821a8354d3429d710e31b",
            "timestamp": "2024-11-24T18:37:49+01:00",
            "author": "alexander.schranner",
            "diff": {
                "added": {
                    "6": "                for item in commit[\"diff\"][\"added\"]:",
                    "7": "                    if commit[\"comments\"][i][\"line\"] == item[0] :",
                    "8": "                        commit[\"comments\"][i][\"edit\"] = \"added\"",
                    "9": "                        break",
                    "10": "                continue",
                    "23": "                shift += 1",
                    "24": "",
                    "25": "def check_inline_comments(data):",
                    "26": "    return",
                    "27": "",
                    "28": "def blockify_comments(data):",
                    "29": "    for file, commits in data.items():",
                    "30": "        for commit in commits:",
                    "31": "            blocks = []",
                    "32": "            for line in commit[\"diff\"][\"added\"]:",
                    "33": "                # Add line to block, if line follows previous line of block",
                    "34": "                if len(blocks) > 0 and blocks[-1][\"lines\"][-1] + 1 == line[0]:",
                    "35": "                    break_outer = False",
                    "36": "                    comment = \"\"",
                    "37": "                    comment_position = -1",
                    "38": "                    edit = \"\"",
                    "39": "                    for item in commit[\"comments\"]:",
                    "40": "                        if line[0] == item[\"line\"]:",
                    "41": "                            comment = item[\"comment\"]",
                    "42": "                            comment_position = item[\"char_position_in_line\"]",
                    "43": "                            edit = item[\"edit\"]",
                    "44": "                            # Except when line is comment following source code, create new block",
                    "45": "                            if blocks[-1][\"comments\"][-1] == \"\":",
                    "46": "                                block = {",
                    "47": "                                    \"lines\": [line[0]],",
                    "48": "                                    \"contents\": [line[1]],",
                    "49": "                                    \"comments\": [comment],",
                    "50": "                                    \"comment_positions\": [comment_position],",
                    "51": "                                    \"edits\": [edit]",
                    "52": "                                }",
                    "53": "                                blocks.append(block)",
                    "54": "                                break_outer = True",
                    "55": "                    if break_outer:",
                    "56": "                        continue",
                    "57": "                    blocks[-1][\"lines\"].append(line[0])",
                    "58": "                    blocks[-1][\"contents\"].append(line[1])",
                    "59": "                    blocks[-1][\"comments\"].append(comment)",
                    "60": "                    blocks[-1][\"comment_positions\"].append(comment_position)",
                    "61": "                    blocks[-1][\"edits\"].append(edit)",
                    "62": "",
                    "63": "                # Create new block, otherwise",
                    "64": "                else:",
                    "65": "                    comment = \"\"",
                    "66": "                    comment_position = -1",
                    "67": "                    edit = \"\"",
                    "68": "                    for item in commit[\"comments\"]:",
                    "69": "                        if line[0] == item[\"line\"]:",
                    "70": "                            comment = item[\"comment\"]",
                    "71": "                            comment_position = item[\"char_position_in_line\"]",
                    "72": "                            edit = item[\"edit\"]",
                    "73": "                    block = {",
                    "74": "                        \"lines\": [line[0]],",
                    "75": "                        \"contents\": [line[1]],",
                    "76": "                        \"comments\": [comment],",
                    "77": "                        \"comment_positions\": [comment_position],",
                    "78": "                        \"edits\": [edit]",
                    "79": "                    }",
                    "80": "                    blocks.append(block)",
                    "81": "            commit[\"diff\"][\"block_diff\"] = blocks",
                    "82": "",
                    "83": "def extract_later_modified_comments(data):",
                    "84": "    analysis_results = []",
                    "85": "    for file, commits in data.items():",
                    "86": "        # Store last modified timestamps for each line",
                    "87": "        last_modified = {}",
                    "88": "        for commit in commits:",
                    "89": "            # print(\"Starting to analyse commit: \", commit[\"commit\"])",
                    "90": "            commit_time = datetime.fromisoformat(commit[\"timestamp\"])",
                    "91": "            # Track modified lines",
                    "92": "            for line in commit[\"diff\"][\"added\"]:",
                    "93": "                last_modified[line] = commit_time",
                    "94": "            # Compare with comments",
                    "95": "            for line in commit[\"comments\"]:",
                    "96": "                comment_time = datetime.fromisoformat(commit[\"timestamp\"])",
                    "97": "                last_modified_lines = list(last_modified.keys())",
                    "98": "                if int(line) in last_modified_lines:",
                    "99": "                    for block in commit[\"diff\"][\"block_diff\"]:",
                    "100": "                        if line in block[\"comments\"] and len(block[\"lines\"]) == 0:",
                    "101": "                            if(comment_time > last_modified[int(line)]):",
                    "102": "                                analysis_results.append({",
                    "103": "                                    \"file\": file,",
                    "104": "                                    \"line\": int(line),",
                    "105": "                                    \"comment\": commit[\"comment_added_diff\"][line],",
                    "106": "                                    \"comment_time\": str(comment_time),",
                    "107": "                                    \"last_code_change_time\": str(last_modified[int(line)])",
                    "108": "                                })",
                    "109": "    return analysis_results"
                },
                "deleted": {
                    "6": "                if commit[\"comments\"][i][\"line\"] in [item[0] for item in commit[\"diff\"][\"added\"]]:",
                    "7": "                    commit[\"comments\"][i][\"edit\"] = \"added\"",
                    "20": "                shift += 1"
                }
            },
            "source_code": {
                "1": "def analyse_diff_comments(data):",
                "2": "    for file, commits in data.items():",
                "3": "        for commit in commits:",
                "4": "            no_change_comments = []",
                "5": "            for i in range(len(commit[\"comments\"])):",
                "6": "                for item in commit[\"diff\"][\"added\"]:",
                "7": "                    if commit[\"comments\"][i][\"line\"] == item[0] :",
                "8": "                        commit[\"comments\"][i][\"edit\"] = \"added\"",
                "9": "                        break",
                "10": "                continue",
                "11": "                if commit[\"comments\"][i][\"line\"] in [item[0] for item in commit[\"diff\"][\"deleted\"]]:",
                "12": "                    if \"edit\" in list(commit[\"comments\"][i].keys()):",
                "13": "                        commit[\"comments\"][i][\"edit\"] = \"modified\"",
                "14": "                        continue",
                "15": "                    else:",
                "16": "                        commit[\"comments\"][i][\"edit\"] = \"deleted\"",
                "17": "                        continue",
                "18": "                no_change_comments.append(i)",
                "19": "            # Ensure the gaps of deleted elements are artificially filled by increasing the shift",
                "20": "            shift = 0",
                "21": "            for j in no_change_comments:",
                "22": "                del commit[\"comments\"][j-shift]",
                "23": "                shift += 1",
                "24": "",
                "25": "def check_inline_comments(data):",
                "26": "    return",
                "27": "",
                "28": "def blockify_comments(data):",
                "29": "    for file, commits in data.items():",
                "30": "        for commit in commits:",
                "31": "            blocks = []",
                "32": "            for line in commit[\"diff\"][\"added\"]:",
                "33": "                # Add line to block, if line follows previous line of block",
                "34": "                if len(blocks) > 0 and blocks[-1][\"lines\"][-1] + 1 == line[0]:",
                "35": "                    break_outer = False",
                "36": "                    comment = \"\"",
                "37": "                    comment_position = -1",
                "38": "                    edit = \"\"",
                "39": "                    for item in commit[\"comments\"]:",
                "40": "                        if line[0] == item[\"line\"]:",
                "41": "                            comment = item[\"comment\"]",
                "42": "                            comment_position = item[\"char_position_in_line\"]",
                "43": "                            edit = item[\"edit\"]",
                "44": "                            # Except when line is comment following source code, create new block",
                "45": "                            if blocks[-1][\"comments\"][-1] == \"\":",
                "46": "                                block = {",
                "47": "                                    \"lines\": [line[0]],",
                "48": "                                    \"contents\": [line[1]],",
                "49": "                                    \"comments\": [comment],",
                "50": "                                    \"comment_positions\": [comment_position],",
                "51": "                                    \"edits\": [edit]",
                "52": "                                }",
                "53": "                                blocks.append(block)",
                "54": "                                break_outer = True",
                "55": "                    if break_outer:   ",
                "56": "                        continue",
                "57": "                    blocks[-1][\"lines\"].append(line[0])",
                "58": "                    blocks[-1][\"contents\"].append(line[1])",
                "59": "                    blocks[-1][\"comments\"].append(comment)",
                "60": "                    blocks[-1][\"comment_positions\"].append(comment_position)",
                "61": "                    blocks[-1][\"edits\"].append(edit)",
                "62": "",
                "63": "                # Create new block, otherwise",
                "64": "                else:",
                "65": "                    comment = \"\"",
                "66": "                    comment_position = -1",
                "67": "                    edit = \"\"",
                "68": "                    for item in commit[\"comments\"]:",
                "69": "                        if line[0] == item[\"line\"]:",
                "70": "                            comment = item[\"comment\"]",
                "71": "                            comment_position = item[\"char_position_in_line\"]",
                "72": "                            edit = item[\"edit\"]",
                "73": "                    block = {",
                "74": "                        \"lines\": [line[0]],",
                "75": "                        \"contents\": [line[1]],",
                "76": "                        \"comments\": [comment],",
                "77": "                        \"comment_positions\": [comment_position],",
                "78": "                        \"edits\": [edit]",
                "79": "                    }",
                "80": "                    blocks.append(block)",
                "81": "            commit[\"diff\"][\"block_diff\"] = blocks",
                "82": "",
                "83": "def extract_later_modified_comments(data): ",
                "84": "    analysis_results = []",
                "85": "    for file, commits in data.items():",
                "86": "        # Store last modified timestamps for each line",
                "87": "        last_modified = {}",
                "88": "        for commit in commits:",
                "89": "            # print(\"Starting to analyse commit: \", commit[\"commit\"])",
                "90": "            commit_time = datetime.fromisoformat(commit[\"timestamp\"])",
                "91": "            # Track modified lines",
                "92": "            for line in commit[\"diff\"][\"added\"]:",
                "93": "                last_modified[line] = commit_time",
                "94": "            # Compare with comments",
                "95": "            for line in commit[\"comments\"]:",
                "96": "                comment_time = datetime.fromisoformat(commit[\"timestamp\"])",
                "97": "                last_modified_lines = list(last_modified.keys())",
                "98": "                if int(line) in last_modified_lines:",
                "99": "                    for block in commit[\"diff\"][\"block_diff\"]:",
                "100": "                        if line in block[\"comments\"] and len(block[\"lines\"]) == 0:",
                "101": "                            if(comment_time > last_modified[int(line)]):",
                "102": "                                analysis_results.append({",
                "103": "                                    \"file\": file,",
                "104": "                                    \"line\": int(line),",
                "105": "                                    \"comment\": commit[\"comment_added_diff\"][line],",
                "106": "                                    \"comment_time\": str(comment_time),",
                "107": "                                    \"last_code_change_time\": str(last_modified[int(line)])",
                "108": "                                })",
                "109": "    return analysis_results"
            },
            "comments": [
                {
                    "line": 19,
                    "comment": "# Ensure the gaps of deleted elements are artificially filled by increasing the shift",
                    "char_position_in_line": 12,
                    "type": [
                        "normal"
                    ]
                },
                {
                    "line": 33,
                    "comment": "# Add line to block, if line follows previous line of block",
                    "char_position_in_line": 16,
                    "type": [
                        "normal"
                    ]
                },
                {
                    "line": 44,
                    "comment": "# Except when line is comment following source code, create new block",
                    "char_position_in_line": 28,
                    "type": [
                        "normal"
                    ]
                },
                {
                    "line": 63,
                    "comment": "# Create new block, otherwise",
                    "char_position_in_line": 16,
                    "type": [
                        "normal"
                    ]
                },
                {
                    "line": 86,
                    "comment": "# Store last modified timestamps for each line",
                    "char_position_in_line": 8,
                    "type": [
                        "normal"
                    ]
                },
                {
                    "line": 89,
                    "comment": "# print(\"Starting to analyse commit: \", commit[\"commit\"])",
                    "char_position_in_line": 12,
                    "type": [
                        "commented-out"
                    ]
                },
                {
                    "line": 91,
                    "comment": "# Track modified lines",
                    "char_position_in_line": 12,
                    "type": [
                        "normal"
                    ]
                },
                {
                    "line": 94,
                    "comment": "# Compare with comments",
                    "char_position_in_line": 12,
                    "type": [
                        "normal"
                    ]
                }
            ]
        },
        {
            "commit": "e20d03792161ba1b90725e6912b40275f06bf2da",
            "timestamp": "2024-11-25T02:54:47+01:00",
            "author": "alexander.schranner",
            "diff": {
                "added": {
                    "1": "from datetime import datetime",
                    "2": "",
                    "8": "                if commit[\"comments\"][i][\"line\"] in list(commit[\"diff\"][\"added\"].keys()):",
                    "9": "                    commit[\"comments\"][i][\"edit\"] = \"added\"",
                    "11": "                if commit[\"comments\"][i][\"line\"] in list(commit[\"diff\"][\"deleted\"].keys()):",
                    "28": "def blockify_comments2(data):",
                    "31": "            block_diff = []",
                    "32": "            for block in commit[\"diff\"][\"block_diff\"]:",
                    "33": "                block_dict = {}",
                    "34": "                for line in block:",
                    "36": "                        if int(line) == item[\"line\"]:",
                    "37": "                            comment_index = item[\"char_position_in_line\"]",
                    "38": "                            break",
                    "39": "                        else:",
                    "40": "                            comment_index = -1",
                    "41": "                    line_info = {",
                    "42": "                        \"content\": commit[\"diff\"][\"added\"][str(line)],",
                    "43": "                        \"comment_index\": comment_index",
                    "44": "                    }",
                    "45": "                    block_dict[line] = line_info",
                    "46": "                block_diff.append(block_dict)",
                    "47": "            commit[\"diff\"][\"block_diff\"] = block_diff",
                    "49": "def blockify_comments(data):",
                    "50": "    for file, commits in data.items():",
                    "51": "        for commit in commits:",
                    "52": "            blocks = []",
                    "53": "            current_block = []",
                    "54": "            for line in list(commit[\"diff\"][\"added\"].keys()):",
                    "55": "                if int(line) in get_comment_lines(commit[\"comments\"]):",
                    "56": "                    if current_block and current_block[-1] not in get_comment_lines(commit[\"comments\"]):",
                    "57": "                        blocks.append(current_block)",
                    "58": "                        current_block = []",
                    "59": "                    current_block.append(int(line))",
                    "61": "                    if current_block and int(line) != current_block[-1] + 1:",
                    "62": "                        blocks.append(current_block)",
                    "63": "                        current_block = []",
                    "64": "                    current_block.append(int(line))",
                    "65": "            if current_block:",
                    "66": "                blocks.append(current_block)",
                    "69": "def get_comment_lines(comments):",
                    "70": "    comment_lines = []",
                    "71": "    for comment in comments:",
                    "72": "        comment_lines.append(comment[\"line\"])",
                    "73": "    return comment_lines",
                    "74": "",
                    "75": "def get_diff_lines(diff):",
                    "76": "    diff_lines = []",
                    "77": "    for line in diff:",
                    "78": "        diff_lines.append(line[0])",
                    "79": "    return diff_lines",
                    "80": "",
                    "90": "            for line in list(commit[\"diff\"][\"added\"].keys()):",
                    "96": "                if str(line[\"line\"]) in last_modified_lines:",
                    "98": "                        # Where there are comment changes and no source code changes in block",
                    "99": "                        if not is_code_in_block(block): # and block[line[\"line\"]][\"comment_index\"] != -1:",
                    "100": "                            if(comment_time > last_modified[str(line[\"line\"])]):",
                    "101": "                                for item in commit[\"comments\"]:",
                    "102": "                                    if line[\"line\"] == item[\"line\"]:",
                    "103": "                                        comment = item[\"comment\"]",
                    "104": "                                        break",
                    "107": "                                    \"line\": line[\"line\"],",
                    "108": "                                    \"comment\": comment,",
                    "110": "                                    \"last_code_change_time\": str(last_modified[str(line[\"line\"])])",
                    "112": "    return analysis_results",
                    "113": "",
                    "114": "def is_code_in_block(block):",
                    "115": "    for line in list(block.keys()):",
                    "116": "        if block[line][\"comment_index\"] == -1:",
                    "117": "            return True"
                },
                "deleted": {
                    "6": "                for item in commit[\"diff\"][\"added\"]:",
                    "7": "                    if commit[\"comments\"][i][\"line\"] == item[0] :",
                    "8": "                        commit[\"comments\"][i][\"edit\"] = \"added\"",
                    "9": "                        break",
                    "11": "                if commit[\"comments\"][i][\"line\"] in [item[0] for item in commit[\"diff\"][\"deleted\"]]:",
                    "28": "def blockify_comments(data):",
                    "31": "            blocks = []",
                    "32": "            for line in commit[\"diff\"][\"added\"]:",
                    "33": "                # Add line to block, if line follows previous line of block",
                    "34": "                if len(blocks) > 0 and blocks[-1][\"lines\"][-1] + 1 == line[0]:",
                    "35": "                    break_outer = False",
                    "36": "                    comment = \"\"",
                    "37": "                    comment_position = -1",
                    "38": "                    edit = \"\"",
                    "40": "                        if line[0] == item[\"line\"]:",
                    "41": "                            comment = item[\"comment\"]",
                    "42": "                            comment_position = item[\"char_position_in_line\"]",
                    "43": "                            edit = item[\"edit\"]",
                    "44": "                            # Except when line is comment following source code, create new block",
                    "45": "                            if blocks[-1][\"comments\"][-1] == \"\":",
                    "46": "                                block = {",
                    "47": "                                    \"lines\": [line[0]],",
                    "48": "                                    \"contents\": [line[1]],",
                    "49": "                                    \"comments\": [comment],",
                    "50": "                                    \"comment_positions\": [comment_position],",
                    "51": "                                    \"edits\": [edit]",
                    "52": "                                }",
                    "53": "                                blocks.append(block)",
                    "54": "                                break_outer = True",
                    "55": "                    if break_outer:",
                    "56": "                        continue",
                    "57": "                    blocks[-1][\"lines\"].append(line[0])",
                    "58": "                    blocks[-1][\"contents\"].append(line[1])",
                    "59": "                    blocks[-1][\"comments\"].append(comment)",
                    "60": "                    blocks[-1][\"comment_positions\"].append(comment_position)",
                    "61": "                    blocks[-1][\"edits\"].append(edit)",
                    "63": "                # Create new block, otherwise",
                    "65": "                    comment = \"\"",
                    "66": "                    comment_position = -1",
                    "67": "                    edit = \"\"",
                    "68": "                    for item in commit[\"comments\"]:",
                    "69": "                        if line[0] == item[\"line\"]:",
                    "70": "                            comment = item[\"comment\"]",
                    "71": "                            comment_position = item[\"char_position_in_line\"]",
                    "72": "                            edit = item[\"edit\"]",
                    "73": "                    block = {",
                    "74": "                        \"lines\": [line[0]],",
                    "75": "                        \"contents\": [line[1]],",
                    "76": "                        \"comments\": [comment],",
                    "77": "                        \"comment_positions\": [comment_position],",
                    "78": "                        \"edits\": [edit]",
                    "79": "                    }",
                    "80": "                    blocks.append(block)",
                    "92": "            for line in commit[\"diff\"][\"added\"]:",
                    "98": "                if int(line) in last_modified_lines:",
                    "100": "                        if line in block[\"comments\"] and len(block[\"lines\"]) == 0:",
                    "101": "                            if(comment_time > last_modified[int(line)]):",
                    "104": "                                    \"line\": int(line),",
                    "105": "                                    \"comment\": commit[\"comment_added_diff\"][line],",
                    "107": "                                    \"last_code_change_time\": str(last_modified[int(line)])",
                    "109": "    return analysis_results"
                }
            },
            "source_code": {
                "1": "from datetime import datetime",
                "2": "",
                "3": "def analyse_diff_comments(data):",
                "4": "    for file, commits in data.items():",
                "5": "        for commit in commits:",
                "6": "            no_change_comments = []",
                "7": "            for i in range(len(commit[\"comments\"])):",
                "8": "                if commit[\"comments\"][i][\"line\"] in list(commit[\"diff\"][\"added\"].keys()):",
                "9": "                    commit[\"comments\"][i][\"edit\"] = \"added\"",
                "10": "                continue",
                "11": "                if commit[\"comments\"][i][\"line\"] in list(commit[\"diff\"][\"deleted\"].keys()):",
                "12": "                    if \"edit\" in list(commit[\"comments\"][i].keys()):",
                "13": "                        commit[\"comments\"][i][\"edit\"] = \"modified\"",
                "14": "                        continue",
                "15": "                    else:",
                "16": "                        commit[\"comments\"][i][\"edit\"] = \"deleted\"",
                "17": "                        continue",
                "18": "                no_change_comments.append(i)",
                "19": "            # Ensure the gaps of deleted elements are artificially filled by increasing the shift",
                "20": "            shift = 0",
                "21": "            for j in no_change_comments:",
                "22": "                del commit[\"comments\"][j-shift]",
                "23": "                shift += 1",
                "24": "",
                "25": "def check_inline_comments(data):",
                "26": "    return",
                "27": "",
                "28": "def blockify_comments2(data):",
                "29": "    for file, commits in data.items():",
                "30": "        for commit in commits:",
                "31": "            block_diff = []",
                "32": "            for block in commit[\"diff\"][\"block_diff\"]:",
                "33": "                block_dict = {}",
                "34": "                for line in block:",
                "35": "                    for item in commit[\"comments\"]:",
                "36": "                        if int(line) == item[\"line\"]:",
                "37": "                            comment_index = item[\"char_position_in_line\"]",
                "38": "                            break",
                "39": "                        else:",
                "40": "                            comment_index = -1",
                "41": "                    line_info = {",
                "42": "                        \"content\": commit[\"diff\"][\"added\"][str(line)],",
                "43": "                        \"comment_index\": comment_index",
                "44": "                    }",
                "45": "                    block_dict[line] = line_info",
                "46": "                block_diff.append(block_dict)",
                "47": "            commit[\"diff\"][\"block_diff\"] = block_diff",
                "48": "",
                "49": "def blockify_comments(data):",
                "50": "    for file, commits in data.items():",
                "51": "        for commit in commits:",
                "52": "            blocks = []",
                "53": "            current_block = []",
                "54": "            for line in list(commit[\"diff\"][\"added\"].keys()):",
                "55": "                if int(line) in get_comment_lines(commit[\"comments\"]):",
                "56": "                    if current_block and current_block[-1] not in get_comment_lines(commit[\"comments\"]):",
                "57": "                        blocks.append(current_block)",
                "58": "                        current_block = []",
                "59": "                    current_block.append(int(line))",
                "60": "                else:",
                "61": "                    if current_block and int(line) != current_block[-1] + 1:",
                "62": "                        blocks.append(current_block)",
                "63": "                        current_block = []",
                "64": "                    current_block.append(int(line))",
                "65": "            if current_block:",
                "66": "                blocks.append(current_block)",
                "67": "            commit[\"diff\"][\"block_diff\"] = blocks",
                "68": "",
                "69": "def get_comment_lines(comments):",
                "70": "    comment_lines = []",
                "71": "    for comment in comments:",
                "72": "        comment_lines.append(comment[\"line\"])",
                "73": "    return comment_lines",
                "74": "",
                "75": "def get_diff_lines(diff):",
                "76": "    diff_lines = []",
                "77": "    for line in diff:",
                "78": "        diff_lines.append(line[0])",
                "79": "    return diff_lines",
                "80": "",
                "81": "def extract_later_modified_comments(data): ",
                "82": "    analysis_results = []",
                "83": "    for file, commits in data.items():",
                "84": "        # Store last modified timestamps for each line",
                "85": "        last_modified = {}",
                "86": "        for commit in commits:",
                "87": "            # print(\"Starting to analyse commit: \", commit[\"commit\"])",
                "88": "            commit_time = datetime.fromisoformat(commit[\"timestamp\"])",
                "89": "            # Track modified lines",
                "90": "            for line in list(commit[\"diff\"][\"added\"].keys()):",
                "91": "                last_modified[line] = commit_time",
                "92": "            # Compare with comments",
                "93": "            for line in commit[\"comments\"]:",
                "94": "                comment_time = datetime.fromisoformat(commit[\"timestamp\"])",
                "95": "                last_modified_lines = list(last_modified.keys())",
                "96": "                if str(line[\"line\"]) in last_modified_lines:",
                "97": "                    for block in commit[\"diff\"][\"block_diff\"]:",
                "98": "                        # Where there are comment changes and no source code changes in block",
                "99": "                        if not is_code_in_block(block): # and block[line[\"line\"]][\"comment_index\"] != -1:",
                "100": "                            if(comment_time > last_modified[str(line[\"line\"])]):",
                "101": "                                for item in commit[\"comments\"]:",
                "102": "                                    if line[\"line\"] == item[\"line\"]:",
                "103": "                                        comment = item[\"comment\"]",
                "104": "                                        break",
                "105": "                                analysis_results.append({",
                "106": "                                    \"file\": file,",
                "107": "                                    \"line\": line[\"line\"],",
                "108": "                                    \"comment\": comment,",
                "109": "                                    \"comment_time\": str(comment_time),",
                "110": "                                    \"last_code_change_time\": str(last_modified[str(line[\"line\"])])",
                "111": "                                })",
                "112": "    return analysis_results",
                "113": "",
                "114": "def is_code_in_block(block):",
                "115": "    for line in list(block.keys()):",
                "116": "        if block[line][\"comment_index\"] == -1:",
                "117": "            return True "
            },
            "comments": [
                {
                    "line": 19,
                    "comment": "# Ensure the gaps of deleted elements are artificially filled by increasing the shift",
                    "char_position_in_line": 12,
                    "type": [
                        "normal"
                    ]
                },
                {
                    "line": 84,
                    "comment": "# Store last modified timestamps for each line",
                    "char_position_in_line": 8,
                    "type": [
                        "normal"
                    ]
                },
                {
                    "line": 87,
                    "comment": "# print(\"Starting to analyse commit: \", commit[\"commit\"])",
                    "char_position_in_line": 12,
                    "type": [
                        "commented-out"
                    ]
                },
                {
                    "line": 89,
                    "comment": "# Track modified lines",
                    "char_position_in_line": 12,
                    "type": [
                        "normal"
                    ]
                },
                {
                    "line": 92,
                    "comment": "# Compare with comments",
                    "char_position_in_line": 12,
                    "type": [
                        "normal"
                    ]
                },
                {
                    "line": 98,
                    "comment": "# Where there are comment changes and no source code changes in block",
                    "char_position_in_line": 24,
                    "type": [
                        "normal"
                    ]
                },
                {
                    "line": 99,
                    "comment": "# and block[line[\"line\"]][\"comment_index\"] != -1:",
                    "char_position_in_line": 56,
                    "type": [
                        "normal"
                    ]
                }
            ]
        },
        {
            "commit": "6f599e9f229a41cfe89bc628bb60c7565944a0ad",
            "timestamp": "2024-11-25T11:26:40+01:00",
            "author": "alexander.schranner",
            "diff": {
                "added": {
                    "117": "            return True",
                    "118": "",
                    "119": "def clean(data):",
                    "120": "    clean_data = []",
                    "121": "    for i in range(len(data)):",
                    "122": "        item = {",
                    "123": "            \"file\": data[i][\"file\"],",
                    "124": "            \"line\": data[i][\"line\"],",
                    "125": "            \"comment\": data[i][\"comment\"],",
                    "126": "            \"comment_time\": data[i][\"comment_time\"],",
                    "127": "            \"last_code_change_time\": data[i][\"last_code_change_time\"]",
                    "128": "        }",
                    "129": "        if len(data) > i + 1 and not is_equal(data[i], data[i+1]):",
                    "130": "            clean_data.append(item)",
                    "131": "    return clean_data",
                    "132": "",
                    "133": "def is_equal(d1,d2):",
                    "134": "    d1_k = list(d1.keys())",
                    "135": "    d2_k = list(d2.keys())",
                    "136": "    for i in d1_k:",
                    "137": "        if d1[i] != d2[i]:",
                    "138": "            return False",
                    "139": "    return True"
                },
                "deleted": {
                    "117": "            return True"
                }
            },
            "source_code": {
                "1": "from datetime import datetime",
                "2": "",
                "3": "def analyse_diff_comments(data):",
                "4": "    for file, commits in data.items():",
                "5": "        for commit in commits:",
                "6": "            no_change_comments = []",
                "7": "            for i in range(len(commit[\"comments\"])):",
                "8": "                if commit[\"comments\"][i][\"line\"] in list(commit[\"diff\"][\"added\"].keys()):",
                "9": "                    commit[\"comments\"][i][\"edit\"] = \"added\"",
                "10": "                continue",
                "11": "                if commit[\"comments\"][i][\"line\"] in list(commit[\"diff\"][\"deleted\"].keys()):",
                "12": "                    if \"edit\" in list(commit[\"comments\"][i].keys()):",
                "13": "                        commit[\"comments\"][i][\"edit\"] = \"modified\"",
                "14": "                        continue",
                "15": "                    else:",
                "16": "                        commit[\"comments\"][i][\"edit\"] = \"deleted\"",
                "17": "                        continue",
                "18": "                no_change_comments.append(i)",
                "19": "            # Ensure the gaps of deleted elements are artificially filled by increasing the shift",
                "20": "            shift = 0",
                "21": "            for j in no_change_comments:",
                "22": "                del commit[\"comments\"][j-shift]",
                "23": "                shift += 1",
                "24": "",
                "25": "def check_inline_comments(data):",
                "26": "    return",
                "27": "",
                "28": "def blockify_comments2(data):",
                "29": "    for file, commits in data.items():",
                "30": "        for commit in commits:",
                "31": "            block_diff = []",
                "32": "            for block in commit[\"diff\"][\"block_diff\"]:",
                "33": "                block_dict = {}",
                "34": "                for line in block:",
                "35": "                    for item in commit[\"comments\"]:",
                "36": "                        if int(line) == item[\"line\"]:",
                "37": "                            comment_index = item[\"char_position_in_line\"]",
                "38": "                            break",
                "39": "                        else:",
                "40": "                            comment_index = -1",
                "41": "                    line_info = {",
                "42": "                        \"content\": commit[\"diff\"][\"added\"][str(line)],",
                "43": "                        \"comment_index\": comment_index",
                "44": "                    }",
                "45": "                    block_dict[line] = line_info",
                "46": "                block_diff.append(block_dict)",
                "47": "            commit[\"diff\"][\"block_diff\"] = block_diff",
                "48": "",
                "49": "def blockify_comments(data):",
                "50": "    for file, commits in data.items():",
                "51": "        for commit in commits:",
                "52": "            blocks = []",
                "53": "            current_block = []",
                "54": "            for line in list(commit[\"diff\"][\"added\"].keys()):",
                "55": "                if int(line) in get_comment_lines(commit[\"comments\"]):",
                "56": "                    if current_block and current_block[-1] not in get_comment_lines(commit[\"comments\"]):",
                "57": "                        blocks.append(current_block)",
                "58": "                        current_block = []",
                "59": "                    current_block.append(int(line))",
                "60": "                else:",
                "61": "                    if current_block and int(line) != current_block[-1] + 1:",
                "62": "                        blocks.append(current_block)",
                "63": "                        current_block = []",
                "64": "                    current_block.append(int(line))",
                "65": "            if current_block:",
                "66": "                blocks.append(current_block)",
                "67": "            commit[\"diff\"][\"block_diff\"] = blocks",
                "68": "",
                "69": "def get_comment_lines(comments):",
                "70": "    comment_lines = []",
                "71": "    for comment in comments:",
                "72": "        comment_lines.append(comment[\"line\"])",
                "73": "    return comment_lines",
                "74": "",
                "75": "def get_diff_lines(diff):",
                "76": "    diff_lines = []",
                "77": "    for line in diff:",
                "78": "        diff_lines.append(line[0])",
                "79": "    return diff_lines",
                "80": "",
                "81": "def extract_later_modified_comments(data): ",
                "82": "    analysis_results = []",
                "83": "    for file, commits in data.items():",
                "84": "        # Store last modified timestamps for each line",
                "85": "        last_modified = {}",
                "86": "        for commit in commits:",
                "87": "            # print(\"Starting to analyse commit: \", commit[\"commit\"])",
                "88": "            commit_time = datetime.fromisoformat(commit[\"timestamp\"])",
                "89": "            # Track modified lines",
                "90": "            for line in list(commit[\"diff\"][\"added\"].keys()):",
                "91": "                last_modified[line] = commit_time",
                "92": "            # Compare with comments",
                "93": "            for line in commit[\"comments\"]:",
                "94": "                comment_time = datetime.fromisoformat(commit[\"timestamp\"])",
                "95": "                last_modified_lines = list(last_modified.keys())",
                "96": "                if str(line[\"line\"]) in last_modified_lines:",
                "97": "                    for block in commit[\"diff\"][\"block_diff\"]:",
                "98": "                        # Where there are comment changes and no source code changes in block",
                "99": "                        if not is_code_in_block(block): # and block[line[\"line\"]][\"comment_index\"] != -1:",
                "100": "                            if(comment_time > last_modified[str(line[\"line\"])]):",
                "101": "                                for item in commit[\"comments\"]:",
                "102": "                                    if line[\"line\"] == item[\"line\"]:",
                "103": "                                        comment = item[\"comment\"]",
                "104": "                                        break",
                "105": "                                analysis_results.append({",
                "106": "                                    \"file\": file,",
                "107": "                                    \"line\": line[\"line\"],",
                "108": "                                    \"comment\": comment,",
                "109": "                                    \"comment_time\": str(comment_time),",
                "110": "                                    \"last_code_change_time\": str(last_modified[str(line[\"line\"])])",
                "111": "                                })",
                "112": "    return analysis_results",
                "113": "",
                "114": "def is_code_in_block(block):",
                "115": "    for line in list(block.keys()):",
                "116": "        if block[line][\"comment_index\"] == -1:",
                "117": "            return True ",
                "118": "",
                "119": "def clean(data):",
                "120": "    clean_data = []",
                "121": "    for i in range(len(data)):",
                "122": "        item = {",
                "123": "            \"file\": data[i][\"file\"],",
                "124": "            \"line\": data[i][\"line\"],",
                "125": "            \"comment\": data[i][\"comment\"],",
                "126": "            \"comment_time\": data[i][\"comment_time\"],",
                "127": "            \"last_code_change_time\": data[i][\"last_code_change_time\"]",
                "128": "        }",
                "129": "        if len(data) > i + 1 and not is_equal(data[i], data[i+1]):",
                "130": "            clean_data.append(item)",
                "131": "    return clean_data",
                "132": "",
                "133": "def is_equal(d1,d2):",
                "134": "    d1_k = list(d1.keys())",
                "135": "    d2_k = list(d2.keys())",
                "136": "    for i in d1_k:",
                "137": "        if d1[i] != d2[i]:",
                "138": "            return False",
                "139": "    return True"
            },
            "comments": [
                {
                    "line": 19,
                    "comment": "# Ensure the gaps of deleted elements are artificially filled by increasing the shift",
                    "char_position_in_line": 12,
                    "type": [
                        "normal"
                    ]
                },
                {
                    "line": 84,
                    "comment": "# Store last modified timestamps for each line",
                    "char_position_in_line": 8,
                    "type": [
                        "normal"
                    ]
                },
                {
                    "line": 87,
                    "comment": "# print(\"Starting to analyse commit: \", commit[\"commit\"])",
                    "char_position_in_line": 12,
                    "type": [
                        "commented-out"
                    ]
                },
                {
                    "line": 89,
                    "comment": "# Track modified lines",
                    "char_position_in_line": 12,
                    "type": [
                        "normal"
                    ]
                },
                {
                    "line": 92,
                    "comment": "# Compare with comments",
                    "char_position_in_line": 12,
                    "type": [
                        "normal"
                    ]
                },
                {
                    "line": 98,
                    "comment": "# Where there are comment changes and no source code changes in block",
                    "char_position_in_line": 24,
                    "type": [
                        "normal"
                    ]
                },
                {
                    "line": 99,
                    "comment": "# and block[line[\"line\"]][\"comment_index\"] != -1:",
                    "char_position_in_line": 56,
                    "type": [
                        "normal"
                    ]
                }
            ]
        },
        {
            "commit": "ca64e3c56b1e147cafb1af0d143d4f60f04cfbe2",
            "timestamp": "2024-11-28T10:16:59+01:00",
            "author": "alexander.schranner",
            "diff": {
                "added": {
                    "1": "from datetime import datetime, timedelta",
                    "139": "    return True",
                    "140": "",
                    "141": "def average_comment_update_time(data):",
                    "142": "    datetime_pairs = []",
                    "143": "    for file in data:",
                    "144": "        start = datetime.fromisoformat(file[\"last_code_change_time\"])",
                    "145": "        end = datetime.fromisoformat(file[\"comment_time\"])",
                    "146": "        datetime_pairs.append((start, end))",
                    "147": "    durations = [end - start for start, end in datetime_pairs]",
                    "148": "    total_duration = sum(durations, timedelta(0))",
                    "149": "    average_duration = total_duration / len(durations)",
                    "150": "    return average_duration"
                },
                "deleted": {
                    "1": "from datetime import datetime",
                    "139": "    return True"
                }
            },
            "source_code": {
                "1": "from datetime import datetime, timedelta",
                "2": "",
                "3": "def analyse_diff_comments(data):",
                "4": "    for file, commits in data.items():",
                "5": "        for commit in commits:",
                "6": "            no_change_comments = []",
                "7": "            for i in range(len(commit[\"comments\"])):",
                "8": "                if commit[\"comments\"][i][\"line\"] in list(commit[\"diff\"][\"added\"].keys()):",
                "9": "                    commit[\"comments\"][i][\"edit\"] = \"added\"",
                "10": "                continue",
                "11": "                if commit[\"comments\"][i][\"line\"] in list(commit[\"diff\"][\"deleted\"].keys()):",
                "12": "                    if \"edit\" in list(commit[\"comments\"][i].keys()):",
                "13": "                        commit[\"comments\"][i][\"edit\"] = \"modified\"",
                "14": "                        continue",
                "15": "                    else:",
                "16": "                        commit[\"comments\"][i][\"edit\"] = \"deleted\"",
                "17": "                        continue",
                "18": "                no_change_comments.append(i)",
                "19": "            # Ensure the gaps of deleted elements are artificially filled by increasing the shift",
                "20": "            shift = 0",
                "21": "            for j in no_change_comments:",
                "22": "                del commit[\"comments\"][j-shift]",
                "23": "                shift += 1",
                "24": "",
                "25": "def check_inline_comments(data):",
                "26": "    return",
                "27": "",
                "28": "def blockify_comments2(data):",
                "29": "    for file, commits in data.items():",
                "30": "        for commit in commits:",
                "31": "            block_diff = []",
                "32": "            for block in commit[\"diff\"][\"block_diff\"]:",
                "33": "                block_dict = {}",
                "34": "                for line in block:",
                "35": "                    for item in commit[\"comments\"]:",
                "36": "                        if int(line) == item[\"line\"]:",
                "37": "                            comment_index = item[\"char_position_in_line\"]",
                "38": "                            break",
                "39": "                        else:",
                "40": "                            comment_index = -1",
                "41": "                    line_info = {",
                "42": "                        \"content\": commit[\"diff\"][\"added\"][str(line)],",
                "43": "                        \"comment_index\": comment_index",
                "44": "                    }",
                "45": "                    block_dict[line] = line_info",
                "46": "                block_diff.append(block_dict)",
                "47": "            commit[\"diff\"][\"block_diff\"] = block_diff",
                "48": "",
                "49": "def blockify_comments(data):",
                "50": "    for file, commits in data.items():",
                "51": "        for commit in commits:",
                "52": "            blocks = []",
                "53": "            current_block = []",
                "54": "            for line in list(commit[\"diff\"][\"added\"].keys()):",
                "55": "                if int(line) in get_comment_lines(commit[\"comments\"]):",
                "56": "                    if current_block and current_block[-1] not in get_comment_lines(commit[\"comments\"]):",
                "57": "                        blocks.append(current_block)",
                "58": "                        current_block = []",
                "59": "                    current_block.append(int(line))",
                "60": "                else:",
                "61": "                    if current_block and int(line) != current_block[-1] + 1:",
                "62": "                        blocks.append(current_block)",
                "63": "                        current_block = []",
                "64": "                    current_block.append(int(line))",
                "65": "            if current_block:",
                "66": "                blocks.append(current_block)",
                "67": "            commit[\"diff\"][\"block_diff\"] = blocks",
                "68": "",
                "69": "def get_comment_lines(comments):",
                "70": "    comment_lines = []",
                "71": "    for comment in comments:",
                "72": "        comment_lines.append(comment[\"line\"])",
                "73": "    return comment_lines",
                "74": "",
                "75": "def get_diff_lines(diff):",
                "76": "    diff_lines = []",
                "77": "    for line in diff:",
                "78": "        diff_lines.append(line[0])",
                "79": "    return diff_lines",
                "80": "",
                "81": "def extract_later_modified_comments(data): ",
                "82": "    analysis_results = []",
                "83": "    for file, commits in data.items():",
                "84": "        # Store last modified timestamps for each line",
                "85": "        last_modified = {}",
                "86": "        for commit in commits:",
                "87": "            # print(\"Starting to analyse commit: \", commit[\"commit\"])",
                "88": "            commit_time = datetime.fromisoformat(commit[\"timestamp\"])",
                "89": "            # Track modified lines",
                "90": "            for line in list(commit[\"diff\"][\"added\"].keys()):",
                "91": "                last_modified[line] = commit_time",
                "92": "            # Compare with comments",
                "93": "            for line in commit[\"comments\"]:",
                "94": "                comment_time = datetime.fromisoformat(commit[\"timestamp\"])",
                "95": "                last_modified_lines = list(last_modified.keys())",
                "96": "                if str(line[\"line\"]) in last_modified_lines:",
                "97": "                    for block in commit[\"diff\"][\"block_diff\"]:",
                "98": "                        # Where there are comment changes and no source code changes in block",
                "99": "                        if not is_code_in_block(block): # and block[line[\"line\"]][\"comment_index\"] != -1:",
                "100": "                            if(comment_time > last_modified[str(line[\"line\"])]):",
                "101": "                                for item in commit[\"comments\"]:",
                "102": "                                    if line[\"line\"] == item[\"line\"]:",
                "103": "                                        comment = item[\"comment\"]",
                "104": "                                        break",
                "105": "                                analysis_results.append({",
                "106": "                                    \"file\": file,",
                "107": "                                    \"line\": line[\"line\"],",
                "108": "                                    \"comment\": comment,",
                "109": "                                    \"comment_time\": str(comment_time),",
                "110": "                                    \"last_code_change_time\": str(last_modified[str(line[\"line\"])])",
                "111": "                                })",
                "112": "    return analysis_results",
                "113": "",
                "114": "def is_code_in_block(block):",
                "115": "    for line in list(block.keys()):",
                "116": "        if block[line][\"comment_index\"] == -1:",
                "117": "            return True ",
                "118": "",
                "119": "def clean(data):",
                "120": "    clean_data = []",
                "121": "    for i in range(len(data)):",
                "122": "        item = {",
                "123": "            \"file\": data[i][\"file\"],",
                "124": "            \"line\": data[i][\"line\"],",
                "125": "            \"comment\": data[i][\"comment\"],",
                "126": "            \"comment_time\": data[i][\"comment_time\"],",
                "127": "            \"last_code_change_time\": data[i][\"last_code_change_time\"]",
                "128": "        }",
                "129": "        if len(data) > i + 1 and not is_equal(data[i], data[i+1]):",
                "130": "            clean_data.append(item)",
                "131": "    return clean_data",
                "132": "",
                "133": "def is_equal(d1,d2):",
                "134": "    d1_k = list(d1.keys())",
                "135": "    d2_k = list(d2.keys())",
                "136": "    for i in d1_k:",
                "137": "        if d1[i] != d2[i]:",
                "138": "            return False",
                "139": "    return True",
                "140": "",
                "141": "def average_comment_update_time(data):",
                "142": "    datetime_pairs = []",
                "143": "    for file in data:",
                "144": "        start = datetime.fromisoformat(file[\"last_code_change_time\"])",
                "145": "        end = datetime.fromisoformat(file[\"comment_time\"])",
                "146": "        datetime_pairs.append((start, end))",
                "147": "    durations = [end - start for start, end in datetime_pairs]",
                "148": "    total_duration = sum(durations, timedelta(0))",
                "149": "    average_duration = total_duration / len(durations)",
                "150": "    return average_duration"
            },
            "comments": [
                {
                    "line": 19,
                    "comment": "# Ensure the gaps of deleted elements are artificially filled by increasing the shift",
                    "char_position_in_line": 12,
                    "type": [
                        "normal"
                    ]
                },
                {
                    "line": 84,
                    "comment": "# Store last modified timestamps for each line",
                    "char_position_in_line": 8,
                    "type": [
                        "normal"
                    ]
                },
                {
                    "line": 87,
                    "comment": "# print(\"Starting to analyse commit: \", commit[\"commit\"])",
                    "char_position_in_line": 12,
                    "type": [
                        "commented-out"
                    ]
                },
                {
                    "line": 89,
                    "comment": "# Track modified lines",
                    "char_position_in_line": 12,
                    "type": [
                        "normal"
                    ]
                },
                {
                    "line": 92,
                    "comment": "# Compare with comments",
                    "char_position_in_line": 12,
                    "type": [
                        "normal"
                    ]
                },
                {
                    "line": 98,
                    "comment": "# Where there are comment changes and no source code changes in block",
                    "char_position_in_line": 24,
                    "type": [
                        "normal"
                    ]
                },
                {
                    "line": 99,
                    "comment": "# and block[line[\"line\"]][\"comment_index\"] != -1:",
                    "char_position_in_line": 56,
                    "type": [
                        "normal"
                    ]
                }
            ]
        },
        {
            "commit": "202468fb39d473251ab81eb3037227cf7af47344",
            "timestamp": "2024-12-04T00:35:22+01:00",
            "author": "alexander.schranner",
            "diff": {
                "added": {
                    "2": "import tokenize",
                    "3": "from io import StringIO",
                    "6": "    \"\"\" some test docstring \"\"\"",
                    "99": "                # TODO investigate why line is null",
                    "100": "                if line != \"null\" and str(line[\"line\"]) in last_modified_lines:",
                    "104": "                            if comment_time > last_modified[str(line[\"line\"])]:",
                    "109": "                                for commit2 in commits:",
                    "110": "                                    if datetime.fromisoformat(commit2[\"timestamp\"]) == comment_time:",
                    "111": "                                        content = commit2[\"source_code\"][str(line[\"line\"])]",
                    "112": "                                        break",
                    "113": "                                    else:",
                    "114": "                                        content = \"PROBLEM\"",
                    "118": "                                    \"content\": content,",
                    "136": "            \"content\": data[i][\"content\"], # Cheeky inline comment",
                    "162": "    return average_duration",
                    "163": "",
                    "164": "def classify_comments(data):",
                    "165": "    for comment in data:",
                    "166": "        line = comment[\"content\"]",
                    "167": "        comment_type = \"\"",
                    "168": "",
                    "169": "        # Tokenize the input code",
                    "170": "        tokens = tokenize.generate_tokens(StringIO(line).readline)",
                    "171": "        prev_token = None",
                    "172": "",
                    "173": "        for token in tokens:",
                    "174": "            token_type, token_string, start, end, line = token",
                    "175": "",
                    "176": "            if token_type == tokenize.COMMENT:",
                    "177": "                comment_text = token_string.lstrip(\"#\").strip()",
                    "178": "",
                    "179": "                # Check if inline",
                    "180": "                if prev_token and prev_token.type != tokenize.NL:",
                    "181": "                    comment_type = \"inline\"",
                    "182": "",
                    "183": "                # Check for block comments (multi-line consecutive)",
                    "184": "                elif comment_text and comment_text[0].isalpha():",
                    "185": "                    comment_type = \"block\"",
                    "186": "",
                    "187": "                # Check for commented-out code (basic heuristic: looks like valid Python code)",
                    "188": "                elif is_potential_code(comment_text):",
                    "189": "                    comment_type = \"commented out\"",
                    "190": "",
                    "191": "                else:",
                    "192": "                    comment_type = \"normal annotation\"",
                    "193": "",
                    "194": "",
                    "195": "            elif token_type == tokenize.STRING:",
                    "196": "                # Check for docstring: string token at module, function, or class start",
                    "197": "                if prev_token and prev_token.type in {tokenize.DEDENT, tokenize.INDENT}:",
                    "198": "                    comment_type = \"documentation\"",
                    "199": "",
                    "200": "            prev_token = token",
                    "201": "",
                    "202": "        comment[\"comment_type\"] = comment_type",
                    "203": "",
                    "204": "    return data",
                    "205": "",
                    "206": "def is_potential_code(text):",
                    "207": "    try:",
                    "208": "        compile(text, \"<string>\", \"exec\")",
                    "209": "        return True",
                    "210": "    except SyntaxError:",
                    "211": "        return False"
                },
                "deleted": {
                    "96": "                if str(line[\"line\"]) in last_modified_lines:",
                    "100": "                            if(comment_time > last_modified[str(line[\"line\"])]):",
                    "150": "    return average_duration"
                }
            },
            "source_code": {
                "1": "from datetime import datetime, timedelta",
                "2": "import tokenize",
                "3": "from io import StringIO",
                "4": "",
                "5": "def analyse_diff_comments(data):",
                "6": "    \"\"\" some test docstring \"\"\"",
                "7": "    for file, commits in data.items():",
                "8": "        for commit in commits:",
                "9": "            no_change_comments = []",
                "10": "            for i in range(len(commit[\"comments\"])):",
                "11": "                if commit[\"comments\"][i][\"line\"] in list(commit[\"diff\"][\"added\"].keys()):",
                "12": "                    commit[\"comments\"][i][\"edit\"] = \"added\"",
                "13": "                continue",
                "14": "                if commit[\"comments\"][i][\"line\"] in list(commit[\"diff\"][\"deleted\"].keys()):",
                "15": "                    if \"edit\" in list(commit[\"comments\"][i].keys()):",
                "16": "                        commit[\"comments\"][i][\"edit\"] = \"modified\"",
                "17": "                        continue",
                "18": "                    else:",
                "19": "                        commit[\"comments\"][i][\"edit\"] = \"deleted\"",
                "20": "                        continue",
                "21": "                no_change_comments.append(i)",
                "22": "            # Ensure the gaps of deleted elements are artificially filled by increasing the shift",
                "23": "            shift = 0",
                "24": "            for j in no_change_comments:",
                "25": "                del commit[\"comments\"][j-shift]",
                "26": "                shift += 1",
                "27": "",
                "28": "def check_inline_comments(data):",
                "29": "    return",
                "30": "",
                "31": "def blockify_comments2(data):",
                "32": "    for file, commits in data.items():",
                "33": "        for commit in commits:",
                "34": "            block_diff = []",
                "35": "            for block in commit[\"diff\"][\"block_diff\"]:",
                "36": "                block_dict = {}",
                "37": "                for line in block:",
                "38": "                    for item in commit[\"comments\"]:",
                "39": "                        if int(line) == item[\"line\"]:",
                "40": "                            comment_index = item[\"char_position_in_line\"]",
                "41": "                            break",
                "42": "                        else:",
                "43": "                            comment_index = -1",
                "44": "                    line_info = {",
                "45": "                        \"content\": commit[\"diff\"][\"added\"][str(line)],",
                "46": "                        \"comment_index\": comment_index",
                "47": "                    }",
                "48": "                    block_dict[line] = line_info",
                "49": "                block_diff.append(block_dict)",
                "50": "            commit[\"diff\"][\"block_diff\"] = block_diff",
                "51": "",
                "52": "def blockify_comments(data):",
                "53": "    for file, commits in data.items():",
                "54": "        for commit in commits:",
                "55": "            blocks = []",
                "56": "            current_block = []",
                "57": "            for line in list(commit[\"diff\"][\"added\"].keys()):",
                "58": "                if int(line) in get_comment_lines(commit[\"comments\"]):",
                "59": "                    if current_block and current_block[-1] not in get_comment_lines(commit[\"comments\"]):",
                "60": "                        blocks.append(current_block)",
                "61": "                        current_block = []",
                "62": "                    current_block.append(int(line))",
                "63": "                else:",
                "64": "                    if current_block and int(line) != current_block[-1] + 1:",
                "65": "                        blocks.append(current_block)",
                "66": "                        current_block = []",
                "67": "                    current_block.append(int(line))",
                "68": "            if current_block:",
                "69": "                blocks.append(current_block)",
                "70": "            commit[\"diff\"][\"block_diff\"] = blocks",
                "71": "",
                "72": "def get_comment_lines(comments):",
                "73": "    comment_lines = []",
                "74": "    for comment in comments:",
                "75": "        comment_lines.append(comment[\"line\"])",
                "76": "    return comment_lines",
                "77": "",
                "78": "def get_diff_lines(diff):",
                "79": "    diff_lines = []",
                "80": "    for line in diff:",
                "81": "        diff_lines.append(line[0])",
                "82": "    return diff_lines",
                "83": "",
                "84": "def extract_later_modified_comments(data): ",
                "85": "    analysis_results = []",
                "86": "    for file, commits in data.items():",
                "87": "        # Store last modified timestamps for each line",
                "88": "        last_modified = {}",
                "89": "        for commit in commits:",
                "90": "            # print(\"Starting to analyse commit: \", commit[\"commit\"])",
                "91": "            commit_time = datetime.fromisoformat(commit[\"timestamp\"])",
                "92": "            # Track modified lines",
                "93": "            for line in list(commit[\"diff\"][\"added\"].keys()):",
                "94": "                last_modified[line] = commit_time",
                "95": "            # Compare with comments",
                "96": "            for line in commit[\"comments\"]:",
                "97": "                comment_time = datetime.fromisoformat(commit[\"timestamp\"])",
                "98": "                last_modified_lines = list(last_modified.keys())",
                "99": "                # TODO investigate why line is null",
                "100": "                if line != \"null\" and str(line[\"line\"]) in last_modified_lines:",
                "101": "                    for block in commit[\"diff\"][\"block_diff\"]:",
                "102": "                        # Where there are comment changes and no source code changes in block",
                "103": "                        if not is_code_in_block(block): # and block[line[\"line\"]][\"comment_index\"] != -1:",
                "104": "                            if comment_time > last_modified[str(line[\"line\"])]:",
                "105": "                                for item in commit[\"comments\"]:",
                "106": "                                    if line[\"line\"] == item[\"line\"]:",
                "107": "                                        comment = item[\"comment\"]",
                "108": "                                        break",
                "109": "                                for commit2 in commits:",
                "110": "                                    if datetime.fromisoformat(commit2[\"timestamp\"]) == comment_time:",
                "111": "                                        content = commit2[\"source_code\"][str(line[\"line\"])]",
                "112": "                                        break",
                "113": "                                    else:",
                "114": "                                        content = \"PROBLEM\"",
                "115": "                                analysis_results.append({",
                "116": "                                    \"file\": file,",
                "117": "                                    \"line\": line[\"line\"],",
                "118": "                                    \"content\": content,",
                "119": "                                    \"comment\": comment,",
                "120": "                                    \"comment_time\": str(comment_time),",
                "121": "                                    \"last_code_change_time\": str(last_modified[str(line[\"line\"])])",
                "122": "                                })",
                "123": "    return analysis_results",
                "124": "",
                "125": "def is_code_in_block(block):",
                "126": "    for line in list(block.keys()):",
                "127": "        if block[line][\"comment_index\"] == -1:",
                "128": "            return True ",
                "129": "",
                "130": "def clean(data):",
                "131": "    clean_data = []",
                "132": "    for i in range(len(data)):",
                "133": "        item = {",
                "134": "            \"file\": data[i][\"file\"],",
                "135": "            \"line\": data[i][\"line\"],",
                "136": "            \"content\": data[i][\"content\"], # Cheeky inline comment",
                "137": "            \"comment\": data[i][\"comment\"],",
                "138": "            \"comment_time\": data[i][\"comment_time\"],",
                "139": "            \"last_code_change_time\": data[i][\"last_code_change_time\"]",
                "140": "        }",
                "141": "        if len(data) > i + 1 and not is_equal(data[i], data[i+1]):",
                "142": "            clean_data.append(item)",
                "143": "    return clean_data",
                "144": "",
                "145": "def is_equal(d1,d2):",
                "146": "    d1_k = list(d1.keys())",
                "147": "    d2_k = list(d2.keys())",
                "148": "    for i in d1_k:",
                "149": "        if d1[i] != d2[i]:",
                "150": "            return False",
                "151": "    return True",
                "152": "",
                "153": "def average_comment_update_time(data):",
                "154": "    datetime_pairs = []",
                "155": "    for file in data:",
                "156": "        start = datetime.fromisoformat(file[\"last_code_change_time\"])",
                "157": "        end = datetime.fromisoformat(file[\"comment_time\"])",
                "158": "        datetime_pairs.append((start, end))",
                "159": "    durations = [end - start for start, end in datetime_pairs]",
                "160": "    total_duration = sum(durations, timedelta(0))",
                "161": "    average_duration = total_duration / len(durations)",
                "162": "    return average_duration",
                "163": "",
                "164": "def classify_comments(data):",
                "165": "    for comment in data:",
                "166": "        line = comment[\"content\"]",
                "167": "        comment_type = \"\"",
                "168": "",
                "169": "        # Tokenize the input code",
                "170": "        tokens = tokenize.generate_tokens(StringIO(line).readline)",
                "171": "        prev_token = None",
                "172": "",
                "173": "        for token in tokens:",
                "174": "            token_type, token_string, start, end, line = token",
                "175": "            ",
                "176": "            if token_type == tokenize.COMMENT:",
                "177": "                comment_text = token_string.lstrip(\"#\").strip()",
                "178": "",
                "179": "                # Check if inline",
                "180": "                if prev_token and prev_token.type != tokenize.NL:",
                "181": "                    comment_type = \"inline\"",
                "182": "                ",
                "183": "                # Check for block comments (multi-line consecutive)",
                "184": "                elif comment_text and comment_text[0].isalpha():",
                "185": "                    comment_type = \"block\"",
                "186": "",
                "187": "                # Check for commented-out code (basic heuristic: looks like valid Python code)",
                "188": "                elif is_potential_code(comment_text):",
                "189": "                    comment_type = \"commented out\"",
                "190": "                ",
                "191": "                else:",
                "192": "                    comment_type = \"normal annotation\"",
                "193": "",
                "194": "",
                "195": "            elif token_type == tokenize.STRING:",
                "196": "                # Check for docstring: string token at module, function, or class start",
                "197": "                if prev_token and prev_token.type in {tokenize.DEDENT, tokenize.INDENT}:",
                "198": "                    comment_type = \"documentation\"",
                "199": "",
                "200": "            prev_token = token",
                "201": "",
                "202": "        comment[\"comment_type\"] = comment_type",
                "203": "        ",
                "204": "    return data",
                "205": "",
                "206": "def is_potential_code(text):",
                "207": "    try:",
                "208": "        compile(text, \"<string>\", \"exec\")",
                "209": "        return True",
                "210": "    except SyntaxError:",
                "211": "        return False"
            },
            "comments": [
                {
                    "line": 6,
                    "comment": "\"\"\" some test docstring \"\"\"",
                    "char_position_in_line": 4,
                    "type": [
                        "docstring"
                    ]
                },
                {
                    "line": 22,
                    "comment": "# Ensure the gaps of deleted elements are artificially filled by increasing the shift",
                    "char_position_in_line": 12,
                    "type": [
                        "normal"
                    ]
                },
                {
                    "line": 87,
                    "comment": "# Store last modified timestamps for each line",
                    "char_position_in_line": 8,
                    "type": [
                        "normal"
                    ]
                },
                {
                    "line": 90,
                    "comment": "# print(\"Starting to analyse commit: \", commit[\"commit\"])",
                    "char_position_in_line": 12,
                    "type": [
                        "commented-out"
                    ]
                },
                {
                    "line": 92,
                    "comment": "# Track modified lines",
                    "char_position_in_line": 12,
                    "type": [
                        "normal"
                    ]
                },
                {
                    "line": 95,
                    "comment": "# Compare with comments",
                    "char_position_in_line": 12,
                    "type": [
                        "normal"
                    ]
                },
                {
                    "line": 99,
                    "comment": "# TODO investigate why line is null",
                    "char_position_in_line": 16,
                    "type": [
                        "normal"
                    ]
                },
                {
                    "line": 102,
                    "comment": "# Where there are comment changes and no source code changes in block",
                    "char_position_in_line": 24,
                    "type": [
                        "normal"
                    ]
                },
                {
                    "line": 103,
                    "comment": "# and block[line[\"line\"]][\"comment_index\"] != -1:",
                    "char_position_in_line": 56,
                    "type": [
                        "normal"
                    ]
                },
                {
                    "line": 136,
                    "comment": "# Cheeky inline comment",
                    "char_position_in_line": 43,
                    "type": [
                        "normal"
                    ]
                },
                {
                    "line": 169,
                    "comment": "# Tokenize the input code",
                    "char_position_in_line": 8,
                    "type": [
                        "normal"
                    ]
                },
                {
                    "line": 179,
                    "comment": "# Check if inline",
                    "char_position_in_line": 16,
                    "type": [
                        "normal"
                    ]
                },
                {
                    "line": 183,
                    "comment": "# Check for block comments (multi-line consecutive)",
                    "char_position_in_line": 16,
                    "type": [
                        "normal"
                    ]
                },
                {
                    "line": 187,
                    "comment": "# Check for commented-out code (basic heuristic: looks like valid Python code)",
                    "char_position_in_line": 16,
                    "type": [
                        "normal"
                    ]
                },
                {
                    "line": 196,
                    "comment": "# Check for docstring: string token at module, function, or class start",
                    "char_position_in_line": 16,
                    "type": [
                        "normal"
                    ]
                }
            ]
        },
        {
            "commit": "22aa658dddb9457b7fae9f52febd52afdf2a413e",
            "timestamp": "2024-12-04T00:55:39+01:00",
            "author": "alexander.schranner",
            "diff": {
                "added": {
                    "137": "            \"comment\": data[i][\"comment\"], # Cheeky 2 inline comment",
                    "181": "                # Check for block comments (multi-line consecutive)",
                    "182": "                elif comment_text and comment_text[0].isalpha():",
                    "183": "                    comment_type = \"block\""
                },
                "deleted": {
                    "137": "            \"comment\": data[i][\"comment\"],",
                    "168": "",
                    "172": "",
                    "175": "",
                    "178": "",
                    "182": "",
                    "183": "                # Check for block comments (multi-line consecutive)",
                    "184": "                elif comment_text and comment_text[0].isalpha():",
                    "185": "                    comment_type = \"block\"",
                    "186": "",
                    "190": "",
                    "193": "",
                    "194": "",
                    "199": "",
                    "201": "",
                    "203": ""
                }
            },
            "source_code": {
                "1": "from datetime import datetime, timedelta",
                "2": "import tokenize",
                "3": "from io import StringIO",
                "4": "",
                "5": "def analyse_diff_comments(data):",
                "6": "    \"\"\" some test docstring \"\"\"",
                "7": "    for file, commits in data.items():",
                "8": "        for commit in commits:",
                "9": "            no_change_comments = []",
                "10": "            for i in range(len(commit[\"comments\"])):",
                "11": "                if commit[\"comments\"][i][\"line\"] in list(commit[\"diff\"][\"added\"].keys()):",
                "12": "                    commit[\"comments\"][i][\"edit\"] = \"added\"",
                "13": "                continue",
                "14": "                if commit[\"comments\"][i][\"line\"] in list(commit[\"diff\"][\"deleted\"].keys()):",
                "15": "                    if \"edit\" in list(commit[\"comments\"][i].keys()):",
                "16": "                        commit[\"comments\"][i][\"edit\"] = \"modified\"",
                "17": "                        continue",
                "18": "                    else:",
                "19": "                        commit[\"comments\"][i][\"edit\"] = \"deleted\"",
                "20": "                        continue",
                "21": "                no_change_comments.append(i)",
                "22": "            # Ensure the gaps of deleted elements are artificially filled by increasing the shift",
                "23": "            shift = 0",
                "24": "            for j in no_change_comments:",
                "25": "                del commit[\"comments\"][j-shift]",
                "26": "                shift += 1",
                "27": "",
                "28": "def check_inline_comments(data):",
                "29": "    return",
                "30": "",
                "31": "def blockify_comments2(data):",
                "32": "    for file, commits in data.items():",
                "33": "        for commit in commits:",
                "34": "            block_diff = []",
                "35": "            for block in commit[\"diff\"][\"block_diff\"]:",
                "36": "                block_dict = {}",
                "37": "                for line in block:",
                "38": "                    for item in commit[\"comments\"]:",
                "39": "                        if int(line) == item[\"line\"]:",
                "40": "                            comment_index = item[\"char_position_in_line\"]",
                "41": "                            break",
                "42": "                        else:",
                "43": "                            comment_index = -1",
                "44": "                    line_info = {",
                "45": "                        \"content\": commit[\"diff\"][\"added\"][str(line)],",
                "46": "                        \"comment_index\": comment_index",
                "47": "                    }",
                "48": "                    block_dict[line] = line_info",
                "49": "                block_diff.append(block_dict)",
                "50": "            commit[\"diff\"][\"block_diff\"] = block_diff",
                "51": "",
                "52": "def blockify_comments(data):",
                "53": "    for file, commits in data.items():",
                "54": "        for commit in commits:",
                "55": "            blocks = []",
                "56": "            current_block = []",
                "57": "            for line in list(commit[\"diff\"][\"added\"].keys()):",
                "58": "                if int(line) in get_comment_lines(commit[\"comments\"]):",
                "59": "                    if current_block and current_block[-1] not in get_comment_lines(commit[\"comments\"]):",
                "60": "                        blocks.append(current_block)",
                "61": "                        current_block = []",
                "62": "                    current_block.append(int(line))",
                "63": "                else:",
                "64": "                    if current_block and int(line) != current_block[-1] + 1:",
                "65": "                        blocks.append(current_block)",
                "66": "                        current_block = []",
                "67": "                    current_block.append(int(line))",
                "68": "            if current_block:",
                "69": "                blocks.append(current_block)",
                "70": "            commit[\"diff\"][\"block_diff\"] = blocks",
                "71": "",
                "72": "def get_comment_lines(comments):",
                "73": "    comment_lines = []",
                "74": "    for comment in comments:",
                "75": "        comment_lines.append(comment[\"line\"])",
                "76": "    return comment_lines",
                "77": "",
                "78": "def get_diff_lines(diff):",
                "79": "    diff_lines = []",
                "80": "    for line in diff:",
                "81": "        diff_lines.append(line[0])",
                "82": "    return diff_lines",
                "83": "",
                "84": "def extract_later_modified_comments(data): ",
                "85": "    analysis_results = []",
                "86": "    for file, commits in data.items():",
                "87": "        # Store last modified timestamps for each line",
                "88": "        last_modified = {}",
                "89": "        for commit in commits:",
                "90": "            # print(\"Starting to analyse commit: \", commit[\"commit\"])",
                "91": "            commit_time = datetime.fromisoformat(commit[\"timestamp\"])",
                "92": "            # Track modified lines",
                "93": "            for line in list(commit[\"diff\"][\"added\"].keys()):",
                "94": "                last_modified[line] = commit_time",
                "95": "            # Compare with comments",
                "96": "            for line in commit[\"comments\"]:",
                "97": "                comment_time = datetime.fromisoformat(commit[\"timestamp\"])",
                "98": "                last_modified_lines = list(last_modified.keys())",
                "99": "                # TODO investigate why line is null",
                "100": "                if line != \"null\" and str(line[\"line\"]) in last_modified_lines:",
                "101": "                    for block in commit[\"diff\"][\"block_diff\"]:",
                "102": "                        # Where there are comment changes and no source code changes in block",
                "103": "                        if not is_code_in_block(block): # and block[line[\"line\"]][\"comment_index\"] != -1:",
                "104": "                            if comment_time > last_modified[str(line[\"line\"])]:",
                "105": "                                for item in commit[\"comments\"]:",
                "106": "                                    if line[\"line\"] == item[\"line\"]:",
                "107": "                                        comment = item[\"comment\"]",
                "108": "                                        break",
                "109": "                                for commit2 in commits:",
                "110": "                                    if datetime.fromisoformat(commit2[\"timestamp\"]) == comment_time:",
                "111": "                                        content = commit2[\"source_code\"][str(line[\"line\"])]",
                "112": "                                        break",
                "113": "                                    else:",
                "114": "                                        content = \"PROBLEM\"",
                "115": "                                analysis_results.append({",
                "116": "                                    \"file\": file,",
                "117": "                                    \"line\": line[\"line\"],",
                "118": "                                    \"content\": content,",
                "119": "                                    \"comment\": comment,",
                "120": "                                    \"comment_time\": str(comment_time),",
                "121": "                                    \"last_code_change_time\": str(last_modified[str(line[\"line\"])])",
                "122": "                                })",
                "123": "    return analysis_results",
                "124": "",
                "125": "def is_code_in_block(block):",
                "126": "    for line in list(block.keys()):",
                "127": "        if block[line][\"comment_index\"] == -1:",
                "128": "            return True ",
                "129": "",
                "130": "def clean(data):",
                "131": "    clean_data = []",
                "132": "    for i in range(len(data)):",
                "133": "        item = {",
                "134": "            \"file\": data[i][\"file\"],",
                "135": "            \"line\": data[i][\"line\"],",
                "136": "            \"content\": data[i][\"content\"], # Cheeky inline comment",
                "137": "            \"comment\": data[i][\"comment\"], # Cheeky 2 inline comment",
                "138": "            \"comment_time\": data[i][\"comment_time\"],",
                "139": "            \"last_code_change_time\": data[i][\"last_code_change_time\"]",
                "140": "        }",
                "141": "        if len(data) > i + 1 and not is_equal(data[i], data[i+1]):",
                "142": "            clean_data.append(item)",
                "143": "    return clean_data",
                "144": "",
                "145": "def is_equal(d1,d2):",
                "146": "    d1_k = list(d1.keys())",
                "147": "    d2_k = list(d2.keys())",
                "148": "    for i in d1_k:",
                "149": "        if d1[i] != d2[i]:",
                "150": "            return False",
                "151": "    return True",
                "152": "",
                "153": "def average_comment_update_time(data):",
                "154": "    datetime_pairs = []",
                "155": "    for file in data:",
                "156": "        start = datetime.fromisoformat(file[\"last_code_change_time\"])",
                "157": "        end = datetime.fromisoformat(file[\"comment_time\"])",
                "158": "        datetime_pairs.append((start, end))",
                "159": "    durations = [end - start for start, end in datetime_pairs]",
                "160": "    total_duration = sum(durations, timedelta(0))",
                "161": "    average_duration = total_duration / len(durations)",
                "162": "    return average_duration",
                "163": "",
                "164": "def classify_comments(data):",
                "165": "    for comment in data:",
                "166": "        line = comment[\"content\"]",
                "167": "        comment_type = \"\"",
                "168": "        # Tokenize the input code",
                "169": "        tokens = tokenize.generate_tokens(StringIO(line).readline)",
                "170": "        prev_token = None",
                "171": "        for token in tokens:",
                "172": "            token_type, token_string, start, end, line = token",
                "173": "            if token_type == tokenize.COMMENT:",
                "174": "                comment_text = token_string.lstrip(\"#\").strip()",
                "175": "                # Check if inline",
                "176": "                if prev_token and prev_token.type != tokenize.NL:",
                "177": "                    comment_type = \"inline\"",
                "178": "                # Check for commented-out code (basic heuristic: looks like valid Python code)",
                "179": "                elif is_potential_code(comment_text):",
                "180": "                    comment_type = \"commented out\"",
                "181": "                # Check for block comments (multi-line consecutive)",
                "182": "                elif comment_text and comment_text[0].isalpha():",
                "183": "                    comment_type = \"block\"",
                "184": "                else:",
                "185": "                    comment_type = \"normal annotation\"",
                "186": "            elif token_type == tokenize.STRING:",
                "187": "                # Check for docstring: string token at module, function, or class start",
                "188": "                if prev_token and prev_token.type in {tokenize.DEDENT, tokenize.INDENT}:",
                "189": "                    comment_type = \"documentation\"",
                "190": "            prev_token = token",
                "191": "        comment[\"comment_type\"] = comment_type",
                "192": "    return data",
                "193": "",
                "194": "def is_potential_code(text):",
                "195": "    try:",
                "196": "        compile(text, \"<string>\", \"exec\")",
                "197": "        return True",
                "198": "    except SyntaxError:",
                "199": "        return False"
            },
            "comments": [
                {
                    "line": 6,
                    "comment": "\"\"\" some test docstring \"\"\"",
                    "char_position_in_line": 4,
                    "type": [
                        "docstring"
                    ]
                },
                {
                    "line": 22,
                    "comment": "# Ensure the gaps of deleted elements are artificially filled by increasing the shift",
                    "char_position_in_line": 12,
                    "type": [
                        "normal"
                    ]
                },
                {
                    "line": 87,
                    "comment": "# Store last modified timestamps for each line",
                    "char_position_in_line": 8,
                    "type": [
                        "normal"
                    ]
                },
                {
                    "line": 90,
                    "comment": "# print(\"Starting to analyse commit: \", commit[\"commit\"])",
                    "char_position_in_line": 12,
                    "type": [
                        "commented-out"
                    ]
                },
                {
                    "line": 92,
                    "comment": "# Track modified lines",
                    "char_position_in_line": 12,
                    "type": [
                        "normal"
                    ]
                },
                {
                    "line": 95,
                    "comment": "# Compare with comments",
                    "char_position_in_line": 12,
                    "type": [
                        "normal"
                    ]
                },
                {
                    "line": 99,
                    "comment": "# TODO investigate why line is null",
                    "char_position_in_line": 16,
                    "type": [
                        "normal"
                    ]
                },
                {
                    "line": 102,
                    "comment": "# Where there are comment changes and no source code changes in block",
                    "char_position_in_line": 24,
                    "type": [
                        "normal"
                    ]
                },
                {
                    "line": 103,
                    "comment": "# and block[line[\"line\"]][\"comment_index\"] != -1:",
                    "char_position_in_line": 56,
                    "type": [
                        "normal"
                    ]
                },
                {
                    "line": 136,
                    "comment": "# Cheeky inline comment",
                    "char_position_in_line": 43,
                    "type": [
                        "block"
                    ]
                },
                {
                    "line": 137,
                    "comment": "# Cheeky 2 inline comment",
                    "char_position_in_line": 43,
                    "type": [
                        "block"
                    ]
                },
                {
                    "line": 168,
                    "comment": "# Tokenize the input code",
                    "char_position_in_line": 8,
                    "type": [
                        "normal"
                    ]
                },
                {
                    "line": 175,
                    "comment": "# Check if inline",
                    "char_position_in_line": 16,
                    "type": [
                        "normal"
                    ]
                },
                {
                    "line": 178,
                    "comment": "# Check for commented-out code (basic heuristic: looks like valid Python code)",
                    "char_position_in_line": 16,
                    "type": [
                        "normal"
                    ]
                },
                {
                    "line": 181,
                    "comment": "# Check for block comments (multi-line consecutive)",
                    "char_position_in_line": 16,
                    "type": [
                        "normal"
                    ]
                },
                {
                    "line": 187,
                    "comment": "# Check for docstring: string token at module, function, or class start",
                    "char_position_in_line": 16,
                    "type": [
                        "normal"
                    ]
                }
            ]
        },
        {
            "commit": "76c0e457bfe78effef6b104d334eed6a7fa3e4e4",
            "timestamp": "2024-12-04T23:01:11+01:00",
            "author": "alexander.schranner",
            "diff": {
                "added": {
                    "8": "        for commit in commits: #inline-test-comment",
                    "108": "                                        type = item[\"type\"]",
                    "121": "                                    \"type\": type,",
                    "140": "            \"type\": data[i][\"type\"],",
                    "158": "    if data == None: return",
                    "168": "def classify_comments(lines):",
                    "169": "        line_types = []",
                    "170": "        # Check for commented-out code (basic heuristic: looks like valid Python code)",
                    "171": "        if is_potential_code(lines.lstrip(\"#\").strip()) and is_potential_code(lines.lstrip(\"\\\"\\\"\\\"\").strip()):",
                    "172": "            line_types.append(\"commented-out\")",
                    "173": "        # Check for block comments (multi-line consecutive)",
                    "174": "        if lines.find(\"\\n\") != -1:",
                    "175": "            line_types.append(\"block\")",
                    "176": "        # Check if text has docstring format with \"\"\"\" somewhere",
                    "177": "        if lines.find(\"\\\"\\\"\\\"\") != -1:",
                    "178": "            line_types.append(\"docstring\")",
                    "179": "        if len(line_types) == 0:",
                    "180": "            line_types.append(\"normal\")",
                    "181": "        return line_types",
                    "188": "        return False",
                    "189": "",
                    "190": "def classify_content(data):",
                    "191": "    for item in data:",
                    "192": "        if \"normal\" in item[\"type\"]:",
                    "193": "            line = item[\"content\"]",
                    "194": "            if bool(line.split(\"#\")[0].strip()) and is_potential_code(line.split(\"#\")[0]):",
                    "195": "                item[\"type\"].append(\"inline\")",
                    "196": "    return data"
                },
                "deleted": {
                    "8": "        for commit in commits:",
                    "164": "def classify_comments(data):",
                    "165": "    for comment in data:",
                    "166": "        line = comment[\"content\"]",
                    "167": "        comment_type = \"\"",
                    "168": "        # Tokenize the input code",
                    "169": "        tokens = tokenize.generate_tokens(StringIO(line).readline)",
                    "170": "        prev_token = None",
                    "171": "        for token in tokens:",
                    "172": "            token_type, token_string, start, end, line = token",
                    "173": "            if token_type == tokenize.COMMENT:",
                    "174": "                comment_text = token_string.lstrip(\"#\").strip()",
                    "175": "                # Check if inline",
                    "176": "                if prev_token and prev_token.type != tokenize.NL:",
                    "177": "                    comment_type = \"inline\"",
                    "178": "                # Check for commented-out code (basic heuristic: looks like valid Python code)",
                    "179": "                elif is_potential_code(comment_text):",
                    "180": "                    comment_type = \"commented out\"",
                    "181": "                # Check for block comments (multi-line consecutive)",
                    "182": "                elif comment_text and comment_text[0].isalpha():",
                    "183": "                    comment_type = \"block\"",
                    "184": "                else:",
                    "185": "                    comment_type = \"normal annotation\"",
                    "186": "            elif token_type == tokenize.STRING:",
                    "187": "                # Check for docstring: string token at module, function, or class start",
                    "188": "                if prev_token and prev_token.type in {tokenize.DEDENT, tokenize.INDENT}:",
                    "189": "                    comment_type = \"documentation\"",
                    "190": "            prev_token = token",
                    "191": "        comment[\"comment_type\"] = comment_type",
                    "192": "    return data",
                    "199": "        return False"
                }
            },
            "source_code": {
                "1": "from datetime import datetime, timedelta",
                "2": "import tokenize",
                "3": "from io import StringIO",
                "4": "",
                "5": "def analyse_diff_comments(data):",
                "6": "    \"\"\" some test docstring \"\"\"",
                "7": "    for file, commits in data.items():",
                "8": "        for commit in commits: #inline-test-comment",
                "9": "            no_change_comments = []",
                "10": "            for i in range(len(commit[\"comments\"])):",
                "11": "                if commit[\"comments\"][i][\"line\"] in list(commit[\"diff\"][\"added\"].keys()):",
                "12": "                    commit[\"comments\"][i][\"edit\"] = \"added\"",
                "13": "                continue",
                "14": "                if commit[\"comments\"][i][\"line\"] in list(commit[\"diff\"][\"deleted\"].keys()):",
                "15": "                    if \"edit\" in list(commit[\"comments\"][i].keys()):",
                "16": "                        commit[\"comments\"][i][\"edit\"] = \"modified\"",
                "17": "                        continue",
                "18": "                    else:",
                "19": "                        commit[\"comments\"][i][\"edit\"] = \"deleted\"",
                "20": "                        continue",
                "21": "                no_change_comments.append(i)",
                "22": "            # Ensure the gaps of deleted elements are artificially filled by increasing the shift",
                "23": "            shift = 0",
                "24": "            for j in no_change_comments:",
                "25": "                del commit[\"comments\"][j-shift]",
                "26": "                shift += 1",
                "27": "",
                "28": "def check_inline_comments(data):",
                "29": "    return",
                "30": "",
                "31": "def blockify_comments2(data):",
                "32": "    for file, commits in data.items():",
                "33": "        for commit in commits:",
                "34": "            block_diff = []",
                "35": "            for block in commit[\"diff\"][\"block_diff\"]:",
                "36": "                block_dict = {}",
                "37": "                for line in block:",
                "38": "                    for item in commit[\"comments\"]:",
                "39": "                        if int(line) == item[\"line\"]:",
                "40": "                            comment_index = item[\"char_position_in_line\"]",
                "41": "                            break",
                "42": "                        else:",
                "43": "                            comment_index = -1",
                "44": "                    line_info = {",
                "45": "                        \"content\": commit[\"diff\"][\"added\"][str(line)],",
                "46": "                        \"comment_index\": comment_index",
                "47": "                    }",
                "48": "                    block_dict[line] = line_info",
                "49": "                block_diff.append(block_dict)",
                "50": "            commit[\"diff\"][\"block_diff\"] = block_diff",
                "51": "",
                "52": "def blockify_comments(data):",
                "53": "    for file, commits in data.items():",
                "54": "        for commit in commits:",
                "55": "            blocks = []",
                "56": "            current_block = []",
                "57": "            for line in list(commit[\"diff\"][\"added\"].keys()):",
                "58": "                if int(line) in get_comment_lines(commit[\"comments\"]):",
                "59": "                    if current_block and current_block[-1] not in get_comment_lines(commit[\"comments\"]):",
                "60": "                        blocks.append(current_block)",
                "61": "                        current_block = []",
                "62": "                    current_block.append(int(line))",
                "63": "                else:",
                "64": "                    if current_block and int(line) != current_block[-1] + 1:",
                "65": "                        blocks.append(current_block)",
                "66": "                        current_block = []",
                "67": "                    current_block.append(int(line))",
                "68": "            if current_block:",
                "69": "                blocks.append(current_block)",
                "70": "            commit[\"diff\"][\"block_diff\"] = blocks",
                "71": "",
                "72": "def get_comment_lines(comments):",
                "73": "    comment_lines = []",
                "74": "    for comment in comments:",
                "75": "        comment_lines.append(comment[\"line\"])",
                "76": "    return comment_lines",
                "77": "",
                "78": "def get_diff_lines(diff):",
                "79": "    diff_lines = []",
                "80": "    for line in diff:",
                "81": "        diff_lines.append(line[0])",
                "82": "    return diff_lines",
                "83": "",
                "84": "def extract_later_modified_comments(data): ",
                "85": "    analysis_results = []",
                "86": "    for file, commits in data.items():",
                "87": "        # Store last modified timestamps for each line",
                "88": "        last_modified = {}",
                "89": "        for commit in commits:",
                "90": "            # print(\"Starting to analyse commit: \", commit[\"commit\"])",
                "91": "            commit_time = datetime.fromisoformat(commit[\"timestamp\"])",
                "92": "            # Track modified lines",
                "93": "            for line in list(commit[\"diff\"][\"added\"].keys()):",
                "94": "                last_modified[line] = commit_time",
                "95": "            # Compare with comments",
                "96": "            for line in commit[\"comments\"]:",
                "97": "                comment_time = datetime.fromisoformat(commit[\"timestamp\"])",
                "98": "                last_modified_lines = list(last_modified.keys())",
                "99": "                # TODO investigate why line is null",
                "100": "                if line != \"null\" and str(line[\"line\"]) in last_modified_lines:",
                "101": "                    for block in commit[\"diff\"][\"block_diff\"]:",
                "102": "                        # Where there are comment changes and no source code changes in block",
                "103": "                        if not is_code_in_block(block): # and block[line[\"line\"]][\"comment_index\"] != -1:",
                "104": "                            if comment_time > last_modified[str(line[\"line\"])]:",
                "105": "                                for item in commit[\"comments\"]:",
                "106": "                                    if line[\"line\"] == item[\"line\"]:",
                "107": "                                        comment = item[\"comment\"]",
                "108": "                                        type = item[\"type\"]",
                "109": "                                        break",
                "110": "                                for commit2 in commits:",
                "111": "                                    if datetime.fromisoformat(commit2[\"timestamp\"]) == comment_time:",
                "112": "                                        content = commit2[\"source_code\"][str(line[\"line\"])]",
                "113": "                                        break",
                "114": "                                    else:",
                "115": "                                        content = \"PROBLEM\"",
                "116": "                                analysis_results.append({",
                "117": "                                    \"file\": file,",
                "118": "                                    \"line\": line[\"line\"],",
                "119": "                                    \"content\": content,",
                "120": "                                    \"comment\": comment,",
                "121": "                                    \"type\": type,",
                "122": "                                    \"comment_time\": str(comment_time),",
                "123": "                                    \"last_code_change_time\": str(last_modified[str(line[\"line\"])])",
                "124": "                                })",
                "125": "    return analysis_results",
                "126": "",
                "127": "def is_code_in_block(block):",
                "128": "    for line in list(block.keys()):",
                "129": "        if block[line][\"comment_index\"] == -1:",
                "130": "            return True ",
                "131": "",
                "132": "def clean(data):",
                "133": "    clean_data = []",
                "134": "    for i in range(len(data)):",
                "135": "        item = {",
                "136": "            \"file\": data[i][\"file\"],",
                "137": "            \"line\": data[i][\"line\"],",
                "138": "            \"content\": data[i][\"content\"], # Cheeky inline comment",
                "139": "            \"comment\": data[i][\"comment\"], # Cheeky 2 inline comment",
                "140": "            \"type\": data[i][\"type\"],",
                "141": "            \"comment_time\": data[i][\"comment_time\"],",
                "142": "            \"last_code_change_time\": data[i][\"last_code_change_time\"]",
                "143": "        }",
                "144": "        if len(data) > i + 1 and not is_equal(data[i], data[i+1]):",
                "145": "            clean_data.append(item)",
                "146": "    return clean_data",
                "147": "",
                "148": "def is_equal(d1,d2):",
                "149": "    d1_k = list(d1.keys())",
                "150": "    d2_k = list(d2.keys())",
                "151": "    for i in d1_k:",
                "152": "        if d1[i] != d2[i]:",
                "153": "            return False",
                "154": "    return True",
                "155": "",
                "156": "def average_comment_update_time(data):",
                "157": "    datetime_pairs = []",
                "158": "    if data == None: return",
                "159": "    for file in data:",
                "160": "        start = datetime.fromisoformat(file[\"last_code_change_time\"])",
                "161": "        end = datetime.fromisoformat(file[\"comment_time\"])",
                "162": "        datetime_pairs.append((start, end))",
                "163": "    durations = [end - start for start, end in datetime_pairs]",
                "164": "    total_duration = sum(durations, timedelta(0))",
                "165": "    average_duration = total_duration / len(durations)",
                "166": "    return average_duration",
                "167": "",
                "168": "def classify_comments(lines):",
                "169": "        line_types = []",
                "170": "        # Check for commented-out code (basic heuristic: looks like valid Python code)",
                "171": "        if is_potential_code(lines.lstrip(\"#\").strip()) and is_potential_code(lines.lstrip(\"\\\"\\\"\\\"\").strip()):",
                "172": "            line_types.append(\"commented-out\")",
                "173": "        # Check for block comments (multi-line consecutive)",
                "174": "        if lines.find(\"\\n\") != -1:",
                "175": "            line_types.append(\"block\")",
                "176": "        # Check if text has docstring format with \"\"\"\" somewhere",
                "177": "        if lines.find(\"\\\"\\\"\\\"\") != -1:",
                "178": "            line_types.append(\"docstring\")",
                "179": "        if len(line_types) == 0:",
                "180": "            line_types.append(\"normal\")",
                "181": "        return line_types",
                "182": "",
                "183": "def is_potential_code(text):",
                "184": "    try:",
                "185": "        compile(text, \"<string>\", \"exec\")",
                "186": "        return True",
                "187": "    except SyntaxError:",
                "188": "        return False",
                "189": "",
                "190": "def classify_content(data):",
                "191": "    for item in data:",
                "192": "        if \"normal\" in item[\"type\"]:",
                "193": "            line = item[\"content\"]",
                "194": "            if bool(line.split(\"#\")[0].strip()) and is_potential_code(line.split(\"#\")[0]):",
                "195": "                item[\"type\"].append(\"inline\")",
                "196": "    return data"
            },
            "comments": [
                {
                    "line": 6,
                    "comment": "\"\"\" some test docstring \"\"\"",
                    "char_position_in_line": 4,
                    "type": [
                        "docstring"
                    ]
                },
                {
                    "line": 8,
                    "comment": "#inline-test-comment",
                    "char_position_in_line": 31,
                    "type": [
                        "commented-out"
                    ]
                },
                {
                    "line": 22,
                    "comment": "# Ensure the gaps of deleted elements are artificially filled by increasing the shift",
                    "char_position_in_line": 12,
                    "type": [
                        "normal"
                    ]
                },
                {
                    "line": 87,
                    "comment": "# Store last modified timestamps for each line",
                    "char_position_in_line": 8,
                    "type": [
                        "normal"
                    ]
                },
                {
                    "line": 90,
                    "comment": "# print(\"Starting to analyse commit: \", commit[\"commit\"])",
                    "char_position_in_line": 12,
                    "type": [
                        "commented-out"
                    ]
                },
                {
                    "line": 92,
                    "comment": "# Track modified lines",
                    "char_position_in_line": 12,
                    "type": [
                        "normal"
                    ]
                },
                {
                    "line": 95,
                    "comment": "# Compare with comments",
                    "char_position_in_line": 12,
                    "type": [
                        "normal"
                    ]
                },
                {
                    "line": 99,
                    "comment": "# TODO investigate why line is null",
                    "char_position_in_line": 16,
                    "type": [
                        "normal"
                    ]
                },
                {
                    "line": 102,
                    "comment": "# Where there are comment changes and no source code changes in block",
                    "char_position_in_line": 24,
                    "type": [
                        "normal"
                    ]
                },
                {
                    "line": 103,
                    "comment": "# and block[line[\"line\"]][\"comment_index\"] != -1:",
                    "char_position_in_line": 56,
                    "type": [
                        "normal"
                    ]
                },
                {
                    "line": 138,
                    "comment": "# Cheeky inline comment",
                    "char_position_in_line": 43,
                    "type": [
                        "block"
                    ]
                },
                {
                    "line": 139,
                    "comment": "# Cheeky 2 inline comment",
                    "char_position_in_line": 43,
                    "type": [
                        "block"
                    ]
                },
                {
                    "line": 170,
                    "comment": "# Check for commented-out code (basic heuristic: looks like valid Python code)",
                    "char_position_in_line": 8,
                    "type": [
                        "normal"
                    ]
                },
                {
                    "line": 173,
                    "comment": "# Check for block comments (multi-line consecutive)",
                    "char_position_in_line": 8,
                    "type": [
                        "normal"
                    ]
                },
                {
                    "line": 176,
                    "comment": "# Check if text has docstring format with \"\"\"\" somewhere",
                    "char_position_in_line": 8,
                    "type": [
                        "docstring"
                    ]
                }
            ]
        }
    ],
    "build/xes_conversion.py": [
        {
            "commit": "ca64e3c56b1e147cafb1af0d143d4f60f04cfbe2",
            "timestamp": "2024-11-28T10:16:59+01:00",
            "author": "alexander.schranner",
            "diff": {
                "added": {},
                "deleted": {}
            },
            "source_code": {},
            "comments": []
        },
        {
            "commit": "39a855de8da8bde2a00bb031978e8a5bda2178ac",
            "timestamp": "2024-12-03T15:33:45+01:00",
            "author": "alexander.schranner",
            "diff": {
                "added": {
                    "1": "import json",
                    "2": "from xml.etree.ElementTree import Element, SubElement, tostring",
                    "3": "from xml.dom.minidom import parseString",
                    "4": "",
                    "5": "def convert_json_to_xes(json_data, output_file):",
                    "6": "    # Create the root XES element",
                    "7": "    root = Element('log')",
                    "8": "    root.set('xes.version', '1.0')",
                    "9": "    root.set('xes.features', 'nested-attributes')",
                    "10": "    root.set('xmlns', 'http://www.xes-standard.org/')",
                    "11": "",
                    "12": "    # Group by file (caseID)",
                    "13": "    grouped_data = {}",
                    "14": "    for entry in json_data:",
                    "15": "        case_id = entry['file']",
                    "16": "        if case_id not in grouped_data:",
                    "17": "            grouped_data[case_id] = []",
                    "18": "        grouped_data[case_id].append(entry)",
                    "19": "",
                    "20": "    # Create traces (cases)",
                    "21": "    for case_id, events in grouped_data.items():",
                    "22": "        trace = SubElement(root, 'trace')",
                    "23": "",
                    "24": "        # Add caseID as attribute",
                    "25": "        trace_string = SubElement(trace, 'string')",
                    "26": "        trace_string.set('key', 'concept:name')",
                    "27": "        trace_string.set('value', case_id)",
                    "28": "",
                    "29": "        # Add events to the trace",
                    "30": "        for event in events:",
                    "31": "            event_element = SubElement(trace, 'event')",
                    "32": "",
                    "33": "            # Add attributes for the event",
                    "34": "            for key, value in event.items():",
                    "35": "                if key == 'file':  # Skip the file as it's used as caseID",
                    "36": "                    continue",
                    "37": "                attr_type = 'string'",
                    "38": "                if 'time' in key:",
                    "39": "                    attr_type = 'date'  # Use date type for time fields",
                    "40": "                event_attr = SubElement(event_element, attr_type)",
                    "41": "                event_attr.set('key', key)",
                    "42": "                event_attr.set('value', str(value))",
                    "43": "",
                    "44": "    # Save the output",
                    "45": "    xml_str = parseString(tostring(root)).toprettyxml(indent=\"  \")",
                    "46": "    with open(output_file, 'w', encoding='utf-8') as f:",
                    "47": "        f.write(xml_str)"
                },
                "deleted": {}
            },
            "source_code": {
                "1": "import json",
                "2": "from xml.etree.ElementTree import Element, SubElement, tostring",
                "3": "from xml.dom.minidom import parseString",
                "4": "",
                "5": "def convert_json_to_xes(json_data, output_file):",
                "6": "    # Create the root XES element",
                "7": "    root = Element('log')",
                "8": "    root.set('xes.version', '1.0')",
                "9": "    root.set('xes.features', 'nested-attributes')",
                "10": "    root.set('xmlns', 'http://www.xes-standard.org/')",
                "11": "",
                "12": "    # Group by file (caseID)",
                "13": "    grouped_data = {}",
                "14": "    for entry in json_data:",
                "15": "        case_id = entry['file']",
                "16": "        if case_id not in grouped_data:",
                "17": "            grouped_data[case_id] = []",
                "18": "        grouped_data[case_id].append(entry)",
                "19": "",
                "20": "    # Create traces (cases)",
                "21": "    for case_id, events in grouped_data.items():",
                "22": "        trace = SubElement(root, 'trace')",
                "23": "        ",
                "24": "        # Add caseID as attribute",
                "25": "        trace_string = SubElement(trace, 'string')",
                "26": "        trace_string.set('key', 'concept:name')",
                "27": "        trace_string.set('value', case_id)",
                "28": "",
                "29": "        # Add events to the trace",
                "30": "        for event in events:",
                "31": "            event_element = SubElement(trace, 'event')",
                "32": "            ",
                "33": "            # Add attributes for the event",
                "34": "            for key, value in event.items():",
                "35": "                if key == 'file':  # Skip the file as it's used as caseID",
                "36": "                    continue",
                "37": "                attr_type = 'string'",
                "38": "                if 'time' in key:",
                "39": "                    attr_type = 'date'  # Use date type for time fields",
                "40": "                event_attr = SubElement(event_element, attr_type)",
                "41": "                event_attr.set('key', key)",
                "42": "                event_attr.set('value', str(value))",
                "43": "",
                "44": "    # Save the output",
                "45": "    xml_str = parseString(tostring(root)).toprettyxml(indent=\"  \")",
                "46": "    with open(output_file, 'w', encoding='utf-8') as f:",
                "47": "        f.write(xml_str)",
                "48": ""
            },
            "comments": [
                {
                    "line": 6,
                    "comment": "# Create the root XES element",
                    "char_position_in_line": 4,
                    "type": [
                        "normal"
                    ]
                },
                {
                    "line": 12,
                    "comment": "# Group by file (caseID)",
                    "char_position_in_line": 4,
                    "type": [
                        "normal"
                    ]
                },
                {
                    "line": 20,
                    "comment": "# Create traces (cases)",
                    "char_position_in_line": 4,
                    "type": [
                        "normal"
                    ]
                },
                {
                    "line": 24,
                    "comment": "# Add caseID as attribute",
                    "char_position_in_line": 8,
                    "type": [
                        "normal"
                    ]
                },
                {
                    "line": 29,
                    "comment": "# Add events to the trace",
                    "char_position_in_line": 8,
                    "type": [
                        "normal"
                    ]
                },
                {
                    "line": 33,
                    "comment": "# Add attributes for the event",
                    "char_position_in_line": 12,
                    "type": [
                        "normal"
                    ]
                },
                {
                    "line": 35,
                    "comment": "# Skip the file as it's used as caseID",
                    "char_position_in_line": 35,
                    "type": [
                        "normal"
                    ]
                },
                {
                    "line": 39,
                    "comment": "# Use date type for time fields",
                    "char_position_in_line": 40,
                    "type": [
                        "normal"
                    ]
                },
                {
                    "line": 44,
                    "comment": "# Save the output",
                    "char_position_in_line": 4,
                    "type": [
                        "normal"
                    ]
                }
            ]
        }
    ],
    "test.py": [
        {
            "commit": "202468fb39d473251ab81eb3037227cf7af47344",
            "timestamp": "2024-12-04T00:35:22+01:00",
            "author": "alexander.schranner",
            "diff": {
                "added": {
                    "1": "from pydriller import Repository",
                    "2": "import json",
                    "3": "from build.utils import save_to_json, list_to_dict",
                    "4": "from datetime import datetime",
                    "5": "",
                    "6": "def get_source_code_from_tag(repo_path, tag_name, dt1, dt2):",
                    "7": "    \"\"\"",
                    "8": "    Extracts the source code of a specific commit tagged in the repository.",
                    "9": "",
                    "10": "    :param repo_path: Path to the local Git repository.",
                    "11": "    :param tag_name: The name of the tag to fetch.",
                    "12": "    :return: A dictionary containing file paths and their contents at the given commit.",
                    "13": "    \"\"\"",
                    "14": "    source_code = []",
                    "15": "",
                    "16": "    for commit in Repository(repo_path, since=dt1, to=dt2).traverse_commits():",
                    "17": "        print(f\"Processing commit: {commit.hash} tagged as {tag_name}\")",
                    "18": "        for modified_file in commit.modified_files:",
                    "19": "            # Save the file path and its source code",
                    "20": "            if modified_file.source_code:",
                    "21": "                if modified_file.filename.find(\".py\") != -1 and modified_file.filename.find(\".pyc\") == -1:",
                    "22": "                    comit = {",
                    "23": "                        commit.hash + \"---\" + modified_file.filename: list_to_dict(modified_file.source_code.split(\"\\n\"))",
                    "24": "                    }",
                    "25": "                    source_code.append(comit)",
                    "26": "",
                    "27": "    return source_code",
                    "28": "",
                    "29": "# Example usage",
                    "30": "repo_path = \"https://github.com/AlexS-1/Bachelor-Code\"",
                    "31": "tag_name = \"a1ad5c2cb35d621f2b187166af65a2b2ee3ea45e\"",
                    "32": "dt1 = datetime(2024,11,21)",
                    "33": "dt2 = datetime(2024,11,22)",
                    "34": "source_code = get_source_code_from_tag(repo_path, tag_name, dt1, dt2)",
                    "35": "save_to_json(source_code, \"Tests/exports.json\")"
                },
                "deleted": {}
            },
            "source_code": {
                "1": "from pydriller import Repository",
                "2": "import json",
                "3": "from build.utils import save_to_json, list_to_dict",
                "4": "from datetime import datetime",
                "5": "",
                "6": "def get_source_code_from_tag(repo_path, tag_name, dt1, dt2):",
                "7": "    \"\"\"",
                "8": "    Extracts the source code of a specific commit tagged in the repository.",
                "9": "",
                "10": "    :param repo_path: Path to the local Git repository.",
                "11": "    :param tag_name: The name of the tag to fetch.",
                "12": "    :return: A dictionary containing file paths and their contents at the given commit.",
                "13": "    \"\"\"",
                "14": "    source_code = []",
                "15": "",
                "16": "    for commit in Repository(repo_path, since=dt1, to=dt2).traverse_commits():",
                "17": "        print(f\"Processing commit: {commit.hash} tagged as {tag_name}\")",
                "18": "        for modified_file in commit.modified_files:",
                "19": "            # Save the file path and its source code",
                "20": "            if modified_file.source_code:",
                "21": "                if modified_file.filename.find(\".py\") != -1 and modified_file.filename.find(\".pyc\") == -1:",
                "22": "                    comit = {",
                "23": "                        commit.hash + \"---\" + modified_file.filename: list_to_dict(modified_file.source_code.split(\"\\n\"))",
                "24": "                    }",
                "25": "                    source_code.append(comit)",
                "26": "",
                "27": "    return source_code",
                "28": "",
                "29": "# Example usage",
                "30": "repo_path = \"https://github.com/AlexS-1/Bachelor-Code\"",
                "31": "tag_name = \"a1ad5c2cb35d621f2b187166af65a2b2ee3ea45e\"",
                "32": "dt1 = datetime(2024,11,21)",
                "33": "dt2 = datetime(2024,11,22)",
                "34": "source_code = get_source_code_from_tag(repo_path, tag_name, dt1, dt2)",
                "35": "save_to_json(source_code, \"Tests/exports.json\")",
                "36": ""
            },
            "comments": [
                {
                    "line": 7,
                    "comment": "\"\"\"",
                    "char_position_in_line": 4,
                    "type": [
                        "block",
                        "docstring"
                    ]
                },
                {
                    "line": 8,
                    "comment": "    Extracts the source code of a specific commit tagged in the repository.",
                    "char_position_in_line": 4,
                    "type": [
                        "block",
                        "docstring"
                    ]
                },
                {
                    "line": 9,
                    "comment": "",
                    "char_position_in_line": 4,
                    "type": [
                        "block",
                        "docstring"
                    ]
                },
                {
                    "line": 10,
                    "comment": "    :param repo_path: Path to the local Git repository.",
                    "char_position_in_line": 4,
                    "type": [
                        "block",
                        "docstring"
                    ]
                },
                {
                    "line": 11,
                    "comment": "    :param tag_name: The name of the tag to fetch.",
                    "char_position_in_line": 4,
                    "type": [
                        "block",
                        "docstring"
                    ]
                },
                {
                    "line": 12,
                    "comment": "    :return: A dictionary containing file paths and their contents at the given commit.",
                    "char_position_in_line": 4,
                    "type": [
                        "block",
                        "docstring"
                    ]
                },
                {
                    "line": 13,
                    "comment": "    \"\"\"",
                    "char_position_in_line": 4,
                    "type": [
                        "block",
                        "docstring"
                    ]
                },
                {
                    "line": 19,
                    "comment": "# Save the file path and its source code",
                    "char_position_in_line": 12,
                    "type": [
                        "normal"
                    ]
                },
                {
                    "line": 29,
                    "comment": "# Example usage",
                    "char_position_in_line": 0,
                    "type": [
                        "normal"
                    ]
                }
            ]
        },
        {
            "commit": "c92ae279606c2cdf0e62136cd793b188fda08371",
            "timestamp": "2024-12-04T14:36:49+01:00",
            "author": "alexander.schranner",
            "diff": {
                "added": {
                    "3": "from build.pydriller import get_commits_data",
                    "4": "from build.comment_lister import run_comment_lister, filter_comments_by_time",
                    "5": "from build.utils import save_to_json",
                    "6": "from build.analysis import analyse_diff_comments, blockify_comments, blockify_comments2, extract_later_modified_comments, clean, average_comment_update_time, classify_comments",
                    "7": "from build.xes_conversion import convert_json_to_xes",
                    "8": "from datetime import datetime, timezone",
                    "9": "import os",
                    "10": "import subprocess",
                    "11": "import shutil",
                    "15": "    DOCSTRIING: Extracts the source code of a specific commit tagged in the repository.",
                    "20": "    END DOCSTRING",
                    "27": "            # NORMAL Save the file path and its source code",
                    "29": "                # BLOCK: Multiple lines",
                    "30": "                # of comment",
                    "33": "                        commit.hash + \"---\" + modified_file.filename: list_to_dict(modified_file.source_code.split(\"\\n\")) #INLINE: identify each block of data",
                    "36": "                    # print(commit) #COMMENTED-OUT",
                    "41": "repo_url = \"https://github.com/AlexS-1/Bachelor-Code\"",
                    "43": "start_time = datetime(2024,12,3)",
                    "44": "end_time = datetime(2024,12,4)",
                    "45": "repo_name = os.path.basename(repo_url).replace(\".git\", \"\")",
                    "46": "temp_dir = \"/Users/as/Library/Mobile Documents/com~apple~CloudDocs/Dokumente/Studium/Bachelor-Thesis/tmp\"",
                    "47": "clone_path = os.path.join(temp_dir, repo_name)",
                    "48": "",
                    "49": "subprocess.run(['git', 'clone', repo_url, clone_path], check=True)",
                    "50": "",
                    "51": "# # Paths",
                    "52": "repo_path = clone_path",
                    "53": "jar_path = \"/Users/as/Library/Mobile Documents/com~apple~CloudDocs/Dokumente/Studium/Bachelor-Thesis/CommentLister/target/CommentLister.jar\"",
                    "54": "file_types = [\".c\", \".c\", \".cc\", \".cp\", \".cpp\", \".cx\", \".cxx\", \".c+\", \".c++\", \".h\", \".hh\", \".hxx\", \".h+\", \".h++\", \".hp\", \".hpp\", \".java\", \".js\", \".cs\", \".py\", \".php\", \".rb\"]",
                    "55": "",
                    "56": "commits_data = get_commits_data(repo_path, start_time, end_time, file_types)",
                    "57": "save_to_json(commits_data, \"Toy-Example/commits_data.json\")",
                    "58": "with open (\"Toy-Example/commits_data.json\", \"r\") as json_file:",
                    "59": "        commits_data = json.load(json_file)",
                    "60": "",
                    "61": "for file, commits in commits_data.items():",
                    "62": "    for commit in commits:",
                    "63": "        tag = \"-target=\" + commit[\"commit\"]",
                    "64": "        output = run_comment_lister(repo_path, jar_path, tag)",
                    "65": "        # Parse output as JSON",
                    "66": "        try:",
                    "67": "            comment_data = json.loads(output)",
                    "68": "        except json.JSONDecodeError as e:",
                    "69": "            print(f\"Failed to parse CommentLister output: {e}\")",
                    "70": "            break",
                    "71": "        # Filter comments by time",
                    "72": "        commit_hash, filtered_comments = filter_comments_by_time(comment_data, start_time, end_time)",
                    "73": "        if commit[\"commit\"] == commit_hash and file in filtered_comments.keys():",
                    "74": "            commit[\"comments\"] = filtered_comments[file]",
                    "75": "        else:",
                    "76": "            print(\"mismatch in commit and comment data or no comments in this commit for investigatet file\")",
                    "77": "            print(\"file could have been deleted\")",
                    "78": "            commit[\"comments\"] = {}",
                    "79": "# Save filtered comments on your system",
                    "80": "save_to_json(commits_data, \"Toy-Example/filtered_commits_data.json\")",
                    "81": "shutil.rmtree(clone_path)",
                    "82": "with open(\"Data/filtered_commits_data.json\", \"r\") as json_file:",
                    "83": "    data = json.load(json_file)",
                    "84": "# analyse_diff_comments(data)",
                    "85": "blockify_comments(data)",
                    "86": "save_to_json(data, \"Toy-Example/blockified_comments_data.json\")",
                    "87": "with open(\"Toy-Example/blockified_comments_data.json\", \"r\") as json_file:",
                    "88": "    data = json.load(json_file)",
                    "89": "blockify_comments2(data)",
                    "90": "save_to_json(data, \"Toy-Example/blockified_comments2_data.json\")",
                    "91": "with open(\"Toy-Example/blockified_comments2_data.json\", \"r\") as json_file:",
                    "92": "    data = json.load(json_file)",
                    "93": "d = extract_later_modified_comments(data)",
                    "94": "save_to_json(d, \"Toy-Example/analysis_results.json\")",
                    "95": "with open(\"Toy-Example/analysis_results.json\", \"r\") as json_file:",
                    "96": "    data = json.load(json_file)",
                    "97": "d = clean(data)",
                    "98": "save_to_json(d, \"Toy-Example/clean_analysis_results.json\")",
                    "99": "with open(\"Toy-Example/clean_analysis_results.json\", \"r\") as json_file:",
                    "100": "    data = json.load(json_file)",
                    "101": "d = classify_comments(data)",
                    "102": "save_to_json(d, \"Toy-Example/clean_analysis_results2.json\")",
                    "103": "print(\"Average duration:\", average_comment_update_time(d))",
                    "104": "convert_json_to_xes(d, 'Toy-Example/output.xes')"
                },
                "deleted": {
                    "3": "from build.utils import save_to_json, list_to_dict",
                    "4": "from datetime import datetime",
                    "8": "    Extracts the source code of a specific commit tagged in the repository.",
                    "19": "            # Save the file path and its source code",
                    "23": "                        commit.hash + \"---\" + modified_file.filename: list_to_dict(modified_file.source_code.split(\"\\n\"))",
                    "30": "repo_path = \"https://github.com/AlexS-1/Bachelor-Code\"",
                    "32": "dt1 = datetime(2024,11,21)",
                    "33": "dt2 = datetime(2024,11,22)",
                    "34": "source_code = get_source_code_from_tag(repo_path, tag_name, dt1, dt2)",
                    "35": "save_to_json(source_code, \"Tests/exports.json\")"
                }
            },
            "source_code": {
                "1": "from pydriller import Repository",
                "2": "import json",
                "3": "from build.pydriller import get_commits_data",
                "4": "from build.comment_lister import run_comment_lister, filter_comments_by_time",
                "5": "from build.utils import save_to_json",
                "6": "from build.analysis import analyse_diff_comments, blockify_comments, blockify_comments2, extract_later_modified_comments, clean, average_comment_update_time, classify_comments",
                "7": "from build.xes_conversion import convert_json_to_xes",
                "8": "from datetime import datetime, timezone",
                "9": "import os",
                "10": "import subprocess",
                "11": "import shutil",
                "12": "",
                "13": "def get_source_code_from_tag(repo_path, tag_name, dt1, dt2):",
                "14": "    \"\"\"",
                "15": "    DOCSTRIING: Extracts the source code of a specific commit tagged in the repository.",
                "16": "",
                "17": "    :param repo_path: Path to the local Git repository.",
                "18": "    :param tag_name: The name of the tag to fetch.",
                "19": "    :return: A dictionary containing file paths and their contents at the given commit.",
                "20": "    END DOCSTRING",
                "21": "    \"\"\"",
                "22": "    source_code = []",
                "23": "",
                "24": "    for commit in Repository(repo_path, since=dt1, to=dt2).traverse_commits():",
                "25": "        print(f\"Processing commit: {commit.hash} tagged as {tag_name}\")",
                "26": "        for modified_file in commit.modified_files:",
                "27": "            # NORMAL Save the file path and its source code",
                "28": "            if modified_file.source_code:",
                "29": "                # BLOCK: Multiple lines",
                "30": "                # of comment",
                "31": "                if modified_file.filename.find(\".py\") != -1 and modified_file.filename.find(\".pyc\") == -1:",
                "32": "                    comit = {",
                "33": "                        commit.hash + \"---\" + modified_file.filename: list_to_dict(modified_file.source_code.split(\"\\n\")) #INLINE: identify each block of data",
                "34": "                    }",
                "35": "                    source_code.append(comit)",
                "36": "                    # print(commit) #COMMENTED-OUT",
                "37": "",
                "38": "    return source_code",
                "39": "",
                "40": "# Example usage",
                "41": "repo_url = \"https://github.com/AlexS-1/Bachelor-Code\"",
                "42": "tag_name = \"a1ad5c2cb35d621f2b187166af65a2b2ee3ea45e\"",
                "43": "start_time = datetime(2024,12,3)",
                "44": "end_time = datetime(2024,12,4)",
                "45": "repo_name = os.path.basename(repo_url).replace(\".git\", \"\")",
                "46": "temp_dir = \"/Users/as/Library/Mobile Documents/com~apple~CloudDocs/Dokumente/Studium/Bachelor-Thesis/tmp\"",
                "47": "clone_path = os.path.join(temp_dir, repo_name)",
                "48": "",
                "49": "subprocess.run(['git', 'clone', repo_url, clone_path], check=True)",
                "50": "",
                "51": "# # Paths",
                "52": "repo_path = clone_path",
                "53": "jar_path = \"/Users/as/Library/Mobile Documents/com~apple~CloudDocs/Dokumente/Studium/Bachelor-Thesis/CommentLister/target/CommentLister.jar\"",
                "54": "file_types = [\".c\", \".c\", \".cc\", \".cp\", \".cpp\", \".cx\", \".cxx\", \".c+\", \".c++\", \".h\", \".hh\", \".hxx\", \".h+\", \".h++\", \".hp\", \".hpp\", \".java\", \".js\", \".cs\", \".py\", \".php\", \".rb\"]",
                "55": "",
                "56": "commits_data = get_commits_data(repo_path, start_time, end_time, file_types)",
                "57": "save_to_json(commits_data, \"Toy-Example/commits_data.json\")",
                "58": "with open (\"Toy-Example/commits_data.json\", \"r\") as json_file:",
                "59": "        commits_data = json.load(json_file)",
                "60": "",
                "61": "for file, commits in commits_data.items():",
                "62": "    for commit in commits:",
                "63": "        tag = \"-target=\" + commit[\"commit\"]",
                "64": "        output = run_comment_lister(repo_path, jar_path, tag)",
                "65": "        # Parse output as JSON",
                "66": "        try:",
                "67": "            comment_data = json.loads(output)",
                "68": "        except json.JSONDecodeError as e:",
                "69": "            print(f\"Failed to parse CommentLister output: {e}\")",
                "70": "            break",
                "71": "        # Filter comments by time",
                "72": "        commit_hash, filtered_comments = filter_comments_by_time(comment_data, start_time, end_time)",
                "73": "        if commit[\"commit\"] == commit_hash and file in filtered_comments.keys():",
                "74": "            commit[\"comments\"] = filtered_comments[file]",
                "75": "        else:",
                "76": "            print(\"mismatch in commit and comment data or no comments in this commit for investigatet file\")",
                "77": "            print(\"file could have been deleted\")",
                "78": "            commit[\"comments\"] = {}",
                "79": "# Save filtered comments on your system",
                "80": "save_to_json(commits_data, \"Toy-Example/filtered_commits_data.json\")",
                "81": "shutil.rmtree(clone_path)",
                "82": "with open(\"Data/filtered_commits_data.json\", \"r\") as json_file:",
                "83": "    data = json.load(json_file)",
                "84": "# analyse_diff_comments(data)",
                "85": "blockify_comments(data)",
                "86": "save_to_json(data, \"Toy-Example/blockified_comments_data.json\")",
                "87": "with open(\"Toy-Example/blockified_comments_data.json\", \"r\") as json_file:",
                "88": "    data = json.load(json_file)",
                "89": "blockify_comments2(data)",
                "90": "save_to_json(data, \"Toy-Example/blockified_comments2_data.json\")",
                "91": "with open(\"Toy-Example/blockified_comments2_data.json\", \"r\") as json_file:",
                "92": "    data = json.load(json_file)",
                "93": "d = extract_later_modified_comments(data)",
                "94": "save_to_json(d, \"Toy-Example/analysis_results.json\")",
                "95": "with open(\"Toy-Example/analysis_results.json\", \"r\") as json_file:",
                "96": "    data = json.load(json_file)",
                "97": "d = clean(data)",
                "98": "save_to_json(d, \"Toy-Example/clean_analysis_results.json\")",
                "99": "with open(\"Toy-Example/clean_analysis_results.json\", \"r\") as json_file:",
                "100": "    data = json.load(json_file)",
                "101": "d = classify_comments(data)",
                "102": "save_to_json(d, \"Toy-Example/clean_analysis_results2.json\")",
                "103": "print(\"Average duration:\", average_comment_update_time(d))",
                "104": "convert_json_to_xes(d, 'Toy-Example/output.xes')"
            },
            "comments": [
                {
                    "line": 14,
                    "comment": "\"\"\"",
                    "char_position_in_line": 4,
                    "type": [
                        "block",
                        "docstring"
                    ]
                },
                {
                    "line": 15,
                    "comment": "    DOCSTRIING: Extracts the source code of a specific commit tagged in the repository.",
                    "char_position_in_line": 4,
                    "type": [
                        "block",
                        "docstring"
                    ]
                },
                {
                    "line": 16,
                    "comment": "",
                    "char_position_in_line": 4,
                    "type": [
                        "block",
                        "docstring"
                    ]
                },
                {
                    "line": 17,
                    "comment": "    :param repo_path: Path to the local Git repository.",
                    "char_position_in_line": 4,
                    "type": [
                        "block",
                        "docstring"
                    ]
                },
                {
                    "line": 18,
                    "comment": "    :param tag_name: The name of the tag to fetch.",
                    "char_position_in_line": 4,
                    "type": [
                        "block",
                        "docstring"
                    ]
                },
                {
                    "line": 19,
                    "comment": "    :return: A dictionary containing file paths and their contents at the given commit.",
                    "char_position_in_line": 4,
                    "type": [
                        "block",
                        "docstring"
                    ]
                },
                {
                    "line": 20,
                    "comment": "    END DOCSTRING",
                    "char_position_in_line": 4,
                    "type": [
                        "block",
                        "docstring"
                    ]
                },
                {
                    "line": 21,
                    "comment": "    \"\"\"",
                    "char_position_in_line": 4,
                    "type": [
                        "block",
                        "docstring"
                    ]
                },
                {
                    "line": 27,
                    "comment": "# NORMAL Save the file path and its source code",
                    "char_position_in_line": 12,
                    "type": [
                        "normal"
                    ]
                },
                {
                    "line": 29,
                    "comment": "# BLOCK: Multiple lines",
                    "char_position_in_line": 16,
                    "type": [
                        "block"
                    ]
                },
                {
                    "line": 30,
                    "comment": "# of comment",
                    "char_position_in_line": 16,
                    "type": [
                        "block"
                    ]
                },
                {
                    "line": 33,
                    "comment": "#INLINE: identify each block of data",
                    "char_position_in_line": 122,
                    "type": [
                        "normal"
                    ]
                },
                {
                    "line": 36,
                    "comment": "# print(commit) #COMMENTED-OUT",
                    "char_position_in_line": 20,
                    "type": [
                        "commented-out"
                    ]
                },
                {
                    "line": 40,
                    "comment": "# Example usage",
                    "char_position_in_line": 0,
                    "type": [
                        "normal"
                    ]
                },
                {
                    "line": 51,
                    "comment": "# # Paths",
                    "char_position_in_line": 0,
                    "type": [
                        "commented-out"
                    ]
                },
                {
                    "line": 65,
                    "comment": "# Parse output as JSON",
                    "char_position_in_line": 8,
                    "type": [
                        "normal"
                    ]
                },
                {
                    "line": 71,
                    "comment": "# Filter comments by time",
                    "char_position_in_line": 8,
                    "type": [
                        "normal"
                    ]
                },
                {
                    "line": 79,
                    "comment": "# Save filtered comments on your system",
                    "char_position_in_line": 0,
                    "type": [
                        "normal"
                    ]
                },
                {
                    "line": 84,
                    "comment": "# analyse_diff_comments(data)",
                    "char_position_in_line": 0,
                    "type": [
                        "commented-out"
                    ]
                }
            ]
        },
        {
            "commit": "76c0e457bfe78effef6b104d334eed6a7fa3e4e4",
            "timestamp": "2024-12-04T23:01:11+01:00",
            "author": "alexander.schranner",
            "diff": {
                "added": {
                    "6": "from build.analysis import analyse_diff_comments, blockify_comments, blockify_comments2, extract_later_modified_comments, clean, average_comment_update_time, classify_comments, classify_content",
                    "37": "    return source_code",
                    "43": "end_time = datetime.today()",
                    "55": "commits_data = get_commits_data(repo_path, start_time, datetime.today(), file_types)",
                    "57": "with open (\"Toy-Example/commits_data.json\", \"r\") as json_file:",
                    "81": "with open(\"Toy-Example/filtered_commits_data.json\", \"r\") as json_file:",
                    "100": "d = classify_content(data)"
                },
                "deleted": {
                    "6": "from build.analysis import analyse_diff_comments, blockify_comments, blockify_comments2, extract_later_modified_comments, clean, average_comment_update_time, classify_comments",
                    "37": "",
                    "38": "    return source_code",
                    "44": "end_time = datetime(2024,12,4)",
                    "56": "commits_data = get_commits_data(repo_path, start_time, end_time, file_types)",
                    "58": "with open (\"Toy-Example/commits_data.json\", \"r\") as json_file:",
                    "82": "with open(\"Data/filtered_commits_data.json\", \"r\") as json_file:",
                    "101": "d = classify_comments(data)"
                }
            },
            "source_code": {
                "1": "from pydriller import Repository",
                "2": "import json",
                "3": "from build.pydriller import get_commits_data",
                "4": "from build.comment_lister import run_comment_lister, filter_comments_by_time",
                "5": "from build.utils import save_to_json",
                "6": "from build.analysis import analyse_diff_comments, blockify_comments, blockify_comments2, extract_later_modified_comments, clean, average_comment_update_time, classify_comments, classify_content",
                "7": "from build.xes_conversion import convert_json_to_xes",
                "8": "from datetime import datetime, timezone",
                "9": "import os",
                "10": "import subprocess",
                "11": "import shutil",
                "12": "",
                "13": "def get_source_code_from_tag(repo_path, tag_name, dt1, dt2):",
                "14": "    \"\"\"",
                "15": "    DOCSTRIING: Extracts the source code of a specific commit tagged in the repository.",
                "16": "",
                "17": "    :param repo_path: Path to the local Git repository.",
                "18": "    :param tag_name: The name of the tag to fetch.",
                "19": "    :return: A dictionary containing file paths and their contents at the given commit.",
                "20": "    END DOCSTRING",
                "21": "    \"\"\"",
                "22": "    source_code = []",
                "23": "",
                "24": "    for commit in Repository(repo_path, since=dt1, to=dt2).traverse_commits():",
                "25": "        print(f\"Processing commit: {commit.hash} tagged as {tag_name}\")",
                "26": "        for modified_file in commit.modified_files:",
                "27": "            # NORMAL Save the file path and its source code",
                "28": "            if modified_file.source_code:",
                "29": "                # BLOCK: Multiple lines",
                "30": "                # of comment",
                "31": "                if modified_file.filename.find(\".py\") != -1 and modified_file.filename.find(\".pyc\") == -1:",
                "32": "                    comit = {",
                "33": "                        commit.hash + \"---\" + modified_file.filename: list_to_dict(modified_file.source_code.split(\"\\n\")) #INLINE: identify each block of data",
                "34": "                    }",
                "35": "                    source_code.append(comit)",
                "36": "                    # print(commit) #COMMENTED-OUT",
                "37": "    return source_code  ",
                "38": "",
                "39": "# Example usage",
                "40": "repo_url = \"https://github.com/AlexS-1/Bachelor-Code\"",
                "41": "tag_name = \"a1ad5c2cb35d621f2b187166af65a2b2ee3ea45e\"",
                "42": "start_time = datetime(2024,12,3)",
                "43": "end_time = datetime.today()",
                "44": "repo_name = os.path.basename(repo_url).replace(\".git\", \"\")",
                "45": "temp_dir = \"/Users/as/Library/Mobile Documents/com~apple~CloudDocs/Dokumente/Studium/Bachelor-Thesis/tmp\"",
                "46": "clone_path = os.path.join(temp_dir, repo_name)",
                "47": "",
                "48": "subprocess.run(['git', 'clone', repo_url, clone_path], check=True)",
                "49": "",
                "50": "# # Paths",
                "51": "repo_path = clone_path",
                "52": "jar_path = \"/Users/as/Library/Mobile Documents/com~apple~CloudDocs/Dokumente/Studium/Bachelor-Thesis/CommentLister/target/CommentLister.jar\"",
                "53": "file_types = [\".c\", \".c\", \".cc\", \".cp\", \".cpp\", \".cx\", \".cxx\", \".c+\", \".c++\", \".h\", \".hh\", \".hxx\", \".h+\", \".h++\", \".hp\", \".hpp\", \".java\", \".js\", \".cs\", \".py\", \".php\", \".rb\"]",
                "54": "",
                "55": "commits_data = get_commits_data(repo_path, start_time, datetime.today(), file_types)",
                "56": "save_to_json(commits_data, \"Toy-Example/commits_data.json\")",
                "57": "with open (\"Toy-Example/commits_data.json\", \"r\") as json_file: ",
                "58": "        commits_data = json.load(json_file)",
                "59": "",
                "60": "for file, commits in commits_data.items():",
                "61": "    for commit in commits:",
                "62": "        tag = \"-target=\" + commit[\"commit\"]",
                "63": "        output = run_comment_lister(repo_path, jar_path, tag)",
                "64": "        # Parse output as JSON",
                "65": "        try:",
                "66": "            comment_data = json.loads(output)",
                "67": "        except json.JSONDecodeError as e:",
                "68": "            print(f\"Failed to parse CommentLister output: {e}\")",
                "69": "            break",
                "70": "        # Filter comments by time",
                "71": "        commit_hash, filtered_comments = filter_comments_by_time(comment_data, start_time, end_time)",
                "72": "        if commit[\"commit\"] == commit_hash and file in filtered_comments.keys():",
                "73": "            commit[\"comments\"] = filtered_comments[file]",
                "74": "        else:",
                "75": "            print(\"mismatch in commit and comment data or no comments in this commit for investigatet file\")",
                "76": "            print(\"file could have been deleted\")",
                "77": "            commit[\"comments\"] = {}",
                "78": "# Save filtered comments on your system",
                "79": "save_to_json(commits_data, \"Toy-Example/filtered_commits_data.json\")",
                "80": "shutil.rmtree(clone_path)",
                "81": "with open(\"Toy-Example/filtered_commits_data.json\", \"r\") as json_file:",
                "82": "    data = json.load(json_file)",
                "83": "# analyse_diff_comments(data)",
                "84": "blockify_comments(data)",
                "85": "save_to_json(data, \"Toy-Example/blockified_comments_data.json\")",
                "86": "with open(\"Toy-Example/blockified_comments_data.json\", \"r\") as json_file:",
                "87": "    data = json.load(json_file)",
                "88": "blockify_comments2(data)",
                "89": "save_to_json(data, \"Toy-Example/blockified_comments2_data.json\")",
                "90": "with open(\"Toy-Example/blockified_comments2_data.json\", \"r\") as json_file:",
                "91": "    data = json.load(json_file)",
                "92": "d = extract_later_modified_comments(data)",
                "93": "save_to_json(d, \"Toy-Example/analysis_results.json\")",
                "94": "with open(\"Toy-Example/analysis_results.json\", \"r\") as json_file:",
                "95": "    data = json.load(json_file)",
                "96": "d = clean(data)",
                "97": "save_to_json(d, \"Toy-Example/clean_analysis_results.json\")",
                "98": "with open(\"Toy-Example/clean_analysis_results.json\", \"r\") as json_file:",
                "99": "    data = json.load(json_file)",
                "100": "d = classify_content(data)",
                "101": "save_to_json(d, \"Toy-Example/clean_analysis_results2.json\")",
                "102": "print(\"Average duration:\", average_comment_update_time(d))",
                "103": "convert_json_to_xes(d, 'Toy-Example/output.xes')"
            },
            "comments": [
                {
                    "line": 14,
                    "comment": "\"\"\"",
                    "char_position_in_line": 4,
                    "type": [
                        "block",
                        "docstring"
                    ]
                },
                {
                    "line": 15,
                    "comment": "    DOCSTRIING: Extracts the source code of a specific commit tagged in the repository.",
                    "char_position_in_line": 4,
                    "type": [
                        "block",
                        "docstring"
                    ]
                },
                {
                    "line": 16,
                    "comment": "",
                    "char_position_in_line": 4,
                    "type": [
                        "block",
                        "docstring"
                    ]
                },
                {
                    "line": 17,
                    "comment": "    :param repo_path: Path to the local Git repository.",
                    "char_position_in_line": 4,
                    "type": [
                        "block",
                        "docstring"
                    ]
                },
                {
                    "line": 18,
                    "comment": "    :param tag_name: The name of the tag to fetch.",
                    "char_position_in_line": 4,
                    "type": [
                        "block",
                        "docstring"
                    ]
                },
                {
                    "line": 19,
                    "comment": "    :return: A dictionary containing file paths and their contents at the given commit.",
                    "char_position_in_line": 4,
                    "type": [
                        "block",
                        "docstring"
                    ]
                },
                {
                    "line": 20,
                    "comment": "    END DOCSTRING",
                    "char_position_in_line": 4,
                    "type": [
                        "block",
                        "docstring"
                    ]
                },
                {
                    "line": 21,
                    "comment": "    \"\"\"",
                    "char_position_in_line": 4,
                    "type": [
                        "block",
                        "docstring"
                    ]
                },
                {
                    "line": 27,
                    "comment": "# NORMAL Save the file path and its source code",
                    "char_position_in_line": 12,
                    "type": [
                        "normal"
                    ]
                },
                {
                    "line": 29,
                    "comment": "# BLOCK: Multiple lines",
                    "char_position_in_line": 16,
                    "type": [
                        "block"
                    ]
                },
                {
                    "line": 30,
                    "comment": "# of comment",
                    "char_position_in_line": 16,
                    "type": [
                        "block"
                    ]
                },
                {
                    "line": 33,
                    "comment": "#INLINE: identify each block of data",
                    "char_position_in_line": 122,
                    "type": [
                        "normal"
                    ]
                },
                {
                    "line": 36,
                    "comment": "# print(commit) #COMMENTED-OUT",
                    "char_position_in_line": 20,
                    "type": [
                        "commented-out"
                    ]
                },
                {
                    "line": 39,
                    "comment": "# Example usage",
                    "char_position_in_line": 0,
                    "type": [
                        "normal"
                    ]
                },
                {
                    "line": 50,
                    "comment": "# # Paths",
                    "char_position_in_line": 0,
                    "type": [
                        "commented-out"
                    ]
                },
                {
                    "line": 64,
                    "comment": "# Parse output as JSON",
                    "char_position_in_line": 8,
                    "type": [
                        "normal"
                    ]
                },
                {
                    "line": 70,
                    "comment": "# Filter comments by time",
                    "char_position_in_line": 8,
                    "type": [
                        "normal"
                    ]
                },
                {
                    "line": 78,
                    "comment": "# Save filtered comments on your system",
                    "char_position_in_line": 0,
                    "type": [
                        "normal"
                    ]
                },
                {
                    "line": 83,
                    "comment": "# analyse_diff_comments(data)",
                    "char_position_in_line": 0,
                    "type": [
                        "commented-out"
                    ]
                }
            ]
        }
    ]
}