{
    "22934a5f507d06c9062632cc27ca51c4caae0042": {
        "filename_old": "main.py",
        "source_old": "import json\nimport shutil\nfrom datetime import datetime, timedelta\n\nfrom build.analysis import (analyse_diff_comments, average_comment_update_time,\n                            blockify_code_data, classify_content,\n                            set_metadata_for_block)\nfrom build.comment_lister import get_comment_data\nfrom build.extraction import (blockify_diff, classify_content,\n                              extract_later_modified_comments, filter_comments_by_time, order_commits_data)\nfrom build.pydriller import get_commits_data, get_parent_commit\nfrom build.utils import clone_ropositoriy, save_to_json\n\n\ndef main():\n    # Convert repo URL to path by cloning repo to temporary dictionary\n    repo_url = \"https://github.com/AlexS-1/Bachelor-Code\"\n    \n    # Setting different timeperiod\n    start_time = datetime.today().replace(tzinfo=None, microsecond=0) - timedelta(days=365)\n    end_time = datetime.today().replace(microsecond=0)\n\n    # Select from the supported file types for comment extraction\n    file_types = [\".c\", \".c\", \".cc\", \".cp\", \".cpp\", \".cx\", \".cxx\", \".c+\", \".c++\", \".h\", \".hh\", \".hxx\", \".h+\", \".h++\", \".hp\", \".hpp\", \".java\", \".js\", \".cs\", \".py\", \".php\", \".rb\"]\n\n    # Get and store the code data usingy PyDriller\n    repo_path = clone_ropositoriy(repo_url)\n    commits_data = get_commits_data(repo_path, start_time, end_time, file_types)\n    code_data = order_commits_data(commits_data)\n    save_to_json(code_data, \"Data/code_data.json\")\n    # Get and store the comment data using CommentLister\n    add_comments_to_code(code_data, repo_path, start_time, end_time)\n    save_to_json(code_data, \"Data/code_data_with_comments.json\")\n\n    # Extract code changes and comment changes from code_data\n    with open(\"Data/code_data_with_comments.json\", \"r\") as json_file:\n        code_data = json.load(json_file)\n    analyse_diff_comments(code_data)\n    blockify_code_data(code_data, old=False)\n    blockify_code_data(code_data, old=True)\n    set_metadata_for_block(code_data)\n    blockify_diff(code_data, \"added\")\n    blockify_diff(code_data, \"deleted\")\n    save_to_json(code_data, \"Data/blockified_code_data_with_comments.json\")\n\n    # TODO Find best point in time to remove tmp repo\n    shutil.rmtree(repo_path)\n    \n    # Extract blocks with either outdated or updated comments\n    comment_data = extract_later_modified_comments(code_data)\n    for block in comment_data:\n        for line, data in block[\"comment_lines\"].items():\n            data[\"type\"] += classify_content(block[\"code_lines\"][line])\n            data[\"type\"] = list(dict.fromkeys(data[\"type\"]))\n    save_to_json(comment_data, \"Exports/comment_data.json\")\n    print(\"Average duration:\", average_comment_update_time(comment_data))\n\ndef add_comments_to_code(code_data, repo_path, start_time, end_time):\n    # Add comments to the code data using CommentLister\n    for file, commits in code_data.items():\n        previous_commit = get_parent_commit(commits[0][\"commit\"], file, repo_path)\n        if previous_commit is None:\n            raise Exception(\"Failed to get parent commit\")\n        for commit in commits:\n            tag = \"-target=\" + commit[\"commit\"]\n            output = get_comment_data(repo_path, tag)\n            output_old = get_comment_data(repo_path, \"-target=\" + previous_commit)\n            # Parse output as JSON\n            try:\n                comment_data = json.loads(output)\n                comment_data_old = json.loads(output_old)\n            except json.JSONDecodeError as e:\n                raise Exception(f\"Failed to parse CommentLister output: {e}\")\n            # Filter comments by time\n            commit_hash, filtered_comments = filter_comments_by_time(comment_data, start_time, end_time)\n            commit_hash_old, filtered_comments_old = filter_comments_by_time(comment_data_old, start_time, end_time)\n            if commit[\"commit\"] == commit_hash and file in filtered_comments.keys():\n                commit[\"comments\"] = filtered_comments[file]\n            else:\n                print(\"No comments in this Commit\", commit[\"commit\"], \"for investigate file\", file)\n                commit[\"comments\"] = {}\n            if previous_commit == commit_hash_old and file in filtered_comments_old.keys():\n                commit[\"comments_old\"] = filtered_comments_old[file]\n            else:\n                print(\"No comments in this commit's parents\", commit[\"commit\"], \"for investigate file\", file)\n                commit[\"comments_old\"] = {}\n            previous_commit = commit[\"commit\"]\n\nif __name__ == \"__main__\":\n    main()",
        "filename_new": "main.py",
        "source_new": "import json\nimport shutil\nfrom datetime import datetime, timedelta\n\nfrom build.analysis import (analyse_diff_comments, average_comment_update_time,\n                            blockify_code_data, classify_content,\n                            set_metadata_for_block)\nfrom build.comment_lister import get_comment_data\nfrom build.extraction import (blockify_diff, classify_content,\n                              extract_later_modified_comments, filter_comments_by_time, order_commits_data)\nfrom build.pydriller import get_commits_data, get_parent_commit\nfrom build.utils import clone_ropositoriy, save_to_json\n\n\ndef main():\n    # Convert repo URL to path by cloning repo to temporary dictionary\n    repo_url = \"https://github.com/AlexS-1/Toy-Example\"\n    \n    # Setting different timeperiod\n    start_time = datetime.today().replace(tzinfo=None, microsecond=0) - timedelta(days=365)\n    end_time = datetime.today().replace(microsecond=0)\n\n    # Select from the supported file types for comment extraction\n    file_types = [\".c\", \".c\", \".cc\", \".cp\", \".cpp\", \".cx\", \".cxx\", \".c+\", \".c++\", \".h\", \".hh\", \".hxx\", \".h+\", \".h++\", \".hp\", \".hpp\", \".java\", \".js\", \".cs\", \".py\", \".php\", \".rb\"]\n\n    # Get and store the code data usingy PyDriller\n    repo_path = clone_ropositoriy(repo_url)\n    commits_data = get_commits_data(repo_path, start_time, end_time, file_types)\n    code_data = order_commits_data(commits_data)\n    save_to_json(code_data, \"Data/code_data.json\")\n    # Get and store the comment data using CommentLister\n    add_comments_to_code(code_data, repo_path, start_time, end_time)\n    save_to_json(code_data, \"Data/code_data_with_comments.json\")\n\n    # Extract code changes and comment changes from code_data\n    with open(\"Data/code_data_with_comments.json\", \"r\") as json_file:\n        code_data = json.load(json_file)\n    analyse_diff_comments(code_data)\n    blockify_code_data(code_data, old=False)\n    blockify_code_data(code_data, old=True)\n    set_metadata_for_block(code_data)\n    blockify_diff(code_data, \"added\")\n    blockify_diff(code_data, \"deleted\")\n    save_to_json(code_data, \"Data/blockified_code_data_with_comments.json\")\n\n    # TODO Find best point in time to remove tmp repo\n    shutil.rmtree(repo_path)\n    \n    # Extract blocks with either outdated or updated comments\n    comment_data = extract_later_modified_comments(code_data)\n    for block in comment_data:\n        for line, data in block[\"comment_lines\"].items():\n            data[\"type\"] += classify_content(block[\"code_lines\"][line])\n            data[\"type\"] = list(dict.fromkeys(data[\"type\"]))\n    save_to_json(comment_data, \"Exports/comment_data.json\")\n    print(\"Average duration:\", average_comment_update_time(comment_data))\n\ndef add_comments_to_code(code_data, repo_path, start_time, end_time):\n    # Add comments to the code data using CommentLister\n    for file, commits in code_data.items():\n        previous_commit = get_parent_commit(commits[0][\"commit\"], file, repo_path)\n        if previous_commit is None:\n            raise Exception(\"Failed to get parent commit\")\n        for commit in commits:\n            tag = \"-target=\" + commit[\"commit\"]\n            output = get_comment_data(repo_path, tag)\n            output_old = get_comment_data(repo_path, \"-target=\" + previous_commit)\n            # Parse output as JSON\n            try:\n                comment_data = json.loads(output)\n                comment_data_old = json.loads(output_old)\n            except json.JSONDecodeError as e:\n                raise Exception(f\"Failed to parse CommentLister output: {e}\")\n            # Filter comments by time\n            commit_hash, filtered_comments = filter_comments_by_time(comment_data, start_time, end_time)\n            commit_hash_old, filtered_comments_old = filter_comments_by_time(comment_data_old, start_time, end_time)\n            if commit[\"commit\"] == commit_hash and file in filtered_comments.keys():\n                commit[\"comments\"] = filtered_comments[file]\n            else:\n                print(\"No comments in this Commit\", commit[\"commit\"], \"for investigate file\", file)\n                commit[\"comments\"] = {}\n            if previous_commit == commit_hash_old and file in filtered_comments_old.keys():\n                commit[\"comments_old\"] = filtered_comments_old[file]\n            else:\n                print(\"No comments in this commit's parents\", commit[\"commit\"], \"for investigate file\", file)\n                commit[\"comments_old\"] = {}\n            previous_commit = commit[\"commit\"]\n\nif __name__ == \"__main__\":\n    main()",
        "additions": 1,
        "deletions": 1,
        "cyclomatic_complexity_new": 12
    },
    "68207dd52b35454a347e97d0dd04a634a92b9d07": {
        "filename_old": "main.py",
        "source_old": "import json\nimport shutil\nfrom datetime import datetime, timedelta\n\nfrom build.analysis import (analyse_diff_comments, average_comment_update_time,\n                            blockify_code_data, classify_content,\n                            set_metadata_for_block)\nfrom build.comment_lister import get_comment_data\nfrom build.extraction import (blockify_diff, classify_content,\n                              extract_later_modified_comments, filter_comments_by_time, order_commits_data)\nfrom build.pydriller import get_commits_data, get_parent_commit\nfrom build.utils import clone_ropositoriy, save_to_json\n\n\ndef main():\n    # Convert repo URL to path by cloning repo to temporary dictionary\n    repo_url = \"https://github.com/AlexS-1/Toy-Example\"\n    \n    # Setting different timeperiod\n    start_time = datetime.today().replace(tzinfo=None, microsecond=0) - timedelta(days=365)\n    end_time = datetime.today().replace(microsecond=0)\n\n    # Select from the supported file types for comment extraction\n    file_types = [\".c\", \".c\", \".cc\", \".cp\", \".cpp\", \".cx\", \".cxx\", \".c+\", \".c++\", \".h\", \".hh\", \".hxx\", \".h+\", \".h++\", \".hp\", \".hpp\", \".java\", \".js\", \".cs\", \".py\", \".php\", \".rb\"]\n\n    # Get and store the code data usingy PyDriller\n    repo_path = clone_ropositoriy(repo_url)\n    commits_data = get_commits_data(repo_path, start_time, end_time, file_types)\n    code_data = order_commits_data(commits_data)\n    save_to_json(code_data, \"Data/code_data.json\")\n    # Get and store the comment data using CommentLister\n    add_comments_to_code(code_data, repo_path, start_time, end_time)\n    save_to_json(code_data, \"Data/code_data_with_comments.json\")\n\n    # Extract code changes and comment changes from code_data\n    with open(\"Data/code_data_with_comments.json\", \"r\") as json_file:\n        code_data = json.load(json_file)\n    analyse_diff_comments(code_data)\n    blockify_code_data(code_data, old=False)\n    blockify_code_data(code_data, old=True)\n    set_metadata_for_block(code_data)\n    blockify_diff(code_data, \"added\")\n    blockify_diff(code_data, \"deleted\")\n    save_to_json(code_data, \"Data/blockified_code_data_with_comments.json\")\n\n    # TODO Find best point in time to remove tmp repo\n    shutil.rmtree(repo_path)\n    \n    # Extract blocks with either outdated or updated comments\n    comment_data = extract_later_modified_comments(code_data)\n    for block in comment_data:\n        for line, data in block[\"comment_lines\"].items():\n            data[\"type\"] += classify_content(block[\"code_lines\"][line])\n            data[\"type\"] = list(dict.fromkeys(data[\"type\"]))\n    save_to_json(comment_data, \"Exports/comment_data.json\")\n    print(\"Average duration:\", average_comment_update_time(comment_data))\n\ndef add_comments_to_code(code_data, repo_path, start_time, end_time):\n    # Add comments to the code data using CommentLister\n    for file, commits in code_data.items():\n        previous_commit = get_parent_commit(commits[0][\"commit\"], file, repo_path)\n        if previous_commit is None:\n            raise Exception(\"Failed to get parent commit\")\n        for commit in commits:\n            tag = \"-target=\" + commit[\"commit\"]\n            output = get_comment_data(repo_path, tag)\n            output_old = get_comment_data(repo_path, \"-target=\" + previous_commit)\n            # Parse output as JSON\n            try:\n                comment_data = json.loads(output)\n                comment_data_old = json.loads(output_old)\n            except json.JSONDecodeError as e:\n                raise Exception(f\"Failed to parse CommentLister output: {e}\")\n            # Filter comments by time\n            commit_hash, filtered_comments = filter_comments_by_time(comment_data, start_time, end_time)\n            commit_hash_old, filtered_comments_old = filter_comments_by_time(comment_data_old, start_time, end_time)\n            if commit[\"commit\"] == commit_hash and file in filtered_comments.keys():\n                commit[\"comments\"] = filtered_comments[file]\n            else:\n                print(\"No comments in this Commit\", commit[\"commit\"], \"for investigate file\", file)\n                commit[\"comments\"] = {}\n            if previous_commit == commit_hash_old and file in filtered_comments_old.keys():\n                commit[\"comments_old\"] = filtered_comments_old[file]\n            else:\n                print(\"No comments in this commit's parents\", commit[\"commit\"], \"for investigate file\", file)\n                commit[\"comments_old\"] = {}\n            previous_commit = commit[\"commit\"]\n\nif __name__ == \"__main__\":\n    main()",
        "filename_new": "main.py",
        "source_new": "import json\nfrom mimetypes import init\nimport shutil\nfrom datetime import datetime, timedelta\n\nfrom build.analysis import analyse_message\nfrom build.api_handler import get_issues, get_repo_information, get_closed_pulls\nfrom build.pydriller import get_and_insert_commits_data\nfrom build.utils import clone_ropositoriy\nfrom build.database_handler import get_commits, initialise_database\n\n\ndef main():\n    # Convert repo URL to path by cloning repo to temporary dictionary\n    repo_url = \"https://github.com/srbhr/Resume-Matcher\"\n    \n    # Setting different timeperiod\n    start_time = datetime.today().replace(tzinfo=None, microsecond=0) - timedelta(days=365)\n    end_time = datetime.today().replace(microsecond=0)\n\n    # Select from the supported file types for comment extraction\n    file_types = [\".c\", \".c\", \".cc\", \".cp\", \".cpp\", \".cx\", \".cxx\", \".c+\", \".c++\", \".h\", \".hh\", \".hxx\", \".h+\", \".h++\", \".hp\", \".hpp\", \".java\", \".js\", \".cs\", \".py\", \".php\", \".rb\"]\n\n    # Get and store the code data usingy PyDriller before deleting the cloned repository\n    repo_path = clone_ropositoriy(repo_url)\n    get_and_insert_commits_data(repo_path, start_time, end_time, file_types)\n    shutil.rmtree(repo_path)\n\n    commits = get_commits()\n    repo_info = get_repo_information()\n    get_closed_pulls(repo_info[\"utility_information\"][\"pulls_url\"])\n    get_issues(repo_info[\"utility_information\"][\"issues_url\"])\n    initialise_database()\n\nif __name__ == \"__main__\":\n    main()",
        "additions": 14,
        "deletions": 68,
        "cyclomatic_complexity_new": 1
    },
    "b95e5b6ba725e84d8258b56485bd25da0c2a1fb0": {
        "filename_old": "build/utils.py",
        "source_old": "import datetime\nimport os\nimport subprocess\n\ndef diff_to_dict(diff):\n    return {\n        diff[0]: diff[1]\n    }   \n\ndef clone_ropositoriy(repo_url, temp_dir=\"/Users/as/Library/Mobile Documents/com~apple~CloudDocs/Dokumente/Studium/Bachelor-Thesis/tmp\"):\n    repo_name = os.path.basename(repo_url).replace(\".git\", \"\")\n    clone_path = os.path.join(temp_dir, repo_name)\n    subprocess.run(['git', 'clone', repo_url, clone_path], check=True)\n    return clone_path\n\ndef array_to_string(array):\n    return \"[\" + \", \".join(map(str, array)) + \"]\"\n\ndef generic_to_python_type(python_type):\n    if (python_type == \"string\"):\n        return str\n    elif (python_type == \"int\"):\n        return int\n    elif (python_type == \"time\"):\n        return datetime.datetime\n    elif (python_type == \"boolean\"):\n        return bool\n    else:\n        return None",
        "filename_new": "build/utils.py",
        "source_new": "import datetime\nimport os\nimport subprocess\n\ndef diff_to_dict(diff):\n    return {\n        diff[0]: diff[1]\n    }   \n\ndef clone_ropositoriy(repo_url, temp_dir=\"/Users/as/Library/Mobile Documents/com~apple~CloudDocs/Dokumente/Studium/Bachelor-Thesis/tmp\"):\n    repo_name = os.path.basename(repo_url).replace(\".git\", \"\")\n    clone_path = os.path.join(temp_dir, repo_name)\n    subprocess.run(['git', 'clone', repo_url, clone_path], check=True)\n    return clone_path\n\ndef array_to_string(array):\n    return \"[\" + \", \".join(map(str, array)) + \"]\"\n\ndef generic_to_python_type(python_type):\n    if (python_type == \"string\"):\n        return str\n    elif (python_type == \"int\"):\n        return int\n    elif (python_type == \"time\"):\n        return datetime.datetime\n    elif (python_type == \"boolean\"):\n        return bool\n    else:\n        return None\n    ",
        "additions": 2,
        "deletions": 1,
        "cyclomatic_complexity_new": 8
    },
    "71bf0e6b1e3cb60e75b842c3a4c6e87a7d64ea7f": {
        "filename_old": "build/utils.py",
        "source_old": "import datetime\nimport os\nimport subprocess\n\ndef diff_to_dict(diff):\n    return {\n        diff[0]: diff[1]\n    }   \n\ndef clone_ropositoriy(repo_url, temp_dir=\"/Users/as/Library/Mobile Documents/com~apple~CloudDocs/Dokumente/Studium/Bachelor-Thesis/tmp\"):\n    repo_name = os.path.basename(repo_url).replace(\".git\", \"\")\n    clone_path = os.path.join(temp_dir, repo_name)\n    subprocess.run(['git', 'clone', repo_url, clone_path], check=True)\n    return clone_path\n\ndef array_to_string(array):\n    return \"[\" + \", \".join(map(str, array)) + \"]\"\n\ndef generic_to_python_type(python_type):\n    if (python_type == \"string\"):\n        return str\n    elif (python_type == \"int\"):\n        return int\n    elif (python_type == \"time\"):\n        return datetime.datetime\n    elif (python_type == \"boolean\"):\n        return bool\n    else:\n        return None",
        "filename_new": "build/utils.py",
        "source_new": "import datetime\nimport os\nimport subprocess\n\ndef diff_to_dict(diff):\n    return {\n        diff[0]: diff[1]\n    }   \n\ndef clone_ropositoriy(repo_url, temp_dir=\"/Users/as/Library/Mobile Documents/com~apple~CloudDocs/Dokumente/Studium/Bachelor-Thesis/tmp\"):\n    repo_name = os.path.basename(repo_url).replace(\".git\", \"\")\n    clone_path = os.path.join(temp_dir, repo_name)\n    subprocess.run(['git', 'clone', repo_url, clone_path], check=True)\n    return clone_path\n\ndef array_to_string(array):\n    return \"[\" + \", \".join(map(str, array)) + \"]\"\n\ndef generic_to_python_type(python_type):\n    if (python_type == \"string\"):\n        return str\n    elif (python_type == \"int\"):\n        return int\n    elif (python_type == \"time\"):\n        return datetime.datetime\n    elif (python_type == \"boolean\"):\n        return bool\n    else:\n        return None\n    ",
        "additions": 2,
        "deletions": 1,
        "cyclomatic_complexity_new": 8
    },
    "db4fba8425d12a3031fb272c01f19a4bdf1fafa4": {
        "filename_old": "main.py",
        "source_old": "import json\nfrom mimetypes import init\nimport shutil\nfrom datetime import datetime, timedelta\n\nfrom build.analysis import analyse_message\nfrom build.api_handler import get_issues, get_repo_information, get_closed_pulls\nfrom build.pydriller import get_and_insert_commits_data\nfrom build.utils import clone_ropositoriy\nfrom build.database_handler import get_commits, initialise_database\n\n\ndef main():\n    # Convert repo URL to path by cloning repo to temporary dictionary\n    repo_url = \"https://github.com/srbhr/Resume-Matcher\"\n    \n    # Setting different timeperiod\n    start_time = datetime.today().replace(tzinfo=None, microsecond=0) - timedelta(days=365)\n    end_time = datetime.today().replace(microsecond=0)\n\n    # Select from the supported file types for comment extraction\n    file_types = [\".c\", \".c\", \".cc\", \".cp\", \".cpp\", \".cx\", \".cxx\", \".c+\", \".c++\", \".h\", \".hh\", \".hxx\", \".h+\", \".h++\", \".hp\", \".hpp\", \".java\", \".js\", \".cs\", \".py\", \".php\", \".rb\"]\n\n    # Get and store the code data usingy PyDriller before deleting the cloned repository\n    repo_path = clone_ropositoriy(repo_url)\n    get_and_insert_commits_data(repo_path, start_time, end_time, file_types)\n    shutil.rmtree(repo_path)\n\n    commits = get_commits()\n    repo_info = get_repo_information()\n    get_closed_pulls(repo_info[\"utility_information\"][\"pulls_url\"])\n    get_issues(repo_info[\"utility_information\"][\"issues_url\"])\n    initialise_database()\n\nif __name__ == \"__main__\":\n    main()",
        "filename_new": "main.py",
        "source_new": "import json\nfrom mimetypes import init\nimport shutil\nfrom datetime import datetime, timedelta\n\nfrom build.analysis import analyse_message\nfrom build.api_handler import get_issues, get_repo_information, get_closed_pulls\nfrom build.pydriller import get_and_insert_commits_data\nfrom build.utils import clone_ropositoriy\nfrom build.database_handler import get_commits, initialise_database\n\n\ndef main():\n    # Convert repo URL to path by cloning repo to temporary dictionary\n    repo_url = \"https://github.com/srbhr/Resume-Matcher\"\n    \n    # Setting different timeperiod\n    start_time = datetime.today().replace(tzinfo=None, microsecond=0) - timedelta(days=365)\n    end_time = datetime.today().replace(microsecond=0)\n\n    # Select from the supported file types for comment extraction\n    file_types = [\".c\", \".c\", \".cc\", \".cp\", \".cpp\", \".cx\", \".cxx\", \".c+\", \".c++\", \".h\", \".hh\", \".hxx\", \".h+\", \".h++\", \".hp\", \".hpp\", \".java\", \".js\", \".cs\", \".py\", \".php\", \".rb\"]\n\n    initialise_database()\n\n    # Get and store the code data usingy PyDriller before deleting the cloned repository\n    repo_path = clone_ropositoriy(repo_url)\n    get_and_insert_commits_data(repo_path, start_time, end_time, file_types)\n    shutil.rmtree(repo_path)\n\n    repo_info = get_repo_information()\n    get_closed_pulls(repo_info[\"utility_information\"][\"pulls_url\"])\n    get_issues(repo_info[\"utility_information\"][\"issues_url\"])\n\nif __name__ == \"__main__\":\n    main()",
        "additions": 2,
        "deletions": 2,
        "cyclomatic_complexity_new": 1
    },
    "1f3dce443771b6dfe05905166123adac5223ee7c": {
        "filename_old": "main.py",
        "source_old": "import json\nfrom mimetypes import init\nimport shutil\nfrom datetime import datetime, timedelta\n\nfrom build.analysis import analyse_message\nfrom build.api_handler import get_issues, get_repo_information, get_closed_pulls\nfrom build.pydriller import get_and_insert_commits_data\nfrom build.utils import clone_ropositoriy\nfrom build.database_handler import get_commits, initialise_database\n\n\ndef main():\n    # Convert repo URL to path by cloning repo to temporary dictionary\n    repo_url = \"https://github.com/srbhr/Resume-Matcher\"\n    \n    # Setting different timeperiod\n    start_time = datetime.today().replace(tzinfo=None, microsecond=0) - timedelta(days=365)\n    end_time = datetime.today().replace(microsecond=0)\n\n    # Select from the supported file types for comment extraction\n    file_types = [\".c\", \".c\", \".cc\", \".cp\", \".cpp\", \".cx\", \".cxx\", \".c+\", \".c++\", \".h\", \".hh\", \".hxx\", \".h+\", \".h++\", \".hp\", \".hpp\", \".java\", \".js\", \".cs\", \".py\", \".php\", \".rb\"]\n\n    initialise_database()\n\n    # Get and store the code data usingy PyDriller before deleting the cloned repository\n    repo_path = clone_ropositoriy(repo_url)\n    get_and_insert_commits_data(repo_path, start_time, end_time, file_types)\n    shutil.rmtree(repo_path)\n\n    repo_info = get_repo_information()\n    get_closed_pulls(repo_info[\"utility_information\"][\"pulls_url\"])\n    get_issues(repo_info[\"utility_information\"][\"issues_url\"])\n\nif __name__ == \"__main__\":\n    main()",
        "filename_new": "main.py",
        "source_new": "import json\nfrom mimetypes import init\nimport shutil\nfrom datetime import datetime, timedelta\n\nfrom httpx import get\n\nfrom build.analysis import analyse_message\nfrom build.api_handler import get_issues, get_repo_information, get_closed_pulls, get_anonymous_user_counter\nfrom build.pydriller import get_and_insert_commits_data\nfrom build.utils import clone_ropositoriy, delete_json, validate_json, write_json\nfrom build.database_handler import get_commits, get_ocel_data, initialise_database\n\n\ndef main():\n    # Convert repo URL to path by cloning repo to temporary dictionary\n    repo_url = \"https://github.com/srbhr/Resume-Matcher\"\n    \n    # Setting different timeperiod\n    start_time = datetime.today().replace(tzinfo=None, microsecond=0) - timedelta(days=365)\n    end_time = datetime.today().replace(microsecond=0)\n\n    # Select from the supported file types for comment extraction\n    file_types = [\".c\", \".c\", \".cc\", \".cp\", \".cpp\", \".cx\", \".cxx\", \".c+\", \".c++\", \".h\", \".hh\", \".hxx\", \".h+\", \".h++\", \".hp\", \".hpp\", \".java\", \".js\", \".cs\", \".py\", \".php\", \".rb\"]\n\n    initialise_database()\n\n    # Get and store the code data usingy PyDriller before deleting the cloned repository\n    repo_path = clone_ropositoriy(repo_url)\n    get_and_insert_commits_data(repo_path, start_time, end_time, file_types)\n    shutil.rmtree(repo_path)\n\n    repo_info = get_repo_information()\n    get_closed_pulls(repo_info[\"utility_information\"][\"pulls_url\"], 9)\n    get_issues(repo_info[\"utility_information\"][\"issues_url\"], 4)\n    count = get_anonymous_user_counter()\n    print(len(list(count.keys())))\n\n    # Validate the JSON data to OCEL format\n    write_json(\"Data/OCEL-Data.json\", get_ocel_data())\n    validate_json(\"Data/OCEL-Data.json\", \"Data/OCEL-Schema.json\")\n    delete_json(\"Data/OCEL-Data.json\")\n\nif __name__ == \"__main__\":\n    main()",
        "additions": 14,
        "deletions": 5,
        "cyclomatic_complexity_new": 1
    },
    "793791b2b41ec4067f17e91bf0fb34f5c7160243": {
        "filename_old": "main.py",
        "source_old": "import json\nfrom mimetypes import init\nimport shutil\nfrom datetime import datetime, timedelta\n\nfrom httpx import get\n\nfrom build.analysis import analyse_message\nfrom build.api_handler import get_issues, get_repo_information, get_closed_pulls, get_anonymous_user_counter\nfrom build.pydriller import get_and_insert_commits_data\nfrom build.utils import clone_ropositoriy, delete_json, validate_json, write_json\nfrom build.database_handler import get_commits, get_ocel_data, initialise_database\n\n\ndef main():\n    # Convert repo URL to path by cloning repo to temporary dictionary\n    repo_url = \"https://github.com/srbhr/Resume-Matcher\"\n    \n    # Setting different timeperiod\n    start_time = datetime.today().replace(tzinfo=None, microsecond=0) - timedelta(days=365)\n    end_time = datetime.today().replace(microsecond=0)\n\n    # Select from the supported file types for comment extraction\n    file_types = [\".c\", \".c\", \".cc\", \".cp\", \".cpp\", \".cx\", \".cxx\", \".c+\", \".c++\", \".h\", \".hh\", \".hxx\", \".h+\", \".h++\", \".hp\", \".hpp\", \".java\", \".js\", \".cs\", \".py\", \".php\", \".rb\"]\n\n    initialise_database()\n\n    # Get and store the code data usingy PyDriller before deleting the cloned repository\n    repo_path = clone_ropositoriy(repo_url)\n    get_and_insert_commits_data(repo_path, start_time, end_time, file_types)\n    shutil.rmtree(repo_path)\n\n    repo_info = get_repo_information()\n    get_closed_pulls(repo_info[\"utility_information\"][\"pulls_url\"], 9)\n    get_issues(repo_info[\"utility_information\"][\"issues_url\"], 4)\n    count = get_anonymous_user_counter()\n    print(len(list(count.keys())))\n\n    # Validate the JSON data to OCEL format\n    write_json(\"Data/OCEL-Data.json\", get_ocel_data())\n    validate_json(\"Data/OCEL-Data.json\", \"Data/OCEL-Schema.json\")\n    delete_json(\"Data/OCEL-Data.json\")\n\nif __name__ == \"__main__\":\n    main()",
        "filename_new": "main.py",
        "source_new": "import json\nfrom mimetypes import init\nimport shutil\nfrom datetime import datetime, timedelta\n\nfrom httpx import get\n\nfrom build.analysis import analyse_message\nfrom build.api_handler import get_issues, get_repo_information, get_closed_pulls, get_anonymous_user_counter\nfrom build.pydriller import get_and_insert_commits_data\nfrom build.utils import clone_ropositoriy, delete_json, validate_json, write_json\nfrom build.database_handler import get_commits, get_ocel_data, initialise_database\n\n\ndef main():\n    # Convert repo URL to path by cloning repo to temporary dictionary\n    repo_url = \"https://github.com/srbhr/Resume-Matcher\"\n    \n    # Setting different timeperiod\n    start_time = datetime.today().replace(tzinfo=None, microsecond=0) - timedelta(days=365)\n    end_time = datetime.today().replace(microsecond=0)\n\n    # Select from the supported file types for comment extraction\n    file_types = [\".c\", \".c\", \".cc\", \".cp\", \".cpp\", \".cx\", \".cxx\", \".c+\", \".c++\", \".h\", \".hh\", \".hxx\", \".h+\", \".h++\", \".hp\", \".hpp\", \".java\", \".js\", \".cs\", \".py\", \".php\", \".rb\"]\n\n    initialise_database()\n\n    # Get and store the code data usingy PyDriller before deleting the cloned repository\n    repo_path = clone_ropositoriy(repo_url)\n    get_and_insert_commits_data(repo_path, start_time, end_time, file_types)\n    shutil.rmtree(repo_path)\n\n    repo_info = get_repo_information()\n    get_closed_pulls(repo_info[\"utility_information\"][\"pulls_url\"], 9)\n    get_issues(repo_info[\"utility_information\"][\"issues_url\"], 4)\n    count = get_anonymous_user_counter()\n    print(len(list(count.keys())))\n\n    # Validate the JSON data to OCEL format\n    write_json(\"Exports/OCEL-Data.jsonocel\", get_ocel_data())\n    validate_json(\"Exports/OCEL-Data.jsonocel\", \"Data/OCEL-Schema.json\")\n\nif __name__ == \"__main__\":\n    main()",
        "additions": 2,
        "deletions": 3,
        "cyclomatic_complexity_new": 1
    }
}