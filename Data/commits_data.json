{
    "python-usage.rst": [],
    "array.rst": [],
    "c-info.beyond-basics.rst": [],
    "cfuncs.py": [
        {
            "commit": "8c77d69a1bab0b27b9f7b235a9df322ef06b1255",
            "timestamp": "2023-07-13T12:17:13+02:00",
            "author": "Lysandros Nikolaou",
            "commit_message": "Move complex utilities to npy_math.h [wheel build]",
            "additions": 3,
            "deletions": 2,
            "change_type": "MODIFY",
            "diff": "@@ -53,6 +53,7 @@\n \n includes['arrayobject.h'] = '''#define PY_ARRAY_UNIQUE_SYMBOL PyArray_API\n #include \"arrayobject.h\"'''\n+includes['npy_math.h'] = '#include \"numpy/npy_math.h\"'\n \n includes['arrayobject.h'] = '#include \"fortranobject.h\"'\n includes['stdarg.h'] = '#include <stdarg.h>'\n@@ -1096,7 +1097,7 @@\n \n \n needs['complex_long_double_from_pyobj'] = ['complex_long_double', 'long_double',\n-                                           'complex_double_from_pyobj']\n+                                           'complex_double_from_pyobj', 'npy_math.h']\n cfuncs['complex_long_double_from_pyobj'] = \"\"\"\\\n static int\n complex_long_double_from_pyobj(complex_long_double* v, PyObject *obj, const char *errmess)\n@@ -1123,7 +1124,7 @@\n \"\"\"\n \n \n-needs['complex_double_from_pyobj'] = ['complex_double']\n+needs['complex_double_from_pyobj'] = ['complex_double', 'npy_math.h']\n cfuncs['complex_double_from_pyobj'] = \"\"\"\\\n static int\n complex_double_from_pyobj(complex_double* v, PyObject *obj, const char *errmess) {\n",
            "comment_added_diff": {
                "56": "includes['npy_math.h'] = '#include \"numpy/npy_math.h\"'"
            },
            "comment_deleted_diff": {},
            "comment_modified_diff": {}
        }
    ],
    "multiarray.pyi": [],
    "arrayfunction_override.c": [],
    "multiarraymodule.c": [],
    "test_multiarray.py": [
        {
            "commit": "d7da7a490fc983f67a1241fe9fe409436d78f10c",
            "timestamp": "2022-10-11T09:13:53-06:00",
            "author": "Charles Harris",
            "commit_message": "TST, BLD: Fix failing aarch64 wheel builds.\n\nThe aarch64 wheel build tests are failing with OOM. The new test for\ncomplex128 dot for huge vectors is responsible as the useable memory\nis incorrectly determined and the check for sufficient memory fails.\nThe fix here is to define the `NPY_AVAILABLE_MEM=\"4 GB\"` environment\nvariable before the test call in `cibw_test_command.sh`.",
            "additions": 1,
            "deletions": 1,
            "change_type": "MODIFY",
            "diff": "@@ -6721,7 +6721,7 @@ def assert_dot_close(A, X, desired):\n \n     @pytest.mark.slow\n     @pytest.mark.parametrize(\"dtype\", [np.float64, np.complex128])\n-    @requires_memory(free_bytes=9*10**9)  # complex case needs 8GiB+\n+    @requires_memory(free_bytes=18e9)  # complex case needs 18GiB+\n     def test_huge_vectordot(self, dtype):\n         # Large vector multiplications are chunked with 32bit BLAS\n         # Test that the chunking does the right thing, see also gh-22262\n",
            "comment_added_diff": {
                "6724": "    @requires_memory(free_bytes=18e9)  # complex case needs 18GiB+"
            },
            "comment_deleted_diff": {
                "6724": "    @requires_memory(free_bytes=9*10**9)  # complex case needs 8GiB+"
            },
            "comment_modified_diff": {
                "6724": "    @requires_memory(free_bytes=9*10**9)  # complex case needs 8GiB+"
            }
        },
        {
            "commit": "eae88ff5f9cc3e3caea5e849600e0f8db52a67db",
            "timestamp": "2022-11-09T15:31:25+01:00",
            "author": "Sebastian Berg",
            "commit_message": "BUG: Fix use and errorchecking of ObjectType use\n\nThis should be replaced really, it is pretty bad API use, and doesn't\nwork well (up to being incorrect probably).\n\nBut working on other things (trying to make promotion strict and thus\nsaner), I realized that the use was always wrong: we cannot pass 0\nsince 0 means `bool`, what was always meant was passing no-type.\n\nSo fixing this, and adding the error check everywhere.  Checking\nfor `PyErr_Occurred()` may have been necessary at some point, but\nis not anymore.",
            "additions": 18,
            "deletions": 6,
            "change_type": "MODIFY",
            "diff": "@@ -1257,9 +1257,9 @@ def test_subarray_comparison(self):\n         # The main importance is that it does not return True:\n         with pytest.raises(TypeError):\n             x == y\n- \n+\n     def test_empty_structured_array_comparison(self):\n-        # Check that comparison works on empty arrays with nontrivially \n+        # Check that comparison works on empty arrays with nontrivially\n         # shaped fields\n         a = np.zeros(0, [('a', '<f8', (1, 1))])\n         assert_equal(a, a)\n@@ -2232,7 +2232,7 @@ def assert_c(arr):\n         assert_c(a.copy('C'))\n         assert_fortran(a.copy('F'))\n         assert_c(a.copy('A'))\n-    \n+\n     @pytest.mark.parametrize(\"dtype\", ['O', np.int32, 'i,O'])\n     def test__deepcopy__(self, dtype):\n         # Force the entry of NULLs into array\n@@ -2441,7 +2441,7 @@ def test_sort_unicode_kind(self):\n         np.array([0, 1, np.nan]),\n     ])\n     def test_searchsorted_floats(self, a):\n-        # test for floats arrays containing nans. Explicitly test \n+        # test for floats arrays containing nans. Explicitly test\n         # half, single, and double precision floats to verify that\n         # the NaN-handling is correct.\n         msg = \"Test real (%s) searchsorted with nans, side='l'\" % a.dtype\n@@ -2457,7 +2457,7 @@ def test_searchsorted_floats(self, a):\n         assert_equal(y, 2)\n \n     def test_searchsorted_complex(self):\n-        # test for complex arrays containing nans. \n+        # test for complex arrays containing nans.\n         # The search sorted routines use the compare functions for the\n         # array type, so this checks if that is consistent with the sort\n         # order.\n@@ -2479,7 +2479,7 @@ def test_searchsorted_complex(self):\n         a = np.array([0, 128], dtype='>i4')\n         b = a.searchsorted(np.array(128, dtype='>i4'))\n         assert_equal(b, 1, msg)\n-        \n+\n     def test_searchsorted_n_elements(self):\n         # Check 0 elements\n         a = np.ones(0)\n@@ -6731,6 +6731,18 @@ def test_huge_vectordot(self, dtype):\n         res = np.dot(data, data)\n         assert res == 2**30+100\n \n+    def test_dtype_discovery_fails(self):\n+        # See gh-14247, error checking was missing for failed dtype discovery\n+        class BadObject(object):\n+            def __array__(self):\n+                raise TypeError(\"just this tiny mint leaf\")\n+\n+        with pytest.raises(TypeError):\n+            np.dot(BadObject(), BadObject())\n+\n+        with pytest.raises(TypeError):\n+            np.dot(3.0, BadObject())\n+\n \n class MatmulCommon:\n     \"\"\"Common tests for '@' operator and numpy.matmul.\n",
            "comment_added_diff": {
                "1262": "        # Check that comparison works on empty arrays with nontrivially",
                "2444": "        # test for floats arrays containing nans. Explicitly test",
                "2460": "        # test for complex arrays containing nans.",
                "6735": "        # See gh-14247, error checking was missing for failed dtype discovery"
            },
            "comment_deleted_diff": {
                "1262": "        # Check that comparison works on empty arrays with nontrivially",
                "2444": "        # test for floats arrays containing nans. Explicitly test",
                "2460": "        # test for complex arrays containing nans."
            },
            "comment_modified_diff": {
                "1262": "        # Check that comparison works on empty arrays with nontrivially",
                "2444": "        # test for floats arrays containing nans. Explicitly test",
                "2460": "        # test for complex arrays containing nans."
            }
        },
        {
            "commit": "04d0e2155704ad939980a4eafefe3d817076fa39",
            "timestamp": "2022-11-21T15:17:52-07:00",
            "author": "Daniel da Silva",
            "commit_message": "ENH: raise TypeError when arange() is called with string dtype (#22087)\n\n* ENH: raise TypeError when arange() is called with string dtype\r\n\r\n* Add release note for dtype=str change to arange()\r\n\r\n* DOC: Minor wording/formatting touchups to release note.\r\n\r\n* Update numpy/core/tests/test_multiarray.py\r\n\r\nCo-authored-by: Ross Barnowski <rossbar@berkeley.edu>\r\n\r\n* Move check to PyArray_ArangeObj\r\n\r\n* remove old code\r\n\r\n* BUG,MAINT: Clean out arange string error and other paths\r\n\r\n* BUGS: Fixup and cleanup arange code a bit\r\n\r\n* DOC: Update release note to new message\r\n\r\n* BUG: Fix refcounting and simplify arange\r\n\r\n* MAINT: Use SETREF to make arange dtype discovery more compact\r\n\r\n* MAINT: Update numpy/core/src/multiarray/ctors.c\r\n\r\nCo-authored-by: Ross Barnowski <rossbar@berkeley.edu>\r\nCo-authored-by: Sebastian Berg <sebastianb@nvidia.com>\r\nCo-authored-by: Charles Harris <charlesr.harris@gmail.com>",
            "additions": 56,
            "deletions": 0,
            "change_type": "MODIFY",
            "diff": "@@ -9,6 +9,7 @@\n import ctypes\n import os\n import gc\n+import re\n import weakref\n import pytest\n from contextlib import contextmanager\n@@ -9324,6 +9325,61 @@ def test_start_stop_kwarg(self):\n         assert len(keyword_start_stop) == 6\n         assert_array_equal(keyword_stop, keyword_zerotostop)\n \n+    def test_arange_booleans(self):\n+        # Arange makes some sense for booleans and works up to length 2.\n+        # But it is weird since `arange(2, 4, dtype=bool)` works.\n+        # Arguably, much or all of this could be deprecated/removed.\n+        res = np.arange(False, dtype=bool)\n+        assert_array_equal(res, np.array([], dtype=\"bool\"))\n+\n+        res = np.arange(True, dtype=\"bool\")\n+        assert_array_equal(res, [False])\n+\n+        res = np.arange(2, dtype=\"bool\")\n+        assert_array_equal(res, [False, True])\n+\n+        # This case is especially weird, but drops out without special case:\n+        res = np.arange(6, 8, dtype=\"bool\")\n+        assert_array_equal(res, [True, True])\n+\n+        with pytest.raises(TypeError):\n+            np.arange(3, dtype=\"bool\")\n+\n+    @pytest.mark.parametrize(\"dtype\", [\"S3\", \"U\", \"5i\"])\n+    def test_rejects_bad_dtypes(self, dtype):\n+        dtype = np.dtype(dtype)\n+        DType_name = re.escape(str(type(dtype)))\n+        with pytest.raises(TypeError,\n+                match=rf\"arange\\(\\) not supported for inputs .* {DType_name}\"):\n+            np.arange(2, dtype=dtype)\n+\n+    def test_rejects_strings(self):\n+        # Explicitly test error for strings which may call \"b\" - \"a\":\n+        DType_name = re.escape(str(type(np.array(\"a\").dtype)))\n+        with pytest.raises(TypeError,\n+                match=rf\"arange\\(\\) not supported for inputs .* {DType_name}\"):\n+            np.arange(\"a\", \"b\")\n+\n+    def test_byteswapped(self):\n+        res_be = np.arange(1, 1000, dtype=\">i4\")\n+        res_le = np.arange(1, 1000, dtype=\"<i4\")\n+        assert res_be.dtype == \">i4\"\n+        assert res_le.dtype == \"<i4\"\n+        assert_array_equal(res_le, res_be)\n+\n+    @pytest.mark.parametrize(\"which\", [0, 1, 2])\n+    def test_error_paths_and_promotion(self, which):\n+        args = [0, 1, 2]  # start, stop, and step\n+        args[which] = np.float64(2.)  # should ensure float64 output\n+\n+        assert np.arange(*args).dtype == np.float64\n+\n+        # Cover stranger error path, test only to achieve code coverage!\n+        args[which] = [None, []]\n+        with pytest.raises(ValueError):\n+            # Fails discovering start dtype\n+            np.arange(*args)\n+\n \n class TestArrayFinalize:\n     \"\"\" Tests __array_finalize__ \"\"\"\n",
            "comment_added_diff": {
                "9329": "        # Arange makes some sense for booleans and works up to length 2.",
                "9330": "        # But it is weird since `arange(2, 4, dtype=bool)` works.",
                "9331": "        # Arguably, much or all of this could be deprecated/removed.",
                "9341": "        # This case is especially weird, but drops out without special case:",
                "9357": "        # Explicitly test error for strings which may call \"b\" - \"a\":",
                "9372": "        args = [0, 1, 2]  # start, stop, and step",
                "9373": "        args[which] = np.float64(2.)  # should ensure float64 output",
                "9377": "        # Cover stranger error path, test only to achieve code coverage!",
                "9380": "            # Fails discovering start dtype"
            },
            "comment_deleted_diff": {},
            "comment_modified_diff": {}
        },
        {
            "commit": "680a6f29bb5887db6b0931e9a7e041e8f74b9cc3",
            "timestamp": "2022-12-01T16:28:32+01:00",
            "author": "Sebastian Berg",
            "commit_message": "TST: Add test to cover basic subclass support of new code paths",
            "additions": 19,
            "deletions": 7,
            "change_type": "MODIFY",
            "diff": "@@ -9494,16 +9494,28 @@ class MyAlwaysEqualNew(MyAlwaysEqual):\n \n \n @pytest.mark.parametrize(\"op\", [operator.eq, operator.ne])\n-@ptest.mark.parametrize(\"dtype\", [\"i,i\", \"M8\", \"d\"])\n-def test_equal_subclass_no_override(op, dtype):\n+@pytest.mark.parametrize([\"dt1\", \"dt2\"], [\n+        ([(\"f\", \"i\")], [(\"f\", \"i\")]),  # structured comparison (successfull)\n+        (\"M8\", \"d\"),  # impossible comparison: result is all True or False\n+        (\"d\", \"d\"),  # valid comparison\n+        ])\n+def test_equal_subclass_no_override(op, dt1, dt2):\n+    # Test how the three different possible code-paths deal with subclasses\n+\n     class MyArr(np.ndarray):\n-        pass\n+        called_wrap = 0\n+\n+        def __array_wrap__(self, new):\n+            type(self).called_wrap += 1\n+            return super().__array_wrap__(new)\n \n-    numpy_arr = np.arange(5)\n-    my_arr = np.zeros(5, dtype=dtype).view(MyArr)\n+    numpy_arr = np.zeros(5, dtype=dt1)\n+    my_arr = np.zeros(5, dtype=dt2).view(MyArr)\n \n-    assert op(arr1, arr2).type is MyArry\n-    assert op(arr2, arr2).type is MyArry\n+    assert type(op(numpy_arr, my_arr)) is MyArr\n+    assert type(op(my_arr, numpy_arr)) is MyArr\n+    # We expect 2 calls (more if there were more fields):\n+    assert MyArr.called_wrap == 2\n \n \n @pytest.mark.parametrize(\n",
            "comment_added_diff": {
                "9498": "        ([(\"f\", \"i\")], [(\"f\", \"i\")]),  # structured comparison (successfull)",
                "9499": "        (\"M8\", \"d\"),  # impossible comparison: result is all True or False",
                "9500": "        (\"d\", \"d\"),  # valid comparison",
                "9503": "    # Test how the three different possible code-paths deal with subclasses",
                "9517": "    # We expect 2 calls (more if there were more fields):"
            },
            "comment_deleted_diff": {},
            "comment_modified_diff": {
                "9498": "def test_equal_subclass_no_override(op, dtype):",
                "9500": "        pass",
                "9503": "    my_arr = np.zeros(5, dtype=dtype).view(MyArr)"
            }
        },
        {
            "commit": "84b428e053021466dc52426cc7172db84ab19d0d",
            "timestamp": "2022-12-01T17:08:10+01:00",
            "author": "Sebastian Berg",
            "commit_message": "API: Always reject equal/not_equal for datetime/timedelta mix\n\nThis allows correctly cathing the error, also adjust the the NoLoop\nerror is used for the the \"binary no loop error\".\nFor now, I think I may want to keep the casting error distinct.",
            "additions": 28,
            "deletions": 0,
            "change_type": "MODIFY",
            "diff": "@@ -9518,6 +9518,34 @@ def __array_wrap__(self, new):\n     assert MyArr.called_wrap == 2\n \n \n+@pytest.mark.parametrize([\"dt1\", \"dt2\"], [\n+        (\"M8[ns]\", \"d\"),\n+        # Note: timedelta currently promotes (and always did) :(\n+        #       so it does not line in here.\n+        (\"M8[s]\", \"m8[s]\"),\n+        (\"S5\", \"U5\"),\n+        # Structured/void dtypes have explicit paths not tested here.\n+])\n+def test_no_loop_gives_all_true_or_false(dt1, dt2):\n+    # Make sure they broadcast to test result shape, use random values, since\n+    # the actual value should be ignored\n+    arr1 = np.random.randint(5, size=100).astype(dt1)\n+    arr2 = np.random.randint(5, size=99)[:, None].astype(dt2)\n+\n+    res = arr1 == arr2\n+    assert res.shape == (99, 100)\n+    assert res.dtype == bool\n+    assert not res.any()\n+\n+    res = arr1 != arr2\n+    assert res.shape == (99, 100)\n+    assert res.dtype == bool\n+    assert res.all()\n+\n+    with pytest.raises(np.core._exceptions._UFuncNoLoopError):\n+        arr1 > arr2\n+\n+\n @pytest.mark.parametrize(\n     [\"fun\", \"npfun\"],\n     [\n",
            "comment_added_diff": {
                "9523": "        # Note: timedelta currently promotes (and always did) :(",
                "9524": "        #       so it does not line in here.",
                "9527": "        # Structured/void dtypes have explicit paths not tested here.",
                "9530": "    # Make sure they broadcast to test result shape, use random values, since",
                "9531": "    # the actual value should be ignored"
            },
            "comment_deleted_diff": {},
            "comment_modified_diff": {}
        },
        {
            "commit": "4929777626bd7263141228de23b32c17dfbfd2b6",
            "timestamp": "2022-12-01T17:58:04+01:00",
            "author": "Sebastian Berg",
            "commit_message": "TST: Remove old deprecation test and convert/add new ones",
            "additions": 52,
            "deletions": 0,
            "change_type": "MODIFY",
            "diff": "@@ -1271,6 +1271,13 @@ def test_empty_structured_array_comparison(self):\n         a = np.zeros((1, 0, 1), [('a', '<f8', (1, 1))])\n         assert_equal(a, a)\n \n+    @pytest.mark.parametrize(\"op\", [operator.eq, operator.ne])\n+    def test_structured_array_comparison_bad_broadcasts(self, op):\n+        a = np.zeros(3, dtype='i,i')\n+        b = np.array([], dtype=\"i,i\")\n+        with pytest.raises(ValueError):\n+            op(a, b)\n+\n     def test_structured_comparisons_with_promotion(self):\n         # Check that structured arrays can be compared so long as their\n         # dtypes promote fine:\n@@ -9542,10 +9549,55 @@ def test_no_loop_gives_all_true_or_false(dt1, dt2):\n     assert res.dtype == bool\n     assert res.all()\n \n+    # incompatible shapes raise though\n+    arr2 = np.random.randint(5, size=99).astype(dt2)\n+    with pytest.raises(ValueError):\n+        arr1 == arr2\n+\n+    with pytest.raises(ValueError):\n+        arr1 != arr2\n+\n+    # Basic test with another operation:\n     with pytest.raises(np.core._exceptions._UFuncNoLoopError):\n         arr1 > arr2\n \n \n+@pytest.mark.parametrize(\"op\", [\n+        operator.eq, operator.ne, operator.le, operator.lt, operator.ge,\n+        operator.gt])\n+def test_comparisons_forwards_error(op):\n+    class NotArray:\n+        def __array__(self):\n+            raise TypeError(\"run you fools\")\n+\n+    with pytest.raises(TypeError, match=\"run you fools\"):\n+        op(np.arange(2), NotArray())\n+\n+    with pytest.raises(TypeError, match=\"run you fools\"):\n+        op(NotArray(), np.arange(2))\n+\n+\n+def test_richcompare_scalar_boolean_singleton_return():\n+    # These are currently guaranteed to be the boolean singletons, but maybe\n+    # returning NumPy booleans would also be OK:\n+    assert (np.array(0) == \"a\") is False\n+    assert (np.array(0) != \"a\") is True\n+    assert (np.int16(0) == \"a\") is False\n+    assert (np.int16(0) != \"a\") is True\n+\n+\n+@pytest.mark.parametrize(\"op\", [\n+        operator.eq, operator.ne, operator.le, operator.lt, operator.ge,\n+        operator.gt])\n+def test_ragged_comparison_fails(op):\n+    # This needs to convert the internal array to True/False, which fails:\n+    a = np.array([1, np.array([1, 2, 3])], dtype=object)\n+    b = np.array([1, np.array([1, 2, 3])], dtype=object)\n+\n+    with pytest.raises(ValueError, match=\"The truth value.*ambiguous\"):\n+        op(a, b)\n+\n+\n @pytest.mark.parametrize(\n     [\"fun\", \"npfun\"],\n     [\n",
            "comment_added_diff": {
                "9552": "    # incompatible shapes raise though",
                "9560": "    # Basic test with another operation:",
                "9581": "    # These are currently guaranteed to be the boolean singletons, but maybe",
                "9582": "    # returning NumPy booleans would also be OK:",
                "9593": "    # This needs to convert the internal array to True/False, which fails:"
            },
            "comment_deleted_diff": {},
            "comment_modified_diff": {}
        },
        {
            "commit": "e0ed8ceae87bcb6ac3ad9190ec409b3ed631429a",
            "timestamp": "2022-12-02T00:29:32+01:00",
            "author": "Bas van Beek",
            "commit_message": "TST: Add tests for inplace matrix multiplication",
            "additions": 28,
            "deletions": 11,
            "change_type": "MODIFY",
            "diff": "@@ -1,3 +1,5 @@\n+from __future__ import annotations\n+\n import collections.abc\n import tempfile\n import sys\n@@ -3693,7 +3695,7 @@ def test_ufunc_binop_interaction(self):\n             'and':      (np.bitwise_and, True, int),\n             'xor':      (np.bitwise_xor, True, int),\n             'or':       (np.bitwise_or, True, int),\n-            'matmul':   (np.matmul, False, float),\n+            'matmul':   (np.matmul, True, float),\n             # 'ge':       (np.less_equal, False),\n             # 'gt':       (np.less, False),\n             # 'le':       (np.greater_equal, False),\n@@ -7155,16 +7157,31 @@ def test_matmul_raises(self):\n         assert_raises(TypeError, self.matmul, np.void(b'abc'), np.void(b'abc'))\n         assert_raises(TypeError, self.matmul, np.arange(10), np.void(b'abc'))\n \n-def test_matmul_inplace():\n-    # It would be nice to support in-place matmul eventually, but for now\n-    # we don't have a working implementation, so better just to error out\n-    # and nudge people to writing \"a = a @ b\".\n-    a = np.eye(3)\n-    b = np.eye(3)\n-    assert_raises(TypeError, a.__imatmul__, b)\n-    import operator\n-    assert_raises(TypeError, operator.imatmul, a, b)\n-    assert_raises(TypeError, exec, \"a @= b\", globals(), locals())\n+\n+class TestMatmulInplace:\n+    DTYPES = {}\n+    for i in MatmulCommon.types:\n+        for j in MatmulCommon.types:\n+            if np.can_cast(j, i):\n+                DTYPES[f\"{i}-{j}\"] = (np.dtype(i), np.dtype(j))\n+\n+    @pytest.mark.parametrize(\"dtype1,dtype2\", DTYPES.values(), ids=DTYPES)\n+    def test_matmul_inplace(self, dtype1: np.dtype, dtype2: np.dtype) -> None:\n+        a = np.arange(10).reshape(5, 2).astype(dtype1)\n+        a_id = id(a)\n+        b = np.ones((2, 2), dtype=dtype2)\n+\n+        ref = a @ b\n+        a @= b\n+\n+        assert id(a) == a_id\n+        assert a.dtype == dtype1\n+        assert a.shape == (5, 2)\n+        if dtype1.kind in \"fc\":\n+            np.testing.assert_allclose(a, ref)\n+        else:\n+            np.testing.assert_array_equal(a, ref)\n+\n \n def test_matmul_axes():\n     a = np.arange(3*4*5).reshape(3, 4, 5)\n",
            "comment_added_diff": {},
            "comment_deleted_diff": {
                "7159": "    # It would be nice to support in-place matmul eventually, but for now",
                "7160": "    # we don't have a working implementation, so better just to error out",
                "7161": "    # and nudge people to writing \"a = a @ b\"."
            },
            "comment_modified_diff": {}
        },
        {
            "commit": "fd30908f5c82447c96190ef74443ebda34fd52db",
            "timestamp": "2022-12-02T11:59:17+01:00",
            "author": "Sebastian Berg",
            "commit_message": "DOC,TST: address review comments and improve test coverage\n\nCo-authored-by: Marten van Kerkwijk <mhvk@astro.utoronto.ca>",
            "additions": 11,
            "deletions": 7,
            "change_type": "MODIFY",
            "diff": "@@ -1298,7 +1298,10 @@ def test_structured_comparisons_with_promotion(self):\n         assert_equal(a == b, [False, True])\n         assert_equal(a != b, [True, False])\n \n-    def test_void_comparison_failures(self):\n+    @pytest.mark.parametrize(\"op\", [\n+            operator.eq, lambda x, y: operator.eq(y, x),\n+            operator.ne, lambda x, y: operator.ne(y, x)])\n+    def test_void_comparison_failures(self, op):\n         # In principle, one could decide to return an array of False for some\n         # if comparisons are impossible.  But right now we return TypeError\n         # when \"void\" dtype are involved.\n@@ -1306,18 +1309,18 @@ def test_void_comparison_failures(self):\n         y = np.zeros(3)\n         # Cannot compare non-structured to structured:\n         with pytest.raises(TypeError):\n-            x == y\n+            op(x, y)\n \n         # Added title prevents promotion, but casts are OK:\n         y = np.zeros(3, dtype=[(('title', 'a'), 'i1')])\n         assert np.can_cast(y.dtype, x.dtype)\n         with pytest.raises(TypeError):\n-            x == y\n+            op(x, y)\n \n         x = np.zeros(3, dtype=\"V7\")\n         y = np.zeros(3, dtype=\"V8\")\n         with pytest.raises(TypeError):\n-            x == y\n+            op(x, y)\n \n     def test_casting(self):\n         # Check that casting a structured array to change its byte order\n@@ -9527,8 +9530,9 @@ def __array_wrap__(self, new):\n \n @pytest.mark.parametrize([\"dt1\", \"dt2\"], [\n         (\"M8[ns]\", \"d\"),\n-        # Note: timedelta currently promotes (and always did) :(\n-        #       so it does not line in here.\n+        (\"M8[s]\", \"l\"),\n+        (\"m8[ns]\", \"d\"),\n+        # Missing: (\"m8[ns]\", \"l\") as timedelta currently promotes ints\n         (\"M8[s]\", \"m8[s]\"),\n         (\"S5\", \"U5\"),\n         # Structured/void dtypes have explicit paths not tested here.\n@@ -9537,7 +9541,7 @@ def test_no_loop_gives_all_true_or_false(dt1, dt2):\n     # Make sure they broadcast to test result shape, use random values, since\n     # the actual value should be ignored\n     arr1 = np.random.randint(5, size=100).astype(dt1)\n-    arr2 = np.random.randint(5, size=99)[:, None].astype(dt2)\n+    arr2 = np.random.randint(5, size=99)[:, np.newaxis].astype(dt2)\n \n     res = arr1 == arr2\n     assert res.shape == (99, 100)\n",
            "comment_added_diff": {
                "9535": "        # Missing: (\"m8[ns]\", \"l\") as timedelta currently promotes ints"
            },
            "comment_deleted_diff": {
                "9530": "        # Note: timedelta currently promotes (and always did) :(",
                "9531": "        #       so it does not line in here."
            },
            "comment_modified_diff": {}
        },
        {
            "commit": "5ed87cb14c121c03b9b49f086092d4c83b83a734",
            "timestamp": "2023-01-08T20:17:24+01:00",
            "author": "Panagiotis Zestanakis",
            "commit_message": "Bug: Fix fill violating read-only flag. (#22959)\n\nPyArray_FillWithScalar checks if destination is writeable before attempting to fill it.  A relevant test is added as a method of TestRegression\r\n\r\nCloses gh-22922\r\n\r\nCo-authored-by: Sebastian Berg <sebastian@sipsolutions.net>",
            "additions": 7,
            "deletions": 0,
            "change_type": "MODIFY",
            "diff": "@@ -403,6 +403,13 @@ def test_fill_struct_array(self):\n         assert_array_equal(x['a'], [3.5, 3.5])\n         assert_array_equal(x['b'], [-2, -2])\n \n+    def test_fill_readonly(self):\n+        # gh-22922\n+        a = np.zeros(11)\n+        a.setflags(write=False)\n+        with pytest.raises(ValueError, match=\".*read-only\"):\n+            a.fill(0)\n+\n \n class TestArrayConstruction:\n     def test_array(self):\n",
            "comment_added_diff": {
                "407": "        # gh-22922"
            },
            "comment_deleted_diff": {},
            "comment_modified_diff": {}
        },
        {
            "commit": "73aa5ea217818b93631cdf61ae0530b75e27303e",
            "timestamp": "2023-01-30T13:43:14-08:00",
            "author": "Raghuveer Devulapalli",
            "commit_message": "TST: Add quicksort test coverage for all 16, 32, 64 bit dtypes",
            "additions": 14,
            "deletions": 14,
            "change_type": "MODIFY",
            "diff": "@@ -9858,39 +9858,39 @@ def test_non_c_contiguous(self):\n \n \n # Test various array sizes that hit different code paths in quicksort-avx512\n-@pytest.mark.parametrize(\"N\", [8, 16, 24, 32, 48, 64, 96, 128, 151, 191,\n-                               256, 383, 512, 1023, 2047])\n-def test_sort_float(N):\n+@pytest.mark.parametrize(\"N\", np.arange(1,512))\n+@pytest.mark.parametrize(\"dtype\", ['e', 'f', 'd'])\n+def test_sort_float(N, dtype):\n     # Regular data with nan sprinkled\n     np.random.seed(42)\n-    arr = -0.5 + np.random.sample(N).astype('f')\n+    arr = -0.5 + np.random.sample(N).astype(dtype)\n     arr[np.random.choice(arr.shape[0], 3)] = np.nan\n     assert_equal(np.sort(arr, kind='quick'), np.sort(arr, kind='heap'))\n \n     # (2) with +INF\n-    infarr = np.inf*np.ones(N, dtype='f')\n+    infarr = np.inf*np.ones(N, dtype=dtype)\n     infarr[np.random.choice(infarr.shape[0], 5)] = -1.0\n     assert_equal(np.sort(infarr, kind='quick'), np.sort(infarr, kind='heap'))\n \n     # (3) with -INF\n-    neginfarr = -np.inf*np.ones(N, dtype='f')\n+    neginfarr = -np.inf*np.ones(N, dtype=dtype)\n     neginfarr[np.random.choice(neginfarr.shape[0], 5)] = 1.0\n     assert_equal(np.sort(neginfarr, kind='quick'),\n                  np.sort(neginfarr, kind='heap'))\n \n     # (4) with +/-INF\n-    infarr = np.inf*np.ones(N, dtype='f')\n+    infarr = np.inf*np.ones(N, dtype=dtype)\n     infarr[np.random.choice(infarr.shape[0], (int)(N/2))] = -np.inf\n     assert_equal(np.sort(infarr, kind='quick'), np.sort(infarr, kind='heap'))\n \n \n-def test_sort_int():\n-    # Random data with NPY_MAX_INT32 and NPY_MIN_INT32 sprinkled\n-    rng = np.random.default_rng(42)\n-    N = 2047\n-    minv = np.iinfo(np.int32).min\n-    maxv = np.iinfo(np.int32).max\n-    arr = rng.integers(low=minv, high=maxv, size=N).astype('int32')\n+@pytest.mark.parametrize(\"N\", np.arange(1,512))\n+@pytest.mark.parametrize(\"dtype\", ['h', 'H', 'i', 'I', 'l', 'L'])\n+def test_sort_int(N, dtype):\n+    # Random data with MAX and MIN sprinkled\n+    minv = np.iinfo(dtype).min\n+    maxv = np.iinfo(dtype).max\n+    arr = np.random.randint(low=minv, high=maxv-1, size=N, dtype=dtype)\n     arr[np.random.choice(arr.shape[0], 10)] = minv\n     arr[np.random.choice(arr.shape[0], 10)] = maxv\n     assert_equal(np.sort(arr, kind='quick'), np.sort(arr, kind='heap'))\n",
            "comment_added_diff": {
                "9890": "    # Random data with MAX and MIN sprinkled"
            },
            "comment_deleted_diff": {
                "9888": "    # Random data with NPY_MAX_INT32 and NPY_MIN_INT32 sprinkled"
            },
            "comment_modified_diff": {
                "9890": "    N = 2047"
            }
        },
        {
            "commit": "e1e487acf1d820cbab8a6f97986bf2fb451dfa8e",
            "timestamp": "2023-02-11T22:46:28+01:00",
            "author": "Dimitri Papadopoulos",
            "commit_message": "Fix typos found by copdespell",
            "additions": 1,
            "deletions": 1,
            "change_type": "MODIFY",
            "diff": "@@ -9534,7 +9534,7 @@ class MyAlwaysEqualNew(MyAlwaysEqual):\n \n @pytest.mark.parametrize(\"op\", [operator.eq, operator.ne])\n @pytest.mark.parametrize([\"dt1\", \"dt2\"], [\n-        ([(\"f\", \"i\")], [(\"f\", \"i\")]),  # structured comparison (successfull)\n+        ([(\"f\", \"i\")], [(\"f\", \"i\")]),  # structured comparison (successful)\n         (\"M8\", \"d\"),  # impossible comparison: result is all True or False\n         (\"d\", \"d\"),  # valid comparison\n         ])\n",
            "comment_added_diff": {
                "9537": "        ([(\"f\", \"i\")], [(\"f\", \"i\")]),  # structured comparison (successful)"
            },
            "comment_deleted_diff": {
                "9537": "        ([(\"f\", \"i\")], [(\"f\", \"i\")]),  # structured comparison (successfull)"
            },
            "comment_modified_diff": {
                "9537": "        ([(\"f\", \"i\")], [(\"f\", \"i\")]),  # structured comparison (successfull)"
            }
        },
        {
            "commit": "6ef6233d5493dc2e18232304e1c3b8b0ae6f27c8",
            "timestamp": "2023-02-15T12:35:30+02:00",
            "author": "mattip",
            "commit_message": "TEST: remove very slow test, add it as a comment to the code snippet it tests",
            "additions": 0,
            "deletions": 27,
            "change_type": "MODIFY",
            "diff": "@@ -5552,33 +5552,6 @@ def test_binary(self, tmp_filename):\n             tmp_filename,\n             dtype='<f4')\n \n-    @pytest.mark.slow  # takes > 1 minute on mechanical hard drive\n-    def test_big_binary(self):\n-        \"\"\"Test workarounds for 32-bit limit for MSVC fwrite, fseek, and ftell\n-\n-        These normally would hang doing something like this.\n-        See : https://github.com/numpy/numpy/issues/2256\n-        \"\"\"\n-        if sys.platform != 'win32' or '[GCC ' in sys.version:\n-            return\n-        try:\n-            # before workarounds, only up to 2**32-1 worked\n-            fourgbplus = 2**32 + 2**16\n-            testbytes = np.arange(8, dtype=np.int8)\n-            n = len(testbytes)\n-            flike = tempfile.NamedTemporaryFile()\n-            f = flike.file\n-            np.tile(testbytes, fourgbplus // testbytes.nbytes).tofile(f)\n-            flike.seek(0)\n-            a = np.fromfile(f, dtype=np.int8)\n-            flike.close()\n-            assert_(len(a) == fourgbplus)\n-            # check only start and end for speed:\n-            assert_((a[:n] == testbytes).all())\n-            assert_((a[-n:] == testbytes).all())\n-        except (MemoryError, ValueError):\n-            pass\n-\n     def test_string(self, tmp_filename):\n         self._check_from(b'1,2,3,4', [1., 2., 3., 4.], tmp_filename, sep=',')\n \n",
            "comment_added_diff": {},
            "comment_deleted_diff": {
                "5555": "    @pytest.mark.slow  # takes > 1 minute on mechanical hard drive",
                "5565": "            # before workarounds, only up to 2**32-1 worked",
                "5576": "            # check only start and end for speed:"
            },
            "comment_modified_diff": {}
        },
        {
            "commit": "ea55ffa2bded2888167369df72f66132b4ae171e",
            "timestamp": "2023-06-05T10:38:01+02:00",
            "author": "Raghuveer Devulapalli",
            "commit_message": "TST: Add tests for np.argsort (#23846)\n\nContributing a few tests I had used when developing AVX-512 based argsort.\r\n\r\nCo-authored-by: Robert Kern <robert.kern@gmail.com>",
            "additions": 42,
            "deletions": 0,
            "change_type": "MODIFY",
            "diff": "@@ -41,6 +41,12 @@\n from datetime import timedelta, datetime\n \n \n+def assert_arg_sorted(arr, arg):\n+    # resulting array should be sorted and arg values should be unique\n+    assert_equal(arr[arg], np.sort(arr))\n+    assert_equal(np.sort(arg), np.arange(len(arg)))\n+\n+\n def _aligned_zeros(shape, dtype=float, order=\"C\", align=None):\n     \"\"\"\n     Allocate a new ndarray with aligned memory.\n@@ -9989,3 +9995,39 @@ def test_sort_uint():\n \n def test_private_get_ndarray_c_version():\n     assert isinstance(_get_ndarray_c_version(), int)\n+\n+\n+@pytest.mark.parametrize(\"N\", np.arange(1, 512))\n+@pytest.mark.parametrize(\"dtype\", [np.float32, np.float64])\n+def test_argsort_float(N, dtype):\n+    rnd = np.random.RandomState(116112)\n+    # (1) Regular data with a few nan: doesn't use vectorized sort\n+    arr = -0.5 + rnd.random(N).astype(dtype)\n+    arr[rnd.choice(arr.shape[0], 3)] = np.nan\n+    assert_arg_sorted(arr, np.argsort(arr, kind='quick'))\n+\n+    # (2) Random data with inf at the end of array\n+    # See: https://github.com/intel/x86-simd-sort/pull/39\n+    arr = -0.5 + rnd.rand(N).astype(dtype)\n+    arr[N-1] = np.inf\n+    assert_arg_sorted(arr, np.argsort(arr, kind='quick'))\n+\n+\n+@pytest.mark.parametrize(\"N\", np.arange(2, 512))\n+@pytest.mark.parametrize(\"dtype\", [np.int32, np.uint32, np.int64, np.uint64])\n+def test_argsort_int(N, dtype):\n+    rnd = np.random.RandomState(1100710816)\n+    # (1) random data with min and max values\n+    minv = np.iinfo(dtype).min\n+    maxv = np.iinfo(dtype).max\n+    arr = rnd.randint(low=minv, high=maxv, size=N, dtype=dtype)\n+    i, j = rnd.choice(N, 2, replace=False)\n+    arr[i] = minv\n+    arr[j] = maxv\n+    assert_arg_sorted(arr, np.argsort(arr, kind='quick'))\n+\n+    # (2) random data with max value at the end of array\n+    # See: https://github.com/intel/x86-simd-sort/pull/39\n+    arr = rnd.randint(low=minv, high=maxv, size=N, dtype=dtype)\n+    arr[N-1] = maxv\n+    assert_arg_sorted(arr, np.argsort(arr, kind='quick'))\n",
            "comment_added_diff": {
                "45": "    # resulting array should be sorted and arg values should be unique",
                "10004": "    # (1) Regular data with a few nan: doesn't use vectorized sort",
                "10009": "    # (2) Random data with inf at the end of array",
                "10010": "    # See: https://github.com/intel/x86-simd-sort/pull/39",
                "10020": "    # (1) random data with min and max values",
                "10029": "    # (2) random data with max value at the end of array",
                "10030": "    # See: https://github.com/intel/x86-simd-sort/pull/39"
            },
            "comment_deleted_diff": {},
            "comment_modified_diff": {}
        },
        {
            "commit": "35d23ba8b2e77a2ca75d4df5e10f1016fd8c1c75",
            "timestamp": "2023-06-05T12:48:16-06:00",
            "author": "Raghuveer Devulapalli",
            "commit_message": "TST: Add tests for np.argsort (#23846)\n\nContributing a few tests I had used when developing AVX-512 based argsort.\r\n\r\nCo-authored-by: Robert Kern <robert.kern@gmail.com>",
            "additions": 42,
            "deletions": 0,
            "change_type": "MODIFY",
            "diff": "@@ -41,6 +41,12 @@\n from datetime import timedelta, datetime\n \n \n+def assert_arg_sorted(arr, arg):\n+    # resulting array should be sorted and arg values should be unique\n+    assert_equal(arr[arg], np.sort(arr))\n+    assert_equal(np.sort(arg), np.arange(len(arg)))\n+\n+\n def _aligned_zeros(shape, dtype=float, order=\"C\", align=None):\n     \"\"\"\n     Allocate a new ndarray with aligned memory.\n@@ -9989,3 +9995,39 @@ def test_sort_uint():\n \n def test_private_get_ndarray_c_version():\n     assert isinstance(_get_ndarray_c_version(), int)\n+\n+\n+@pytest.mark.parametrize(\"N\", np.arange(1, 512))\n+@pytest.mark.parametrize(\"dtype\", [np.float32, np.float64])\n+def test_argsort_float(N, dtype):\n+    rnd = np.random.RandomState(116112)\n+    # (1) Regular data with a few nan: doesn't use vectorized sort\n+    arr = -0.5 + rnd.random(N).astype(dtype)\n+    arr[rnd.choice(arr.shape[0], 3)] = np.nan\n+    assert_arg_sorted(arr, np.argsort(arr, kind='quick'))\n+\n+    # (2) Random data with inf at the end of array\n+    # See: https://github.com/intel/x86-simd-sort/pull/39\n+    arr = -0.5 + rnd.rand(N).astype(dtype)\n+    arr[N-1] = np.inf\n+    assert_arg_sorted(arr, np.argsort(arr, kind='quick'))\n+\n+\n+@pytest.mark.parametrize(\"N\", np.arange(2, 512))\n+@pytest.mark.parametrize(\"dtype\", [np.int32, np.uint32, np.int64, np.uint64])\n+def test_argsort_int(N, dtype):\n+    rnd = np.random.RandomState(1100710816)\n+    # (1) random data with min and max values\n+    minv = np.iinfo(dtype).min\n+    maxv = np.iinfo(dtype).max\n+    arr = rnd.randint(low=minv, high=maxv, size=N, dtype=dtype)\n+    i, j = rnd.choice(N, 2, replace=False)\n+    arr[i] = minv\n+    arr[j] = maxv\n+    assert_arg_sorted(arr, np.argsort(arr, kind='quick'))\n+\n+    # (2) random data with max value at the end of array\n+    # See: https://github.com/intel/x86-simd-sort/pull/39\n+    arr = rnd.randint(low=minv, high=maxv, size=N, dtype=dtype)\n+    arr[N-1] = maxv\n+    assert_arg_sorted(arr, np.argsort(arr, kind='quick'))\n",
            "comment_added_diff": {
                "45": "    # resulting array should be sorted and arg values should be unique",
                "10004": "    # (1) Regular data with a few nan: doesn't use vectorized sort",
                "10009": "    # (2) Random data with inf at the end of array",
                "10010": "    # See: https://github.com/intel/x86-simd-sort/pull/39",
                "10020": "    # (1) random data with min and max values",
                "10029": "    # (2) random data with max value at the end of array",
                "10030": "    # See: https://github.com/intel/x86-simd-sort/pull/39"
            },
            "comment_deleted_diff": {},
            "comment_modified_diff": {}
        },
        {
            "commit": "b9d0cd30b559d27b21d889a7b01ae67ea748380a",
            "timestamp": "2023-07-18T11:50:24+02:00",
            "author": "Sebastian Berg",
            "commit_message": "TST: STrengthen test on newer Python versions\n\nIt seems older Python versions rounded `str` sometimes, I do not\nthink this is strictly true anymore.\nIF this is wrong on certain systems, we need to rethink some things!",
            "additions": 1,
            "deletions": 2,
            "change_type": "MODIFY",
            "diff": "@@ -5347,10 +5347,9 @@ def test_roundtrip_str(self, x):\n         x = x.real.ravel()\n         s = \"@\".join(map(str, x))\n         y = np.fromstring(s, sep=\"@\")\n-        # NB. str imbues less precision\n         nan_mask = ~np.isfinite(x)\n         assert_array_equal(x[nan_mask], y[nan_mask])\n-        assert_array_almost_equal(x[~nan_mask], y[~nan_mask], decimal=5)\n+        assert_array_equal(x[~nan_mask], y[~nan_mask])\n \n     def test_roundtrip_repr(self, x):\n         x = x.real.ravel()\n",
            "comment_added_diff": {},
            "comment_deleted_diff": {
                "5350": "        # NB. str imbues less precision"
            },
            "comment_modified_diff": {}
        },
        {
            "commit": "94b9f2a78733d322cea8fd02c76c39beecb11aec",
            "timestamp": "2023-09-15T16:19:38+02:00",
            "author": "Ralf Gommers",
            "commit_message": "MAINT: remove relaxed strides debug build setting\n\nThis follows up on gh-21039. As noted there, this doesn't really\nserve any purpose anymore and historically seems to have resulted in\nonly 2 bug reports.",
            "additions": 1,
            "deletions": 24,
            "change_type": "MODIFY",
            "diff": "@@ -3520,8 +3520,6 @@ def test_ravel(self):\n         # 1-element tidy strides test:\n         a = np.array([[1]])\n         a.strides = (123, 432)\n-        # If the following stride is not 8, NPY_RELAXED_STRIDES_DEBUG is\n-        # messing them up on purpose:\n         if np.ones(1).strides == (8,):\n             assert_(np.may_share_memory(a.ravel('K'), a))\n             assert_equal(a.ravel('K').strides, (a.dtype.itemsize,))\n@@ -8032,9 +8030,7 @@ def test_export_record(self):\n             assert_equal(y.format, 'T{b:a:=h:b:i:c:l:d:q:dx:B:e:@H:f:=I:g:L:h:Q:hx:f:i:d:j:^g:k:=Zf:ix:Zd:jx:^Zg:kx:4s:l:=4w:m:3x:n:?:o:@e:p:}')\n         else:\n             assert_equal(y.format, 'T{b:a:=h:b:i:c:q:d:q:dx:B:e:@H:f:=I:g:Q:h:Q:hx:f:i:d:j:^g:k:=Zf:ix:Zd:jx:^Zg:kx:4s:l:=4w:m:3x:n:?:o:@e:p:}')\n-        # Cannot test if NPY_RELAXED_STRIDES_DEBUG changes the strides\n-        if not (np.ones(1).strides[0] == np.iinfo(np.intp).max):\n-            assert_equal(y.strides, (sz,))\n+        assert_equal(y.strides, (sz,))\n         assert_equal(y.itemsize, sz)\n \n     def test_export_subarray(self):\n@@ -8143,23 +8139,6 @@ def test_relaxed_strides(self, c=np.ones((1, 10, 10), dtype='i8')):\n                     arr, ['C_CONTIGUOUS'])\n             assert_(strides[-1] == 8)\n \n-    @pytest.mark.valgrind_error(reason=\"leaks buffer info cache temporarily.\")\n-    @pytest.mark.skipif(not np.ones((10, 1), order=\"C\").flags.f_contiguous,\n-            reason=\"Test is unnecessary (but fails) without relaxed strides.\")\n-    def test_relaxed_strides_buffer_info_leak(self, arr=np.ones((1, 10))):\n-        \"\"\"Test that alternating export of C- and F-order buffers from\n-        an array which is both C- and F-order when relaxed strides is\n-        active works.\n-        This test defines array in the signature to ensure leaking more\n-        references every time the test is run (catching the leak with\n-        pytest-leaks).\n-        \"\"\"\n-        for i in range(10):\n-            _, s = _multiarray_tests.get_buffer_info(arr, ['F_CONTIGUOUS'])\n-            assert s == (8, 8)\n-            _, s = _multiarray_tests.get_buffer_info(arr, ['C_CONTIGUOUS'])\n-            assert s == (80, 8)\n-\n     def test_out_of_order_fields(self):\n         dt = np.dtype(dict(\n             formats=['<i4', '<i4'],\n@@ -9788,8 +9767,6 @@ def test_uintalignment_and_alignment():\n class TestAlignment:\n     # adapted from scipy._lib.tests.test__util.test__aligned_zeros\n     # Checks that unusual memory alignments don't trip up numpy.\n-    # In particular, check RELAXED_STRIDES don't trip alignment assertions in\n-    # NDEBUG mode for size-0 arrays (gh-12503)\n \n     def check(self, shape, dtype, order, align):\n         err_msg = repr((shape, dtype, order, align))\n",
            "comment_added_diff": {},
            "comment_deleted_diff": {
                "3523": "        # If the following stride is not 8, NPY_RELAXED_STRIDES_DEBUG is",
                "3524": "        # messing them up on purpose:",
                "8035": "        # Cannot test if NPY_RELAXED_STRIDES_DEBUG changes the strides",
                "9791": "    # In particular, check RELAXED_STRIDES don't trip alignment assertions in",
                "9792": "    # NDEBUG mode for size-0 arrays (gh-12503)"
            },
            "comment_modified_diff": {}
        },
        {
            "commit": "12ae67db43880cb3f69195f743021eb771129309",
            "timestamp": "2023-09-18T10:35:05-07:00",
            "author": "Raghuveer Devulapalli",
            "commit_message": "TST: Add tests for np.partition",
            "additions": 40,
            "deletions": 0,
            "change_type": "MODIFY",
            "diff": "@@ -30,6 +30,7 @@\n     assert_array_equal, assert_raises_regex, assert_array_almost_equal,\n     assert_allclose, IS_PYPY, IS_PYSTON, HAS_REFCOUNT, assert_array_less,\n     runstring, temppath, suppress_warnings, break_cycles, _SUPPORTS_SVE,\n+    assert_array_compare,\n     )\n from numpy.testing._private.utils import requires_memory, _no_tracing\n from numpy.core.tests._locales import CommaDecimalPointLocale\n@@ -46,6 +47,12 @@ def assert_arg_sorted(arr, arg):\n     assert_equal(np.sort(arg), np.arange(len(arg)))\n \n \n+def assert_arr_partitioned(kth, k, arr_part):\n+    assert_equal(arr_part[k], kth)\n+    assert_array_compare(operator.__le__, arr_part[:k], kth)\n+    assert_array_compare(operator.__ge__, arr_part[k:], kth)\n+\n+\n def _aligned_zeros(shape, dtype=float, order=\"C\", align=None):\n     \"\"\"\n     Allocate a new ndarray with aligned memory.\n@@ -10026,3 +10033,36 @@ def test_gh_24459():\n     a = np.zeros((50, 3), dtype=np.float64)\n     with pytest.raises(TypeError):\n         np.choose(a, [3, -1])\n+\n+\n+@pytest.mark.parametrize(\"N\", np.arange(2, 512))\n+@pytest.mark.parametrize(\"dtype\", [np.int16, np.uint16,\n+                        np.int32, np.uint32, np.int64, np.uint64])\n+def test_partition_int(N, dtype):\n+    rnd = np.random.RandomState(1100710816)\n+    # (1) random data with min and max values\n+    minv = np.iinfo(dtype).min\n+    maxv = np.iinfo(dtype).max\n+    arr = rnd.randint(low=minv, high=maxv, size=N, dtype=dtype)\n+    i, j = rnd.choice(N, 2, replace=False)\n+    arr[i] = minv\n+    arr[j] = maxv\n+    k = rnd.choice(N, 1)[0]\n+    assert_arr_partitioned(np.sort(arr)[k], k,\n+            np.partition(arr, k, kind='introselect'))\n+\n+    # (2) random data with max value at the end of array\n+    arr = rnd.randint(low=minv, high=maxv, size=N, dtype=dtype)\n+    arr[N-1] = maxv\n+    assert_arr_partitioned(np.sort(arr)[k], k,\n+            np.partition(arr, k, kind='introselect'))\n+\n+\n+@pytest.mark.parametrize(\"N\", np.arange(2, 512))\n+@pytest.mark.parametrize(\"dtype\", [np.float16, np.float32, np.float64])\n+def test_partition_fp(N, dtype):\n+    rnd = np.random.RandomState(1100710816)\n+    arr = -0.5 + rnd.random(N).astype(dtype)\n+    k = rnd.choice(N, 1)[0]\n+    assert_arr_partitioned(np.sort(arr)[k], k,\n+            np.partition(arr, k, kind='introselect'))\n",
            "comment_added_diff": {
                "10043": "    # (1) random data with min and max values",
                "10054": "    # (2) random data with max value at the end of array"
            },
            "comment_deleted_diff": {},
            "comment_modified_diff": {}
        },
        {
            "commit": "395cff66c045755bb3dd740b62b8902c1de9138d",
            "timestamp": "2023-10-04T17:19:23-06:00",
            "author": "Charles Harris",
            "commit_message": "MAINT: Xfail test failing on PyPy.\n\nSkip a test that fails on pypy3.9-v1.3.13 and pypy3.10-v1.3.13,\nreleased 9-29-2023. This may be a bit harsh, but it is hard to\nmake it more specific.\n\nIn reference to #24862.",
            "additions": 1,
            "deletions": 0,
            "change_type": "MODIFY",
            "diff": "@@ -3729,6 +3729,7 @@ def test_inplace(self):\n     #   - defer if other has __array_ufunc__ and it is None\n     #           or other is not a subclass and has higher array priority\n     #   - else, call ufunc\n+    @pytest.mark.xfail(IS_PYPY, reason=\"Bug in pypy3.{9, 10}-v7.3.13, #24862\")\n     def test_ufunc_binop_interaction(self):\n         # Python method name (without underscores)\n         #   -> (numpy ufunc, has_in_place_version, preferred_dtype)\n",
            "comment_added_diff": {
                "3732": "    @pytest.mark.xfail(IS_PYPY, reason=\"Bug in pypy3.{9, 10}-v7.3.13, #24862\")"
            },
            "comment_deleted_diff": {},
            "comment_modified_diff": {}
        },
        {
            "commit": "040ed2dc9847265c581a342301dd87d2b518a3c2",
            "timestamp": "2023-10-07T13:40:49-06:00",
            "author": "Ralf Gommers",
            "commit_message": "BLD: fix math func feature checks, fix FreeBSD build, add CI job (#24876)\n\n* BLD: fix incorrect feature checks for mandatory math functions\r\n\r\nShould fix the build on FreeBSD and other OSes that are not\r\nC99-compliant.\r\n\r\nCloses gh-24873\r\n\r\n* CI: add a FreeBSD job on Cirrus CI\r\n\r\n* BUG: define `_npy_scaled_cexpl` when ccoshl/csinhl are missing\r\n\r\nThis was a regression in the 1.24.x branch, after a lot of churn\r\nin this file. In 1.22.x/1.23.x, the conditional is the same as in\r\nthis fix.\r\n\r\n* TST: avoid failures for FPE errors/warnings in `abs` on BSDs",
            "additions": 6,
            "deletions": 3,
            "change_type": "MODIFY",
            "diff": "@@ -9706,9 +9706,12 @@ def test_ragged_comparison_fails(op):\n def test_npymath_complex(fun, npfun, x, y, test_dtype):\n     # Smoketest npymath functions\n     z = test_dtype(complex(x, y))\n-    got = fun(z)\n-    expected = npfun(z)\n-    assert_allclose(got, expected)\n+    with np.errstate(invalid='ignore'):\n+        # Fallback implementations may emit a warning for +-inf (see gh-24876):\n+        #     RuntimeWarning: invalid value encountered in absolute\n+        got = fun(z)\n+        expected = npfun(z)\n+        assert_allclose(got, expected)\n \n \n def test_npymath_real():\n",
            "comment_added_diff": {
                "9710": "        # Fallback implementations may emit a warning for +-inf (see gh-24876):",
                "9711": "        #     RuntimeWarning: invalid value encountered in absolute"
            },
            "comment_deleted_diff": {},
            "comment_modified_diff": {
                "9710": "    expected = npfun(z)",
                "9711": "    assert_allclose(got, expected)"
            }
        }
    ],
    "test_overrides.py": [
        {
            "commit": "4d19f55dd94eb16e02a605ce963d02c51411057d",
            "timestamp": "2022-10-11T13:40:27+02:00",
            "author": "Jordy Williams",
            "commit_message": "ENH: allow explicit `like=None` in all array creation functions (#22379)\n\nAddresses #22069. As discussed in the existing issue @seberg stated that it would be best to support explicitly defined like=None when creating arange objects.\r\n\r\nExtended to all other array_creation methods which did not currently support like=None.\r\n\r\nCloses gh-22069",
            "additions": 18,
            "deletions": 0,
            "change_type": "MODIFY",
            "diff": "@@ -622,3 +622,21 @@ def test_exception_handling(self):\n         with assert_raises(TypeError):\n             # Raises the error about `value_error` being invalid first\n             np.array(1, value_error=True, like=ref)\n+\n+    @pytest.mark.parametrize('function, args, kwargs', _array_tests)\n+    def test_like_as_none(self, function, args, kwargs):\n+        self.add_method('array', self.MyArray)\n+        self.add_method(function, self.MyArray)\n+        np_func = getattr(np, function)\n+\n+        like_args = tuple(a() if callable(a) else a for a in args)\n+        # required for loadtxt and genfromtxt to init w/o error.\n+        like_args_exp = tuple(a() if callable(a) else a for a in args)\n+\n+        array_like = np_func(*like_args, **kwargs, like=None)\n+        expected = np_func(*like_args_exp, **kwargs)\n+        # Special-case np.empty to ensure values match\n+        if function == \"empty\":\n+            array_like.fill(1)\n+            expected.fill(1)\n+        assert_equal(array_like, expected)\n",
            "comment_added_diff": {
                "633": "        # required for loadtxt and genfromtxt to init w/o error.",
                "638": "        # Special-case np.empty to ensure values match"
            },
            "comment_deleted_diff": {},
            "comment_modified_diff": {}
        },
        {
            "commit": "d67baad8aab0c14ec24f516a8919bb32f58b1973",
            "timestamp": "2023-01-17T18:40:44+01:00",
            "author": "Sebastian Berg",
            "commit_message": "TST: Cover some more internal array-function code paths",
            "additions": 50,
            "deletions": 0,
            "change_type": "MODIFY",
            "diff": "@@ -394,6 +394,56 @@ def func():\n         except TypeError as exc:\n             assert exc is error  # unmodified exception\n \n+    def test_properties(self):\n+        # Check that str and repr are sensible\n+        func = dispatched_two_arg\n+        assert str(func) == str(func._implementation)\n+        repr_no_id = repr(func).split(\"at \")[0]\n+        repr_no_id_impl = repr(func._implementation).split(\"at \")[0]\n+        assert repr_no_id == repr_no_id_impl\n+\n+    @pytest.mark.parametrize(\"func\", [\n+            lambda x, y: 0,  # no like argument\n+            lambda like=None: 0,  # not keyword only\n+            lambda *, like=None, a=3: 0,  # not last (not that it matters)\n+        ])\n+    def test_bad_like_sig(self, func):\n+        # We sanity check the signature, and these should fail.\n+        with pytest.raises(RuntimeError):\n+            array_function_dispatch()(func)\n+\n+    def test_bad_like_passing(self):\n+        # Cover internal sanity check for passing like as first positional arg\n+        def func(*, like=None):\n+            pass\n+\n+        func_with_like = array_function_dispatch()(func)\n+        with pytest.raises(TypeError):\n+            func_with_like()\n+        with pytest.raises(TypeError):\n+            func_with_like(like=234)\n+\n+    def test_too_many_args(self):\n+        # Mainly a unit-test to increase coverage\n+        objs = []\n+        for i in range(40):\n+            class MyArr:\n+                def __array_function__(self, *args, **kwargs):\n+                    return NotImplemented\n+\n+            objs.append(MyArr())\n+\n+        def _dispatch(*args):\n+            return args\n+\n+        @array_function_dispatch(_dispatch)\n+        def func(*args):\n+            pass\n+\n+        with pytest.raises(TypeError, match=\"maximum number\"):\n+            func(*objs)\n+\n+\n \n class TestNDArrayMethods:\n \n",
            "comment_added_diff": {
                "398": "        # Check that str and repr are sensible",
                "406": "            lambda x, y: 0,  # no like argument",
                "407": "            lambda like=None: 0,  # not keyword only",
                "408": "            lambda *, like=None, a=3: 0,  # not last (not that it matters)",
                "411": "        # We sanity check the signature, and these should fail.",
                "416": "        # Cover internal sanity check for passing like as first positional arg",
                "427": "        # Mainly a unit-test to increase coverage"
            },
            "comment_deleted_diff": {},
            "comment_modified_diff": {}
        },
        {
            "commit": "c7b12699f4fb377920142d034140d4d2324be867",
            "timestamp": "2023-01-18T22:00:01+01:00",
            "author": "Sebastian Berg",
            "commit_message": "BUG: Impelement `ArrayFunctionDispatcher.__get__`\n\nWhile functions should not normally need this, Python functions do\nprovide it (C functions do not, but we are a fatter object anyway).\n\nBy implementing `__get__` we also ensure that `inspect.isroutine()`\npasses.  And by that we ensure that Sphinx considers these a `py:function:`\nrole.\n\nCloses gh-23032",
            "additions": 33,
            "deletions": 0,
            "change_type": "MODIFY",
            "diff": "@@ -690,3 +690,36 @@ def test_like_as_none(self, function, args, kwargs):\n             array_like.fill(1)\n             expected.fill(1)\n         assert_equal(array_like, expected)\n+\n+\n+@requires_array_function\n+def test_function_like():\n+    # We provide a `__get__` implementation, make sure it works\n+    assert type(np.mean) is np.core._multiarray_umath._ArrayFunctionDispatcher \n+\n+    class MyClass:\n+        def __array__(self):\n+            # valid argument to mean:\n+            return np.arange(3)\n+\n+        func1 = staticmethod(np.mean)\n+        func2 = np.mean\n+        func3 = classmethod(np.mean)\n+\n+    m = MyClass()\n+    assert m.func1([10]) == 10\n+    assert m.func2() == 1  # mean of the arange\n+    with pytest.raises(TypeError, match=\"unsupported operand type\"):\n+        # Tries to operate on the class\n+        m.func3()\n+\n+    # Manual binding also works (the above may shortcut):\n+    bound = np.mean.__get__(m, MyClass)\n+    assert bound() == 1\n+\n+    bound = np.mean.__get__(None, MyClass)  # unbound actually\n+    assert bound([10]) == 10\n+\n+    bound = np.mean.__get__(MyClass)  # classmethod\n+    with pytest.raises(TypeError, match=\"unsupported operand type\"):\n+        bound()\n",
            "comment_added_diff": {
                "697": "    # We provide a `__get__` implementation, make sure it works",
                "702": "            # valid argument to mean:",
                "711": "    assert m.func2() == 1  # mean of the arange",
                "713": "        # Tries to operate on the class",
                "716": "    # Manual binding also works (the above may shortcut):",
                "720": "    bound = np.mean.__get__(None, MyClass)  # unbound actually",
                "723": "    bound = np.mean.__get__(MyClass)  # classmethod"
            },
            "comment_deleted_diff": {},
            "comment_modified_diff": {}
        },
        {
            "commit": "8334d57785997b6a3f6a3bae0ffea273632c65f1",
            "timestamp": "2023-01-20T13:22:56+01:00",
            "author": "Sebastian Berg",
            "commit_message": "API: Allow SciPy to get away with assuming `trapz` is a Python function\n\nThis wraps `trapz` into a proper python function and then copies all\nattributes expected on a Python function over from the \"fake\" version\nto the real one.\n\nThis allows SciPy to pretend `trapz` is a Python function to create\ntheir own version.",
            "additions": 21,
            "deletions": 0,
            "change_type": "MODIFY",
            "diff": "@@ -723,3 +723,24 @@ def __array__(self):\n     bound = np.mean.__get__(MyClass)  # classmethod\n     with pytest.raises(TypeError, match=\"unsupported operand type\"):\n         bound()\n+\n+\n+def test_scipy_trapz_support_shim():\n+    # SciPy 1.10 and earlier \"clone\" trapz in this way, so we have a\n+    # support shim in place: https://github.com/scipy/scipy/issues/17811\n+    # That should be removed eventually.  This test copies what SciPy does.\n+    # Hopefully removable 1 year after SciPy 1.11; shim added to NumPy 1.25.\n+    import types\n+    import functools\n+\n+    def _copy_func(f):\n+        # Based on http://stackoverflow.com/a/6528148/190597 (Glenn Maynard)\n+        g = types.FunctionType(f.__code__, f.__globals__, name=f.__name__,\n+                            argdefs=f.__defaults__, closure=f.__closure__)\n+        g = functools.update_wrapper(g, f)\n+        g.__kwdefaults__ = f.__kwdefaults__\n+        return g\n+\n+    trapezoid = _copy_func(np.trapz)\n+\n+    assert np.trapz([1, 2]) == trapezoid([1, 2])\n",
            "comment_added_diff": {
                "729": "    # SciPy 1.10 and earlier \"clone\" trapz in this way, so we have a",
                "730": "    # support shim in place: https://github.com/scipy/scipy/issues/17811",
                "731": "    # That should be removed eventually.  This test copies what SciPy does.",
                "732": "    # Hopefully removable 1 year after SciPy 1.11; shim added to NumPy 1.25.",
                "737": "        # Based on http://stackoverflow.com/a/6528148/190597 (Glenn Maynard)"
            },
            "comment_deleted_diff": {},
            "comment_modified_diff": {}
        },
        {
            "commit": "61610e74340a4a22f2782274600ae34bd882b929",
            "timestamp": "2023-04-25T12:39:33+02:00",
            "author": "Sebastian Berg",
            "commit_message": "ENH: Restore TypeError cleanup in array function dispatching\n\nWhen the dispathcer raises a TypeError and it starts with the dispatchers\nname (or actually __qualname__ not that it normally matters), then it is\nnicer for users if we just raise a new error with the public symbol name.\n\nPython does not seem to normalize exception and goes down the unicode path,\nbut I assume that e.g. PyPy may not do that.  And there might be other\nweirder reason why we go down the full path.  I have manually tested it\nby forcing Normalization.\n\nCloses gh-23029",
            "additions": 12,
            "deletions": 1,
            "change_type": "MODIFY",
            "diff": "@@ -359,6 +359,17 @@ def func(array):\n                 TypeError, \"no implementation found for 'my.func'\"):\n             func(MyArray())\n \n+    @pytest.mark.parametrize(\"name\", [\"concatenate\", \"mean\", \"asarray\"])\n+    def test_signature_error_message_simple(self, name):\n+        func = getattr(np, name)\n+        try:\n+            # all of these functions need an argument:\n+            func()\n+        except TypeError as e:\n+            exc = e\n+\n+        assert exc.args[0].startswith(f\"{name}()\")\n+\n     def test_signature_error_message(self):\n         # The lambda function will be named \"<lambda>\", but the TypeError\n         # should show the name as \"func\"\n@@ -370,7 +381,7 @@ def func():\n             pass\n \n         try:\n-            func(bad_arg=3)\n+            func._implementation(bad_arg=3)\n         except TypeError as e:\n             expected_exception = e\n \n",
            "comment_added_diff": {
                "366": "            # all of these functions need an argument:"
            },
            "comment_deleted_diff": {},
            "comment_modified_diff": {}
        },
        {
            "commit": "5019e0abc3eda9bbfbead97b08b4302da2c31437",
            "timestamp": "2023-04-25T13:32:51+02:00",
            "author": "Sebastian Berg",
            "commit_message": "TST: Skip test on older Python versions which use `__name__`",
            "additions": 6,
            "deletions": 0,
            "change_type": "MODIFY",
            "diff": "@@ -389,6 +389,12 @@ def func():\n             func(bad_arg=3)\n             raise AssertionError(\"must fail\")\n         except TypeError as exc:\n+            if exc.args[0].startswith(\"_dispatcher\"):\n+                # We replace the qualname currently, but it used `__name__`\n+                # (relevant functions have the same name and qualname anyway)\n+                pytest.skip(\"Python version is not using __qualname__ for \"\n+                            \"TypeError formatting.\")\n+\n             assert exc.args == expected_exception.args\n \n     @pytest.mark.parametrize(\"value\", [234, \"this func is not replaced\"])\n",
            "comment_added_diff": {
                "393": "                # We replace the qualname currently, but it used `__name__`",
                "394": "                # (relevant functions have the same name and qualname anyway)"
            },
            "comment_deleted_diff": {},
            "comment_modified_diff": {}
        },
        {
            "commit": "2f0bd6e86a77e4401d0384d9a75edf9470c5deb6",
            "timestamp": "2023-09-05T21:58:22+02:00",
            "author": "Mateusz Sok\u00f3\u0142",
            "commit_message": "API: Cleaning `numpy/__init__.py` and main namespace - Part 4 [NEP 52] (#24445)\n\n[skip ci]",
            "additions": 0,
            "deletions": 21,
            "change_type": "MODIFY",
            "diff": "@@ -737,24 +737,3 @@ def __array__(self):\n     bound = np.mean.__get__(MyClass)  # classmethod\n     with pytest.raises(TypeError, match=\"unsupported operand type\"):\n         bound()\n-\n-\n-def test_scipy_trapz_support_shim():\n-    # SciPy 1.10 and earlier \"clone\" trapz in this way, so we have a\n-    # support shim in place: https://github.com/scipy/scipy/issues/17811\n-    # That should be removed eventually.  This test copies what SciPy does.\n-    # Hopefully removable 1 year after SciPy 1.11; shim added to NumPy 1.25.\n-    import types\n-    import functools\n-\n-    def _copy_func(f):\n-        # Based on http://stackoverflow.com/a/6528148/190597 (Glenn Maynard)\n-        g = types.FunctionType(f.__code__, f.__globals__, name=f.__name__,\n-                            argdefs=f.__defaults__, closure=f.__closure__)\n-        g = functools.update_wrapper(g, f)\n-        g.__kwdefaults__ = f.__kwdefaults__\n-        return g\n-\n-    trapezoid = _copy_func(np.trapz)\n-\n-    assert np.trapz([1, 2]) == trapezoid([1, 2])\n",
            "comment_added_diff": {},
            "comment_deleted_diff": {
                "743": "    # SciPy 1.10 and earlier \"clone\" trapz in this way, so we have a",
                "744": "    # support shim in place: https://github.com/scipy/scipy/issues/17811",
                "745": "    # That should be removed eventually.  This test copies what SciPy does.",
                "746": "    # Hopefully removable 1 year after SciPy 1.11; shim added to NumPy 1.25.",
                "751": "        # Based on http://stackoverflow.com/a/6528148/190597 (Glenn Maynard)"
            },
            "comment_modified_diff": {}
        }
    ],
    "core.py": [
        {
            "commit": "cfad62fd73c2a0f3d0dc4d73989071e331898a09",
            "timestamp": "2022-12-05T11:47:07+01:00",
            "author": "Lefteris Loukas",
            "commit_message": "ENH: Speedup masked array creation when mask=None (#22725)\n\nThis PR Closes gh-17046.\r\n\r\nThe problem was that when calling mask=None, the array creation took seconds compared to the microseconds needed when calling mask=False.\r\n\r\nUsing `mask=None` is a bit dubious, since it has a different meaning from the default `mask=nomask`, but the speed trap is so hard to find, that it seems pragmatic to support it.  OTOH, it also would seem fine to deprecate the whole path (or maybe see if the path can be sped up so that the speed difference isn't forbidding eough to bother).",
            "additions": 7,
            "deletions": 1,
            "change_type": "MODIFY",
            "diff": "@@ -2835,7 +2835,6 @@ def __new__(cls, data=None, mask=nomask, dtype=None, copy=False,\n         # Process mask.\n         # Type of the mask\n         mdtype = make_mask_descr(_data.dtype)\n-\n         if mask is nomask:\n             # Case 1. : no mask in input.\n             # Erase the current mask ?\n@@ -2870,6 +2869,12 @@ def __new__(cls, data=None, mask=nomask, dtype=None, copy=False,\n         else:\n             # Case 2. : With a mask in input.\n             # If mask is boolean, create an array of True or False\n+            \n+            # if users pass `mask=None` be forgiving here and cast it False\n+            # for speed; although the default is `mask=nomask` and can differ.\n+            if mask is None:\n+                mask = False\n+\n             if mask is True and mdtype == MaskType:\n                 mask = np.ones(_data.shape, dtype=mdtype)\n             elif mask is False and mdtype == MaskType:\n@@ -2917,6 +2922,7 @@ def _recursive_or(a, b):\n                     else:\n                         _data._mask = np.logical_or(mask, _data._mask)\n                     _data._sharedmask = False\n+        \n         # Update fill_value.\n         if fill_value is None:\n             fill_value = getattr(data, '_fill_value', None)\n",
            "comment_added_diff": {
                "2873": "            # if users pass `mask=None` be forgiving here and cast it False",
                "2874": "            # for speed; although the default is `mask=nomask` and can differ."
            },
            "comment_deleted_diff": {},
            "comment_modified_diff": {}
        },
        {
            "commit": "954aee7ab016d6f76d263bfe4c70de114eed63eb",
            "timestamp": "2022-12-21T09:41:19+01:00",
            "author": "Sebastian Berg",
            "commit_message": "API: Ensure a full mask is returned for masked_invalid\n\nMatplotlib relies on this, so we don't seem to have much of a choice.\nI am surprised that we were not notified of the issue before release\ntime.\n\nCloses gh-22720, gh-22720",
            "additions": 6,
            "deletions": 1,
            "change_type": "MODIFY",
            "diff": "@@ -2357,7 +2357,12 @@ def masked_invalid(a, copy=True):\n \n     \"\"\"\n     a = np.array(a, copy=False, subok=True)\n-    return masked_where(~(np.isfinite(a)), a, copy=copy)\n+    res = masked_where(~(np.isfinite(a)), a, copy=copy)\n+    # masked_invalid previously never returned nomask as a mask and doing so\n+    # threw off matplotlib (gh-22842).  So use shrink=False:\n+    if res._mask is nomask:\n+        res._mask = make_mask_none(res.shape, res.dtype)\n+    return res\n \n ###############################################################################\n #                            Printing options                                 #\n",
            "comment_added_diff": {
                "2361": "    # masked_invalid previously never returned nomask as a mask and doing so",
                "2362": "    # threw off matplotlib (gh-22842).  So use shrink=False:"
            },
            "comment_deleted_diff": {},
            "comment_modified_diff": {}
        },
        {
            "commit": "720cabc2331a0f003b708e55eb9bf0df204a085a",
            "timestamp": "2023-01-19T00:40:34+01:00",
            "author": "Marko Pacak",
            "commit_message": "BUG: fix ma.diff not preserving mask when using append/prepend (#22776)\n\nPort CORE diff relevant code to MA and adapt docstrings examples and add tsts.\r\n\r\nCloses gh-22465",
            "additions": 135,
            "deletions": 6,
            "change_type": "MODIFY",
            "diff": "@@ -7378,6 +7378,141 @@ def size(obj, axis=None):\n size.__doc__ = np.size.__doc__\n \n \n+def diff(a, /, n=1, axis=-1, prepend=np._NoValue, append=np._NoValue):\n+    \"\"\"\n+    Calculate the n-th discrete difference along the given axis.\n+    The first difference is given by ``out[i] = a[i+1] - a[i]`` along\n+    the given axis, higher differences are calculated by using `diff`\n+    recursively.\n+    Preserves the input mask.\n+\n+    Parameters\n+    ----------\n+    a : array_like\n+        Input array\n+    n : int, optional\n+        The number of times values are differenced. If zero, the input\n+        is returned as-is.\n+    axis : int, optional\n+        The axis along which the difference is taken, default is the\n+        last axis.\n+    prepend, append : array_like, optional\n+        Values to prepend or append to `a` along axis prior to\n+        performing the difference.  Scalar values are expanded to\n+        arrays with length 1 in the direction of axis and the shape\n+        of the input array in along all other axes.  Otherwise the\n+        dimension and shape must match `a` except along axis.\n+\n+    Returns\n+    -------\n+    diff : MaskedArray\n+        The n-th differences. The shape of the output is the same as `a`\n+        except along `axis` where the dimension is smaller by `n`. The\n+        type of the output is the same as the type of the difference\n+        between any two elements of `a`. This is the same as the type of\n+        `a` in most cases. A notable exception is `datetime64`, which\n+        results in a `timedelta64` output array.\n+\n+    See Also\n+    --------\n+    numpy.diff : Equivalent function in the top-level NumPy module.\n+\n+    Notes\n+    -----\n+    Type is preserved for boolean arrays, so the result will contain\n+    `False` when consecutive elements are the same and `True` when they\n+    differ.\n+\n+    For unsigned integer arrays, the results will also be unsigned. This\n+    should not be surprising, as the result is consistent with\n+    calculating the difference directly:\n+\n+    >>> u8_arr = np.array([1, 0], dtype=np.uint8)\n+    >>> np.ma.diff(u8_arr)\n+    masked_array(data=[255],\n+                 mask=False,\n+           fill_value=999999,\n+                dtype=uint8)\n+    >>> u8_arr[1,...] - u8_arr[0,...]\n+    255\n+\n+    If this is not desirable, then the array should be cast to a larger\n+    integer type first:\n+\n+    >>> i16_arr = u8_arr.astype(np.int16)\n+    >>> np.ma.diff(i16_arr)\n+    masked_array(data=[-1],\n+                 mask=False,\n+           fill_value=999999,\n+                dtype=int16)\n+\n+    Examples\n+    --------\n+    >>> a = np.array([1, 2, 3, 4, 7, 0, 2, 3])\n+    >>> x = np.ma.masked_where(a < 2, a)\n+    >>> np.ma.diff(x)\n+    masked_array(data=[--, 1, 1, 3, --, --, 1],\n+            mask=[ True, False, False, False,  True,  True, False],\n+        fill_value=999999)\n+\n+    >>> np.ma.diff(x, n=2)\n+    masked_array(data=[--, 0, 2, --, --, --],\n+                mask=[ True, False, False,  True,  True,  True],\n+        fill_value=999999)\n+\n+    >>> a = np.array([[1, 3, 1, 5, 10], [0, 1, 5, 6, 8]])\n+    >>> x = np.ma.masked_equal(a, value=1)\n+    >>> np.ma.diff(x)\n+    masked_array(\n+        data=[[--, --, --, 5],\n+                [--, --, 1, 2]],\n+        mask=[[ True,  True,  True, False],\n+                [ True,  True, False, False]],\n+        fill_value=1)\n+\n+    >>> np.ma.diff(x, axis=0)\n+    masked_array(data=[[--, --, --, 1, -2]],\n+            mask=[[ True,  True,  True, False, False]],\n+        fill_value=1)\n+\n+    \"\"\"\n+    if n == 0:\n+        return a\n+    if n < 0:\n+        raise ValueError(\"order must be non-negative but got \" + repr(n))\n+\n+    a = np.ma.asanyarray(a)\n+    if a.ndim == 0:\n+        raise ValueError(\n+            \"diff requires input that is at least one dimensional\"\n+            )\n+\n+    combined = []\n+    if prepend is not np._NoValue:\n+        prepend = np.ma.asanyarray(prepend)\n+        if prepend.ndim == 0:\n+            shape = list(a.shape)\n+            shape[axis] = 1\n+            prepend = np.broadcast_to(prepend, tuple(shape))\n+        combined.append(prepend)\n+\n+    combined.append(a)\n+\n+    if append is not np._NoValue:\n+        append = np.ma.asanyarray(append)\n+        if append.ndim == 0:\n+            shape = list(a.shape)\n+            shape[axis] = 1\n+            append = np.broadcast_to(append, tuple(shape))\n+        combined.append(append)\n+\n+    if len(combined) > 1:\n+        a = np.ma.concatenate(combined, axis)\n+\n+    # GH 22465 np.diff without prepend/append preserves the mask \n+    return np.diff(a, n, axis)\n+\n+\n ##############################################################################\n #                            Extra functions                                 #\n ##############################################################################\n@@ -8318,12 +8453,6 @@ def __call__(self, *args, **params):\n     np_ret='clipped_array : ndarray',\n     np_ma_ret='clipped_array : MaskedArray',\n )\n-diff = _convert2ma(\n-    'diff',\n-    params=dict(fill_value=None, hardmask=False),\n-    np_ret='diff : ndarray',\n-    np_ma_ret='diff : MaskedArray',\n-)\n empty = _convert2ma(\n     'empty',\n     params=dict(fill_value=None, hardmask=False),\n",
            "comment_added_diff": {
                "7512": "    # GH 22465 np.diff without prepend/append preserves the mask"
            },
            "comment_deleted_diff": {},
            "comment_modified_diff": {}
        },
        {
            "commit": "c61900d5675d4a33a893da5c8e343b6c0c1666dd",
            "timestamp": "2023-02-23T17:32:01-07:00",
            "author": "Tyler Reddy",
            "commit_message": "BUG: masked array proper deepcopies\n\nFixes #22556\nFixes #21022\n\n* add regression test and fix for gh-22556, where\nwe were relying on the array `copy` arg to deepcopy\na compound object type; I thought about performance issues\nhere, but if you are already in the land of `object` and\nyou are explicitly opting in to `deepcopy`, it seems like\nperformance might be wishful thinking anyway\n\n* add regression test and fix for gh-21022--this one was\nweirder but seems possible to sidestep by not trying\nto assign a shape of `()` to something that already has\nshape `()` and a non-writeable `shape` attribute",
            "additions": 14,
            "deletions": 1,
            "change_type": "MODIFY",
            "diff": "@@ -2870,7 +2870,13 @@ def __new__(cls, data=None, mask=nomask, dtype=None, copy=False,\n                     _data._mask = _data._mask.copy()\n                     # Reset the shape of the original mask\n                     if getmask(data) is not nomask:\n-                        data._mask.shape = data.shape\n+                        # gh-21022 encounters an issue here\n+                        # because data._mask.shape is not writeable, but\n+                        # the op was also pointless in that case, because\n+                        # the shapes were the same, so we can at least\n+                        # avoid that path\n+                        if data._mask.shape != data.shape:\n+                            data._mask.shape = data.shape\n         else:\n             # Case 2. : With a mask in input.\n             # If mask is boolean, create an array of True or False\n@@ -6300,6 +6306,13 @@ def __deepcopy__(self, memo=None):\n         memo[id(self)] = copied\n         for (k, v) in self.__dict__.items():\n             copied.__dict__[k] = deepcopy(v, memo)\n+        # as clearly documented for np.copy(), you need to use\n+        # deepcopy() directly for arrays of object type that may\n+        # contain compound types--you cannot depend on normal\n+        # copy semantics to do the right thing here\n+        if self.dtype.type is np.object_:\n+            for idx, _ in enumerate(copied):\n+                copied[idx] = deepcopy(copied[idx])\n         return copied\n \n \n",
            "comment_added_diff": {
                "2873": "                        # gh-21022 encounters an issue here",
                "2874": "                        # because data._mask.shape is not writeable, but",
                "2875": "                        # the op was also pointless in that case, because",
                "2876": "                        # the shapes were the same, so we can at least",
                "2877": "                        # avoid that path",
                "6309": "        # as clearly documented for np.copy(), you need to use",
                "6310": "        # deepcopy() directly for arrays of object type that may",
                "6311": "        # contain compound types--you cannot depend on normal",
                "6312": "        # copy semantics to do the right thing here"
            },
            "comment_deleted_diff": {},
            "comment_modified_diff": {
                "2873": "                        data._mask.shape = data.shape"
            }
        },
        {
            "commit": "03edb7b8dddb12198509d2bf602ea8b382d27199",
            "timestamp": "2023-03-03T03:07:17+00:00",
            "author": "yuki",
            "commit_message": "add support for non-2d arrays",
            "additions": 36,
            "deletions": 25,
            "change_type": "MODIFY",
            "diff": "@@ -7110,7 +7110,7 @@ def diag(v, k=0):\n \n     Examples\n     --------\n-    \n+\n     Create an array with negative values masked:\n \n     >>> import numpy as np\n@@ -7521,7 +7521,7 @@ def diff(a, /, n=1, axis=-1, prepend=np._NoValue, append=np._NoValue):\n     if len(combined) > 1:\n         a = np.ma.concatenate(combined, axis)\n \n-    # GH 22465 np.diff without prepend/append preserves the mask \n+    # GH 22465 np.diff without prepend/append preserves the mask\n     return np.diff(a, n, axis)\n \n \n@@ -7752,6 +7752,21 @@ def round_(a, decimals=0, out=None):\n round = round_\n \n \n+def _mask_propagate(a, axis):\n+    \"\"\"\n+    Mask whole 1-d vectors of an array that contain masked values.\n+    \"\"\"\n+    a = array(a, subok=False)\n+    m = getmask(a)\n+    if m is nomask or not m.any() or axis is None:\n+        return a\n+    a._mask = a._mask.copy()\n+    axes = normalize_axis_tuple(axis, a.ndim)\n+    for ax in axes:\n+        a._mask |= m.any(axis=ax, keepdims=True)\n+    return a\n+\n+\n # Needed by dot, so move here from extras.py. It will still be exported\n # from extras.py for compatibility.\n def mask_rowcols(a, axis=None):\n@@ -7770,7 +7785,7 @@ def mask_rowcols(a, axis=None):\n     ----------\n     a : array_like, MaskedArray\n         The array to mask.  If not a MaskedArray instance (or if no array\n-        elements are masked).  The result is a MaskedArray with `mask` set\n+        elements are masked), the result is a MaskedArray with `mask` set\n         to `nomask` (False). Must be a 2D array.\n     axis : int, optional\n         Axis along which to perform the operation. If None, applies to a\n@@ -7827,20 +7842,16 @@ def mask_rowcols(a, axis=None):\n       fill_value=1)\n \n     \"\"\"\n-    a = array(a, subok=False)\n     if a.ndim != 2:\n         raise NotImplementedError(\"mask_rowcols works for 2D arrays only.\")\n-    m = getmask(a)\n-    # Nothing is masked: return a\n-    if m is nomask or not m.any():\n-        return a\n-    maskedval = m.nonzero()\n-    a._mask = a._mask.copy()\n-    if not axis:\n-        a[np.unique(maskedval[0])] = masked\n-    if axis in [None, 1, -1]:\n-        a[:, np.unique(maskedval[1])] = masked\n-    return a\n+\n+    if axis is None:\n+        return _mask_propagate(a, axis=(0, 1))\n+    elif axis == 0:\n+        return _mask_propagate(a, axis=1)\n+    else:\n+        return _mask_propagate(a, axis=0)\n+\n \n \n # Include masked dot here to avoid import problems in getting it from\n@@ -7856,10 +7867,6 @@ def dot(a, b, strict=False, out=None):\n     corresponding method, it is recommended that the optional arguments be\n     treated as keyword only.  At some point that may be mandatory.\n \n-    .. note::\n-      Works only with 2-D arrays at the moment.\n-\n-\n     Parameters\n     ----------\n     a, b : masked_array_like\n@@ -7903,18 +7910,22 @@ def dot(a, b, strict=False, out=None):\n       fill_value=999999)\n \n     \"\"\"\n-    # !!!: Works only with 2D arrays. There should be a way to get it to run\n-    # with higher dimension\n-    if strict and (a.ndim == 2) and (b.ndim == 2):\n-        a = mask_rowcols(a, 0)\n-        b = mask_rowcols(b, 1)\n+    if strict is True:\n+        if np.ndim(a) == 0 or np.ndim(b) == 0:\n+            pass\n+        elif b.ndim == 1:\n+            a = _mask_propagate(a, a.ndim - 1)\n+            b = _mask_propagate(b, b.ndim - 1)\n+        else:\n+            a = _mask_propagate(a, a.ndim - 1)\n+            b = _mask_propagate(b, b.ndim - 2)\n     am = ~getmaskarray(a)\n     bm = ~getmaskarray(b)\n \n     if out is None:\n         d = np.dot(filled(a, 0), filled(b, 0))\n         m = ~np.dot(am, bm)\n-        if d.ndim == 0:\n+        if np.ndim(d) == 0:\n             d = np.asarray(d)\n         r = d.view(get_masked_subclass(a, b))\n         r.__setmask__(m)\n",
            "comment_added_diff": {
                "7524": "    # GH 22465 np.diff without prepend/append preserves the mask"
            },
            "comment_deleted_diff": {
                "7524": "    # GH 22465 np.diff without prepend/append preserves the mask",
                "7834": "    # Nothing is masked: return a",
                "7906": "    # !!!: Works only with 2D arrays. There should be a way to get it to run",
                "7907": "    # with higher dimension"
            },
            "comment_modified_diff": {
                "7524": "    # GH 22465 np.diff without prepend/append preserves the mask"
            }
        },
        {
            "commit": "03a87196e9f51d87b7c28d726b795b8fd0547a98",
            "timestamp": "2023-03-22T01:23:16+00:00",
            "author": "yuki",
            "commit_message": "ENH: revert changes of `mask_rowcols`",
            "additions": 12,
            "deletions": 8,
            "change_type": "MODIFY",
            "diff": "@@ -7842,16 +7842,20 @@ def mask_rowcols(a, axis=None):\n       fill_value=1)\n \n     \"\"\"\n+    a = array(a, subok=False)\n     if a.ndim != 2:\n         raise NotImplementedError(\"mask_rowcols works for 2D arrays only.\")\n-\n-    if axis is None:\n-        return _mask_propagate(a, axis=(0, 1))\n-    elif axis == 0:\n-        return _mask_propagate(a, axis=1)\n-    else:\n-        return _mask_propagate(a, axis=0)\n-\n+    m = getmask(a)\n+    # Nothing is masked: return a\n+    if m is nomask or not m.any():\n+        return a\n+    maskedval = m.nonzero()\n+    a._mask = a._mask.copy()\n+    if not axis:\n+        a[np.unique(maskedval[0])] = masked\n+    if axis in [None, 1, -1]:\n+        a[:, np.unique(maskedval[1])] = masked\n+    return a\n \n \n # Include masked dot here to avoid import problems in getting it from\n",
            "comment_added_diff": {
                "7849": "    # Nothing is masked: return a"
            },
            "comment_deleted_diff": {},
            "comment_modified_diff": {
                "7849": "        return _mask_propagate(a, axis=(0, 1))"
            }
        },
        {
            "commit": "13086bdf5e3cafb64c265f8de475b618a6a0252f",
            "timestamp": "2023-03-25T01:49:09+00:00",
            "author": "yuki",
            "commit_message": "MAINT: move `mask_rowcols` to `ma/extras.py`",
            "additions": 0,
            "deletions": 91,
            "change_type": "MODIFY",
            "diff": "@@ -7767,97 +7767,6 @@ def _mask_propagate(a, axis):\n     return a\n \n \n-# Needed by dot, so move here from extras.py. It will still be exported\n-# from extras.py for compatibility.\n-def mask_rowcols(a, axis=None):\n-    \"\"\"\n-    Mask rows and/or columns of a 2D array that contain masked values.\n-\n-    Mask whole rows and/or columns of a 2D array that contain\n-    masked values.  The masking behavior is selected using the\n-    `axis` parameter.\n-\n-      - If `axis` is None, rows *and* columns are masked.\n-      - If `axis` is 0, only rows are masked.\n-      - If `axis` is 1 or -1, only columns are masked.\n-\n-    Parameters\n-    ----------\n-    a : array_like, MaskedArray\n-        The array to mask.  If not a MaskedArray instance (or if no array\n-        elements are masked), the result is a MaskedArray with `mask` set\n-        to `nomask` (False). Must be a 2D array.\n-    axis : int, optional\n-        Axis along which to perform the operation. If None, applies to a\n-        flattened version of the array.\n-\n-    Returns\n-    -------\n-    a : MaskedArray\n-        A modified version of the input array, masked depending on the value\n-        of the `axis` parameter.\n-\n-    Raises\n-    ------\n-    NotImplementedError\n-        If input array `a` is not 2D.\n-\n-    See Also\n-    --------\n-    mask_rows : Mask rows of a 2D array that contain masked values.\n-    mask_cols : Mask cols of a 2D array that contain masked values.\n-    masked_where : Mask where a condition is met.\n-\n-    Notes\n-    -----\n-    The input array's mask is modified by this function.\n-\n-    Examples\n-    --------\n-    >>> import numpy.ma as ma\n-    >>> a = np.zeros((3, 3), dtype=int)\n-    >>> a[1, 1] = 1\n-    >>> a\n-    array([[0, 0, 0],\n-           [0, 1, 0],\n-           [0, 0, 0]])\n-    >>> a = ma.masked_equal(a, 1)\n-    >>> a\n-    masked_array(\n-      data=[[0, 0, 0],\n-            [0, --, 0],\n-            [0, 0, 0]],\n-      mask=[[False, False, False],\n-            [False,  True, False],\n-            [False, False, False]],\n-      fill_value=1)\n-    >>> ma.mask_rowcols(a)\n-    masked_array(\n-      data=[[0, --, 0],\n-            [--, --, --],\n-            [0, --, 0]],\n-      mask=[[False,  True, False],\n-            [ True,  True,  True],\n-            [False,  True, False]],\n-      fill_value=1)\n-\n-    \"\"\"\n-    a = array(a, subok=False)\n-    if a.ndim != 2:\n-        raise NotImplementedError(\"mask_rowcols works for 2D arrays only.\")\n-    m = getmask(a)\n-    # Nothing is masked: return a\n-    if m is nomask or not m.any():\n-        return a\n-    maskedval = m.nonzero()\n-    a._mask = a._mask.copy()\n-    if not axis:\n-        a[np.unique(maskedval[0])] = masked\n-    if axis in [None, 1, -1]:\n-        a[:, np.unique(maskedval[1])] = masked\n-    return a\n-\n-\n # Include masked dot here to avoid import problems in getting it from\n # extras.py. Note that it is not included in __all__, but rather exported\n # from extras in order to avoid backward compatibility problems.\n",
            "comment_added_diff": {},
            "comment_deleted_diff": {
                "7770": "# Needed by dot, so move here from extras.py. It will still be exported",
                "7771": "# from extras.py for compatibility.",
                "7849": "    # Nothing is masked: return a"
            },
            "comment_modified_diff": {}
        },
        {
            "commit": "77a5aab54e10e063773e9aaf0e3730bf0cfefa9b",
            "timestamp": "2023-04-21T11:52:06+02:00",
            "author": "Sebastian Berg",
            "commit_message": "BUG: Fix masked array raveling when `order=\"A\"` or `order=\"K\"`\n\nThis transitively fixes gh-22912.  I had alooked a bit into whether\nit is worthwhile to preserve the mask order, but TBH, we seem to not\ndo so in so many places, that I don't think it really is worthwhile.\n\nApplying `order=\"K\"` or `order=\"A\"` to the data and mask separately\nis an big bug though.\n\nCloses gh-22912",
            "additions": 8,
            "deletions": 0,
            "change_type": "MODIFY",
            "diff": "@@ -4631,6 +4631,7 @@ def ravel(self, order='C'):\n             otherwise.  'K' means to read the elements in the order they occur\n             in memory, except for reversing the data when strides are negative.\n             By default, 'C' index order is used.\n+            (Masked arrays currently use 'A' on the data when 'K' is passed.)\n \n         Returns\n         -------\n@@ -4657,6 +4658,13 @@ def ravel(self, order='C'):\n                fill_value=999999)\n \n         \"\"\"\n+        # The order of _data and _mask could be different (it shouldn't be\n+        # normally).  Passing order `K` or `A` would be incorrect.\n+        # So we ignore the mask memory order.\n+        # TODO: We don't actually support K, so use A instead.  We could\n+        #       try to guess this correct by sorting strides or deprecate.\n+        if order in \"kKaA\":\n+            order = \"C\" if self._data.flags.fnc else \"F\"\n         r = ndarray.ravel(self._data, order=order).view(type(self))\n         r._update_from(self)\n         if self._mask is not nomask:\n",
            "comment_added_diff": {
                "4661": "        # The order of _data and _mask could be different (it shouldn't be",
                "4662": "        # normally).  Passing order `K` or `A` would be incorrect.",
                "4663": "        # So we ignore the mask memory order.",
                "4664": "        # TODO: We don't actually support K, so use A instead.  We could",
                "4665": "        #       try to guess this correct by sorting strides or deprecate."
            },
            "comment_deleted_diff": {},
            "comment_modified_diff": {}
        },
        {
            "commit": "0655cb9e24cff97642c70a98c5258bcbe7a6c3d3",
            "timestamp": "2023-04-21T12:23:23+02:00",
            "author": "Sebastian Berg",
            "commit_message": "BUG: Ignore invalid and overflow warnings in masked setitem\n\nGiving a warning for invalid/overflow in settitem/casts is right\n(IMO), however for masked arrays it can be surprising since the\nwarning is not useful if the value is invalid but also masked.\n\nSo, simply blanket ignore the relevant warnings in setitem via errstate.\n(There may be some other cases like `.astype()` where it might be\nhelpful to MA users to just blanket opt-out of these new warnings.)\n\nCloses gh-23000",
            "additions": 12,
            "deletions": 0,
            "change_type": "MODIFY",
            "diff": "@@ -3340,6 +3340,10 @@ def _scalar_heuristic(arr, elem):\n                 # Note: Don't try to check for m.any(), that'll take too long\n         return dout\n \n+    # setitem may put NaNs into integer arrays or occasionally overflow a\n+    # float.  But this may happen in masked values, so avoid otherwise\n+    # correct warnings (as is typical also in masked calculations).\n+    @np.errstate(over='ignore', invalid='ignore')\n     def __setitem__(self, indx, value):\n         \"\"\"\n         x.__setitem__(i, y) <==> x[i]=y\n@@ -4631,6 +4635,7 @@ def ravel(self, order='C'):\n             otherwise.  'K' means to read the elements in the order they occur\n             in memory, except for reversing the data when strides are negative.\n             By default, 'C' index order is used.\n+            (Masked arrays currently use 'A' on the data when 'K' is passed.)\n \n         Returns\n         -------\n@@ -4657,6 +4662,13 @@ def ravel(self, order='C'):\n                fill_value=999999)\n \n         \"\"\"\n+        # The order of _data and _mask could be different (it shouldn't be\n+        # normally).  Passing order `K` or `A` would be incorrect.\n+        # So we ignore the mask memory order.\n+        # TODO: We don't actually support K, so use A instead.  We could\n+        #       try to guess this correct by sorting strides or deprecate.\n+        if order in \"kKaA\":\n+            order = \"C\" if self._data.flags.fnc else \"F\"\n         r = ndarray.ravel(self._data, order=order).view(type(self))\n         r._update_from(self)\n         if self._mask is not nomask:\n",
            "comment_added_diff": {
                "3343": "    # setitem may put NaNs into integer arrays or occasionally overflow a",
                "3344": "    # float.  But this may happen in masked values, so avoid otherwise",
                "3345": "    # correct warnings (as is typical also in masked calculations).",
                "4665": "        # The order of _data and _mask could be different (it shouldn't be",
                "4666": "        # normally).  Passing order `K` or `A` would be incorrect.",
                "4667": "        # So we ignore the mask memory order.",
                "4668": "        # TODO: We don't actually support K, so use A instead.  We could",
                "4669": "        #       try to guess this correct by sorting strides or deprecate."
            },
            "comment_deleted_diff": {},
            "comment_modified_diff": {}
        },
        {
            "commit": "e7ce9eb387765f48f03849244d7c92c007f06213",
            "timestamp": "2023-07-18T11:50:24+02:00",
            "author": "Sebastian Berg",
            "commit_message": "ENH: Only print type information when helpful for MA fill value",
            "additions": 11,
            "deletions": 2,
            "change_type": "MODIFY",
            "diff": "@@ -4067,7 +4067,16 @@ def __repr__(self):\n             separator=\", \",\n             prefix=indents['mask'] + 'mask=',\n             suffix=',')\n-        reprs['fill_value'] = repr(self.fill_value)\n+\n+        if self.fill_value.dtype == self.dtype:\n+            # The dtype of the fill value should match that of the array\n+            # and it is unnecessary to print the full `np.int64(...)`\n+            fill_repr = np.core.arrayprint.get_formatter(\n+                    dtype=self.fill_value.dtype, fmt=\"r\")(self.fill_value)\n+        else:\n+            # Fall back to the normal repr just in case something is weird:\n+            fill_repr = repr(self.fill_value)\n+        reprs['fill_value'] = fill_repr\n         if dtype_needed:\n             reprs['dtype'] = np.core.arrayprint.dtype_short_repr(self.dtype)\n \n@@ -5054,7 +5063,7 @@ def nonzero(self):\n                 [ True,  True,  True],\n                 [ True,  True,  True]],\n           mask=False,\n-          fill_value=numpy.True_)\n+          fill_value=True)\n         >>> ma.nonzero(a > 3)\n         (array([1, 1, 1, 2, 2, 2]), array([0, 1, 2, 0, 1, 2]))\n \n",
            "comment_added_diff": {
                "4072": "            # The dtype of the fill value should match that of the array",
                "4073": "            # and it is unnecessary to print the full `np.int64(...)`",
                "4077": "            # Fall back to the normal repr just in case something is weird:"
            },
            "comment_deleted_diff": {},
            "comment_modified_diff": {}
        },
        {
            "commit": "413f1a7d8f2b69f4fe1d536a16f153d4c01c85e2",
            "timestamp": "2023-07-18T11:50:24+02:00",
            "author": "Sebastian Berg",
            "commit_message": "MAINT: Switch to also have str explicitly, fixup MA and forward str/repr to scalar\n\nMost importantly, use the scalar `str()` for formatting str/repr of\n\"array elements\" when a format code is given (for floats).\nThe reason is that floats to print differently currently and falling back is\nsimplest.",
            "additions": 3,
            "deletions": 1,
            "change_type": "MODIFY",
            "diff": "@@ -4068,7 +4068,9 @@ def __repr__(self):\n             prefix=indents['mask'] + 'mask=',\n             suffix=',')\n \n-        if self.fill_value.dtype == self.dtype:\n+        if self._fill_value is None:\n+            self.fill_value  # initialize fill_value\n+        if self._fill_value.dtype == self.dtype:\n             # The dtype of the fill value should match that of the array\n             # and it is unnecessary to print the full `np.int64(...)`\n             fill_repr = np.core.arrayprint.get_formatter(\n",
            "comment_added_diff": {
                "4072": "            self.fill_value  # initialize fill_value"
            },
            "comment_deleted_diff": {},
            "comment_modified_diff": {}
        },
        {
            "commit": "4c6ff640f918c4c0d221ebbfc3fb2226eb8f1b54",
            "timestamp": "2023-07-18T11:50:24+02:00",
            "author": "Sebastian Berg",
            "commit_message": "DOC: Update reference to pass refguide\n\nThis makes things pass even though there should be a massive amount\nof types added in other places, so that it is not complete.\n\nBut I think this may be useful, since the complete change would be\nquite verbose...\n\nOne thing to note may be that `np.float64(nan)` currently does not\nround-trip correctly of course, this would require adding `np.`\nto the repr (or quotes).",
            "additions": 12,
            "deletions": 12,
            "change_type": "MODIFY",
            "diff": "@@ -1876,7 +1876,7 @@ def masked_where(condition, a, copy=True):\n     >>> ma.masked_where(a == 2, b)\n     masked_array(data=['a', 'b', --, 'd'],\n                  mask=[False, False,  True, False],\n-           fill_value='N/A',\n+           fill_value=np.str_('N/A'),\n                 dtype='<U1')\n \n     Effect of the `copy` argument.\n@@ -3728,23 +3728,23 @@ def fill_value(self):\n         >>> for dt in [np.int32, np.int64, np.float64, np.complex128]:\n         ...     np.ma.array([0, 1], dtype=dt).get_fill_value()\n         ...\n-        999999\n-        999999\n-        1e+20\n-        (1e+20+0j)\n+        np.int64(999999)\n+        np.int64(999999)\n+        np.float64(1e+20)\n+        np.complex128(1e+20+0j)\n \n         >>> x = np.ma.array([0, 1.], fill_value=-np.inf)\n         >>> x.fill_value\n-        -inf\n+        np.float64(-inf)\n         >>> x.fill_value = np.pi\n         >>> x.fill_value\n-        3.1415926535897931 # may vary\n+        np.float64(3.1415926535897931)\n \n         Reset to default:\n \n         >>> x.fill_value = None\n         >>> x.fill_value\n-        1e+20\n+        np.float64(1e+20)\n \n         \"\"\"\n         if self._fill_value is None:\n@@ -4074,7 +4074,7 @@ def __repr__(self):\n             # The dtype of the fill value should match that of the array\n             # and it is unnecessary to print the full `np.int64(...)`\n             fill_repr = np.core.arrayprint.get_formatter(\n-                    dtype=self.fill_value.dtype, fmt=repr)(self.fill_value)\n+                    dtype=self._fill_value.dtype, fmt=repr)(self.fill_value)\n         else:\n             # Fall back to the normal repr just in case something is weird:\n             fill_repr = repr(self.fill_value)\n@@ -6042,7 +6042,7 @@ def ptp(self, axis=None, out=None, fill_value=None, keepdims=False):\n         >>> y.ptp(axis=1)\n         masked_array(data=[ 126,  127, -128, -127],\n                      mask=False,\n-               fill_value=999999,\n+               fill_value=np.int64(999999),\n                     dtype=int8)\n \n         A work-around is to use the `view()` method to view the result as\n@@ -6051,7 +6051,7 @@ def ptp(self, axis=None, out=None, fill_value=None, keepdims=False):\n         >>> y.ptp(axis=1).view(np.uint8)\n         masked_array(data=[126, 127, 128, 129],\n                      mask=False,\n-               fill_value=999999,\n+               fill_value=np.int64(999999),\n                     dtype=uint8)\n         \"\"\"\n         if out is None:\n@@ -8417,7 +8417,7 @@ def fromflex(fxarray):\n             [0, 0]],\n       mask=[[False, False],\n             [False, False]],\n-      fill_value=999999,\n+      fill_value=np.int64(999999),\n       dtype=int32)\n \n     \"\"\"\n",
            "comment_added_diff": {},
            "comment_deleted_diff": {
                "3741": "        3.1415926535897931 # may vary"
            },
            "comment_modified_diff": {}
        },
        {
            "commit": "b6f83b6ab904839820cfada1d6d83a08e18ebb36",
            "timestamp": "2023-07-18T11:50:24+02:00",
            "author": "Sebastian Berg",
            "commit_message": "MAINT: Special case string fill-value in MA repr\n\nJust to remove unnecessary visual clutter in this special case\nwhere it is almost always no additional information.",
            "additions": 5,
            "deletions": 2,
            "change_type": "MODIFY",
            "diff": "@@ -1876,7 +1876,7 @@ def masked_where(condition, a, copy=True):\n     >>> ma.masked_where(a == 2, b)\n     masked_array(data=['a', 'b', --, 'd'],\n                  mask=[False, False,  True, False],\n-           fill_value=np.str_('N/A'),\n+           fill_value='N/A',\n                 dtype='<U1')\n \n     Effect of the `copy` argument.\n@@ -4070,9 +4070,12 @@ def __repr__(self):\n \n         if self._fill_value is None:\n             self.fill_value  # initialize fill_value\n-        if self._fill_value.dtype == self.dtype:\n+        if self._fill_value.dtype == self.dtype or (\n+                self.dtype.kind in \"SU\"\n+                and self.dtype.kind == self._fill_value.dtype.kind):\n             # The dtype of the fill value should match that of the array\n             # and it is unnecessary to print the full `np.int64(...)`\n+            # Allow strings: \"N/A\" has length 3 so would mismatch often.\n             fill_repr = np.core.arrayprint.get_formatter(\n                     dtype=self._fill_value.dtype, fmt=repr)(self.fill_value)\n         else:\n",
            "comment_added_diff": {
                "4078": "            # Allow strings: \"N/A\" has length 3 so would mismatch often."
            },
            "comment_deleted_diff": {},
            "comment_modified_diff": {}
        },
        {
            "commit": "01a1d29e156a111694659137a1ac05d54bf44774",
            "timestamp": "2023-07-18T11:50:24+02:00",
            "author": "Marten van Kerkwijk",
            "commit_message": "Simplify, removing new fmt option",
            "additions": 2,
            "deletions": 3,
            "change_type": "MODIFY",
            "diff": "@@ -4076,11 +4076,10 @@ def __repr__(self):\n             # The dtype of the fill value should match that of the array\n             # and it is unnecessary to print the full `np.int64(...)`\n             # Allow strings: \"N/A\" has length 3 so would mismatch often.\n-            fill_repr = np.core.arrayprint.get_formatter(\n-                    dtype=self._fill_value.dtype, fmt=repr)(self.fill_value)\n+            fill_repr = str(self.fill_value)\n         else:\n-            # Fall back to the normal repr just in case something is weird:\n             fill_repr = repr(self.fill_value)\n+\n         reprs['fill_value'] = fill_repr\n         if dtype_needed:\n             reprs['dtype'] = np.core.arrayprint.dtype_short_repr(self.dtype)\n",
            "comment_added_diff": {},
            "comment_deleted_diff": {
                "4082": "            # Fall back to the normal repr just in case something is weird:"
            },
            "comment_modified_diff": {}
        },
        {
            "commit": "60973898705966d4b6c009d6a6f856db87ac06eb",
            "timestamp": "2023-07-18T12:27:43+02:00",
            "author": "Sebastian Berg",
            "commit_message": "Hack strings to be correct for fill-value",
            "additions": 6,
            "deletions": 5,
            "change_type": "MODIFY",
            "diff": "@@ -4070,12 +4070,13 @@ def __repr__(self):\n \n         if self._fill_value is None:\n             self.fill_value  # initialize fill_value\n-        if self._fill_value.dtype == self.dtype or (\n-                self.dtype.kind in \"SU\"\n+        if (self._fill_value.dtype in \"SU\"\n                 and self.dtype.kind == self._fill_value.dtype.kind):\n-            # The dtype of the fill value should match that of the array\n-            # and it is unnecessary to print the full `np.int64(...)`\n-            # Allow strings: \"N/A\" has length 3 so would mismatch often.\n+            # Allow strings: \"N/A\" has length 3 so would mismatch.\n+            fill_repr = repr(self.fill_value.item())\n+        if self._fill_value.dtype == self.dtype:\n+            # Guess that it is OK to use the string as item repr.  To really\n+            # fix this, it needs new logic (shared with structured scalars)\n             fill_repr = str(self.fill_value)\n         else:\n             fill_repr = repr(self.fill_value)\n",
            "comment_added_diff": {
                "4075": "            # Allow strings: \"N/A\" has length 3 so would mismatch.",
                "4078": "            # Guess that it is OK to use the string as item repr.  To really",
                "4079": "            # fix this, it needs new logic (shared with structured scalars)"
            },
            "comment_deleted_diff": {
                "4076": "            # The dtype of the fill value should match that of the array",
                "4077": "            # and it is unnecessary to print the full `np.int64(...)`",
                "4078": "            # Allow strings: \"N/A\" has length 3 so would mismatch often."
            },
            "comment_modified_diff": {
                "4078": "            # Allow strings: \"N/A\" has length 3 so would mismatch often."
            }
        },
        {
            "commit": "9a07a5efe8ab20cacee9235d9a8fc4de02433581",
            "timestamp": "2023-08-26T17:33:03+02:00",
            "author": "Marten van Kerkwijk",
            "commit_message": "BUG: fix comparisons between masked and unmasked structured arrays",
            "additions": 3,
            "deletions": 0,
            "change_type": "MODIFY",
            "diff": "@@ -4140,6 +4140,9 @@ def _comparison(self, other, compare):\n             # Now take care of the mask; the merged mask should have an item\n             # masked if all fields were masked (in one and/or other).\n             mask = (mask == np.ones((), mask.dtype))\n+            # Ensure we can compare masks below if other was not masked.\n+            if omask is np.False_:\n+                omask = np.zeros((), smask.dtype)\n \n         else:\n             # For regular arrays, just use the data as they come.\n",
            "comment_added_diff": {
                "4143": "            # Ensure we can compare masks below if other was not masked."
            },
            "comment_deleted_diff": {},
            "comment_modified_diff": {}
        },
        {
            "commit": "d00eaf358675064a827392442f6ed9ac365e62f9",
            "timestamp": "2023-08-26T17:33:05+02:00",
            "author": "Marten van Kerkwijk",
            "commit_message": "BUG: ensure mask on comparison results always has properly broadcast shape",
            "additions": 5,
            "deletions": 4,
            "change_type": "MODIFY",
            "diff": "@@ -4159,10 +4159,11 @@ def _comparison(self, other, compare):\n             # Note that this works automatically for structured arrays too.\n             # Ignore this for operations other than `==` and `!=`\n             check = np.where(mask, compare(smask, omask), check)\n-            if mask.shape != check.shape:\n-                # Guarantee consistency of the shape, making a copy since the\n-                # the mask may need to get written to later.\n-                mask = np.broadcast_to(mask, check.shape).copy()\n+\n+        if mask.shape != check.shape:\n+            # Guarantee consistency of the shape, making a copy since the\n+            # the mask may need to get written to later.\n+            mask = np.broadcast_to(mask, check.shape).copy()\n \n         check = check.view(type(self))\n         check._update_from(self)\n",
            "comment_added_diff": {
                "4164": "            # Guarantee consistency of the shape, making a copy since the",
                "4165": "            # the mask may need to get written to later."
            },
            "comment_deleted_diff": {
                "4163": "                # Guarantee consistency of the shape, making a copy since the",
                "4164": "                # the mask may need to get written to later."
            },
            "comment_modified_diff": {
                "4164": "                # the mask may need to get written to later.",
                "4165": "                mask = np.broadcast_to(mask, check.shape).copy()"
            }
        },
        {
            "commit": "e865b3379b25270ec0fe8e9cff2ab9acf6b5ba8a",
            "timestamp": "2023-08-27T11:30:04+02:00",
            "author": "Mateusz Sok\u00f3\u0142",
            "commit_message": "API: Update `lib.function_base` namespace (#24538)\n\n[skip ci]",
            "additions": 3,
            "deletions": 5,
            "change_type": "MODIFY",
            "diff": "@@ -32,10 +32,8 @@\n import numpy.core.umath as umath\n import numpy.core.numerictypes as ntypes\n from numpy.core import multiarray as mu\n-from numpy import ndarray, amax, amin, iscomplexobj, bool_, _NoValue\n-from numpy import array as narray\n-from numpy.lib.function_base import angle\n-from numpy import expand_dims\n+from numpy import ndarray, amax, amin, iscomplexobj, bool_, _NoValue, angle\n+from numpy import array as narray, expand_dims\n from numpy.core.numeric import normalize_axis_tuple\n from numpy._utils._inspect import getargspec, formatargspec\n \n@@ -1206,7 +1204,7 @@ def __call__(self, a, b, *args, **kwargs):\n cosh = _MaskedUnaryOperation(umath.cosh)\n tanh = _MaskedUnaryOperation(umath.tanh)\n abs = absolute = _MaskedUnaryOperation(umath.absolute)\n-angle = _MaskedUnaryOperation(angle)  # from numpy.lib.function_base\n+angle = _MaskedUnaryOperation(angle)\n fabs = _MaskedUnaryOperation(umath.fabs)\n negative = _MaskedUnaryOperation(umath.negative)\n floor = _MaskedUnaryOperation(umath.floor)\n",
            "comment_added_diff": {},
            "comment_deleted_diff": {
                "1209": "angle = _MaskedUnaryOperation(angle)  # from numpy.lib.function_base"
            },
            "comment_modified_diff": {}
        },
        {
            "commit": "17440d692dd648159c304676772ac1d5c6e40288",
            "timestamp": "2023-08-27T12:46:44+02:00",
            "author": "Marten van Kerkwijk",
            "commit_message": "BUG: ensure nomask in comparison result is not broadcast",
            "additions": 12,
            "deletions": 11,
            "change_type": "MODIFY",
            "diff": "@@ -4153,17 +4153,18 @@ def _comparison(self, other, compare):\n         if isinstance(check, (np.bool_, bool)):\n             return masked if mask else check\n \n-        if mask is not nomask and compare in (operator.eq, operator.ne):\n-            # Adjust elements that were masked, which should be treated\n-            # as equal if masked in both, unequal if masked in one.\n-            # Note that this works automatically for structured arrays too.\n-            # Ignore this for operations other than `==` and `!=`\n-            check = np.where(mask, compare(smask, omask), check)\n-\n-        if mask.shape != check.shape:\n-            # Guarantee consistency of the shape, making a copy since the\n-            # the mask may need to get written to later.\n-            mask = np.broadcast_to(mask, check.shape).copy()\n+        if mask is not nomask:\n+            if compare in (operator.eq, operator.ne):\n+                # Adjust elements that were masked, which should be treated\n+                # as equal if masked in both, unequal if masked in one.\n+                # Note that this works automatically for structured arrays too.\n+                # Ignore this for operations other than `==` and `!=`\n+                check = np.where(mask, compare(smask, omask), check)\n+\n+            if mask.shape != check.shape:\n+                # Guarantee consistency of the shape, making a copy since the\n+                # the mask may need to get written to later.\n+                mask = np.broadcast_to(mask, check.shape).copy()\n \n         check = check.view(type(self))\n         check._update_from(self)\n",
            "comment_added_diff": {
                "4158": "                # Adjust elements that were masked, which should be treated",
                "4159": "                # as equal if masked in both, unequal if masked in one.",
                "4160": "                # Note that this works automatically for structured arrays too.",
                "4161": "                # Ignore this for operations other than `==` and `!=`",
                "4165": "                # Guarantee consistency of the shape, making a copy since the",
                "4166": "                # the mask may need to get written to later."
            },
            "comment_deleted_diff": {
                "4157": "            # Adjust elements that were masked, which should be treated",
                "4158": "            # as equal if masked in both, unequal if masked in one.",
                "4159": "            # Note that this works automatically for structured arrays too.",
                "4160": "            # Ignore this for operations other than `==` and `!=`",
                "4164": "            # Guarantee consistency of the shape, making a copy since the",
                "4165": "            # the mask may need to get written to later."
            },
            "comment_modified_diff": {
                "4158": "            # as equal if masked in both, unequal if masked in one.",
                "4159": "            # Note that this works automatically for structured arrays too.",
                "4160": "            # Ignore this for operations other than `==` and `!=`",
                "4161": "            check = np.where(mask, compare(smask, omask), check)",
                "4165": "            # the mask may need to get written to later.",
                "4166": "            mask = np.broadcast_to(mask, check.shape).copy()"
            }
        }
    ],
    "cibw_test_command.sh": [],
    "how-to-index.rst": [],
    "how-to-partition.rst": [],
    "_add_newdocs.py": [
        {
            "commit": "2e9479eb2f964566d25826b21445c59a88ccd319",
            "timestamp": "2023-05-26T00:51:14-07:00",
            "author": "Sebastian Berg",
            "commit_message": "DOC: Clean up errstate handling in our tests (#23813)\n\n* TST: Clean up errstate handling in our tests\r\n\r\nIt is plausible that this doesn't fix everything, but it fixes\r\nat least most.\r\n\r\nCo-authored-by: Ross Barnowski <rossbar@berkeley.edu>",
            "additions": 8,
            "deletions": 5,
            "change_type": "MODIFY",
            "diff": "@@ -4808,7 +4808,8 @@\n \n     Examples\n     --------\n-    >>> np.geterrobj()  # first get the defaults\n+    >>> orig_errobj = np.geterrobj()[:]  # get a copy of the errobj\n+    >>> orig_errobj\n     [8192, 521, None]\n \n     >>> def err_handler(type, flag):\n@@ -4818,7 +4819,7 @@\n     >>> old_err = np.seterr(divide='raise')\n     >>> old_handler = np.seterrcall(err_handler)\n     >>> np.geterrobj()\n-    [8192, 521, <function err_handler at 0x91dcaac>]\n+    [20000, 522, <function err_handler at 0x...>]\n \n     >>> old_err = np.seterr(all='ignore')\n     >>> np.base_repr(np.geterrobj()[1], 8)\n@@ -4827,6 +4828,7 @@\n     ...                     invalid='print')\n     >>> np.base_repr(np.geterrobj()[1], 8)\n     '4351'\n+    >>> old_errobj = np.seterrobj(orig_errobj)  # restore the error state\n \n     \"\"\")\n \n@@ -4871,8 +4873,8 @@\n \n     Examples\n     --------\n-    >>> old_errobj = np.geterrobj()  # first get the defaults\n-    >>> old_errobj\n+    >>> orig_errobj = np.geterrobj()[:]  # get a copy of the errobj\n+    >>> orig_errobj\n     [8192, 521, None]\n \n     >>> def err_handler(type, flag):\n@@ -4883,9 +4885,10 @@\n     >>> np.base_repr(12, 8)  # int for divide=4 ('print') and over=1 ('warn')\n     '14'\n     >>> np.geterr()\n-    {'over': 'warn', 'divide': 'print', 'invalid': 'ignore', 'under': 'ignore'}\n+    {'divide': 'print', 'over': 'warn', 'under': 'ignore', 'invalid': 'ignore'}\n     >>> np.geterrcall() is err_handler\n     True\n+    >>> old_errobj = np.seterrobj(orig_errobj)  # restore the original state\n \n     \"\"\")\n \n",
            "comment_added_diff": {
                "4811": "    >>> orig_errobj = np.geterrobj()[:]  # get a copy of the errobj",
                "4831": "    >>> old_errobj = np.seterrobj(orig_errobj)  # restore the error state",
                "4876": "    >>> orig_errobj = np.geterrobj()[:]  # get a copy of the errobj",
                "4891": "    >>> old_errobj = np.seterrobj(orig_errobj)  # restore the original state"
            },
            "comment_deleted_diff": {
                "4811": "    >>> np.geterrobj()  # first get the defaults",
                "4874": "    >>> old_errobj = np.geterrobj()  # first get the defaults"
            },
            "comment_modified_diff": {
                "4811": "    >>> np.geterrobj()  # first get the defaults"
            }
        },
        {
            "commit": "95b8c249551eb54d07f05794e0ef21dc6c16d73d",
            "timestamp": "2023-06-13T09:26:30+02:00",
            "author": "Sebastian Berg",
            "commit_message": "API: Remove `np.geterrobj` and `np.seterrobj`\n\nThese are left for now as a private implementation detail.",
            "additions": 0,
            "deletions": 126,
            "change_type": "MODIFY",
            "diff": "@@ -4766,132 +4766,6 @@\n \n     \"\"\")\n \n-add_newdoc('numpy.core.umath', 'geterrobj',\n-    \"\"\"\n-    geterrobj()\n-\n-    Return the current object that defines floating-point error handling.\n-\n-    The error object contains all information that defines the error handling\n-    behavior in NumPy. `geterrobj` is used internally by the other\n-    functions that get and set error handling behavior (`geterr`, `seterr`,\n-    `geterrcall`, `seterrcall`).\n-\n-    Returns\n-    -------\n-    errobj : list\n-        The error object, a list containing three elements:\n-        [internal numpy buffer size, error mask, error callback function].\n-\n-        The error mask is a single integer that holds the treatment information\n-        on all four floating point errors. The information for each error type\n-        is contained in three bits of the integer. If we print it in base 8, we\n-        can see what treatment is set for \"invalid\", \"under\", \"over\", and\n-        \"divide\" (in that order). The printed string can be interpreted with\n-\n-        * 0 : 'ignore'\n-        * 1 : 'warn'\n-        * 2 : 'raise'\n-        * 3 : 'call'\n-        * 4 : 'print'\n-        * 5 : 'log'\n-\n-    See Also\n-    --------\n-    seterrobj, seterr, geterr, seterrcall, geterrcall\n-    getbufsize, setbufsize\n-\n-    Notes\n-    -----\n-    For complete documentation of the types of floating-point exceptions and\n-    treatment options, see `seterr`.\n-\n-    Examples\n-    --------\n-    >>> orig_errobj = np.geterrobj()[:]  # get a copy of the errobj\n-    >>> orig_errobj\n-    [8192, 521, None]\n-\n-    >>> def err_handler(type, flag):\n-    ...     print(\"Floating point error (%s), with flag %s\" % (type, flag))\n-    ...\n-    >>> old_bufsize = np.setbufsize(20000)\n-    >>> old_err = np.seterr(divide='raise')\n-    >>> old_handler = np.seterrcall(err_handler)\n-    >>> np.geterrobj()\n-    [20000, 522, <function err_handler at 0x...>]\n-\n-    >>> old_err = np.seterr(all='ignore')\n-    >>> np.base_repr(np.geterrobj()[1], 8)\n-    '0'\n-    >>> old_err = np.seterr(divide='warn', over='log', under='call',\n-    ...                     invalid='print')\n-    >>> np.base_repr(np.geterrobj()[1], 8)\n-    '4351'\n-    >>> old_errobj = np.seterrobj(orig_errobj)  # restore the error state\n-\n-    \"\"\")\n-\n-add_newdoc('numpy.core.umath', 'seterrobj',\n-    \"\"\"\n-    seterrobj(errobj, /)\n-\n-    Set the object that defines floating-point error handling.\n-\n-    The error object contains all information that defines the error handling\n-    behavior in NumPy. `seterrobj` is used internally by the other\n-    functions that set error handling behavior (`seterr`, `seterrcall`).\n-\n-    Parameters\n-    ----------\n-    errobj : list\n-        The error object, a list containing three elements:\n-        [internal numpy buffer size, error mask, error callback function].\n-\n-        The error mask is a single integer that holds the treatment information\n-        on all four floating point errors. The information for each error type\n-        is contained in three bits of the integer. If we print it in base 8, we\n-        can see what treatment is set for \"invalid\", \"under\", \"over\", and\n-        \"divide\" (in that order). The printed string can be interpreted with\n-\n-        * 0 : 'ignore'\n-        * 1 : 'warn'\n-        * 2 : 'raise'\n-        * 3 : 'call'\n-        * 4 : 'print'\n-        * 5 : 'log'\n-\n-    See Also\n-    --------\n-    geterrobj, seterr, geterr, seterrcall, geterrcall\n-    getbufsize, setbufsize\n-\n-    Notes\n-    -----\n-    For complete documentation of the types of floating-point exceptions and\n-    treatment options, see `seterr`.\n-\n-    Examples\n-    --------\n-    >>> orig_errobj = np.geterrobj()[:]  # get a copy of the errobj\n-    >>> orig_errobj\n-    [8192, 521, None]\n-\n-    >>> def err_handler(type, flag):\n-    ...     print(\"Floating point error (%s), with flag %s\" % (type, flag))\n-    ...\n-    >>> new_errobj = [20000, 12, err_handler]\n-    >>> np.seterrobj(new_errobj)\n-    >>> np.base_repr(12, 8)  # int for divide=4 ('print') and over=1 ('warn')\n-    '14'\n-    >>> np.geterr()\n-    {'divide': 'print', 'over': 'warn', 'under': 'ignore', 'invalid': 'ignore'}\n-    >>> np.geterrcall() is err_handler\n-    True\n-    >>> old_errobj = np.seterrobj(orig_errobj)  # restore the original state\n-\n-    \"\"\")\n-\n \n ##############################################################################\n #\n",
            "comment_added_diff": {},
            "comment_deleted_diff": {
                "4811": "    >>> orig_errobj = np.geterrobj()[:]  # get a copy of the errobj",
                "4831": "    >>> old_errobj = np.seterrobj(orig_errobj)  # restore the error state",
                "4876": "    >>> orig_errobj = np.geterrobj()[:]  # get a copy of the errobj",
                "4885": "    >>> np.base_repr(12, 8)  # int for divide=4 ('print') and over=1 ('warn')",
                "4891": "    >>> old_errobj = np.seterrobj(orig_errobj)  # restore the original state"
            },
            "comment_modified_diff": {}
        },
        {
            "commit": "b6b5d977ea340f96ac5f746eff5907f1b4a66615",
            "timestamp": "2023-06-19T22:12:51+02:00",
            "author": "Sebastian Berg",
            "commit_message": "DEP: Expire `set_numeric_ops` and the corresponding C functions deprecation\n\nThis simply expires the deprecations because I saw they were one cause\nof doctest failures in the other PR, but its good anyway.",
            "additions": 0,
            "deletions": 50,
            "change_type": "MODIFY",
            "diff": "@@ -1822,56 +1822,6 @@\n \n     \"\"\")\n \n-add_newdoc('numpy.core.multiarray', 'set_numeric_ops',\n-    \"\"\"\n-    set_numeric_ops(op1=func1, op2=func2, ...)\n-\n-    Set numerical operators for array objects.\n-\n-    .. deprecated:: 1.16\n-\n-        For the general case, use :c:func:`PyUFunc_ReplaceLoopBySignature`.\n-        For ndarray subclasses, define the ``__array_ufunc__`` method and\n-        override the relevant ufunc.\n-\n-    Parameters\n-    ----------\n-    op1, op2, ... : callable\n-        Each ``op = func`` pair describes an operator to be replaced.\n-        For example, ``add = lambda x, y: np.add(x, y) % 5`` would replace\n-        addition by modulus 5 addition.\n-\n-    Returns\n-    -------\n-    saved_ops : list of callables\n-        A list of all operators, stored before making replacements.\n-\n-    Notes\n-    -----\n-    .. warning::\n-       Use with care!  Incorrect usage may lead to memory errors.\n-\n-    A function replacing an operator cannot make use of that operator.\n-    For example, when replacing add, you may not use ``+``.  Instead,\n-    directly call ufuncs.\n-\n-    Examples\n-    --------\n-    >>> def add_mod5(x, y):\n-    ...     return np.add(x, y) % 5\n-    ...\n-    >>> old_funcs = np.set_numeric_ops(add=add_mod5)\n-\n-    >>> x = np.arange(12).reshape((3, 4))\n-    >>> x + x\n-    array([[0, 2, 4, 1],\n-           [3, 0, 2, 4],\n-           [1, 3, 0, 2]])\n-\n-    >>> ignore = np.set_numeric_ops(**old_funcs) # restore operators\n-\n-    \"\"\")\n-\n add_newdoc('numpy.core.multiarray', 'promote_types',\n     \"\"\"\n     promote_types(type1, type2)\n",
            "comment_added_diff": {},
            "comment_deleted_diff": {
                "1871": "    >>> ignore = np.set_numeric_ops(**old_funcs) # restore operators"
            },
            "comment_modified_diff": {}
        },
        {
            "commit": "cb176c1f8b453a923e65ddde9245cb6f1c8da47e",
            "timestamp": "2023-09-18T10:35:06-07:00",
            "author": "Raghuveer Devulapalli",
            "commit_message": "Add may vary to output of np.partition and np.argpartition in docs",
            "additions": 1,
            "deletions": 1,
            "change_type": "MODIFY",
            "diff": "@@ -4239,7 +4239,7 @@\n     >>> a = np.array([3, 4, 2, 1])\n     >>> a.partition(3)\n     >>> a\n-    array([2, 1, 3, 4])\n+    array([2, 1, 3, 4]) # may vary\n \n     >>> a.partition((1, 3))\n     >>> a\n",
            "comment_added_diff": {
                "4242": "    array([2, 1, 3, 4]) # may vary"
            },
            "comment_deleted_diff": {},
            "comment_modified_diff": {
                "4242": "    array([2, 1, 3, 4])"
            }
        }
    ],
    "function_base.py": [
        {
            "commit": "806a9ecb0d26fd2f08ef6bfe28dda21a8799f071",
            "timestamp": "2022-10-20T10:53:29-07:00",
            "author": "Ross Barnowski",
            "commit_message": "DOC: Update numpy/lib/function_base.py\r\n\r\nAdd date to deprecation in comment\n\nCo-authored-by: Matti Picus <matti.picus@gmail.com>",
            "additions": 1,
            "deletions": 0,
            "change_type": "MODIFY",
            "diff": "@@ -3678,6 +3678,7 @@ def msort(a):\n            [3, 4]])\n \n     \"\"\"\n+    # 2022-10-20 1.24\n     warnings.warn(\n         \"msort is deprecated, use np.sort(a, axis=0) instead\",\n         DeprecationWarning,\n",
            "comment_added_diff": {
                "3681": "    # 2022-10-20 1.24"
            },
            "comment_deleted_diff": {},
            "comment_modified_diff": {}
        },
        {
            "commit": "8334d57785997b6a3f6a3bae0ffea273632c65f1",
            "timestamp": "2023-01-20T13:22:56+01:00",
            "author": "Sebastian Berg",
            "commit_message": "API: Allow SciPy to get away with assuming `trapz` is a Python function\n\nThis wraps `trapz` into a proper python function and then copies all\nattributes expected on a Python function over from the \"fake\" version\nto the real one.\n\nThis allows SciPy to pretend `trapz` is a Python function to create\ntheir own version.",
            "additions": 19,
            "deletions": 0,
            "change_type": "MODIFY",
            "diff": "@@ -4912,6 +4912,25 @@ def trapz(y, x=None, dx=1.0, axis=-1):\n     return ret\n \n \n+if overrides.ARRAY_FUNCTION_ENABLED:\n+    # If array-function is enabled (normal), we wrap everything into a C\n+    # callable, which has no __code__ or other attributes normal Python funcs\n+    # have.  SciPy however, tries to \"clone\" `trapz` into a new Python function\n+    # which requires `__code__` and a few other attributes.\n+    # So we create a dummy clone and copy over its attributes allowing\n+    # SciPy <= 1.10 to work: https://github.com/scipy/scipy/issues/17811\n+    assert not hasattr(trapz, \"__code__\")\n+\n+    def _fake_trapz(y, x=None, dx=1.0, axis=-1):\n+        return trapz(y, x=x, dx=dx, axis=axis)\n+\n+    trapz.__code__ = _fake_trapz.__code__\n+    trapz.__globals__ = _fake_trapz.__globals__\n+    trapz.__defaults__ = _fake_trapz.__defaults__\n+    trapz.__closure__ = _fake_trapz.__closure__\n+    trapz.__kwdefaults__ = _fake_trapz.__kwdefaults__\n+\n+\n def _meshgrid_dispatcher(*xi, copy=None, sparse=None, indexing=None):\n     return xi\n \n",
            "comment_added_diff": {
                "4916": "    # If array-function is enabled (normal), we wrap everything into a C",
                "4917": "    # callable, which has no __code__ or other attributes normal Python funcs",
                "4918": "    # have.  SciPy however, tries to \"clone\" `trapz` into a new Python function",
                "4919": "    # which requires `__code__` and a few other attributes.",
                "4920": "    # So we create a dummy clone and copy over its attributes allowing",
                "4921": "    # SciPy <= 1.10 to work: https://github.com/scipy/scipy/issues/17811"
            },
            "comment_deleted_diff": {},
            "comment_modified_diff": {}
        },
        {
            "commit": "ade008bf5fba3cbc43ffbdf5ee261953a8a71a3a",
            "timestamp": "2023-01-21T20:20:30-05:00",
            "author": "Matteo Raso",
            "commit_message": "ENH: Enabled use of numpy.vectorize as decorator (#9477)\n\nMost of this code comes from PR-9593, but with the default value for pyfunc\nbeing numpy._NoValue instead of None, no override of __new__, and no effort\nto make subclassing possible.\n\nCo-Authored-By: Michael Lamparski <diagonaldevice@gmail.com>",
            "additions": 46,
            "deletions": 12,
            "change_type": "MODIFY",
            "diff": "@@ -2119,8 +2119,8 @@ def _create_arrays(broadcast_shape, dim_sizes, list_of_core_dims, dtypes,\n @set_module('numpy')\n class vectorize:\n     \"\"\"\n-    vectorize(pyfunc, otypes=None, doc=None, excluded=None, cache=False,\n-              signature=None)\n+    vectorize(pyfunc=np._NoValue, otypes=None, doc=None, excluded=None,\n+    cache=False, signature=None)\n \n     Generalized function class.\n \n@@ -2136,8 +2136,9 @@ class vectorize:\n \n     Parameters\n     ----------\n-    pyfunc : callable\n+    pyfunc : callable, optional\n         A python function or method.\n+        Can be omitted to produce a decorator with keyword arguments.\n     otypes : str or list of dtypes, optional\n         The output data type. It must be specified as either a string of\n         typecode characters or a list of data type specifiers. There should\n@@ -2169,8 +2170,9 @@ class vectorize:\n \n     Returns\n     -------\n-    vectorized : callable\n-        Vectorized function.\n+    out : callable\n+        A vectorized function if ``pyfunc`` was provided,\n+        a decorator otherwise.\n \n     See Also\n     --------\n@@ -2267,19 +2269,36 @@ class vectorize:\n            [0., 0., 1., 2., 1., 0.],\n            [0., 0., 0., 1., 2., 1.]])\n \n+    Decorator syntax is supported.  The decorator can be called as\n+    a function to provide keyword arguments.\n+    >>>@np.vectorize\n+    ...def identity(x):\n+    ...    return x\n+    ...\n+    >>>identity([0, 1, 2])\n+    array([0, 1, 2])\n+    >>>@np.vectorize(otypes=[float])\n+    ...def as_float(x):\n+    ...    return x\n+    ...\n+    >>>as_float([0, 1, 2])\n+    array([0., 1., 2.])\n     \"\"\"\n-    def __init__(self, pyfunc, otypes=None, doc=None, excluded=None,\n-                 cache=False, signature=None):\n+    def __init__(self, pyfunc=np._NoValue, otypes=None, doc=None,\n+                 excluded=None, cache=False, signature=None):\n         self.pyfunc = pyfunc\n         self.cache = cache\n         self.signature = signature\n-        self.__name__ = pyfunc.__name__\n-        self._ufunc = {}    # Caching to improve default performance\n+        if pyfunc != np._NoValue:\n+            self.__name__ = pyfunc.__name__\n \n+        self._ufunc = {}    # Caching to improve default performance\n+        self._doc = None\n+        self.__doc__ = doc\n         if doc is None:\n             self.__doc__ = pyfunc.__doc__\n         else:\n-            self.__doc__ = doc\n+            self._doc = doc\n \n         if isinstance(otypes, str):\n             for char in otypes:\n@@ -2301,7 +2320,15 @@ def __init__(self, pyfunc, otypes=None, doc=None, excluded=None,\n         else:\n             self._in_and_out_core_dims = None\n \n-    def __call__(self, *args, **kwargs):\n+    def _init_stage_2(self, pyfunc, *args, **kwargs):\n+        self.__name__ = pyfunc.__name__\n+        self.pyfunc = pyfunc\n+        if self._doc is None:\n+            self.__doc__ = pyfunc.__doc__\n+        else:\n+            self.__doc__ = self._doc\n+\n+    def _call_as_normal(self, *args, **kwargs):\n         \"\"\"\n         Return arrays with the results of `pyfunc` broadcast (vectorized) over\n         `args` and `kwargs` not in `excluded`.\n@@ -2331,6 +2358,13 @@ def func(*vargs):\n \n         return self._vectorize_call(func=func, args=vargs)\n \n+    def __call__(self, *args, **kwargs):\n+        if self.pyfunc is np._NoValue:\n+            self._init_stage_2(*args, **kwargs)\n+            return self\n+\n+        return self._call_as_normal(*args, **kwargs)\n+\n     def _get_ufunc_and_otypes(self, func, args):\n         \"\"\"Return (ufunc, otypes).\"\"\"\n         # frompyfunc will fail if args is empty\n@@ -2457,7 +2491,7 @@ def _vectorize_call_with_signature(self, func, args):\n \n             if outputs is None:\n                 for result, core_dims in zip(results, output_core_dims):\n-                    _update_dim_sizes(dim_sizes, result, core_dims)\n+                        _update_dim_sizes(dim_sizes, result, core_dims)\n \n                 outputs = _create_arrays(broadcast_shape, dim_sizes,\n                                          output_core_dims, otypes, results)\n",
            "comment_added_diff": {
                "2295": "        self._ufunc = {}    # Caching to improve default performance"
            },
            "comment_deleted_diff": {
                "2277": "        self._ufunc = {}    # Caching to improve default performance"
            },
            "comment_modified_diff": {}
        },
        {
            "commit": "4b82e29a18cd2c17b258bab0e2d937ab2157377b",
            "timestamp": "2023-02-09T22:59:46-05:00",
            "author": "Matteo Raso",
            "commit_message": "@vectorize now requires arguments to specify keywords\n\nThis reverses commit 7a2ded1522305",
            "additions": 6,
            "deletions": 17,
            "change_type": "MODIFY",
            "diff": "@@ -2287,23 +2287,12 @@ class vectorize:\n     def __init__(self, pyfunc=np._NoValue, otypes=None, doc=None,\n                  excluded=None, cache=False, signature=None):\n \n-        if not callable(pyfunc):\n-            p_temp = pyfunc\n-            pyfunc = np._NoValue\n-            if p_temp is not None and p_temp is not np._NoValue:\n-                o_temp = otypes\n-                otypes = p_temp\n-                if o_temp is not None:\n-                    d_temp = doc\n-                    doc = o_temp\n-                    if d_temp is not None:\n-                        e_temp = excluded\n-                        excluded = d_temp\n-                        if e_temp is True or e_temp is False:\n-                            c_temp = cache\n-                            cache = e_temp\n-                            if c_temp is not None:\n-                                signature = c_temp\n+        if (pyfunc != np._NoValue) and (not callable(pyfunc)):\n+            #Splitting the error message to keep\n+            #the length below 79 characters.\n+            part1 = \"When used as a decorator, \"\n+            part2 = \"only accepts keyword arguments.\"\n+            raise TypeError(part1 + part2)\n \n         self.pyfunc = pyfunc\n         self.cache = cache\n",
            "comment_added_diff": {
                "2291": "            #Splitting the error message to keep",
                "2292": "            #the length below 79 characters."
            },
            "comment_deleted_diff": {},
            "comment_modified_diff": {
                "2291": "            p_temp = pyfunc",
                "2292": "            pyfunc = np._NoValue"
            }
        },
        {
            "commit": "1da1663196c95b3811ca84d9e335f32cfeb05e32",
            "timestamp": "2023-03-12T22:31:28+00:00",
            "author": "Ralf Gommers",
            "commit_message": "MAINT: remove `NUMPY_EXPERIMENTAL_ARRAY_FUNCTION` env var\n\nAs discussed in\nhttps://mail.python.org/archives/list/numpy-discussion@python.org/thread/UKZJACAP5FUG7KP2AQDPE4P5ADNWLOHZ/\n\nThis flag was always meant to be temporary, and cleaning it up is\nlong overdue.",
            "additions": 16,
            "deletions": 17,
            "change_type": "MODIFY",
            "diff": "@@ -4910,23 +4910,22 @@ def trapz(y, x=None, dx=1.0, axis=-1):\n     return ret\n \n \n-if overrides.ARRAY_FUNCTION_ENABLED:\n-    # If array-function is enabled (normal), we wrap everything into a C\n-    # callable, which has no __code__ or other attributes normal Python funcs\n-    # have.  SciPy however, tries to \"clone\" `trapz` into a new Python function\n-    # which requires `__code__` and a few other attributes.\n-    # So we create a dummy clone and copy over its attributes allowing\n-    # SciPy <= 1.10 to work: https://github.com/scipy/scipy/issues/17811\n-    assert not hasattr(trapz, \"__code__\")\n-\n-    def _fake_trapz(y, x=None, dx=1.0, axis=-1):\n-        return trapz(y, x=x, dx=dx, axis=axis)\n-\n-    trapz.__code__ = _fake_trapz.__code__\n-    trapz.__globals__ = _fake_trapz.__globals__\n-    trapz.__defaults__ = _fake_trapz.__defaults__\n-    trapz.__closure__ = _fake_trapz.__closure__\n-    trapz.__kwdefaults__ = _fake_trapz.__kwdefaults__\n+# __array_function__ has no __code__ or other attributes normal Python funcs we\n+# wrap everything into a C callable. SciPy however, tries to \"clone\" `trapz`\n+# into a new Python function which requires `__code__` and a few other\n+# attributes. So we create a dummy clone and copy over its attributes allowing\n+# SciPy <= 1.10 to work: https://github.com/scipy/scipy/issues/17811\n+assert not hasattr(trapz, \"__code__\")\n+\n+def _fake_trapz(y, x=None, dx=1.0, axis=-1):\n+    return trapz(y, x=x, dx=dx, axis=axis)\n+\n+\n+trapz.__code__ = _fake_trapz.__code__\n+trapz.__globals__ = _fake_trapz.__globals__\n+trapz.__defaults__ = _fake_trapz.__defaults__\n+trapz.__closure__ = _fake_trapz.__closure__\n+trapz.__kwdefaults__ = _fake_trapz.__kwdefaults__\n \n \n def _meshgrid_dispatcher(*xi, copy=None, sparse=None, indexing=None):\n",
            "comment_added_diff": {
                "4913": "# __array_function__ has no __code__ or other attributes normal Python funcs we",
                "4914": "# wrap everything into a C callable. SciPy however, tries to \"clone\" `trapz`",
                "4915": "# into a new Python function which requires `__code__` and a few other",
                "4916": "# attributes. So we create a dummy clone and copy over its attributes allowing",
                "4917": "# SciPy <= 1.10 to work: https://github.com/scipy/scipy/issues/17811"
            },
            "comment_deleted_diff": {
                "4914": "    # If array-function is enabled (normal), we wrap everything into a C",
                "4915": "    # callable, which has no __code__ or other attributes normal Python funcs",
                "4916": "    # have.  SciPy however, tries to \"clone\" `trapz` into a new Python function",
                "4917": "    # which requires `__code__` and a few other attributes.",
                "4918": "    # So we create a dummy clone and copy over its attributes allowing",
                "4919": "    # SciPy <= 1.10 to work: https://github.com/scipy/scipy/issues/17811"
            },
            "comment_modified_diff": {
                "4913": "if overrides.ARRAY_FUNCTION_ENABLED:",
                "4914": "    # If array-function is enabled (normal), we wrap everything into a C",
                "4915": "    # callable, which has no __code__ or other attributes normal Python funcs",
                "4916": "    # have.  SciPy however, tries to \"clone\" `trapz` into a new Python function",
                "4917": "    # which requires `__code__` and a few other attributes."
            }
        },
        {
            "commit": "ed1732410f51293e4c5f63dcf162d9f1d417335a",
            "timestamp": "2023-03-27T09:36:25+03:00",
            "author": "Matti Picus",
            "commit_message": "Revert \"ENH: Enabled the use of numpy.vectorize as a decorator\"",
            "additions": 11,
            "deletions": 54,
            "change_type": "MODIFY",
            "diff": "@@ -2117,10 +2117,10 @@ def _create_arrays(broadcast_shape, dim_sizes, list_of_core_dims, dtypes,\n @set_module('numpy')\n class vectorize:\n     \"\"\"\n-    vectorize(pyfunc=np._NoValue, otypes=None, doc=None, excluded=None,\n-    cache=False, signature=None)\n+    vectorize(pyfunc, otypes=None, doc=None, excluded=None, cache=False,\n+              signature=None)\n \n-    Returns an object that acts like pyfunc, but takes arrays as input.\n+    Generalized function class.\n \n     Define a vectorized function which takes a nested sequence of objects or\n     numpy arrays as inputs and returns a single numpy array or a tuple of numpy\n@@ -2134,9 +2134,8 @@ class vectorize:\n \n     Parameters\n     ----------\n-    pyfunc : callable, optional\n+    pyfunc : callable\n         A python function or method.\n-        Can be omitted to produce a decorator with keyword arguments.\n     otypes : str or list of dtypes, optional\n         The output data type. It must be specified as either a string of\n         typecode characters or a list of data type specifiers. There should\n@@ -2168,9 +2167,8 @@ class vectorize:\n \n     Returns\n     -------\n-    out : callable\n-        A vectorized function if ``pyfunc`` was provided,\n-        a decorator otherwise.\n+    vectorized : callable\n+        Vectorized function.\n \n     See Also\n     --------\n@@ -2267,44 +2265,18 @@ class vectorize:\n            [0., 0., 1., 2., 1., 0.],\n            [0., 0., 0., 1., 2., 1.]])\n \n-    Decorator syntax is supported.  The decorator can be called as\n-    a function to provide keyword arguments.\n-    >>>@np.vectorize\n-    ...def identity(x):\n-    ...    return x\n-    ...\n-    >>>identity([0, 1, 2])\n-    array([0, 1, 2])\n-    >>>@np.vectorize(otypes=[float])\n-    ...def as_float(x):\n-    ...    return x\n-    ...\n-    >>>as_float([0, 1, 2])\n-    array([0., 1., 2.])\n     \"\"\"\n-    def __init__(self, pyfunc=np._NoValue, otypes=None, doc=None,\n-                 excluded=None, cache=False, signature=None):\n-\n-        if (pyfunc != np._NoValue) and (not callable(pyfunc)):\n-            #Splitting the error message to keep\n-            #the length below 79 characters.\n-            part1 = \"When used as a decorator, \"\n-            part2 = \"only accepts keyword arguments.\"\n-            raise TypeError(part1 + part2)\n-\n+    def __init__(self, pyfunc, otypes=None, doc=None, excluded=None,\n+                 cache=False, signature=None):\n         self.pyfunc = pyfunc\n         self.cache = cache\n         self.signature = signature\n-        if pyfunc != np._NoValue:\n-            self.__name__ = pyfunc.__name__\n-\n         self._ufunc = {}    # Caching to improve default performance\n-        self._doc = None\n-        self.__doc__ = doc\n+\n         if doc is None:\n             self.__doc__ = pyfunc.__doc__\n         else:\n-            self._doc = doc\n+            self.__doc__ = doc\n \n         if isinstance(otypes, str):\n             for char in otypes:\n@@ -2326,15 +2298,7 @@ def __init__(self, pyfunc=np._NoValue, otypes=None, doc=None,\n         else:\n             self._in_and_out_core_dims = None\n \n-    def _init_stage_2(self, pyfunc, *args, **kwargs):\n-        self.__name__ = pyfunc.__name__\n-        self.pyfunc = pyfunc\n-        if self._doc is None:\n-            self.__doc__ = pyfunc.__doc__\n-        else:\n-            self.__doc__ = self._doc\n-\n-    def _call_as_normal(self, *args, **kwargs):\n+    def __call__(self, *args, **kwargs):\n         \"\"\"\n         Return arrays with the results of `pyfunc` broadcast (vectorized) over\n         `args` and `kwargs` not in `excluded`.\n@@ -2364,13 +2328,6 @@ def func(*vargs):\n \n         return self._vectorize_call(func=func, args=vargs)\n \n-    def __call__(self, *args, **kwargs):\n-        if self.pyfunc is np._NoValue:\n-            self._init_stage_2(*args, **kwargs)\n-            return self\n-\n-        return self._call_as_normal(*args, **kwargs)\n-\n     def _get_ufunc_and_otypes(self, func, args):\n         \"\"\"Return (ufunc, otypes).\"\"\"\n         # frompyfunc will fail if args is empty\n",
            "comment_added_diff": {},
            "comment_deleted_diff": {
                "2289": "            #Splitting the error message to keep",
                "2290": "            #the length below 79 characters."
            },
            "comment_modified_diff": {}
        },
        {
            "commit": "dfaa72d72453b8738ec711180e03da824651e46b",
            "timestamp": "2023-04-01T21:45:16-04:00",
            "author": "Matteo Raso",
            "commit_message": "Fixed edge case where pyfunc has no attribute `__name__`",
            "additions": 66,
            "deletions": 39,
            "change_type": "MODIFY",
            "diff": "@@ -24,7 +24,7 @@\n from numpy.core.function_base import add_newdoc\n from numpy.lib.twodim_base import diag\n from numpy.core.multiarray import (\n-    _place, add_docstring, bincount, normalize_axis_index, _monotonicity,\n+    _insert, add_docstring, bincount, normalize_axis_index, _monotonicity,\n     interp as compiled_interp, interp_complex as compiled_interp_complex\n     )\n from numpy.core.umath import _add_newdoc_ufunc as add_newdoc_ufunc\n@@ -1311,8 +1311,6 @@ def gradient(f, *varargs, axis=None, edge_order=1):\n \n     if len_axes == 1:\n         return outvals[0]\n-    elif np._using_numpy2_behavior():\n-        return tuple(outvals)\n     else:\n         return outvals\n \n@@ -1951,7 +1949,11 @@ def place(arr, mask, vals):\n            [44, 55, 44]])\n \n     \"\"\"\n-    return _place(arr, mask, vals)\n+    if not isinstance(arr, np.ndarray):\n+        raise TypeError(\"argument 1 must be numpy.ndarray, \"\n+                        \"not {name}\".format(name=type(arr).__name__))\n+\n+    return _insert(arr, mask, vals)\n \n \n def disp(mesg, device=None, linefeed=True):\n@@ -2117,10 +2119,10 @@ def _create_arrays(broadcast_shape, dim_sizes, list_of_core_dims, dtypes,\n @set_module('numpy')\n class vectorize:\n     \"\"\"\n-    vectorize(pyfunc, otypes=None, doc=None, excluded=None, cache=False,\n-              signature=None)\n+    vectorize(pyfunc=np._NoValue, otypes=None, doc=None, excluded=None,\n+    cache=False, signature=None)\n \n-    Generalized function class.\n+    Returns an object that acts like pyfunc, but takes arrays as input.\n \n     Define a vectorized function which takes a nested sequence of objects or\n     numpy arrays as inputs and returns a single numpy array or a tuple of numpy\n@@ -2134,8 +2136,9 @@ class vectorize:\n \n     Parameters\n     ----------\n-    pyfunc : callable\n+    pyfunc : callable, optional\n         A python function or method.\n+        Can be omitted to produce a decorator with keyword arguments.\n     otypes : str or list of dtypes, optional\n         The output data type. It must be specified as either a string of\n         typecode characters or a list of data type specifiers. There should\n@@ -2167,8 +2170,9 @@ class vectorize:\n \n     Returns\n     -------\n-    vectorized : callable\n-        Vectorized function.\n+    out : callable\n+        A vectorized function if ``pyfunc`` was provided,\n+        a decorator otherwise.\n \n     See Also\n     --------\n@@ -2265,18 +2269,44 @@ class vectorize:\n            [0., 0., 1., 2., 1., 0.],\n            [0., 0., 0., 1., 2., 1.]])\n \n+    Decorator syntax is supported.  The decorator can be called as\n+    a function to provide keyword arguments.\n+    >>>@np.vectorize\n+    ...def identity(x):\n+    ...    return x\n+    ...\n+    >>>identity([0, 1, 2])\n+    array([0, 1, 2])\n+    >>>@np.vectorize(otypes=[float])\n+    ...def as_float(x):\n+    ...    return x\n+    ...\n+    >>>as_float([0, 1, 2])\n+    array([0., 1., 2.])\n     \"\"\"\n-    def __init__(self, pyfunc, otypes=None, doc=None, excluded=None,\n-                 cache=False, signature=None):\n+    def __init__(self, pyfunc=np._NoValue, otypes=None, doc=None,\n+                 excluded=None, cache=False, signature=None):\n+\n+        if (pyfunc != np._NoValue) and (not callable(pyfunc)):\n+            #Splitting the error message to keep\n+            #the length below 79 characters.\n+            part1 = \"When used as a decorator, \"\n+            part2 = \"only accepts keyword arguments.\"\n+            raise TypeError(part1 + part2)\n+\n         self.pyfunc = pyfunc\n         self.cache = cache\n         self.signature = signature\n-        self._ufunc = {}    # Caching to improve default performance\n+        if pyfunc != np._NoValue and hasattr(pyfunc, '__name__'):\n+            self.__name__ = pyfunc.__name__\n \n+        self._ufunc = {}    # Caching to improve default performance\n+        self._doc = None\n+        self.__doc__ = doc\n         if doc is None:\n             self.__doc__ = pyfunc.__doc__\n         else:\n-            self.__doc__ = doc\n+            self._doc = doc\n \n         if isinstance(otypes, str):\n             for char in otypes:\n@@ -2298,7 +2328,15 @@ def __init__(self, pyfunc, otypes=None, doc=None, excluded=None,\n         else:\n             self._in_and_out_core_dims = None\n \n-    def __call__(self, *args, **kwargs):\n+    def _init_stage_2(self, pyfunc, *args, **kwargs):\n+        self.__name__ = pyfunc.__name__\n+        self.pyfunc = pyfunc\n+        if self._doc is None:\n+            self.__doc__ = pyfunc.__doc__\n+        else:\n+            self.__doc__ = self._doc\n+\n+    def _call_as_normal(self, *args, **kwargs):\n         \"\"\"\n         Return arrays with the results of `pyfunc` broadcast (vectorized) over\n         `args` and `kwargs` not in `excluded`.\n@@ -2328,6 +2366,13 @@ def func(*vargs):\n \n         return self._vectorize_call(func=func, args=vargs)\n \n+    def __call__(self, *args, **kwargs):\n+        if self.pyfunc is np._NoValue:\n+            self._init_stage_2(*args, **kwargs)\n+            return self\n+\n+        return self._call_as_normal(*args, **kwargs)\n+\n     def _get_ufunc_and_otypes(self, func, args):\n         \"\"\"Return (ufunc, otypes).\"\"\"\n         # frompyfunc will fail if args is empty\n@@ -2693,7 +2738,7 @@ def cov(m, y=None, rowvar=True, bias=False, ddof=None, fweights=None,\n \n     if fact <= 0:\n         warnings.warn(\"Degrees of freedom <= 0 for slice\",\n-                      RuntimeWarning, stacklevel=2)\n+                      RuntimeWarning, stacklevel=3)\n         fact = 0.0\n \n     X -= avg[:, None]\n@@ -2842,7 +2887,7 @@ def corrcoef(x, y=None, rowvar=True, bias=np._NoValue, ddof=np._NoValue, *,\n     if bias is not np._NoValue or ddof is not np._NoValue:\n         # 2015-03-15, 1.10\n         warnings.warn('bias and ddof have no effect and are deprecated',\n-                      DeprecationWarning, stacklevel=2)\n+                      DeprecationWarning, stacklevel=3)\n     c = cov(x, y, rowvar, dtype=dtype)\n     try:\n         d = diag(c)\n@@ -3682,7 +3727,7 @@ def msort(a):\n     warnings.warn(\n         \"msort is deprecated, use np.sort(a, axis=0) instead\",\n         DeprecationWarning,\n-        stacklevel=2,\n+        stacklevel=3,\n     )\n     b = array(a, subok=True, copy=True)\n     b.sort(0)\n@@ -4910,24 +4955,6 @@ def trapz(y, x=None, dx=1.0, axis=-1):\n     return ret\n \n \n-# __array_function__ has no __code__ or other attributes normal Python funcs we\n-# wrap everything into a C callable. SciPy however, tries to \"clone\" `trapz`\n-# into a new Python function which requires `__code__` and a few other\n-# attributes. So we create a dummy clone and copy over its attributes allowing\n-# SciPy <= 1.10 to work: https://github.com/scipy/scipy/issues/17811\n-assert not hasattr(trapz, \"__code__\")\n-\n-def _fake_trapz(y, x=None, dx=1.0, axis=-1):\n-    return trapz(y, x=x, dx=dx, axis=axis)\n-\n-\n-trapz.__code__ = _fake_trapz.__code__\n-trapz.__globals__ = _fake_trapz.__globals__\n-trapz.__defaults__ = _fake_trapz.__defaults__\n-trapz.__closure__ = _fake_trapz.__closure__\n-trapz.__kwdefaults__ = _fake_trapz.__kwdefaults__\n-\n-\n def _meshgrid_dispatcher(*xi, copy=None, sparse=None, indexing=None):\n     return xi\n \n@@ -4936,7 +4963,7 @@ def _meshgrid_dispatcher(*xi, copy=None, sparse=None, indexing=None):\n @array_function_dispatch(_meshgrid_dispatcher)\n def meshgrid(*xi, copy=True, sparse=False, indexing='xy'):\n     \"\"\"\n-    Return a list of coordinate matrices from coordinate vectors.\n+    Return coordinate matrices from coordinate vectors.\n \n     Make N-D coordinate arrays for vectorized evaluations of\n     N-D scalar/vector fields over N-D grids, given\n@@ -4977,7 +5004,7 @@ def meshgrid(*xi, copy=True, sparse=False, indexing='xy'):\n \n     Returns\n     -------\n-    X1, X2,..., XN : list of ndarrays\n+    X1, X2,..., XN : ndarray\n         For vectors `x1`, `x2`,..., `xn` with lengths ``Ni=len(xi)``,\n         returns ``(N1, N2, N3,..., Nn)`` shaped arrays if indexing='ij'\n         or ``(N2, N1, N3,..., Nn)`` shaped arrays if indexing='xy'\n@@ -5414,7 +5441,7 @@ def insert(arr, obj, values, axis=None):\n             warnings.warn(\n                 \"in the future insert will treat boolean arrays and \"\n                 \"array-likes as a boolean index instead of casting it to \"\n-                \"integer\", FutureWarning, stacklevel=2)\n+                \"integer\", FutureWarning, stacklevel=3)\n             indices = indices.astype(intp)\n             # Code after warning period:\n             #if obj.ndim != 1:\n",
            "comment_added_diff": {
                "2291": "            #Splitting the error message to keep",
                "2292": "            #the length below 79 characters.",
                "2303": "        self._ufunc = {}    # Caching to improve default performance"
            },
            "comment_deleted_diff": {
                "2274": "        self._ufunc = {}    # Caching to improve default performance",
                "4913": "# __array_function__ has no __code__ or other attributes normal Python funcs we",
                "4914": "# wrap everything into a C callable. SciPy however, tries to \"clone\" `trapz`",
                "4915": "# into a new Python function which requires `__code__` and a few other",
                "4916": "# attributes. So we create a dummy clone and copy over its attributes allowing",
                "4917": "# SciPy <= 1.10 to work: https://github.com/scipy/scipy/issues/17811"
            },
            "comment_modified_diff": {}
        },
        {
            "commit": "f283fe07eb302df130c5dafaf07d06a8fb9c34fd",
            "timestamp": "2023-04-01T23:11:26-04:00",
            "author": "Matteo Raso",
            "commit_message": "Fixed failing tests",
            "additions": 28,
            "deletions": 12,
            "change_type": "MODIFY",
            "diff": "@@ -24,7 +24,7 @@\n from numpy.core.function_base import add_newdoc\n from numpy.lib.twodim_base import diag\n from numpy.core.multiarray import (\n-    _insert, add_docstring, bincount, normalize_axis_index, _monotonicity,\n+    _place, add_docstring, bincount, normalize_axis_index, _monotonicity,\n     interp as compiled_interp, interp_complex as compiled_interp_complex\n     )\n from numpy.core.umath import _add_newdoc_ufunc as add_newdoc_ufunc\n@@ -1311,6 +1311,8 @@ def gradient(f, *varargs, axis=None, edge_order=1):\n \n     if len_axes == 1:\n         return outvals[0]\n+    elif np._using_numpy2_behavior():\n+        return tuple(outvals)\n     else:\n         return outvals\n \n@@ -1949,11 +1951,7 @@ def place(arr, mask, vals):\n            [44, 55, 44]])\n \n     \"\"\"\n-    if not isinstance(arr, np.ndarray):\n-        raise TypeError(\"argument 1 must be numpy.ndarray, \"\n-                        \"not {name}\".format(name=type(arr).__name__))\n-\n-    return _insert(arr, mask, vals)\n+    return _place(arr, mask, vals)\n \n \n def disp(mesg, device=None, linefeed=True):\n@@ -2738,7 +2736,7 @@ def cov(m, y=None, rowvar=True, bias=False, ddof=None, fweights=None,\n \n     if fact <= 0:\n         warnings.warn(\"Degrees of freedom <= 0 for slice\",\n-                      RuntimeWarning, stacklevel=3)\n+                      RuntimeWarning, stacklevel=2)\n         fact = 0.0\n \n     X -= avg[:, None]\n@@ -2887,7 +2885,7 @@ def corrcoef(x, y=None, rowvar=True, bias=np._NoValue, ddof=np._NoValue, *,\n     if bias is not np._NoValue or ddof is not np._NoValue:\n         # 2015-03-15, 1.10\n         warnings.warn('bias and ddof have no effect and are deprecated',\n-                      DeprecationWarning, stacklevel=3)\n+                      DeprecationWarning, stacklevel=2)\n     c = cov(x, y, rowvar, dtype=dtype)\n     try:\n         d = diag(c)\n@@ -3727,7 +3725,7 @@ def msort(a):\n     warnings.warn(\n         \"msort is deprecated, use np.sort(a, axis=0) instead\",\n         DeprecationWarning,\n-        stacklevel=3,\n+        stacklevel=2,\n     )\n     b = array(a, subok=True, copy=True)\n     b.sort(0)\n@@ -4955,6 +4953,24 @@ def trapz(y, x=None, dx=1.0, axis=-1):\n     return ret\n \n \n+# __array_function__ has no __code__ or other attributes normal Python funcs we\n+# wrap everything into a C callable. SciPy however, tries to \"clone\" `trapz`\n+# into a new Python function which requires `__code__` and a few other\n+# attributes. So we create a dummy clone and copy over its attributes allowing\n+# SciPy <= 1.10 to work: https://github.com/scipy/scipy/issues/17811\n+assert not hasattr(trapz, \"__code__\")\n+\n+def _fake_trapz(y, x=None, dx=1.0, axis=-1):\n+    return trapz(y, x=x, dx=dx, axis=axis)\n+\n+\n+trapz.__code__ = _fake_trapz.__code__\n+trapz.__globals__ = _fake_trapz.__globals__\n+trapz.__defaults__ = _fake_trapz.__defaults__\n+trapz.__closure__ = _fake_trapz.__closure__\n+trapz.__kwdefaults__ = _fake_trapz.__kwdefaults__\n+\n+\n def _meshgrid_dispatcher(*xi, copy=None, sparse=None, indexing=None):\n     return xi\n \n@@ -4963,7 +4979,7 @@ def _meshgrid_dispatcher(*xi, copy=None, sparse=None, indexing=None):\n @array_function_dispatch(_meshgrid_dispatcher)\n def meshgrid(*xi, copy=True, sparse=False, indexing='xy'):\n     \"\"\"\n-    Return coordinate matrices from coordinate vectors.\n+    Return a list of coordinate matrices from coordinate vectors.\n \n     Make N-D coordinate arrays for vectorized evaluations of\n     N-D scalar/vector fields over N-D grids, given\n@@ -5004,7 +5020,7 @@ def meshgrid(*xi, copy=True, sparse=False, indexing='xy'):\n \n     Returns\n     -------\n-    X1, X2,..., XN : ndarray\n+    X1, X2,..., XN : list of ndarrays\n         For vectors `x1`, `x2`,..., `xn` with lengths ``Ni=len(xi)``,\n         returns ``(N1, N2, N3,..., Nn)`` shaped arrays if indexing='ij'\n         or ``(N2, N1, N3,..., Nn)`` shaped arrays if indexing='xy'\n@@ -5441,7 +5457,7 @@ def insert(arr, obj, values, axis=None):\n             warnings.warn(\n                 \"in the future insert will treat boolean arrays and \"\n                 \"array-likes as a boolean index instead of casting it to \"\n-                \"integer\", FutureWarning, stacklevel=3)\n+                \"integer\", FutureWarning, stacklevel=2)\n             indices = indices.astype(intp)\n             # Code after warning period:\n             #if obj.ndim != 1:\n",
            "comment_added_diff": {
                "4956": "# __array_function__ has no __code__ or other attributes normal Python funcs we",
                "4957": "# wrap everything into a C callable. SciPy however, tries to \"clone\" `trapz`",
                "4958": "# into a new Python function which requires `__code__` and a few other",
                "4959": "# attributes. So we create a dummy clone and copy over its attributes allowing",
                "4960": "# SciPy <= 1.10 to work: https://github.com/scipy/scipy/issues/17811"
            },
            "comment_deleted_diff": {},
            "comment_modified_diff": {}
        },
        {
            "commit": "89486a335a478aac46be91b92e69267f9409b1be",
            "timestamp": "2023-05-03T12:44:59+02:00",
            "author": "Sebastian Berg",
            "commit_message": "MAINT: Reorganize the way windowing functions ensure float64 result\n\nThis roughly changes things so that we ensure a float64 working\nvalues up-front.  There is a tiny chance of precision changes if the\ninput was not float64 or error changes on bad input.\n\nI don't think this should matter in practice, precision changes\n(as far as I can tell) should happen rather the other way around.\n\nSince float64 has 53bits mantissa, I think the arange should give\nthe correct result reliably for any sensible inputs.\n\nThere is an argument to be made that the windowing functions could\nreturn float32 for float32 input, but I somewhat think this is OK\nand users can be expected to just cast manually after the fact.\n\nThe result type is tested, but this ensures the tests pass also\nwhen enabling weak promotion.",
            "additions": 38,
            "deletions": 10,
            "change_type": "MODIFY",
            "diff": "@@ -2999,10 +2999,15 @@ def blackman(M):\n     >>> plt.show()\n \n     \"\"\"\n+    # Ensures at least float64 via 0.0.  M should be an integer, but conversion\n+    # to double is safe for a range.\n+    values = np.array([0.0, M])\n+    M = values[1]\n+\n     if M < 1:\n-        return array([], dtype=np.result_type(M, 0.0))\n+        return array([], dtype=values.dtype)\n     if M == 1:\n-        return ones(1, dtype=np.result_type(M, 0.0))\n+        return ones(1, dtype=values.dtype)\n     n = arange(1-M, M, 2)\n     return 0.42 + 0.5*cos(pi*n/(M-1)) + 0.08*cos(2.0*pi*n/(M-1))\n \n@@ -3107,10 +3112,15 @@ def bartlett(M):\n     >>> plt.show()\n \n     \"\"\"\n+    # Ensures at least float64 via 0.0.  M should be an integer, but conversion\n+    # to double is safe for a range.\n+    values = np.array([0.0, M])\n+    M = values[1]\n+\n     if M < 1:\n-        return array([], dtype=np.result_type(M, 0.0))\n+        return array([], dtype=values.dtype)\n     if M == 1:\n-        return ones(1, dtype=np.result_type(M, 0.0))\n+        return ones(1, dtype=values.dtype)\n     n = arange(1-M, M, 2)\n     return where(less_equal(n, 0), 1 + n/(M-1), 1 - n/(M-1))\n \n@@ -3211,10 +3221,15 @@ def hanning(M):\n     >>> plt.show()\n \n     \"\"\"\n+    # Ensures at least float64 via 0.0.  M should be an integer, but conversion\n+    # to double is safe for a range.\n+    values = np.array([0.0, M])\n+    M = values[1]\n+\n     if M < 1:\n-        return array([], dtype=np.result_type(M, 0.0))\n+        return array([], dtype=values.dtype)\n     if M == 1:\n-        return ones(1, dtype=np.result_type(M, 0.0))\n+        return ones(1, dtype=values.dtype)\n     n = arange(1-M, M, 2)\n     return 0.5 + 0.5*cos(pi*n/(M-1))\n \n@@ -3311,10 +3326,15 @@ def hamming(M):\n     >>> plt.show()\n \n     \"\"\"\n+    # Ensures at least float64 via 0.0.  M should be an integer, but conversion\n+    # to double is safe for a range.\n+    values = np.array([0.0, M])\n+    M = values[1]\n+\n     if M < 1:\n-        return array([], dtype=np.result_type(M, 0.0))\n+        return array([], dtype=values.dtype)\n     if M == 1:\n-        return ones(1, dtype=np.result_type(M, 0.0))\n+        return ones(1, dtype=values.dtype)\n     n = arange(1-M, M, 2)\n     return 0.54 + 0.46*cos(pi*n/(M-1))\n \n@@ -3590,11 +3610,19 @@ def kaiser(M, beta):\n     >>> plt.show()\n \n     \"\"\"\n+    # Ensures at least float64 via 0.0.  M should be an integer, but conversion\n+    # to double is safe for a range.  (Simplified result_type with 0.0\n+    # strongly typed.  result-type is not/less order sensitive, but that mainly\n+    # matters for integers anyway.)\n+    values = np.array([0.0, M, beta])\n+    M = values[1]\n+    beta = values[2]\n+\n     if M == 1:\n-        return np.ones(1, dtype=np.result_type(M, 0.0))\n+        return np.ones(1, dtype=values.dtype)\n     n = arange(0, M)\n     alpha = (M-1)/2.0\n-    return i0(beta * sqrt(1-((n-alpha)/alpha)**2.0))/i0(float(beta))\n+    return i0(beta * sqrt(1-((n-alpha)/alpha)**2.0))/i0(beta)\n \n \n def _sinc_dispatcher(x):\n",
            "comment_added_diff": {
                "3002": "    # Ensures at least float64 via 0.0.  M should be an integer, but conversion",
                "3003": "    # to double is safe for a range.",
                "3115": "    # Ensures at least float64 via 0.0.  M should be an integer, but conversion",
                "3116": "    # to double is safe for a range.",
                "3224": "    # Ensures at least float64 via 0.0.  M should be an integer, but conversion",
                "3225": "    # to double is safe for a range.",
                "3329": "    # Ensures at least float64 via 0.0.  M should be an integer, but conversion",
                "3330": "    # to double is safe for a range.",
                "3613": "    # Ensures at least float64 via 0.0.  M should be an integer, but conversion",
                "3614": "    # to double is safe for a range.  (Simplified result_type with 0.0",
                "3615": "    # strongly typed.  result-type is not/less order sensitive, but that mainly",
                "3616": "    # matters for integers anyway.)"
            },
            "comment_deleted_diff": {},
            "comment_modified_diff": {
                "3003": "        return array([], dtype=np.result_type(M, 0.0))"
            }
        },
        {
            "commit": "6ac4d6ded2dded576a5af12820fa49a961130468",
            "timestamp": "2023-05-17T14:03:39+02:00",
            "author": "Sebastian Berg",
            "commit_message": "BUG: Fix median and quantile NaT handling\n\nNote that this doesn't mean that rounding is correct at least for\nquantiles, so there is some dubious about it being a good idea to\nuse this.\n\nBut, it does fix the issue, and I the `copyto` solution seems rather\ngood to me, the only thing that isn't ideal is the `supports_nan`\ndefinition itself.\n\nCloses gh-20376",
            "additions": 22,
            "deletions": 19,
            "change_type": "MODIFY",
            "diff": "@@ -3943,8 +3943,10 @@ def _median(a, axis=None, out=None, overwrite_input=False):\n         kth = [szh - 1, szh]\n     else:\n         kth = [(sz - 1) // 2]\n-    # Check if the array contains any nan's\n-    if np.issubdtype(a.dtype, np.inexact):\n+\n+    # We have to check for NaNs (as of writing 'M' doesn't actually work).\n+    supports_nans = np.issubdtype(a.dtype, np.inexact) or a.dtype.kind in 'Mm'\n+    if supports_nans:\n         kth.append(-1)\n \n     if overwrite_input:\n@@ -3975,8 +3977,7 @@ def _median(a, axis=None, out=None, overwrite_input=False):\n     # Use mean in both odd and even case to coerce data type,\n     # using out array if needed.\n     rout = mean(part[indexer], axis=axis, out=out)\n-    # Check if the array contains any nan's\n-    if np.issubdtype(a.dtype, np.inexact) and sz > 0:\n+    if supports_nans and sz > 0:\n         # If nans are possible, warn and replace by nans like mean would.\n         rout = np.lib.utils._median_nancheck(part, rout, axis)\n \n@@ -4784,9 +4785,9 @@ def _quantile(\n     values_count = arr.shape[axis]\n     # The dimensions of `q` are prepended to the output shape, so we need the\n     # axis being sampled from `arr` to be last.\n-    DATA_AXIS = 0\n-    if axis != DATA_AXIS:  # But moveaxis is slow, so only call it if axis!=0.\n-        arr = np.moveaxis(arr, axis, destination=DATA_AXIS)\n+\n+    if axis != 0:  # But moveaxis is slow, so only call it if necessary.\n+        arr = np.moveaxis(arr, axis, destination=0)\n     # --- Computation of indexes\n     # Index where to find the value in the sorted array.\n     # Virtual because it is a floating point value, not an valid index.\n@@ -4799,12 +4800,16 @@ def _quantile(\n             f\"{_QuantileMethods.keys()}\") from None\n     virtual_indexes = method[\"get_virtual_index\"](values_count, quantiles)\n     virtual_indexes = np.asanyarray(virtual_indexes)\n+\n+    supports_nans = (\n+            np.issubdtype(arr.dtype, np.inexact) or arr.dtype.kind in 'Mm')\n+\n     if np.issubdtype(virtual_indexes.dtype, np.integer):\n         # No interpolation needed, take the points along axis\n-        if np.issubdtype(arr.dtype, np.inexact):\n+        if supports_nans:\n             # may contain nan, which would sort to the end\n             arr.partition(concatenate((virtual_indexes.ravel(), [-1])), axis=0)\n-            slices_having_nans = np.isnan(arr[-1])\n+            slices_having_nans = np.isnan(arr[-1, ...])\n         else:\n             # cannot contain nan\n             arr.partition(virtual_indexes.ravel(), axis=0)\n@@ -4820,16 +4825,14 @@ def _quantile(\n                                       previous_indexes.ravel(),\n                                       next_indexes.ravel(),\n                                       ))),\n-            axis=DATA_AXIS)\n-        if np.issubdtype(arr.dtype, np.inexact):\n-            slices_having_nans = np.isnan(\n-                take(arr, indices=-1, axis=DATA_AXIS)\n-            )\n+            axis=0)\n+        if supports_nans:\n+            slices_having_nans = np.isnan(arr[-1, ...])\n         else:\n             slices_having_nans = None\n         # --- Get values from indexes\n-        previous = np.take(arr, previous_indexes, axis=DATA_AXIS)\n-        next = np.take(arr, next_indexes, axis=DATA_AXIS)\n+        previous = arr[previous_indexes]\n+        next = arr[next_indexes]\n         # --- Linear interpolation\n         gamma = _get_gamma(virtual_indexes, previous_indexes, method)\n         result_shape = virtual_indexes.shape + (1,) * (arr.ndim - 1)\n@@ -4840,10 +4843,10 @@ def _quantile(\n                        out=out)\n     if np.any(slices_having_nans):\n         if result.ndim == 0 and out is None:\n-            # can't write to a scalar\n-            result = arr.dtype.type(np.nan)\n+            # can't write to a scalar, but indexing will be correct\n+            result = arr[-1]\n         else:\n-            result[..., slices_having_nans] = np.nan\n+            np.copyto(result, arr[-1, ...], where=slices_having_nans)\n     return result\n \n \n",
            "comment_added_diff": {
                "3947": "    # We have to check for NaNs (as of writing 'M' doesn't actually work).",
                "4789": "    if axis != 0:  # But moveaxis is slow, so only call it if necessary.",
                "4846": "            # can't write to a scalar, but indexing will be correct"
            },
            "comment_deleted_diff": {
                "3946": "    # Check if the array contains any nan's",
                "3978": "    # Check if the array contains any nan's",
                "4788": "    if axis != DATA_AXIS:  # But moveaxis is slow, so only call it if axis!=0.",
                "4843": "            # can't write to a scalar"
            },
            "comment_modified_diff": {
                "3947": "    if np.issubdtype(a.dtype, np.inexact):",
                "4789": "        arr = np.moveaxis(arr, axis, destination=DATA_AXIS)",
                "4846": "            result[..., slices_having_nans] = np.nan"
            }
        },
        {
            "commit": "11beedfd9906a6359f34e7d4c1f79d85fa8833d3",
            "timestamp": "2023-05-25T17:54:40-07:00",
            "author": "Brigitta Sip\u0151cz",
            "commit_message": "DOC: switching to use the plot directive",
            "additions": 121,
            "deletions": 158,
            "change_type": "MODIFY",
            "diff": "@@ -2964,39 +2964,32 @@ def blackman(M):\n             9.67046769e-01,   7.36045180e-01,   4.14397981e-01,\n             1.59903635e-01,   3.26064346e-02,  -1.38777878e-17])\n \n-    Plot the window and the frequency response:\n+    Plot the window and the frequency response.\n \n-    >>> from numpy.fft import fft, fftshift\n-    >>> window = np.blackman(51)\n-    >>> plt.plot(window)\n-    [<matplotlib.lines.Line2D object at 0x...>]\n-    >>> plt.title(\"Blackman window\")\n-    Text(0.5, 1.0, 'Blackman window')\n-    >>> plt.ylabel(\"Amplitude\")\n-    Text(0, 0.5, 'Amplitude')\n-    >>> plt.xlabel(\"Sample\")\n-    Text(0.5, 0, 'Sample')\n-    >>> plt.show()\n-\n-    >>> plt.figure()\n-    <Figure size 640x480 with 0 Axes>\n-    >>> A = fft(window, 2048) / 25.5\n-    >>> mag = np.abs(fftshift(A))\n-    >>> freq = np.linspace(-0.5, 0.5, len(A))\n-    >>> with np.errstate(divide='ignore', invalid='ignore'):\n-    ...     response = 20 * np.log10(mag)\n-    ...\n-    >>> response = np.clip(response, -100, 100)\n-    >>> plt.plot(freq, response)\n-    [<matplotlib.lines.Line2D object at 0x...>]\n-    >>> plt.title(\"Frequency response of Blackman window\")\n-    Text(0.5, 1.0, 'Frequency response of Blackman window')\n-    >>> plt.ylabel(\"Magnitude [dB]\")\n-    Text(0, 0.5, 'Magnitude [dB]')\n-    >>> plt.xlabel(\"Normalized frequency [cycles per sample]\")\n-    Text(0.5, 0, 'Normalized frequency [cycles per sample]')\n-    >>> _ = plt.axis('tight')\n-    >>> plt.show()\n+    .. plot::\n+        :include-source:\n+\n+        from numpy.fft import fft, fftshift\n+        window = np.blackman(51)\n+        plt.plot(window)\n+        plt.title(\"Blackman window\")\n+        plt.ylabel(\"Amplitude\")\n+        plt.xlabel(\"Sample\")\n+        plt.show()  # doctest: +SKIP\n+\n+        plt.figure()\n+        A = fft(window, 2048) / 25.5\n+        mag = np.abs(fftshift(A))\n+        freq = np.linspace(-0.5, 0.5, len(A))\n+        with np.errstate(divide='ignore', invalid='ignore'):\n+            response = 20 * np.log10(mag)\n+        response = np.clip(response, -100, 100)\n+        plt.plot(freq, response)\n+        plt.title(\"Frequency response of Blackman window\")\n+        plt.ylabel(\"Magnitude [dB]\")\n+        plt.xlabel(\"Normalized frequency [cycles per sample]\")\n+        plt.axis('tight')\n+        plt.show()\n \n     \"\"\"\n     # Ensures at least float64 via 0.0.  M should be an integer, but conversion\n@@ -3077,39 +3070,31 @@ def bartlett(M):\n             0.90909091,  0.90909091,  0.72727273,  0.54545455,  0.36363636,\n             0.18181818,  0.        ])\n \n-    Plot the window and its frequency response (requires SciPy and matplotlib):\n-\n-    >>> from numpy.fft import fft, fftshift\n-    >>> window = np.bartlett(51)\n-    >>> plt.plot(window)\n-    [<matplotlib.lines.Line2D object at 0x...>]\n-    >>> plt.title(\"Bartlett window\")\n-    Text(0.5, 1.0, 'Bartlett window')\n-    >>> plt.ylabel(\"Amplitude\")\n-    Text(0, 0.5, 'Amplitude')\n-    >>> plt.xlabel(\"Sample\")\n-    Text(0.5, 0, 'Sample')\n-    >>> plt.show()\n+    Plot the window and its frequency response (requires SciPy and matplotlib).\n \n-    >>> plt.figure()\n-    <Figure size 640x480 with 0 Axes>\n-    >>> A = fft(window, 2048) / 25.5\n-    >>> mag = np.abs(fftshift(A))\n-    >>> freq = np.linspace(-0.5, 0.5, len(A))\n-    >>> with np.errstate(divide='ignore', invalid='ignore'):\n-    ...     response = 20 * np.log10(mag)\n-    ...\n-    >>> response = np.clip(response, -100, 100)\n-    >>> plt.plot(freq, response)\n-    [<matplotlib.lines.Line2D object at 0x...>]\n-    >>> plt.title(\"Frequency response of Bartlett window\")\n-    Text(0.5, 1.0, 'Frequency response of Bartlett window')\n-    >>> plt.ylabel(\"Magnitude [dB]\")\n-    Text(0, 0.5, 'Magnitude [dB]')\n-    >>> plt.xlabel(\"Normalized frequency [cycles per sample]\")\n-    Text(0.5, 0, 'Normalized frequency [cycles per sample]')\n-    >>> _ = plt.axis('tight')\n-    >>> plt.show()\n+    .. plot::\n+        :include-source:\n+\n+        from numpy.fft import fft, fftshift\n+        window = np.bartlett(51)\n+        plt.plot(window)\n+        plt.title(\"Bartlett window\")\n+        plt.ylabel(\"Amplitude\")\n+        plt.xlabel(\"Sample\")\n+        plt.show()\n+        plt.figure()\n+        A = fft(window, 2048) / 25.5\n+        mag = np.abs(fftshift(A))\n+        freq = np.linspace(-0.5, 0.5, len(A))\n+        with np.errstate(divide='ignore', invalid='ignore'):\n+            response = 20 * np.log10(mag)\n+        response = np.clip(response, -100, 100)\n+        plt.plot(freq, response)\n+        plt.title(\"Frequency response of Bartlett window\")\n+        plt.ylabel(\"Magnitude [dB]\")\n+        plt.xlabel(\"Normalized frequency [cycles per sample]\")\n+        plt.axis('tight')\n+        plt.show()\n \n     \"\"\"\n     # Ensures at least float64 via 0.0.  M should be an integer, but conversion\n@@ -3184,41 +3169,33 @@ def hanning(M):\n            0.97974649, 0.97974649, 0.82743037, 0.57115742, 0.29229249,\n            0.07937323, 0.        ])\n \n-    Plot the window and its frequency response:\n+    Plot the window and its frequency response.\n \n-    >>> import matplotlib.pyplot as plt\n-    >>> from numpy.fft import fft, fftshift\n-    >>> window = np.hanning(51)\n-    >>> plt.plot(window)\n-    [<matplotlib.lines.Line2D object at 0x...>]\n-    >>> plt.title(\"Hann window\")\n-    Text(0.5, 1.0, 'Hann window')\n-    >>> plt.ylabel(\"Amplitude\")\n-    Text(0, 0.5, 'Amplitude')\n-    >>> plt.xlabel(\"Sample\")\n-    Text(0.5, 0, 'Sample')\n-    >>> plt.show()\n+    .. plot::\n+        :include-source:\n \n-    >>> plt.figure()\n-    <Figure size 640x480 with 0 Axes>\n-    >>> A = fft(window, 2048) / 25.5\n-    >>> mag = np.abs(fftshift(A))\n-    >>> freq = np.linspace(-0.5, 0.5, len(A))\n-    >>> with np.errstate(divide='ignore', invalid='ignore'):\n-    ...     response = 20 * np.log10(mag)\n-    ...\n-    >>> response = np.clip(response, -100, 100)\n-    >>> plt.plot(freq, response)\n-    [<matplotlib.lines.Line2D object at 0x...>]\n-    >>> plt.title(\"Frequency response of the Hann window\")\n-    Text(0.5, 1.0, 'Frequency response of the Hann window')\n-    >>> plt.ylabel(\"Magnitude [dB]\")\n-    Text(0, 0.5, 'Magnitude [dB]')\n-    >>> plt.xlabel(\"Normalized frequency [cycles per sample]\")\n-    Text(0.5, 0, 'Normalized frequency [cycles per sample]')\n-    >>> plt.axis('tight')\n-    ...\n-    >>> plt.show()\n+        import matplotlib.pyplot as plt\n+        from numpy.fft import fft, fftshift\n+        window = np.hanning(51)\n+        plt.plot(window)\n+        plt.title(\"Hann window\")\n+        plt.ylabel(\"Amplitude\")\n+        plt.xlabel(\"Sample\")\n+        plt.show()\n+\n+        plt.figure()\n+        A = fft(window, 2048) / 25.5\n+        mag = np.abs(fftshift(A))\n+        freq = np.linspace(-0.5, 0.5, len(A))\n+        with np.errstate(divide='ignore', invalid='ignore'):\n+            response = 20 * np.log10(mag)\n+        response = np.clip(response, -100, 100)\n+        plt.plot(freq, response)\n+        plt.title(\"Frequency response of the Hann window\")\n+        plt.ylabel(\"Magnitude [dB]\")\n+        plt.xlabel(\"Normalized frequency [cycles per sample]\")\n+        plt.axis('tight')\n+        plt.show()\n \n     \"\"\"\n     # Ensures at least float64 via 0.0.  M should be an integer, but conversion\n@@ -3291,39 +3268,32 @@ def hamming(M):\n             0.98136677,  0.98136677,  0.84123594,  0.60546483,  0.34890909,\n             0.15302337,  0.08      ])\n \n-    Plot the window and the frequency response:\n+    Plot the window and the frequency response.\n \n-    >>> import matplotlib.pyplot as plt\n-    >>> from numpy.fft import fft, fftshift\n-    >>> window = np.hamming(51)\n-    >>> plt.plot(window)\n-    [<matplotlib.lines.Line2D object at 0x...>]\n-    >>> plt.title(\"Hamming window\")\n-    Text(0.5, 1.0, 'Hamming window')\n-    >>> plt.ylabel(\"Amplitude\")\n-    Text(0, 0.5, 'Amplitude')\n-    >>> plt.xlabel(\"Sample\")\n-    Text(0.5, 0, 'Sample')\n-    >>> plt.show()\n+    .. plot::\n+        :include-source:\n \n-    >>> plt.figure()\n-    <Figure size 640x480 with 0 Axes>\n-    >>> A = fft(window, 2048) / 25.5\n-    >>> mag = np.abs(fftshift(A))\n-    >>> freq = np.linspace(-0.5, 0.5, len(A))\n-    >>> response = 20 * np.log10(mag)\n-    >>> response = np.clip(response, -100, 100)\n-    >>> plt.plot(freq, response)\n-    [<matplotlib.lines.Line2D object at 0x...>]\n-    >>> plt.title(\"Frequency response of Hamming window\")\n-    Text(0.5, 1.0, 'Frequency response of Hamming window')\n-    >>> plt.ylabel(\"Magnitude [dB]\")\n-    Text(0, 0.5, 'Magnitude [dB]')\n-    >>> plt.xlabel(\"Normalized frequency [cycles per sample]\")\n-    Text(0.5, 0, 'Normalized frequency [cycles per sample]')\n-    >>> plt.axis('tight')\n-    ...\n-    >>> plt.show()\n+        import matplotlib.pyplot as plt\n+        from numpy.fft import fft, fftshift\n+        window = np.hamming(51)\n+        plt.plot(window)\n+        plt.title(\"Hamming window\")\n+        plt.ylabel(\"Amplitude\")\n+        plt.xlabel(\"Sample\")\n+        plt.show()\n+\n+        plt.figure()\n+        A = fft(window, 2048) / 25.5\n+        mag = np.abs(fftshift(A))\n+        freq = np.linspace(-0.5, 0.5, len(A))\n+        response = 20 * np.log10(mag)\n+        response = np.clip(response, -100, 100)\n+        plt.plot(freq, response)\n+        plt.title(\"Frequency response of Hamming window\")\n+        plt.ylabel(\"Magnitude [dB]\")\n+        plt.xlabel(\"Normalized frequency [cycles per sample]\")\n+        plt.axis('tight')\n+        plt.show()\n \n     \"\"\"\n     # Ensures at least float64 via 0.0.  M should be an integer, but conversion\n@@ -3576,38 +3546,31 @@ def kaiser(M, beta):\n             4.65200189e-02, 3.46009194e-03, 7.72686684e-06])\n \n \n-    Plot the window and the frequency response:\n+    Plot the window and the frequency response.\n \n-    >>> from numpy.fft import fft, fftshift\n-    >>> window = np.kaiser(51, 14)\n-    >>> plt.plot(window)\n-    [<matplotlib.lines.Line2D object at 0x...>]\n-    >>> plt.title(\"Kaiser window\")\n-    Text(0.5, 1.0, 'Kaiser window')\n-    >>> plt.ylabel(\"Amplitude\")\n-    Text(0, 0.5, 'Amplitude')\n-    >>> plt.xlabel(\"Sample\")\n-    Text(0.5, 0, 'Sample')\n-    >>> plt.show()\n+    .. plot::\n+        :include-source:\n+\n+        from numpy.fft import fft, fftshift\n+        window = np.kaiser(51, 14)\n+        plt.plot(window)\n+        plt.title(\"Kaiser window\")\n+        plt.ylabel(\"Amplitude\")\n+        plt.xlabel(\"Sample\")\n+        plt.show()\n \n-    >>> plt.figure()\n-    <Figure size 640x480 with 0 Axes>\n-    >>> A = fft(window, 2048) / 25.5\n-    >>> mag = np.abs(fftshift(A))\n-    >>> freq = np.linspace(-0.5, 0.5, len(A))\n-    >>> response = 20 * np.log10(mag)\n-    >>> response = np.clip(response, -100, 100)\n-    >>> plt.plot(freq, response)\n-    [<matplotlib.lines.Line2D object at 0x...>]\n-    >>> plt.title(\"Frequency response of Kaiser window\")\n-    Text(0.5, 1.0, 'Frequency response of Kaiser window')\n-    >>> plt.ylabel(\"Magnitude [dB]\")\n-    Text(0, 0.5, 'Magnitude [dB]')\n-    >>> plt.xlabel(\"Normalized frequency [cycles per sample]\")\n-    Text(0.5, 0, 'Normalized frequency [cycles per sample]')\n-    >>> plt.axis('tight')\n-    (-0.5, 0.5, -100.0, ...) # may vary\n-    >>> plt.show()\n+        plt.figure()\n+        A = fft(window, 2048) / 25.5\n+        mag = np.abs(fftshift(A))\n+        freq = np.linspace(-0.5, 0.5, len(A))\n+        response = 20 * np.log10(mag)\n+        response = np.clip(response, -100, 100)\n+        plt.plot(freq, response)\n+        plt.title(\"Frequency response of Kaiser window\")\n+        plt.ylabel(\"Magnitude [dB]\")\n+        plt.xlabel(\"Normalized frequency [cycles per sample]\")\n+        plt.axis('tight')\n+        plt.show()\n \n     \"\"\"\n     # Ensures at least float64 via 0.0.  M should be an integer, but conversion\n",
            "comment_added_diff": {
                "2978": "        plt.show()  # doctest: +SKIP"
            },
            "comment_deleted_diff": {
                "3609": "    (-0.5, 0.5, -100.0, ...) # may vary"
            },
            "comment_modified_diff": {
                "2978": "    Text(0.5, 0, 'Sample')"
            }
        },
        {
            "commit": "017cef318ad37ac05738830ebb3173c65c128759",
            "timestamp": "2023-07-11T22:25:10+02:00",
            "author": "Mateusz Sok\u00f3\u0142",
            "commit_message": "API: apply review comments",
            "additions": 11,
            "deletions": 2,
            "change_type": "MODIFY",
            "diff": "@@ -28,7 +28,7 @@\n     interp as compiled_interp, interp_complex as compiled_interp_complex\n     )\n from numpy.core.umath import _add_newdoc_ufunc as add_newdoc_ufunc\n-from numpy._utils import deprecate, set_module\n+from numpy._utils import set_module\n \n # needed in this module for compatibility\n from numpy.lib.histograms import histogram, histogramdd  # noqa: F401\n@@ -1950,7 +1950,6 @@ def place(arr, mask, vals):\n     return _place(arr, mask, vals)\n \n \n-@deprecate\n def disp(mesg, device=None, linefeed=True):\n     \"\"\"\n     Display a message on a device.\n@@ -1983,6 +1982,16 @@ def disp(mesg, device=None, linefeed=True):\n     '\"Display\" in a file\\\\n'\n \n     \"\"\"\n+\n+    # Deprecated in NumPy 2.0, 2023-07-11\n+    warnings.warn(\n+        \"`disp` is deprecated, \"\n+        \"use your own printing function instead. \"\n+        \"(deprecated in NumPy 2.0)\",\n+        DeprecationWarning,\n+        stacklevel=2\n+    )    \n+\n     if device is None:\n         device = sys.stdout\n     if linefeed:\n",
            "comment_added_diff": {
                "1986": "    # Deprecated in NumPy 2.0, 2023-07-11"
            },
            "comment_deleted_diff": {},
            "comment_modified_diff": {}
        },
        {
            "commit": "3032e84ff34f20def2ef4ebf9f8695947af3fd24",
            "timestamp": "2023-08-22T22:03:33+02:00",
            "author": "Mateusz Sok\u00f3\u0142",
            "commit_message": "API: Remove deprecated `msort` function (#24494)\n\n[skip ci]",
            "additions": 1,
            "deletions": 51,
            "change_type": "MODIFY",
            "diff": "@@ -42,7 +42,7 @@\n     'diff', 'gradient', 'angle', 'unwrap', 'sort_complex', 'disp', 'flip',\n     'rot90', 'extract', 'place', 'vectorize', 'asarray_chkfinite', 'average',\n     'bincount', 'digitize', 'cov', 'corrcoef',\n-    'msort', 'median', 'sinc', 'hamming', 'hanning', 'bartlett',\n+    'median', 'sinc', 'hamming', 'hanning', 'bartlett',\n     'blackman', 'kaiser', 'trapz', 'i0',\n     'meshgrid', 'delete', 'insert', 'append', 'interp',\n     'quantile'\n@@ -3685,56 +3685,6 @@ def sinc(x):\n     return sin(y)/y\n \n \n-def _msort_dispatcher(a):\n-    return (a,)\n-\n-\n-@array_function_dispatch(_msort_dispatcher)\n-def msort(a):\n-    \"\"\"\n-    Return a copy of an array sorted along the first axis.\n-\n-    .. deprecated:: 1.24\n-\n-       msort is deprecated, use ``np.sort(a, axis=0)`` instead.\n-\n-    Parameters\n-    ----------\n-    a : array_like\n-        Array to be sorted.\n-\n-    Returns\n-    -------\n-    sorted_array : ndarray\n-        Array of the same type and shape as `a`.\n-\n-    See Also\n-    --------\n-    sort\n-\n-    Notes\n-    -----\n-    ``np.msort(a)`` is equivalent to  ``np.sort(a, axis=0)``.\n-\n-    Examples\n-    --------\n-    >>> a = np.array([[1, 4], [3, 1]])\n-    >>> np.msort(a)  # sort along the first axis\n-    array([[1, 1],\n-           [3, 4]])\n-\n-    \"\"\"\n-    # 2022-10-20 1.24\n-    warnings.warn(\n-        \"msort is deprecated, use np.sort(a, axis=0) instead\",\n-        DeprecationWarning,\n-        stacklevel=2,\n-    )\n-    b = array(a, subok=True, copy=True)\n-    b.sort(0)\n-    return b\n-\n-\n def _ureduce(a, func, keepdims=False, **kwargs):\n     \"\"\"\n     Internal Function.\n",
            "comment_added_diff": {},
            "comment_deleted_diff": {
                "3722": "    >>> np.msort(a)  # sort along the first axis",
                "3727": "    # 2022-10-20 1.24"
            },
            "comment_modified_diff": {}
        },
        {
            "commit": "89a6f3ee13c56a27b13cfe76f353f96f6695af28",
            "timestamp": "2023-08-24T12:09:17+02:00",
            "author": "Mateusz Sok\u00f3\u0142",
            "commit_message": "API: Update lib.histograms namespace (#24513)\n\n[skip ci]",
            "additions": 1,
            "deletions": 1,
            "change_type": "MODIFY",
            "diff": "@@ -30,7 +30,7 @@\n from numpy._utils import set_module\n \n # needed in this module for compatibility\n-from numpy.lib.histograms import histogram, histogramdd  # noqa: F401\n+from numpy.lib._histograms_impl import histogram, histogramdd  # noqa: F401\n \n \n array_function_dispatch = functools.partial(\n",
            "comment_added_diff": {
                "33": "from numpy.lib._histograms_impl import histogram, histogramdd  # noqa: F401"
            },
            "comment_deleted_diff": {
                "33": "from numpy.lib.histograms import histogram, histogramdd  # noqa: F401"
            },
            "comment_modified_diff": {
                "33": "from numpy.lib.histograms import histogram, histogramdd  # noqa: F401"
            }
        }
    ],
    "index_tricks.py": [
        {
            "commit": "b70a1e995faaf01cbf89cfe1316a7c14570cdbec",
            "timestamp": "2022-11-07T12:08:10+01:00",
            "author": "Sebastian Berg",
            "commit_message": "MAINT: Refactor AxisConcatenator to not use find_common_type\n\nRather, use `result_type` instead.  There are some exceedingly small\ntheoretical changes, since `result_type` currently uses value-inspection\nlogic.\n`find_common_type` did not, because it pre-dates the value inspection\nlogic.\n(I.e. in theory, this switches it to value-based promotion, just to\npartially undo that in NEP 50; although more changes there.)\n\nThe only place where it is fathomable to matter is if someone is using\n`np.c_[uint8_arr, -1]` to append 255 to an unsigned integer array.",
            "additions": 16,
            "deletions": 15,
            "change_type": "MODIFY",
            "diff": "@@ -7,7 +7,7 @@\n from numpy.core.numeric import (\n     asarray, ScalarType, array, alltrue, cumprod, arange, ndim\n )\n-from numpy.core.numerictypes import find_common_type, issubdtype\n+from numpy.core.numerictypes import issubdtype\n \n import numpy.matrixlib as matrixlib\n from .function_base import diff\n@@ -339,9 +339,8 @@ def __getitem__(self, key):\n         axis = self.axis\n \n         objs = []\n-        scalars = []\n-        arraytypes = []\n-        scalartypes = []\n+        # dtypes or scalars for weak scalar handling in result_type\n+        result_type_objs = []\n \n         for k, item in enumerate(key):\n             scalar = False\n@@ -387,10 +386,8 @@ def __getitem__(self, key):\n                 except (ValueError, TypeError) as e:\n                     raise ValueError(\"unknown special directive\") from e\n             elif type(item) in ScalarType:\n-                newobj = array(item, ndmin=ndmin)\n-                scalars.append(len(objs))\n                 scalar = True\n-                scalartypes.append(newobj.dtype)\n+                newobj = array(item, ndmin=ndmin)\n             else:\n                 item_ndim = ndim(item)\n                 newobj = array(item, copy=False, subok=True, ndmin=ndmin)\n@@ -402,15 +399,19 @@ def __getitem__(self, key):\n                     defaxes = list(range(ndmin))\n                     axes = defaxes[:k1] + defaxes[k2:] + defaxes[k1:k2]\n                     newobj = newobj.transpose(axes)\n+\n             objs.append(newobj)\n-            if not scalar and isinstance(newobj, _nx.ndarray):\n-                arraytypes.append(newobj.dtype)\n-\n-        # Ensure that scalars won't up-cast unless warranted\n-        final_dtype = find_common_type(arraytypes, scalartypes)\n-        if final_dtype is not None:\n-            for k in scalars:\n-                objs[k] = objs[k].astype(final_dtype)\n+            if scalar:\n+                result_type_objs.append(item)\n+            else:\n+                result_type_objs.append(newobj.dtype)\n+\n+        # Ensure that scalars won't up-cast unless warranted, for 0, drops\n+        # through to error in concatenate.\n+        if len(result_type_objs) != 0:\n+            final_dtype = _nx.result_type(*result_type_objs)\n+            # concatenate could do cast, but that can be overriden:\n+            objs = [obj.astype(final_dtype, copy=False) for obj in objs]\n \n         res = self.concatenate(tuple(objs), axis=axis)\n \n",
            "comment_added_diff": {
                "342": "        # dtypes or scalars for weak scalar handling in result_type",
                "409": "        # Ensure that scalars won't up-cast unless warranted, for 0, drops",
                "410": "        # through to error in concatenate.",
                "413": "            # concatenate could do cast, but that can be overriden:"
            },
            "comment_deleted_diff": {
                "409": "        # Ensure that scalars won't up-cast unless warranted"
            },
            "comment_modified_diff": {
                "342": "        scalars = []",
                "409": "        # Ensure that scalars won't up-cast unless warranted",
                "410": "        final_dtype = find_common_type(arraytypes, scalartypes)",
                "413": "                objs[k] = objs[k].astype(final_dtype)"
            }
        },
        {
            "commit": "d68d7b62659a9e8a4c5c51a6d9705f9496733caa",
            "timestamp": "2023-05-28T12:47:52+02:00",
            "author": "Dimitri Papadopoulos",
            "commit_message": "MAINT: fix typos found by codespell",
            "additions": 1,
            "deletions": 1,
            "change_type": "MODIFY",
            "diff": "@@ -411,7 +411,7 @@ def __getitem__(self, key):\n         # through to error in concatenate.\n         if len(result_type_objs) != 0:\n             final_dtype = _nx.result_type(*result_type_objs)\n-            # concatenate could do cast, but that can be overriden:\n+            # concatenate could do cast, but that can be overridden:\n             objs = [array(obj, copy=False, subok=True,\n                           ndmin=ndmin, dtype=final_dtype) for obj in objs]\n \n",
            "comment_added_diff": {
                "414": "            # concatenate could do cast, but that can be overridden:"
            },
            "comment_deleted_diff": {
                "414": "            # concatenate could do cast, but that can be overriden:"
            },
            "comment_modified_diff": {
                "414": "            # concatenate could do cast, but that can be overriden:"
            }
        }
    ],
    "ufunc_object.c": [],
    "test_ufunc.py": [
        {
            "commit": "53040310b61f8161c2fe2835f9ae77f305ae09f1",
            "timestamp": "2022-10-12T10:05:39+02:00",
            "author": "Sebastian Berg",
            "commit_message": "TST: Add basic tests for lowlevel access (including direct loop call)",
            "additions": 97,
            "deletions": 0,
            "change_type": "MODIFY",
            "diff": "@@ -2624,3 +2624,100 @@ def test_addition_reduce_negative_zero(dtype, use_initial):\n             # `sum([])` should probably be 0.0 and not -0.0 like `sum([-0.0])`\n             assert not np.signbit(res.real)\n             assert not np.signbit(res.imag)\n+\n+class TestLowlevelAPIAccess:\n+    def test_resolve_dtypes_basic(self):\n+        # Basic test for dtype resolution:\n+        i4 = np.dtype(\"i4\")\n+        f4 = np.dtype(\"f4\")\n+        f8 = np.dtype(\"f8\")\n+\n+        r = np.add.resolve_dtypes((i4, f4, None))\n+        assert r == (f8, f8, f8)\n+\n+        # Signature uses the same logic to parse as ufunc (less strict)\n+        # the following is \"same-kind\" casting so works:\n+        r = np.add.resolve_dtypes((\n+                i4, i4, None), signature=(None, None, \"f4\"))\n+        assert r == (f4, f4, f4)\n+\n+        # Check NEP 50 \"weak\" promotion also:\n+        r = np.add.resolve_dtypes((f4, int, None))\n+        assert r == (f4, f4, f4)\n+\n+        with pytest.raises(TypeError):\n+            np.add.resolve_dtypes((i4, f4, None), casting=\"no\")\n+\n+    def test_weird_dtypes(self):\n+        S0 = np.dtype(\"S0\")\n+        # S0 is often converted by NumPy to S1, but not here:\n+        r = np.equal.resolve_dtypes((S0, S0, None))\n+        assert r == (S0, S0, np.dtype(bool))\n+\n+        # Subarray dtypes are weird and only really exist nested, they need\n+        # the shift to full NEP 50 to be implemented nicely:\n+        dts = np.dtype(\"10i\")\n+        with pytest.raises(NotImplementedError):\n+            np.equal.resolve_dtypes((dts, dts, None))\n+\n+    def test_resolve_dtypes_reduction(self):\n+        i4 = np.dtype(\"i4\")\n+        with pytest.raises(NotImplementedError):\n+            np.add.resolve_dtypes((i4, i4, i4), reduction=True)\n+\n+    @pytest.mark.parametrize(\"dtypes\", [\n+            (np.dtype(\"i\"), np.dtype(\"i\")),\n+            (None, np.dtype(\"i\"), np.dtype(\"f\")),\n+            (np.dtype(\"i\"), None, np.dtype(\"f\")),\n+            (\"i4\", \"i4\", None)])\n+    def test_resolve_dtypes_errors(self, dtypes):\n+        with pytest.raises(TypeError):\n+            np.add.resolve_dtypes(dtypes)\n+\n+    def test_loop_access(self):\n+        # This is a basic test for the full strided loop access\n+        import ctypes as ct\n+\n+        data_t = ct.ARRAY(ct.c_char_p, 2)\n+        dim_t = ct.ARRAY(ct.c_ssize_t, 1)\n+        strides_t = ct.ARRAY(ct.c_ssize_t, 2)\n+        strided_loop_t = ct.CFUNCTYPE(\n+                ct.c_int, ct.c_void_p, data_t, dim_t, strides_t, ct.c_void_p)\n+\n+        class call_info_t(ct.Structure):\n+            _fields_ = [\n+                (\"strided_loop\", strided_loop_t),\n+                (\"context\", ct.c_void_p),\n+                (\"auxdata\", ct.c_void_p),\n+                (\"requires_pyapi\", ct.c_byte),\n+                (\"no_floatingpoint_errors\", ct.c_byte),\n+            ]\n+\n+        i4 = np.dtype(\"i4\")\n+        dt, call_info_obj = np.negative._resolve_dtypes_and_context((i4, i4))\n+        assert dt == (i4, i4)  # can be used without casting\n+\n+        # Fill in the rest of the information:\n+        np.negative._get_strided_loop(call_info_obj)\n+\n+        ct.pythonapi.PyCapsule_GetPointer.restype = ct.c_void_p\n+        call_info = ct.pythonapi.PyCapsule_GetPointer(\n+                ct.py_object(call_info_obj),\n+                ct.c_char_p(b\"numpy_1.24_ufunc_call_info\"))\n+\n+        call_info = ct.cast(call_info, ct.POINTER(call_info_t)).contents\n+\n+        arr = np.arange(10, dtype=i4)\n+        call_info.strided_loop(\n+                call_info.context,\n+                data_t(arr.ctypes.data, arr.ctypes.data),\n+                arr.ctypes.shape,  # is a C-array with 10 here\n+                strides_t(arr.ctypes.strides[0], arr.ctypes.strides[0]),\n+                call_info.auxdata)\n+\n+        # We just directly called the negative inner-loop in-place:\n+        assert_array_equal(arr, -np.arange(10, dtype=i4))\n+\n+        with pytest.raises(TypeError):\n+            # we refuse to do this twice with the same capsule:\n+            np.negative._get_strided_loop(call_info_obj)\n",
            "comment_added_diff": {
                "2630": "        # Basic test for dtype resolution:",
                "2638": "        # Signature uses the same logic to parse as ufunc (less strict)",
                "2639": "        # the following is \"same-kind\" casting so works:",
                "2644": "        # Check NEP 50 \"weak\" promotion also:",
                "2653": "        # S0 is often converted by NumPy to S1, but not here:",
                "2657": "        # Subarray dtypes are weird and only really exist nested, they need",
                "2658": "        # the shift to full NEP 50 to be implemented nicely:",
                "2678": "        # This is a basic test for the full strided loop access",
                "2698": "        assert dt == (i4, i4)  # can be used without casting",
                "2700": "        # Fill in the rest of the information:",
                "2714": "                arr.ctypes.shape,  # is a C-array with 10 here",
                "2718": "        # We just directly called the negative inner-loop in-place:",
                "2722": "            # we refuse to do this twice with the same capsule:"
            },
            "comment_deleted_diff": {},
            "comment_modified_diff": {}
        },
        {
            "commit": "2179be29c2d921422bf48f7f2375ea12fe07333e",
            "timestamp": "2022-10-12T10:06:59+02:00",
            "author": "Sebastian Berg",
            "commit_message": "ENH: Allow reductions in `np.add.resolve_dtypes`",
            "additions": 16,
            "deletions": 0,
            "change_type": "MODIFY",
            "diff": "@@ -2674,6 +2674,22 @@ def test_resolve_dtypes_errors(self, dtypes):\n         with pytest.raises(TypeError):\n             np.add.resolve_dtypes(dtypes)\n \n+    def test_resolve_dtypes_reduction(self):\n+        i2 = np.dtype(\"i2\")\n+        long_ = np.dtype(\"long\")\n+        # Check special addition resolution:\n+        res = np.add.resolve_dtypes((None, i2, None), reduction=True)\n+        assert res == (long_, long_, long_)\n+\n+    def test_resolve_dtypes_reduction_errors(self):\n+        i2 = np.dtype(\"i2\")\n+\n+        with pytest.raises(TypeError):\n+            np.add.resolve_dtypes((None, i2, i2))\n+\n+        with pytest.raises(TypeError):\n+            np.add.signature((None, None, \"i4\"))\n+\n     def test_loop_access(self):\n         # This is a basic test for the full strided loop access\n         import ctypes as ct\n",
            "comment_added_diff": {
                "2680": "        # Check special addition resolution:"
            },
            "comment_deleted_diff": {},
            "comment_modified_diff": {}
        },
        {
            "commit": "1c64f9b7d08520b78bba282180bfc59ad2d6a89e",
            "timestamp": "2022-10-27T16:13:53+02:00",
            "author": "Sebastian Berg",
            "commit_message": "BUG: Fix error checking of _get_strided_Loop fixed_strides\n\nAlso add tests (including for a bad capsule)",
            "additions": 23,
            "deletions": 0,
            "change_type": "MODIFY",
            "diff": "@@ -2738,3 +2738,26 @@ class call_info_t(ct.Structure):\n         with pytest.raises(TypeError):\n             # we refuse to do this twice with the same capsule:\n             np.negative._get_strided_loop(call_info_obj)\n+\n+    @pytest.mark.parametrize(\"strides\", [1, (1, 2, 3), (1, \"2\")])\n+    def test__get_strided_loop_errors_bad_strides(self, strides):\n+        i4 = np.dtype(\"i4\")\n+        dt, call_info = np.negative._resolve_dtypes_and_context((i4, i4))\n+\n+        with pytest.raises(TypeError):\n+            np.negative._get_strided_loop(call_info, fixed_strides=strides)\n+\n+    def test__get_strided_loop_errors_bad_call_info(self):\n+        i4 = np.dtype(\"i4\")\n+        dt, call_info = np.negative._resolve_dtypes_and_context((i4, i4))\n+\n+        with pytest.raises(ValueError, match=\"PyCapsule\"):\n+            np.negative._get_strided_loop(\"not the capsule!\")\n+\n+        with pytest.raises(TypeError, match=\".*incompatible context\"):\n+            np.add._get_strided_loop(call_info)\n+\n+        np.negative._get_strided_loop(call_info)\n+        with pytest.raises(TypeError):\n+            # cannot call it a second time:\n+            np.negative._get_strided_loop(call_info)\n",
            "comment_added_diff": {
                "2762": "            # cannot call it a second time:"
            },
            "comment_deleted_diff": {},
            "comment_modified_diff": {}
        },
        {
            "commit": "c973fc90c65e6864444677b4d8e0f70060a17da3",
            "timestamp": "2022-10-27T18:47:38+02:00",
            "author": "Sebastian Berg",
            "commit_message": "MAINT: Adopt changes from Stuart's review\n\nCo-authored-by: stuartarchibald <stuartarchibald@users.noreply.github.com>",
            "additions": 1,
            "deletions": 5,
            "change_type": "MODIFY",
            "diff": "@@ -2735,16 +2735,12 @@ class call_info_t(ct.Structure):\n         # We just directly called the negative inner-loop in-place:\n         assert_array_equal(arr, -np.arange(10, dtype=i4))\n \n-        with pytest.raises(TypeError):\n-            # we refuse to do this twice with the same capsule:\n-            np.negative._get_strided_loop(call_info_obj)\n-\n     @pytest.mark.parametrize(\"strides\", [1, (1, 2, 3), (1, \"2\")])\n     def test__get_strided_loop_errors_bad_strides(self, strides):\n         i4 = np.dtype(\"i4\")\n         dt, call_info = np.negative._resolve_dtypes_and_context((i4, i4))\n \n-        with pytest.raises(TypeError):\n+        with pytest.raises(TypeError, match=\"fixed_strides.*tuple.*or None\"):\n             np.negative._get_strided_loop(call_info, fixed_strides=strides)\n \n     def test__get_strided_loop_errors_bad_call_info(self):\n",
            "comment_added_diff": {},
            "comment_deleted_diff": {
                "2739": "            # we refuse to do this twice with the same capsule:"
            },
            "comment_modified_diff": {}
        },
        {
            "commit": "2c72fbf88f81ba7a7dc7fd83d737fe4139ec7133",
            "timestamp": "2022-11-07T16:05:17+01:00",
            "author": "Sebastian Berg",
            "commit_message": "DEP: Expire deprecation of dtype/signature allowing instances\n\nWe never really allowed instances here and deprecated it since\nNumPy 1.21 (it just failed completely in 1.21.0).",
            "additions": 29,
            "deletions": 0,
            "change_type": "MODIFY",
            "diff": "@@ -3,6 +3,7 @@\n import sys\n \n import pytest\n+from pytest import param\n \n import numpy as np\n import numpy.core._umath_tests as umt\n@@ -477,6 +478,34 @@ def test_signature_dtype_type(self):\n         float_dtype = type(np.dtype(np.float64))\n         np.add(3, 4, signature=(float_dtype, float_dtype, None))\n \n+    @pytest.mark.parametrize(\"get_kwarg\", [\n+            lambda dt: dict(dtype=x),\n+            lambda dt: dict(signature=(x, None, None))])\n+    def test_signature_dtype_instances_allowed(self, get_kwarg):\n+        # We allow certain dtype instances when there is a clear singleton\n+        # and the given one is equivalent; mainly for backcompat.\n+        int64 = np.dtype(\"int64\")\n+        int64_2 = pickle.loads(pickle.dumps(int64))\n+        # Relies on pickling behavior, if assert fails just remove test...\n+        assert int64 is not int64_2\n+\n+        assert np.add(1, 2, **get_kwarg(int64_2)).dtype == int64\n+        td = np.timedelta(2, \"s\")\n+        assert np.add(td, td, **get_kwarg(\"m8\")).dtype == \"m8[s]\"\n+\n+    @pytest.mark.parametrize(\"get_kwarg\", [\n+            param(lambda x: dict(dtype=x), id=\"dtype\"),\n+            param(lambda x: dict(signature=(x, None, None)), id=\"signature\")])\n+    def test_signature_dtype_instances_allowed(self, get_kwarg):\n+        msg = \"The `dtype` and `signature` arguments to ufuncs\"\n+\n+        with pytest.raises(TypeError, match=msg):\n+            np.add(3, 5, **get_kwarg(np.dtype(\"int64\").newbyteorder()))\n+        with pytest.raises(TypeError, match=msg):\n+            np.add(3, 5, **get_kwarg(np.dtype(\"m8[ns]\")))\n+        with pytest.raises(TypeError, match=msg):\n+            np.add(3, 5, **get_kwarg(\"m8[ns]\"))\n+\n     @pytest.mark.parametrize(\"casting\", [\"unsafe\", \"same_kind\", \"safe\"])\n     def test_partial_signature_mismatch(self, casting):\n         # If the second argument matches already, no need to specify it:\n",
            "comment_added_diff": {
                "485": "        # We allow certain dtype instances when there is a clear singleton",
                "486": "        # and the given one is equivalent; mainly for backcompat.",
                "489": "        # Relies on pickling behavior, if assert fails just remove test..."
            },
            "comment_deleted_diff": {},
            "comment_modified_diff": {}
        },
        {
            "commit": "4dc5321a3d8d308808a200a3bf621651a5e67f5a",
            "timestamp": "2022-11-28T17:43:13+01:00",
            "author": "Sebastian Berg",
            "commit_message": "ENH: Slightly improve error when gufunc axes has wrong size (#22675)\n\n* DOC: Slightly improve error when gufunc axes has wrong size\r\n\r\nMy hope was to tweak it into something useful that:\r\n\r\n    a @= b\r\n\r\ncan raise when `b` should have two dimensions and has two axes specified\r\nbut actually only has one.\r\nI didn't succeed, but I still think it a slight improvement to give the\r\nufunc name and the actual core dimensions.\r\n\r\n* ENH: Use AxisError when gufunc axes appear wrong due to the number of entries\r\n\r\nThis allows catching the error relatively targeted for in-place matmul `a @= b`\r\nwhich may use this path.\r\n\r\n* MAINT: Restore most TypeErrors (a bit more compexl than nice, but...)\r\n\r\n* DOC: add a release note\r\n\r\nCo-authored-by: mattip <matti.picus@gmail.com>",
            "additions": 8,
            "deletions": 6,
            "change_type": "MODIFY",
            "diff": "@@ -972,10 +972,10 @@ def test_axes_argument(self):\n         assert_raises(TypeError, inner1d, a, b, axes=[None, 1])\n         # cannot pass an index unless there is only one dimension\n         # (output is wrong in this case)\n-        assert_raises(TypeError, inner1d, a, b, axes=[-1, -1, -1])\n+        assert_raises(np.AxisError, inner1d, a, b, axes=[-1, -1, -1])\n         # or pass in generally the wrong number of axes\n-        assert_raises(ValueError, inner1d, a, b, axes=[-1, -1, (-1,)])\n-        assert_raises(ValueError, inner1d, a, b, axes=[-1, (-2, -1), ()])\n+        assert_raises(np.AxisError, inner1d, a, b, axes=[-1, -1, (-1,)])\n+        assert_raises(np.AxisError, inner1d, a, b, axes=[-1, (-2, -1), ()])\n         # axes need to have same length.\n         assert_raises(ValueError, inner1d, a, b, axes=[0, 1])\n \n@@ -1015,14 +1015,16 @@ def test_axes_argument(self):\n         # list needs to have right length\n         assert_raises(ValueError, mm, a, b, axes=[])\n         assert_raises(ValueError, mm, a, b, axes=[(-2, -1)])\n-        # list should contain tuples for multiple axes\n-        assert_raises(TypeError, mm, a, b, axes=[-1, -1, -1])\n-        assert_raises(TypeError, mm, a, b, axes=[(-2, -1), (-2, -1), -1])\n+        # list should not contain None, or lists\n+        assert_raises(TypeError, mm, a, b, axes=[None, None, None])\n         assert_raises(TypeError,\n                       mm, a, b, axes=[[-2, -1], [-2, -1], [-2, -1]])\n         assert_raises(TypeError,\n                       mm, a, b, axes=[(-2, -1), (-2, -1), [-2, -1]])\n         assert_raises(TypeError, mm, a, b, axes=[(-2, -1), (-2, -1), None])\n+        # single integers are AxisErrors if more are required\n+        assert_raises(np.AxisError, mm, a, b, axes=[-1, -1, -1])\n+        assert_raises(np.AxisError, mm, a, b, axes=[(-2, -1), (-2, -1), -1])\n         # tuples should not have duplicated values\n         assert_raises(ValueError, mm, a, b, axes=[(-2, -1), (-2, -1), (-2, -2)])\n         # arrays should have enough axes.\n",
            "comment_added_diff": {
                "1018": "        # list should not contain None, or lists",
                "1025": "        # single integers are AxisErrors if more are required"
            },
            "comment_deleted_diff": {
                "1018": "        # list should contain tuples for multiple axes"
            },
            "comment_modified_diff": {
                "1018": "        # list should contain tuples for multiple axes"
            }
        },
        {
            "commit": "217e2a7086a30bd0cf1f2fe84ae6b6e0ee7dc7c9",
            "timestamp": "2023-01-03T11:41:30+02:00",
            "author": "mattip",
            "commit_message": "MAINT: improve coverage of slow path by using a user-defined dtype",
            "additions": 9,
            "deletions": 2,
            "change_type": "MODIFY",
            "diff": "@@ -1923,12 +1923,19 @@ def __rmul__(self, other):\n         assert_(MyThing.rmul_count == 1, MyThing.rmul_count)\n         assert_(MyThing.getitem_count <= 2, MyThing.getitem_count)\n \n-    def test_inplace_fancy_indexing(self):\n+    @pytest.mark.parametrize(\"a\", (\n+                             np.arange(10, dtype=int),\n+                             np.arange(10, dtype=_rational_tests.rational),\n \n-        a = np.arange(10)\n+    ))\n+    def test_inplace_fancy_indexing(self, a):\n         np.add.at(a, [2, 5, 2], 1)\n         assert_equal(a, [0, 1, 4, 3, 4, 6, 6, 7, 8, 9])\n \n+        np.negative.at(a, [2, 5, 3])\n+        assert_equal(a, [0, 1, -4, -3, 4, -6, 6, 7, 8, 9])\n+\n+        # reset a\n         a = np.arange(10)\n         b = np.array([100, 100, 100])\n         np.add.at(a, [2, 5, 2], b)\n",
            "comment_added_diff": {
                "1938": "        # reset a"
            },
            "comment_deleted_diff": {},
            "comment_modified_diff": {}
        },
        {
            "commit": "11fa9763899d1b9294be52d7733277ad73a6aaa2",
            "timestamp": "2023-01-03T12:52:19+02:00",
            "author": "mattip",
            "commit_message": "MAINT: add missing test cases (from review)",
            "additions": 20,
            "deletions": 3,
            "change_type": "MODIFY",
            "diff": "@@ -1926,15 +1926,26 @@ def __rmul__(self, other):\n     @pytest.mark.parametrize(\"a\", (\n                              np.arange(10, dtype=int),\n                              np.arange(10, dtype=_rational_tests.rational),\n-\n-    ))\n-    def test_inplace_fancy_indexing(self, a):\n+                             ))\n+    def test_ufunc_at(self, a):\n         np.add.at(a, [2, 5, 2], 1)\n         assert_equal(a, [0, 1, 4, 3, 4, 6, 6, 7, 8, 9])\n \n+        with pytest.raises(ValueError):\n+            # missing second operand\n+            np.add.at(a, [2, 5, 3])\n+\n         np.negative.at(a, [2, 5, 3])\n         assert_equal(a, [0, 1, -4, -3, 4, -6, 6, 7, 8, 9])\n \n+        with pytest.raises(ValueError):\n+            # extraneous second operand \n+            np.negative.at(a, [2, 5, 3], [1, 2, 3])\n+\n+        with pytest.raises(ValueError):\n+            # second operand cannot be converted to an array\n+            np.add.at(a, [2, 5, 3], [[1, 2], 1])\n+\n         # reset a\n         a = np.arange(10)\n         b = np.array([100, 100, 100])\n@@ -2082,6 +2093,12 @@ def test_at_not_none_signature(self):\n         a = np.array([[[1, 2], [3, 4]]])\n         assert_raises(TypeError, np.linalg._umath_linalg.det.at, a, [0])\n \n+    def test_at_no_loop_for_op(self):\n+        # str dtype does not have a ufunc loop for np.add\n+        arr = np.ones(10, dtype=str)\n+        with pytest.raises(np.core._exceptions._UFuncNoLoopError):\n+            np.add.at(arr, [0, 1], [0, 1])\n+\n     def test_reduce_arguments(self):\n         f = np.add.reduce\n         d = np.ones((5,2), dtype=int)\n",
            "comment_added_diff": {
                "1935": "            # missing second operand",
                "1942": "            # extraneous second operand",
                "1946": "            # second operand cannot be converted to an array",
                "2097": "        # str dtype does not have a ufunc loop for np.add"
            },
            "comment_deleted_diff": {},
            "comment_modified_diff": {}
        },
        {
            "commit": "f73d53d1a44ac841458c5be852e05021feec5dff",
            "timestamp": "2023-01-03T18:28:55+01:00",
            "author": "Matti Picus",
            "commit_message": "TST: split long ufunc.at test (#22918)\n\nCleanup from #22889",
            "additions": 12,
            "deletions": 13,
            "change_type": "MODIFY",
            "diff": "@@ -1927,7 +1927,7 @@ def __rmul__(self, other):\n                              np.arange(10, dtype=int),\n                              np.arange(10, dtype=_rational_tests.rational),\n                              ))\n-    def test_ufunc_at(self, a):\n+    def test_ufunc_at_basic(self, a):\n \n         aa = a.copy()\n         np.add.at(aa, [2, 5, 2], 1)\n@@ -1941,6 +1941,11 @@ def test_ufunc_at(self, a):\n         np.negative.at(aa, [2, 5, 3])\n         assert_equal(aa, [0, 1, -2, -3, 4, -5, 6, 7, 8, 9])\n \n+        aa = a.copy()\n+        b = np.array([100, 100, 100])\n+        np.add.at(aa, [2, 5, 2], b)\n+        assert_equal(aa, [0, 1, 202, 3, 4, 105, 6, 7, 8, 9])\n+\n         with pytest.raises(ValueError):\n             # extraneous second operand \n             np.negative.at(a, [2, 5, 3], [1, 2, 3])\n@@ -1949,12 +1954,7 @@ def test_ufunc_at(self, a):\n             # second operand cannot be converted to an array\n             np.add.at(a, [2, 5, 3], [[1, 2], 1])\n \n-        # reset a\n-        a = np.arange(10)\n-        b = np.array([100, 100, 100])\n-        np.add.at(a, [2, 5, 2], b)\n-        assert_equal(a, [0, 1, 202, 3, 4, 105, 6, 7, 8, 9])\n-\n+    def test_ufunc_at_multiD(self):\n         a = np.arange(9).reshape(3, 3)\n         b = np.array([[100, 100, 100], [200, 200, 200], [300, 300, 300]])\n         np.add.at(a, (slice(None), [1, 2, 1]), b)\n@@ -2034,11 +2034,7 @@ def test_ufunc_at(self, a):\n               [121, 222, 323],\n               [124, 225, 326]]])\n \n-        a = np.arange(10)\n-        np.negative.at(a, [2, 5, 2])\n-        assert_equal(a, [0, 1, 2, 3, 4, -5, 6, 7, 8, 9])\n-\n-        # Test 0-dim array\n+    def test_ufunc_at_0D(self):\n         a = np.array(0)\n         np.add.at(a, (), 1)\n         assert_equal(a, 1)\n@@ -2046,11 +2042,13 @@ def test_ufunc_at(self, a):\n         assert_raises(IndexError, np.add.at, a, 0, 1)\n         assert_raises(IndexError, np.add.at, a, [], 1)\n \n+    def test_ufunc_at_dtypes(self):\n         # Test mixed dtypes\n         a = np.arange(10)\n         np.power.at(a, [1, 2, 3, 2], 3.5)\n         assert_equal(a, np.array([0, 1, 4414, 46, 4, 5, 6, 7, 8, 9]))\n \n+    def test_ufunc_at_boolean(self):\n         # Test boolean indexing and boolean ufuncs\n         a = np.arange(10)\n         index = a % 2 == 0\n@@ -2062,6 +2060,7 @@ def test_ufunc_at(self, a):\n         np.invert.at(a, [2, 5, 2])\n         assert_equal(a, [0, 1, 2, 3, 4, 5 ^ 0xffffffff, 6, 7, 8, 9])\n \n+    def test_ufunc_at_advanced(self):\n         # Test empty subspace\n         orig = np.arange(4)\n         a = orig[:, None][:, 0:0]\n@@ -2085,7 +2084,7 @@ def test_ufunc_at(self, a):\n         # Test maximum\n         a = np.array([1, 2, 3])\n         np.maximum.at(a, [0], 0)\n-        assert_equal(np.array([1, 2, 3]), a)\n+        assert_equal(a, np.array([1, 2, 3]))\n \n     def test_at_not_none_signature(self):\n         # Test ufuncs with non-trivial signature raise a TypeError\n",
            "comment_added_diff": {},
            "comment_deleted_diff": {
                "1952": "        # reset a",
                "2041": "        # Test 0-dim array"
            },
            "comment_modified_diff": {}
        },
        {
            "commit": "ffe18c971d4d122dc36cafcc6651b44118d08d39",
            "timestamp": "2023-01-20T15:28:47+01:00",
            "author": "Sebastian Berg",
            "commit_message": "ENH: Move identity to the ArrayMethod to allow customization\n\nThis also fixes/changes that the identity value is defined by the\nreduce dtype and not by the result dtype.  That does not make a\ndifferent in any sane use-case, but there is a theoretical change:\n\n    arr = np.array([])\n    out = np.array(None)  # object array\n    np.add.reduce(arr, out=out, dtype=np.float64)\n\nWhere the output result was previously an integer 0, due to the\noutput being of `object` dtype, but now it is the correct float\ndue to the _operation_ being done in `float64`, so that the output\nshould be `np.zeros((), dtype=np.float64).astype(object)`.",
            "additions": 16,
            "deletions": 1,
            "change_type": "MODIFY",
            "diff": "@@ -1650,6 +1650,18 @@ def test_identityless_reduction_noncontig_unaligned(self):\n         a = a[1:, 1:, 1:]\n         self.check_identityless_reduction(a)\n \n+    def test_reduce_identity_depends_on_loop(self):\n+        \"\"\"\n+        The type of the result should always depend on the selected loop, not\n+        necessarily the output (only relevant for object arrays).\n+        \"\"\"\n+        # For an object loop, the default value 0 with type int is used:\n+        assert type(np.add.reduce([], dtype=object)) is int\n+        out = np.array(None, dtype=object)\n+        # When the loop is float but `out` is object this does not happen:\n+        np.add.reduce([], out=out, dtype=np.float64)\n+        assert type(out[()]) is not int\n+\n     def test_initial_reduction(self):\n         # np.minimum.reduce is an identityless reduction\n \n@@ -1670,6 +1682,8 @@ def test_initial_reduction(self):\n         # Check initial=None raises ValueError for both types of ufunc reductions\n         assert_raises(ValueError, np.minimum.reduce, [], initial=None)\n         assert_raises(ValueError, np.add.reduce, [], initial=None)\n+        # Also in the somewhat special object case:\n+        assert_raises(ValueError, np.add.reduce, [], initial=None, dtype=object)\n \n         # Check that np._NoValue gives default behavior.\n         assert_equal(np.add.reduce([], initial=np._NoValue), 0)\n@@ -2617,7 +2631,8 @@ def test_reduce_casterrors(offset):\n     with pytest.raises(ValueError, match=\"invalid literal\"):\n         # This is an unsafe cast, but we currently always allow that.\n         # Note that the double loop is picked, but the cast fails.\n-        np.add.reduce(arr, dtype=np.intp, out=out)\n+        # `initial=None` disables the use of an identity here.\n+        np.add.reduce(arr, dtype=np.intp, out=out, initial=None)\n     assert count == sys.getrefcount(value)\n     # If an error occurred during casting, the operation is done at most until\n     # the error occurs (the result of which would be `value * offset`) and -1\n",
            "comment_added_diff": {
                "1658": "        # For an object loop, the default value 0 with type int is used:",
                "1661": "        # When the loop is float but `out` is object this does not happen:",
                "1685": "        # Also in the somewhat special object case:",
                "2634": "        # `initial=None` disables the use of an identity here."
            },
            "comment_deleted_diff": {},
            "comment_modified_diff": {}
        },
        {
            "commit": "88ee23a928f1e1fb5525cf024d7ed6bf6f23ba7a",
            "timestamp": "2023-01-20T15:28:48+01:00",
            "author": "Sebastian Berg",
            "commit_message": "TST: Fix lint and ensure we have tests for empty arrays and reduce",
            "additions": 14,
            "deletions": 1,
            "change_type": "MODIFY",
            "diff": "@@ -1683,7 +1683,8 @@ def test_initial_reduction(self):\n         assert_raises(ValueError, np.minimum.reduce, [], initial=None)\n         assert_raises(ValueError, np.add.reduce, [], initial=None)\n         # Also in the somewhat special object case:\n-        assert_raises(ValueError, np.add.reduce, [], initial=None, dtype=object)\n+        with pytest.raises(ValueError):\n+            np.add.reduce([], initial=None, dtype=object)\n \n         # Check that np._NoValue gives default behavior.\n         assert_equal(np.add.reduce([], initial=np._NoValue), 0)\n@@ -1693,6 +1694,18 @@ def test_initial_reduction(self):\n         res = np.add.reduce(a, initial=5)\n         assert_equal(res, 15)\n \n+    def test_empty_reduction_and_idenity(self):\n+        arr = np.zeros((0, 5))\n+        # OK, since the reduction itself is *not* empty, the result is\n+        assert np.true_divide.reduce(arr, axis=1).shape == (0,)\n+        # Not OK, the reduction itself is empty and we have no idenity\n+        with pytest.raises(ValueError):\n+            np.true_divide.reduce(arr, axis=0)\n+        # Test that an empty reduction fails also if the result is empty\n+        arr = np.zeros((0, 0, 5))\n+        with pytest.raises(ValueError):\n+            np.true_divide.reduce(arr, axis=1)\n+\n     @pytest.mark.parametrize('axis', (0, 1, None))\n     @pytest.mark.parametrize('where', (np.array([False, True, True]),\n                                        np.array([[True], [False], [True]]),\n",
            "comment_added_diff": {
                "1699": "        # OK, since the reduction itself is *not* empty, the result is",
                "1701": "        # Not OK, the reduction itself is empty and we have no idenity",
                "1704": "        # Test that an empty reduction fails also if the result is empty"
            },
            "comment_deleted_diff": {},
            "comment_modified_diff": {}
        },
        {
            "commit": "2a76b6608b36962f4e8b5f5d5c53caccf54aa662",
            "timestamp": "2023-01-20T15:28:48+01:00",
            "author": "Sebastian Berg",
            "commit_message": "MAINT: Address many of Marten's comments",
            "additions": 19,
            "deletions": 3,
            "change_type": "MODIFY",
            "diff": "@@ -1658,9 +1658,10 @@ def test_reduce_identity_depends_on_loop(self):\n         # For an object loop, the default value 0 with type int is used:\n         assert type(np.add.reduce([], dtype=object)) is int\n         out = np.array(None, dtype=object)\n-        # When the loop is float but `out` is object this does not happen:\n+        # When the loop is float64 but `out` is object this does not happen,\n+        # the result is float64 cast to object (which gives Python `float`).\n         np.add.reduce([], out=out, dtype=np.float64)\n-        assert type(out[()]) is not int\n+        assert type(out[()]) is float\n \n     def test_initial_reduction(self):\n         # np.minimum.reduce is an identityless reduction\n@@ -1701,11 +1702,16 @@ def test_empty_reduction_and_idenity(self):\n         # Not OK, the reduction itself is empty and we have no idenity\n         with pytest.raises(ValueError):\n             np.true_divide.reduce(arr, axis=0)\n+\n         # Test that an empty reduction fails also if the result is empty\n         arr = np.zeros((0, 0, 5))\n         with pytest.raises(ValueError):\n             np.true_divide.reduce(arr, axis=1)\n \n+        # Division reduction makes sense with `initial=1` (empty or not):\n+        res = np.true_divide.reduce(arr, axis=1, initial=1)\n+        assert_array_equal(res, np.ones((0, 5)))\n+\n     @pytest.mark.parametrize('axis', (0, 1, None))\n     @pytest.mark.parametrize('where', (np.array([False, True, True]),\n                                        np.array([[True], [False], [True]]),\n@@ -2644,7 +2650,8 @@ def test_reduce_casterrors(offset):\n     with pytest.raises(ValueError, match=\"invalid literal\"):\n         # This is an unsafe cast, but we currently always allow that.\n         # Note that the double loop is picked, but the cast fails.\n-        # `initial=None` disables the use of an identity here.\n+        # `initial=None` disables the use of an identity here to test failures\n+        # while copying the first values path (not used when identity exists).\n         np.add.reduce(arr, dtype=np.intp, out=out, initial=None)\n     assert count == sys.getrefcount(value)\n     # If an error occurred during casting, the operation is done at most until\n@@ -2654,6 +2661,15 @@ def test_reduce_casterrors(offset):\n     assert out[()] < value * offset\n \n \n+def test_object_reduce_cleanup_on_failure():\n+    # Test cleanup, including of the initial value (manually provided or not)\n+    with pytest.raises(TypeError):\n+        np.add.reduce([1, 2, None], initial=4)\n+\n+    with pytest.raises(TypeError):\n+        np.add.reduce([1, 2, None])\n+\n+\n @pytest.mark.skipif(IS_WASM, reason=\"fp errors don't work in wasm\")\n @pytest.mark.parametrize(\"method\",\n         [np.add.accumulate, np.add.reduce,\n",
            "comment_added_diff": {
                "1661": "        # When the loop is float64 but `out` is object this does not happen,",
                "1662": "        # the result is float64 cast to object (which gives Python `float`).",
                "1711": "        # Division reduction makes sense with `initial=1` (empty or not):",
                "2653": "        # `initial=None` disables the use of an identity here to test failures",
                "2654": "        # while copying the first values path (not used when identity exists).",
                "2665": "    # Test cleanup, including of the initial value (manually provided or not)"
            },
            "comment_deleted_diff": {
                "1661": "        # When the loop is float but `out` is object this does not happen:",
                "2647": "        # `initial=None` disables the use of an identity here."
            },
            "comment_modified_diff": {
                "1661": "        # When the loop is float but `out` is object this does not happen:"
            }
        },
        {
            "commit": "28706afcbe1bf35413049d0283e6e01ef9abcb1a",
            "timestamp": "2023-02-01T16:14:23+02:00",
            "author": "mattip",
            "commit_message": "MAINT, BUG: fixes from review and testing",
            "additions": 34,
            "deletions": 0,
            "change_type": "MODIFY",
            "diff": "@@ -1986,6 +1986,40 @@ def test_ufunc_at_basic(self, a):\n         with pytest.raises(ValueError):\n             # second operand cannot be converted to an array\n             np.add.at(a, [2, 5, 3], [[1, 2], 1])\n+    \n+    # ufuncs with indexed loops for perfomance in ufunc.at\n+    indexed_ufuncs = [np.add, np.subtract, np.multiply, np.floor_divide, np.divide]\n+    @pytest.mark.parametrize(\n+                \"typecode\", np.typecodes['AllInteger'] + np.typecodes['Float'])\n+    @pytest.mark.parametrize(\"ufunc\", indexed_ufuncs)\n+    def test_ufunc_at_inner_loops(self, typecode, ufunc):\n+        if ufunc is np.divide and typecode in  np.typecodes['AllInteger']:\n+            # Avoid divide-by-zero and inf for integer divide\n+            a = np.ones(100, dtype=typecode)\n+            indx = np.random.randint(100, size=30, dtype=np.intp)\n+            vals = np.arange(1, 31, dtype=typecode)\n+        else:\n+            a = np.ones(1000, dtype=typecode)\n+            indx = np.random.randint(1000, size=3000, dtype=np.intp)\n+            vals = np.arange(3000, dtype=typecode)\n+        atag = a.copy()\n+        # Do the calculation twice and compare the answers\n+        with warnings.catch_warnings(record=True) as w_at:\n+            warnings.simplefilter('always')\n+            ufunc.at(a, indx, vals)\n+        with warnings.catch_warnings(record=True) as w_loop:\n+            warnings.simplefilter('always')\n+            for i,v in zip(indx, vals):\n+                # Make sure all the work happens inside the ufunc\n+                # in order to duplicate error/warning handling\n+                ufunc(atag[i], v, out=atag[i:i+1], casting=\"unsafe\")\n+        assert_equal(atag, a)\n+        # If w_loop warned, make sure w_at warned as well\n+        if len(w_loop) > 0:\n+            #\n+            assert len(w_at) > 0\n+            assert w_at[0].category == w_loop[0].category\n+            assert str(w_at[0].message)[:10] == str(w_loop[0].message)[:10]\n \n     def test_ufunc_at_multiD(self):\n         a = np.arange(9).reshape(3, 3)\n",
            "comment_added_diff": {
                "1990": "    # ufuncs with indexed loops for perfomance in ufunc.at",
                "1997": "            # Avoid divide-by-zero and inf for integer divide",
                "2006": "        # Do the calculation twice and compare the answers",
                "2013": "                # Make sure all the work happens inside the ufunc",
                "2014": "                # in order to duplicate error/warning handling",
                "2017": "        # If w_loop warned, make sure w_at warned as well",
                "2019": "            #"
            },
            "comment_deleted_diff": {},
            "comment_modified_diff": {}
        },
        {
            "commit": "6cf9cc91233e63c2f2aa26166a50cac84574d67c",
            "timestamp": "2023-02-06T15:23:18+01:00",
            "author": "Sebastian Berg",
            "commit_message": "BUG: Deal with casting and non-contig/1-D indices",
            "additions": 8,
            "deletions": 0,
            "change_type": "MODIFY",
            "diff": "@@ -2023,6 +2023,14 @@ def test_ufunc_at_inner_loops(self, typecode, ufunc):\n             assert w_at[0].category == w_loop[0].category\n             assert str(w_at[0].message)[:10] == str(w_loop[0].message)[:10]\n \n+    def test_cast_index_fastpath(self):\n+        arr = np.zeros(10)\n+        values = np.ones(100000)\n+        # index must be cast, which may be buffered in chunks:\n+        index = np.zeros(len(values), dtype=np.uint8)\n+        np.add.at(arr, index, values)\n+        assert arr[0] == len(values)\n+\n     def test_ufunc_at_multiD(self):\n         a = np.arange(9).reshape(3, 3)\n         b = np.array([[100, 100, 100], [200, 200, 200], [300, 300, 300]])\n",
            "comment_added_diff": {
                "2029": "        # index must be cast, which may be buffered in chunks:"
            },
            "comment_deleted_diff": {},
            "comment_modified_diff": {}
        },
        {
            "commit": "f79325e70e1a79e1fdda9edfcdebd78048e67d2c",
            "timestamp": "2023-02-07T11:55:34+02:00",
            "author": "mattip",
            "commit_message": "TST: add test (from review)",
            "additions": 7,
            "deletions": 0,
            "change_type": "MODIFY",
            "diff": "@@ -2023,6 +2023,13 @@ def test_ufunc_at_inner_loops(self, typecode, ufunc):\n             assert w_at[0].category == w_loop[0].category\n             assert str(w_at[0].message)[:10] == str(w_loop[0].message)[:10]\n \n+    def test_ufunc_at_ellipsis(self):\n+        # Make sure the indexed loop check does not choke on iters\n+        # with subspaces\n+        arr = np.zeros(5)\n+        np.add.at(arr, slice(None), np.ones(5))\n+        assert_array_equal(arr, np.ones(5))\n+\n     def test_cast_index_fastpath(self):\n         arr = np.zeros(10)\n         values = np.ones(100000)\n",
            "comment_added_diff": {
                "2027": "        # Make sure the indexed loop check does not choke on iters",
                "2028": "        # with subspaces"
            },
            "comment_deleted_diff": {},
            "comment_modified_diff": {}
        },
        {
            "commit": "790426e16466af4ef05ed295a27cbf5e7ff76a14",
            "timestamp": "2023-02-07T22:05:23+02:00",
            "author": "mattip",
            "commit_message": "TST, BUG: add a test for unary ufuncs, fix condition for indexed loops",
            "additions": 7,
            "deletions": 0,
            "change_type": "MODIFY",
            "diff": "@@ -2030,6 +2030,13 @@ def test_ufunc_at_ellipsis(self):\n         np.add.at(arr, slice(None), np.ones(5))\n         assert_array_equal(arr, np.ones(5))\n \n+    def test_ufunc_at_negative(self):\n+        arr = np.ones(5, dtype=np.int32)\n+        indx = np.arange(5)\n+        umt.indexed_negative.at(arr, indx)\n+        # If it is [-1, -1, -1, -100, 0] then the regular strided loop was used\n+        assert np.all(arr == [-1, -1, -1, -200, -1])\n+\n     def test_cast_index_fastpath(self):\n         arr = np.zeros(10)\n         values = np.ones(100000)\n",
            "comment_added_diff": {
                "2037": "        # If it is [-1, -1, -1, -100, 0] then the regular strided loop was used"
            },
            "comment_deleted_diff": {},
            "comment_modified_diff": {}
        },
        {
            "commit": "3b3d035eb6ec52353020ad2a76de1326795276e7",
            "timestamp": "2023-02-08T19:23:02-05:00",
            "author": "Marten van Kerkwijk",
            "commit_message": "BUG/ENH: Fix fast index loops for 1-el array / allow scalar value",
            "additions": 9,
            "deletions": 0,
            "change_type": "MODIFY",
            "diff": "@@ -2045,6 +2045,15 @@ def test_cast_index_fastpath(self):\n         np.add.at(arr, index, values)\n         assert arr[0] == len(values)\n \n+    @pytest.mark.parametrize(\"value\", [\n+        np.ones(1), np.ones(()), np.float64(1.), 1.])\n+    def test_ufunc_at_scalar_value_fastpath(self, value):\n+        arr = np.zeros(1000)\n+        # index must be cast, which may be buffered in chunks:\n+        index = np.repeat(np.arange(1000), 2)\n+        np.add.at(arr, index, value)\n+        assert_array_equal(arr, np.full_like(arr, 2 * value))\n+\n     def test_ufunc_at_multiD(self):\n         a = np.arange(9).reshape(3, 3)\n         b = np.array([[100, 100, 100], [200, 200, 200], [300, 300, 300]])\n",
            "comment_added_diff": {
                "2052": "        # index must be cast, which may be buffered in chunks:"
            },
            "comment_deleted_diff": {},
            "comment_modified_diff": {}
        },
        {
            "commit": "11a7e2d4aa85e902384bcb9459a83045fab602b4",
            "timestamp": "2023-02-09T16:01:15+01:00",
            "author": "Matti Picus",
            "commit_message": "ENH: add indexed loops for maximum, minimum, fmax, fmin (#23177)\n\nContinuation of the ufunc.at optimizations:\r\n\r\nadd indexed loops for maximum, minimum, fmax, fmin ufuncs and a benchmark for maximum.at (performance increased by ~13x)\r\n\r\n* BENCH: add np.maximum.at benchmark\r\n\r\n* remove 'explain_chain'\r\n\r\n* add a seed to `default_rng() (from review)\r\n\r\n* MAINT: formatting and change comments, from review",
            "additions": 5,
            "deletions": 5,
            "change_type": "MODIFY",
            "diff": "@@ -1980,16 +1980,16 @@ def test_ufunc_at_basic(self, a):\n         assert_equal(aa, [0, 1, 202, 3, 4, 105, 6, 7, 8, 9])\n \n         with pytest.raises(ValueError):\n-            # extraneous second operand \n+            # extraneous second operand\n             np.negative.at(a, [2, 5, 3], [1, 2, 3])\n \n         with pytest.raises(ValueError):\n             # second operand cannot be converted to an array\n             np.add.at(a, [2, 5, 3], [[1, 2], 1])\n-    \n+\n     # ufuncs with indexed loops for perfomance in ufunc.at\n-    indexed_ufuncs = [np.add, np.subtract, np.multiply,\n-                      np.floor_divide, np.divide]\n+    indexed_ufuncs = [np.add, np.subtract, np.multiply, np.floor_divide,\n+                      np.maximum, np.minimum, np.fmax, np.fmin]\n \n     @pytest.mark.parametrize(\n                 \"typecode\", np.typecodes['AllInteger'] + np.typecodes['Float'])\n@@ -2204,7 +2204,7 @@ def test_at_no_loop_for_op(self):\n     def test_at_output_casting(self):\n         arr = np.array([-1])\n         np.equal.at(arr, [0], [0])\n-        assert arr[0] == 0 \n+        assert arr[0] == 0\n \n     def test_at_broadcast_failure(self):\n         arr = np.arange(5)\n",
            "comment_added_diff": {
                "1983": "            # extraneous second operand"
            },
            "comment_deleted_diff": {
                "1983": "            # extraneous second operand"
            },
            "comment_modified_diff": {
                "1983": "            # extraneous second operand"
            }
        },
        {
            "commit": "b862e4f4ec4b5d02b30a2f1b2ec9d1c9478b9977",
            "timestamp": "2023-02-09T14:25:49-05:00",
            "author": "Marten van Kerkwijk",
            "commit_message": "ENH: enable fast indexed loops for complex add, subtract, multiply",
            "additions": 20,
            "deletions": 3,
            "change_type": "MODIFY",
            "diff": "@@ -1980,13 +1980,13 @@ def test_ufunc_at_basic(self, a):\n         assert_equal(aa, [0, 1, 202, 3, 4, 105, 6, 7, 8, 9])\n \n         with pytest.raises(ValueError):\n-            # extraneous second operand \n+            # extraneous second operand\n             np.negative.at(a, [2, 5, 3], [1, 2, 3])\n \n         with pytest.raises(ValueError):\n             # second operand cannot be converted to an array\n             np.add.at(a, [2, 5, 3], [[1, 2], 1])\n-    \n+\n     # ufuncs with indexed loops for perfomance in ufunc.at\n     indexed_ufuncs = [np.add, np.subtract, np.multiply,\n                       np.floor_divide, np.divide]\n@@ -2023,6 +2023,23 @@ def test_ufunc_at_inner_loops(self, typecode, ufunc):\n             assert w_at[0].category == w_loop[0].category\n             assert str(w_at[0].message)[:10] == str(w_loop[0].message)[:10]\n \n+    @pytest.mark.parametrize(\"typecode\", np.typecodes['Complex'])\n+    @pytest.mark.parametrize(\"ufunc\", [np.add, np.subtract, np.multiply])\n+    def test_ufunc_at_inner_loops_complex(self, typecode, ufunc):\n+        a = np.ones(10, dtype=typecode)\n+        indx = np.concatenate([np.ones(6, dtype=np.intp),\n+                               np.full(18, 4, dtype=np.intp)])\n+        value = a.dtype.type(1j)\n+        ufunc.at(a, indx, value)\n+        expected = np.ones_like(a)\n+        if ufunc is np.multiply:\n+            expected[1] = expected[4] = -1\n+        else:\n+            expected[1] += 6 * (value if ufunc is np.add else -value)\n+            expected[4] += 18 * (value if ufunc is np.add else -value)\n+\n+        assert_array_equal(a, expected)\n+\n     def test_ufunc_at_ellipsis(self):\n         # Make sure the indexed loop check does not choke on iters\n         # with subspaces\n@@ -2204,7 +2221,7 @@ def test_at_no_loop_for_op(self):\n     def test_at_output_casting(self):\n         arr = np.array([-1])\n         np.equal.at(arr, [0], [0])\n-        assert arr[0] == 0 \n+        assert arr[0] == 0\n \n     def test_at_broadcast_failure(self):\n         arr = np.arange(5)\n",
            "comment_added_diff": {
                "1983": "            # extraneous second operand"
            },
            "comment_deleted_diff": {
                "1983": "            # extraneous second operand"
            },
            "comment_modified_diff": {
                "1983": "            # extraneous second operand"
            }
        },
        {
            "commit": "e1e487acf1d820cbab8a6f97986bf2fb451dfa8e",
            "timestamp": "2023-02-11T22:46:28+01:00",
            "author": "Dimitri Papadopoulos",
            "commit_message": "Fix typos found by copdespell",
            "additions": 1,
            "deletions": 1,
            "change_type": "MODIFY",
            "diff": "@@ -1987,7 +1987,7 @@ def test_ufunc_at_basic(self, a):\n             # second operand cannot be converted to an array\n             np.add.at(a, [2, 5, 3], [[1, 2], 1])\n \n-    # ufuncs with indexed loops for perfomance in ufunc.at\n+    # ufuncs with indexed loops for performance in ufunc.at\n     indexed_ufuncs = [np.add, np.subtract, np.multiply, np.floor_divide,\n                       np.maximum, np.minimum, np.fmax, np.fmin]\n \n",
            "comment_added_diff": {
                "1990": "    # ufuncs with indexed loops for performance in ufunc.at"
            },
            "comment_deleted_diff": {
                "1990": "    # ufuncs with indexed loops for perfomance in ufunc.at"
            },
            "comment_modified_diff": {
                "1990": "    # ufuncs with indexed loops for perfomance in ufunc.at"
            }
        },
        {
            "commit": "47df67bb6c83d68c3254b69a44426b02d3f24caa",
            "timestamp": "2023-03-27T09:55:35+03:00",
            "author": "mattip",
            "commit_message": "BUG: in the fastest path form ufunc.at, properly increment args[2]",
            "additions": 11,
            "deletions": 0,
            "change_type": "MODIFY",
            "diff": "@@ -2054,6 +2054,17 @@ def test_ufunc_at_negative(self):\n         # If it is [-1, -1, -1, -100, 0] then the regular strided loop was used\n         assert np.all(arr == [-1, -1, -1, -200, -1])\n \n+    def test_ufunc_at_large(self):\n+        # issue gh-23457\n+        indices = np.zeros(8195, dtype=np.int16)\n+        b = np.zeros(8195, dtype=float)\n+        b[0] = 10\n+        b[1] = 5\n+        b[8192:] = 100\n+        a = np.zeros(1, dtype=float)\n+        np.add.at(a, indices, b)\n+        assert a[0] == b.sum()\n+\n     def test_cast_index_fastpath(self):\n         arr = np.zeros(10)\n         values = np.ones(100000)\n",
            "comment_added_diff": {
                "2058": "        # issue gh-23457"
            },
            "comment_deleted_diff": {},
            "comment_modified_diff": {}
        },
        {
            "commit": "ca3df13ea111d08ac1b365040b45c13117510ded",
            "timestamp": "2023-04-26T14:57:52+02:00",
            "author": "Sebastian Berg",
            "commit_message": "MAINT: Fixup handling of subarray dtype in ufunc.resolve_dtypes\n\nThis is now OK to just support, we won't replace things and things\nshould work out for the most part (probably).",
            "additions": 3,
            "deletions": 3,
            "change_type": "MODIFY",
            "diff": "@@ -2865,10 +2865,10 @@ def test_weird_dtypes(self):\n         r = np.equal.resolve_dtypes((S0, S0, None))\n         assert r == (S0, S0, np.dtype(bool))\n \n-        # Subarray dtypes are weird and only really exist nested, they need\n-        # the shift to full NEP 50 to be implemented nicely:\n+        # Subarray dtypes are weird and may not work fully, we preserve them\n+        # leading to a TypeError (currently no equal loop for void/structured)\n         dts = np.dtype(\"10i\")\n-        with pytest.raises(NotImplementedError):\n+        with pytest.raises(TypeError):\n             np.equal.resolve_dtypes((dts, dts, None))\n \n     def test_resolve_dtypes_reduction(self):\n",
            "comment_added_diff": {
                "2868": "        # Subarray dtypes are weird and may not work fully, we preserve them",
                "2869": "        # leading to a TypeError (currently no equal loop for void/structured)"
            },
            "comment_deleted_diff": {
                "2868": "        # Subarray dtypes are weird and only really exist nested, they need",
                "2869": "        # the shift to full NEP 50 to be implemented nicely:"
            },
            "comment_modified_diff": {
                "2868": "        # Subarray dtypes are weird and only really exist nested, they need",
                "2869": "        # the shift to full NEP 50 to be implemented nicely:"
            }
        },
        {
            "commit": "425600ab746aab7d232d9a0136af7474f2091f03",
            "timestamp": "2023-05-29T12:17:07+03:00",
            "author": "mattip",
            "commit_message": "BUG: skip test that fails because of cython 5411",
            "additions": 10,
            "deletions": 0,
            "change_type": "MODIFY",
            "diff": "@@ -20,6 +20,13 @@\n from numpy.compat import pickle\n \n \n+import cython\n+from packaging.version import parse, Version\n+\n+# Remove this when cython fixes https://github.com/cython/cython/issues/5411\n+cython_version = parse(cython.__version__)\n+BUG_5411 = Version(\"3.0.0a7\") <= cython_version <= Version(\"3.0.0b3\")\n+\n UNARY_UFUNCS = [obj for obj in np.core.umath.__dict__.values()\n                     if isinstance(obj, np.ufunc)]\n UNARY_OBJECT_UFUNCS = [uf for uf in UNARY_UFUNCS if \"O->O\" in uf.types]\n@@ -203,6 +210,9 @@ def test_pickle_withstring(self):\n                    b\"(S'numpy.core.umath'\\np1\\nS'cos'\\np2\\ntp3\\nRp4\\n.\")\n         assert_(pickle.loads(astring) is np.cos)\n \n+    @pytest.mark.skipif(BUG_5411,\n+        reason=(\"cython raises a AttributeError where it should raise a \"\n+                \"ModuleNotFoundError\"))\n     @pytest.mark.skipif(IS_PYPY, reason=\"'is' check does not work on PyPy\")\n     def test_pickle_name_is_qualname(self):\n         # This tests that a simplification of our ufunc pickle code will\n",
            "comment_added_diff": {
                "26": "# Remove this when cython fixes https://github.com/cython/cython/issues/5411"
            },
            "comment_deleted_diff": {},
            "comment_modified_diff": {}
        },
        {
            "commit": "83be397482646fdd373a6f82bce6c7b8a93899bc",
            "timestamp": "2023-06-13T09:26:30+02:00",
            "author": "Sebastian Berg",
            "commit_message": "TST: Extobj= kwarg is removed, so rephrase the test slightly",
            "additions": 2,
            "deletions": 3,
            "change_type": "MODIFY",
            "diff": "@@ -53,9 +53,8 @@ def test_sig_dtype(self):\n         assert_raises(TypeError, np.add, 1, 2, signature='ii->i',\n                       dtype=int)\n \n-    def test_extobj_refcount(self):\n-        # Should not segfault with USE_DEBUG.\n-        assert_raises(TypeError, np.add, 1, 2, extobj=[4096], parrot=True)\n+    def test_extobj_removed(self):\n+        assert_raises(TypeError, np.add, 1, 2, extobj=[4096])\n \n \n class TestUfuncGenericLoops:\n",
            "comment_added_diff": {},
            "comment_deleted_diff": {
                "57": "        # Should not segfault with USE_DEBUG."
            },
            "comment_modified_diff": {}
        },
        {
            "commit": "b774b3a50091b61ec8719819d377924a4df6b125",
            "timestamp": "2023-07-09T10:10:34+03:00",
            "author": "mattip",
            "commit_message": "BUG: properly handle negative indexes in ufunc_at fast path",
            "additions": 8,
            "deletions": 0,
            "change_type": "MODIFY",
            "diff": "@@ -2224,6 +2224,14 @@ def test_ufunc_at_advanced(self):\n         np.maximum.at(a, [0], 0)\n         assert_equal(a, np.array([1, 2, 3]))\n \n+    def test_at_negative_indexes(self):\n+        a = np.arange(10)\n+        indxs = np.array([-1, 1, -1, 2])\n+        np.add.at(a, indxs, 1)\n+        assert a[-1] == 11  # issue 24147\n+        assert a[1] == 2\n+        assert a[2] == 3\n+\n     def test_at_not_none_signature(self):\n         # Test ufuncs with non-trivial signature raise a TypeError\n         a = np.ones((2, 2, 2))\n",
            "comment_added_diff": {
                "2231": "        assert a[-1] == 11  # issue 24147"
            },
            "comment_deleted_diff": {},
            "comment_modified_diff": {}
        },
        {
            "commit": "a4c582d4b3c530176be393ac596dd00130367ddb",
            "timestamp": "2023-07-18T08:53:25+02:00",
            "author": "Dimitri Papadopoulos",
            "commit_message": "MAINT: Fix new or residual typos found by codespell",
            "additions": 2,
            "deletions": 2,
            "change_type": "MODIFY",
            "diff": "@@ -1705,11 +1705,11 @@ def test_initial_reduction(self):\n         res = np.add.reduce(a, initial=5)\n         assert_equal(res, 15)\n \n-    def test_empty_reduction_and_idenity(self):\n+    def test_empty_reduction_and_identity(self):\n         arr = np.zeros((0, 5))\n         # OK, since the reduction itself is *not* empty, the result is\n         assert np.true_divide.reduce(arr, axis=1).shape == (0,)\n-        # Not OK, the reduction itself is empty and we have no idenity\n+        # Not OK, the reduction itself is empty and we have no identity\n         with pytest.raises(ValueError):\n             np.true_divide.reduce(arr, axis=0)\n \n",
            "comment_added_diff": {
                "1712": "        # Not OK, the reduction itself is empty and we have no identity"
            },
            "comment_deleted_diff": {
                "1712": "        # Not OK, the reduction itself is empty and we have no idenity"
            },
            "comment_modified_diff": {
                "1712": "        # Not OK, the reduction itself is empty and we have no idenity"
            }
        },
        {
            "commit": "ee16c2222216c39caf0d79dca007a3d706fd25ff",
            "timestamp": "2023-07-28T21:25:42+02:00",
            "author": "Sebastian Berg",
            "commit_message": "BUG: Further fixes to indexing loop and added tests\n\nThis is a follow-up to gh-24272 which missed a few files.",
            "additions": 15,
            "deletions": 7,
            "change_type": "MODIFY",
            "diff": "@@ -2224,13 +2224,21 @@ def test_ufunc_at_advanced(self):\n         np.maximum.at(a, [0], 0)\n         assert_equal(a, np.array([1, 2, 3]))\n \n-    def test_at_negative_indexes(self):\n-        a = np.arange(10)\n-        indxs = np.array([-1, 1, -1, 2])\n-        np.add.at(a, indxs, 1)\n-        assert a[-1] == 11  # issue 24147\n-        assert a[1] == 2\n-        assert a[2] == 3\n+    @pytest.mark.parametrize(\"dtype\",\n+            np.typecodes['AllInteger'] + np.typecodes['Float'])\n+    @pytest.mark.parametrize(\"ufunc\",\n+            [np.add, np.subtract, np.divide, np.minimum, np.maximum])\n+    def test_at_negative_indexes(self, dtype, ufunc):\n+        a = np.arange(0, 10).astype(dtype)\n+        indxs = np.array([-1, 1, -1, 2]).astype(np.intp)\n+        vals = np.array([1, 5, 2, 10], dtype=a.dtype)\n+\n+        expected = a.copy()\n+        for i, v in zip(indxs, vals):\n+            expected[i] = ufunc(expected[i], v)\n+\n+        ufunc.at(a, indxs, vals)\n+        assert_array_equal(a, expected)\n         assert np.all(indxs == [-1, 1, -1, 2])\n \n     def test_at_not_none_signature(self):\n",
            "comment_added_diff": {},
            "comment_deleted_diff": {
                "2231": "        assert a[-1] == 11  # issue 24147"
            },
            "comment_modified_diff": {}
        },
        {
            "commit": "08047d51e0771e2b42f51a75d2936bdd39a43338",
            "timestamp": "2023-09-27T23:31:27+00:00",
            "author": "ganesh-k13",
            "commit_message": "TST: Misc bitwise_count changes\n\n* Changed popcount -> bitwise_count\n* Added comment on why we remove certain function in float64 object\n  loops\n* Comment on when we skip bitwise_count",
            "additions": 2,
            "deletions": 0,
            "change_type": "MODIFY",
            "diff": "@@ -32,6 +32,8 @@\n UNARY_UFUNCS = [obj for obj in np.core.umath.__dict__.values()\n                     if isinstance(obj, np.ufunc)]\n UNARY_OBJECT_UFUNCS = [uf for uf in UNARY_UFUNCS if \"O->O\" in uf.types]\n+\n+# Remove functions that do not support `floats`\n UNARY_OBJECT_UFUNCS.remove(getattr(np, 'bitwise_count'))\n \n \n",
            "comment_added_diff": {
                "36": "# Remove functions that do not support `floats`"
            },
            "comment_deleted_diff": {},
            "comment_modified_diff": {}
        }
    ],
    "scalarmath.c.src": [],
    "test_nep50_promotions.py": [
        {
            "commit": "409cccf70341a9ef9c80138dd9569f6ca3720cef",
            "timestamp": "2022-10-12T10:41:37+02:00",
            "author": "Sebastian Berg",
            "commit_message": "ENH: Implement safe integers and weak python scalars for scalars\n\nThis requires adding a path that uses the \"normal\" Python object\nto dtype conversion (setitem) function, rather than always converting\nto the default dtype.",
            "additions": 16,
            "deletions": 1,
            "change_type": "MODIFY",
            "diff": "@@ -30,7 +30,7 @@ def test_nep50_examples():\n     assert res.dtype == np.int64\n \n     with pytest.warns(UserWarning, match=\"result dtype changed\"):\n-        # Note: Should warn (error with the errstate), but does not:\n+        # Note: Overflow would be nice, but does not warn with change warning\n         with np.errstate(over=\"raise\"):\n             res = np.uint8(100) + 200\n     assert res.dtype == np.uint8\n@@ -61,6 +61,21 @@ def test_nep50_examples():\n     assert res.dtype == np.float64\n \n \n+def test_nep50_without_warnings():\n+    # Test that avoid the \"warn\" method, since that may lead to different\n+    # code paths in some cases.\n+    # Set promotion to weak (no warning), the auto-fixture will reset it.\n+    np._set_promotion_state(\"weak\")\n+    with np.errstate(over=\"warn\"):\n+        with pytest.warns(RuntimeWarning):\n+            res = np.uint8(100) + 200\n+    assert res.dtype == np.uint8\n+\n+    with pytest.warns(RuntimeWarning):\n+        res = np.float32(1) + 3e100\n+    assert res.dtype == np.float32\n+\n+\n @pytest.mark.xfail\n def test_nep50_integer_conversion_errors():\n     # Implementation for error paths is mostly missing (as of writing)\n",
            "comment_added_diff": {
                "33": "        # Note: Overflow would be nice, but does not warn with change warning",
                "65": "    # Test that avoid the \"warn\" method, since that may lead to different",
                "66": "    # code paths in some cases.",
                "67": "    # Set promotion to weak (no warning), the auto-fixture will reset it."
            },
            "comment_deleted_diff": {
                "33": "        # Note: Should warn (error with the errstate), but does not:"
            },
            "comment_modified_diff": {
                "33": "        # Note: Should warn (error with the errstate), but does not:"
            }
        },
        {
            "commit": "d894650a624234bd1bc7fb20d74a58b44715622c",
            "timestamp": "2022-10-12T10:41:40+02:00",
            "author": "Sebastian Berg",
            "commit_message": "ENH: Get correct integer conversion into ufuncs",
            "additions": 6,
            "deletions": 3,
            "change_type": "MODIFY",
            "diff": "@@ -76,12 +76,15 @@ def test_nep50_without_warnings():\n     assert res.dtype == np.float32\n \n \n-@pytest.mark.xfail\n def test_nep50_integer_conversion_errors():\n+    # Do not worry about warnings here (auto-fixture will reset).\n+    np._set_promotion_state(\"weak\")\n     # Implementation for error paths is mostly missing (as of writing)\n-    with pytest.raises(ValueError):  # (or TypeError?)\n+    with pytest.raises(OverflowError, match=\".*uint8\"):\n         np.array([1], np.uint8) + 300\n \n-    with pytest.raises(ValueError):  # (or TypeError?)\n+    with pytest.raises(OverflowError, match=\".*uint8\"):\n         np.uint8(1) + 300\n \n+    with pytest.raises(OverflowError, match=\".*unsigned int\"):\n+        np.uint8(1) + -1\n",
            "comment_added_diff": {
                "80": "    # Do not worry about warnings here (auto-fixture will reset)."
            },
            "comment_deleted_diff": {
                "82": "    with pytest.raises(ValueError):  # (or TypeError?)",
                "85": "    with pytest.raises(ValueError):  # (or TypeError?)"
            },
            "comment_modified_diff": {}
        },
        {
            "commit": "0a0f96fe86cea5009730def695c27b5a334bd6c1",
            "timestamp": "2022-10-12T10:41:40+02:00",
            "author": "Sebastian Berg",
            "commit_message": "TST: Fix int overflow test for windows platforms where message differs",
            "additions": 2,
            "deletions": 1,
            "change_type": "MODIFY",
            "diff": "@@ -86,5 +86,6 @@ def test_nep50_integer_conversion_errors():\n     with pytest.raises(OverflowError, match=\".*uint8\"):\n         np.uint8(1) + 300\n \n-    with pytest.raises(OverflowError, match=\".*unsigned int\"):\n+    # Error message depends on platform (maybe unsigned int or unsigned long)\n+    with pytest.raises(OverflowError, match=\".*unsigned\"):\n         np.uint8(1) + -1\n",
            "comment_added_diff": {
                "89": "    # Error message depends on platform (maybe unsigned int or unsigned long)"
            },
            "comment_deleted_diff": {},
            "comment_modified_diff": {
                "89": "    with pytest.raises(OverflowError, match=\".*unsigned int\"):"
            }
        },
        {
            "commit": "fed11cd2553dc3b9742a1540cd3805c5689c1ec0",
            "timestamp": "2022-10-12T10:41:40+02:00",
            "author": "Sebastian Berg",
            "commit_message": "BUG: Ensure new-style promotion is not accidentally used for some ints\n\nThis is the *actual* correct fix for the test adaptations.  The test\nadaptations should only be necessary when running in weak-promotion mode,\nbut they are NOT doing that currently.",
            "additions": 9,
            "deletions": 0,
            "change_type": "MODIFY",
            "diff": "@@ -89,3 +89,12 @@ def test_nep50_integer_conversion_errors():\n     # Error message depends on platform (maybe unsigned int or unsigned long)\n     with pytest.raises(OverflowError, match=\".*unsigned\"):\n         np.uint8(1) + -1\n+\n+\n+def test_nep50_integer_regression():\n+    # Test the old integer promotion rules.  When the integer is too large,\n+    # we need to keep using the old-style promotion.\n+    np._set_promotion_state(\"legacy\")\n+    arr = np.array(1)\n+    assert (arr + 2**63).dtype == np.float64\n+    assert (arr[()] + 2**63).dtype == np.float64\n",
            "comment_added_diff": {
                "95": "    # Test the old integer promotion rules.  When the integer is too large,",
                "96": "    # we need to keep using the old-style promotion."
            },
            "comment_deleted_diff": {},
            "comment_modified_diff": {}
        },
        {
            "commit": "d47b09c047c2128e820f4d4f27f10b3fd7d8b05b",
            "timestamp": "2022-10-12T10:41:40+02:00",
            "author": "Sebastian Berg",
            "commit_message": "TST: Improve tests for python integer special cases",
            "additions": 43,
            "deletions": 9,
            "change_type": "MODIFY",
            "diff": "@@ -61,19 +61,53 @@ def test_nep50_examples():\n     assert res.dtype == np.float64\n \n \n-def test_nep50_without_warnings():\n-    # Test that avoid the \"warn\" method, since that may lead to different\n-    # code paths in some cases.\n-    # Set promotion to weak (no warning), the auto-fixture will reset it.\n+@pytest.mark.parametrize(\"dtype\", np.typecodes[\"AllInteger\"])\n+def test_nep50_weak_integers(dtype):\n+    # Avoids warning (different code path for scalars)\n     np._set_promotion_state(\"weak\")\n+    scalar_type = np.dtype(dtype).type\n+\n+    maxint = int(np.iinfo(dtype).max)\n+\n     with np.errstate(over=\"warn\"):\n         with pytest.warns(RuntimeWarning):\n-            res = np.uint8(100) + 200\n-    assert res.dtype == np.uint8\n+            res = scalar_type(100) + maxint\n+    assert res.dtype == dtype\n \n-    with pytest.warns(RuntimeWarning):\n-        res = np.float32(1) + 3e100\n-    assert res.dtype == np.float32\n+    # Array operations are not expected to warn, but should give the same\n+    # result dtype.\n+    res = np.array(100, dtype=dtype) + maxint\n+    assert res.dtype == dtype\n+\n+\n+@pytest.mark.parametrize(\"dtype\", np.typecodes[\"AllFloat\"])\n+def test_nep50_weak_integers_with_inexact(dtype):\n+    # Avoids warning (different code path for scalars)\n+    np._set_promotion_state(\"weak\")\n+    scalar_type = np.dtype(dtype).type\n+\n+    too_big_int = int(np.finfo(dtype).max) * 2\n+\n+    if dtype in \"dDG\":\n+        # These dtypes currently convert to Python float internally, which\n+        # raises an OverflowError, while the other dtypes overflow to inf.\n+        # NOTE: It may make sense to normalize the behavior!\n+        with pytest.raises(OverflowError):\n+            scalar_type(1) + too_big_int\n+\n+        with pytest.raises(OverflowError):\n+            np.array(1, dtype=dtype) + too_big_int\n+    else:\n+        # Otherwise, we overflow to infinity:\n+        with pytest.warns(RuntimeWarning):\n+            res = scalar_type(1) + too_big_int\n+        assert res.dtype == dtype\n+        assert res == np.inf\n+\n+        with pytest.warns(RuntimeWarning):\n+            res = np.array(1, dtype=dtype) + too_big_int\n+        assert res.dtype == dtype\n+        assert res == np.inf\n \n \n def test_nep50_integer_conversion_errors():\n",
            "comment_added_diff": {
                "66": "    # Avoids warning (different code path for scalars)",
                "77": "    # Array operations are not expected to warn, but should give the same",
                "78": "    # result dtype.",
                "85": "    # Avoids warning (different code path for scalars)",
                "92": "        # These dtypes currently convert to Python float internally, which",
                "93": "        # raises an OverflowError, while the other dtypes overflow to inf.",
                "94": "        # NOTE: It may make sense to normalize the behavior!",
                "101": "        # Otherwise, we overflow to infinity:"
            },
            "comment_deleted_diff": {
                "65": "    # Test that avoid the \"warn\" method, since that may lead to different",
                "66": "    # code paths in some cases.",
                "67": "    # Set promotion to weak (no warning), the auto-fixture will reset it."
            },
            "comment_modified_diff": {
                "66": "    # code paths in some cases."
            }
        },
        {
            "commit": "c4f1419c0484e2772f181210bfc7a9b86267a5d3",
            "timestamp": "2022-10-12T10:41:40+02:00",
            "author": "Sebastian Berg",
            "commit_message": "TST: Rebustify NEP 50 integer tests for windows and 32bit linux",
            "additions": 14,
            "deletions": 1,
            "change_type": "MODIFY",
            "diff": "@@ -98,6 +98,16 @@ def test_nep50_weak_integers_with_inexact(dtype):\n         with pytest.raises(OverflowError):\n             np.array(1, dtype=dtype) + too_big_int\n     else:\n+        # NumPy uses (or used) `int -> string -> longdouble` for the\n+        # conversion.  But Python may refuse `str(int)` for huge ints.\n+        # In that case, RuntimeWarning would be correct, but conversion\n+        # fails earlier (seems to happen on 32bit linux, possibly only debug).\n+        if dtype in \"gG\":\n+            try:\n+                str(too_big_int)\n+            except ValueError:\n+                pytest.skip(\"`huge_int -> string -> longdouble` failed\")\n+\n         # Otherwise, we overflow to infinity:\n         with pytest.warns(RuntimeWarning):\n             res = scalar_type(1) + too_big_int\n@@ -105,7 +115,10 @@ def test_nep50_weak_integers_with_inexact(dtype):\n         assert res == np.inf\n \n         with pytest.warns(RuntimeWarning):\n-            res = np.array(1, dtype=dtype) + too_big_int\n+            # We force the dtype here, since windows may otherwise pick the\n+            # double instead of the longdouble loop.  That leads to slightly\n+            # different results (conversion of the int fails as above).\n+            res = np.add(np.array(1, dtype=dtype), too_big_int, dtype=dtype)\n         assert res.dtype == dtype\n         assert res == np.inf\n \n",
            "comment_added_diff": {
                "101": "        # NumPy uses (or used) `int -> string -> longdouble` for the",
                "102": "        # conversion.  But Python may refuse `str(int)` for huge ints.",
                "103": "        # In that case, RuntimeWarning would be correct, but conversion",
                "104": "        # fails earlier (seems to happen on 32bit linux, possibly only debug).",
                "118": "            # We force the dtype here, since windows may otherwise pick the",
                "119": "            # double instead of the longdouble loop.  That leads to slightly",
                "120": "            # different results (conversion of the int fails as above)."
            },
            "comment_deleted_diff": {},
            "comment_modified_diff": {}
        },
        {
            "commit": "4ecc03549750e733690d057f7a9c06909788a586",
            "timestamp": "2022-10-12T10:41:40+02:00",
            "author": "Sebastian Berg",
            "commit_message": "TST: Cover additional NEP 50 scalar paths\n\nEspecially adding coverage also for the power operator which\ndoes not share its code perfectly.",
            "additions": 22,
            "deletions": 0,
            "change_type": "MODIFY",
            "diff": "@@ -4,7 +4,10 @@\n is adopted in the main test suite.  A few may be moved elsewhere.\n \"\"\"\n \n+import operator\n+\n import numpy as np\n+\n import pytest\n \n \n@@ -123,6 +126,25 @@ def test_nep50_weak_integers_with_inexact(dtype):\n         assert res == np.inf\n \n \n+@pytest.mark.parametrize(\"op\", [operator.add, operator.pow, operator.eq])\n+def test_weak_promotion_scalar_path(op):\n+    # Some additional paths excercising the weak scalars.\n+    np._set_promotion_state(\"weak\")\n+\n+    # Integer path:\n+    res = op(np.uint8(3), 5)\n+    assert res == op(3, 5)\n+    assert res.dtype == np.uint8 or res.dtype == bool\n+\n+    with pytest.raises(OverflowError):\n+        op(np.uint8(3), 1000)\n+\n+    # Float path:\n+    res = op(np.float32(3), 5.)\n+    assert res == op(3., 5.)\n+    assert res.dtype == np.float32 or res.dtype == bool\n+\n+\n def test_nep50_complex_promotion():\n     np._set_promotion_state(\"weak\")\n \n",
            "comment_added_diff": {
                "131": "    # Some additional paths excercising the weak scalars.",
                "134": "    # Integer path:",
                "142": "    # Float path:"
            },
            "comment_deleted_diff": {},
            "comment_modified_diff": {}
        },
        {
            "commit": "ee3c20b9eafa840232ae3e4bfa85ec262fcfa578",
            "timestamp": "2022-10-12T17:13:05+02:00",
            "author": "Sebastian Berg",
            "commit_message": "MAINT: Minor include fixup in scalarmath after rebase and comment fixup",
            "additions": 2,
            "deletions": 1,
            "change_type": "MODIFY",
            "diff": "@@ -33,7 +33,8 @@ def test_nep50_examples():\n     assert res.dtype == np.int64\n \n     with pytest.warns(UserWarning, match=\"result dtype changed\"):\n-        # Note: Overflow would be nice, but does not warn with change warning\n+        # Note: For \"weak_and_warn\" promotion state the overflow warning is\n+        #       unfortunately not given (because we use the full array path).\n         with np.errstate(over=\"raise\"):\n             res = np.uint8(100) + 200\n     assert res.dtype == np.uint8\n",
            "comment_added_diff": {
                "36": "        # Note: For \"weak_and_warn\" promotion state the overflow warning is",
                "37": "        #       unfortunately not given (because we use the full array path)."
            },
            "comment_deleted_diff": {
                "36": "        # Note: Overflow would be nice, but does not warn with change warning"
            },
            "comment_modified_diff": {
                "36": "        # Note: Overflow would be nice, but does not warn with change warning"
            }
        },
        {
            "commit": "518313ba108a86ac803a723e81ed858a53c368ae",
            "timestamp": "2022-11-08T15:27:12+01:00",
            "author": "Sebastian Berg",
            "commit_message": "ENH: Ensure we will transition to an error for `np.r_[int8_arr, 300]`\n\nThat is, once the NEP 50 transition happens",
            "additions": 8,
            "deletions": 0,
            "change_type": "MODIFY",
            "diff": "@@ -178,3 +178,11 @@ def test_nep50_integer_regression():\n     arr = np.array(1)\n     assert (arr + 2**63).dtype == np.float64\n     assert (arr[()] + 2**63).dtype == np.float64\n+\n+def test_nep50_with_axisconcatenator():\n+    # I promised that this will be an error in the future in the 1.24\n+    # release notes;  test this (NEP 50 opt-in makes the deprecation an error).\n+    np._set_promotion_state(\"weak\")\n+\n+    with pytest.raises(OverflowError):\n+        np.r_[np.arange(5, dtype=np.int8), 255]\n",
            "comment_added_diff": {
                "183": "    # I promised that this will be an error in the future in the 1.24",
                "184": "    # release notes;  test this (NEP 50 opt-in makes the deprecation an error)."
            },
            "comment_deleted_diff": {},
            "comment_modified_diff": {}
        },
        {
            "commit": "e1e487acf1d820cbab8a6f97986bf2fb451dfa8e",
            "timestamp": "2023-02-11T22:46:28+01:00",
            "author": "Dimitri Papadopoulos",
            "commit_message": "Fix typos found by copdespell",
            "additions": 1,
            "deletions": 1,
            "change_type": "MODIFY",
            "diff": "@@ -131,7 +131,7 @@ def test_nep50_weak_integers_with_inexact(dtype):\n \n @pytest.mark.parametrize(\"op\", [operator.add, operator.pow, operator.eq])\n def test_weak_promotion_scalar_path(op):\n-    # Some additional paths excercising the weak scalars.\n+    # Some additional paths exercising the weak scalars.\n     np._set_promotion_state(\"weak\")\n \n     # Integer path:\n",
            "comment_added_diff": {
                "134": "    # Some additional paths exercising the weak scalars."
            },
            "comment_deleted_diff": {
                "134": "    # Some additional paths excercising the weak scalars."
            },
            "comment_modified_diff": {
                "134": "    # Some additional paths excercising the weak scalars."
            }
        },
        {
            "commit": "b786189222ac5bf2f4efbb04399261f7f760bc18",
            "timestamp": "2023-02-19T12:10:25-07:00",
            "author": "Charles Harris",
            "commit_message": "MAINT: Update release from 1.24 to 1.25",
            "additions": 1,
            "deletions": 1,
            "change_type": "MODIFY",
            "diff": "@@ -180,7 +180,7 @@ def test_nep50_integer_regression():\n     assert (arr[()] + 2**63).dtype == np.float64\n \n def test_nep50_with_axisconcatenator():\n-    # I promised that this will be an error in the future in the 1.24\n+    # I promised that this will be an error in the future in the 1.25\n     # release notes;  test this (NEP 50 opt-in makes the deprecation an error).\n     np._set_promotion_state(\"weak\")\n \n",
            "comment_added_diff": {
                "183": "    # I promised that this will be an error in the future in the 1.25"
            },
            "comment_deleted_diff": {
                "183": "    # I promised that this will be an error in the future in the 1.24"
            },
            "comment_modified_diff": {
                "183": "    # I promised that this will be an error in the future in the 1.24"
            }
        },
        {
            "commit": "21602a8b1673a7b468d032ef19c20c53ac15c0b9",
            "timestamp": "2023-05-15T12:43:30+02:00",
            "author": "Sebastian Berg",
            "commit_message": "BUG: Fix weak scalar logic for large ints in ufuncs\n\nThis fixes it, breaks warnings (partially), but most or all of\nthose paths should be errors anyway.",
            "additions": 32,
            "deletions": 0,
            "change_type": "MODIFY",
            "diff": "@@ -181,6 +181,7 @@ def test_nep50_integer_regression():\n     assert (arr + 2**63).dtype == np.float64\n     assert (arr[()] + 2**63).dtype == np.float64\n \n+\n def test_nep50_with_axisconcatenator():\n     # I promised that this will be an error in the future in the 1.25\n     # release notes;  test this (NEP 50 opt-in makes the deprecation an error).\n@@ -188,3 +189,34 @@ def test_nep50_with_axisconcatenator():\n \n     with pytest.raises(OverflowError):\n         np.r_[np.arange(5, dtype=np.int8), 255]\n+\n+\n+@pytest.mark.parametrize(\"ufunc\", [np.add, np.power])\n+@pytest.mark.parametrize(\"state\", [\"weak\", \"weak_and_warn\"])\n+def test_nep50_huge_integers(ufunc, state):\n+    # Very large integers are complicated, because they go to uint64 or\n+    # object dtype.  When mixed with another uint64 that should\n+    np._set_promotion_state(state)\n+\n+    with pytest.raises(OverflowError):\n+        ufunc(np.int64(0), 2**63)  # 2**63 too large for int64\n+\n+    if state == \"weak_and_warn\":\n+        with pytest.warns(UserWarning,\n+                match=\"result dtype changed.*float64.*uint64\"):\n+            with pytest.raises(OverflowError):\n+                ufunc(np.uint64(0), 2**64)\n+    else:\n+        with pytest.raises(OverflowError):\n+            ufunc(np.uint64(0), 2**64)  # 2**64 cannot be represented by uint64\n+\n+    # However, 2**63 can be represented by the uint64 (and that is used):\n+    if state == \"weak_and_warn\":\n+        with pytest.warns(UserWarning,\n+                match=\"result dtype changed.*float64.*uint64\"):\n+            res = ufunc(np.uint64(1), 2**63)\n+    else:\n+        res = ufunc(np.uint64(1), 2**63)\n+\n+    assert res.dtype == np.uint64\n+    assert res == ufunc(1, 2**63, dtype=object)\n",
            "comment_added_diff": {
                "197": "    # Very large integers are complicated, because they go to uint64 or",
                "198": "    # object dtype.  When mixed with another uint64 that should",
                "202": "        ufunc(np.int64(0), 2**63)  # 2**63 too large for int64",
                "211": "            ufunc(np.uint64(0), 2**64)  # 2**64 cannot be represented by uint64",
                "213": "    # However, 2**63 can be represented by the uint64 (and that is used):"
            },
            "comment_deleted_diff": {},
            "comment_modified_diff": {}
        },
        {
            "commit": "626d32fc47a9803e706168ea2d72d3584a4696bd",
            "timestamp": "2023-05-15T17:11:06+02:00",
            "author": "Sebastian Berg",
            "commit_message": "MAINT: Address Marten's review",
            "additions": 13,
            "deletions": 1,
            "change_type": "MODIFY",
            "diff": "@@ -195,7 +195,8 @@ def test_nep50_with_axisconcatenator():\n @pytest.mark.parametrize(\"state\", [\"weak\", \"weak_and_warn\"])\n def test_nep50_huge_integers(ufunc, state):\n     # Very large integers are complicated, because they go to uint64 or\n-    # object dtype.  When mixed with another uint64 that should\n+    # object dtype.  This tests covers a few possible paths (some of which\n+    # cannot give the NEP 50 warnings).\n     np._set_promotion_state(state)\n \n     with pytest.raises(OverflowError):\n@@ -220,3 +221,14 @@ def test_nep50_huge_integers(ufunc, state):\n \n     assert res.dtype == np.uint64\n     assert res == ufunc(1, 2**63, dtype=object)\n+\n+    # The following paths fail to warn correctly about the change:\n+    with pytest.raises(OverflowError):\n+        ufunc(np.int64(1), 2**63)  # np.array(2**63) would go to uint\n+\n+    with pytest.raises(OverflowError):\n+        ufunc(np.int64(1), 2**100)  # np.array(2**100) would go to object\n+\n+    # This would go to object and thus a Python float, not a NumPy one:\n+    res = ufunc(1.0, 2**100)\n+    assert isinstance(res, np.float64)\n",
            "comment_added_diff": {
                "198": "    # object dtype.  This tests covers a few possible paths (some of which",
                "199": "    # cannot give the NEP 50 warnings).",
                "225": "    # The following paths fail to warn correctly about the change:",
                "227": "        ufunc(np.int64(1), 2**63)  # np.array(2**63) would go to uint",
                "230": "        ufunc(np.int64(1), 2**100)  # np.array(2**100) would go to object",
                "232": "    # This would go to object and thus a Python float, not a NumPy one:"
            },
            "comment_deleted_diff": {
                "198": "    # object dtype.  When mixed with another uint64 that should"
            },
            "comment_modified_diff": {
                "198": "    # object dtype.  When mixed with another uint64 that should"
            }
        },
        {
            "commit": "bc9c57858abf9071c40a308337d30fb6cd56b1a2",
            "timestamp": "2023-09-11T18:06:45+02:00",
            "author": "Sebastian Berg",
            "commit_message": "BUG: Fix weak promotion with some mixed float/int dtypes\n\nThis reorganizes/simplifies promotion a bit, but unfortunately this\nmeans moving the promotion of our concrete DTypes with the abstract\n\"weak\" ones into the concrete DTypes making the order of \"who deals\nwith whom\" fully defined by:\n\n    complex' > floats > pycomplex > pyfloat > ints > pyint > bool\n\nwhere py<...> is the DType corresponding to the Python type.\nThere are two points here:\n* \"inexact\" is the meaningful category not complex\n* previously, the abstract DTypes `pyfloat`, etc. knew about the\n  concrete ones and there was some special handling for that.\n  (as mentioned above, this is gone now)\n\nWhy was it this complicated?!  I think the initial code wanted to\ndeal with value-based promotion and the abstract DTypes could have\nhad a value attached to them.\nThere was thus more code later on, dealing with abstract DTypes\nmore concretely (rather than identical to the rest).\n\nThis is now gone, concrete DTypes promote basically the same as\ntheir concrete counter-parts.  This makes the promotion logic simpler\n(and I think still just as correct), although the concrete type\npromotion now needs to know about it.",
            "additions": 27,
            "deletions": 0,
            "change_type": "MODIFY",
            "diff": "@@ -9,6 +9,9 @@\n import numpy as np\n \n import pytest\n+import hypothesis\n+from hypothesis import strategies\n+\n from numpy.testing import IS_WASM\n \n \n@@ -244,3 +247,27 @@ def test_nep50_in_concat_and_choose():\n     with pytest.warns(UserWarning, match=\"result dtype changed\"):\n         res = np.choose(1, [np.float32(1), 1.])\n     assert res.dtype == \"float32\"\n+\n+\n+@pytest.mark.parametrize(\"expected,dtypes,optional_dtypes\", [\n+        (np.float32, [np.float32],\n+            [np.float16, 0.0, np.uint16, np.int16, np.int8, 0]),\n+        (np.complex64, [np.float32, 0j],\n+            [np.float16, 0.0, np.uint16, np.int16, np.int8, 0]),\n+        (np.float32, [np.int16, np.uint16, np.float16],\n+            [np.int8, np.uint8, np.float32, 0., 0]),\n+        (np.int32, [np.int16, np.uint16],\n+            [np.int8, np.uint8, 0, np.bool_]),\n+        ])\n+@hypothesis.given(data=strategies.data())\n+def test_expected_promotion(expected, dtypes, optional_dtypes, data):\n+    np._set_promotion_state(\"weak\")\n+\n+    # Sample randomly while ensuring \"dtypes\" is always present:\n+    optional = data.draw(strategies.lists(\n+            strategies.sampled_from(dtypes + optional_dtypes)))\n+    all_dtypes = dtypes + optional\n+    dtypes_sample = data.draw(strategies.permutations(all_dtypes))\n+\n+    res = np.result_type(*dtypes_sample)\n+    assert res == expected\n",
            "comment_added_diff": {
                "266": "    # Sample randomly while ensuring \"dtypes\" is always present:"
            },
            "comment_deleted_diff": {},
            "comment_modified_diff": {}
        }
    ],
    "dtypemeta.h": [],
    "test_mem_overlap.py": [],
    "ndarray_misc.py": [],
    "arraytypes.c.src": [],
    "mingw32ccompiler.py": [
        {
            "commit": "eba83419b0668d2609a4f15a43eddf921813037f",
            "timestamp": "2022-10-13T10:28:40+02:00",
            "author": "Dimitri Papadopoulos",
            "commit_message": "MAINT: always use sys.base_prefix, never use sys.real_prefix\n\n`sys.base_prefix` was introduced in Python 3.3, so it will always be\navailable in supported versions of Python >= 3.3:\n\thttps://docs.python.org/3/library/sys.html#sys.base_prefix\n\nAs a result, `sys.real_prefix` will never be used in supported versions\nof Python >= 3.3. Besides, it has been removed from recent versions of\nvirtualenv:\n\thttps://github.com/pypa/virtualenv/issues/1622",
            "additions": 1,
            "deletions": 4,
            "change_type": "MODIFY",
            "diff": "@@ -184,14 +184,11 @@ def find_python_dll():\n     # We can't do much here:\n     # - find it in the virtualenv (sys.prefix)\n     # - find it in python main dir (sys.base_prefix, if in a virtualenv)\n-    # - sys.real_prefix is main dir for virtualenvs in Python 2.7\n     # - in system32,\n     # - ortherwise (Sxs), I don't know how to get it.\n     stems = [sys.prefix]\n-    if hasattr(sys, 'base_prefix') and sys.base_prefix != sys.prefix:\n+    if sys.base_prefix != sys.prefix:\n         stems.append(sys.base_prefix)\n-    elif hasattr(sys, 'real_prefix') and sys.real_prefix != sys.prefix:\n-        stems.append(sys.real_prefix)\n \n     sub_dirs = ['', 'lib', 'bin']\n     # generate possible combinations of directory trees and sub-directories\n",
            "comment_added_diff": {},
            "comment_deleted_diff": {
                "187": "    # - sys.real_prefix is main dir for virtualenvs in Python 2.7"
            },
            "comment_modified_diff": {}
        }
    ],
    "_version.py": [
        {
            "commit": "b6d94fab5a7ef8897fe06d0b7a161bc8080fff26",
            "timestamp": "2022-10-15T22:19:57+02:00",
            "author": "Dimitri Papadopoulos",
            "commit_message": "MAINT: Update versioneer 0.19 \u2192 0.26\n\nKeep classic vendored mode.\n\nThis update will shut up this codespell warning:\n\tunparseable ==> unparsable",
            "additions": 182,
            "deletions": 47,
            "change_type": "MODIFY",
            "diff": "@@ -1,11 +1,13 @@\n+\n # This file helps to compute a version number in source trees obtained from\n # git-archive tarball (such as those provided by githubs download-from-tag\n # feature). Distribution tarballs (built by setup.py sdist) and build\n # directories (produced by setup.py build) will contain a much shorter file\n # that just contains the computed version number.\n \n-# This file is released into the public domain. Generated by\n-# versioneer-0.19 (https://github.com/python-versioneer/python-versioneer)\n+# This file is released into the public domain.\n+# Generated by versioneer-0.26\n+# https://github.com/python-versioneer/python-versioneer\n \n \"\"\"Git implementation of _version.py.\"\"\"\n \n@@ -14,6 +16,8 @@\n import re\n import subprocess\n import sys\n+from typing import Callable, Dict\n+import functools\n \n \n def get_keywords():\n@@ -51,8 +55,8 @@ class NotThisMethod(Exception):\n     \"\"\"Exception raised if a method is not valid for the current scenario.\"\"\"\n \n \n-LONG_VERSION_PY = {}\n-HANDLERS = {}\n+LONG_VERSION_PY: Dict[str, str] = {}\n+HANDLERS: Dict[str, Dict[str, Callable]] = {}\n \n \n def register_vcs_handler(vcs, method):  # decorator\n@@ -70,17 +74,25 @@ def run_command(commands, args, cwd=None, verbose=False, hide_stderr=False,\n                 env=None):\n     \"\"\"Call the given command(s).\"\"\"\n     assert isinstance(commands, list)\n-    p = None\n-    for c in commands:\n+    process = None\n+\n+    popen_kwargs = {}\n+    if sys.platform == \"win32\":\n+        # This hides the console window if pythonw.exe is used\n+        startupinfo = subprocess.STARTUPINFO()\n+        startupinfo.dwFlags |= subprocess.STARTF_USESHOWWINDOW\n+        popen_kwargs[\"startupinfo\"] = startupinfo\n+\n+    for command in commands:\n         try:\n-            dispcmd = str([c] + args)\n+            dispcmd = str([command] + args)\n             # remember shell=False, so use git.cmd on windows, not just git\n-            p = subprocess.Popen([c] + args, cwd=cwd, env=env,\n-                                 stdout=subprocess.PIPE,\n-                                 stderr=(subprocess.PIPE if hide_stderr\n-                                         else None))\n+            process = subprocess.Popen([command] + args, cwd=cwd, env=env,\n+                                       stdout=subprocess.PIPE,\n+                                       stderr=(subprocess.PIPE if hide_stderr\n+                                               else None), **popen_kwargs)\n             break\n-        except EnvironmentError:\n+        except OSError:\n             e = sys.exc_info()[1]\n             if e.errno == errno.ENOENT:\n                 continue\n@@ -92,13 +104,13 @@ def run_command(commands, args, cwd=None, verbose=False, hide_stderr=False,\n         if verbose:\n             print(\"unable to find command, tried %s\" % (commands,))\n         return None, None\n-    stdout = p.communicate()[0].strip().decode()\n-    if p.returncode != 0:\n+    stdout = process.communicate()[0].strip().decode()\n+    if process.returncode != 0:\n         if verbose:\n             print(\"unable to run %s (error)\" % dispcmd)\n             print(\"stdout was %s\" % stdout)\n-        return None, p.returncode\n-    return stdout, p.returncode\n+        return None, process.returncode\n+    return stdout, process.returncode\n \n \n def versions_from_parentdir(parentdir_prefix, root, verbose):\n@@ -110,15 +122,14 @@ def versions_from_parentdir(parentdir_prefix, root, verbose):\n     \"\"\"\n     rootdirs = []\n \n-    for i in range(3):\n+    for _ in range(3):\n         dirname = os.path.basename(root)\n         if dirname.startswith(parentdir_prefix):\n             return {\"version\": dirname[len(parentdir_prefix):],\n                     \"full-revisionid\": None,\n                     \"dirty\": False, \"error\": None, \"date\": None}\n-        else:\n-            rootdirs.append(root)\n-            root = os.path.dirname(root)  # up a level\n+        rootdirs.append(root)\n+        root = os.path.dirname(root)  # up a level\n \n     if verbose:\n         print(\"Tried directories %s but none started with prefix %s\" %\n@@ -135,8 +146,8 @@ def git_get_keywords(versionfile_abs):\n     # _version.py.\n     keywords = {}\n     try:\n-        with open(versionfile_abs, \"r\") as f:\n-            for line in f.readlines():\n+        with open(versionfile_abs, \"r\") as fobj:\n+            for line in fobj:\n                 if line.strip().startswith(\"git_refnames =\"):\n                     mo = re.search(r'=\\s*\"(.*)\"', line)\n                     if mo:\n@@ -149,7 +160,7 @@ def git_get_keywords(versionfile_abs):\n                     mo = re.search(r'=\\s*\"(.*)\"', line)\n                     if mo:\n                         keywords[\"date\"] = mo.group(1)\n-    except EnvironmentError:\n+    except OSError:\n         pass\n     return keywords\n \n@@ -157,8 +168,8 @@ def git_get_keywords(versionfile_abs):\n @register_vcs_handler(\"git\", \"keywords\")\n def git_versions_from_keywords(keywords, tag_prefix, verbose):\n     \"\"\"Get version information from git keywords.\"\"\"\n-    if not keywords:\n-        raise NotThisMethod(\"no keywords at all, weird\")\n+    if \"refnames\" not in keywords:\n+        raise NotThisMethod(\"Short version file found\")\n     date = keywords.get(\"date\")\n     if date is not None:\n         # Use only the last line.  Previous lines may contain GPG signature\n@@ -177,11 +188,11 @@ def git_versions_from_keywords(keywords, tag_prefix, verbose):\n         if verbose:\n             print(\"keywords are unexpanded, not using\")\n         raise NotThisMethod(\"unexpanded keywords, not a git-archive tarball\")\n-    refs = set([r.strip() for r in refnames.strip(\"()\").split(\",\")])\n+    refs = {r.strip() for r in refnames.strip(\"()\").split(\",\")}\n     # starting in git-1.8.3, tags are listed as \"tag: foo-1.0\" instead of\n     # just \"foo-1.0\". If we see a \"tag: \" prefix, prefer those.\n     TAG = \"tag: \"\n-    tags = set([r[len(TAG):] for r in refs if r.startswith(TAG)])\n+    tags = {r[len(TAG):] for r in refs if r.startswith(TAG)}\n     if not tags:\n         # Either we're using git < 1.8.3, or there really are no tags. We use\n         # a heuristic: assume all version tags have a digit. The old git %d\n@@ -189,8 +200,8 @@ def git_versions_from_keywords(keywords, tag_prefix, verbose):\n         # refs/heads/ and refs/tags/ prefixes that would let us distinguish\n         # between branches and tags. By ignoring refnames without digits, we\n         # filter out many common branch names like \"release\" and\n-        # \"stabilization\", as well as \"HEAD\" and \"main\".\n-        tags = set([r for r in refs if re.search(r'\\d', r)])\n+        # \"stabilization\", as well as \"HEAD\" and \"master\".\n+        tags = {r for r in refs if re.search(r'\\d', r)}\n         if verbose:\n             print(\"discarding '%s', no digits\" % \",\".join(refs - tags))\n     if verbose:\n@@ -199,6 +210,11 @@ def git_versions_from_keywords(keywords, tag_prefix, verbose):\n         # sorting will prefer e.g. \"2.0\" over \"2.0rc1\"\n         if ref.startswith(tag_prefix):\n             r = ref[len(tag_prefix):]\n+            # Filter out refs that exactly match prefix or that don't start\n+            # with a number once the prefix is stripped (mostly a concern\n+            # when prefix is '')\n+            if not re.match(r'\\d', r):\n+                continue\n             if verbose:\n                 print(\"picking %s\" % r)\n             return {\"version\": r,\n@@ -214,7 +230,7 @@ def git_versions_from_keywords(keywords, tag_prefix, verbose):\n \n \n @register_vcs_handler(\"git\", \"pieces_from_vcs\")\n-def git_pieces_from_vcs(tag_prefix, root, verbose, run_command=run_command):\n+def git_pieces_from_vcs(tag_prefix, root, verbose, runner=run_command):\n     \"\"\"Get version from 'git describe' in the root of the source tree.\n \n     This only gets called if the git-archive 'subst' keywords were *not*\n@@ -225,8 +241,15 @@ def git_pieces_from_vcs(tag_prefix, root, verbose, run_command=run_command):\n     if sys.platform == \"win32\":\n         GITS = [\"git.cmd\", \"git.exe\"]\n \n-    out, rc = run_command(GITS, [\"rev-parse\", \"--git-dir\"], cwd=root,\n-                          hide_stderr=True)\n+    # GIT_DIR can interfere with correct operation of Versioneer.\n+    # It may be intended to be passed to the Versioneer-versioned project,\n+    # but that should not change where we get our version from.\n+    env = os.environ.copy()\n+    env.pop(\"GIT_DIR\", None)\n+    runner = functools.partial(runner, env=env)\n+\n+    _, rc = runner(GITS, [\"rev-parse\", \"--git-dir\"], cwd=root,\n+                   hide_stderr=not verbose)\n     if rc != 0:\n         if verbose:\n             print(\"Directory %s not under git control\" % root)\n@@ -234,15 +257,15 @@ def git_pieces_from_vcs(tag_prefix, root, verbose, run_command=run_command):\n \n     # if there is a tag matching tag_prefix, this yields TAG-NUM-gHEX[-dirty]\n     # if there isn't one, this yields HEX[-dirty] (no NUM)\n-    describe_out, rc = run_command(GITS, [\"describe\", \"--tags\", \"--dirty=\",\n-                                          \"--always\", \"--long\",\n-                                          \"--match\", \"%s*\" % tag_prefix],\n-                                   cwd=root)\n+    describe_out, rc = runner(GITS, [\n+        \"describe\", \"--tags\", \"--dirty\", \"--always\", \"--long\",\n+        \"--match\", f\"{tag_prefix}[[:digit:]]*\"\n+    ], cwd=root)\n     # --long was added in git-1.5.5\n     if describe_out is None:\n         raise NotThisMethod(\"'git describe' failed\")\n     describe_out = describe_out.strip()\n-    full_out, rc = run_command(GITS, [\"rev-parse\", \"HEAD\"], cwd=root)\n+    full_out, rc = runner(GITS, [\"rev-parse\", \"HEAD\"], cwd=root)\n     if full_out is None:\n         raise NotThisMethod(\"'git rev-parse' failed\")\n     full_out = full_out.strip()\n@@ -252,6 +275,39 @@ def git_pieces_from_vcs(tag_prefix, root, verbose, run_command=run_command):\n     pieces[\"short\"] = full_out[:7]  # maybe improved later\n     pieces[\"error\"] = None\n \n+    branch_name, rc = runner(GITS, [\"rev-parse\", \"--abbrev-ref\", \"HEAD\"],\n+                             cwd=root)\n+    # --abbrev-ref was added in git-1.6.3\n+    if rc != 0 or branch_name is None:\n+        raise NotThisMethod(\"'git rev-parse --abbrev-ref' returned error\")\n+    branch_name = branch_name.strip()\n+\n+    if branch_name == \"HEAD\":\n+        # If we aren't exactly on a branch, pick a branch which represents\n+        # the current commit. If all else fails, we are on a branchless\n+        # commit.\n+        branches, rc = runner(GITS, [\"branch\", \"--contains\"], cwd=root)\n+        # --contains was added in git-1.5.4\n+        if rc != 0 or branches is None:\n+            raise NotThisMethod(\"'git branch --contains' returned error\")\n+        branches = branches.split(\"\\n\")\n+\n+        # Remove the first line if we're running detached\n+        if \"(\" in branches[0]:\n+            branches.pop(0)\n+\n+        # Strip off the leading \"* \" from the list of branches.\n+        branches = [branch[2:] for branch in branches]\n+        if \"master\" in branches:\n+            branch_name = \"master\"\n+        elif not branches:\n+            branch_name = None\n+        else:\n+            # Pick the first branch that is returned. Good or bad.\n+            branch_name = branches[0]\n+\n+    pieces[\"branch\"] = branch_name\n+\n     # parse describe_out. It will be like TAG-NUM-gHEX[-dirty] or HEX[-dirty]\n     # TAG might have hyphens.\n     git_describe = describe_out\n@@ -268,7 +324,7 @@ def git_pieces_from_vcs(tag_prefix, root, verbose, run_command=run_command):\n         # TAG-NUM-gHEX\n         mo = re.search(r'^(.+)-(\\d+)-g([0-9a-f]+)$', git_describe)\n         if not mo:\n-            # unparseable. Maybe git-describe is misbehaving?\n+            # unparsable. Maybe git-describe is misbehaving?\n             pieces[\"error\"] = (\"unable to parse git-describe output: '%s'\"\n                                % describe_out)\n             return pieces\n@@ -293,13 +349,11 @@ def git_pieces_from_vcs(tag_prefix, root, verbose, run_command=run_command):\n     else:\n         # HEX: no tags\n         pieces[\"closest-tag\"] = None\n-        count_out, rc = run_command(GITS, [\"rev-list\", \"HEAD\", \"--count\"],\n-                                    cwd=root)\n-        pieces[\"distance\"] = int(count_out)  # total number of commits\n+        out, rc = runner(GITS, [\"rev-list\", \"HEAD\", \"--left-right\"], cwd=root)\n+        pieces[\"distance\"] = len(out.split())  # total number of commits\n \n     # commit date: see ISO-8601 comment in git_versions_from_keywords()\n-    date = run_command(GITS, [\"show\", \"-s\", \"--format=%ci\", \"HEAD\"],\n-                       cwd=root)[0].strip()\n+    date = runner(GITS, [\"show\", \"-s\", \"--format=%ci\", \"HEAD\"], cwd=root)[0].strip()\n     # Use only the last line.  Previous lines may contain GPG signature\n     # information.\n     date = date.splitlines()[-1]\n@@ -340,16 +394,64 @@ def render_pep440(pieces):\n     return rendered\n \n \n+def render_pep440_branch(pieces):\n+    \"\"\"TAG[[.dev0]+DISTANCE.gHEX[.dirty]] .\n+\n+    The \".dev0\" means not master branch. Note that .dev0 sorts backwards\n+    (a feature branch will appear \"older\" than the master branch).\n+\n+    Exceptions:\n+    1: no tags. 0[.dev0]+untagged.DISTANCE.gHEX[.dirty]\n+    \"\"\"\n+    if pieces[\"closest-tag\"]:\n+        rendered = pieces[\"closest-tag\"]\n+        if pieces[\"distance\"] or pieces[\"dirty\"]:\n+            if pieces[\"branch\"] != \"master\":\n+                rendered += \".dev0\"\n+            rendered += plus_or_dot(pieces)\n+            rendered += \"%d.g%s\" % (pieces[\"distance\"], pieces[\"short\"])\n+            if pieces[\"dirty\"]:\n+                rendered += \".dirty\"\n+    else:\n+        # exception #1\n+        rendered = \"0\"\n+        if pieces[\"branch\"] != \"master\":\n+            rendered += \".dev0\"\n+        rendered += \"+untagged.%d.g%s\" % (pieces[\"distance\"],\n+                                          pieces[\"short\"])\n+        if pieces[\"dirty\"]:\n+            rendered += \".dirty\"\n+    return rendered\n+\n+\n+def pep440_split_post(ver):\n+    \"\"\"Split pep440 version string at the post-release segment.\n+\n+    Returns the release segments before the post-release and the\n+    post-release version number (or -1 if no post-release segment is present).\n+    \"\"\"\n+    vc = str.split(ver, \".post\")\n+    return vc[0], int(vc[1] or 0) if len(vc) == 2 else None\n+\n+\n def render_pep440_pre(pieces):\n-    \"\"\"TAG[.post0.devDISTANCE] -- No -dirty.\n+    \"\"\"TAG[.postN.devDISTANCE] -- No -dirty.\n \n     Exceptions:\n     1: no tags. 0.post0.devDISTANCE\n     \"\"\"\n     if pieces[\"closest-tag\"]:\n-        rendered = pieces[\"closest-tag\"]\n         if pieces[\"distance\"]:\n-            rendered += \".post0.dev%d\" % pieces[\"distance\"]\n+            # update the post release segment\n+            tag_version, post_version = pep440_split_post(pieces[\"closest-tag\"])\n+            rendered = tag_version\n+            if post_version is not None:\n+                rendered += \".post%d.dev%d\" % (post_version + 1, pieces[\"distance\"])\n+            else:\n+                rendered += \".post0.dev%d\" % (pieces[\"distance\"])\n+        else:\n+            # no commits, use the tag as the version\n+            rendered = pieces[\"closest-tag\"]\n     else:\n         # exception #1\n         rendered = \"0.post0.dev%d\" % pieces[\"distance\"]\n@@ -383,6 +485,35 @@ def render_pep440_post(pieces):\n     return rendered\n \n \n+def render_pep440_post_branch(pieces):\n+    \"\"\"TAG[.postDISTANCE[.dev0]+gHEX[.dirty]] .\n+\n+    The \".dev0\" means not master branch.\n+\n+    Exceptions:\n+    1: no tags. 0.postDISTANCE[.dev0]+gHEX[.dirty]\n+    \"\"\"\n+    if pieces[\"closest-tag\"]:\n+        rendered = pieces[\"closest-tag\"]\n+        if pieces[\"distance\"] or pieces[\"dirty\"]:\n+            rendered += \".post%d\" % pieces[\"distance\"]\n+            if pieces[\"branch\"] != \"master\":\n+                rendered += \".dev0\"\n+            rendered += plus_or_dot(pieces)\n+            rendered += \"g%s\" % pieces[\"short\"]\n+            if pieces[\"dirty\"]:\n+                rendered += \".dirty\"\n+    else:\n+        # exception #1\n+        rendered = \"0.post%d\" % pieces[\"distance\"]\n+        if pieces[\"branch\"] != \"master\":\n+            rendered += \".dev0\"\n+        rendered += \"+g%s\" % pieces[\"short\"]\n+        if pieces[\"dirty\"]:\n+            rendered += \".dirty\"\n+    return rendered\n+\n+\n def render_pep440_old(pieces):\n     \"\"\"TAG[.postDISTANCE[.dev0]] .\n \n@@ -459,10 +590,14 @@ def render(pieces, style):\n \n     if style == \"pep440\":\n         rendered = render_pep440(pieces)\n+    elif style == \"pep440-branch\":\n+        rendered = render_pep440_branch(pieces)\n     elif style == \"pep440-pre\":\n         rendered = render_pep440_pre(pieces)\n     elif style == \"pep440-post\":\n         rendered = render_pep440_post(pieces)\n+    elif style == \"pep440-post-branch\":\n+        rendered = render_pep440_post_branch(pieces)\n     elif style == \"pep440-old\":\n         rendered = render_pep440_old(pieces)\n     elif style == \"git-describe\":\n@@ -498,7 +633,7 @@ def get_versions():\n         # versionfile_source is the relative path from the top of the source\n         # tree (where the .git directory might live) to this file. Invert\n         # this to find the root from __file__.\n-        for i in cfg.versionfile_source.split('/'):\n+        for _ in cfg.versionfile_source.split('/'):\n             root = os.path.dirname(root)\n     except NameError:\n         return {\"version\": \"0+unknown\", \"full-revisionid\": None,\n",
            "comment_added_diff": {
                "8": "# This file is released into the public domain.",
                "9": "# Generated by versioneer-0.26",
                "10": "# https://github.com/python-versioneer/python-versioneer",
                "81": "        # This hides the console window if pythonw.exe is used",
                "132": "        root = os.path.dirname(root)  # up a level",
                "203": "        # \"stabilization\", as well as \"HEAD\" and \"master\".",
                "213": "            # Filter out refs that exactly match prefix or that don't start",
                "214": "            # with a number once the prefix is stripped (mostly a concern",
                "215": "            # when prefix is '')",
                "244": "    # GIT_DIR can interfere with correct operation of Versioneer.",
                "245": "    # It may be intended to be passed to the Versioneer-versioned project,",
                "246": "    # but that should not change where we get our version from.",
                "280": "    # --abbrev-ref was added in git-1.6.3",
                "286": "        # If we aren't exactly on a branch, pick a branch which represents",
                "287": "        # the current commit. If all else fails, we are on a branchless",
                "288": "        # commit.",
                "290": "        # --contains was added in git-1.5.4",
                "295": "        # Remove the first line if we're running detached",
                "299": "        # Strip off the leading \"* \" from the list of branches.",
                "306": "            # Pick the first branch that is returned. Good or bad.",
                "327": "            # unparsable. Maybe git-describe is misbehaving?",
                "353": "        pieces[\"distance\"] = len(out.split())  # total number of commits",
                "416": "        # exception #1",
                "445": "            # update the post release segment",
                "453": "            # no commits, use the tag as the version",
                "507": "        # exception #1"
            },
            "comment_deleted_diff": {
                "7": "# This file is released into the public domain. Generated by",
                "8": "# versioneer-0.19 (https://github.com/python-versioneer/python-versioneer)",
                "121": "            root = os.path.dirname(root)  # up a level",
                "192": "        # \"stabilization\", as well as \"HEAD\" and \"main\".",
                "271": "            # unparseable. Maybe git-describe is misbehaving?",
                "298": "        pieces[\"distance\"] = int(count_out)  # total number of commits"
            },
            "comment_modified_diff": {
                "8": "# versioneer-0.19 (https://github.com/python-versioneer/python-versioneer)",
                "81": "                                         else None))",
                "245": "    full_out, rc = run_command(GITS, [\"rev-parse\", \"HEAD\"], cwd=root)"
            }
        },
        {
            "commit": "d1f3c326b6691d6725b94f6477febe18386e0d36",
            "timestamp": "2023-05-28T23:20:34+02:00",
            "author": "Dimitri Papadopoulos",
            "commit_message": "MAINT: Update versioneer: 0.26 \u2192 0.28\n\nThis will help move the configuration from setup.cfg to pyproject.toml.",
            "additions": 2,
            "deletions": 2,
            "change_type": "MODIFY",
            "diff": "@@ -6,7 +6,7 @@\n # that just contains the computed version number.\n \n # This file is released into the public domain.\n-# Generated by versioneer-0.26\n+# Generated by versioneer-0.28\n # https://github.com/python-versioneer/python-versioneer\n \n \"\"\"Git implementation of _version.py.\"\"\"\n@@ -258,7 +258,7 @@ def git_pieces_from_vcs(tag_prefix, root, verbose, runner=run_command):\n     # if there is a tag matching tag_prefix, this yields TAG-NUM-gHEX[-dirty]\n     # if there isn't one, this yields HEX[-dirty] (no NUM)\n     describe_out, rc = runner(GITS, [\n-        \"describe\", \"--tags\", \"--dirty=\", \"--always\", \"--long\",\n+        \"describe\", \"--tags\", \"--dirty\", \"--always\", \"--long\",\n         \"--match\", f\"{tag_prefix}[[:digit:]]*\"\n     ], cwd=root)\n     # --long was added in git-1.5.5\n",
            "comment_added_diff": {
                "9": "# Generated by versioneer-0.28"
            },
            "comment_deleted_diff": {
                "9": "# Generated by versioneer-0.26"
            },
            "comment_modified_diff": {
                "9": "# Generated by versioneer-0.26"
            }
        },
        {
            "commit": "be896c9c652c010093d26bd0ff732dbfabcd6361",
            "timestamp": "2023-08-11T22:40:16+02:00",
            "author": "Stefan van der Walt",
            "commit_message": "MAINT: Remove versioneer (#24196)\n\nCo-authored-by: Ralf Gommers <ralf.gommers@gmail.com>",
            "additions": 0,
            "deletions": 658,
            "change_type": "DELETE",
            "diff": "@@ -1,658 +0,0 @@\n-\n-# This file helps to compute a version number in source trees obtained from\n-# git-archive tarball (such as those provided by githubs download-from-tag\n-# feature). Distribution tarballs (built by setup.py sdist) and build\n-# directories (produced by setup.py build) will contain a much shorter file\n-# that just contains the computed version number.\n-\n-# This file is released into the public domain.\n-# Generated by versioneer-0.28\n-# https://github.com/python-versioneer/python-versioneer\n-\n-\"\"\"Git implementation of _version.py.\"\"\"\n-\n-import errno\n-import os\n-import re\n-import subprocess\n-import sys\n-from typing import Callable, Dict\n-import functools\n-\n-\n-def get_keywords():\n-    \"\"\"Get the keywords needed to look up the version information.\"\"\"\n-    # these strings will be replaced by git during git-archive.\n-    # setup.py/versioneer.py will grep for the variable names, so they must\n-    # each be defined on a line of their own. _version.py will just call\n-    # get_keywords().\n-    git_refnames = \"$Format:%d$\"\n-    git_full = \"$Format:%H$\"\n-    git_date = \"$Format:%ci$\"\n-    keywords = {\"refnames\": git_refnames, \"full\": git_full, \"date\": git_date}\n-    return keywords\n-\n-\n-class VersioneerConfig:\n-    \"\"\"Container for Versioneer configuration parameters.\"\"\"\n-\n-\n-def get_config():\n-    \"\"\"Create, populate and return the VersioneerConfig() object.\"\"\"\n-    # these strings are filled in when 'setup.py versioneer' creates\n-    # _version.py\n-    cfg = VersioneerConfig()\n-    cfg.VCS = \"git\"\n-    cfg.style = \"pep440\"\n-    cfg.tag_prefix = \"v\"\n-    cfg.parentdir_prefix = \"numpy-\"\n-    cfg.versionfile_source = \"numpy/_version.py\"\n-    cfg.verbose = False\n-    return cfg\n-\n-\n-class NotThisMethod(Exception):\n-    \"\"\"Exception raised if a method is not valid for the current scenario.\"\"\"\n-\n-\n-LONG_VERSION_PY: Dict[str, str] = {}\n-HANDLERS: Dict[str, Dict[str, Callable]] = {}\n-\n-\n-def register_vcs_handler(vcs, method):  # decorator\n-    \"\"\"Create decorator to mark a method as the handler of a VCS.\"\"\"\n-    def decorate(f):\n-        \"\"\"Store f in HANDLERS[vcs][method].\"\"\"\n-        if vcs not in HANDLERS:\n-            HANDLERS[vcs] = {}\n-        HANDLERS[vcs][method] = f\n-        return f\n-    return decorate\n-\n-\n-def run_command(commands, args, cwd=None, verbose=False, hide_stderr=False,\n-                env=None):\n-    \"\"\"Call the given command(s).\"\"\"\n-    assert isinstance(commands, list)\n-    process = None\n-\n-    popen_kwargs = {}\n-    if sys.platform == \"win32\":\n-        # This hides the console window if pythonw.exe is used\n-        startupinfo = subprocess.STARTUPINFO()\n-        startupinfo.dwFlags |= subprocess.STARTF_USESHOWWINDOW\n-        popen_kwargs[\"startupinfo\"] = startupinfo\n-\n-    for command in commands:\n-        try:\n-            dispcmd = str([command] + args)\n-            # remember shell=False, so use git.cmd on windows, not just git\n-            process = subprocess.Popen([command] + args, cwd=cwd, env=env,\n-                                       stdout=subprocess.PIPE,\n-                                       stderr=(subprocess.PIPE if hide_stderr\n-                                               else None), **popen_kwargs)\n-            break\n-        except OSError:\n-            e = sys.exc_info()[1]\n-            if e.errno == errno.ENOENT:\n-                continue\n-            if verbose:\n-                print(\"unable to run %s\" % dispcmd)\n-                print(e)\n-            return None, None\n-    else:\n-        if verbose:\n-            print(\"unable to find command, tried %s\" % (commands,))\n-        return None, None\n-    stdout = process.communicate()[0].strip().decode()\n-    if process.returncode != 0:\n-        if verbose:\n-            print(\"unable to run %s (error)\" % dispcmd)\n-            print(\"stdout was %s\" % stdout)\n-        return None, process.returncode\n-    return stdout, process.returncode\n-\n-\n-def versions_from_parentdir(parentdir_prefix, root, verbose):\n-    \"\"\"Try to determine the version from the parent directory name.\n-\n-    Source tarballs conventionally unpack into a directory that includes both\n-    the project name and a version string. We will also support searching up\n-    two directory levels for an appropriately named parent directory\n-    \"\"\"\n-    rootdirs = []\n-\n-    for _ in range(3):\n-        dirname = os.path.basename(root)\n-        if dirname.startswith(parentdir_prefix):\n-            return {\"version\": dirname[len(parentdir_prefix):],\n-                    \"full-revisionid\": None,\n-                    \"dirty\": False, \"error\": None, \"date\": None}\n-        rootdirs.append(root)\n-        root = os.path.dirname(root)  # up a level\n-\n-    if verbose:\n-        print(\"Tried directories %s but none started with prefix %s\" %\n-              (str(rootdirs), parentdir_prefix))\n-    raise NotThisMethod(\"rootdir doesn't start with parentdir_prefix\")\n-\n-\n-@register_vcs_handler(\"git\", \"get_keywords\")\n-def git_get_keywords(versionfile_abs):\n-    \"\"\"Extract version information from the given file.\"\"\"\n-    # the code embedded in _version.py can just fetch the value of these\n-    # keywords. When used from setup.py, we don't want to import _version.py,\n-    # so we do it with a regexp instead. This function is not used from\n-    # _version.py.\n-    keywords = {}\n-    try:\n-        with open(versionfile_abs, \"r\") as fobj:\n-            for line in fobj:\n-                if line.strip().startswith(\"git_refnames =\"):\n-                    mo = re.search(r'=\\s*\"(.*)\"', line)\n-                    if mo:\n-                        keywords[\"refnames\"] = mo.group(1)\n-                if line.strip().startswith(\"git_full =\"):\n-                    mo = re.search(r'=\\s*\"(.*)\"', line)\n-                    if mo:\n-                        keywords[\"full\"] = mo.group(1)\n-                if line.strip().startswith(\"git_date =\"):\n-                    mo = re.search(r'=\\s*\"(.*)\"', line)\n-                    if mo:\n-                        keywords[\"date\"] = mo.group(1)\n-    except OSError:\n-        pass\n-    return keywords\n-\n-\n-@register_vcs_handler(\"git\", \"keywords\")\n-def git_versions_from_keywords(keywords, tag_prefix, verbose):\n-    \"\"\"Get version information from git keywords.\"\"\"\n-    if \"refnames\" not in keywords:\n-        raise NotThisMethod(\"Short version file found\")\n-    date = keywords.get(\"date\")\n-    if date is not None:\n-        # Use only the last line.  Previous lines may contain GPG signature\n-        # information.\n-        date = date.splitlines()[-1]\n-\n-        # git-2.2.0 added \"%cI\", which expands to an ISO-8601 -compliant\n-        # datestamp. However we prefer \"%ci\" (which expands to an \"ISO-8601\n-        # -like\" string, which we must then edit to make compliant), because\n-        # it's been around since git-1.5.3, and it's too difficult to\n-        # discover which version we're using, or to work around using an\n-        # older one.\n-        date = date.strip().replace(\" \", \"T\", 1).replace(\" \", \"\", 1)\n-    refnames = keywords[\"refnames\"].strip()\n-    if refnames.startswith(\"$Format\"):\n-        if verbose:\n-            print(\"keywords are unexpanded, not using\")\n-        raise NotThisMethod(\"unexpanded keywords, not a git-archive tarball\")\n-    refs = {r.strip() for r in refnames.strip(\"()\").split(\",\")}\n-    # starting in git-1.8.3, tags are listed as \"tag: foo-1.0\" instead of\n-    # just \"foo-1.0\". If we see a \"tag: \" prefix, prefer those.\n-    TAG = \"tag: \"\n-    tags = {r[len(TAG):] for r in refs if r.startswith(TAG)}\n-    if not tags:\n-        # Either we're using git < 1.8.3, or there really are no tags. We use\n-        # a heuristic: assume all version tags have a digit. The old git %d\n-        # expansion behaves like git log --decorate=short and strips out the\n-        # refs/heads/ and refs/tags/ prefixes that would let us distinguish\n-        # between branches and tags. By ignoring refnames without digits, we\n-        # filter out many common branch names like \"release\" and\n-        # \"stabilization\", as well as \"HEAD\" and \"master\".\n-        tags = {r for r in refs if re.search(r'\\d', r)}\n-        if verbose:\n-            print(\"discarding '%s', no digits\" % \",\".join(refs - tags))\n-    if verbose:\n-        print(\"likely tags: %s\" % \",\".join(sorted(tags)))\n-    for ref in sorted(tags):\n-        # sorting will prefer e.g. \"2.0\" over \"2.0rc1\"\n-        if ref.startswith(tag_prefix):\n-            r = ref[len(tag_prefix):]\n-            # Filter out refs that exactly match prefix or that don't start\n-            # with a number once the prefix is stripped (mostly a concern\n-            # when prefix is '')\n-            if not re.match(r'\\d', r):\n-                continue\n-            if verbose:\n-                print(\"picking %s\" % r)\n-            return {\"version\": r,\n-                    \"full-revisionid\": keywords[\"full\"].strip(),\n-                    \"dirty\": False, \"error\": None,\n-                    \"date\": date}\n-    # no suitable tags, so version is \"0+unknown\", but full hex is still there\n-    if verbose:\n-        print(\"no suitable tags, using unknown + full revision id\")\n-    return {\"version\": \"0+unknown\",\n-            \"full-revisionid\": keywords[\"full\"].strip(),\n-            \"dirty\": False, \"error\": \"no suitable tags\", \"date\": None}\n-\n-\n-@register_vcs_handler(\"git\", \"pieces_from_vcs\")\n-def git_pieces_from_vcs(tag_prefix, root, verbose, runner=run_command):\n-    \"\"\"Get version from 'git describe' in the root of the source tree.\n-\n-    This only gets called if the git-archive 'subst' keywords were *not*\n-    expanded, and _version.py hasn't already been rewritten with a short\n-    version string, meaning we're inside a checked out source tree.\n-    \"\"\"\n-    GITS = [\"git\"]\n-    if sys.platform == \"win32\":\n-        GITS = [\"git.cmd\", \"git.exe\"]\n-\n-    # GIT_DIR can interfere with correct operation of Versioneer.\n-    # It may be intended to be passed to the Versioneer-versioned project,\n-    # but that should not change where we get our version from.\n-    env = os.environ.copy()\n-    env.pop(\"GIT_DIR\", None)\n-    runner = functools.partial(runner, env=env)\n-\n-    _, rc = runner(GITS, [\"rev-parse\", \"--git-dir\"], cwd=root,\n-                   hide_stderr=not verbose)\n-    if rc != 0:\n-        if verbose:\n-            print(\"Directory %s not under git control\" % root)\n-        raise NotThisMethod(\"'git rev-parse --git-dir' returned error\")\n-\n-    # if there is a tag matching tag_prefix, this yields TAG-NUM-gHEX[-dirty]\n-    # if there isn't one, this yields HEX[-dirty] (no NUM)\n-    describe_out, rc = runner(GITS, [\n-        \"describe\", \"--tags\", \"--dirty=\", \"--always\", \"--long\",\n-        \"--match\", f\"{tag_prefix}[[:digit:]]*\"\n-    ], cwd=root)\n-    # --long was added in git-1.5.5\n-    if describe_out is None:\n-        raise NotThisMethod(\"'git describe' failed\")\n-    describe_out = describe_out.strip()\n-    full_out, rc = runner(GITS, [\"rev-parse\", \"HEAD\"], cwd=root)\n-    if full_out is None:\n-        raise NotThisMethod(\"'git rev-parse' failed\")\n-    full_out = full_out.strip()\n-\n-    pieces = {}\n-    pieces[\"long\"] = full_out\n-    pieces[\"short\"] = full_out[:7]  # maybe improved later\n-    pieces[\"error\"] = None\n-\n-    branch_name, rc = runner(GITS, [\"rev-parse\", \"--abbrev-ref\", \"HEAD\"],\n-                             cwd=root)\n-    # --abbrev-ref was added in git-1.6.3\n-    if rc != 0 or branch_name is None:\n-        raise NotThisMethod(\"'git rev-parse --abbrev-ref' returned error\")\n-    branch_name = branch_name.strip()\n-\n-    if branch_name == \"HEAD\":\n-        # If we aren't exactly on a branch, pick a branch which represents\n-        # the current commit. If all else fails, we are on a branchless\n-        # commit.\n-        branches, rc = runner(GITS, [\"branch\", \"--contains\"], cwd=root)\n-        # --contains was added in git-1.5.4\n-        if rc != 0 or branches is None:\n-            raise NotThisMethod(\"'git branch --contains' returned error\")\n-        branches = branches.split(\"\\n\")\n-\n-        # Remove the first line if we're running detached\n-        if \"(\" in branches[0]:\n-            branches.pop(0)\n-\n-        # Strip off the leading \"* \" from the list of branches.\n-        branches = [branch[2:] for branch in branches]\n-        if \"master\" in branches:\n-            branch_name = \"master\"\n-        elif not branches:\n-            branch_name = None\n-        else:\n-            # Pick the first branch that is returned. Good or bad.\n-            branch_name = branches[0]\n-\n-    pieces[\"branch\"] = branch_name\n-\n-    # parse describe_out. It will be like TAG-NUM-gHEX[-dirty] or HEX[-dirty]\n-    # TAG might have hyphens.\n-    git_describe = describe_out\n-\n-    # look for -dirty suffix\n-    dirty = git_describe.endswith(\"-dirty\")\n-    pieces[\"dirty\"] = dirty\n-    if dirty:\n-        git_describe = git_describe[:git_describe.rindex(\"-dirty\")]\n-\n-    # now we have TAG-NUM-gHEX or HEX\n-\n-    if \"-\" in git_describe:\n-        # TAG-NUM-gHEX\n-        mo = re.search(r'^(.+)-(\\d+)-g([0-9a-f]+)$', git_describe)\n-        if not mo:\n-            # unparsable. Maybe git-describe is misbehaving?\n-            pieces[\"error\"] = (\"unable to parse git-describe output: '%s'\"\n-                               % describe_out)\n-            return pieces\n-\n-        # tag\n-        full_tag = mo.group(1)\n-        if not full_tag.startswith(tag_prefix):\n-            if verbose:\n-                fmt = \"tag '%s' doesn't start with prefix '%s'\"\n-                print(fmt % (full_tag, tag_prefix))\n-            pieces[\"error\"] = (\"tag '%s' doesn't start with prefix '%s'\"\n-                               % (full_tag, tag_prefix))\n-            return pieces\n-        pieces[\"closest-tag\"] = full_tag[len(tag_prefix):]\n-\n-        # distance: number of commits since tag\n-        pieces[\"distance\"] = int(mo.group(2))\n-\n-        # commit: short hex revision ID\n-        pieces[\"short\"] = mo.group(3)\n-\n-    else:\n-        # HEX: no tags\n-        pieces[\"closest-tag\"] = None\n-        out, rc = runner(GITS, [\"rev-list\", \"HEAD\", \"--left-right\"], cwd=root)\n-        pieces[\"distance\"] = len(out.split())  # total number of commits\n-\n-    # commit date: see ISO-8601 comment in git_versions_from_keywords()\n-    date = runner(GITS, [\"show\", \"-s\", \"--format=%ci\", \"HEAD\"], cwd=root)[0].strip()\n-    # Use only the last line.  Previous lines may contain GPG signature\n-    # information.\n-    date = date.splitlines()[-1]\n-    pieces[\"date\"] = date.strip().replace(\" \", \"T\", 1).replace(\" \", \"\", 1)\n-\n-    return pieces\n-\n-\n-def plus_or_dot(pieces):\n-    \"\"\"Return a + if we don't already have one, else return a .\"\"\"\n-    if \"+\" in pieces.get(\"closest-tag\", \"\"):\n-        return \".\"\n-    return \"+\"\n-\n-\n-def render_pep440(pieces):\n-    \"\"\"Build up version string, with post-release \"local version identifier\".\n-\n-    Our goal: TAG[+DISTANCE.gHEX[.dirty]] . Note that if you\n-    get a tagged build and then dirty it, you'll get TAG+0.gHEX.dirty\n-\n-    Exceptions:\n-    1: no tags. git_describe was just HEX. 0+untagged.DISTANCE.gHEX[.dirty]\n-    \"\"\"\n-    if pieces[\"closest-tag\"]:\n-        rendered = pieces[\"closest-tag\"]\n-        if pieces[\"distance\"] or pieces[\"dirty\"]:\n-            rendered += plus_or_dot(pieces)\n-            rendered += \"%d.g%s\" % (pieces[\"distance\"], pieces[\"short\"])\n-            if pieces[\"dirty\"]:\n-                rendered += \".dirty\"\n-    else:\n-        # exception #1\n-        rendered = \"0+untagged.%d.g%s\" % (pieces[\"distance\"],\n-                                          pieces[\"short\"])\n-        if pieces[\"dirty\"]:\n-            rendered += \".dirty\"\n-    return rendered\n-\n-\n-def render_pep440_branch(pieces):\n-    \"\"\"TAG[[.dev0]+DISTANCE.gHEX[.dirty]] .\n-\n-    The \".dev0\" means not master branch. Note that .dev0 sorts backwards\n-    (a feature branch will appear \"older\" than the master branch).\n-\n-    Exceptions:\n-    1: no tags. 0[.dev0]+untagged.DISTANCE.gHEX[.dirty]\n-    \"\"\"\n-    if pieces[\"closest-tag\"]:\n-        rendered = pieces[\"closest-tag\"]\n-        if pieces[\"distance\"] or pieces[\"dirty\"]:\n-            if pieces[\"branch\"] != \"master\":\n-                rendered += \".dev0\"\n-            rendered += plus_or_dot(pieces)\n-            rendered += \"%d.g%s\" % (pieces[\"distance\"], pieces[\"short\"])\n-            if pieces[\"dirty\"]:\n-                rendered += \".dirty\"\n-    else:\n-        # exception #1\n-        rendered = \"0\"\n-        if pieces[\"branch\"] != \"master\":\n-            rendered += \".dev0\"\n-        rendered += \"+untagged.%d.g%s\" % (pieces[\"distance\"],\n-                                          pieces[\"short\"])\n-        if pieces[\"dirty\"]:\n-            rendered += \".dirty\"\n-    return rendered\n-\n-\n-def pep440_split_post(ver):\n-    \"\"\"Split pep440 version string at the post-release segment.\n-\n-    Returns the release segments before the post-release and the\n-    post-release version number (or -1 if no post-release segment is present).\n-    \"\"\"\n-    vc = str.split(ver, \".post\")\n-    return vc[0], int(vc[1] or 0) if len(vc) == 2 else None\n-\n-\n-def render_pep440_pre(pieces):\n-    \"\"\"TAG[.postN.devDISTANCE] -- No -dirty.\n-\n-    Exceptions:\n-    1: no tags. 0.post0.devDISTANCE\n-    \"\"\"\n-    if pieces[\"closest-tag\"]:\n-        if pieces[\"distance\"]:\n-            # update the post release segment\n-            tag_version, post_version = pep440_split_post(pieces[\"closest-tag\"])\n-            rendered = tag_version\n-            if post_version is not None:\n-                rendered += \".post%d.dev%d\" % (post_version + 1, pieces[\"distance\"])\n-            else:\n-                rendered += \".post0.dev%d\" % (pieces[\"distance\"])\n-        else:\n-            # no commits, use the tag as the version\n-            rendered = pieces[\"closest-tag\"]\n-    else:\n-        # exception #1\n-        rendered = \"0.post0.dev%d\" % pieces[\"distance\"]\n-    return rendered\n-\n-\n-def render_pep440_post(pieces):\n-    \"\"\"TAG[.postDISTANCE[.dev0]+gHEX] .\n-\n-    The \".dev0\" means dirty. Note that .dev0 sorts backwards\n-    (a dirty tree will appear \"older\" than the corresponding clean one),\n-    but you shouldn't be releasing software with -dirty anyways.\n-\n-    Exceptions:\n-    1: no tags. 0.postDISTANCE[.dev0]\n-    \"\"\"\n-    if pieces[\"closest-tag\"]:\n-        rendered = pieces[\"closest-tag\"]\n-        if pieces[\"distance\"] or pieces[\"dirty\"]:\n-            rendered += \".post%d\" % pieces[\"distance\"]\n-            if pieces[\"dirty\"]:\n-                rendered += \".dev0\"\n-            rendered += plus_or_dot(pieces)\n-            rendered += \"g%s\" % pieces[\"short\"]\n-    else:\n-        # exception #1\n-        rendered = \"0.post%d\" % pieces[\"distance\"]\n-        if pieces[\"dirty\"]:\n-            rendered += \".dev0\"\n-        rendered += \"+g%s\" % pieces[\"short\"]\n-    return rendered\n-\n-\n-def render_pep440_post_branch(pieces):\n-    \"\"\"TAG[.postDISTANCE[.dev0]+gHEX[.dirty]] .\n-\n-    The \".dev0\" means not master branch.\n-\n-    Exceptions:\n-    1: no tags. 0.postDISTANCE[.dev0]+gHEX[.dirty]\n-    \"\"\"\n-    if pieces[\"closest-tag\"]:\n-        rendered = pieces[\"closest-tag\"]\n-        if pieces[\"distance\"] or pieces[\"dirty\"]:\n-            rendered += \".post%d\" % pieces[\"distance\"]\n-            if pieces[\"branch\"] != \"master\":\n-                rendered += \".dev0\"\n-            rendered += plus_or_dot(pieces)\n-            rendered += \"g%s\" % pieces[\"short\"]\n-            if pieces[\"dirty\"]:\n-                rendered += \".dirty\"\n-    else:\n-        # exception #1\n-        rendered = \"0.post%d\" % pieces[\"distance\"]\n-        if pieces[\"branch\"] != \"master\":\n-            rendered += \".dev0\"\n-        rendered += \"+g%s\" % pieces[\"short\"]\n-        if pieces[\"dirty\"]:\n-            rendered += \".dirty\"\n-    return rendered\n-\n-\n-def render_pep440_old(pieces):\n-    \"\"\"TAG[.postDISTANCE[.dev0]] .\n-\n-    The \".dev0\" means dirty.\n-\n-    Exceptions:\n-    1: no tags. 0.postDISTANCE[.dev0]\n-    \"\"\"\n-    if pieces[\"closest-tag\"]:\n-        rendered = pieces[\"closest-tag\"]\n-        if pieces[\"distance\"] or pieces[\"dirty\"]:\n-            rendered += \".post%d\" % pieces[\"distance\"]\n-            if pieces[\"dirty\"]:\n-                rendered += \".dev0\"\n-    else:\n-        # exception #1\n-        rendered = \"0.post%d\" % pieces[\"distance\"]\n-        if pieces[\"dirty\"]:\n-            rendered += \".dev0\"\n-    return rendered\n-\n-\n-def render_git_describe(pieces):\n-    \"\"\"TAG[-DISTANCE-gHEX][-dirty].\n-\n-    Like 'git describe --tags --dirty --always'.\n-\n-    Exceptions:\n-    1: no tags. HEX[-dirty]  (note: no 'g' prefix)\n-    \"\"\"\n-    if pieces[\"closest-tag\"]:\n-        rendered = pieces[\"closest-tag\"]\n-        if pieces[\"distance\"]:\n-            rendered += \"-%d-g%s\" % (pieces[\"distance\"], pieces[\"short\"])\n-    else:\n-        # exception #1\n-        rendered = pieces[\"short\"]\n-    if pieces[\"dirty\"]:\n-        rendered += \"-dirty\"\n-    return rendered\n-\n-\n-def render_git_describe_long(pieces):\n-    \"\"\"TAG-DISTANCE-gHEX[-dirty].\n-\n-    Like 'git describe --tags --dirty --always -long'.\n-    The distance/hash is unconditional.\n-\n-    Exceptions:\n-    1: no tags. HEX[-dirty]  (note: no 'g' prefix)\n-    \"\"\"\n-    if pieces[\"closest-tag\"]:\n-        rendered = pieces[\"closest-tag\"]\n-        rendered += \"-%d-g%s\" % (pieces[\"distance\"], pieces[\"short\"])\n-    else:\n-        # exception #1\n-        rendered = pieces[\"short\"]\n-    if pieces[\"dirty\"]:\n-        rendered += \"-dirty\"\n-    return rendered\n-\n-\n-def render(pieces, style):\n-    \"\"\"Render the given version pieces into the requested style.\"\"\"\n-    if pieces[\"error\"]:\n-        return {\"version\": \"unknown\",\n-                \"full-revisionid\": pieces.get(\"long\"),\n-                \"dirty\": None,\n-                \"error\": pieces[\"error\"],\n-                \"date\": None}\n-\n-    if not style or style == \"default\":\n-        style = \"pep440\"  # the default\n-\n-    if style == \"pep440\":\n-        rendered = render_pep440(pieces)\n-    elif style == \"pep440-branch\":\n-        rendered = render_pep440_branch(pieces)\n-    elif style == \"pep440-pre\":\n-        rendered = render_pep440_pre(pieces)\n-    elif style == \"pep440-post\":\n-        rendered = render_pep440_post(pieces)\n-    elif style == \"pep440-post-branch\":\n-        rendered = render_pep440_post_branch(pieces)\n-    elif style == \"pep440-old\":\n-        rendered = render_pep440_old(pieces)\n-    elif style == \"git-describe\":\n-        rendered = render_git_describe(pieces)\n-    elif style == \"git-describe-long\":\n-        rendered = render_git_describe_long(pieces)\n-    else:\n-        raise ValueError(\"unknown style '%s'\" % style)\n-\n-    return {\"version\": rendered, \"full-revisionid\": pieces[\"long\"],\n-            \"dirty\": pieces[\"dirty\"], \"error\": None,\n-            \"date\": pieces.get(\"date\")}\n-\n-\n-def get_versions():\n-    \"\"\"Get version information or return default if unable to do so.\"\"\"\n-    # I am in _version.py, which lives at ROOT/VERSIONFILE_SOURCE. If we have\n-    # __file__, we can work backwards from there to the root. Some\n-    # py2exe/bbfreeze/non-CPython implementations don't do __file__, in which\n-    # case we can only use expanded keywords.\n-\n-    cfg = get_config()\n-    verbose = cfg.verbose\n-\n-    try:\n-        return git_versions_from_keywords(get_keywords(), cfg.tag_prefix,\n-                                          verbose)\n-    except NotThisMethod:\n-        pass\n-\n-    try:\n-        root = os.path.realpath(__file__)\n-        # versionfile_source is the relative path from the top of the source\n-        # tree (where the .git directory might live) to this file. Invert\n-        # this to find the root from __file__.\n-        for _ in cfg.versionfile_source.split('/'):\n-            root = os.path.dirname(root)\n-    except NameError:\n-        return {\"version\": \"0+unknown\", \"full-revisionid\": None,\n-                \"dirty\": None,\n-                \"error\": \"unable to find root of source tree\",\n-                \"date\": None}\n-\n-    try:\n-        pieces = git_pieces_from_vcs(cfg.tag_prefix, root, verbose)\n-        return render(pieces, cfg.style)\n-    except NotThisMethod:\n-        pass\n-\n-    try:\n-        if cfg.parentdir_prefix:\n-            return versions_from_parentdir(cfg.parentdir_prefix, root, verbose)\n-    except NotThisMethod:\n-        pass\n-\n-    return {\"version\": \"0+unknown\", \"full-revisionid\": None,\n-            \"dirty\": None,\n-            \"error\": \"unable to compute version\", \"date\": None}\n",
            "comment_added_diff": {},
            "comment_deleted_diff": {
                "2": "# This file helps to compute a version number in source trees obtained from",
                "3": "# git-archive tarball (such as those provided by githubs download-from-tag",
                "4": "# feature). Distribution tarballs (built by setup.py sdist) and build",
                "5": "# directories (produced by setup.py build) will contain a much shorter file",
                "6": "# that just contains the computed version number.",
                "8": "# This file is released into the public domain.",
                "9": "# Generated by versioneer-0.28",
                "10": "# https://github.com/python-versioneer/python-versioneer",
                "25": "    # these strings will be replaced by git during git-archive.",
                "26": "    # setup.py/versioneer.py will grep for the variable names, so they must",
                "27": "    # each be defined on a line of their own. _version.py will just call",
                "28": "    # get_keywords().",
                "42": "    # these strings are filled in when 'setup.py versioneer' creates",
                "43": "    # _version.py",
                "62": "def register_vcs_handler(vcs, method):  # decorator",
                "81": "        # This hides the console window if pythonw.exe is used",
                "89": "            # remember shell=False, so use git.cmd on windows, not just git",
                "132": "        root = os.path.dirname(root)  # up a level",
                "143": "    # the code embedded in _version.py can just fetch the value of these",
                "144": "    # keywords. When used from setup.py, we don't want to import _version.py,",
                "145": "    # so we do it with a regexp instead. This function is not used from",
                "146": "    # _version.py.",
                "175": "        # Use only the last line.  Previous lines may contain GPG signature",
                "176": "        # information.",
                "179": "        # git-2.2.0 added \"%cI\", which expands to an ISO-8601 -compliant",
                "180": "        # datestamp. However we prefer \"%ci\" (which expands to an \"ISO-8601",
                "181": "        # -like\" string, which we must then edit to make compliant), because",
                "182": "        # it's been around since git-1.5.3, and it's too difficult to",
                "183": "        # discover which version we're using, or to work around using an",
                "184": "        # older one.",
                "192": "    # starting in git-1.8.3, tags are listed as \"tag: foo-1.0\" instead of",
                "193": "    # just \"foo-1.0\". If we see a \"tag: \" prefix, prefer those.",
                "197": "        # Either we're using git < 1.8.3, or there really are no tags. We use",
                "198": "        # a heuristic: assume all version tags have a digit. The old git %d",
                "199": "        # expansion behaves like git log --decorate=short and strips out the",
                "200": "        # refs/heads/ and refs/tags/ prefixes that would let us distinguish",
                "201": "        # between branches and tags. By ignoring refnames without digits, we",
                "202": "        # filter out many common branch names like \"release\" and",
                "203": "        # \"stabilization\", as well as \"HEAD\" and \"master\".",
                "210": "        # sorting will prefer e.g. \"2.0\" over \"2.0rc1\"",
                "213": "            # Filter out refs that exactly match prefix or that don't start",
                "214": "            # with a number once the prefix is stripped (mostly a concern",
                "215": "            # when prefix is '')",
                "224": "    # no suitable tags, so version is \"0+unknown\", but full hex is still there",
                "244": "    # GIT_DIR can interfere with correct operation of Versioneer.",
                "245": "    # It may be intended to be passed to the Versioneer-versioned project,",
                "246": "    # but that should not change where we get our version from.",
                "258": "    # if there is a tag matching tag_prefix, this yields TAG-NUM-gHEX[-dirty]",
                "259": "    # if there isn't one, this yields HEX[-dirty] (no NUM)",
                "264": "    # --long was added in git-1.5.5",
                "275": "    pieces[\"short\"] = full_out[:7]  # maybe improved later",
                "280": "    # --abbrev-ref was added in git-1.6.3",
                "286": "        # If we aren't exactly on a branch, pick a branch which represents",
                "287": "        # the current commit. If all else fails, we are on a branchless",
                "288": "        # commit.",
                "290": "        # --contains was added in git-1.5.4",
                "295": "        # Remove the first line if we're running detached",
                "299": "        # Strip off the leading \"* \" from the list of branches.",
                "306": "            # Pick the first branch that is returned. Good or bad.",
                "311": "    # parse describe_out. It will be like TAG-NUM-gHEX[-dirty] or HEX[-dirty]",
                "312": "    # TAG might have hyphens.",
                "315": "    # look for -dirty suffix",
                "321": "    # now we have TAG-NUM-gHEX or HEX",
                "324": "        # TAG-NUM-gHEX",
                "327": "            # unparsable. Maybe git-describe is misbehaving?",
                "332": "        # tag",
                "343": "        # distance: number of commits since tag",
                "346": "        # commit: short hex revision ID",
                "350": "        # HEX: no tags",
                "353": "        pieces[\"distance\"] = len(out.split())  # total number of commits",
                "355": "    # commit date: see ISO-8601 comment in git_versions_from_keywords()",
                "357": "    # Use only the last line.  Previous lines may contain GPG signature",
                "358": "    # information.",
                "389": "        # exception #1",
                "416": "        # exception #1",
                "445": "            # update the post release segment",
                "453": "            # no commits, use the tag as the version",
                "456": "        # exception #1",
                "480": "        # exception #1",
                "507": "        # exception #1",
                "532": "        # exception #1",
                "552": "        # exception #1",
                "572": "        # exception #1",
                "589": "        style = \"pep440\"  # the default",
                "617": "    # I am in _version.py, which lives at ROOT/VERSIONFILE_SOURCE. If we have",
                "618": "    # __file__, we can work backwards from there to the root. Some",
                "619": "    # py2exe/bbfreeze/non-CPython implementations don't do __file__, in which",
                "620": "    # case we can only use expanded keywords.",
                "633": "        # versionfile_source is the relative path from the top of the source",
                "634": "        # tree (where the .git directory might live) to this file. Invert",
                "635": "        # this to find the root from __file__."
            },
            "comment_modified_diff": {}
        }
    ],
    "versioneer.py": [
        {
            "commit": "b6d94fab5a7ef8897fe06d0b7a161bc8080fff26",
            "timestamp": "2022-10-15T22:19:57+02:00",
            "author": "Dimitri Papadopoulos",
            "commit_message": "MAINT: Update versioneer 0.19 \u2192 0.26\n\nKeep classic vendored mode.\n\nThis update will shut up this codespell warning:\n\tunparseable ==> unparsable",
            "additions": 555,
            "deletions": 213,
            "change_type": "MODIFY",
            "diff": "@@ -1,5 +1,5 @@\n \n-# Version: 0.19\n+# Version: 0.26\n \n \"\"\"The Versioneer - like a rocketeer, but for versions.\n \n@@ -9,12 +9,12 @@\n * like a rocketeer, but for versions!\n * https://github.com/python-versioneer/python-versioneer\n * Brian Warner\n-* License: Public Domain\n-* Compatible with: Python 3.6, 3.7, 3.8, 3.9 and pypy3\n+* License: Public Domain (Unlicense)\n+* Compatible with: Python 3.7, 3.8, 3.9, 3.10 and pypy3\n * [![Latest Version][pypi-image]][pypi-url]\n * [![Build Status][travis-image]][travis-url]\n \n-This is a tool for managing a recorded version number in distutils-based\n+This is a tool for managing a recorded version number in setuptools-based\n python projects. The goal is to remove the tedious and error-prone \"update\n the embedded version string\" step from your release process. Making a new\n release should be as easy as recording a new tag in your version-control\n@@ -23,10 +23,32 @@\n \n ## Quick Install\n \n+Versioneer provides two installation modes. The \"classic\" vendored mode installs\n+a copy of versioneer into your repository. The experimental build-time dependency mode\n+is intended to allow you to skip this step and simplify the process of upgrading.\n+\n+### Vendored mode\n+\n+* `pip install versioneer` to somewhere in your $PATH\n+* add a `[tool.versioneer]` section to your `pyproject.toml or a\n+  `[versioneer]` section to your `setup.cfg` (see [Install](INSTALL.md))\n+* run `versioneer install --vendor` in your source tree, commit the results\n+* verify version information with `python setup.py version`\n+\n+### Build-time dependency mode\n+\n * `pip install versioneer` to somewhere in your $PATH\n-* add a `[versioneer]` section to your setup.cfg (see [Install](INSTALL.md))\n-* run `versioneer install` in your source tree, commit the results\n-* Verify version information with `python setup.py version`\n+* add a `[tool.versioneer]` section to your `pyproject.toml or a\n+  `[versioneer]` section to your `setup.cfg` (see [Install](INSTALL.md))\n+* add `versioneer` to the `requires` key of the `build-system` table in\n+  `pyproject.toml`:\n+  ```toml\n+  [build-system]\n+  requires = [\"setuptools\", \"versioneer\"]\n+  build-backend = \"setuptools.build_meta\"\n+  ```\n+* run `versioneer install --no-vendor` in your source tree, commit the results\n+* verify version information with `python setup.py version`\n \n ## Version Identifiers\n \n@@ -231,9 +253,10 @@\n To upgrade your project to a new release of Versioneer, do the following:\n \n * install the new Versioneer (`pip install -U versioneer` or equivalent)\n-* edit `setup.cfg`, if necessary, to include any new configuration settings\n-  indicated by the release notes. See [UPGRADING](./UPGRADING.md) for details.\n-* re-run `versioneer install` in your source tree, to replace\n+* edit `setup.cfg` and `pyproject.toml`, if necessary,\n+  to include any new configuration settings indicated by the release notes.\n+  See [UPGRADING](./UPGRADING.md) for details.\n+* re-run `versioneer install --[no-]vendor` in your source tree, to replace\n   `SRC/_version.py`\n * commit any changed files\n \n@@ -256,6 +279,8 @@\n   dependency\n * [minver](https://github.com/jbweston/miniver) - a lightweight reimplementation of\n   versioneer\n+* [versioningit](https://github.com/jwodder/versioningit) - a PEP 518-based setuptools\n+  plugin\n \n ## License\n \n@@ -272,6 +297,11 @@\n [travis-url]: https://travis-ci.com/github/python-versioneer/python-versioneer\n \n \"\"\"\n+# pylint:disable=invalid-name,import-outside-toplevel,missing-function-docstring\n+# pylint:disable=missing-class-docstring,too-many-branches,too-many-statements\n+# pylint:disable=raise-missing-from,too-many-lines,too-many-locals,import-error\n+# pylint:disable=too-few-public-methods,redefined-outer-name,consider-using-with\n+# pylint:disable=attribute-defined-outside-init,too-many-arguments\n \n import configparser\n import errno\n@@ -280,6 +310,14 @@\n import re\n import subprocess\n import sys\n+from pathlib import Path\n+from typing import Callable, Dict\n+import functools\n+try:\n+    import tomli\n+    have_tomli = True\n+except ImportError:\n+    have_tomli = False\n \n \n class VersioneerConfig:\n@@ -314,12 +352,12 @@ def get_root():\n         # module-import table will cache the first one. So we can't use\n         # os.path.dirname(__file__), as that will find whichever\n         # versioneer.py was first imported, even in later projects.\n-        me = os.path.realpath(os.path.abspath(__file__))\n-        me_dir = os.path.normcase(os.path.splitext(me)[0])\n+        my_path = os.path.realpath(os.path.abspath(__file__))\n+        me_dir = os.path.normcase(os.path.splitext(my_path)[0])\n         vsr_dir = os.path.normcase(os.path.splitext(versioneer_py)[0])\n-        if me_dir != vsr_dir:\n+        if me_dir != vsr_dir and \"VERSIONEER_PEP518\" not in globals():\n             print(\"Warning: build in %s is using versioneer.py from %s\"\n-                  % (os.path.dirname(me), versioneer_py))\n+                  % (os.path.dirname(my_path), versioneer_py))\n     except NameError:\n         pass\n     return root\n@@ -327,30 +365,39 @@ def get_root():\n \n def get_config_from_root(root):\n     \"\"\"Read the project setup.cfg file to determine Versioneer config.\"\"\"\n-    # This might raise EnvironmentError (if setup.cfg is missing), or\n+    # This might raise OSError (if setup.cfg is missing), or\n     # configparser.NoSectionError (if it lacks a [versioneer] section), or\n     # configparser.NoOptionError (if it lacks \"VCS=\"). See the docstring at\n     # the top of versioneer.py for instructions on writing your setup.cfg .\n-    setup_cfg = os.path.join(root, \"setup.cfg\")\n-    parser = configparser.ConfigParser()\n-    with open(setup_cfg, \"r\") as f:\n-        parser.read_file(f)\n-    VCS = parser.get(\"versioneer\", \"VCS\")  # mandatory\n-\n-    def get(parser, name):\n-        if parser.has_option(\"versioneer\", name):\n-            return parser.get(\"versioneer\", name)\n-        return None\n+    root = Path(root)\n+    pyproject_toml = root / \"pyproject.toml\"\n+    setup_cfg = root / \"setup.cfg\"\n+    section = None\n+    if pyproject_toml.exists() and have_tomli:\n+        try:\n+            with open(pyproject_toml, 'rb') as fobj:\n+                pp = tomli.load(fobj)\n+            section = pp['tool']['versioneer']\n+        except (tomli.TOMLDecodeError, KeyError):\n+            pass\n+    if not section:\n+        parser = configparser.ConfigParser()\n+        with open(setup_cfg) as cfg_file:\n+            parser.read_file(cfg_file)\n+        parser.get(\"versioneer\", \"VCS\")  # raise error if missing\n+\n+        section = parser[\"versioneer\"]\n+\n     cfg = VersioneerConfig()\n-    cfg.VCS = VCS\n-    cfg.style = get(parser, \"style\") or \"\"\n-    cfg.versionfile_source = get(parser, \"versionfile_source\")\n-    cfg.versionfile_build = get(parser, \"versionfile_build\")\n-    cfg.tag_prefix = get(parser, \"tag_prefix\")\n-    if cfg.tag_prefix in (\"''\", '\"\"'):\n+    cfg.VCS = section['VCS']\n+    cfg.style = section.get(\"style\", \"\")\n+    cfg.versionfile_source = section.get(\"versionfile_source\")\n+    cfg.versionfile_build = section.get(\"versionfile_build\")\n+    cfg.tag_prefix = section.get(\"tag_prefix\")\n+    if cfg.tag_prefix in (\"''\", '\"\"', None):\n         cfg.tag_prefix = \"\"\n-    cfg.parentdir_prefix = get(parser, \"parentdir_prefix\")\n-    cfg.verbose = get(parser, \"verbose\")\n+    cfg.parentdir_prefix = section.get(\"parentdir_prefix\")\n+    cfg.verbose = section.get(\"verbose\")\n     return cfg\n \n \n@@ -359,17 +406,15 @@ class NotThisMethod(Exception):\n \n \n # these dictionaries contain VCS-specific tools\n-LONG_VERSION_PY = {}\n-HANDLERS = {}\n+LONG_VERSION_PY: Dict[str, str] = {}\n+HANDLERS: Dict[str, Dict[str, Callable]] = {}\n \n \n def register_vcs_handler(vcs, method):  # decorator\n     \"\"\"Create decorator to mark a method as the handler of a VCS.\"\"\"\n     def decorate(f):\n         \"\"\"Store f in HANDLERS[vcs][method].\"\"\"\n-        if vcs not in HANDLERS:\n-            HANDLERS[vcs] = {}\n-        HANDLERS[vcs][method] = f\n+        HANDLERS.setdefault(vcs, {})[method] = f\n         return f\n     return decorate\n \n@@ -378,17 +423,25 @@ def run_command(commands, args, cwd=None, verbose=False, hide_stderr=False,\n                 env=None):\n     \"\"\"Call the given command(s).\"\"\"\n     assert isinstance(commands, list)\n-    p = None\n-    for c in commands:\n+    process = None\n+\n+    popen_kwargs = {}\n+    if sys.platform == \"win32\":\n+        # This hides the console window if pythonw.exe is used\n+        startupinfo = subprocess.STARTUPINFO()\n+        startupinfo.dwFlags |= subprocess.STARTF_USESHOWWINDOW\n+        popen_kwargs[\"startupinfo\"] = startupinfo\n+\n+    for command in commands:\n         try:\n-            dispcmd = str([c] + args)\n+            dispcmd = str([command] + args)\n             # remember shell=False, so use git.cmd on windows, not just git\n-            p = subprocess.Popen([c] + args, cwd=cwd, env=env,\n-                                 stdout=subprocess.PIPE,\n-                                 stderr=(subprocess.PIPE if hide_stderr\n-                                         else None))\n+            process = subprocess.Popen([command] + args, cwd=cwd, env=env,\n+                                       stdout=subprocess.PIPE,\n+                                       stderr=(subprocess.PIPE if hide_stderr\n+                                               else None), **popen_kwargs)\n             break\n-        except EnvironmentError:\n+        except OSError:\n             e = sys.exc_info()[1]\n             if e.errno == errno.ENOENT:\n                 continue\n@@ -400,13 +453,13 @@ def run_command(commands, args, cwd=None, verbose=False, hide_stderr=False,\n         if verbose:\n             print(\"unable to find command, tried %s\" % (commands,))\n         return None, None\n-    stdout = p.communicate()[0].strip().decode()\n-    if p.returncode != 0:\n+    stdout = process.communicate()[0].strip().decode()\n+    if process.returncode != 0:\n         if verbose:\n             print(\"unable to run %s (error)\" % dispcmd)\n             print(\"stdout was %s\" % stdout)\n-        return None, p.returncode\n-    return stdout, p.returncode\n+        return None, process.returncode\n+    return stdout, process.returncode\n \n \n LONG_VERSION_PY['git'] = r'''\n@@ -416,8 +469,9 @@ def run_command(commands, args, cwd=None, verbose=False, hide_stderr=False,\n # directories (produced by setup.py build) will contain a much shorter file\n # that just contains the computed version number.\n \n-# This file is released into the public domain. Generated by\n-# versioneer-0.19 (https://github.com/python-versioneer/python-versioneer)\n+# This file is released into the public domain.\n+# Generated by versioneer-0.26\n+# https://github.com/python-versioneer/python-versioneer\n \n \"\"\"Git implementation of _version.py.\"\"\"\n \n@@ -426,6 +480,8 @@ def run_command(commands, args, cwd=None, verbose=False, hide_stderr=False,\n import re\n import subprocess\n import sys\n+from typing import Callable, Dict\n+import functools\n \n \n def get_keywords():\n@@ -463,8 +519,8 @@ class NotThisMethod(Exception):\n     \"\"\"Exception raised if a method is not valid for the current scenario.\"\"\"\n \n \n-LONG_VERSION_PY = {}\n-HANDLERS = {}\n+LONG_VERSION_PY: Dict[str, str] = {}\n+HANDLERS: Dict[str, Dict[str, Callable]] = {}\n \n \n def register_vcs_handler(vcs, method):  # decorator\n@@ -482,17 +538,25 @@ def run_command(commands, args, cwd=None, verbose=False, hide_stderr=False,\n                 env=None):\n     \"\"\"Call the given command(s).\"\"\"\n     assert isinstance(commands, list)\n-    p = None\n-    for c in commands:\n+    process = None\n+\n+    popen_kwargs = {}\n+    if sys.platform == \"win32\":\n+        # This hides the console window if pythonw.exe is used\n+        startupinfo = subprocess.STARTUPINFO()\n+        startupinfo.dwFlags |= subprocess.STARTF_USESHOWWINDOW\n+        popen_kwargs[\"startupinfo\"] = startupinfo\n+\n+    for command in commands:\n         try:\n-            dispcmd = str([c] + args)\n+            dispcmd = str([command] + args)\n             # remember shell=False, so use git.cmd on windows, not just git\n-            p = subprocess.Popen([c] + args, cwd=cwd, env=env,\n-                                 stdout=subprocess.PIPE,\n-                                 stderr=(subprocess.PIPE if hide_stderr\n-                                         else None))\n+            process = subprocess.Popen([command] + args, cwd=cwd, env=env,\n+                                       stdout=subprocess.PIPE,\n+                                       stderr=(subprocess.PIPE if hide_stderr\n+                                               else None), **popen_kwargs)\n             break\n-        except EnvironmentError:\n+        except OSError:\n             e = sys.exc_info()[1]\n             if e.errno == errno.ENOENT:\n                 continue\n@@ -504,13 +568,13 @@ def run_command(commands, args, cwd=None, verbose=False, hide_stderr=False,\n         if verbose:\n             print(\"unable to find command, tried %%s\" %% (commands,))\n         return None, None\n-    stdout = p.communicate()[0].strip().decode()\n-    if p.returncode != 0:\n+    stdout = process.communicate()[0].strip().decode()\n+    if process.returncode != 0:\n         if verbose:\n             print(\"unable to run %%s (error)\" %% dispcmd)\n             print(\"stdout was %%s\" %% stdout)\n-        return None, p.returncode\n-    return stdout, p.returncode\n+        return None, process.returncode\n+    return stdout, process.returncode\n \n \n def versions_from_parentdir(parentdir_prefix, root, verbose):\n@@ -522,15 +586,14 @@ def versions_from_parentdir(parentdir_prefix, root, verbose):\n     \"\"\"\n     rootdirs = []\n \n-    for i in range(3):\n+    for _ in range(3):\n         dirname = os.path.basename(root)\n         if dirname.startswith(parentdir_prefix):\n             return {\"version\": dirname[len(parentdir_prefix):],\n                     \"full-revisionid\": None,\n                     \"dirty\": False, \"error\": None, \"date\": None}\n-        else:\n-            rootdirs.append(root)\n-            root = os.path.dirname(root)  # up a level\n+        rootdirs.append(root)\n+        root = os.path.dirname(root)  # up a level\n \n     if verbose:\n         print(\"Tried directories %%s but none started with prefix %%s\" %%\n@@ -547,22 +610,21 @@ def git_get_keywords(versionfile_abs):\n     # _version.py.\n     keywords = {}\n     try:\n-        f = open(versionfile_abs, \"r\")\n-        for line in f.readlines():\n-            if line.strip().startswith(\"git_refnames =\"):\n-                mo = re.search(r'=\\s*\"(.*)\"', line)\n-                if mo:\n-                    keywords[\"refnames\"] = mo.group(1)\n-            if line.strip().startswith(\"git_full =\"):\n-                mo = re.search(r'=\\s*\"(.*)\"', line)\n-                if mo:\n-                    keywords[\"full\"] = mo.group(1)\n-            if line.strip().startswith(\"git_date =\"):\n-                mo = re.search(r'=\\s*\"(.*)\"', line)\n-                if mo:\n-                    keywords[\"date\"] = mo.group(1)\n-        f.close()\n-    except EnvironmentError:\n+        with open(versionfile_abs, \"r\") as fobj:\n+            for line in fobj:\n+                if line.strip().startswith(\"git_refnames =\"):\n+                    mo = re.search(r'=\\s*\"(.*)\"', line)\n+                    if mo:\n+                        keywords[\"refnames\"] = mo.group(1)\n+                if line.strip().startswith(\"git_full =\"):\n+                    mo = re.search(r'=\\s*\"(.*)\"', line)\n+                    if mo:\n+                        keywords[\"full\"] = mo.group(1)\n+                if line.strip().startswith(\"git_date =\"):\n+                    mo = re.search(r'=\\s*\"(.*)\"', line)\n+                    if mo:\n+                        keywords[\"date\"] = mo.group(1)\n+    except OSError:\n         pass\n     return keywords\n \n@@ -570,8 +632,8 @@ def git_get_keywords(versionfile_abs):\n @register_vcs_handler(\"git\", \"keywords\")\n def git_versions_from_keywords(keywords, tag_prefix, verbose):\n     \"\"\"Get version information from git keywords.\"\"\"\n-    if not keywords:\n-        raise NotThisMethod(\"no keywords at all, weird\")\n+    if \"refnames\" not in keywords:\n+        raise NotThisMethod(\"Short version file found\")\n     date = keywords.get(\"date\")\n     if date is not None:\n         # Use only the last line.  Previous lines may contain GPG signature\n@@ -590,11 +652,11 @@ def git_versions_from_keywords(keywords, tag_prefix, verbose):\n         if verbose:\n             print(\"keywords are unexpanded, not using\")\n         raise NotThisMethod(\"unexpanded keywords, not a git-archive tarball\")\n-    refs = set([r.strip() for r in refnames.strip(\"()\").split(\",\")])\n+    refs = {r.strip() for r in refnames.strip(\"()\").split(\",\")}\n     # starting in git-1.8.3, tags are listed as \"tag: foo-1.0\" instead of\n     # just \"foo-1.0\". If we see a \"tag: \" prefix, prefer those.\n     TAG = \"tag: \"\n-    tags = set([r[len(TAG):] for r in refs if r.startswith(TAG)])\n+    tags = {r[len(TAG):] for r in refs if r.startswith(TAG)}\n     if not tags:\n         # Either we're using git < 1.8.3, or there really are no tags. We use\n         # a heuristic: assume all version tags have a digit. The old git %%d\n@@ -603,7 +665,7 @@ def git_versions_from_keywords(keywords, tag_prefix, verbose):\n         # between branches and tags. By ignoring refnames without digits, we\n         # filter out many common branch names like \"release\" and\n         # \"stabilization\", as well as \"HEAD\" and \"master\".\n-        tags = set([r for r in refs if re.search(r'\\d', r)])\n+        tags = {r for r in refs if re.search(r'\\d', r)}\n         if verbose:\n             print(\"discarding '%%s', no digits\" %% \",\".join(refs - tags))\n     if verbose:\n@@ -612,6 +674,11 @@ def git_versions_from_keywords(keywords, tag_prefix, verbose):\n         # sorting will prefer e.g. \"2.0\" over \"2.0rc1\"\n         if ref.startswith(tag_prefix):\n             r = ref[len(tag_prefix):]\n+            # Filter out refs that exactly match prefix or that don't start\n+            # with a number once the prefix is stripped (mostly a concern\n+            # when prefix is '')\n+            if not re.match(r'\\d', r):\n+                continue\n             if verbose:\n                 print(\"picking %%s\" %% r)\n             return {\"version\": r,\n@@ -627,7 +694,7 @@ def git_versions_from_keywords(keywords, tag_prefix, verbose):\n \n \n @register_vcs_handler(\"git\", \"pieces_from_vcs\")\n-def git_pieces_from_vcs(tag_prefix, root, verbose, run_command=run_command):\n+def git_pieces_from_vcs(tag_prefix, root, verbose, runner=run_command):\n     \"\"\"Get version from 'git describe' in the root of the source tree.\n \n     This only gets called if the git-archive 'subst' keywords were *not*\n@@ -638,8 +705,15 @@ def git_pieces_from_vcs(tag_prefix, root, verbose, run_command=run_command):\n     if sys.platform == \"win32\":\n         GITS = [\"git.cmd\", \"git.exe\"]\n \n-    out, rc = run_command(GITS, [\"rev-parse\", \"--git-dir\"], cwd=root,\n-                          hide_stderr=True)\n+    # GIT_DIR can interfere with correct operation of Versioneer.\n+    # It may be intended to be passed to the Versioneer-versioned project,\n+    # but that should not change where we get our version from.\n+    env = os.environ.copy()\n+    env.pop(\"GIT_DIR\", None)\n+    runner = functools.partial(runner, env=env)\n+\n+    _, rc = runner(GITS, [\"rev-parse\", \"--git-dir\"], cwd=root,\n+                   hide_stderr=not verbose)\n     if rc != 0:\n         if verbose:\n             print(\"Directory %%s not under git control\" %% root)\n@@ -647,15 +721,15 @@ def git_pieces_from_vcs(tag_prefix, root, verbose, run_command=run_command):\n \n     # if there is a tag matching tag_prefix, this yields TAG-NUM-gHEX[-dirty]\n     # if there isn't one, this yields HEX[-dirty] (no NUM)\n-    describe_out, rc = run_command(GITS, [\"describe\", \"--tags\", \"--dirty=\",\n-                                          \"--always\", \"--long\",\n-                                          \"--match\", \"%%s*\" %% tag_prefix],\n-                                   cwd=root)\n+    describe_out, rc = runner(GITS, [\n+        \"describe\", \"--tags\", \"--dirty\", \"--always\", \"--long\",\n+        \"--match\", f\"{tag_prefix}[[:digit:]]*\"\n+    ], cwd=root)\n     # --long was added in git-1.5.5\n     if describe_out is None:\n         raise NotThisMethod(\"'git describe' failed\")\n     describe_out = describe_out.strip()\n-    full_out, rc = run_command(GITS, [\"rev-parse\", \"HEAD\"], cwd=root)\n+    full_out, rc = runner(GITS, [\"rev-parse\", \"HEAD\"], cwd=root)\n     if full_out is None:\n         raise NotThisMethod(\"'git rev-parse' failed\")\n     full_out = full_out.strip()\n@@ -665,6 +739,39 @@ def git_pieces_from_vcs(tag_prefix, root, verbose, run_command=run_command):\n     pieces[\"short\"] = full_out[:7]  # maybe improved later\n     pieces[\"error\"] = None\n \n+    branch_name, rc = runner(GITS, [\"rev-parse\", \"--abbrev-ref\", \"HEAD\"],\n+                             cwd=root)\n+    # --abbrev-ref was added in git-1.6.3\n+    if rc != 0 or branch_name is None:\n+        raise NotThisMethod(\"'git rev-parse --abbrev-ref' returned error\")\n+    branch_name = branch_name.strip()\n+\n+    if branch_name == \"HEAD\":\n+        # If we aren't exactly on a branch, pick a branch which represents\n+        # the current commit. If all else fails, we are on a branchless\n+        # commit.\n+        branches, rc = runner(GITS, [\"branch\", \"--contains\"], cwd=root)\n+        # --contains was added in git-1.5.4\n+        if rc != 0 or branches is None:\n+            raise NotThisMethod(\"'git branch --contains' returned error\")\n+        branches = branches.split(\"\\n\")\n+\n+        # Remove the first line if we're running detached\n+        if \"(\" in branches[0]:\n+            branches.pop(0)\n+\n+        # Strip off the leading \"* \" from the list of branches.\n+        branches = [branch[2:] for branch in branches]\n+        if \"master\" in branches:\n+            branch_name = \"master\"\n+        elif not branches:\n+            branch_name = None\n+        else:\n+            # Pick the first branch that is returned. Good or bad.\n+            branch_name = branches[0]\n+\n+    pieces[\"branch\"] = branch_name\n+\n     # parse describe_out. It will be like TAG-NUM-gHEX[-dirty] or HEX[-dirty]\n     # TAG might have hyphens.\n     git_describe = describe_out\n@@ -681,7 +788,7 @@ def git_pieces_from_vcs(tag_prefix, root, verbose, run_command=run_command):\n         # TAG-NUM-gHEX\n         mo = re.search(r'^(.+)-(\\d+)-g([0-9a-f]+)$', git_describe)\n         if not mo:\n-            # unparseable. Maybe git-describe is misbehaving?\n+            # unparsable. Maybe git-describe is misbehaving?\n             pieces[\"error\"] = (\"unable to parse git-describe output: '%%s'\"\n                                %% describe_out)\n             return pieces\n@@ -706,13 +813,11 @@ def git_pieces_from_vcs(tag_prefix, root, verbose, run_command=run_command):\n     else:\n         # HEX: no tags\n         pieces[\"closest-tag\"] = None\n-        count_out, rc = run_command(GITS, [\"rev-list\", \"HEAD\", \"--count\"],\n-                                    cwd=root)\n-        pieces[\"distance\"] = int(count_out)  # total number of commits\n+        out, rc = runner(GITS, [\"rev-list\", \"HEAD\", \"--left-right\"], cwd=root)\n+        pieces[\"distance\"] = len(out.split())  # total number of commits\n \n     # commit date: see ISO-8601 comment in git_versions_from_keywords()\n-    date = run_command(GITS, [\"show\", \"-s\", \"--format=%%ci\", \"HEAD\"],\n-                       cwd=root)[0].strip()\n+    date = runner(GITS, [\"show\", \"-s\", \"--format=%%ci\", \"HEAD\"], cwd=root)[0].strip()\n     # Use only the last line.  Previous lines may contain GPG signature\n     # information.\n     date = date.splitlines()[-1]\n@@ -753,16 +858,64 @@ def render_pep440(pieces):\n     return rendered\n \n \n+def render_pep440_branch(pieces):\n+    \"\"\"TAG[[.dev0]+DISTANCE.gHEX[.dirty]] .\n+\n+    The \".dev0\" means not master branch. Note that .dev0 sorts backwards\n+    (a feature branch will appear \"older\" than the master branch).\n+\n+    Exceptions:\n+    1: no tags. 0[.dev0]+untagged.DISTANCE.gHEX[.dirty]\n+    \"\"\"\n+    if pieces[\"closest-tag\"]:\n+        rendered = pieces[\"closest-tag\"]\n+        if pieces[\"distance\"] or pieces[\"dirty\"]:\n+            if pieces[\"branch\"] != \"master\":\n+                rendered += \".dev0\"\n+            rendered += plus_or_dot(pieces)\n+            rendered += \"%%d.g%%s\" %% (pieces[\"distance\"], pieces[\"short\"])\n+            if pieces[\"dirty\"]:\n+                rendered += \".dirty\"\n+    else:\n+        # exception #1\n+        rendered = \"0\"\n+        if pieces[\"branch\"] != \"master\":\n+            rendered += \".dev0\"\n+        rendered += \"+untagged.%%d.g%%s\" %% (pieces[\"distance\"],\n+                                          pieces[\"short\"])\n+        if pieces[\"dirty\"]:\n+            rendered += \".dirty\"\n+    return rendered\n+\n+\n+def pep440_split_post(ver):\n+    \"\"\"Split pep440 version string at the post-release segment.\n+\n+    Returns the release segments before the post-release and the\n+    post-release version number (or -1 if no post-release segment is present).\n+    \"\"\"\n+    vc = str.split(ver, \".post\")\n+    return vc[0], int(vc[1] or 0) if len(vc) == 2 else None\n+\n+\n def render_pep440_pre(pieces):\n-    \"\"\"TAG[.post0.devDISTANCE] -- No -dirty.\n+    \"\"\"TAG[.postN.devDISTANCE] -- No -dirty.\n \n     Exceptions:\n     1: no tags. 0.post0.devDISTANCE\n     \"\"\"\n     if pieces[\"closest-tag\"]:\n-        rendered = pieces[\"closest-tag\"]\n         if pieces[\"distance\"]:\n-            rendered += \".post0.dev%%d\" %% pieces[\"distance\"]\n+            # update the post release segment\n+            tag_version, post_version = pep440_split_post(pieces[\"closest-tag\"])\n+            rendered = tag_version\n+            if post_version is not None:\n+                rendered += \".post%%d.dev%%d\" %% (post_version + 1, pieces[\"distance\"])\n+            else:\n+                rendered += \".post0.dev%%d\" %% (pieces[\"distance\"])\n+        else:\n+            # no commits, use the tag as the version\n+            rendered = pieces[\"closest-tag\"]\n     else:\n         # exception #1\n         rendered = \"0.post0.dev%%d\" %% pieces[\"distance\"]\n@@ -796,6 +949,35 @@ def render_pep440_post(pieces):\n     return rendered\n \n \n+def render_pep440_post_branch(pieces):\n+    \"\"\"TAG[.postDISTANCE[.dev0]+gHEX[.dirty]] .\n+\n+    The \".dev0\" means not master branch.\n+\n+    Exceptions:\n+    1: no tags. 0.postDISTANCE[.dev0]+gHEX[.dirty]\n+    \"\"\"\n+    if pieces[\"closest-tag\"]:\n+        rendered = pieces[\"closest-tag\"]\n+        if pieces[\"distance\"] or pieces[\"dirty\"]:\n+            rendered += \".post%%d\" %% pieces[\"distance\"]\n+            if pieces[\"branch\"] != \"master\":\n+                rendered += \".dev0\"\n+            rendered += plus_or_dot(pieces)\n+            rendered += \"g%%s\" %% pieces[\"short\"]\n+            if pieces[\"dirty\"]:\n+                rendered += \".dirty\"\n+    else:\n+        # exception #1\n+        rendered = \"0.post%%d\" %% pieces[\"distance\"]\n+        if pieces[\"branch\"] != \"master\":\n+            rendered += \".dev0\"\n+        rendered += \"+g%%s\" %% pieces[\"short\"]\n+        if pieces[\"dirty\"]:\n+            rendered += \".dirty\"\n+    return rendered\n+\n+\n def render_pep440_old(pieces):\n     \"\"\"TAG[.postDISTANCE[.dev0]] .\n \n@@ -872,10 +1054,14 @@ def render(pieces, style):\n \n     if style == \"pep440\":\n         rendered = render_pep440(pieces)\n+    elif style == \"pep440-branch\":\n+        rendered = render_pep440_branch(pieces)\n     elif style == \"pep440-pre\":\n         rendered = render_pep440_pre(pieces)\n     elif style == \"pep440-post\":\n         rendered = render_pep440_post(pieces)\n+    elif style == \"pep440-post-branch\":\n+        rendered = render_pep440_post_branch(pieces)\n     elif style == \"pep440-old\":\n         rendered = render_pep440_old(pieces)\n     elif style == \"git-describe\":\n@@ -911,7 +1097,7 @@ def get_versions():\n         # versionfile_source is the relative path from the top of the source\n         # tree (where the .git directory might live) to this file. Invert\n         # this to find the root from __file__.\n-        for i in cfg.versionfile_source.split('/'):\n+        for _ in cfg.versionfile_source.split('/'):\n             root = os.path.dirname(root)\n     except NameError:\n         return {\"version\": \"0+unknown\", \"full-revisionid\": None,\n@@ -946,8 +1132,8 @@ def git_get_keywords(versionfile_abs):\n     # _version.py.\n     keywords = {}\n     try:\n-        with open(versionfile_abs, \"r\") as f:\n-            for line in f.readlines():\n+        with open(versionfile_abs, \"r\") as fobj:\n+            for line in fobj:\n                 if line.strip().startswith(\"git_refnames =\"):\n                     mo = re.search(r'=\\s*\"(.*)\"', line)\n                     if mo:\n@@ -960,7 +1146,7 @@ def git_get_keywords(versionfile_abs):\n                     mo = re.search(r'=\\s*\"(.*)\"', line)\n                     if mo:\n                         keywords[\"date\"] = mo.group(1)\n-    except EnvironmentError:\n+    except OSError:\n         pass\n     return keywords\n \n@@ -968,8 +1154,8 @@ def git_get_keywords(versionfile_abs):\n @register_vcs_handler(\"git\", \"keywords\")\n def git_versions_from_keywords(keywords, tag_prefix, verbose):\n     \"\"\"Get version information from git keywords.\"\"\"\n-    if not keywords:\n-        raise NotThisMethod(\"no keywords at all, weird\")\n+    if \"refnames\" not in keywords:\n+        raise NotThisMethod(\"Short version file found\")\n     date = keywords.get(\"date\")\n     if date is not None:\n         # Use only the last line.  Previous lines may contain GPG signature\n@@ -988,11 +1174,11 @@ def git_versions_from_keywords(keywords, tag_prefix, verbose):\n         if verbose:\n             print(\"keywords are unexpanded, not using\")\n         raise NotThisMethod(\"unexpanded keywords, not a git-archive tarball\")\n-    refs = set([r.strip() for r in refnames.strip(\"()\").split(\",\")])\n+    refs = {r.strip() for r in refnames.strip(\"()\").split(\",\")}\n     # starting in git-1.8.3, tags are listed as \"tag: foo-1.0\" instead of\n     # just \"foo-1.0\". If we see a \"tag: \" prefix, prefer those.\n     TAG = \"tag: \"\n-    tags = set([r[len(TAG):] for r in refs if r.startswith(TAG)])\n+    tags = {r[len(TAG):] for r in refs if r.startswith(TAG)}\n     if not tags:\n         # Either we're using git < 1.8.3, or there really are no tags. We use\n         # a heuristic: assume all version tags have a digit. The old git %d\n@@ -1001,7 +1187,7 @@ def git_versions_from_keywords(keywords, tag_prefix, verbose):\n         # between branches and tags. By ignoring refnames without digits, we\n         # filter out many common branch names like \"release\" and\n         # \"stabilization\", as well as \"HEAD\" and \"master\".\n-        tags = set([r for r in refs if re.search(r'\\d', r)])\n+        tags = {r for r in refs if re.search(r'\\d', r)}\n         if verbose:\n             print(\"discarding '%s', no digits\" % \",\".join(refs - tags))\n     if verbose:\n@@ -1010,6 +1196,11 @@ def git_versions_from_keywords(keywords, tag_prefix, verbose):\n         # sorting will prefer e.g. \"2.0\" over \"2.0rc1\"\n         if ref.startswith(tag_prefix):\n             r = ref[len(tag_prefix):]\n+            # Filter out refs that exactly match prefix or that don't start\n+            # with a number once the prefix is stripped (mostly a concern\n+            # when prefix is '')\n+            if not re.match(r'\\d', r):\n+                continue\n             if verbose:\n                 print(\"picking %s\" % r)\n             return {\"version\": r,\n@@ -1025,7 +1216,7 @@ def git_versions_from_keywords(keywords, tag_prefix, verbose):\n \n \n @register_vcs_handler(\"git\", \"pieces_from_vcs\")\n-def git_pieces_from_vcs(tag_prefix, root, verbose, run_command=run_command):\n+def git_pieces_from_vcs(tag_prefix, root, verbose, runner=run_command):\n     \"\"\"Get version from 'git describe' in the root of the source tree.\n \n     This only gets called if the git-archive 'subst' keywords were *not*\n@@ -1036,8 +1227,15 @@ def git_pieces_from_vcs(tag_prefix, root, verbose, run_command=run_command):\n     if sys.platform == \"win32\":\n         GITS = [\"git.cmd\", \"git.exe\"]\n \n-    out, rc = run_command(GITS, [\"rev-parse\", \"--git-dir\"], cwd=root,\n-                          hide_stderr=True)\n+    # GIT_DIR can interfere with correct operation of Versioneer.\n+    # It may be intended to be passed to the Versioneer-versioned project,\n+    # but that should not change where we get our version from.\n+    env = os.environ.copy()\n+    env.pop(\"GIT_DIR\", None)\n+    runner = functools.partial(runner, env=env)\n+\n+    _, rc = runner(GITS, [\"rev-parse\", \"--git-dir\"], cwd=root,\n+                   hide_stderr=not verbose)\n     if rc != 0:\n         if verbose:\n             print(\"Directory %s not under git control\" % root)\n@@ -1045,15 +1243,15 @@ def git_pieces_from_vcs(tag_prefix, root, verbose, run_command=run_command):\n \n     # if there is a tag matching tag_prefix, this yields TAG-NUM-gHEX[-dirty]\n     # if there isn't one, this yields HEX[-dirty] (no NUM)\n-    describe_out, rc = run_command(GITS, [\"describe\", \"--tags\", \"--dirty=\",\n-                                          \"--always\", \"--long\",\n-                                          \"--match\", \"%s*\" % tag_prefix],\n-                                   cwd=root)\n+    describe_out, rc = runner(GITS, [\n+        \"describe\", \"--tags\", \"--dirty\", \"--always\", \"--long\",\n+        \"--match\", f\"{tag_prefix}[[:digit:]]*\"\n+    ], cwd=root)\n     # --long was added in git-1.5.5\n     if describe_out is None:\n         raise NotThisMethod(\"'git describe' failed\")\n     describe_out = describe_out.strip()\n-    full_out, rc = run_command(GITS, [\"rev-parse\", \"HEAD\"], cwd=root)\n+    full_out, rc = runner(GITS, [\"rev-parse\", \"HEAD\"], cwd=root)\n     if full_out is None:\n         raise NotThisMethod(\"'git rev-parse' failed\")\n     full_out = full_out.strip()\n@@ -1063,6 +1261,39 @@ def git_pieces_from_vcs(tag_prefix, root, verbose, run_command=run_command):\n     pieces[\"short\"] = full_out[:7]  # maybe improved later\n     pieces[\"error\"] = None\n \n+    branch_name, rc = runner(GITS, [\"rev-parse\", \"--abbrev-ref\", \"HEAD\"],\n+                             cwd=root)\n+    # --abbrev-ref was added in git-1.6.3\n+    if rc != 0 or branch_name is None:\n+        raise NotThisMethod(\"'git rev-parse --abbrev-ref' returned error\")\n+    branch_name = branch_name.strip()\n+\n+    if branch_name == \"HEAD\":\n+        # If we aren't exactly on a branch, pick a branch which represents\n+        # the current commit. If all else fails, we are on a branchless\n+        # commit.\n+        branches, rc = runner(GITS, [\"branch\", \"--contains\"], cwd=root)\n+        # --contains was added in git-1.5.4\n+        if rc != 0 or branches is None:\n+            raise NotThisMethod(\"'git branch --contains' returned error\")\n+        branches = branches.split(\"\\n\")\n+\n+        # Remove the first line if we're running detached\n+        if \"(\" in branches[0]:\n+            branches.pop(0)\n+\n+        # Strip off the leading \"* \" from the list of branches.\n+        branches = [branch[2:] for branch in branches]\n+        if \"master\" in branches:\n+            branch_name = \"master\"\n+        elif not branches:\n+            branch_name = None\n+        else:\n+            # Pick the first branch that is returned. Good or bad.\n+            branch_name = branches[0]\n+\n+    pieces[\"branch\"] = branch_name\n+\n     # parse describe_out. It will be like TAG-NUM-gHEX[-dirty] or HEX[-dirty]\n     # TAG might have hyphens.\n     git_describe = describe_out\n@@ -1079,7 +1310,7 @@ def git_pieces_from_vcs(tag_prefix, root, verbose, run_command=run_command):\n         # TAG-NUM-gHEX\n         mo = re.search(r'^(.+)-(\\d+)-g([0-9a-f]+)$', git_describe)\n         if not mo:\n-            # unparseable. Maybe git-describe is misbehaving?\n+            # unparsable. Maybe git-describe is misbehaving?\n             pieces[\"error\"] = (\"unable to parse git-describe output: '%s'\"\n                                % describe_out)\n             return pieces\n@@ -1104,13 +1335,11 @@ def git_pieces_from_vcs(tag_prefix, root, verbose, run_command=run_command):\n     else:\n         # HEX: no tags\n         pieces[\"closest-tag\"] = None\n-        count_out, rc = run_command(GITS, [\"rev-list\", \"HEAD\", \"--count\"],\n-                                    cwd=root)\n-        pieces[\"distance\"] = int(count_out)  # total number of commits\n+        out, rc = runner(GITS, [\"rev-list\", \"HEAD\", \"--left-right\"], cwd=root)\n+        pieces[\"distance\"] = len(out.split())  # total number of commits\n \n     # commit date: see ISO-8601 comment in git_versions_from_keywords()\n-    date = run_command(GITS, [\"show\", \"-s\", \"--format=%ci\", \"HEAD\"],\n-                       cwd=root)[0].strip()\n+    date = runner(GITS, [\"show\", \"-s\", \"--format=%ci\", \"HEAD\"], cwd=root)[0].strip()\n     # Use only the last line.  Previous lines may contain GPG signature\n     # information.\n     date = date.splitlines()[-1]\n@@ -1119,7 +1348,7 @@ def git_pieces_from_vcs(tag_prefix, root, verbose, run_command=run_command):\n     return pieces\n \n \n-def do_vcs_install(manifest_in, versionfile_source, ipy):\n+def do_vcs_install(versionfile_source, ipy):\n     \"\"\"Git-specific installation logic for Versioneer.\n \n     For Git, this means creating/changing .gitattributes to mark _version.py\n@@ -1128,29 +1357,31 @@ def do_vcs_install(manifest_in, versionfile_source, ipy):\n     GITS = [\"git\"]\n     if sys.platform == \"win32\":\n         GITS = [\"git.cmd\", \"git.exe\"]\n-    files = [manifest_in, versionfile_source]\n+    files = [versionfile_source]\n     if ipy:\n         files.append(ipy)\n-    try:\n-        me = __file__\n-        if me.endswith(\".pyc\") or me.endswith(\".pyo\"):\n-            me = os.path.splitext(me)[0] + \".py\"\n-        versioneer_file = os.path.relpath(me)\n-    except NameError:\n-        versioneer_file = \"versioneer.py\"\n-    files.append(versioneer_file)\n+    if \"VERSIONEER_PEP518\" not in globals():\n+        try:\n+            my_path = __file__\n+            if my_path.endswith(\".pyc\") or my_path.endswith(\".pyo\"):\n+                my_path = os.path.splitext(my_path)[0] + \".py\"\n+            versioneer_file = os.path.relpath(my_path)\n+        except NameError:\n+            versioneer_file = \"versioneer.py\"\n+        files.append(versioneer_file)\n     present = False\n     try:\n-        with open(\".gitattributes\", \"r\") as f:\n-            for line in f.readlines():\n+        with open(\".gitattributes\", \"r\") as fobj:\n+            for line in fobj:\n                 if line.strip().startswith(versionfile_source):\n                     if \"export-subst\" in line.strip().split()[1:]:\n                         present = True\n-    except EnvironmentError:\n+                        break\n+    except OSError:\n         pass\n     if not present:\n-        with open(\".gitattributes\", \"a+\") as f:\n-            f.write(\"%s export-subst\\n\" % versionfile_source)\n+        with open(\".gitattributes\", \"a+\") as fobj:\n+            fobj.write(f\"{versionfile_source} export-subst\\n\")\n         files.append(\".gitattributes\")\n     run_command(GITS, [\"add\", \"--\"] + files)\n \n@@ -1164,15 +1395,14 @@ def versions_from_parentdir(parentdir_prefix, root, verbose):\n     \"\"\"\n     rootdirs = []\n \n-    for i in range(3):\n+    for _ in range(3):\n         dirname = os.path.basename(root)\n         if dirname.startswith(parentdir_prefix):\n             return {\"version\": dirname[len(parentdir_prefix):],\n                     \"full-revisionid\": None,\n                     \"dirty\": False, \"error\": None, \"date\": None}\n-        else:\n-            rootdirs.append(root)\n-            root = os.path.dirname(root)  # up a level\n+        rootdirs.append(root)\n+        root = os.path.dirname(root)  # up a level\n \n     if verbose:\n         print(\"Tried directories %s but none started with prefix %s\" %\n@@ -1181,7 +1411,7 @@ def versions_from_parentdir(parentdir_prefix, root, verbose):\n \n \n SHORT_VERSION_PY = \"\"\"\n-# This file was generated by 'versioneer.py' (0.19) from\n+# This file was generated by 'versioneer.py' (0.26) from\n # revision-control system data, or from the parent directory name of an\n # unpacked source archive. Distribution tarballs contain a pre-generated copy\n # of this file.\n@@ -1203,7 +1433,7 @@ def versions_from_file(filename):\n     try:\n         with open(filename) as f:\n             contents = f.read()\n-    except EnvironmentError:\n+    except OSError:\n         raise NotThisMethod(\"unable to read _version.py\")\n     mo = re.search(r\"version_json = '''\\n(.*)'''  # END VERSION_JSON\",\n                    contents, re.M | re.S)\n@@ -1258,16 +1488,64 @@ def render_pep440(pieces):\n     return rendered\n \n \n+def render_pep440_branch(pieces):\n+    \"\"\"TAG[[.dev0]+DISTANCE.gHEX[.dirty]] .\n+\n+    The \".dev0\" means not master branch. Note that .dev0 sorts backwards\n+    (a feature branch will appear \"older\" than the master branch).\n+\n+    Exceptions:\n+    1: no tags. 0[.dev0]+untagged.DISTANCE.gHEX[.dirty]\n+    \"\"\"\n+    if pieces[\"closest-tag\"]:\n+        rendered = pieces[\"closest-tag\"]\n+        if pieces[\"distance\"] or pieces[\"dirty\"]:\n+            if pieces[\"branch\"] != \"master\":\n+                rendered += \".dev0\"\n+            rendered += plus_or_dot(pieces)\n+            rendered += \"%d.g%s\" % (pieces[\"distance\"], pieces[\"short\"])\n+            if pieces[\"dirty\"]:\n+                rendered += \".dirty\"\n+    else:\n+        # exception #1\n+        rendered = \"0\"\n+        if pieces[\"branch\"] != \"master\":\n+            rendered += \".dev0\"\n+        rendered += \"+untagged.%d.g%s\" % (pieces[\"distance\"],\n+                                          pieces[\"short\"])\n+        if pieces[\"dirty\"]:\n+            rendered += \".dirty\"\n+    return rendered\n+\n+\n+def pep440_split_post(ver):\n+    \"\"\"Split pep440 version string at the post-release segment.\n+\n+    Returns the release segments before the post-release and the\n+    post-release version number (or -1 if no post-release segment is present).\n+    \"\"\"\n+    vc = str.split(ver, \".post\")\n+    return vc[0], int(vc[1] or 0) if len(vc) == 2 else None\n+\n+\n def render_pep440_pre(pieces):\n-    \"\"\"TAG[.post0.devDISTANCE] -- No -dirty.\n+    \"\"\"TAG[.postN.devDISTANCE] -- No -dirty.\n \n     Exceptions:\n     1: no tags. 0.post0.devDISTANCE\n     \"\"\"\n     if pieces[\"closest-tag\"]:\n-        rendered = pieces[\"closest-tag\"]\n         if pieces[\"distance\"]:\n-            rendered += \".post0.dev%d\" % pieces[\"distance\"]\n+            # update the post release segment\n+            tag_version, post_version = pep440_split_post(pieces[\"closest-tag\"])\n+            rendered = tag_version\n+            if post_version is not None:\n+                rendered += \".post%d.dev%d\" % (post_version + 1, pieces[\"distance\"])\n+            else:\n+                rendered += \".post0.dev%d\" % (pieces[\"distance\"])\n+        else:\n+            # no commits, use the tag as the version\n+            rendered = pieces[\"closest-tag\"]\n     else:\n         # exception #1\n         rendered = \"0.post0.dev%d\" % pieces[\"distance\"]\n@@ -1301,6 +1579,35 @@ def render_pep440_post(pieces):\n     return rendered\n \n \n+def render_pep440_post_branch(pieces):\n+    \"\"\"TAG[.postDISTANCE[.dev0]+gHEX[.dirty]] .\n+\n+    The \".dev0\" means not master branch.\n+\n+    Exceptions:\n+    1: no tags. 0.postDISTANCE[.dev0]+gHEX[.dirty]\n+    \"\"\"\n+    if pieces[\"closest-tag\"]:\n+        rendered = pieces[\"closest-tag\"]\n+        if pieces[\"distance\"] or pieces[\"dirty\"]:\n+            rendered += \".post%d\" % pieces[\"distance\"]\n+            if pieces[\"branch\"] != \"master\":\n+                rendered += \".dev0\"\n+            rendered += plus_or_dot(pieces)\n+            rendered += \"g%s\" % pieces[\"short\"]\n+            if pieces[\"dirty\"]:\n+                rendered += \".dirty\"\n+    else:\n+        # exception #1\n+        rendered = \"0.post%d\" % pieces[\"distance\"]\n+        if pieces[\"branch\"] != \"master\":\n+            rendered += \".dev0\"\n+        rendered += \"+g%s\" % pieces[\"short\"]\n+        if pieces[\"dirty\"]:\n+            rendered += \".dirty\"\n+    return rendered\n+\n+\n def render_pep440_old(pieces):\n     \"\"\"TAG[.postDISTANCE[.dev0]] .\n \n@@ -1377,10 +1684,14 @@ def render(pieces, style):\n \n     if style == \"pep440\":\n         rendered = render_pep440(pieces)\n+    elif style == \"pep440-branch\":\n+        rendered = render_pep440_branch(pieces)\n     elif style == \"pep440-pre\":\n         rendered = render_pep440_pre(pieces)\n     elif style == \"pep440-post\":\n         rendered = render_pep440_post(pieces)\n+    elif style == \"pep440-post-branch\":\n+        rendered = render_pep440_post_branch(pieces)\n     elif style == \"pep440-old\":\n         rendered = render_pep440_old(pieces)\n     elif style == \"git-describe\":\n@@ -1481,7 +1792,7 @@ def get_version():\n \n \n def get_cmdclass(cmdclass=None):\n-    \"\"\"Get the custom setuptools/distutils subclasses used by Versioneer.\n+    \"\"\"Get the custom setuptools subclasses used by Versioneer.\n \n     If the package uses a different cmdclass (e.g. one from numpy), it\n     should be provide as an argument.\n@@ -1503,8 +1814,8 @@ def get_cmdclass(cmdclass=None):\n \n     cmds = {} if cmdclass is None else cmdclass.copy()\n \n-    # we add \"version\" to both distutils and setuptools\n-    from distutils.core import Command\n+    # we add \"version\" to setuptools\n+    from setuptools import Command\n \n     class cmd_version(Command):\n         description = \"report generated version string\"\n@@ -1527,7 +1838,7 @@ def run(self):\n                 print(\" error: %s\" % vers[\"error\"])\n     cmds[\"version\"] = cmd_version\n \n-    # we override \"build_py\" in both distutils and setuptools\n+    # we override \"build_py\" in setuptools\n     #\n     # most invocation pathways end up running build_py:\n     #  distutils/build -> build_py\n@@ -1542,13 +1853,14 @@ def run(self):\n     #   then does setup.py bdist_wheel, or sometimes setup.py install\n     #  setup.py egg_info -> ?\n \n+    # pip install -e . and setuptool/editable_wheel will invoke build_py\n+    # but the build_py command is not expected to copy any files.\n+\n     # we override different \"build_py\" commands for both environments\n     if 'build_py' in cmds:\n         _build_py = cmds['build_py']\n-    elif \"setuptools\" in sys.modules:\n-        from setuptools.command.build_py import build_py as _build_py\n     else:\n-        from distutils.command.build_py import build_py as _build_py\n+        from setuptools.command.build_py import build_py as _build_py\n \n     class cmd_build_py(_build_py):\n         def run(self):\n@@ -1556,6 +1868,10 @@ def run(self):\n             cfg = get_config_from_root(root)\n             versions = get_versions()\n             _build_py.run(self)\n+            if getattr(self, \"editable_mode\", False):\n+                # During editable installs `.py` and data files are\n+                # not copied to build_lib\n+                return\n             # now locate _version.py in the new build/ directory and replace\n             # it with an updated value\n             if cfg.versionfile_build:\n@@ -1565,10 +1881,10 @@ def run(self):\n                 write_to_version_file(target_versionfile, versions)\n     cmds[\"build_py\"] = cmd_build_py\n \n-    if \"setuptools\" in sys.modules:\n-        from setuptools.command.build_ext import build_ext as _build_ext\n+    if 'build_ext' in cmds:\n+        _build_ext = cmds['build_ext']\n     else:\n-        from distutils.command.build_ext import build_ext as _build_ext\n+        from setuptools.command.build_ext import build_ext as _build_ext\n \n     class cmd_build_ext(_build_ext):\n         def run(self):\n@@ -1585,7 +1901,12 @@ def run(self):\n             # now locate _version.py in the new build/ directory and replace\n             # it with an updated value\n             target_versionfile = os.path.join(self.build_lib,\n-                                              cfg.versionfile_source)\n+                                              cfg.versionfile_build)\n+            if not os.path.exists(target_versionfile):\n+                print(f\"Warning: {target_versionfile} does not exist, skipping \"\n+                      \"version update. This can happen if you are running build_ext \"\n+                      \"without first running build_py.\")\n+                return\n             print(\"UPDATING %s\" % target_versionfile)\n             write_to_version_file(target_versionfile, versions)\n     cmds[\"build_ext\"] = cmd_build_ext\n@@ -1623,7 +1944,10 @@ def run(self):\n         del cmds[\"build_py\"]\n \n     if 'py2exe' in sys.modules:  # py2exe enabled?\n-        from py2exe.distutils_buildexe import py2exe as _py2exe\n+        try:\n+            from py2exe.setuptools_buildexe import py2exe as _py2exe\n+        except ImportError:\n+            from py2exe.distutils_buildexe import py2exe as _py2exe\n \n         class cmd_py2exe(_py2exe):\n             def run(self):\n@@ -1647,13 +1971,48 @@ def run(self):\n                              })\n         cmds[\"py2exe\"] = cmd_py2exe\n \n+    # sdist farms its file list building out to egg_info\n+    if 'egg_info' in cmds:\n+        _sdist = cmds['egg_info']\n+    else:\n+        from setuptools.command.egg_info import egg_info as _egg_info\n+\n+    class cmd_egg_info(_egg_info):\n+        def find_sources(self):\n+            # egg_info.find_sources builds the manifest list and writes it\n+            # in one shot\n+            super().find_sources()\n+\n+            # Modify the filelist and normalize it\n+            root = get_root()\n+            cfg = get_config_from_root(root)\n+            self.filelist.append('versioneer.py')\n+            if cfg.versionfile_source:\n+                # There are rare cases where versionfile_source might not be\n+                # included by default, so we must be explicit\n+                self.filelist.append(cfg.versionfile_source)\n+            self.filelist.sort()\n+            self.filelist.remove_duplicates()\n+\n+            # The write method is hidden in the manifest_maker instance that\n+            # generated the filelist and was thrown away\n+            # We will instead replicate their final normalization (to unicode,\n+            # and POSIX-style paths)\n+            from setuptools import unicode_utils\n+            normalized = [unicode_utils.filesys_decode(f).replace(os.sep, '/')\n+                          for f in self.filelist.files]\n+\n+            manifest_filename = os.path.join(self.egg_info, 'SOURCES.txt')\n+            with open(manifest_filename, 'w') as fobj:\n+                fobj.write('\\n'.join(normalized))\n+\n+    cmds['egg_info'] = cmd_egg_info\n+\n     # we override different \"sdist\" commands for both environments\n     if 'sdist' in cmds:\n         _sdist = cmds['sdist']\n-    elif \"setuptools\" in sys.modules:\n-        from setuptools.command.sdist import sdist as _sdist\n     else:\n-        from distutils.command.sdist import sdist as _sdist\n+        from setuptools.command.sdist import sdist as _sdist\n \n     class cmd_sdist(_sdist):\n         def run(self):\n@@ -1717,21 +2076,26 @@ def make_release_tree(self, base_dir, files):\n \n \"\"\"\n \n-INIT_PY_SNIPPET = \"\"\"\n+OLD_SNIPPET = \"\"\"\n from ._version import get_versions\n __version__ = get_versions()['version']\n del get_versions\n \"\"\"\n \n+INIT_PY_SNIPPET = \"\"\"\n+from . import {0}\n+__version__ = {0}.get_versions()['version']\n+\"\"\"\n+\n \n def do_setup():\n     \"\"\"Do main VCS-independent setup function for installing Versioneer.\"\"\"\n     root = get_root()\n     try:\n         cfg = get_config_from_root(root)\n-    except (EnvironmentError, configparser.NoSectionError,\n+    except (OSError, configparser.NoSectionError,\n             configparser.NoOptionError) as e:\n-        if isinstance(e, (EnvironmentError, configparser.NoSectionError)):\n+        if isinstance(e, (OSError, configparser.NoSectionError)):\n             print(\"Adding sample versioneer config to setup.cfg\",\n                   file=sys.stderr)\n             with open(os.path.join(root, \"setup.cfg\"), \"a\") as f:\n@@ -1755,54 +2119,28 @@ def do_setup():\n         try:\n             with open(ipy, \"r\") as f:\n                 old = f.read()\n-        except EnvironmentError:\n+        except OSError:\n             old = \"\"\n-        if INIT_PY_SNIPPET not in old:\n+        module = os.path.splitext(os.path.basename(cfg.versionfile_source))[0]\n+        snippet = INIT_PY_SNIPPET.format(module)\n+        if OLD_SNIPPET in old:\n+            print(\" replacing boilerplate in %s\" % ipy)\n+            with open(ipy, \"w\") as f:\n+                f.write(old.replace(OLD_SNIPPET, snippet))\n+        elif snippet not in old:\n             print(\" appending to %s\" % ipy)\n             with open(ipy, \"a\") as f:\n-                f.write(INIT_PY_SNIPPET)\n+                f.write(snippet)\n         else:\n             print(\" %s unmodified\" % ipy)\n     else:\n         print(\" %s doesn't exist, ok\" % ipy)\n         ipy = None\n \n-    # Make sure both the top-level \"versioneer.py\" and versionfile_source\n-    # (PKG/_version.py, used by runtime code) are in MANIFEST.in, so\n-    # they'll be copied into source distributions. Pip won't be able to\n-    # install the package without this.\n-    manifest_in = os.path.join(root, \"MANIFEST.in\")\n-    simple_includes = set()\n-    try:\n-        with open(manifest_in, \"r\") as f:\n-            for line in f:\n-                if line.startswith(\"include \"):\n-                    for include in line.split()[1:]:\n-                        simple_includes.add(include)\n-    except EnvironmentError:\n-        pass\n-    # That doesn't cover everything MANIFEST.in can do\n-    # (http://docs.python.org/2/distutils/sourcedist.html#commands), so\n-    # it might give some false negatives. Appending redundant 'include'\n-    # lines is safe, though.\n-    if \"versioneer.py\" not in simple_includes:\n-        print(\" appending 'versioneer.py' to MANIFEST.in\")\n-        with open(manifest_in, \"a\") as f:\n-            f.write(\"include versioneer.py\\n\")\n-    else:\n-        print(\" 'versioneer.py' already in MANIFEST.in\")\n-    if cfg.versionfile_source not in simple_includes:\n-        print(\" appending versionfile_source ('%s') to MANIFEST.in\" %\n-              cfg.versionfile_source)\n-        with open(manifest_in, \"a\") as f:\n-            f.write(\"include %s\\n\" % cfg.versionfile_source)\n-    else:\n-        print(\" versionfile_source already in MANIFEST.in\")\n-\n     # Make VCS-specific changes. For git, this means creating/changing\n     # .gitattributes to mark _version.py for export-subst keyword\n     # substitution.\n-    do_vcs_install(manifest_in, cfg.versionfile_source, ipy)\n+    do_vcs_install(cfg.versionfile_source, ipy)\n     return 0\n \n \n@@ -1843,10 +2181,14 @@ def scan_setup_py():\n     return errors\n \n \n+def setup_command():\n+    \"\"\"Set up Versioneer and exit with appropriate error code.\"\"\"\n+    errors = do_setup()\n+    errors += scan_setup_py()\n+    sys.exit(1 if errors else 0)\n+\n+\n if __name__ == \"__main__\":\n     cmd = sys.argv[1]\n     if cmd == \"setup\":\n-        errors = do_setup()\n-        errors += scan_setup_py()\n-        if errors:\n-            sys.exit(1)\n+        setup_command()\n",
            "comment_added_diff": {
                "2": "# Version: 0.26",
                "30": "### Vendored mode",
                "38": "### Build-time dependency mode",
                "300": "# pylint:disable=invalid-name,import-outside-toplevel,missing-function-docstring",
                "301": "# pylint:disable=missing-class-docstring,too-many-branches,too-many-statements",
                "302": "# pylint:disable=raise-missing-from,too-many-lines,too-many-locals,import-error",
                "303": "# pylint:disable=too-few-public-methods,redefined-outer-name,consider-using-with",
                "304": "# pylint:disable=attribute-defined-outside-init,too-many-arguments",
                "368": "    # This might raise OSError (if setup.cfg is missing), or",
                "387": "        parser.get(\"versioneer\", \"VCS\")  # raise error if missing",
                "430": "        # This hides the console window if pythonw.exe is used",
                "472": "# This file is released into the public domain.",
                "473": "# Generated by versioneer-0.26",
                "474": "# https://github.com/python-versioneer/python-versioneer",
                "545": "        # This hides the console window if pythonw.exe is used",
                "596": "        root = os.path.dirname(root)  # up a level",
                "677": "            # Filter out refs that exactly match prefix or that don't start",
                "678": "            # with a number once the prefix is stripped (mostly a concern",
                "679": "            # when prefix is '')",
                "708": "    # GIT_DIR can interfere with correct operation of Versioneer.",
                "709": "    # It may be intended to be passed to the Versioneer-versioned project,",
                "710": "    # but that should not change where we get our version from.",
                "744": "    # --abbrev-ref was added in git-1.6.3",
                "750": "        # If we aren't exactly on a branch, pick a branch which represents",
                "751": "        # the current commit. If all else fails, we are on a branchless",
                "752": "        # commit.",
                "754": "        # --contains was added in git-1.5.4",
                "759": "        # Remove the first line if we're running detached",
                "763": "        # Strip off the leading \"* \" from the list of branches.",
                "770": "            # Pick the first branch that is returned. Good or bad.",
                "791": "            # unparsable. Maybe git-describe is misbehaving?",
                "817": "        pieces[\"distance\"] = len(out.split())  # total number of commits",
                "880": "        # exception #1",
                "909": "            # update the post release segment",
                "917": "            # no commits, use the tag as the version",
                "971": "        # exception #1",
                "1199": "            # Filter out refs that exactly match prefix or that don't start",
                "1200": "            # with a number once the prefix is stripped (mostly a concern",
                "1201": "            # when prefix is '')",
                "1230": "    # GIT_DIR can interfere with correct operation of Versioneer.",
                "1231": "    # It may be intended to be passed to the Versioneer-versioned project,",
                "1232": "    # but that should not change where we get our version from.",
                "1266": "    # --abbrev-ref was added in git-1.6.3",
                "1272": "        # If we aren't exactly on a branch, pick a branch which represents",
                "1273": "        # the current commit. If all else fails, we are on a branchless",
                "1274": "        # commit.",
                "1276": "        # --contains was added in git-1.5.4",
                "1281": "        # Remove the first line if we're running detached",
                "1285": "        # Strip off the leading \"* \" from the list of branches.",
                "1292": "            # Pick the first branch that is returned. Good or bad.",
                "1313": "            # unparsable. Maybe git-describe is misbehaving?",
                "1339": "        pieces[\"distance\"] = len(out.split())  # total number of commits",
                "1405": "        root = os.path.dirname(root)  # up a level",
                "1414": "# This file was generated by 'versioneer.py' (0.26) from",
                "1510": "        # exception #1",
                "1539": "            # update the post release segment",
                "1547": "            # no commits, use the tag as the version",
                "1601": "        # exception #1",
                "1817": "    # we add \"version\" to setuptools",
                "1841": "    # we override \"build_py\" in setuptools",
                "1856": "    # pip install -e . and setuptool/editable_wheel will invoke build_py",
                "1857": "    # but the build_py command is not expected to copy any files.",
                "1872": "                # During editable installs `.py` and data files are",
                "1873": "                # not copied to build_lib",
                "1974": "    # sdist farms its file list building out to egg_info",
                "1982": "            # egg_info.find_sources builds the manifest list and writes it",
                "1983": "            # in one shot",
                "1986": "            # Modify the filelist and normalize it",
                "1991": "                # There are rare cases where versionfile_source might not be",
                "1992": "                # included by default, so we must be explicit",
                "1997": "            # The write method is hidden in the manifest_maker instance that",
                "1998": "            # generated the filelist and was thrown away",
                "1999": "            # We will instead replicate their final normalization (to unicode,",
                "2000": "            # and POSIX-style paths)"
            },
            "comment_deleted_diff": {
                "2": "# Version: 0.19",
                "330": "    # This might raise EnvironmentError (if setup.cfg is missing), or",
                "338": "    VCS = parser.get(\"versioneer\", \"VCS\")  # mandatory",
                "419": "# This file is released into the public domain. Generated by",
                "420": "# versioneer-0.19 (https://github.com/python-versioneer/python-versioneer)",
                "533": "            root = os.path.dirname(root)  # up a level",
                "684": "            # unparseable. Maybe git-describe is misbehaving?",
                "711": "        pieces[\"distance\"] = int(count_out)  # total number of commits",
                "1082": "            # unparseable. Maybe git-describe is misbehaving?",
                "1109": "        pieces[\"distance\"] = int(count_out)  # total number of commits",
                "1175": "            root = os.path.dirname(root)  # up a level",
                "1184": "# This file was generated by 'versioneer.py' (0.19) from",
                "1506": "    # we add \"version\" to both distutils and setuptools",
                "1530": "    # we override \"build_py\" in both distutils and setuptools",
                "1770": "    # Make sure both the top-level \"versioneer.py\" and versionfile_source",
                "1771": "    # (PKG/_version.py, used by runtime code) are in MANIFEST.in, so",
                "1772": "    # they'll be copied into source distributions. Pip won't be able to",
                "1773": "    # install the package without this.",
                "1784": "    # That doesn't cover everything MANIFEST.in can do",
                "1785": "    # (http://docs.python.org/2/distutils/sourcedist.html#commands), so",
                "1786": "    # it might give some false negatives. Appending redundant 'include'",
                "1787": "    # lines is safe, though."
            },
            "comment_modified_diff": {
                "2": "# Version: 0.19",
                "387": "                                 stdout=subprocess.PIPE,",
                "709": "        count_out, rc = run_command(GITS, [\"rev-list\", \"HEAD\", \"--count\"],",
                "710": "                                    cwd=root)",
                "763": "        rendered = pieces[\"closest-tag\"]",
                "971": "    if not keywords:"
            }
        },
        {
            "commit": "d1f3c326b6691d6725b94f6477febe18386e0d36",
            "timestamp": "2023-05-28T23:20:34+02:00",
            "author": "Dimitri Papadopoulos",
            "commit_message": "MAINT: Update versioneer: 0.26 \u2192 0.28\n\nThis will help move the configuration from setup.cfg to pyproject.toml.",
            "additions": 33,
            "deletions": 22,
            "change_type": "MODIFY",
            "diff": "@@ -1,5 +1,5 @@\n \n-# Version: 0.26\n+# Version: 0.28\n \n \"\"\"The Versioneer - like a rocketeer, but for versions.\n \n@@ -30,21 +30,27 @@\n ### Vendored mode\n \n * `pip install versioneer` to somewhere in your $PATH\n-* add a `[tool.versioneer]` section to your `pyproject.toml or a\n+   * A [conda-forge recipe](https://github.com/conda-forge/versioneer-feedstock) is\n+     available, so you can also use `conda install -c conda-forge versioneer`\n+* add a `[tool.versioneer]` section to your `pyproject.toml` or a\n   `[versioneer]` section to your `setup.cfg` (see [Install](INSTALL.md))\n+   * Note that you will need to add `tomli; python_version < \"3.11\"` to your\n+     build-time dependencies if you use `pyproject.toml`\n * run `versioneer install --vendor` in your source tree, commit the results\n * verify version information with `python setup.py version`\n \n ### Build-time dependency mode\n \n * `pip install versioneer` to somewhere in your $PATH\n-* add a `[tool.versioneer]` section to your `pyproject.toml or a\n+   * A [conda-forge recipe](https://github.com/conda-forge/versioneer-feedstock) is\n+     available, so you can also use `conda install -c conda-forge versioneer`\n+* add a `[tool.versioneer]` section to your `pyproject.toml` or a\n   `[versioneer]` section to your `setup.cfg` (see [Install](INSTALL.md))\n-* add `versioneer` to the `requires` key of the `build-system` table in\n-  `pyproject.toml`:\n+* add `versioneer` (with `[toml]` extra, if configuring in `pyproject.toml`)\n+  to the `requires` key of the `build-system` table in `pyproject.toml`:\n   ```toml\n   [build-system]\n-  requires = [\"setuptools\", \"versioneer\"]\n+  requires = [\"setuptools\", \"versioneer[toml]\"]\n   build-backend = \"setuptools.build_meta\"\n   ```\n * run `versioneer install --no-vendor` in your source tree, commit the results\n@@ -286,9 +292,8 @@\n \n To make Versioneer easier to embed, all its code is dedicated to the public\n domain. The `_version.py` that it creates is also in the public domain.\n-Specifically, both are released under the Creative Commons \"Public Domain\n-Dedication\" license (CC0-1.0), as described in\n-https://creativecommons.org/publicdomain/zero/1.0/ .\n+Specifically, both are released under the \"Unlicense\", as described in\n+https://unlicense.org/.\n \n [pypi-image]: https://img.shields.io/pypi/v/versioneer.svg\n [pypi-url]: https://pypi.python.org/pypi/versioneer/\n@@ -313,11 +318,15 @@\n from pathlib import Path\n from typing import Callable, Dict\n import functools\n-try:\n-    import tomli\n-    have_tomli = True\n-except ImportError:\n-    have_tomli = False\n+\n+have_tomllib = True\n+if sys.version_info >= (3, 11):\n+    import tomllib\n+else:\n+    try:\n+        import tomli as tomllib\n+    except ImportError:\n+        have_tomllib = False\n \n \n class VersioneerConfig:\n@@ -373,12 +382,12 @@ def get_config_from_root(root):\n     pyproject_toml = root / \"pyproject.toml\"\n     setup_cfg = root / \"setup.cfg\"\n     section = None\n-    if pyproject_toml.exists() and have_tomli:\n+    if pyproject_toml.exists() and have_tomllib:\n         try:\n             with open(pyproject_toml, 'rb') as fobj:\n-                pp = tomli.load(fobj)\n+                pp = tomllib.load(fobj)\n             section = pp['tool']['versioneer']\n-        except (tomli.TOMLDecodeError, KeyError):\n+        except (tomllib.TOMLDecodeError, KeyError):\n             pass\n     if not section:\n         parser = configparser.ConfigParser()\n@@ -470,7 +479,7 @@ def run_command(commands, args, cwd=None, verbose=False, hide_stderr=False,\n # that just contains the computed version number.\n \n # This file is released into the public domain.\n-# Generated by versioneer-0.26\n+# Generated by versioneer-0.28\n # https://github.com/python-versioneer/python-versioneer\n \n \"\"\"Git implementation of _version.py.\"\"\"\n@@ -722,7 +731,7 @@ def git_pieces_from_vcs(tag_prefix, root, verbose, runner=run_command):\n     # if there is a tag matching tag_prefix, this yields TAG-NUM-gHEX[-dirty]\n     # if there isn't one, this yields HEX[-dirty] (no NUM)\n     describe_out, rc = runner(GITS, [\n-        \"describe\", \"--tags\", \"--dirty=\", \"--always\", \"--long\",\n+        \"describe\", \"--tags\", \"--dirty\", \"--always\", \"--long\",\n         \"--match\", f\"{tag_prefix}[[:digit:]]*\"\n     ], cwd=root)\n     # --long was added in git-1.5.5\n@@ -1244,7 +1253,7 @@ def git_pieces_from_vcs(tag_prefix, root, verbose, runner=run_command):\n     # if there is a tag matching tag_prefix, this yields TAG-NUM-gHEX[-dirty]\n     # if there isn't one, this yields HEX[-dirty] (no NUM)\n     describe_out, rc = runner(GITS, [\n-        \"describe\", \"--tags\", \"--dirty=\", \"--always\", \"--long\",\n+        \"describe\", \"--tags\", \"--dirty\", \"--always\", \"--long\",\n         \"--match\", f\"{tag_prefix}[[:digit:]]*\"\n     ], cwd=root)\n     # --long was added in git-1.5.5\n@@ -1363,7 +1372,7 @@ def do_vcs_install(versionfile_source, ipy):\n     if \"VERSIONEER_PEP518\" not in globals():\n         try:\n             my_path = __file__\n-            if my_path.endswith(\".pyc\") or my_path.endswith(\".pyo\"):\n+            if my_path.endswith((\".pyc\", \".pyo\")):\n                 my_path = os.path.splitext(my_path)[0] + \".py\"\n             versioneer_file = os.path.relpath(my_path)\n         except NameError:\n@@ -1411,7 +1420,7 @@ def versions_from_parentdir(parentdir_prefix, root, verbose):\n \n \n SHORT_VERSION_PY = \"\"\"\n-# This file was generated by 'versioneer.py' (0.26) from\n+# This file was generated by 'versioneer.py' (0.28) from\n # revision-control system data, or from the parent directory name of an\n # unpacked source archive. Distribution tarballs contain a pre-generated copy\n # of this file.\n@@ -1900,6 +1909,8 @@ def run(self):\n                 return\n             # now locate _version.py in the new build/ directory and replace\n             # it with an updated value\n+            if not cfg.versionfile_build:\n+                return\n             target_versionfile = os.path.join(self.build_lib,\n                                               cfg.versionfile_build)\n             if not os.path.exists(target_versionfile):\n",
            "comment_added_diff": {
                "2": "# Version: 0.28",
                "482": "# Generated by versioneer-0.28",
                "1423": "# This file was generated by 'versioneer.py' (0.28) from"
            },
            "comment_deleted_diff": {
                "2": "# Version: 0.26",
                "473": "# Generated by versioneer-0.26",
                "1414": "# This file was generated by 'versioneer.py' (0.26) from"
            },
            "comment_modified_diff": {
                "2": "# Version: 0.26"
            }
        },
        {
            "commit": "be896c9c652c010093d26bd0ff732dbfabcd6361",
            "timestamp": "2023-08-11T22:40:16+02:00",
            "author": "Stefan van der Walt",
            "commit_message": "MAINT: Remove versioneer (#24196)\n\nCo-authored-by: Ralf Gommers <ralf.gommers@gmail.com>",
            "additions": 0,
            "deletions": 2204,
            "change_type": "DELETE",
            "diff": "@@ -1,2205 +0,0 @@\n-\n-# Version: 0.28\n-\n-\"\"\"The Versioneer - like a rocketeer, but for versions.\n-\n-The Versioneer\n-==============\n-\n-* like a rocketeer, but for versions!\n-* https://github.com/python-versioneer/python-versioneer\n-* Brian Warner\n-* License: Public Domain (Unlicense)\n-* Compatible with: Python 3.7, 3.8, 3.9, 3.10 and pypy3\n-* [![Latest Version][pypi-image]][pypi-url]\n-* [![Build Status][travis-image]][travis-url]\n-\n-This is a tool for managing a recorded version number in setuptools-based\n-python projects. The goal is to remove the tedious and error-prone \"update\n-the embedded version string\" step from your release process. Making a new\n-release should be as easy as recording a new tag in your version-control\n-system, and maybe making new tarballs.\n-\n-\n-## Quick Install\n-\n-Versioneer provides two installation modes. The \"classic\" vendored mode installs\n-a copy of versioneer into your repository. The experimental build-time dependency mode\n-is intended to allow you to skip this step and simplify the process of upgrading.\n-\n-### Vendored mode\n-\n-* `pip install versioneer` to somewhere in your $PATH\n-   * A [conda-forge recipe](https://github.com/conda-forge/versioneer-feedstock) is\n-     available, so you can also use `conda install -c conda-forge versioneer`\n-* add a `[tool.versioneer]` section to your `pyproject.toml` or a\n-  `[versioneer]` section to your `setup.cfg` (see [Install](INSTALL.md))\n-   * Note that you will need to add `tomli; python_version < \"3.11\"` to your\n-     build-time dependencies if you use `pyproject.toml`\n-* run `versioneer install --vendor` in your source tree, commit the results\n-* verify version information with `python setup.py version`\n-\n-### Build-time dependency mode\n-\n-* `pip install versioneer` to somewhere in your $PATH\n-   * A [conda-forge recipe](https://github.com/conda-forge/versioneer-feedstock) is\n-     available, so you can also use `conda install -c conda-forge versioneer`\n-* add a `[tool.versioneer]` section to your `pyproject.toml` or a\n-  `[versioneer]` section to your `setup.cfg` (see [Install](INSTALL.md))\n-* add `versioneer` (with `[toml]` extra, if configuring in `pyproject.toml`)\n-  to the `requires` key of the `build-system` table in `pyproject.toml`:\n-  ```toml\n-  [build-system]\n-  requires = [\"setuptools\", \"versioneer[toml]\"]\n-  build-backend = \"setuptools.build_meta\"\n-  ```\n-* run `versioneer install --no-vendor` in your source tree, commit the results\n-* verify version information with `python setup.py version`\n-\n-## Version Identifiers\n-\n-Source trees come from a variety of places:\n-\n-* a version-control system checkout (mostly used by developers)\n-* a nightly tarball, produced by build automation\n-* a snapshot tarball, produced by a web-based VCS browser, like github's\n-  \"tarball from tag\" feature\n-* a release tarball, produced by \"setup.py sdist\", distributed through PyPI\n-\n-Within each source tree, the version identifier (either a string or a number,\n-this tool is format-agnostic) can come from a variety of places:\n-\n-* ask the VCS tool itself, e.g. \"git describe\" (for checkouts), which knows\n-  about recent \"tags\" and an absolute revision-id\n-* the name of the directory into which the tarball was unpacked\n-* an expanded VCS keyword ($Id$, etc)\n-* a `_version.py` created by some earlier build step\n-\n-For released software, the version identifier is closely related to a VCS\n-tag. Some projects use tag names that include more than just the version\n-string (e.g. \"myproject-1.2\" instead of just \"1.2\"), in which case the tool\n-needs to strip the tag prefix to extract the version identifier. For\n-unreleased software (between tags), the version identifier should provide\n-enough information to help developers recreate the same tree, while also\n-giving them an idea of roughly how old the tree is (after version 1.2, before\n-version 1.3). Many VCS systems can report a description that captures this,\n-for example `git describe --tags --dirty --always` reports things like\n-\"0.7-1-g574ab98-dirty\" to indicate that the checkout is one revision past the\n-0.7 tag, has a unique revision id of \"574ab98\", and is \"dirty\" (it has\n-uncommitted changes).\n-\n-The version identifier is used for multiple purposes:\n-\n-* to allow the module to self-identify its version: `myproject.__version__`\n-* to choose a name and prefix for a 'setup.py sdist' tarball\n-\n-## Theory of Operation\n-\n-Versioneer works by adding a special `_version.py` file into your source\n-tree, where your `__init__.py` can import it. This `_version.py` knows how to\n-dynamically ask the VCS tool for version information at import time.\n-\n-`_version.py` also contains `$Revision$` markers, and the installation\n-process marks `_version.py` to have this marker rewritten with a tag name\n-during the `git archive` command. As a result, generated tarballs will\n-contain enough information to get the proper version.\n-\n-To allow `setup.py` to compute a version too, a `versioneer.py` is added to\n-the top level of your source tree, next to `setup.py` and the `setup.cfg`\n-that configures it. This overrides several distutils/setuptools commands to\n-compute the version when invoked, and changes `setup.py build` and `setup.py\n-sdist` to replace `_version.py` with a small static file that contains just\n-the generated version data.\n-\n-## Installation\n-\n-See [INSTALL.md](./INSTALL.md) for detailed installation instructions.\n-\n-## Version-String Flavors\n-\n-Code which uses Versioneer can learn about its version string at runtime by\n-importing `_version` from your main `__init__.py` file and running the\n-`get_versions()` function. From the \"outside\" (e.g. in `setup.py`), you can\n-import the top-level `versioneer.py` and run `get_versions()`.\n-\n-Both functions return a dictionary with different flavors of version\n-information:\n-\n-* `['version']`: A condensed version string, rendered using the selected\n-  style. This is the most commonly used value for the project's version\n-  string. The default \"pep440\" style yields strings like `0.11`,\n-  `0.11+2.g1076c97`, or `0.11+2.g1076c97.dirty`. See the \"Styles\" section\n-  below for alternative styles.\n-\n-* `['full-revisionid']`: detailed revision identifier. For Git, this is the\n-  full SHA1 commit id, e.g. \"1076c978a8d3cfc70f408fe5974aa6c092c949ac\".\n-\n-* `['date']`: Date and time of the latest `HEAD` commit. For Git, it is the\n-  commit date in ISO 8601 format. This will be None if the date is not\n-  available.\n-\n-* `['dirty']`: a boolean, True if the tree has uncommitted changes. Note that\n-  this is only accurate if run in a VCS checkout, otherwise it is likely to\n-  be False or None\n-\n-* `['error']`: if the version string could not be computed, this will be set\n-  to a string describing the problem, otherwise it will be None. It may be\n-  useful to throw an exception in setup.py if this is set, to avoid e.g.\n-  creating tarballs with a version string of \"unknown\".\n-\n-Some variants are more useful than others. Including `full-revisionid` in a\n-bug report should allow developers to reconstruct the exact code being tested\n-(or indicate the presence of local changes that should be shared with the\n-developers). `version` is suitable for display in an \"about\" box or a CLI\n-`--version` output: it can be easily compared against release notes and lists\n-of bugs fixed in various releases.\n-\n-The installer adds the following text to your `__init__.py` to place a basic\n-version in `YOURPROJECT.__version__`:\n-\n-    from ._version import get_versions\n-    __version__ = get_versions()['version']\n-    del get_versions\n-\n-## Styles\n-\n-The setup.cfg `style=` configuration controls how the VCS information is\n-rendered into a version string.\n-\n-The default style, \"pep440\", produces a PEP440-compliant string, equal to the\n-un-prefixed tag name for actual releases, and containing an additional \"local\n-version\" section with more detail for in-between builds. For Git, this is\n-TAG[+DISTANCE.gHEX[.dirty]] , using information from `git describe --tags\n---dirty --always`. For example \"0.11+2.g1076c97.dirty\" indicates that the\n-tree is like the \"1076c97\" commit but has uncommitted changes (\".dirty\"), and\n-that this commit is two revisions (\"+2\") beyond the \"0.11\" tag. For released\n-software (exactly equal to a known tag), the identifier will only contain the\n-stripped tag, e.g. \"0.11\".\n-\n-Other styles are available. See [details.md](details.md) in the Versioneer\n-source tree for descriptions.\n-\n-## Debugging\n-\n-Versioneer tries to avoid fatal errors: if something goes wrong, it will tend\n-to return a version of \"0+unknown\". To investigate the problem, run `setup.py\n-version`, which will run the version-lookup code in a verbose mode, and will\n-display the full contents of `get_versions()` (including the `error` string,\n-which may help identify what went wrong).\n-\n-## Known Limitations\n-\n-Some situations are known to cause problems for Versioneer. This details the\n-most significant ones. More can be found on Github\n-[issues page](https://github.com/python-versioneer/python-versioneer/issues).\n-\n-### Subprojects\n-\n-Versioneer has limited support for source trees in which `setup.py` is not in\n-the root directory (e.g. `setup.py` and `.git/` are *not* siblings). The are\n-two common reasons why `setup.py` might not be in the root:\n-\n-* Source trees which contain multiple subprojects, such as\n-  [Buildbot](https://github.com/buildbot/buildbot), which contains both\n-  \"master\" and \"slave\" subprojects, each with their own `setup.py`,\n-  `setup.cfg`, and `tox.ini`. Projects like these produce multiple PyPI\n-  distributions (and upload multiple independently-installable tarballs).\n-* Source trees whose main purpose is to contain a C library, but which also\n-  provide bindings to Python (and perhaps other languages) in subdirectories.\n-\n-Versioneer will look for `.git` in parent directories, and most operations\n-should get the right version string. However `pip` and `setuptools` have bugs\n-and implementation details which frequently cause `pip install .` from a\n-subproject directory to fail to find a correct version string (so it usually\n-defaults to `0+unknown`).\n-\n-`pip install --editable .` should work correctly. `setup.py install` might\n-work too.\n-\n-Pip-8.1.1 is known to have this problem, but hopefully it will get fixed in\n-some later version.\n-\n-[Bug #38](https://github.com/python-versioneer/python-versioneer/issues/38) is tracking\n-this issue. The discussion in\n-[PR #61](https://github.com/python-versioneer/python-versioneer/pull/61) describes the\n-issue from the Versioneer side in more detail.\n-[pip PR#3176](https://github.com/pypa/pip/pull/3176) and\n-[pip PR#3615](https://github.com/pypa/pip/pull/3615) contain work to improve\n-pip to let Versioneer work correctly.\n-\n-Versioneer-0.16 and earlier only looked for a `.git` directory next to the\n-`setup.cfg`, so subprojects were completely unsupported with those releases.\n-\n-### Editable installs with setuptools <= 18.5\n-\n-`setup.py develop` and `pip install --editable .` allow you to install a\n-project into a virtualenv once, then continue editing the source code (and\n-test) without re-installing after every change.\n-\n-\"Entry-point scripts\" (`setup(entry_points={\"console_scripts\": ..})`) are a\n-convenient way to specify executable scripts that should be installed along\n-with the python package.\n-\n-These both work as expected when using modern setuptools. When using\n-setuptools-18.5 or earlier, however, certain operations will cause\n-`pkg_resources.DistributionNotFound` errors when running the entrypoint\n-script, which must be resolved by re-installing the package. This happens\n-when the install happens with one version, then the egg_info data is\n-regenerated while a different version is checked out. Many setup.py commands\n-cause egg_info to be rebuilt (including `sdist`, `wheel`, and installing into\n-a different virtualenv), so this can be surprising.\n-\n-[Bug #83](https://github.com/python-versioneer/python-versioneer/issues/83) describes\n-this one, but upgrading to a newer version of setuptools should probably\n-resolve it.\n-\n-\n-## Updating Versioneer\n-\n-To upgrade your project to a new release of Versioneer, do the following:\n-\n-* install the new Versioneer (`pip install -U versioneer` or equivalent)\n-* edit `setup.cfg` and `pyproject.toml`, if necessary,\n-  to include any new configuration settings indicated by the release notes.\n-  See [UPGRADING](./UPGRADING.md) for details.\n-* re-run `versioneer install --[no-]vendor` in your source tree, to replace\n-  `SRC/_version.py`\n-* commit any changed files\n-\n-## Future Directions\n-\n-This tool is designed to make it easily extended to other version-control\n-systems: all VCS-specific components are in separate directories like\n-src/git/ . The top-level `versioneer.py` script is assembled from these\n-components by running make-versioneer.py . In the future, make-versioneer.py\n-will take a VCS name as an argument, and will construct a version of\n-`versioneer.py` that is specific to the given VCS. It might also take the\n-configuration arguments that are currently provided manually during\n-installation by editing setup.py . Alternatively, it might go the other\n-direction and include code from all supported VCS systems, reducing the\n-number of intermediate scripts.\n-\n-## Similar projects\n-\n-* [setuptools_scm](https://github.com/pypa/setuptools_scm/) - a non-vendored build-time\n-  dependency\n-* [minver](https://github.com/jbweston/miniver) - a lightweight reimplementation of\n-  versioneer\n-* [versioningit](https://github.com/jwodder/versioningit) - a PEP 518-based setuptools\n-  plugin\n-\n-## License\n-\n-To make Versioneer easier to embed, all its code is dedicated to the public\n-domain. The `_version.py` that it creates is also in the public domain.\n-Specifically, both are released under the \"Unlicense\", as described in\n-https://unlicense.org/.\n-\n-[pypi-image]: https://img.shields.io/pypi/v/versioneer.svg\n-[pypi-url]: https://pypi.python.org/pypi/versioneer/\n-[travis-image]:\n-https://img.shields.io/travis/com/python-versioneer/python-versioneer.svg\n-[travis-url]: https://travis-ci.com/github/python-versioneer/python-versioneer\n-\n-\"\"\"\n-# pylint:disable=invalid-name,import-outside-toplevel,missing-function-docstring\n-# pylint:disable=missing-class-docstring,too-many-branches,too-many-statements\n-# pylint:disable=raise-missing-from,too-many-lines,too-many-locals,import-error\n-# pylint:disable=too-few-public-methods,redefined-outer-name,consider-using-with\n-# pylint:disable=attribute-defined-outside-init,too-many-arguments\n-\n-import configparser\n-import errno\n-import json\n-import os\n-import re\n-import subprocess\n-import sys\n-from pathlib import Path\n-from typing import Callable, Dict\n-import functools\n-\n-have_tomllib = True\n-if sys.version_info >= (3, 11):\n-    import tomllib\n-else:\n-    try:\n-        import tomli as tomllib\n-    except ImportError:\n-        have_tomllib = False\n-\n-\n-class VersioneerConfig:\n-    \"\"\"Container for Versioneer configuration parameters.\"\"\"\n-\n-\n-def get_root():\n-    \"\"\"Get the project root directory.\n-\n-    We require that all commands are run from the project root, i.e. the\n-    directory that contains setup.py, setup.cfg, and versioneer.py .\n-    \"\"\"\n-    root = os.path.realpath(os.path.abspath(os.getcwd()))\n-    setup_py = os.path.join(root, \"setup.py\")\n-    versioneer_py = os.path.join(root, \"versioneer.py\")\n-    if not (os.path.exists(setup_py) or os.path.exists(versioneer_py)):\n-        # allow 'python path/to/setup.py COMMAND'\n-        root = os.path.dirname(os.path.realpath(os.path.abspath(sys.argv[0])))\n-        setup_py = os.path.join(root, \"setup.py\")\n-        versioneer_py = os.path.join(root, \"versioneer.py\")\n-    if not (os.path.exists(setup_py) or os.path.exists(versioneer_py)):\n-        err = (\"Versioneer was unable to run the project root directory. \"\n-               \"Versioneer requires setup.py to be executed from \"\n-               \"its immediate directory (like 'python setup.py COMMAND'), \"\n-               \"or in a way that lets it use sys.argv[0] to find the root \"\n-               \"(like 'python path/to/setup.py COMMAND').\")\n-        raise VersioneerBadRootError(err)\n-    try:\n-        # Certain runtime workflows (setup.py install/develop in a setuptools\n-        # tree) execute all dependencies in a single python process, so\n-        # \"versioneer\" may be imported multiple times, and python's shared\n-        # module-import table will cache the first one. So we can't use\n-        # os.path.dirname(__file__), as that will find whichever\n-        # versioneer.py was first imported, even in later projects.\n-        my_path = os.path.realpath(os.path.abspath(__file__))\n-        me_dir = os.path.normcase(os.path.splitext(my_path)[0])\n-        vsr_dir = os.path.normcase(os.path.splitext(versioneer_py)[0])\n-        if me_dir != vsr_dir and \"VERSIONEER_PEP518\" not in globals():\n-            print(\"Warning: build in %s is using versioneer.py from %s\"\n-                  % (os.path.dirname(my_path), versioneer_py))\n-    except NameError:\n-        pass\n-    return root\n-\n-\n-def get_config_from_root(root):\n-    \"\"\"Read the project setup.cfg file to determine Versioneer config.\"\"\"\n-    # This might raise OSError (if setup.cfg is missing), or\n-    # configparser.NoSectionError (if it lacks a [versioneer] section), or\n-    # configparser.NoOptionError (if it lacks \"VCS=\"). See the docstring at\n-    # the top of versioneer.py for instructions on writing your setup.cfg .\n-    root = Path(root)\n-    pyproject_toml = root / \"pyproject.toml\"\n-    setup_cfg = root / \"setup.cfg\"\n-    section = None\n-    if pyproject_toml.exists() and have_tomllib:\n-        try:\n-            with open(pyproject_toml, 'rb') as fobj:\n-                pp = tomllib.load(fobj)\n-            section = pp['tool']['versioneer']\n-        except (tomllib.TOMLDecodeError, KeyError):\n-            pass\n-    if not section:\n-        parser = configparser.ConfigParser()\n-        with open(setup_cfg) as cfg_file:\n-            parser.read_file(cfg_file)\n-        parser.get(\"versioneer\", \"VCS\")  # raise error if missing\n-\n-        section = parser[\"versioneer\"]\n-\n-    cfg = VersioneerConfig()\n-    cfg.VCS = section['VCS']\n-    cfg.style = section.get(\"style\", \"\")\n-    cfg.versionfile_source = section.get(\"versionfile_source\")\n-    cfg.versionfile_build = section.get(\"versionfile_build\")\n-    cfg.tag_prefix = section.get(\"tag_prefix\")\n-    if cfg.tag_prefix in (\"''\", '\"\"', None):\n-        cfg.tag_prefix = \"\"\n-    cfg.parentdir_prefix = section.get(\"parentdir_prefix\")\n-    cfg.verbose = section.get(\"verbose\")\n-    return cfg\n-\n-\n-class NotThisMethod(Exception):\n-    \"\"\"Exception raised if a method is not valid for the current scenario.\"\"\"\n-\n-\n-# these dictionaries contain VCS-specific tools\n-LONG_VERSION_PY: Dict[str, str] = {}\n-HANDLERS: Dict[str, Dict[str, Callable]] = {}\n-\n-\n-def register_vcs_handler(vcs, method):  # decorator\n-    \"\"\"Create decorator to mark a method as the handler of a VCS.\"\"\"\n-    def decorate(f):\n-        \"\"\"Store f in HANDLERS[vcs][method].\"\"\"\n-        HANDLERS.setdefault(vcs, {})[method] = f\n-        return f\n-    return decorate\n-\n-\n-def run_command(commands, args, cwd=None, verbose=False, hide_stderr=False,\n-                env=None):\n-    \"\"\"Call the given command(s).\"\"\"\n-    assert isinstance(commands, list)\n-    process = None\n-\n-    popen_kwargs = {}\n-    if sys.platform == \"win32\":\n-        # This hides the console window if pythonw.exe is used\n-        startupinfo = subprocess.STARTUPINFO()\n-        startupinfo.dwFlags |= subprocess.STARTF_USESHOWWINDOW\n-        popen_kwargs[\"startupinfo\"] = startupinfo\n-\n-    for command in commands:\n-        try:\n-            dispcmd = str([command] + args)\n-            # remember shell=False, so use git.cmd on windows, not just git\n-            process = subprocess.Popen([command] + args, cwd=cwd, env=env,\n-                                       stdout=subprocess.PIPE,\n-                                       stderr=(subprocess.PIPE if hide_stderr\n-                                               else None), **popen_kwargs)\n-            break\n-        except OSError:\n-            e = sys.exc_info()[1]\n-            if e.errno == errno.ENOENT:\n-                continue\n-            if verbose:\n-                print(\"unable to run %s\" % dispcmd)\n-                print(e)\n-            return None, None\n-    else:\n-        if verbose:\n-            print(\"unable to find command, tried %s\" % (commands,))\n-        return None, None\n-    stdout = process.communicate()[0].strip().decode()\n-    if process.returncode != 0:\n-        if verbose:\n-            print(\"unable to run %s (error)\" % dispcmd)\n-            print(\"stdout was %s\" % stdout)\n-        return None, process.returncode\n-    return stdout, process.returncode\n-\n-\n-LONG_VERSION_PY['git'] = r'''\n-# This file helps to compute a version number in source trees obtained from\n-# git-archive tarball (such as those provided by githubs download-from-tag\n-# feature). Distribution tarballs (built by setup.py sdist) and build\n-# directories (produced by setup.py build) will contain a much shorter file\n-# that just contains the computed version number.\n-\n-# This file is released into the public domain.\n-# Generated by versioneer-0.28\n-# https://github.com/python-versioneer/python-versioneer\n-\n-\"\"\"Git implementation of _version.py.\"\"\"\n-\n-import errno\n-import os\n-import re\n-import subprocess\n-import sys\n-from typing import Callable, Dict\n-import functools\n-\n-\n-def get_keywords():\n-    \"\"\"Get the keywords needed to look up the version information.\"\"\"\n-    # these strings will be replaced by git during git-archive.\n-    # setup.py/versioneer.py will grep for the variable names, so they must\n-    # each be defined on a line of their own. _version.py will just call\n-    # get_keywords().\n-    git_refnames = \"%(DOLLAR)sFormat:%%d%(DOLLAR)s\"\n-    git_full = \"%(DOLLAR)sFormat:%%H%(DOLLAR)s\"\n-    git_date = \"%(DOLLAR)sFormat:%%ci%(DOLLAR)s\"\n-    keywords = {\"refnames\": git_refnames, \"full\": git_full, \"date\": git_date}\n-    return keywords\n-\n-\n-class VersioneerConfig:\n-    \"\"\"Container for Versioneer configuration parameters.\"\"\"\n-\n-\n-def get_config():\n-    \"\"\"Create, populate and return the VersioneerConfig() object.\"\"\"\n-    # these strings are filled in when 'setup.py versioneer' creates\n-    # _version.py\n-    cfg = VersioneerConfig()\n-    cfg.VCS = \"git\"\n-    cfg.style = \"%(STYLE)s\"\n-    cfg.tag_prefix = \"%(TAG_PREFIX)s\"\n-    cfg.parentdir_prefix = \"%(PARENTDIR_PREFIX)s\"\n-    cfg.versionfile_source = \"%(VERSIONFILE_SOURCE)s\"\n-    cfg.verbose = False\n-    return cfg\n-\n-\n-class NotThisMethod(Exception):\n-    \"\"\"Exception raised if a method is not valid for the current scenario.\"\"\"\n-\n-\n-LONG_VERSION_PY: Dict[str, str] = {}\n-HANDLERS: Dict[str, Dict[str, Callable]] = {}\n-\n-\n-def register_vcs_handler(vcs, method):  # decorator\n-    \"\"\"Create decorator to mark a method as the handler of a VCS.\"\"\"\n-    def decorate(f):\n-        \"\"\"Store f in HANDLERS[vcs][method].\"\"\"\n-        if vcs not in HANDLERS:\n-            HANDLERS[vcs] = {}\n-        HANDLERS[vcs][method] = f\n-        return f\n-    return decorate\n-\n-\n-def run_command(commands, args, cwd=None, verbose=False, hide_stderr=False,\n-                env=None):\n-    \"\"\"Call the given command(s).\"\"\"\n-    assert isinstance(commands, list)\n-    process = None\n-\n-    popen_kwargs = {}\n-    if sys.platform == \"win32\":\n-        # This hides the console window if pythonw.exe is used\n-        startupinfo = subprocess.STARTUPINFO()\n-        startupinfo.dwFlags |= subprocess.STARTF_USESHOWWINDOW\n-        popen_kwargs[\"startupinfo\"] = startupinfo\n-\n-    for command in commands:\n-        try:\n-            dispcmd = str([command] + args)\n-            # remember shell=False, so use git.cmd on windows, not just git\n-            process = subprocess.Popen([command] + args, cwd=cwd, env=env,\n-                                       stdout=subprocess.PIPE,\n-                                       stderr=(subprocess.PIPE if hide_stderr\n-                                               else None), **popen_kwargs)\n-            break\n-        except OSError:\n-            e = sys.exc_info()[1]\n-            if e.errno == errno.ENOENT:\n-                continue\n-            if verbose:\n-                print(\"unable to run %%s\" %% dispcmd)\n-                print(e)\n-            return None, None\n-    else:\n-        if verbose:\n-            print(\"unable to find command, tried %%s\" %% (commands,))\n-        return None, None\n-    stdout = process.communicate()[0].strip().decode()\n-    if process.returncode != 0:\n-        if verbose:\n-            print(\"unable to run %%s (error)\" %% dispcmd)\n-            print(\"stdout was %%s\" %% stdout)\n-        return None, process.returncode\n-    return stdout, process.returncode\n-\n-\n-def versions_from_parentdir(parentdir_prefix, root, verbose):\n-    \"\"\"Try to determine the version from the parent directory name.\n-\n-    Source tarballs conventionally unpack into a directory that includes both\n-    the project name and a version string. We will also support searching up\n-    two directory levels for an appropriately named parent directory\n-    \"\"\"\n-    rootdirs = []\n-\n-    for _ in range(3):\n-        dirname = os.path.basename(root)\n-        if dirname.startswith(parentdir_prefix):\n-            return {\"version\": dirname[len(parentdir_prefix):],\n-                    \"full-revisionid\": None,\n-                    \"dirty\": False, \"error\": None, \"date\": None}\n-        rootdirs.append(root)\n-        root = os.path.dirname(root)  # up a level\n-\n-    if verbose:\n-        print(\"Tried directories %%s but none started with prefix %%s\" %%\n-              (str(rootdirs), parentdir_prefix))\n-    raise NotThisMethod(\"rootdir doesn't start with parentdir_prefix\")\n-\n-\n-@register_vcs_handler(\"git\", \"get_keywords\")\n-def git_get_keywords(versionfile_abs):\n-    \"\"\"Extract version information from the given file.\"\"\"\n-    # the code embedded in _version.py can just fetch the value of these\n-    # keywords. When used from setup.py, we don't want to import _version.py,\n-    # so we do it with a regexp instead. This function is not used from\n-    # _version.py.\n-    keywords = {}\n-    try:\n-        with open(versionfile_abs, \"r\") as fobj:\n-            for line in fobj:\n-                if line.strip().startswith(\"git_refnames =\"):\n-                    mo = re.search(r'=\\s*\"(.*)\"', line)\n-                    if mo:\n-                        keywords[\"refnames\"] = mo.group(1)\n-                if line.strip().startswith(\"git_full =\"):\n-                    mo = re.search(r'=\\s*\"(.*)\"', line)\n-                    if mo:\n-                        keywords[\"full\"] = mo.group(1)\n-                if line.strip().startswith(\"git_date =\"):\n-                    mo = re.search(r'=\\s*\"(.*)\"', line)\n-                    if mo:\n-                        keywords[\"date\"] = mo.group(1)\n-    except OSError:\n-        pass\n-    return keywords\n-\n-\n-@register_vcs_handler(\"git\", \"keywords\")\n-def git_versions_from_keywords(keywords, tag_prefix, verbose):\n-    \"\"\"Get version information from git keywords.\"\"\"\n-    if \"refnames\" not in keywords:\n-        raise NotThisMethod(\"Short version file found\")\n-    date = keywords.get(\"date\")\n-    if date is not None:\n-        # Use only the last line.  Previous lines may contain GPG signature\n-        # information.\n-        date = date.splitlines()[-1]\n-\n-        # git-2.2.0 added \"%%cI\", which expands to an ISO-8601 -compliant\n-        # datestamp. However we prefer \"%%ci\" (which expands to an \"ISO-8601\n-        # -like\" string, which we must then edit to make compliant), because\n-        # it's been around since git-1.5.3, and it's too difficult to\n-        # discover which version we're using, or to work around using an\n-        # older one.\n-        date = date.strip().replace(\" \", \"T\", 1).replace(\" \", \"\", 1)\n-    refnames = keywords[\"refnames\"].strip()\n-    if refnames.startswith(\"$Format\"):\n-        if verbose:\n-            print(\"keywords are unexpanded, not using\")\n-        raise NotThisMethod(\"unexpanded keywords, not a git-archive tarball\")\n-    refs = {r.strip() for r in refnames.strip(\"()\").split(\",\")}\n-    # starting in git-1.8.3, tags are listed as \"tag: foo-1.0\" instead of\n-    # just \"foo-1.0\". If we see a \"tag: \" prefix, prefer those.\n-    TAG = \"tag: \"\n-    tags = {r[len(TAG):] for r in refs if r.startswith(TAG)}\n-    if not tags:\n-        # Either we're using git < 1.8.3, or there really are no tags. We use\n-        # a heuristic: assume all version tags have a digit. The old git %%d\n-        # expansion behaves like git log --decorate=short and strips out the\n-        # refs/heads/ and refs/tags/ prefixes that would let us distinguish\n-        # between branches and tags. By ignoring refnames without digits, we\n-        # filter out many common branch names like \"release\" and\n-        # \"stabilization\", as well as \"HEAD\" and \"master\".\n-        tags = {r for r in refs if re.search(r'\\d', r)}\n-        if verbose:\n-            print(\"discarding '%%s', no digits\" %% \",\".join(refs - tags))\n-    if verbose:\n-        print(\"likely tags: %%s\" %% \",\".join(sorted(tags)))\n-    for ref in sorted(tags):\n-        # sorting will prefer e.g. \"2.0\" over \"2.0rc1\"\n-        if ref.startswith(tag_prefix):\n-            r = ref[len(tag_prefix):]\n-            # Filter out refs that exactly match prefix or that don't start\n-            # with a number once the prefix is stripped (mostly a concern\n-            # when prefix is '')\n-            if not re.match(r'\\d', r):\n-                continue\n-            if verbose:\n-                print(\"picking %%s\" %% r)\n-            return {\"version\": r,\n-                    \"full-revisionid\": keywords[\"full\"].strip(),\n-                    \"dirty\": False, \"error\": None,\n-                    \"date\": date}\n-    # no suitable tags, so version is \"0+unknown\", but full hex is still there\n-    if verbose:\n-        print(\"no suitable tags, using unknown + full revision id\")\n-    return {\"version\": \"0+unknown\",\n-            \"full-revisionid\": keywords[\"full\"].strip(),\n-            \"dirty\": False, \"error\": \"no suitable tags\", \"date\": None}\n-\n-\n-@register_vcs_handler(\"git\", \"pieces_from_vcs\")\n-def git_pieces_from_vcs(tag_prefix, root, verbose, runner=run_command):\n-    \"\"\"Get version from 'git describe' in the root of the source tree.\n-\n-    This only gets called if the git-archive 'subst' keywords were *not*\n-    expanded, and _version.py hasn't already been rewritten with a short\n-    version string, meaning we're inside a checked out source tree.\n-    \"\"\"\n-    GITS = [\"git\"]\n-    if sys.platform == \"win32\":\n-        GITS = [\"git.cmd\", \"git.exe\"]\n-\n-    # GIT_DIR can interfere with correct operation of Versioneer.\n-    # It may be intended to be passed to the Versioneer-versioned project,\n-    # but that should not change where we get our version from.\n-    env = os.environ.copy()\n-    env.pop(\"GIT_DIR\", None)\n-    runner = functools.partial(runner, env=env)\n-\n-    _, rc = runner(GITS, [\"rev-parse\", \"--git-dir\"], cwd=root,\n-                   hide_stderr=not verbose)\n-    if rc != 0:\n-        if verbose:\n-            print(\"Directory %%s not under git control\" %% root)\n-        raise NotThisMethod(\"'git rev-parse --git-dir' returned error\")\n-\n-    # if there is a tag matching tag_prefix, this yields TAG-NUM-gHEX[-dirty]\n-    # if there isn't one, this yields HEX[-dirty] (no NUM)\n-    describe_out, rc = runner(GITS, [\n-        \"describe\", \"--tags\", \"--dirty=\", \"--always\", \"--long\",\n-        \"--match\", f\"{tag_prefix}[[:digit:]]*\"\n-    ], cwd=root)\n-    # --long was added in git-1.5.5\n-    if describe_out is None:\n-        raise NotThisMethod(\"'git describe' failed\")\n-    describe_out = describe_out.strip()\n-    full_out, rc = runner(GITS, [\"rev-parse\", \"HEAD\"], cwd=root)\n-    if full_out is None:\n-        raise NotThisMethod(\"'git rev-parse' failed\")\n-    full_out = full_out.strip()\n-\n-    pieces = {}\n-    pieces[\"long\"] = full_out\n-    pieces[\"short\"] = full_out[:7]  # maybe improved later\n-    pieces[\"error\"] = None\n-\n-    branch_name, rc = runner(GITS, [\"rev-parse\", \"--abbrev-ref\", \"HEAD\"],\n-                             cwd=root)\n-    # --abbrev-ref was added in git-1.6.3\n-    if rc != 0 or branch_name is None:\n-        raise NotThisMethod(\"'git rev-parse --abbrev-ref' returned error\")\n-    branch_name = branch_name.strip()\n-\n-    if branch_name == \"HEAD\":\n-        # If we aren't exactly on a branch, pick a branch which represents\n-        # the current commit. If all else fails, we are on a branchless\n-        # commit.\n-        branches, rc = runner(GITS, [\"branch\", \"--contains\"], cwd=root)\n-        # --contains was added in git-1.5.4\n-        if rc != 0 or branches is None:\n-            raise NotThisMethod(\"'git branch --contains' returned error\")\n-        branches = branches.split(\"\\n\")\n-\n-        # Remove the first line if we're running detached\n-        if \"(\" in branches[0]:\n-            branches.pop(0)\n-\n-        # Strip off the leading \"* \" from the list of branches.\n-        branches = [branch[2:] for branch in branches]\n-        if \"master\" in branches:\n-            branch_name = \"master\"\n-        elif not branches:\n-            branch_name = None\n-        else:\n-            # Pick the first branch that is returned. Good or bad.\n-            branch_name = branches[0]\n-\n-    pieces[\"branch\"] = branch_name\n-\n-    # parse describe_out. It will be like TAG-NUM-gHEX[-dirty] or HEX[-dirty]\n-    # TAG might have hyphens.\n-    git_describe = describe_out\n-\n-    # look for -dirty suffix\n-    dirty = git_describe.endswith(\"-dirty\")\n-    pieces[\"dirty\"] = dirty\n-    if dirty:\n-        git_describe = git_describe[:git_describe.rindex(\"-dirty\")]\n-\n-    # now we have TAG-NUM-gHEX or HEX\n-\n-    if \"-\" in git_describe:\n-        # TAG-NUM-gHEX\n-        mo = re.search(r'^(.+)-(\\d+)-g([0-9a-f]+)$', git_describe)\n-        if not mo:\n-            # unparsable. Maybe git-describe is misbehaving?\n-            pieces[\"error\"] = (\"unable to parse git-describe output: '%%s'\"\n-                               %% describe_out)\n-            return pieces\n-\n-        # tag\n-        full_tag = mo.group(1)\n-        if not full_tag.startswith(tag_prefix):\n-            if verbose:\n-                fmt = \"tag '%%s' doesn't start with prefix '%%s'\"\n-                print(fmt %% (full_tag, tag_prefix))\n-            pieces[\"error\"] = (\"tag '%%s' doesn't start with prefix '%%s'\"\n-                               %% (full_tag, tag_prefix))\n-            return pieces\n-        pieces[\"closest-tag\"] = full_tag[len(tag_prefix):]\n-\n-        # distance: number of commits since tag\n-        pieces[\"distance\"] = int(mo.group(2))\n-\n-        # commit: short hex revision ID\n-        pieces[\"short\"] = mo.group(3)\n-\n-    else:\n-        # HEX: no tags\n-        pieces[\"closest-tag\"] = None\n-        out, rc = runner(GITS, [\"rev-list\", \"HEAD\", \"--left-right\"], cwd=root)\n-        pieces[\"distance\"] = len(out.split())  # total number of commits\n-\n-    # commit date: see ISO-8601 comment in git_versions_from_keywords()\n-    date = runner(GITS, [\"show\", \"-s\", \"--format=%%ci\", \"HEAD\"], cwd=root)[0].strip()\n-    # Use only the last line.  Previous lines may contain GPG signature\n-    # information.\n-    date = date.splitlines()[-1]\n-    pieces[\"date\"] = date.strip().replace(\" \", \"T\", 1).replace(\" \", \"\", 1)\n-\n-    return pieces\n-\n-\n-def plus_or_dot(pieces):\n-    \"\"\"Return a + if we don't already have one, else return a .\"\"\"\n-    if \"+\" in pieces.get(\"closest-tag\", \"\"):\n-        return \".\"\n-    return \"+\"\n-\n-\n-def render_pep440(pieces):\n-    \"\"\"Build up version string, with post-release \"local version identifier\".\n-\n-    Our goal: TAG[+DISTANCE.gHEX[.dirty]] . Note that if you\n-    get a tagged build and then dirty it, you'll get TAG+0.gHEX.dirty\n-\n-    Exceptions:\n-    1: no tags. git_describe was just HEX. 0+untagged.DISTANCE.gHEX[.dirty]\n-    \"\"\"\n-    if pieces[\"closest-tag\"]:\n-        rendered = pieces[\"closest-tag\"]\n-        if pieces[\"distance\"] or pieces[\"dirty\"]:\n-            rendered += plus_or_dot(pieces)\n-            rendered += \"%%d.g%%s\" %% (pieces[\"distance\"], pieces[\"short\"])\n-            if pieces[\"dirty\"]:\n-                rendered += \".dirty\"\n-    else:\n-        # exception #1\n-        rendered = \"0+untagged.%%d.g%%s\" %% (pieces[\"distance\"],\n-                                          pieces[\"short\"])\n-        if pieces[\"dirty\"]:\n-            rendered += \".dirty\"\n-    return rendered\n-\n-\n-def render_pep440_branch(pieces):\n-    \"\"\"TAG[[.dev0]+DISTANCE.gHEX[.dirty]] .\n-\n-    The \".dev0\" means not master branch. Note that .dev0 sorts backwards\n-    (a feature branch will appear \"older\" than the master branch).\n-\n-    Exceptions:\n-    1: no tags. 0[.dev0]+untagged.DISTANCE.gHEX[.dirty]\n-    \"\"\"\n-    if pieces[\"closest-tag\"]:\n-        rendered = pieces[\"closest-tag\"]\n-        if pieces[\"distance\"] or pieces[\"dirty\"]:\n-            if pieces[\"branch\"] != \"master\":\n-                rendered += \".dev0\"\n-            rendered += plus_or_dot(pieces)\n-            rendered += \"%%d.g%%s\" %% (pieces[\"distance\"], pieces[\"short\"])\n-            if pieces[\"dirty\"]:\n-                rendered += \".dirty\"\n-    else:\n-        # exception #1\n-        rendered = \"0\"\n-        if pieces[\"branch\"] != \"master\":\n-            rendered += \".dev0\"\n-        rendered += \"+untagged.%%d.g%%s\" %% (pieces[\"distance\"],\n-                                          pieces[\"short\"])\n-        if pieces[\"dirty\"]:\n-            rendered += \".dirty\"\n-    return rendered\n-\n-\n-def pep440_split_post(ver):\n-    \"\"\"Split pep440 version string at the post-release segment.\n-\n-    Returns the release segments before the post-release and the\n-    post-release version number (or -1 if no post-release segment is present).\n-    \"\"\"\n-    vc = str.split(ver, \".post\")\n-    return vc[0], int(vc[1] or 0) if len(vc) == 2 else None\n-\n-\n-def render_pep440_pre(pieces):\n-    \"\"\"TAG[.postN.devDISTANCE] -- No -dirty.\n-\n-    Exceptions:\n-    1: no tags. 0.post0.devDISTANCE\n-    \"\"\"\n-    if pieces[\"closest-tag\"]:\n-        if pieces[\"distance\"]:\n-            # update the post release segment\n-            tag_version, post_version = pep440_split_post(pieces[\"closest-tag\"])\n-            rendered = tag_version\n-            if post_version is not None:\n-                rendered += \".post%%d.dev%%d\" %% (post_version + 1, pieces[\"distance\"])\n-            else:\n-                rendered += \".post0.dev%%d\" %% (pieces[\"distance\"])\n-        else:\n-            # no commits, use the tag as the version\n-            rendered = pieces[\"closest-tag\"]\n-    else:\n-        # exception #1\n-        rendered = \"0.post0.dev%%d\" %% pieces[\"distance\"]\n-    return rendered\n-\n-\n-def render_pep440_post(pieces):\n-    \"\"\"TAG[.postDISTANCE[.dev0]+gHEX] .\n-\n-    The \".dev0\" means dirty. Note that .dev0 sorts backwards\n-    (a dirty tree will appear \"older\" than the corresponding clean one),\n-    but you shouldn't be releasing software with -dirty anyways.\n-\n-    Exceptions:\n-    1: no tags. 0.postDISTANCE[.dev0]\n-    \"\"\"\n-    if pieces[\"closest-tag\"]:\n-        rendered = pieces[\"closest-tag\"]\n-        if pieces[\"distance\"] or pieces[\"dirty\"]:\n-            rendered += \".post%%d\" %% pieces[\"distance\"]\n-            if pieces[\"dirty\"]:\n-                rendered += \".dev0\"\n-            rendered += plus_or_dot(pieces)\n-            rendered += \"g%%s\" %% pieces[\"short\"]\n-    else:\n-        # exception #1\n-        rendered = \"0.post%%d\" %% pieces[\"distance\"]\n-        if pieces[\"dirty\"]:\n-            rendered += \".dev0\"\n-        rendered += \"+g%%s\" %% pieces[\"short\"]\n-    return rendered\n-\n-\n-def render_pep440_post_branch(pieces):\n-    \"\"\"TAG[.postDISTANCE[.dev0]+gHEX[.dirty]] .\n-\n-    The \".dev0\" means not master branch.\n-\n-    Exceptions:\n-    1: no tags. 0.postDISTANCE[.dev0]+gHEX[.dirty]\n-    \"\"\"\n-    if pieces[\"closest-tag\"]:\n-        rendered = pieces[\"closest-tag\"]\n-        if pieces[\"distance\"] or pieces[\"dirty\"]:\n-            rendered += \".post%%d\" %% pieces[\"distance\"]\n-            if pieces[\"branch\"] != \"master\":\n-                rendered += \".dev0\"\n-            rendered += plus_or_dot(pieces)\n-            rendered += \"g%%s\" %% pieces[\"short\"]\n-            if pieces[\"dirty\"]:\n-                rendered += \".dirty\"\n-    else:\n-        # exception #1\n-        rendered = \"0.post%%d\" %% pieces[\"distance\"]\n-        if pieces[\"branch\"] != \"master\":\n-            rendered += \".dev0\"\n-        rendered += \"+g%%s\" %% pieces[\"short\"]\n-        if pieces[\"dirty\"]:\n-            rendered += \".dirty\"\n-    return rendered\n-\n-\n-def render_pep440_old(pieces):\n-    \"\"\"TAG[.postDISTANCE[.dev0]] .\n-\n-    The \".dev0\" means dirty.\n-\n-    Exceptions:\n-    1: no tags. 0.postDISTANCE[.dev0]\n-    \"\"\"\n-    if pieces[\"closest-tag\"]:\n-        rendered = pieces[\"closest-tag\"]\n-        if pieces[\"distance\"] or pieces[\"dirty\"]:\n-            rendered += \".post%%d\" %% pieces[\"distance\"]\n-            if pieces[\"dirty\"]:\n-                rendered += \".dev0\"\n-    else:\n-        # exception #1\n-        rendered = \"0.post%%d\" %% pieces[\"distance\"]\n-        if pieces[\"dirty\"]:\n-            rendered += \".dev0\"\n-    return rendered\n-\n-\n-def render_git_describe(pieces):\n-    \"\"\"TAG[-DISTANCE-gHEX][-dirty].\n-\n-    Like 'git describe --tags --dirty --always'.\n-\n-    Exceptions:\n-    1: no tags. HEX[-dirty]  (note: no 'g' prefix)\n-    \"\"\"\n-    if pieces[\"closest-tag\"]:\n-        rendered = pieces[\"closest-tag\"]\n-        if pieces[\"distance\"]:\n-            rendered += \"-%%d-g%%s\" %% (pieces[\"distance\"], pieces[\"short\"])\n-    else:\n-        # exception #1\n-        rendered = pieces[\"short\"]\n-    if pieces[\"dirty\"]:\n-        rendered += \"-dirty\"\n-    return rendered\n-\n-\n-def render_git_describe_long(pieces):\n-    \"\"\"TAG-DISTANCE-gHEX[-dirty].\n-\n-    Like 'git describe --tags --dirty --always -long'.\n-    The distance/hash is unconditional.\n-\n-    Exceptions:\n-    1: no tags. HEX[-dirty]  (note: no 'g' prefix)\n-    \"\"\"\n-    if pieces[\"closest-tag\"]:\n-        rendered = pieces[\"closest-tag\"]\n-        rendered += \"-%%d-g%%s\" %% (pieces[\"distance\"], pieces[\"short\"])\n-    else:\n-        # exception #1\n-        rendered = pieces[\"short\"]\n-    if pieces[\"dirty\"]:\n-        rendered += \"-dirty\"\n-    return rendered\n-\n-\n-def render(pieces, style):\n-    \"\"\"Render the given version pieces into the requested style.\"\"\"\n-    if pieces[\"error\"]:\n-        return {\"version\": \"unknown\",\n-                \"full-revisionid\": pieces.get(\"long\"),\n-                \"dirty\": None,\n-                \"error\": pieces[\"error\"],\n-                \"date\": None}\n-\n-    if not style or style == \"default\":\n-        style = \"pep440\"  # the default\n-\n-    if style == \"pep440\":\n-        rendered = render_pep440(pieces)\n-    elif style == \"pep440-branch\":\n-        rendered = render_pep440_branch(pieces)\n-    elif style == \"pep440-pre\":\n-        rendered = render_pep440_pre(pieces)\n-    elif style == \"pep440-post\":\n-        rendered = render_pep440_post(pieces)\n-    elif style == \"pep440-post-branch\":\n-        rendered = render_pep440_post_branch(pieces)\n-    elif style == \"pep440-old\":\n-        rendered = render_pep440_old(pieces)\n-    elif style == \"git-describe\":\n-        rendered = render_git_describe(pieces)\n-    elif style == \"git-describe-long\":\n-        rendered = render_git_describe_long(pieces)\n-    else:\n-        raise ValueError(\"unknown style '%%s'\" %% style)\n-\n-    return {\"version\": rendered, \"full-revisionid\": pieces[\"long\"],\n-            \"dirty\": pieces[\"dirty\"], \"error\": None,\n-            \"date\": pieces.get(\"date\")}\n-\n-\n-def get_versions():\n-    \"\"\"Get version information or return default if unable to do so.\"\"\"\n-    # I am in _version.py, which lives at ROOT/VERSIONFILE_SOURCE. If we have\n-    # __file__, we can work backwards from there to the root. Some\n-    # py2exe/bbfreeze/non-CPython implementations don't do __file__, in which\n-    # case we can only use expanded keywords.\n-\n-    cfg = get_config()\n-    verbose = cfg.verbose\n-\n-    try:\n-        return git_versions_from_keywords(get_keywords(), cfg.tag_prefix,\n-                                          verbose)\n-    except NotThisMethod:\n-        pass\n-\n-    try:\n-        root = os.path.realpath(__file__)\n-        # versionfile_source is the relative path from the top of the source\n-        # tree (where the .git directory might live) to this file. Invert\n-        # this to find the root from __file__.\n-        for _ in cfg.versionfile_source.split('/'):\n-            root = os.path.dirname(root)\n-    except NameError:\n-        return {\"version\": \"0+unknown\", \"full-revisionid\": None,\n-                \"dirty\": None,\n-                \"error\": \"unable to find root of source tree\",\n-                \"date\": None}\n-\n-    try:\n-        pieces = git_pieces_from_vcs(cfg.tag_prefix, root, verbose)\n-        return render(pieces, cfg.style)\n-    except NotThisMethod:\n-        pass\n-\n-    try:\n-        if cfg.parentdir_prefix:\n-            return versions_from_parentdir(cfg.parentdir_prefix, root, verbose)\n-    except NotThisMethod:\n-        pass\n-\n-    return {\"version\": \"0+unknown\", \"full-revisionid\": None,\n-            \"dirty\": None,\n-            \"error\": \"unable to compute version\", \"date\": None}\n-'''\n-\n-\n-@register_vcs_handler(\"git\", \"get_keywords\")\n-def git_get_keywords(versionfile_abs):\n-    \"\"\"Extract version information from the given file.\"\"\"\n-    # the code embedded in _version.py can just fetch the value of these\n-    # keywords. When used from setup.py, we don't want to import _version.py,\n-    # so we do it with a regexp instead. This function is not used from\n-    # _version.py.\n-    keywords = {}\n-    try:\n-        with open(versionfile_abs, \"r\") as fobj:\n-            for line in fobj:\n-                if line.strip().startswith(\"git_refnames =\"):\n-                    mo = re.search(r'=\\s*\"(.*)\"', line)\n-                    if mo:\n-                        keywords[\"refnames\"] = mo.group(1)\n-                if line.strip().startswith(\"git_full =\"):\n-                    mo = re.search(r'=\\s*\"(.*)\"', line)\n-                    if mo:\n-                        keywords[\"full\"] = mo.group(1)\n-                if line.strip().startswith(\"git_date =\"):\n-                    mo = re.search(r'=\\s*\"(.*)\"', line)\n-                    if mo:\n-                        keywords[\"date\"] = mo.group(1)\n-    except OSError:\n-        pass\n-    return keywords\n-\n-\n-@register_vcs_handler(\"git\", \"keywords\")\n-def git_versions_from_keywords(keywords, tag_prefix, verbose):\n-    \"\"\"Get version information from git keywords.\"\"\"\n-    if \"refnames\" not in keywords:\n-        raise NotThisMethod(\"Short version file found\")\n-    date = keywords.get(\"date\")\n-    if date is not None:\n-        # Use only the last line.  Previous lines may contain GPG signature\n-        # information.\n-        date = date.splitlines()[-1]\n-\n-        # git-2.2.0 added \"%cI\", which expands to an ISO-8601 -compliant\n-        # datestamp. However we prefer \"%ci\" (which expands to an \"ISO-8601\n-        # -like\" string, which we must then edit to make compliant), because\n-        # it's been around since git-1.5.3, and it's too difficult to\n-        # discover which version we're using, or to work around using an\n-        # older one.\n-        date = date.strip().replace(\" \", \"T\", 1).replace(\" \", \"\", 1)\n-    refnames = keywords[\"refnames\"].strip()\n-    if refnames.startswith(\"$Format\"):\n-        if verbose:\n-            print(\"keywords are unexpanded, not using\")\n-        raise NotThisMethod(\"unexpanded keywords, not a git-archive tarball\")\n-    refs = {r.strip() for r in refnames.strip(\"()\").split(\",\")}\n-    # starting in git-1.8.3, tags are listed as \"tag: foo-1.0\" instead of\n-    # just \"foo-1.0\". If we see a \"tag: \" prefix, prefer those.\n-    TAG = \"tag: \"\n-    tags = {r[len(TAG):] for r in refs if r.startswith(TAG)}\n-    if not tags:\n-        # Either we're using git < 1.8.3, or there really are no tags. We use\n-        # a heuristic: assume all version tags have a digit. The old git %d\n-        # expansion behaves like git log --decorate=short and strips out the\n-        # refs/heads/ and refs/tags/ prefixes that would let us distinguish\n-        # between branches and tags. By ignoring refnames without digits, we\n-        # filter out many common branch names like \"release\" and\n-        # \"stabilization\", as well as \"HEAD\" and \"master\".\n-        tags = {r for r in refs if re.search(r'\\d', r)}\n-        if verbose:\n-            print(\"discarding '%s', no digits\" % \",\".join(refs - tags))\n-    if verbose:\n-        print(\"likely tags: %s\" % \",\".join(sorted(tags)))\n-    for ref in sorted(tags):\n-        # sorting will prefer e.g. \"2.0\" over \"2.0rc1\"\n-        if ref.startswith(tag_prefix):\n-            r = ref[len(tag_prefix):]\n-            # Filter out refs that exactly match prefix or that don't start\n-            # with a number once the prefix is stripped (mostly a concern\n-            # when prefix is '')\n-            if not re.match(r'\\d', r):\n-                continue\n-            if verbose:\n-                print(\"picking %s\" % r)\n-            return {\"version\": r,\n-                    \"full-revisionid\": keywords[\"full\"].strip(),\n-                    \"dirty\": False, \"error\": None,\n-                    \"date\": date}\n-    # no suitable tags, so version is \"0+unknown\", but full hex is still there\n-    if verbose:\n-        print(\"no suitable tags, using unknown + full revision id\")\n-    return {\"version\": \"0+unknown\",\n-            \"full-revisionid\": keywords[\"full\"].strip(),\n-            \"dirty\": False, \"error\": \"no suitable tags\", \"date\": None}\n-\n-\n-@register_vcs_handler(\"git\", \"pieces_from_vcs\")\n-def git_pieces_from_vcs(tag_prefix, root, verbose, runner=run_command):\n-    \"\"\"Get version from 'git describe' in the root of the source tree.\n-\n-    This only gets called if the git-archive 'subst' keywords were *not*\n-    expanded, and _version.py hasn't already been rewritten with a short\n-    version string, meaning we're inside a checked out source tree.\n-    \"\"\"\n-    GITS = [\"git\"]\n-    if sys.platform == \"win32\":\n-        GITS = [\"git.cmd\", \"git.exe\"]\n-\n-    # GIT_DIR can interfere with correct operation of Versioneer.\n-    # It may be intended to be passed to the Versioneer-versioned project,\n-    # but that should not change where we get our version from.\n-    env = os.environ.copy()\n-    env.pop(\"GIT_DIR\", None)\n-    runner = functools.partial(runner, env=env)\n-\n-    _, rc = runner(GITS, [\"rev-parse\", \"--git-dir\"], cwd=root,\n-                   hide_stderr=not verbose)\n-    if rc != 0:\n-        if verbose:\n-            print(\"Directory %s not under git control\" % root)\n-        raise NotThisMethod(\"'git rev-parse --git-dir' returned error\")\n-\n-    # if there is a tag matching tag_prefix, this yields TAG-NUM-gHEX[-dirty]\n-    # if there isn't one, this yields HEX[-dirty] (no NUM)\n-    describe_out, rc = runner(GITS, [\n-        \"describe\", \"--tags\", \"--dirty=\", \"--always\", \"--long\",\n-        \"--match\", f\"{tag_prefix}[[:digit:]]*\"\n-    ], cwd=root)\n-    # --long was added in git-1.5.5\n-    if describe_out is None:\n-        raise NotThisMethod(\"'git describe' failed\")\n-    describe_out = describe_out.strip()\n-    full_out, rc = runner(GITS, [\"rev-parse\", \"HEAD\"], cwd=root)\n-    if full_out is None:\n-        raise NotThisMethod(\"'git rev-parse' failed\")\n-    full_out = full_out.strip()\n-\n-    pieces = {}\n-    pieces[\"long\"] = full_out\n-    pieces[\"short\"] = full_out[:7]  # maybe improved later\n-    pieces[\"error\"] = None\n-\n-    branch_name, rc = runner(GITS, [\"rev-parse\", \"--abbrev-ref\", \"HEAD\"],\n-                             cwd=root)\n-    # --abbrev-ref was added in git-1.6.3\n-    if rc != 0 or branch_name is None:\n-        raise NotThisMethod(\"'git rev-parse --abbrev-ref' returned error\")\n-    branch_name = branch_name.strip()\n-\n-    if branch_name == \"HEAD\":\n-        # If we aren't exactly on a branch, pick a branch which represents\n-        # the current commit. If all else fails, we are on a branchless\n-        # commit.\n-        branches, rc = runner(GITS, [\"branch\", \"--contains\"], cwd=root)\n-        # --contains was added in git-1.5.4\n-        if rc != 0 or branches is None:\n-            raise NotThisMethod(\"'git branch --contains' returned error\")\n-        branches = branches.split(\"\\n\")\n-\n-        # Remove the first line if we're running detached\n-        if \"(\" in branches[0]:\n-            branches.pop(0)\n-\n-        # Strip off the leading \"* \" from the list of branches.\n-        branches = [branch[2:] for branch in branches]\n-        if \"master\" in branches:\n-            branch_name = \"master\"\n-        elif not branches:\n-            branch_name = None\n-        else:\n-            # Pick the first branch that is returned. Good or bad.\n-            branch_name = branches[0]\n-\n-    pieces[\"branch\"] = branch_name\n-\n-    # parse describe_out. It will be like TAG-NUM-gHEX[-dirty] or HEX[-dirty]\n-    # TAG might have hyphens.\n-    git_describe = describe_out\n-\n-    # look for -dirty suffix\n-    dirty = git_describe.endswith(\"-dirty\")\n-    pieces[\"dirty\"] = dirty\n-    if dirty:\n-        git_describe = git_describe[:git_describe.rindex(\"-dirty\")]\n-\n-    # now we have TAG-NUM-gHEX or HEX\n-\n-    if \"-\" in git_describe:\n-        # TAG-NUM-gHEX\n-        mo = re.search(r'^(.+)-(\\d+)-g([0-9a-f]+)$', git_describe)\n-        if not mo:\n-            # unparsable. Maybe git-describe is misbehaving?\n-            pieces[\"error\"] = (\"unable to parse git-describe output: '%s'\"\n-                               % describe_out)\n-            return pieces\n-\n-        # tag\n-        full_tag = mo.group(1)\n-        if not full_tag.startswith(tag_prefix):\n-            if verbose:\n-                fmt = \"tag '%s' doesn't start with prefix '%s'\"\n-                print(fmt % (full_tag, tag_prefix))\n-            pieces[\"error\"] = (\"tag '%s' doesn't start with prefix '%s'\"\n-                               % (full_tag, tag_prefix))\n-            return pieces\n-        pieces[\"closest-tag\"] = full_tag[len(tag_prefix):]\n-\n-        # distance: number of commits since tag\n-        pieces[\"distance\"] = int(mo.group(2))\n-\n-        # commit: short hex revision ID\n-        pieces[\"short\"] = mo.group(3)\n-\n-    else:\n-        # HEX: no tags\n-        pieces[\"closest-tag\"] = None\n-        out, rc = runner(GITS, [\"rev-list\", \"HEAD\", \"--left-right\"], cwd=root)\n-        pieces[\"distance\"] = len(out.split())  # total number of commits\n-\n-    # commit date: see ISO-8601 comment in git_versions_from_keywords()\n-    date = runner(GITS, [\"show\", \"-s\", \"--format=%ci\", \"HEAD\"], cwd=root)[0].strip()\n-    # Use only the last line.  Previous lines may contain GPG signature\n-    # information.\n-    date = date.splitlines()[-1]\n-    pieces[\"date\"] = date.strip().replace(\" \", \"T\", 1).replace(\" \", \"\", 1)\n-\n-    return pieces\n-\n-\n-def do_vcs_install(versionfile_source, ipy):\n-    \"\"\"Git-specific installation logic for Versioneer.\n-\n-    For Git, this means creating/changing .gitattributes to mark _version.py\n-    for export-subst keyword substitution.\n-    \"\"\"\n-    GITS = [\"git\"]\n-    if sys.platform == \"win32\":\n-        GITS = [\"git.cmd\", \"git.exe\"]\n-    files = [versionfile_source]\n-    if ipy:\n-        files.append(ipy)\n-    if \"VERSIONEER_PEP518\" not in globals():\n-        try:\n-            my_path = __file__\n-            if my_path.endswith((\".pyc\", \".pyo\")):\n-                my_path = os.path.splitext(my_path)[0] + \".py\"\n-            versioneer_file = os.path.relpath(my_path)\n-        except NameError:\n-            versioneer_file = \"versioneer.py\"\n-        files.append(versioneer_file)\n-    present = False\n-    try:\n-        with open(\".gitattributes\", \"r\") as fobj:\n-            for line in fobj:\n-                if line.strip().startswith(versionfile_source):\n-                    if \"export-subst\" in line.strip().split()[1:]:\n-                        present = True\n-                        break\n-    except OSError:\n-        pass\n-    if not present:\n-        with open(\".gitattributes\", \"a+\") as fobj:\n-            fobj.write(f\"{versionfile_source} export-subst\\n\")\n-        files.append(\".gitattributes\")\n-    run_command(GITS, [\"add\", \"--\"] + files)\n-\n-\n-def versions_from_parentdir(parentdir_prefix, root, verbose):\n-    \"\"\"Try to determine the version from the parent directory name.\n-\n-    Source tarballs conventionally unpack into a directory that includes both\n-    the project name and a version string. We will also support searching up\n-    two directory levels for an appropriately named parent directory\n-    \"\"\"\n-    rootdirs = []\n-\n-    for _ in range(3):\n-        dirname = os.path.basename(root)\n-        if dirname.startswith(parentdir_prefix):\n-            return {\"version\": dirname[len(parentdir_prefix):],\n-                    \"full-revisionid\": None,\n-                    \"dirty\": False, \"error\": None, \"date\": None}\n-        rootdirs.append(root)\n-        root = os.path.dirname(root)  # up a level\n-\n-    if verbose:\n-        print(\"Tried directories %s but none started with prefix %s\" %\n-              (str(rootdirs), parentdir_prefix))\n-    raise NotThisMethod(\"rootdir doesn't start with parentdir_prefix\")\n-\n-\n-SHORT_VERSION_PY = \"\"\"\n-# This file was generated by 'versioneer.py' (0.28) from\n-# revision-control system data, or from the parent directory name of an\n-# unpacked source archive. Distribution tarballs contain a pre-generated copy\n-# of this file.\n-\n-import json\n-\n-version_json = '''\n-%s\n-'''  # END VERSION_JSON\n-\n-\n-def get_versions():\n-    return json.loads(version_json)\n-\"\"\"\n-\n-\n-def versions_from_file(filename):\n-    \"\"\"Try to determine the version from _version.py if present.\"\"\"\n-    try:\n-        with open(filename) as f:\n-            contents = f.read()\n-    except OSError:\n-        raise NotThisMethod(\"unable to read _version.py\")\n-    mo = re.search(r\"version_json = '''\\n(.*)'''  # END VERSION_JSON\",\n-                   contents, re.M | re.S)\n-    if not mo:\n-        mo = re.search(r\"version_json = '''\\r\\n(.*)'''  # END VERSION_JSON\",\n-                       contents, re.M | re.S)\n-    if not mo:\n-        raise NotThisMethod(\"no version_json in _version.py\")\n-    return json.loads(mo.group(1))\n-\n-\n-def write_to_version_file(filename, versions):\n-    \"\"\"Write the given version number to the given _version.py file.\"\"\"\n-    os.unlink(filename)\n-    contents = json.dumps(versions, sort_keys=True,\n-                          indent=1, separators=(\",\", \": \"))\n-    with open(filename, \"w\") as f:\n-        f.write(SHORT_VERSION_PY % contents)\n-\n-    print(\"set %s to '%s'\" % (filename, versions[\"version\"]))\n-\n-\n-def plus_or_dot(pieces):\n-    \"\"\"Return a + if we don't already have one, else return a .\"\"\"\n-    if \"+\" in pieces.get(\"closest-tag\", \"\"):\n-        return \".\"\n-    return \"+\"\n-\n-\n-def render_pep440(pieces):\n-    \"\"\"Build up version string, with post-release \"local version identifier\".\n-\n-    Our goal: TAG[+DISTANCE.gHEX[.dirty]] . Note that if you\n-    get a tagged build and then dirty it, you'll get TAG+0.gHEX.dirty\n-\n-    Exceptions:\n-    1: no tags. git_describe was just HEX. 0+untagged.DISTANCE.gHEX[.dirty]\n-    \"\"\"\n-    if pieces[\"closest-tag\"]:\n-        rendered = pieces[\"closest-tag\"]\n-        if pieces[\"distance\"] or pieces[\"dirty\"]:\n-            rendered += plus_or_dot(pieces)\n-            rendered += \"%d.g%s\" % (pieces[\"distance\"], pieces[\"short\"])\n-            if pieces[\"dirty\"]:\n-                rendered += \".dirty\"\n-    else:\n-        # exception #1\n-        rendered = \"0+untagged.%d.g%s\" % (pieces[\"distance\"],\n-                                          pieces[\"short\"])\n-        if pieces[\"dirty\"]:\n-            rendered += \".dirty\"\n-    return rendered\n-\n-\n-def render_pep440_branch(pieces):\n-    \"\"\"TAG[[.dev0]+DISTANCE.gHEX[.dirty]] .\n-\n-    The \".dev0\" means not master branch. Note that .dev0 sorts backwards\n-    (a feature branch will appear \"older\" than the master branch).\n-\n-    Exceptions:\n-    1: no tags. 0[.dev0]+untagged.DISTANCE.gHEX[.dirty]\n-    \"\"\"\n-    if pieces[\"closest-tag\"]:\n-        rendered = pieces[\"closest-tag\"]\n-        if pieces[\"distance\"] or pieces[\"dirty\"]:\n-            if pieces[\"branch\"] != \"master\":\n-                rendered += \".dev0\"\n-            rendered += plus_or_dot(pieces)\n-            rendered += \"%d.g%s\" % (pieces[\"distance\"], pieces[\"short\"])\n-            if pieces[\"dirty\"]:\n-                rendered += \".dirty\"\n-    else:\n-        # exception #1\n-        rendered = \"0\"\n-        if pieces[\"branch\"] != \"master\":\n-            rendered += \".dev0\"\n-        rendered += \"+untagged.%d.g%s\" % (pieces[\"distance\"],\n-                                          pieces[\"short\"])\n-        if pieces[\"dirty\"]:\n-            rendered += \".dirty\"\n-    return rendered\n-\n-\n-def pep440_split_post(ver):\n-    \"\"\"Split pep440 version string at the post-release segment.\n-\n-    Returns the release segments before the post-release and the\n-    post-release version number (or -1 if no post-release segment is present).\n-    \"\"\"\n-    vc = str.split(ver, \".post\")\n-    return vc[0], int(vc[1] or 0) if len(vc) == 2 else None\n-\n-\n-def render_pep440_pre(pieces):\n-    \"\"\"TAG[.postN.devDISTANCE] -- No -dirty.\n-\n-    Exceptions:\n-    1: no tags. 0.post0.devDISTANCE\n-    \"\"\"\n-    if pieces[\"closest-tag\"]:\n-        if pieces[\"distance\"]:\n-            # update the post release segment\n-            tag_version, post_version = pep440_split_post(pieces[\"closest-tag\"])\n-            rendered = tag_version\n-            if post_version is not None:\n-                rendered += \".post%d.dev%d\" % (post_version + 1, pieces[\"distance\"])\n-            else:\n-                rendered += \".post0.dev%d\" % (pieces[\"distance\"])\n-        else:\n-            # no commits, use the tag as the version\n-            rendered = pieces[\"closest-tag\"]\n-    else:\n-        # exception #1\n-        rendered = \"0.post0.dev%d\" % pieces[\"distance\"]\n-    return rendered\n-\n-\n-def render_pep440_post(pieces):\n-    \"\"\"TAG[.postDISTANCE[.dev0]+gHEX] .\n-\n-    The \".dev0\" means dirty. Note that .dev0 sorts backwards\n-    (a dirty tree will appear \"older\" than the corresponding clean one),\n-    but you shouldn't be releasing software with -dirty anyways.\n-\n-    Exceptions:\n-    1: no tags. 0.postDISTANCE[.dev0]\n-    \"\"\"\n-    if pieces[\"closest-tag\"]:\n-        rendered = pieces[\"closest-tag\"]\n-        if pieces[\"distance\"] or pieces[\"dirty\"]:\n-            rendered += \".post%d\" % pieces[\"distance\"]\n-            if pieces[\"dirty\"]:\n-                rendered += \".dev0\"\n-            rendered += plus_or_dot(pieces)\n-            rendered += \"g%s\" % pieces[\"short\"]\n-    else:\n-        # exception #1\n-        rendered = \"0.post%d\" % pieces[\"distance\"]\n-        if pieces[\"dirty\"]:\n-            rendered += \".dev0\"\n-        rendered += \"+g%s\" % pieces[\"short\"]\n-    return rendered\n-\n-\n-def render_pep440_post_branch(pieces):\n-    \"\"\"TAG[.postDISTANCE[.dev0]+gHEX[.dirty]] .\n-\n-    The \".dev0\" means not master branch.\n-\n-    Exceptions:\n-    1: no tags. 0.postDISTANCE[.dev0]+gHEX[.dirty]\n-    \"\"\"\n-    if pieces[\"closest-tag\"]:\n-        rendered = pieces[\"closest-tag\"]\n-        if pieces[\"distance\"] or pieces[\"dirty\"]:\n-            rendered += \".post%d\" % pieces[\"distance\"]\n-            if pieces[\"branch\"] != \"master\":\n-                rendered += \".dev0\"\n-            rendered += plus_or_dot(pieces)\n-            rendered += \"g%s\" % pieces[\"short\"]\n-            if pieces[\"dirty\"]:\n-                rendered += \".dirty\"\n-    else:\n-        # exception #1\n-        rendered = \"0.post%d\" % pieces[\"distance\"]\n-        if pieces[\"branch\"] != \"master\":\n-            rendered += \".dev0\"\n-        rendered += \"+g%s\" % pieces[\"short\"]\n-        if pieces[\"dirty\"]:\n-            rendered += \".dirty\"\n-    return rendered\n-\n-\n-def render_pep440_old(pieces):\n-    \"\"\"TAG[.postDISTANCE[.dev0]] .\n-\n-    The \".dev0\" means dirty.\n-\n-    Exceptions:\n-    1: no tags. 0.postDISTANCE[.dev0]\n-    \"\"\"\n-    if pieces[\"closest-tag\"]:\n-        rendered = pieces[\"closest-tag\"]\n-        if pieces[\"distance\"] or pieces[\"dirty\"]:\n-            rendered += \".post%d\" % pieces[\"distance\"]\n-            if pieces[\"dirty\"]:\n-                rendered += \".dev0\"\n-    else:\n-        # exception #1\n-        rendered = \"0.post%d\" % pieces[\"distance\"]\n-        if pieces[\"dirty\"]:\n-            rendered += \".dev0\"\n-    return rendered\n-\n-\n-def render_git_describe(pieces):\n-    \"\"\"TAG[-DISTANCE-gHEX][-dirty].\n-\n-    Like 'git describe --tags --dirty --always'.\n-\n-    Exceptions:\n-    1: no tags. HEX[-dirty]  (note: no 'g' prefix)\n-    \"\"\"\n-    if pieces[\"closest-tag\"]:\n-        rendered = pieces[\"closest-tag\"]\n-        if pieces[\"distance\"]:\n-            rendered += \"-%d-g%s\" % (pieces[\"distance\"], pieces[\"short\"])\n-    else:\n-        # exception #1\n-        rendered = pieces[\"short\"]\n-    if pieces[\"dirty\"]:\n-        rendered += \"-dirty\"\n-    return rendered\n-\n-\n-def render_git_describe_long(pieces):\n-    \"\"\"TAG-DISTANCE-gHEX[-dirty].\n-\n-    Like 'git describe --tags --dirty --always -long'.\n-    The distance/hash is unconditional.\n-\n-    Exceptions:\n-    1: no tags. HEX[-dirty]  (note: no 'g' prefix)\n-    \"\"\"\n-    if pieces[\"closest-tag\"]:\n-        rendered = pieces[\"closest-tag\"]\n-        rendered += \"-%d-g%s\" % (pieces[\"distance\"], pieces[\"short\"])\n-    else:\n-        # exception #1\n-        rendered = pieces[\"short\"]\n-    if pieces[\"dirty\"]:\n-        rendered += \"-dirty\"\n-    return rendered\n-\n-\n-def render(pieces, style):\n-    \"\"\"Render the given version pieces into the requested style.\"\"\"\n-    if pieces[\"error\"]:\n-        return {\"version\": \"unknown\",\n-                \"full-revisionid\": pieces.get(\"long\"),\n-                \"dirty\": None,\n-                \"error\": pieces[\"error\"],\n-                \"date\": None}\n-\n-    if not style or style == \"default\":\n-        style = \"pep440\"  # the default\n-\n-    if style == \"pep440\":\n-        rendered = render_pep440(pieces)\n-    elif style == \"pep440-branch\":\n-        rendered = render_pep440_branch(pieces)\n-    elif style == \"pep440-pre\":\n-        rendered = render_pep440_pre(pieces)\n-    elif style == \"pep440-post\":\n-        rendered = render_pep440_post(pieces)\n-    elif style == \"pep440-post-branch\":\n-        rendered = render_pep440_post_branch(pieces)\n-    elif style == \"pep440-old\":\n-        rendered = render_pep440_old(pieces)\n-    elif style == \"git-describe\":\n-        rendered = render_git_describe(pieces)\n-    elif style == \"git-describe-long\":\n-        rendered = render_git_describe_long(pieces)\n-    else:\n-        raise ValueError(\"unknown style '%s'\" % style)\n-\n-    return {\"version\": rendered, \"full-revisionid\": pieces[\"long\"],\n-            \"dirty\": pieces[\"dirty\"], \"error\": None,\n-            \"date\": pieces.get(\"date\")}\n-\n-\n-class VersioneerBadRootError(Exception):\n-    \"\"\"The project root directory is unknown or missing key files.\"\"\"\n-\n-\n-def get_versions(verbose=False):\n-    \"\"\"Get the project version from whatever source is available.\n-\n-    Returns dict with two keys: 'version' and 'full'.\n-    \"\"\"\n-    if \"versioneer\" in sys.modules:\n-        # see the discussion in cmdclass.py:get_cmdclass()\n-        del sys.modules[\"versioneer\"]\n-\n-    root = get_root()\n-    cfg = get_config_from_root(root)\n-\n-    assert cfg.VCS is not None, \"please set [versioneer]VCS= in setup.cfg\"\n-    handlers = HANDLERS.get(cfg.VCS)\n-    assert handlers, \"unrecognized VCS '%s'\" % cfg.VCS\n-    verbose = verbose or cfg.verbose\n-    assert cfg.versionfile_source is not None, \\\n-        \"please set versioneer.versionfile_source\"\n-    assert cfg.tag_prefix is not None, \"please set versioneer.tag_prefix\"\n-\n-    versionfile_abs = os.path.join(root, cfg.versionfile_source)\n-\n-    # extract version from first of: _version.py, VCS command (e.g. 'git\n-    # describe'), parentdir. This is meant to work for developers using a\n-    # source checkout, for users of a tarball created by 'setup.py sdist',\n-    # and for users of a tarball/zipball created by 'git archive' or github's\n-    # download-from-tag feature or the equivalent in other VCSes.\n-\n-    get_keywords_f = handlers.get(\"get_keywords\")\n-    from_keywords_f = handlers.get(\"keywords\")\n-    if get_keywords_f and from_keywords_f:\n-        try:\n-            keywords = get_keywords_f(versionfile_abs)\n-            ver = from_keywords_f(keywords, cfg.tag_prefix, verbose)\n-            if verbose:\n-                print(\"got version from expanded keyword %s\" % ver)\n-            return ver\n-        except NotThisMethod:\n-            pass\n-\n-    try:\n-        ver = versions_from_file(versionfile_abs)\n-        if verbose:\n-            print(\"got version from file %s %s\" % (versionfile_abs, ver))\n-        return ver\n-    except NotThisMethod:\n-        pass\n-\n-    from_vcs_f = handlers.get(\"pieces_from_vcs\")\n-    if from_vcs_f:\n-        try:\n-            pieces = from_vcs_f(cfg.tag_prefix, root, verbose)\n-            ver = render(pieces, cfg.style)\n-            if verbose:\n-                print(\"got version from VCS %s\" % ver)\n-            return ver\n-        except NotThisMethod:\n-            pass\n-\n-    try:\n-        if cfg.parentdir_prefix:\n-            ver = versions_from_parentdir(cfg.parentdir_prefix, root, verbose)\n-            if verbose:\n-                print(\"got version from parentdir %s\" % ver)\n-            return ver\n-    except NotThisMethod:\n-        pass\n-\n-    if verbose:\n-        print(\"unable to compute version\")\n-\n-    return {\"version\": \"0+unknown\", \"full-revisionid\": None,\n-            \"dirty\": None, \"error\": \"unable to compute version\",\n-            \"date\": None}\n-\n-\n-def get_version():\n-    \"\"\"Get the short version string for this project.\"\"\"\n-    return get_versions()[\"version\"]\n-\n-\n-def get_cmdclass(cmdclass=None):\n-    \"\"\"Get the custom setuptools subclasses used by Versioneer.\n-\n-    If the package uses a different cmdclass (e.g. one from numpy), it\n-    should be provide as an argument.\n-    \"\"\"\n-    if \"versioneer\" in sys.modules:\n-        del sys.modules[\"versioneer\"]\n-        # this fixes the \"python setup.py develop\" case (also 'install' and\n-        # 'easy_install .'), in which subdependencies of the main project are\n-        # built (using setup.py bdist_egg) in the same python process. Assume\n-        # a main project A and a dependency B, which use different versions\n-        # of Versioneer. A's setup.py imports A's Versioneer, leaving it in\n-        # sys.modules by the time B's setup.py is executed, causing B to run\n-        # with the wrong versioneer. Setuptools wraps the sub-dep builds in a\n-        # sandbox that restores sys.modules to it's pre-build state, so the\n-        # parent is protected against the child's \"import versioneer\". By\n-        # removing ourselves from sys.modules here, before the child build\n-        # happens, we protect the child from the parent's versioneer too.\n-        # Also see https://github.com/python-versioneer/python-versioneer/issues/52\n-\n-    cmds = {} if cmdclass is None else cmdclass.copy()\n-\n-    # we add \"version\" to setuptools\n-    from setuptools import Command\n-\n-    class cmd_version(Command):\n-        description = \"report generated version string\"\n-        user_options = []\n-        boolean_options = []\n-\n-        def initialize_options(self):\n-            pass\n-\n-        def finalize_options(self):\n-            pass\n-\n-        def run(self):\n-            vers = get_versions(verbose=True)\n-            print(\"Version: %s\" % vers[\"version\"])\n-            print(\" full-revisionid: %s\" % vers.get(\"full-revisionid\"))\n-            print(\" dirty: %s\" % vers.get(\"dirty\"))\n-            print(\" date: %s\" % vers.get(\"date\"))\n-            if vers[\"error\"]:\n-                print(\" error: %s\" % vers[\"error\"])\n-    cmds[\"version\"] = cmd_version\n-\n-    # we override \"build_py\" in setuptools\n-    #\n-    # most invocation pathways end up running build_py:\n-    #  distutils/build -> build_py\n-    #  distutils/install -> distutils/build ->..\n-    #  setuptools/bdist_wheel -> distutils/install ->..\n-    #  setuptools/bdist_egg -> distutils/install_lib -> build_py\n-    #  setuptools/install -> bdist_egg ->..\n-    #  setuptools/develop -> ?\n-    #  pip install:\n-    #   copies source tree to a tempdir before running egg_info/etc\n-    #   if .git isn't copied too, 'git describe' will fail\n-    #   then does setup.py bdist_wheel, or sometimes setup.py install\n-    #  setup.py egg_info -> ?\n-\n-    # pip install -e . and setuptool/editable_wheel will invoke build_py\n-    # but the build_py command is not expected to copy any files.\n-\n-    # we override different \"build_py\" commands for both environments\n-    if 'build_py' in cmds:\n-        _build_py = cmds['build_py']\n-    else:\n-        from setuptools.command.build_py import build_py as _build_py\n-\n-    class cmd_build_py(_build_py):\n-        def run(self):\n-            root = get_root()\n-            cfg = get_config_from_root(root)\n-            versions = get_versions()\n-            _build_py.run(self)\n-            if getattr(self, \"editable_mode\", False):\n-                # During editable installs `.py` and data files are\n-                # not copied to build_lib\n-                return\n-            # now locate _version.py in the new build/ directory and replace\n-            # it with an updated value\n-            if cfg.versionfile_build:\n-                target_versionfile = os.path.join(self.build_lib,\n-                                                  cfg.versionfile_build)\n-                print(\"UPDATING %s\" % target_versionfile)\n-                write_to_version_file(target_versionfile, versions)\n-    cmds[\"build_py\"] = cmd_build_py\n-\n-    if 'build_ext' in cmds:\n-        _build_ext = cmds['build_ext']\n-    else:\n-        from setuptools.command.build_ext import build_ext as _build_ext\n-\n-    class cmd_build_ext(_build_ext):\n-        def run(self):\n-            root = get_root()\n-            cfg = get_config_from_root(root)\n-            versions = get_versions()\n-            _build_ext.run(self)\n-            if self.inplace:\n-                # build_ext --inplace will only build extensions in\n-                # build/lib<..> dir with no _version.py to write to.\n-                # As in place builds will already have a _version.py\n-                # in the module dir, we do not need to write one.\n-                return\n-            # now locate _version.py in the new build/ directory and replace\n-            # it with an updated value\n-            if not cfg.versionfile_build:\n-                return\n-            target_versionfile = os.path.join(self.build_lib,\n-                                              cfg.versionfile_build)\n-            if not os.path.exists(target_versionfile):\n-                print(f\"Warning: {target_versionfile} does not exist, skipping \"\n-                      \"version update. This can happen if you are running build_ext \"\n-                      \"without first running build_py.\")\n-                return\n-            print(\"UPDATING %s\" % target_versionfile)\n-            write_to_version_file(target_versionfile, versions)\n-    cmds[\"build_ext\"] = cmd_build_ext\n-\n-    if \"cx_Freeze\" in sys.modules:  # cx_freeze enabled?\n-        from cx_Freeze.dist import build_exe as _build_exe\n-        # nczeczulin reports that py2exe won't like the pep440-style string\n-        # as FILEVERSION, but it can be used for PRODUCTVERSION, e.g.\n-        # setup(console=[{\n-        #   \"version\": versioneer.get_version().split(\"+\", 1)[0], # FILEVERSION\n-        #   \"product_version\": versioneer.get_version(),\n-        #   ...\n-\n-        class cmd_build_exe(_build_exe):\n-            def run(self):\n-                root = get_root()\n-                cfg = get_config_from_root(root)\n-                versions = get_versions()\n-                target_versionfile = cfg.versionfile_source\n-                print(\"UPDATING %s\" % target_versionfile)\n-                write_to_version_file(target_versionfile, versions)\n-\n-                _build_exe.run(self)\n-                os.unlink(target_versionfile)\n-                with open(cfg.versionfile_source, \"w\") as f:\n-                    LONG = LONG_VERSION_PY[cfg.VCS]\n-                    f.write(LONG %\n-                            {\"DOLLAR\": \"$\",\n-                             \"STYLE\": cfg.style,\n-                             \"TAG_PREFIX\": cfg.tag_prefix,\n-                             \"PARENTDIR_PREFIX\": cfg.parentdir_prefix,\n-                             \"VERSIONFILE_SOURCE\": cfg.versionfile_source,\n-                             })\n-        cmds[\"build_exe\"] = cmd_build_exe\n-        del cmds[\"build_py\"]\n-\n-    if 'py2exe' in sys.modules:  # py2exe enabled?\n-        try:\n-            from py2exe.setuptools_buildexe import py2exe as _py2exe\n-        except ImportError:\n-            from py2exe.distutils_buildexe import py2exe as _py2exe\n-\n-        class cmd_py2exe(_py2exe):\n-            def run(self):\n-                root = get_root()\n-                cfg = get_config_from_root(root)\n-                versions = get_versions()\n-                target_versionfile = cfg.versionfile_source\n-                print(\"UPDATING %s\" % target_versionfile)\n-                write_to_version_file(target_versionfile, versions)\n-\n-                _py2exe.run(self)\n-                os.unlink(target_versionfile)\n-                with open(cfg.versionfile_source, \"w\") as f:\n-                    LONG = LONG_VERSION_PY[cfg.VCS]\n-                    f.write(LONG %\n-                            {\"DOLLAR\": \"$\",\n-                             \"STYLE\": cfg.style,\n-                             \"TAG_PREFIX\": cfg.tag_prefix,\n-                             \"PARENTDIR_PREFIX\": cfg.parentdir_prefix,\n-                             \"VERSIONFILE_SOURCE\": cfg.versionfile_source,\n-                             })\n-        cmds[\"py2exe\"] = cmd_py2exe\n-\n-    # sdist farms its file list building out to egg_info\n-    if 'egg_info' in cmds:\n-        _egg_info = cmds['egg_info']\n-    else:\n-        from setuptools.command.egg_info import egg_info as _egg_info\n-\n-    class cmd_egg_info(_egg_info):\n-        def find_sources(self):\n-            # egg_info.find_sources builds the manifest list and writes it\n-            # in one shot\n-            super().find_sources()\n-\n-            # Modify the filelist and normalize it\n-            root = get_root()\n-            cfg = get_config_from_root(root)\n-            self.filelist.append('versioneer.py')\n-            if cfg.versionfile_source:\n-                # There are rare cases where versionfile_source might not be\n-                # included by default, so we must be explicit\n-                self.filelist.append(cfg.versionfile_source)\n-            self.filelist.sort()\n-            self.filelist.remove_duplicates()\n-\n-            # The write method is hidden in the manifest_maker instance that\n-            # generated the filelist and was thrown away\n-            # We will instead replicate their final normalization (to unicode,\n-            # and POSIX-style paths)\n-            from setuptools import unicode_utils\n-            normalized = [unicode_utils.filesys_decode(f).replace(os.sep, '/')\n-                          for f in self.filelist.files]\n-\n-            manifest_filename = os.path.join(self.egg_info, 'SOURCES.txt')\n-            with open(manifest_filename, 'w') as fobj:\n-                fobj.write('\\n'.join(normalized))\n-\n-    cmds['egg_info'] = cmd_egg_info\n-\n-    # we override different \"sdist\" commands for both environments\n-    if 'sdist' in cmds:\n-        _sdist = cmds['sdist']\n-    else:\n-        from setuptools.command.sdist import sdist as _sdist\n-\n-    class cmd_sdist(_sdist):\n-        def run(self):\n-            versions = get_versions()\n-            self._versioneer_generated_versions = versions\n-            # unless we update this, the command will keep using the old\n-            # version\n-            self.distribution.metadata.version = versions[\"version\"]\n-            return _sdist.run(self)\n-\n-        def make_release_tree(self, base_dir, files):\n-            root = get_root()\n-            cfg = get_config_from_root(root)\n-            _sdist.make_release_tree(self, base_dir, files)\n-            # now locate _version.py in the new base_dir directory\n-            # (remembering that it may be a hardlink) and replace it with an\n-            # updated value\n-            target_versionfile = os.path.join(base_dir, cfg.versionfile_source)\n-            print(\"UPDATING %s\" % target_versionfile)\n-            write_to_version_file(target_versionfile,\n-                                  self._versioneer_generated_versions)\n-    cmds[\"sdist\"] = cmd_sdist\n-\n-    return cmds\n-\n-\n-CONFIG_ERROR = \"\"\"\n-setup.cfg is missing the necessary Versioneer configuration. You need\n-a section like:\n-\n- [versioneer]\n- VCS = git\n- style = pep440\n- versionfile_source = src/myproject/_version.py\n- versionfile_build = myproject/_version.py\n- tag_prefix =\n- parentdir_prefix = myproject-\n-\n-You will also need to edit your setup.py to use the results:\n-\n- import versioneer\n- setup(version=versioneer.get_version(),\n-       cmdclass=versioneer.get_cmdclass(), ...)\n-\n-Please read the docstring in ./versioneer.py for configuration instructions,\n-edit setup.cfg, and re-run the installer or 'python versioneer.py setup'.\n-\"\"\"\n-\n-SAMPLE_CONFIG = \"\"\"\n-# See the docstring in versioneer.py for instructions. Note that you must\n-# re-run 'versioneer.py setup' after changing this section, and commit the\n-# resulting files.\n-\n-[versioneer]\n-#VCS = git\n-#style = pep440\n-#versionfile_source =\n-#versionfile_build =\n-#tag_prefix =\n-#parentdir_prefix =\n-\n-\"\"\"\n-\n-OLD_SNIPPET = \"\"\"\n-from ._version import get_versions\n-__version__ = get_versions()['version']\n-del get_versions\n-\"\"\"\n-\n-INIT_PY_SNIPPET = \"\"\"\n-from . import {0}\n-__version__ = {0}.get_versions()['version']\n-\"\"\"\n-\n-\n-def do_setup():\n-    \"\"\"Do main VCS-independent setup function for installing Versioneer.\"\"\"\n-    root = get_root()\n-    try:\n-        cfg = get_config_from_root(root)\n-    except (OSError, configparser.NoSectionError,\n-            configparser.NoOptionError) as e:\n-        if isinstance(e, (OSError, configparser.NoSectionError)):\n-            print(\"Adding sample versioneer config to setup.cfg\",\n-                  file=sys.stderr)\n-            with open(os.path.join(root, \"setup.cfg\"), \"a\") as f:\n-                f.write(SAMPLE_CONFIG)\n-        print(CONFIG_ERROR, file=sys.stderr)\n-        return 1\n-\n-    print(\" creating %s\" % cfg.versionfile_source)\n-    with open(cfg.versionfile_source, \"w\") as f:\n-        LONG = LONG_VERSION_PY[cfg.VCS]\n-        f.write(LONG % {\"DOLLAR\": \"$\",\n-                        \"STYLE\": cfg.style,\n-                        \"TAG_PREFIX\": cfg.tag_prefix,\n-                        \"PARENTDIR_PREFIX\": cfg.parentdir_prefix,\n-                        \"VERSIONFILE_SOURCE\": cfg.versionfile_source,\n-                        })\n-\n-    ipy = os.path.join(os.path.dirname(cfg.versionfile_source),\n-                       \"__init__.py\")\n-    if os.path.exists(ipy):\n-        try:\n-            with open(ipy, \"r\") as f:\n-                old = f.read()\n-        except OSError:\n-            old = \"\"\n-        module = os.path.splitext(os.path.basename(cfg.versionfile_source))[0]\n-        snippet = INIT_PY_SNIPPET.format(module)\n-        if OLD_SNIPPET in old:\n-            print(\" replacing boilerplate in %s\" % ipy)\n-            with open(ipy, \"w\") as f:\n-                f.write(old.replace(OLD_SNIPPET, snippet))\n-        elif snippet not in old:\n-            print(\" appending to %s\" % ipy)\n-            with open(ipy, \"a\") as f:\n-                f.write(snippet)\n-        else:\n-            print(\" %s unmodified\" % ipy)\n-    else:\n-        print(\" %s doesn't exist, ok\" % ipy)\n-        ipy = None\n-\n-    # Make VCS-specific changes. For git, this means creating/changing\n-    # .gitattributes to mark _version.py for export-subst keyword\n-    # substitution.\n-    do_vcs_install(cfg.versionfile_source, ipy)\n-    return 0\n-\n-\n-def scan_setup_py():\n-    \"\"\"Validate the contents of setup.py against Versioneer's expectations.\"\"\"\n-    found = set()\n-    setters = False\n-    errors = 0\n-    with open(\"setup.py\", \"r\") as f:\n-        for line in f.readlines():\n-            if \"import versioneer\" in line:\n-                found.add(\"import\")\n-            if \"versioneer.get_cmdclass()\" in line:\n-                found.add(\"cmdclass\")\n-            if \"versioneer.get_version()\" in line:\n-                found.add(\"get_version\")\n-            if \"versioneer.VCS\" in line:\n-                setters = True\n-            if \"versioneer.versionfile_source\" in line:\n-                setters = True\n-    if len(found) != 3:\n-        print(\"\")\n-        print(\"Your setup.py appears to be missing some important items\")\n-        print(\"(but I might be wrong). Please make sure it has something\")\n-        print(\"roughly like the following:\")\n-        print(\"\")\n-        print(\" import versioneer\")\n-        print(\" setup( version=versioneer.get_version(),\")\n-        print(\"        cmdclass=versioneer.get_cmdclass(),  ...)\")\n-        print(\"\")\n-        errors += 1\n-    if setters:\n-        print(\"You should remove lines like 'versioneer.VCS = ' and\")\n-        print(\"'versioneer.versionfile_source = ' . This configuration\")\n-        print(\"now lives in setup.cfg, and should be removed from setup.py\")\n-        print(\"\")\n-        errors += 1\n-    return errors\n-\n-\n-def setup_command():\n-    \"\"\"Set up Versioneer and exit with appropriate error code.\"\"\"\n-    errors = do_setup()\n-    errors += scan_setup_py()\n-    sys.exit(1 if errors else 0)\n-\n-\n-if __name__ == \"__main__\":\n-    cmd = sys.argv[1]\n-    if cmd == \"setup\":\n-        setup_command()\n",
            "comment_added_diff": {},
            "comment_deleted_diff": {
                "2": "# Version: 0.28",
                "24": "## Quick Install",
                "30": "### Vendored mode",
                "42": "### Build-time dependency mode",
                "59": "## Version Identifiers",
                "96": "## Theory of Operation",
                "114": "## Installation",
                "118": "## Version-String Flavors",
                "164": "## Styles",
                "182": "## Debugging",
                "190": "## Known Limitations",
                "196": "### Subprojects",
                "222": "[Bug #38](https://github.com/python-versioneer/python-versioneer/issues/38) is tracking",
                "224": "[PR #61](https://github.com/python-versioneer/python-versioneer/pull/61) describes the",
                "226": "[pip PR#3176](https://github.com/pypa/pip/pull/3176) and",
                "227": "[pip PR#3615](https://github.com/pypa/pip/pull/3615) contain work to improve",
                "233": "### Editable installs with setuptools <= 18.5",
                "252": "[Bug #83](https://github.com/python-versioneer/python-versioneer/issues/83) describes",
                "257": "## Updating Versioneer",
                "269": "## Future Directions",
                "282": "## Similar projects",
                "291": "## License",
                "305": "# pylint:disable=invalid-name,import-outside-toplevel,missing-function-docstring",
                "306": "# pylint:disable=missing-class-docstring,too-many-branches,too-many-statements",
                "307": "# pylint:disable=raise-missing-from,too-many-lines,too-many-locals,import-error",
                "308": "# pylint:disable=too-few-public-methods,redefined-outer-name,consider-using-with",
                "309": "# pylint:disable=attribute-defined-outside-init,too-many-arguments",
                "346": "        # allow 'python path/to/setup.py COMMAND'",
                "358": "        # Certain runtime workflows (setup.py install/develop in a setuptools",
                "359": "        # tree) execute all dependencies in a single python process, so",
                "360": "        # \"versioneer\" may be imported multiple times, and python's shared",
                "361": "        # module-import table will cache the first one. So we can't use",
                "362": "        # os.path.dirname(__file__), as that will find whichever",
                "363": "        # versioneer.py was first imported, even in later projects.",
                "377": "    # This might raise OSError (if setup.cfg is missing), or",
                "378": "    # configparser.NoSectionError (if it lacks a [versioneer] section), or",
                "379": "    # configparser.NoOptionError (if it lacks \"VCS=\"). See the docstring at",
                "380": "    # the top of versioneer.py for instructions on writing your setup.cfg .",
                "396": "        parser.get(\"versioneer\", \"VCS\")  # raise error if missing",
                "417": "# these dictionaries contain VCS-specific tools",
                "422": "def register_vcs_handler(vcs, method):  # decorator",
                "439": "        # This hides the console window if pythonw.exe is used",
                "447": "            # remember shell=False, so use git.cmd on windows, not just git",
                "475": "# This file helps to compute a version number in source trees obtained from",
                "476": "# git-archive tarball (such as those provided by githubs download-from-tag",
                "477": "# feature). Distribution tarballs (built by setup.py sdist) and build",
                "478": "# directories (produced by setup.py build) will contain a much shorter file",
                "479": "# that just contains the computed version number.",
                "481": "# This file is released into the public domain.",
                "482": "# Generated by versioneer-0.28",
                "483": "# https://github.com/python-versioneer/python-versioneer",
                "498": "    # these strings will be replaced by git during git-archive.",
                "499": "    # setup.py/versioneer.py will grep for the variable names, so they must",
                "500": "    # each be defined on a line of their own. _version.py will just call",
                "501": "    # get_keywords().",
                "515": "    # these strings are filled in when 'setup.py versioneer' creates",
                "516": "    # _version.py",
                "535": "def register_vcs_handler(vcs, method):  # decorator",
                "554": "        # This hides the console window if pythonw.exe is used",
                "562": "            # remember shell=False, so use git.cmd on windows, not just git",
                "605": "        root = os.path.dirname(root)  # up a level",
                "616": "    # the code embedded in _version.py can just fetch the value of these",
                "617": "    # keywords. When used from setup.py, we don't want to import _version.py,",
                "618": "    # so we do it with a regexp instead. This function is not used from",
                "619": "    # _version.py.",
                "648": "        # Use only the last line.  Previous lines may contain GPG signature",
                "649": "        # information.",
                "652": "        # git-2.2.0 added \"%%cI\", which expands to an ISO-8601 -compliant",
                "653": "        # datestamp. However we prefer \"%%ci\" (which expands to an \"ISO-8601",
                "654": "        # -like\" string, which we must then edit to make compliant), because",
                "655": "        # it's been around since git-1.5.3, and it's too difficult to",
                "656": "        # discover which version we're using, or to work around using an",
                "657": "        # older one.",
                "665": "    # starting in git-1.8.3, tags are listed as \"tag: foo-1.0\" instead of",
                "666": "    # just \"foo-1.0\". If we see a \"tag: \" prefix, prefer those.",
                "670": "        # Either we're using git < 1.8.3, or there really are no tags. We use",
                "671": "        # a heuristic: assume all version tags have a digit. The old git %%d",
                "672": "        # expansion behaves like git log --decorate=short and strips out the",
                "673": "        # refs/heads/ and refs/tags/ prefixes that would let us distinguish",
                "674": "        # between branches and tags. By ignoring refnames without digits, we",
                "675": "        # filter out many common branch names like \"release\" and",
                "676": "        # \"stabilization\", as well as \"HEAD\" and \"master\".",
                "683": "        # sorting will prefer e.g. \"2.0\" over \"2.0rc1\"",
                "686": "            # Filter out refs that exactly match prefix or that don't start",
                "687": "            # with a number once the prefix is stripped (mostly a concern",
                "688": "            # when prefix is '')",
                "697": "    # no suitable tags, so version is \"0+unknown\", but full hex is still there",
                "717": "    # GIT_DIR can interfere with correct operation of Versioneer.",
                "718": "    # It may be intended to be passed to the Versioneer-versioned project,",
                "719": "    # but that should not change where we get our version from.",
                "731": "    # if there is a tag matching tag_prefix, this yields TAG-NUM-gHEX[-dirty]",
                "732": "    # if there isn't one, this yields HEX[-dirty] (no NUM)",
                "737": "    # --long was added in git-1.5.5",
                "748": "    pieces[\"short\"] = full_out[:7]  # maybe improved later",
                "753": "    # --abbrev-ref was added in git-1.6.3",
                "759": "        # If we aren't exactly on a branch, pick a branch which represents",
                "760": "        # the current commit. If all else fails, we are on a branchless",
                "761": "        # commit.",
                "763": "        # --contains was added in git-1.5.4",
                "768": "        # Remove the first line if we're running detached",
                "772": "        # Strip off the leading \"* \" from the list of branches.",
                "779": "            # Pick the first branch that is returned. Good or bad.",
                "784": "    # parse describe_out. It will be like TAG-NUM-gHEX[-dirty] or HEX[-dirty]",
                "785": "    # TAG might have hyphens.",
                "788": "    # look for -dirty suffix",
                "794": "    # now we have TAG-NUM-gHEX or HEX",
                "797": "        # TAG-NUM-gHEX",
                "800": "            # unparsable. Maybe git-describe is misbehaving?",
                "805": "        # tag",
                "816": "        # distance: number of commits since tag",
                "819": "        # commit: short hex revision ID",
                "823": "        # HEX: no tags",
                "826": "        pieces[\"distance\"] = len(out.split())  # total number of commits",
                "828": "    # commit date: see ISO-8601 comment in git_versions_from_keywords()",
                "830": "    # Use only the last line.  Previous lines may contain GPG signature",
                "831": "    # information.",
                "862": "        # exception #1",
                "889": "        # exception #1",
                "918": "            # update the post release segment",
                "926": "            # no commits, use the tag as the version",
                "929": "        # exception #1",
                "953": "        # exception #1",
                "980": "        # exception #1",
                "1005": "        # exception #1",
                "1025": "        # exception #1",
                "1045": "        # exception #1",
                "1062": "        style = \"pep440\"  # the default",
                "1090": "    # I am in _version.py, which lives at ROOT/VERSIONFILE_SOURCE. If we have",
                "1091": "    # __file__, we can work backwards from there to the root. Some",
                "1092": "    # py2exe/bbfreeze/non-CPython implementations don't do __file__, in which",
                "1093": "    # case we can only use expanded keywords.",
                "1106": "        # versionfile_source is the relative path from the top of the source",
                "1107": "        # tree (where the .git directory might live) to this file. Invert",
                "1108": "        # this to find the root from __file__.",
                "1138": "    # the code embedded in _version.py can just fetch the value of these",
                "1139": "    # keywords. When used from setup.py, we don't want to import _version.py,",
                "1140": "    # so we do it with a regexp instead. This function is not used from",
                "1141": "    # _version.py.",
                "1170": "        # Use only the last line.  Previous lines may contain GPG signature",
                "1171": "        # information.",
                "1174": "        # git-2.2.0 added \"%cI\", which expands to an ISO-8601 -compliant",
                "1175": "        # datestamp. However we prefer \"%ci\" (which expands to an \"ISO-8601",
                "1176": "        # -like\" string, which we must then edit to make compliant), because",
                "1177": "        # it's been around since git-1.5.3, and it's too difficult to",
                "1178": "        # discover which version we're using, or to work around using an",
                "1179": "        # older one.",
                "1187": "    # starting in git-1.8.3, tags are listed as \"tag: foo-1.0\" instead of",
                "1188": "    # just \"foo-1.0\". If we see a \"tag: \" prefix, prefer those.",
                "1192": "        # Either we're using git < 1.8.3, or there really are no tags. We use",
                "1193": "        # a heuristic: assume all version tags have a digit. The old git %d",
                "1194": "        # expansion behaves like git log --decorate=short and strips out the",
                "1195": "        # refs/heads/ and refs/tags/ prefixes that would let us distinguish",
                "1196": "        # between branches and tags. By ignoring refnames without digits, we",
                "1197": "        # filter out many common branch names like \"release\" and",
                "1198": "        # \"stabilization\", as well as \"HEAD\" and \"master\".",
                "1205": "        # sorting will prefer e.g. \"2.0\" over \"2.0rc1\"",
                "1208": "            # Filter out refs that exactly match prefix or that don't start",
                "1209": "            # with a number once the prefix is stripped (mostly a concern",
                "1210": "            # when prefix is '')",
                "1219": "    # no suitable tags, so version is \"0+unknown\", but full hex is still there",
                "1239": "    # GIT_DIR can interfere with correct operation of Versioneer.",
                "1240": "    # It may be intended to be passed to the Versioneer-versioned project,",
                "1241": "    # but that should not change where we get our version from.",
                "1253": "    # if there is a tag matching tag_prefix, this yields TAG-NUM-gHEX[-dirty]",
                "1254": "    # if there isn't one, this yields HEX[-dirty] (no NUM)",
                "1259": "    # --long was added in git-1.5.5",
                "1270": "    pieces[\"short\"] = full_out[:7]  # maybe improved later",
                "1275": "    # --abbrev-ref was added in git-1.6.3",
                "1281": "        # If we aren't exactly on a branch, pick a branch which represents",
                "1282": "        # the current commit. If all else fails, we are on a branchless",
                "1283": "        # commit.",
                "1285": "        # --contains was added in git-1.5.4",
                "1290": "        # Remove the first line if we're running detached",
                "1294": "        # Strip off the leading \"* \" from the list of branches.",
                "1301": "            # Pick the first branch that is returned. Good or bad.",
                "1306": "    # parse describe_out. It will be like TAG-NUM-gHEX[-dirty] or HEX[-dirty]",
                "1307": "    # TAG might have hyphens.",
                "1310": "    # look for -dirty suffix",
                "1316": "    # now we have TAG-NUM-gHEX or HEX",
                "1319": "        # TAG-NUM-gHEX",
                "1322": "            # unparsable. Maybe git-describe is misbehaving?",
                "1327": "        # tag",
                "1338": "        # distance: number of commits since tag",
                "1341": "        # commit: short hex revision ID",
                "1345": "        # HEX: no tags",
                "1348": "        pieces[\"distance\"] = len(out.split())  # total number of commits",
                "1350": "    # commit date: see ISO-8601 comment in git_versions_from_keywords()",
                "1352": "    # Use only the last line.  Previous lines may contain GPG signature",
                "1353": "    # information.",
                "1414": "        root = os.path.dirname(root)  # up a level",
                "1423": "# This file was generated by 'versioneer.py' (0.28) from",
                "1424": "# revision-control system data, or from the parent directory name of an",
                "1425": "# unpacked source archive. Distribution tarballs contain a pre-generated copy",
                "1426": "# of this file.",
                "1432": "'''  # END VERSION_JSON",
                "1447": "    mo = re.search(r\"version_json = '''\\n(.*)'''  # END VERSION_JSON\",",
                "1450": "        mo = re.search(r\"version_json = '''\\r\\n(.*)'''  # END VERSION_JSON\",",
                "1492": "        # exception #1",
                "1519": "        # exception #1",
                "1548": "            # update the post release segment",
                "1556": "            # no commits, use the tag as the version",
                "1559": "        # exception #1",
                "1583": "        # exception #1",
                "1610": "        # exception #1",
                "1635": "        # exception #1",
                "1655": "        # exception #1",
                "1675": "        # exception #1",
                "1692": "        style = \"pep440\"  # the default",
                "1728": "        # see the discussion in cmdclass.py:get_cmdclass()",
                "1744": "    # extract version from first of: _version.py, VCS command (e.g. 'git",
                "1745": "    # describe'), parentdir. This is meant to work for developers using a",
                "1746": "    # source checkout, for users of a tarball created by 'setup.py sdist',",
                "1747": "    # and for users of a tarball/zipball created by 'git archive' or github's",
                "1748": "    # download-from-tag feature or the equivalent in other VCSes.",
                "1811": "        # this fixes the \"python setup.py develop\" case (also 'install' and",
                "1812": "        # 'easy_install .'), in which subdependencies of the main project are",
                "1813": "        # built (using setup.py bdist_egg) in the same python process. Assume",
                "1814": "        # a main project A and a dependency B, which use different versions",
                "1815": "        # of Versioneer. A's setup.py imports A's Versioneer, leaving it in",
                "1816": "        # sys.modules by the time B's setup.py is executed, causing B to run",
                "1817": "        # with the wrong versioneer. Setuptools wraps the sub-dep builds in a",
                "1818": "        # sandbox that restores sys.modules to it's pre-build state, so the",
                "1819": "        # parent is protected against the child's \"import versioneer\". By",
                "1820": "        # removing ourselves from sys.modules here, before the child build",
                "1821": "        # happens, we protect the child from the parent's versioneer too.",
                "1822": "        # Also see https://github.com/python-versioneer/python-versioneer/issues/52",
                "1826": "    # we add \"version\" to setuptools",
                "1850": "    # we override \"build_py\" in setuptools",
                "1851": "    #",
                "1852": "    # most invocation pathways end up running build_py:",
                "1853": "    #  distutils/build -> build_py",
                "1854": "    #  distutils/install -> distutils/build ->..",
                "1855": "    #  setuptools/bdist_wheel -> distutils/install ->..",
                "1856": "    #  setuptools/bdist_egg -> distutils/install_lib -> build_py",
                "1857": "    #  setuptools/install -> bdist_egg ->..",
                "1858": "    #  setuptools/develop -> ?",
                "1859": "    #  pip install:",
                "1860": "    #   copies source tree to a tempdir before running egg_info/etc",
                "1861": "    #   if .git isn't copied too, 'git describe' will fail",
                "1862": "    #   then does setup.py bdist_wheel, or sometimes setup.py install",
                "1863": "    #  setup.py egg_info -> ?",
                "1865": "    # pip install -e . and setuptool/editable_wheel will invoke build_py",
                "1866": "    # but the build_py command is not expected to copy any files.",
                "1868": "    # we override different \"build_py\" commands for both environments",
                "1881": "                # During editable installs `.py` and data files are",
                "1882": "                # not copied to build_lib",
                "1884": "            # now locate _version.py in the new build/ directory and replace",
                "1885": "            # it with an updated value",
                "1905": "                # build_ext --inplace will only build extensions in",
                "1906": "                # build/lib<..> dir with no _version.py to write to.",
                "1907": "                # As in place builds will already have a _version.py",
                "1908": "                # in the module dir, we do not need to write one.",
                "1910": "            # now locate _version.py in the new build/ directory and replace",
                "1911": "            # it with an updated value",
                "1925": "    if \"cx_Freeze\" in sys.modules:  # cx_freeze enabled?",
                "1927": "        # nczeczulin reports that py2exe won't like the pep440-style string",
                "1928": "        # as FILEVERSION, but it can be used for PRODUCTVERSION, e.g.",
                "1929": "        # setup(console=[{",
                "1930": "        #   \"version\": versioneer.get_version().split(\"+\", 1)[0], # FILEVERSION",
                "1931": "        #   \"product_version\": versioneer.get_version(),",
                "1932": "        #   ...",
                "1957": "    if 'py2exe' in sys.modules:  # py2exe enabled?",
                "1985": "    # sdist farms its file list building out to egg_info",
                "1993": "            # egg_info.find_sources builds the manifest list and writes it",
                "1994": "            # in one shot",
                "1997": "            # Modify the filelist and normalize it",
                "2002": "                # There are rare cases where versionfile_source might not be",
                "2003": "                # included by default, so we must be explicit",
                "2008": "            # The write method is hidden in the manifest_maker instance that",
                "2009": "            # generated the filelist and was thrown away",
                "2010": "            # We will instead replicate their final normalization (to unicode,",
                "2011": "            # and POSIX-style paths)",
                "2022": "    # we override different \"sdist\" commands for both environments",
                "2032": "            # unless we update this, the command will keep using the old",
                "2033": "            # version",
                "2041": "            # now locate _version.py in the new base_dir directory",
                "2042": "            # (remembering that it may be a hardlink) and replace it with an",
                "2043": "            # updated value",
                "2076": "# See the docstring in versioneer.py for instructions. Note that you must",
                "2077": "# re-run 'versioneer.py setup' after changing this section, and commit the",
                "2078": "# resulting files.",
                "2081": "#VCS = git",
                "2082": "#style = pep440",
                "2083": "#versionfile_source =",
                "2084": "#versionfile_build =",
                "2085": "#tag_prefix =",
                "2086": "#parentdir_prefix =",
                "2151": "    # Make VCS-specific changes. For git, this means creating/changing",
                "2152": "    # .gitattributes to mark _version.py for export-subst keyword",
                "2153": "    # substitution."
            },
            "comment_modified_diff": {}
        }
    ],
    "lint_diff.ini": [],
    "config.py": [
        {
            "commit": "7da70ce023d9e5b1dcd3f71a782f12c080b1c590",
            "timestamp": "2022-10-18T10:03:00+02:00",
            "author": "Dimitri Papadopoulos",
            "commit_message": "DOC: remove reference to Python 2\n\nRemove reference to Visual Studio version required by old versions\nof Python. Python >= 3.5 is built with Microsoft Visual C++ 14.0 /\nVisual Studio 2015:\n\thttps://wiki.python.org/moin/WindowsCompilers",
            "additions": 2,
            "deletions": 3,
            "change_type": "MODIFY",
            "diff": "@@ -46,7 +46,7 @@ def _check_compiler (self):\n             # XXX: hack to circumvent a python 2.6 bug with msvc9compiler:\n             # initialize call query_vcvarsall, which throws an IOError, and\n             # causes an error along the way without much information. We try to\n-            # catch it here, hoping it is early enough, and print an helpful\n+            # catch it here, hoping it is early enough, and print a helpful\n             # message instead of Error: None.\n             if not self.compiler.initialized:\n                 try:\n@@ -56,8 +56,7 @@ def _check_compiler (self):\n                         Could not initialize compiler instance: do you have Visual Studio\n                         installed?  If you are trying to build with MinGW, please use \"python setup.py\n                         build -c mingw32\" instead.  If you have Visual Studio installed, check it is\n-                        correctly installed, and the right version (VS 2008 for python 2.6, 2.7 and 3.2,\n-                        VS 2010 for >= 3.3).\n+                        correctly installed, and the right version (VS 2015 as of this writing).\n \n                         Original exception was: %s, and the Compiler class was %s\n                         ============================================================================\"\"\") \\\n",
            "comment_added_diff": {
                "49": "            # catch it here, hoping it is early enough, and print a helpful"
            },
            "comment_deleted_diff": {
                "49": "            # catch it here, hoping it is early enough, and print an helpful"
            },
            "comment_modified_diff": {
                "49": "            # catch it here, hoping it is early enough, and print an helpful"
            }
        },
        {
            "commit": "a5718226cf0a7666624a06d8eff3da2b207580f6",
            "timestamp": "2023-05-28T22:33:39+02:00",
            "author": "Dimitri Papadopoulos",
            "commit_message": "MAINT: IOError \u2192 OSError\n\nIOError has been merged into OSError in Python 3.3.",
            "additions": 2,
            "deletions": 2,
            "change_type": "MODIFY",
            "diff": "@@ -44,14 +44,14 @@ def _check_compiler (self):\n         if sys.platform == 'win32' and (self.compiler.compiler_type in\n                                         ('msvc', 'intelw', 'intelemw')):\n             # XXX: hack to circumvent a python 2.6 bug with msvc9compiler:\n-            # initialize call query_vcvarsall, which throws an IOError, and\n+            # initialize call query_vcvarsall, which throws an OSError, and\n             # causes an error along the way without much information. We try to\n             # catch it here, hoping it is early enough, and print a helpful\n             # message instead of Error: None.\n             if not self.compiler.initialized:\n                 try:\n                     self.compiler.initialize()\n-                except IOError as e:\n+                except OSError as e:\n                     msg = textwrap.dedent(\"\"\"\\\n                         Could not initialize compiler instance: do you have Visual Studio\n                         installed?  If you are trying to build with MinGW, please use \"python setup.py\n",
            "comment_added_diff": {
                "47": "            # initialize call query_vcvarsall, which throws an OSError, and"
            },
            "comment_deleted_diff": {
                "47": "            # initialize call query_vcvarsall, which throws an IOError, and"
            },
            "comment_modified_diff": {
                "47": "            # initialize call query_vcvarsall, which throws an IOError, and"
            }
        }
    ],
    "_common.pxd": [],
    "_common.pyx": [],
    "_generator.pyx": [],
    "mtrand.pyx": [],
    "test_generator_mt19937.py": [
        {
            "commit": "8750d042c9796f84da4118b19b08cd0c3954866b",
            "timestamp": "2022-10-18T17:53:09+02:00",
            "author": "Sebastian Berg",
            "commit_message": "BUG: Fix boundschecking for `random.logseries`\n\nLogseries previously did not enforce bounds to be strictly exclusive\nfor the upper bound, where it leads to incorrect behavior.\n\nThe NOT_NAN check is removed, since it was never used: The current bounded\nversion always excludes NaNs.",
            "additions": 16,
            "deletions": 4,
            "change_type": "MODIFY",
            "diff": "@@ -1363,10 +1363,22 @@ def test_logseries(self):\n                             [5, 1]])\n         assert_array_equal(actual, desired)\n \n-    def test_logseries_exceptions(self):\n-        with np.errstate(invalid='ignore'):\n-            assert_raises(ValueError, random.logseries, np.nan)\n-            assert_raises(ValueError, random.logseries, [np.nan] * 10)\n+    def test_logseries_zero(self):\n+        random = Generator(MT19937(self.seed))\n+        assert random.logseries(0) == 1\n+\n+    @pytest.mark.parametrize(\"value\", [np.nextafter(0., -1), 1., np.nan, 5.])\n+    def test_logseries_exceptions(self, value):\n+        random = Generator(MT19937(self.seed))\n+        with np.errstate(invalid=\"ignore\"):\n+            with pytest.raises(ValueError):\n+                random.logseries(value)\n+            with pytest.raises(ValueError):\n+                # contiguous path:\n+                random.logseries(np.array([value] * 10))\n+            with pytest.raises(ValueError):\n+                # non-contiguous path:\n+                random.logseries(np.array([value] * 10)[::2])\n \n     def test_multinomial(self):\n         random = Generator(MT19937(self.seed))\n",
            "comment_added_diff": {
                "1377": "                # contiguous path:",
                "1380": "                # non-contiguous path:"
            },
            "comment_deleted_diff": {},
            "comment_modified_diff": {}
        },
        {
            "commit": "c27678e73549824046dc699e053c3ac89304033e",
            "timestamp": "2023-07-21T13:42:17-04:00",
            "author": "warren",
            "commit_message": "BUG: random: Fix generation of nan by dirichlet.\n\nDon't call the C function random_beta() with both parameters\n`a` and `b` set to 0.  In the case where this would occur, we\nknow that the remaining values in the random vector being\ngenerated must be 0, so can break out of the loop early.\n\nAfter this change, when alpha is all zero, the random variates\nwill also be all zero.\n\nCloses gh-24210.",
            "additions": 22,
            "deletions": 2,
            "change_type": "MODIFY",
            "diff": "@@ -35,6 +35,7 @@\n     },\n ]\n \n+\n @pytest.fixture(scope='module', params=[True, False])\n def endpoint(request):\n     return request.param\n@@ -145,6 +146,7 @@ def test_multinomial_pvals_float32(self):\n         with pytest.raises(ValueError, match=match):\n             random.multinomial(1, pvals)\n \n+\n class TestMultivariateHypergeometric:\n \n     def setup_method(self):\n@@ -1238,6 +1240,25 @@ def test_dirichlet_moderately_small_alpha(self):\n         sample_mean = sample.mean(axis=0)\n         assert_allclose(sample_mean, exact_mean, rtol=1e-3)\n \n+    # This set of parameters includes inputs with alpha.max() >= 0.1 and\n+    # alpha.max() < 0.1 to exercise both generation methods within the\n+    # dirichlet code.\n+    @pytest.mark.parametrize(\n+        'alpha',\n+        [[5, 9, 0, 8],\n+         [0.5, 0, 0, 0],\n+         [1, 5, 0, 0, 1.5, 0, 0, 0],\n+         [0.01, 0.03, 0, 0.005],\n+         [1e-5, 0, 0, 0],\n+         [0.002, 0.015, 0, 0, 0.04, 0, 0, 0],\n+         [0.0],\n+         [0, 0, 0]],\n+    )\n+    def test_dirichlet_multiple_zeros_in_alpha(self, alpha):\n+        alpha = np.array(alpha)\n+        y = random.dirichlet(alpha)\n+        assert_equal(y[alpha == 0], 0.0)\n+\n     def test_exponential(self):\n         random = Generator(MT19937(self.seed))\n         actual = random.exponential(1.1234, size=(3, 2))\n@@ -1467,7 +1488,7 @@ def test_multivariate_normal(self, method):\n                       mu, np.empty((3, 2)))\n         assert_raises(ValueError, random.multivariate_normal,\n                       mu, np.eye(3))\n-        \n+\n     @pytest.mark.parametrize('mean, cov', [([0], [[1+1j]]), ([0j], [[1]])])\n     def test_multivariate_normal_disallow_complex(self, mean, cov):\n         random = Generator(MT19937(self.seed))\n@@ -1847,7 +1868,6 @@ class TestBroadcast:\n     def setup_method(self):\n         self.seed = 123456789\n \n-\n     def test_uniform(self):\n         random = Generator(MT19937(self.seed))\n         low = [0]\n",
            "comment_added_diff": {
                "1243": "    # This set of parameters includes inputs with alpha.max() >= 0.1 and",
                "1244": "    # alpha.max() < 0.1 to exercise both generation methods within the",
                "1245": "    # dirichlet code."
            },
            "comment_deleted_diff": {},
            "comment_modified_diff": {}
        }
    ],
    "test_randomstate.py": [
        {
            "commit": "8750d042c9796f84da4118b19b08cd0c3954866b",
            "timestamp": "2022-10-18T17:53:09+02:00",
            "author": "Sebastian Berg",
            "commit_message": "BUG: Fix boundschecking for `random.logseries`\n\nLogseries previously did not enforce bounds to be strictly exclusive\nfor the upper bound, where it leads to incorrect behavior.\n\nThe NOT_NAN check is removed, since it was never used: The current bounded\nversion always excludes NaNs.",
            "additions": 14,
            "deletions": 5,
            "change_type": "MODIFY",
            "diff": "@@ -950,11 +950,20 @@ def test_logseries(self):\n                             [3, 6]])\n         assert_array_equal(actual, desired)\n \n-    def test_logseries_exceptions(self):\n-        with suppress_warnings() as sup:\n-            sup.record(RuntimeWarning)\n-            assert_raises(ValueError, random.logseries, np.nan)\n-            assert_raises(ValueError, random.logseries, [np.nan] * 10)\n+    def test_logseries_zero(self):\n+        assert random.logseries(0) == 1\n+\n+    @pytest.mark.parametrize(\"value\", [np.nextafter(0., -1), 1., np.nan, 5.])\n+    def test_logseries_exceptions(self, value):\n+        with np.errstate(invalid=\"ignore\"):\n+            with pytest.raises(ValueError):\n+                random.logseries(value)\n+            with pytest.raises(ValueError):\n+                # contiguous path:\n+                random.logseries(np.array([value] * 10))\n+            with pytest.raises(ValueError):\n+                # non-contiguous path:\n+                random.logseries(np.array([value] * 10)[::2])\n \n     def test_multinomial(self):\n         random.seed(self.seed)\n",
            "comment_added_diff": {
                "962": "                # contiguous path:",
                "965": "                # non-contiguous path:"
            },
            "comment_deleted_diff": {},
            "comment_modified_diff": {}
        }
    ],
    "recfunctions.py": [
        {
            "commit": "a8449b52163f871b75a4873a9543db6612d38ccf",
            "timestamp": "2022-10-19T17:07:39+02:00",
            "author": "LeonaTaric",
            "commit_message": "ENH: unstructured_to_structured converts dtype argument  (#22442)\n\nBefore:\r\n\r\n>>> field = unstructured_to_structured(np.zeros((20, 2)), dtype=[('x', float), ('y', float)]) # failed\r\n>>> field = unstructured_to_structured(np.zeros((20, 2)), dtype=np.dtype([('x', float), ('y', float)])) # success\r\nAfter:\r\n\r\n>>> field = unstructured_to_structured(np.zeros((20, 2)), dtype=[('x', float), ('y', float)]) # success\r\n>>> field = unstructured_to_structured(np.zeros((20, 2)), dtype=np.dtype([('x', float), ('y', float)])) # success\r\n\r\nCloses gh-22428",
            "additions": 2,
            "deletions": 0,
            "change_type": "MODIFY",
            "diff": "@@ -1057,6 +1057,8 @@ def unstructured_to_structured(arr, dtype=None, names=None, align=False,\n     else:\n         if names is not None:\n             raise ValueError(\"don't supply both dtype and names\")\n+        # if dtype is the args of np.dtype, construct it\n+        dtype = np.dtype(dtype)\n         # sanity check of the input dtype\n         fields = _get_fields_and_offsets(dtype)\n         if len(fields) == 0:\n",
            "comment_added_diff": {
                "1060": "        # if dtype is the args of np.dtype, construct it"
            },
            "comment_deleted_diff": {},
            "comment_modified_diff": {}
        },
        {
            "commit": "549756836095efed97ee73c82bb623674bb07df4",
            "timestamp": "2023-04-24T11:14:20+02:00",
            "author": "Philip Holzmann",
            "commit_message": "ENH: structured_to_unstructured: view more often\n\nConverting with structured_to_unstructured() now returns a view, if all\nthe fields have a constant stride.",
            "additions": 71,
            "deletions": 1,
            "change_type": "MODIFY",
            "diff": "@@ -885,6 +885,52 @@ def count_elem(dt):\n                     fields.extend([(d, c, o + i*size) for d, c, o in subfields])\n     return fields\n \n+def _common_stride(offsets, counts, itemsize):\n+    \"\"\"\n+    Returns the stride between the fields, or None if the stride is not\n+    constant. The values in \"counts\" designate the lengths of\n+    sub-arrays. Sub-arrays are treated as many contiguous fields, with\n+    always positive stride.\n+    \"\"\"\n+\n+    if len(offsets) <= 1:\n+        return itemsize\n+\n+    negative = offsets[1] < offsets[0]  # negative stride\n+    if negative:\n+        # reverse, so offsets will be ascending\n+        it = zip(reversed(offsets), reversed(counts))\n+    else:\n+        it = zip(offsets, counts)\n+\n+    prev_offset = None\n+    stride = None\n+    for offset, count in it:\n+        if count != 1:  # sub array: always c-contiguous\n+            if negative:\n+                return None  # sub-arrays can never have a negative stride\n+            if stride is None:\n+                stride = itemsize\n+            if stride != itemsize:\n+                return None\n+            end_offset = offset + (count - 1) * itemsize\n+        else:\n+            end_offset = offset\n+\n+        if prev_offset is not None:\n+            new_stride = offset - prev_offset\n+            if stride is None:\n+                stride = new_stride\n+            if stride != new_stride:\n+                return None\n+\n+        prev_offset = end_offset\n+\n+    if stride is not None:\n+        if negative:\n+            return -stride\n+        return stride\n+\n \n def _structured_to_unstructured_dispatcher(arr, dtype=None, copy=None,\n                                            casting=None):\n@@ -960,7 +1006,7 @@ def structured_to_unstructured(arr, dtype=None, copy=False, casting='unsafe'):\n     if dtype is None:\n         out_dtype = np.result_type(*[dt.base for dt in dts])\n     else:\n-        out_dtype = dtype\n+        out_dtype = np.dtype(dtype)\n \n     # Use a series of views and casts to convert to an unstructured array:\n \n@@ -972,6 +1018,30 @@ def structured_to_unstructured(arr, dtype=None, copy=False, casting='unsafe'):\n                                  'itemsize': arr.dtype.itemsize})\n     arr = arr.view(flattened_fields)\n \n+    if (not copy) and all(dt.base == out_dtype for dt in dts):\n+        # all elements have the right dtype already; if they have a common\n+        # stride, we can just return a view\n+        common_stride = _common_stride(offsets, counts, out_dtype.itemsize)\n+        if common_stride is not None:\n+            # ensure that we have a real ndarray; other types (e.g. matrix)\n+            # have strange slicing behavior\n+            arr = arr.view(type=np.ndarray)\n+            new_shape = arr.shape + (sum(counts), out_dtype.itemsize)\n+            new_strides = arr.strides + (abs(common_stride), 1)\n+\n+            arr = arr[..., None].view(np.uint8)  # view as bytes\n+            arr = arr[..., min(offsets):]  # remove the leading unused data\n+            arr = np.lib.stride_tricks.as_strided(arr,\n+                                                  new_shape,\n+                                                  new_strides)\n+\n+            # cast and drop the last dimension again\n+            arr = arr.view(out_dtype)[..., 0]\n+\n+            if common_stride < 0:\n+                arr = arr[..., ::-1]  # reverse, if the stride was negative\n+            return arr\n+\n     # next cast to a packed format with all fields converted to new dtype\n     packed_fields = np.dtype({'names': names,\n                               'formats': [(out_dtype, dt.shape) for dt in dts]})\n",
            "comment_added_diff": {
                "899": "    negative = offsets[1] < offsets[0]  # negative stride",
                "901": "        # reverse, so offsets will be ascending",
                "909": "        if count != 1:  # sub array: always c-contiguous",
                "911": "                return None  # sub-arrays can never have a negative stride",
                "1022": "        # all elements have the right dtype already; if they have a common",
                "1023": "        # stride, we can just return a view",
                "1026": "            # ensure that we have a real ndarray; other types (e.g. matrix)",
                "1027": "            # have strange slicing behavior",
                "1032": "            arr = arr[..., None].view(np.uint8)  # view as bytes",
                "1033": "            arr = arr[..., min(offsets):]  # remove the leading unused data",
                "1038": "            # cast and drop the last dimension again",
                "1042": "                arr = arr[..., ::-1]  # reverse, if the stride was negative"
            },
            "comment_deleted_diff": {},
            "comment_modified_diff": {}
        },
        {
            "commit": "326f3bb043dea8369c2f52d31ae548df957cb0fc",
            "timestamp": "2023-04-28T15:22:15+02:00",
            "author": "Philip Holzmann",
            "commit_message": "only unstructure a few array types",
            "additions": 8,
            "deletions": 5,
            "change_type": "MODIFY",
            "diff": "@@ -1018,14 +1018,16 @@ def structured_to_unstructured(arr, dtype=None, copy=False, casting='unsafe'):\n                                  'itemsize': arr.dtype.itemsize})\n     arr = arr.view(flattened_fields)\n \n-    if (not copy) and all(dt.base == out_dtype for dt in dts):\n+\n+    # we only allow a few types to be unstructured by manipulating the\n+    # strides, because we know it won't work with, for example, np.matrix nor\n+    # np.ma.MaskedArray.\n+    can_view = type(arr) in (np.ndarray, np.recarray, np.memmap)\n+    if (not copy) and can_view and all(dt.base == out_dtype for dt in dts):\n         # all elements have the right dtype already; if they have a common\n         # stride, we can just return a view\n         common_stride = _common_stride(offsets, counts, out_dtype.itemsize)\n         if common_stride is not None:\n-            # ensure that we have a real ndarray; other types (e.g. matrix)\n-            # have strange slicing behavior\n-            arr = arr.view(type=np.ndarray)\n             new_shape = arr.shape + (sum(counts), out_dtype.itemsize)\n             new_strides = arr.strides + (abs(common_stride), 1)\n \n@@ -1033,7 +1035,8 @@ def structured_to_unstructured(arr, dtype=None, copy=False, casting='unsafe'):\n             arr = arr[..., min(offsets):]  # remove the leading unused data\n             arr = np.lib.stride_tricks.as_strided(arr,\n                                                   new_shape,\n-                                                  new_strides)\n+                                                  new_strides,\n+                                                  subok=True)\n \n             # cast and drop the last dimension again\n             arr = arr.view(out_dtype)[..., 0]\n",
            "comment_added_diff": {
                "1022": "    # we only allow a few types to be unstructured by manipulating the",
                "1023": "    # strides, because we know it won't work with, for example, np.matrix nor",
                "1024": "    # np.ma.MaskedArray."
            },
            "comment_deleted_diff": {
                "1026": "            # ensure that we have a real ndarray; other types (e.g. matrix)",
                "1027": "            # have strange slicing behavior"
            },
            "comment_modified_diff": {}
        },
        {
            "commit": "a3040219b453943a3c2a08a12ad351888dc17250",
            "timestamp": "2023-04-28T15:23:36+02:00",
            "author": "Philip Holzmann",
            "commit_message": "use np.newaxis instead of None",
            "additions": 1,
            "deletions": 1,
            "change_type": "MODIFY",
            "diff": "@@ -1031,7 +1031,7 @@ def structured_to_unstructured(arr, dtype=None, copy=False, casting='unsafe'):\n             new_shape = arr.shape + (sum(counts), out_dtype.itemsize)\n             new_strides = arr.strides + (abs(common_stride), 1)\n \n-            arr = arr[..., None].view(np.uint8)  # view as bytes\n+            arr = arr[..., np.newaxis].view(np.uint8)  # view as bytes\n             arr = arr[..., min(offsets):]  # remove the leading unused data\n             arr = np.lib.stride_tricks.as_strided(arr,\n                                                   new_shape,\n",
            "comment_added_diff": {
                "1034": "            arr = arr[..., np.newaxis].view(np.uint8)  # view as bytes"
            },
            "comment_deleted_diff": {
                "1034": "            arr = arr[..., None].view(np.uint8)  # view as bytes"
            },
            "comment_modified_diff": {
                "1034": "            arr = arr[..., None].view(np.uint8)  # view as bytes"
            }
        },
        {
            "commit": "6501fc9ec65e93b583e92566f9e47028722795bb",
            "timestamp": "2023-05-02T10:46:47+02:00",
            "author": "Philip Holzmann",
            "commit_message": "whitespace. remove redundant if",
            "additions": 6,
            "deletions": 8,
            "change_type": "MODIFY",
            "diff": "@@ -889,10 +889,9 @@ def _common_stride(offsets, counts, itemsize):\n     \"\"\"\n     Returns the stride between the fields, or None if the stride is not\n     constant. The values in \"counts\" designate the lengths of\n-    sub-arrays. Sub-arrays are treated as many contiguous fields, with\n+    subarrays. Subarrays are treated as many contiguous fields, with\n     always positive stride.\n     \"\"\"\n-\n     if len(offsets) <= 1:\n         return itemsize\n \n@@ -906,9 +905,9 @@ def _common_stride(offsets, counts, itemsize):\n     prev_offset = None\n     stride = None\n     for offset, count in it:\n-        if count != 1:  # sub array: always c-contiguous\n+        if count != 1:  # subarray: always c-contiguous\n             if negative:\n-                return None  # sub-arrays can never have a negative stride\n+                return None  # subarrays can never have a negative stride\n             if stride is None:\n                 stride = itemsize\n             if stride != itemsize:\n@@ -926,10 +925,9 @@ def _common_stride(offsets, counts, itemsize):\n \n         prev_offset = end_offset\n \n-    if stride is not None:\n-        if negative:\n-            return -stride\n-        return stride\n+    if negative:\n+        return -stride\n+    return stride\n \n \n def _structured_to_unstructured_dispatcher(arr, dtype=None, copy=None,\n",
            "comment_added_diff": {
                "908": "        if count != 1:  # subarray: always c-contiguous",
                "910": "                return None  # subarrays can never have a negative stride"
            },
            "comment_deleted_diff": {
                "909": "        if count != 1:  # sub array: always c-contiguous",
                "911": "                return None  # sub-arrays can never have a negative stride"
            },
            "comment_modified_diff": {}
        },
        {
            "commit": "3fa629528014aa6917ae12c3dee46c3c666be2a8",
            "timestamp": "2023-05-02T16:07:31+02:00",
            "author": "Philip Holzmann",
            "commit_message": "add note for the wrap call",
            "additions": 3,
            "deletions": 0,
            "change_type": "MODIFY",
            "diff": "@@ -1043,6 +1043,9 @@ def structured_to_unstructured(arr, dtype=None, copy=False, casting='unsafe'):\n             if common_stride < 0:\n                 arr = arr[..., ::-1]  # reverse, if the stride was negative\n             if type(arr) is not type(wrap.__self__):\n+                # Some types (e.g. recarray) turn into an ndarray along the\n+                # way, so we have to wrap it again in order to match the\n+                # behavior with copy=True.\n                 arr = wrap(arr)\n             return arr\n \n",
            "comment_added_diff": {
                "1046": "                # Some types (e.g. recarray) turn into an ndarray along the",
                "1047": "                # way, so we have to wrap it again in order to match the",
                "1048": "                # behavior with copy=True."
            },
            "comment_deleted_diff": {},
            "comment_modified_diff": {}
        }
    ],
    "test_recfunctions.py": [
        {
            "commit": "a8449b52163f871b75a4873a9543db6612d38ccf",
            "timestamp": "2022-10-19T17:07:39+02:00",
            "author": "LeonaTaric",
            "commit_message": "ENH: unstructured_to_structured converts dtype argument  (#22442)\n\nBefore:\r\n\r\n>>> field = unstructured_to_structured(np.zeros((20, 2)), dtype=[('x', float), ('y', float)]) # failed\r\n>>> field = unstructured_to_structured(np.zeros((20, 2)), dtype=np.dtype([('x', float), ('y', float)])) # success\r\nAfter:\r\n\r\n>>> field = unstructured_to_structured(np.zeros((20, 2)), dtype=[('x', float), ('y', float)]) # success\r\n>>> field = unstructured_to_structured(np.zeros((20, 2)), dtype=np.dtype([('x', float), ('y', float)])) # success\r\n\r\nCloses gh-22428",
            "additions": 9,
            "deletions": 0,
            "change_type": "MODIFY",
            "diff": "@@ -318,6 +318,15 @@ def inspect(dt, dtype=None):\n         assert_raises(NotImplementedError, unstructured_to_structured,\n                                            np.zeros((3,0), dtype=np.int32))\n \n+    def test_unstructured_to_structured(self):\n+        # test if dtype is the args of np.dtype\n+        a = np.zeros((20, 2))\n+        test_dtype_args = [('x', float), ('y', float)]\n+        test_dtype = np.dtype(test_dtype_args)\n+        field1 = unstructured_to_structured(a, dtype=test_dtype_args)  # now\n+        field2 = unstructured_to_structured(a, dtype=test_dtype)  # before\n+        assert_equal(field1, field2)\n+\n     def test_field_assignment_by_name(self):\n         a = np.ones(2, dtype=[('a', 'i4'), ('b', 'f8'), ('c', 'u1')])\n         newdt = [('b', 'f4'), ('c', 'u1')]\n",
            "comment_added_diff": {
                "322": "        # test if dtype is the args of np.dtype",
                "326": "        field1 = unstructured_to_structured(a, dtype=test_dtype_args)  # now",
                "327": "        field2 = unstructured_to_structured(a, dtype=test_dtype)  # before"
            },
            "comment_deleted_diff": {},
            "comment_modified_diff": {}
        },
        {
            "commit": "549756836095efed97ee73c82bb623674bb07df4",
            "timestamp": "2023-04-24T11:14:20+02:00",
            "author": "Philip Holzmann",
            "commit_message": "ENH: structured_to_unstructured: view more often\n\nConverting with structured_to_unstructured() now returns a view, if all\nthe fields have a constant stride.",
            "additions": 31,
            "deletions": 4,
            "change_type": "MODIFY",
            "diff": "@@ -263,8 +263,13 @@ def test_structured_to_unstructured(self):\n                      dtype=[('x', 'i4'), ('y', 'i4'), ('z', 'i4')])\n         dd = structured_to_unstructured(d)\n         ddd = unstructured_to_structured(dd, d.dtype)\n-        assert_(dd.base is d)\n-        assert_(ddd.base is d)\n+        assert_(np.shares_memory(dd, d))\n+        assert_(np.shares_memory(ddd, d))\n+\n+        # check that reversing the order of attributes works\n+        dd_attrib_rev = structured_to_unstructured(d[['z', 'x']])\n+        assert_equal(dd_attrib_rev, [[5, 1], [7, 4], [11, 7], [12, 10]])\n+        assert_(np.shares_memory(dd_attrib_rev, d))\n \n         # including uniform fields with subarrays unpacked\n         d = np.array([(1, [2,  3], [[ 4,  5], [ 6,  7]]),\n@@ -273,8 +278,30 @@ def test_structured_to_unstructured(self):\n                             ('x2', ('i4', (2, 2)))])\n         dd = structured_to_unstructured(d)\n         ddd = unstructured_to_structured(dd, d.dtype)\n-        assert_(dd.base is d)\n-        assert_(ddd.base is d)\n+        assert_(np.shares_memory(dd, d))\n+        assert_(np.shares_memory(ddd, d))\n+\n+        # check that reversing with sub-arrays works as expected\n+        d_rev = d[::-1]\n+        dd_rev = structured_to_unstructured(d_rev)\n+        assert_equal(dd_rev, [[8, 9, 10, 11, 12, 13, 14],\n+                              [1, 2, 3, 4, 5, 6, 7]])\n+\n+        # check that sub-arrays keep the order of their values\n+        d_attrib_rev = d[['x2', 'x1', 'x0']]\n+        dd_attrib_rev = structured_to_unstructured(d_attrib_rev)\n+        assert_equal(dd_attrib_rev, [[4, 5, 6, 7, 2, 3, 1],\n+                                     [11, 12, 13, 14, 9, 10, 8]])\n+\n+        # with ignored field at the end\n+        d = np.array([(1, [2,  3], [[4, 5], [6, 7]], 32),\n+                      (8, [9, 10], [[11, 12], [13, 14]], 64)],\n+                     dtype=[('x0', 'i4'), ('x1', ('i4', 2)),\n+                            ('x2', ('i4', (2, 2))), ('ignored', 'u1')])\n+        dd = structured_to_unstructured(d[['x0', 'x1', 'x2']])\n+        assert_(np.shares_memory(dd, d))\n+        assert_equal(dd, [[1, 2, 3, 4, 5, 6, 7],\n+                          [8, 9, 10, 11, 12, 13, 14]])\n \n         # test that nested fields with identical names don't break anything\n         point = np.dtype([('x', int), ('y', int)])\n",
            "comment_added_diff": {
                "269": "        # check that reversing the order of attributes works",
                "284": "        # check that reversing with sub-arrays works as expected",
                "290": "        # check that sub-arrays keep the order of their values",
                "296": "        # with ignored field at the end"
            },
            "comment_deleted_diff": {},
            "comment_modified_diff": {}
        },
        {
            "commit": "ed29bda33ae752b781df6ee02a91c1456884d441",
            "timestamp": "2023-05-02T12:04:00+02:00",
            "author": "Philip Holzmann",
            "commit_message": "ensure that ndarray subclasses are passed through",
            "additions": 31,
            "deletions": 1,
            "change_type": "MODIFY",
            "diff": "@@ -228,7 +228,7 @@ def test_repack_fields(self):\n         dt = np.dtype((np.record, dt))\n         assert_(repack_fields(dt).type is np.record)\n \n-    def test_structured_to_unstructured(self):\n+    def test_structured_to_unstructured(self, tmp_path):\n         a = np.zeros(4, dtype=[('a', 'i4'), ('b', 'f4,u2'), ('c', 'f4', 2)])\n         out = structured_to_unstructured(a)\n         assert_equal(out, np.zeros((4,5), dtype='f8'))\n@@ -345,6 +345,36 @@ def inspect(dt, dtype=None):\n         assert_raises(NotImplementedError, unstructured_to_structured,\n                                            np.zeros((3,0), dtype=np.int32))\n \n+\n+        # test supported ndarray subclasses\n+        d_plain = np.array([(1, 2), (3, 4)], dtype=[('a', 'i4'), ('b', 'i4')])\n+        dd_expected = structured_to_unstructured(d_plain, copy=True)\n+\n+        # recarray\n+        d = d_plain.view(np.recarray)\n+\n+        dd = structured_to_unstructured(d, copy=False)\n+        ddd = structured_to_unstructured(d, copy=True)\n+        assert_(np.shares_memory(d, dd))\n+        assert_(type(dd) is np.recarray)\n+        assert_(type(ddd) is np.recarray)\n+        assert_equal(dd, dd_expected)\n+        assert_equal(ddd, dd_expected)\n+\n+        # memmap\n+        d = np.memmap(tmp_path / 'memmap',\n+                      mode='w+',\n+                      dtype=d_plain.dtype,\n+                      shape=d_plain.shape)\n+        d[:] = d_plain\n+        dd = structured_to_unstructured(d, copy=False)\n+        ddd = structured_to_unstructured(d, copy=True)\n+        assert_(np.shares_memory(d, dd))\n+        assert_(type(dd) is np.memmap)\n+        assert_(type(ddd) is np.memmap)\n+        assert_equal(dd, dd_expected)\n+        assert_equal(ddd, dd_expected)\n+\n     def test_unstructured_to_structured(self):\n         # test if dtype is the args of np.dtype\n         a = np.zeros((20, 2))\n",
            "comment_added_diff": {
                "349": "        # test supported ndarray subclasses",
                "353": "        # recarray",
                "364": "        # memmap"
            },
            "comment_deleted_diff": {},
            "comment_modified_diff": {}
        }
    ],
    "fromnumeric.py": [
        {
            "commit": "486878b37fc7439a3b2b87747f50db9b62fea8eb",
            "timestamp": "2023-02-28T20:02:40+01:00",
            "author": "Inessa Pawson",
            "commit_message": "DOC: Update the docstring for `np.round_` to disrecommend it (#22527)\n\n[skip ci]",
            "additions": 6,
            "deletions": 2,
            "change_type": "MODIFY",
            "diff": "@@ -3748,14 +3748,18 @@ def var(a, axis=None, dtype=None, out=None, ddof=0, keepdims=np._NoValue, *,\n                          **kwargs)\n \n \n-# Aliases of other functions. These have their own definitions only so that\n-# they can have unique docstrings.\n+# Aliases of other functions. Provided unique docstrings \n+# are reference purposes only. Wherever possible,\n+# avoid using them.\n \n @array_function_dispatch(_around_dispatcher)\n def round_(a, decimals=0, out=None):\n     \"\"\"\n     Round an array to the given number of decimals.\n \n+    `~numpy.round_` is a disrecommended backwards-compatibility\n+    alias of `~numpy.around` and `~numpy.round`.\n+\n     See Also\n     --------\n     around : equivalent function; see for details.\n",
            "comment_added_diff": {
                "3751": "# Aliases of other functions. Provided unique docstrings",
                "3752": "# are reference purposes only. Wherever possible,",
                "3753": "# avoid using them."
            },
            "comment_deleted_diff": {
                "3751": "# Aliases of other functions. These have their own definitions only so that",
                "3752": "# they can have unique docstrings."
            },
            "comment_modified_diff": {
                "3751": "# Aliases of other functions. These have their own definitions only so that",
                "3752": "# they can have unique docstrings."
            }
        },
        {
            "commit": "cb62246d33386205f4e1d70429da17865dfdfbd9",
            "timestamp": "2023-03-01T13:51:41+00:00",
            "author": "Ralf Gommers",
            "commit_message": "DOC: add `np.round` to the html docs, and make it the preferred alias\n\nThe function is more commonly called `round`, both in the array API\nstandard and in other array libraries (e.g., PyTorch has `round` but not\naround). Plus we have `ndarray.round`.\n`around` is heavily used, so keep it as an alias - but prefer `round`.\nFor both this switch and for keeping the alias, xref gh-13877.\n\nCloses gh-19717",
            "additions": 26,
            "deletions": 13,
            "change_type": "MODIFY",
            "diff": "@@ -3238,12 +3238,12 @@ def size(a, axis=None):\n             return asarray(a).shape[axis]\n \n \n-def _around_dispatcher(a, decimals=None, out=None):\n+def _round_dispatcher(a, decimals=None, out=None):\n     return (a, out)\n \n \n-@array_function_dispatch(_around_dispatcher)\n-def around(a, decimals=0, out=None):\n+@array_function_dispatch(_round_dispatcher)\n+def round(a, decimals=0, out=None):\n     \"\"\"\n     Evenly round to the given number of decimals.\n \n@@ -3274,18 +3274,17 @@ def around(a, decimals=0, out=None):\n     See Also\n     --------\n     ndarray.round : equivalent method\n+    around : an alias for this function\n     ceil, fix, floor, rint, trunc\n \n \n     Notes\n     -----\n-    `~numpy.round` is often used as an alias for `~numpy.around`.\n-    \n     For values exactly halfway between rounded decimal values, NumPy\n     rounds to the nearest even value. Thus 1.5 and 2.5 round to 2.0,\n     -0.5 and 0.5 round to 0.0, etc.\n \n-    ``np.around`` uses a fast but sometimes inexact algorithm to round\n+    ``np.round`` uses a fast but sometimes inexact algorithm to round\n     floating-point datatypes. For positive `decimals` it is equivalent to\n     ``np.true_divide(np.rint(a * 10**decimals), 10**decimals)``, which has\n     error due to the inexact representation of decimal fractions in the IEEE\n@@ -3322,22 +3321,36 @@ def around(a, decimals=0, out=None):\n \n     Examples\n     --------\n-    >>> np.around([0.37, 1.64])\n+    >>> np.round([0.37, 1.64])\n     array([0., 2.])\n-    >>> np.around([0.37, 1.64], decimals=1)\n+    >>> np.round([0.37, 1.64], decimals=1)\n     array([0.4, 1.6])\n-    >>> np.around([.5, 1.5, 2.5, 3.5, 4.5]) # rounds to nearest even value\n+    >>> np.round([.5, 1.5, 2.5, 3.5, 4.5]) # rounds to nearest even value\n     array([0., 2., 2., 4., 4.])\n-    >>> np.around([1,2,3,11], decimals=1) # ndarray of ints is returned\n+    >>> np.round([1,2,3,11], decimals=1) # ndarray of ints is returned\n     array([ 1,  2,  3, 11])\n-    >>> np.around([1,2,3,11], decimals=-1)\n+    >>> np.round([1,2,3,11], decimals=-1)\n     array([ 0,  0,  0, 10])\n \n     \"\"\"\n     return _wrapfunc(a, 'round', decimals=decimals, out=out)\n \n \n-round = around\n+@array_function_dispatch(_round_dispatcher)\n+def around(a, decimals=0, out=None):\n+    \"\"\"\n+    Round an array to the given number of decimals.\n+\n+    `around` is an alias of `~numpy.round`.\n+\n+    See Also\n+    --------\n+    ndarray.round : equivalent method\n+    round : alias for this function\n+    ceil, fix, floor, rint, trunc\n+\n+    \"\"\"\n+    return _wrapfunc(a, 'round', decimals=decimals, out=out)\n \n \n def _mean_dispatcher(a, axis=None, dtype=None, out=None, keepdims=None, *,\n@@ -3755,7 +3768,7 @@ def var(a, axis=None, dtype=None, out=None, ddof=0, keepdims=np._NoValue, *,\n # are reference purposes only. Wherever possible,\n # avoid using them.\n \n-@array_function_dispatch(_around_dispatcher)\n+@array_function_dispatch(_round_dispatcher)\n def round_(a, decimals=0, out=None):\n     \"\"\"\n     Round an array to the given number of decimals.\n",
            "comment_added_diff": {
                "3328": "    >>> np.round([.5, 1.5, 2.5, 3.5, 4.5]) # rounds to nearest even value",
                "3330": "    >>> np.round([1,2,3,11], decimals=1) # ndarray of ints is returned"
            },
            "comment_deleted_diff": {
                "3329": "    >>> np.around([.5, 1.5, 2.5, 3.5, 4.5]) # rounds to nearest even value",
                "3331": "    >>> np.around([1,2,3,11], decimals=1) # ndarray of ints is returned"
            },
            "comment_modified_diff": {}
        },
        {
            "commit": "10743b0bcfa0905b15ccadfe164cfdb57ad6cf6a",
            "timestamp": "2023-03-01T13:51:46+00:00",
            "author": "Ralf Gommers",
            "commit_message": "MAINT: switch min/max with amin/amax, and add them to html docs\n\nCloses gh-13877",
            "additions": 70,
            "deletions": 33,
            "change_type": "MODIFY",
            "diff": "@@ -6,6 +6,7 @@\n import warnings\n \n import numpy as np\n+from .._utils import set_module\n from . import multiarray as mu\n from . import overrides\n from . import umath as um\n@@ -20,6 +21,7 @@\n     'all', 'alltrue', 'amax', 'amin', 'any', 'argmax',\n     'argmin', 'argpartition', 'argsort', 'around', 'choose', 'clip',\n     'compress', 'cumprod', 'cumproduct', 'cumsum', 'diagonal', 'mean',\n+    'max', 'min',\n     'ndim', 'nonzero', 'partition', 'prod', 'product', 'ptp', 'put',\n     'ravel', 'repeat', 'reshape', 'resize', 'round', 'round_',\n     'searchsorted', 'shape', 'size', 'sometrue', 'sort', 'squeeze',\n@@ -2695,13 +2697,14 @@ def ptp(a, axis=None, out=None, keepdims=np._NoValue):\n     return _methods._ptp(a, axis=axis, out=out, **kwargs)\n \n \n-def _amax_dispatcher(a, axis=None, out=None, keepdims=None, initial=None,\n-                     where=None):\n+def _max_dispatcher(a, axis=None, out=None, keepdims=None, initial=None,\n+                    where=None):\n     return (a, out)\n \n \n-@array_function_dispatch(_amax_dispatcher)\n-def amax(a, axis=None, out=None, keepdims=np._NoValue, initial=np._NoValue,\n+@array_function_dispatch(_max_dispatcher)\n+@set_module('numpy')\n+def max(a, axis=None, out=None, keepdims=np._NoValue, initial=np._NoValue,\n          where=np._NoValue):\n     \"\"\"\n     Return the maximum of an array or maximum along an axis.\n@@ -2729,7 +2732,7 @@ def amax(a, axis=None, out=None, keepdims=np._NoValue, initial=np._NoValue,\n         the result will broadcast correctly against the input array.\n \n         If the default value is passed, then `keepdims` will not be\n-        passed through to the `amax` method of sub-classes of\n+        passed through to the ``max`` method of sub-classes of\n         `ndarray`, however any non-default value will be.  If the\n         sub-class' method does not implement `keepdims` any\n         exceptions will be raised.\n@@ -2748,7 +2751,7 @@ def amax(a, axis=None, out=None, keepdims=np._NoValue, initial=np._NoValue,\n \n     Returns\n     -------\n-    amax : ndarray or scalar\n+    max : ndarray or scalar\n         Maximum of `a`. If `axis` is None, the result is a scalar value.\n         If `axis` is an int, the result is an array of dimension\n         ``a.ndim - 1``. If `axis` is a tuple, the result is an array of \n@@ -2775,9 +2778,9 @@ def amax(a, axis=None, out=None, keepdims=np._NoValue, initial=np._NoValue,\n     corresponding max value will be NaN as well. To ignore NaN values\n     (MATLAB behavior), please use nanmax.\n \n-    Don't use `amax` for element-wise comparison of 2 arrays; when\n+    Don't use `~numpy.max` for element-wise comparison of 2 arrays; when\n     ``a.shape[0]`` is 2, ``maximum(a[0], a[1])`` is faster than\n-    ``amax(a, axis=0)``.\n+    ``max(a, axis=0)``.\n \n     Examples\n     --------\n@@ -2785,19 +2788,19 @@ def amax(a, axis=None, out=None, keepdims=np._NoValue, initial=np._NoValue,\n     >>> a\n     array([[0, 1],\n            [2, 3]])\n-    >>> np.amax(a)           # Maximum of the flattened array\n+    >>> np.max(a)           # Maximum of the flattened array\n     3\n-    >>> np.amax(a, axis=0)   # Maxima along the first axis\n+    >>> np.max(a, axis=0)   # Maxima along the first axis\n     array([2, 3])\n-    >>> np.amax(a, axis=1)   # Maxima along the second axis\n+    >>> np.max(a, axis=1)   # Maxima along the second axis\n     array([1, 3])\n-    >>> np.amax(a, where=[False, True], initial=-1, axis=0)\n+    >>> np.max(a, where=[False, True], initial=-1, axis=0)\n     array([-1,  3])\n     >>> b = np.arange(5, dtype=float)\n     >>> b[2] = np.NaN\n-    >>> np.amax(b)\n+    >>> np.max(b)\n     nan\n-    >>> np.amax(b, where=~np.isnan(b), initial=-1)\n+    >>> np.max(b, where=~np.isnan(b), initial=-1)\n     4.0\n     >>> np.nanmax(b)\n     4.0\n@@ -2805,14 +2808,14 @@ def amax(a, axis=None, out=None, keepdims=np._NoValue, initial=np._NoValue,\n     You can use an initial value to compute the maximum of an empty slice, or\n     to initialize it to a different value:\n \n-    >>> np.amax([[-50], [10]], axis=-1, initial=0)\n+    >>> np.max([[-50], [10]], axis=-1, initial=0)\n     array([ 0, 10])\n \n     Notice that the initial value is used as one of the elements for which the\n     maximum is determined, unlike for the default argument Python's max\n     function, which is only used for empty iterables.\n \n-    >>> np.amax([5], initial=6)\n+    >>> np.max([5], initial=6)\n     6\n     >>> max([5], default=6)\n     5\n@@ -2821,14 +2824,31 @@ def amax(a, axis=None, out=None, keepdims=np._NoValue, initial=np._NoValue,\n                           keepdims=keepdims, initial=initial, where=where)\n \n \n-def _amin_dispatcher(a, axis=None, out=None, keepdims=None, initial=None,\n-                     where=None):\n+@array_function_dispatch(_max_dispatcher)\n+def amax(a, axis=None, out=None, keepdims=np._NoValue, initial=np._NoValue,\n+         where=np._NoValue):\n+    \"\"\"\n+    Return the maximum of an array or maximum along an axis.\n+\n+    `amax` is an alias of `~numpy.max`.\n+\n+    See Also\n+    --------\n+    max : alias of this function\n+    ndarray.max : equivalent method\n+    \"\"\"\n+    return _wrapreduction(a, np.maximum, 'max', axis, None, out,\n+                          keepdims=keepdims, initial=initial, where=where)\n+\n+\n+def _min_dispatcher(a, axis=None, out=None, keepdims=None, initial=None,\n+                    where=None):\n     return (a, out)\n \n \n-@array_function_dispatch(_amin_dispatcher)\n-def amin(a, axis=None, out=None, keepdims=np._NoValue, initial=np._NoValue,\n-         where=np._NoValue):\n+@array_function_dispatch(_min_dispatcher)\n+def min(a, axis=None, out=None, keepdims=np._NoValue, initial=np._NoValue,\n+        where=np._NoValue):\n     \"\"\"\n     Return the minimum of an array or minimum along an axis.\n \n@@ -2855,7 +2875,7 @@ def amin(a, axis=None, out=None, keepdims=np._NoValue, initial=np._NoValue,\n         the result will broadcast correctly against the input array.\n \n         If the default value is passed, then `keepdims` will not be\n-        passed through to the `amin` method of sub-classes of\n+        passed through to the ``min`` method of sub-classes of\n         `ndarray`, however any non-default value will be.  If the\n         sub-class' method does not implement `keepdims` any\n         exceptions will be raised.\n@@ -2874,7 +2894,7 @@ def amin(a, axis=None, out=None, keepdims=np._NoValue, initial=np._NoValue,\n \n     Returns\n     -------\n-    amin : ndarray or scalar\n+    min : ndarray or scalar\n         Minimum of `a`. If `axis` is None, the result is a scalar value.\n         If `axis` is an int, the result is an array of dimension\n         ``a.ndim - 1``.  If `axis` is a tuple, the result is an array of \n@@ -2901,9 +2921,9 @@ def amin(a, axis=None, out=None, keepdims=np._NoValue, initial=np._NoValue,\n     corresponding min value will be NaN as well. To ignore NaN values\n     (MATLAB behavior), please use nanmin.\n \n-    Don't use `amin` for element-wise comparison of 2 arrays; when\n+    Don't use `~numpy.min` for element-wise comparison of 2 arrays; when\n     ``a.shape[0]`` is 2, ``minimum(a[0], a[1])`` is faster than\n-    ``amin(a, axis=0)``.\n+    ``min(a, axis=0)``.\n \n     Examples\n     --------\n@@ -2911,25 +2931,25 @@ def amin(a, axis=None, out=None, keepdims=np._NoValue, initial=np._NoValue,\n     >>> a\n     array([[0, 1],\n            [2, 3]])\n-    >>> np.amin(a)           # Minimum of the flattened array\n+    >>> np.min(a)           # Minimum of the flattened array\n     0\n-    >>> np.amin(a, axis=0)   # Minima along the first axis\n+    >>> np.min(a, axis=0)   # Minima along the first axis\n     array([0, 1])\n-    >>> np.amin(a, axis=1)   # Minima along the second axis\n+    >>> np.min(a, axis=1)   # Minima along the second axis\n     array([0, 2])\n-    >>> np.amin(a, where=[False, True], initial=10, axis=0)\n+    >>> np.min(a, where=[False, True], initial=10, axis=0)\n     array([10,  1])\n \n     >>> b = np.arange(5, dtype=float)\n     >>> b[2] = np.NaN\n-    >>> np.amin(b)\n+    >>> np.min(b)\n     nan\n-    >>> np.amin(b, where=~np.isnan(b), initial=10)\n+    >>> np.min(b, where=~np.isnan(b), initial=10)\n     0.0\n     >>> np.nanmin(b)\n     0.0\n \n-    >>> np.amin([[-50], [10]], axis=-1, initial=0)\n+    >>> np.min([[-50], [10]], axis=-1, initial=0)\n     array([-50,   0])\n \n     Notice that the initial value is used as one of the elements for which the\n@@ -2938,7 +2958,7 @@ def amin(a, axis=None, out=None, keepdims=np._NoValue, initial=np._NoValue,\n \n     Notice that this isn't the same as Python's ``default`` argument.\n \n-    >>> np.amin([6], initial=5)\n+    >>> np.min([6], initial=5)\n     5\n     >>> min([6], default=5)\n     6\n@@ -2947,6 +2967,23 @@ def amin(a, axis=None, out=None, keepdims=np._NoValue, initial=np._NoValue,\n                           keepdims=keepdims, initial=initial, where=where)\n \n \n+@array_function_dispatch(_min_dispatcher)\n+def amin(a, axis=None, out=None, keepdims=np._NoValue, initial=np._NoValue,\n+         where=np._NoValue):\n+    \"\"\"\n+    Return the minimum of an array or minimum along an axis.\n+\n+    `amin` is an alias of `~numpy.min`.\n+\n+    See Also\n+    --------\n+    min : alias of this function\n+    ndarray.min : equivalent method\n+    \"\"\"\n+    return _wrapreduction(a, np.minimum, 'min', axis, None, out,\n+                          keepdims=keepdims, initial=initial, where=where)\n+\n+\n def _prod_dispatcher(a, axis=None, dtype=None, out=None, keepdims=None,\n                      initial=None, where=None):\n     return (a, out)\n",
            "comment_added_diff": {
                "2791": "    >>> np.max(a)           # Maximum of the flattened array",
                "2793": "    >>> np.max(a, axis=0)   # Maxima along the first axis",
                "2795": "    >>> np.max(a, axis=1)   # Maxima along the second axis",
                "2934": "    >>> np.min(a)           # Minimum of the flattened array",
                "2936": "    >>> np.min(a, axis=0)   # Minima along the first axis",
                "2938": "    >>> np.min(a, axis=1)   # Minima along the second axis"
            },
            "comment_deleted_diff": {
                "2788": "    >>> np.amax(a)           # Maximum of the flattened array",
                "2790": "    >>> np.amax(a, axis=0)   # Maxima along the first axis",
                "2792": "    >>> np.amax(a, axis=1)   # Maxima along the second axis",
                "2914": "    >>> np.amin(a)           # Minimum of the flattened array",
                "2916": "    >>> np.amin(a, axis=0)   # Minima along the first axis",
                "2918": "    >>> np.amin(a, axis=1)   # Minima along the second axis"
            },
            "comment_modified_diff": {}
        },
        {
            "commit": "3dcc33aa585339f36639f78da70bb35f26609bef",
            "timestamp": "2023-03-02T21:10:41+00:00",
            "author": "Ralf Gommers",
            "commit_message": "MAINT: add dates/versions to deprecations, fix linter complaint\n\n[skip cirrus] [skip circle]",
            "additions": 5,
            "deletions": 0,
            "change_type": "MODIFY",
            "diff": "@@ -3821,6 +3821,7 @@ def round_(a, decimals=0, out=None):\n     --------\n     around : equivalent function; see for details.\n     \"\"\"\n+    # 2023-02-28, 1.25.0\n     warnings.warn(\"`round_` is deprecated as of NumPy 1.25.0, and will be \"\n                   \"removed in NumPy 2.0. Please use `round` instead.\",\n                   DeprecationWarning, stacklevel=2)\n@@ -3840,6 +3841,7 @@ def product(*args, **kwargs):\n     --------\n     prod : equivalent function; see for details.\n     \"\"\"\n+    # 2023-03-02, 1.25.0\n     warnings.warn(\"`product` is deprecated as of NumPy 1.25.0, and will be \"\n                   \"removed in NumPy 2.0. Please use `prod` instead.\",\n                   DeprecationWarning, stacklevel=2)\n@@ -3859,6 +3861,7 @@ def cumproduct(*args, **kwargs):\n     --------\n     cumprod : equivalent function; see for details.\n     \"\"\"\n+    # 2023-03-02, 1.25.0\n     warnings.warn(\"`cumproduct` is deprecated as of NumPy 1.25.0, and will be \"\n                   \"removed in NumPy 2.0. Please use `cumprod` instead.\",\n                   DeprecationWarning, stacklevel=2)\n@@ -3880,6 +3883,7 @@ def sometrue(*args, **kwargs):\n     --------\n     any : equivalent function; see for details.\n     \"\"\"\n+    # 2023-03-02, 1.25.0\n     warnings.warn(\"`sometrue` is deprecated as of NumPy 1.25.0, and will be \"\n                   \"removed in NumPy 2.0. Please use `any` instead.\",\n                   DeprecationWarning, stacklevel=2)\n@@ -3899,6 +3903,7 @@ def alltrue(*args, **kwargs):\n     --------\n     numpy.all : Equivalent function; see for details.\n     \"\"\"\n+    # 2023-03-02, 1.25.0\n     warnings.warn(\"`alltrue` is deprecated as of NumPy 1.25.0, and will be \"\n                   \"removed in NumPy 2.0. Please use `all` instead.\",\n                   DeprecationWarning, stacklevel=2)\n",
            "comment_added_diff": {
                "3824": "    # 2023-02-28, 1.25.0",
                "3844": "    # 2023-03-02, 1.25.0",
                "3864": "    # 2023-03-02, 1.25.0",
                "3886": "    # 2023-03-02, 1.25.0",
                "3906": "    # 2023-03-02, 1.25.0"
            },
            "comment_deleted_diff": {},
            "comment_modified_diff": {}
        },
        {
            "commit": "fe8d76ed3974a34397612cbed9f7a61a78fe6b58",
            "timestamp": "2023-03-10T12:26:39+00:00",
            "author": "Ralf Gommers",
            "commit_message": "DEP: update deprecations for `np.product` and co to emit from dispatcher\n\nThis follows up on a review comment on gh-23314\n\n[skip cirrus] [skip circle]",
            "additions": 68,
            "deletions": 25,
            "change_type": "MODIFY",
            "diff": "@@ -3805,7 +3805,16 @@ def var(a, axis=None, dtype=None, out=None, ddof=0, keepdims=np._NoValue, *,\n # are reference purposes only. Wherever possible,\n # avoid using them.\n \n-@array_function_dispatch(_round_dispatcher)\n+\n+def _round__dispatcher(a, decimals=None, out=None):\n+    # 2023-02-28, 1.25.0\n+    warnings.warn(\"`round_` is deprecated as of NumPy 1.25.0, and will be \"\n+                  \"removed in NumPy 2.0. Please use `round` instead.\",\n+                  DeprecationWarning, stacklevel=3)\n+    return (a, out)\n+\n+\n+@array_function_dispatch(_round__dispatcher)\n def round_(a, decimals=0, out=None):\n     \"\"\"\n     Round an array to the given number of decimals.\n@@ -3821,14 +3830,23 @@ def round_(a, decimals=0, out=None):\n     --------\n     around : equivalent function; see for details.\n     \"\"\"\n-    # 2023-02-28, 1.25.0\n-    warnings.warn(\"`round_` is deprecated as of NumPy 1.25.0, and will be \"\n-                  \"removed in NumPy 2.0. Please use `round` instead.\",\n-                  DeprecationWarning, stacklevel=2)\n+    if not overrides.ARRAY_FUNCTION_ENABLED:\n+        # call dispatch helper explicitly, as it emits a deprecation warning\n+        _round__dispatcher(a, decimals=decimals, out=out)\n+\n     return around(a, decimals=decimals, out=out)\n \n \n-@array_function_dispatch(_prod_dispatcher, verify=False)\n+def _product_dispatcher(a, axis=None, dtype=None, out=None, keepdims=None,\n+                        initial=None, where=None):\n+    # 2023-03-02, 1.25.0\n+    warnings.warn(\"`product` is deprecated as of NumPy 1.25.0, and will be \"\n+                  \"removed in NumPy 2.0. Please use `prod` instead.\",\n+                  DeprecationWarning, stacklevel=3)\n+    return (a, out)\n+\n+\n+@array_function_dispatch(_product_dispatcher, verify=False)\n def product(*args, **kwargs):\n     \"\"\"\n     Return the product of array elements over a given axis.\n@@ -3841,14 +3859,22 @@ def product(*args, **kwargs):\n     --------\n     prod : equivalent function; see for details.\n     \"\"\"\n-    # 2023-03-02, 1.25.0\n-    warnings.warn(\"`product` is deprecated as of NumPy 1.25.0, and will be \"\n-                  \"removed in NumPy 2.0. Please use `prod` instead.\",\n-                  DeprecationWarning, stacklevel=2)\n+    if not overrides.ARRAY_FUNCTION_ENABLED:\n+        # call dispatch helper explicitly, as it emits a deprecation warning\n+        _product_dispatcher(*args, **kwargs)\n+\n     return prod(*args, **kwargs)\n \n \n-@array_function_dispatch(_cumprod_dispatcher, verify=False)\n+def _cumproduct_dispatcher(a, axis=None, dtype=None, out=None):\n+    # 2023-03-02, 1.25.0\n+    warnings.warn(\"`cumproduct` is deprecated as of NumPy 1.25.0, and will be \"\n+                  \"removed in NumPy 2.0. Please use `cumprod` instead.\",\n+                  DeprecationWarning, stacklevel=3)\n+    return (a, out)\n+\n+\n+@array_function_dispatch(_cumproduct_dispatcher, verify=False)\n def cumproduct(*args, **kwargs):\n     \"\"\"\n     Return the cumulative product over the given axis.\n@@ -3861,14 +3887,23 @@ def cumproduct(*args, **kwargs):\n     --------\n     cumprod : equivalent function; see for details.\n     \"\"\"\n-    # 2023-03-02, 1.25.0\n-    warnings.warn(\"`cumproduct` is deprecated as of NumPy 1.25.0, and will be \"\n-                  \"removed in NumPy 2.0. Please use `cumprod` instead.\",\n-                  DeprecationWarning, stacklevel=2)\n+    if not overrides.ARRAY_FUNCTION_ENABLED:\n+        # call dispatch helper explicitly, as it emits a deprecation warning\n+        _cumproduct_dispatcher(*args, **kwargs)\n+\n     return cumprod(*args, **kwargs)\n \n \n-@array_function_dispatch(_any_dispatcher, verify=False)\n+def _sometrue_dispatcher(a, axis=None, out=None, keepdims=None, *,\n+                         where=np._NoValue):\n+    # 2023-03-02, 1.25.0\n+    warnings.warn(\"`sometrue` is deprecated as of NumPy 1.25.0, and will be \"\n+                  \"removed in NumPy 2.0. Please use `any` instead.\",\n+                  DeprecationWarning, stacklevel=3)\n+    return (a, where, out)\n+\n+\n+@array_function_dispatch(_sometrue_dispatcher, verify=False)\n def sometrue(*args, **kwargs):\n     \"\"\"\n     Check whether some values are true.\n@@ -3883,14 +3918,22 @@ def sometrue(*args, **kwargs):\n     --------\n     any : equivalent function; see for details.\n     \"\"\"\n-    # 2023-03-02, 1.25.0\n-    warnings.warn(\"`sometrue` is deprecated as of NumPy 1.25.0, and will be \"\n-                  \"removed in NumPy 2.0. Please use `any` instead.\",\n-                  DeprecationWarning, stacklevel=2)\n+    if not overrides.ARRAY_FUNCTION_ENABLED:\n+        # call dispatch helper explicitly, as it emits a deprecation warning\n+        _sometrue_dispatcher(*args, **kwargs)\n+\n     return any(*args, **kwargs)\n \n \n-@array_function_dispatch(_all_dispatcher, verify=False)\n+def _alltrue_dispatcher(a, axis=None, out=None, keepdims=None, *, where=None):\n+    # 2023-03-02, 1.25.0\n+    warnings.warn(\"`alltrue` is deprecated as of NumPy 1.25.0, and will be \"\n+                  \"removed in NumPy 2.0. Please use `all` instead.\",\n+                  DeprecationWarning, stacklevel=3)\n+    return (a, where, out)\n+\n+\n+@array_function_dispatch(_alltrue_dispatcher, verify=False)\n def alltrue(*args, **kwargs):\n     \"\"\"\n     Check if all elements of input array are true.\n@@ -3903,8 +3946,8 @@ def alltrue(*args, **kwargs):\n     --------\n     numpy.all : Equivalent function; see for details.\n     \"\"\"\n-    # 2023-03-02, 1.25.0\n-    warnings.warn(\"`alltrue` is deprecated as of NumPy 1.25.0, and will be \"\n-                  \"removed in NumPy 2.0. Please use `all` instead.\",\n-                  DeprecationWarning, stacklevel=2)\n+    if not overrides.ARRAY_FUNCTION_ENABLED:\n+        # call dispatch helper explicitly, as it emits a deprecation warning\n+        _alltrue_dispatcher(*args, **kwargs)\n+\n     return all(*args, **kwargs)\n",
            "comment_added_diff": {
                "3810": "    # 2023-02-28, 1.25.0",
                "3834": "        # call dispatch helper explicitly, as it emits a deprecation warning",
                "3842": "    # 2023-03-02, 1.25.0",
                "3863": "        # call dispatch helper explicitly, as it emits a deprecation warning",
                "3870": "    # 2023-03-02, 1.25.0",
                "3891": "        # call dispatch helper explicitly, as it emits a deprecation warning",
                "3899": "    # 2023-03-02, 1.25.0",
                "3922": "        # call dispatch helper explicitly, as it emits a deprecation warning",
                "3929": "    # 2023-03-02, 1.25.0",
                "3950": "        # call dispatch helper explicitly, as it emits a deprecation warning"
            },
            "comment_deleted_diff": {
                "3824": "    # 2023-02-28, 1.25.0",
                "3844": "    # 2023-03-02, 1.25.0",
                "3864": "    # 2023-03-02, 1.25.0",
                "3886": "    # 2023-03-02, 1.25.0",
                "3906": "    # 2023-03-02, 1.25.0"
            },
            "comment_modified_diff": {}
        },
        {
            "commit": "1da1663196c95b3811ca84d9e335f32cfeb05e32",
            "timestamp": "2023-03-12T22:31:28+00:00",
            "author": "Ralf Gommers",
            "commit_message": "MAINT: remove `NUMPY_EXPERIMENTAL_ARRAY_FUNCTION` env var\n\nAs discussed in\nhttps://mail.python.org/archives/list/numpy-discussion@python.org/thread/UKZJACAP5FUG7KP2AQDPE4P5ADNWLOHZ/\n\nThis flag was always meant to be temporary, and cleaning it up is\nlong overdue.",
            "additions": 0,
            "deletions": 20,
            "change_type": "MODIFY",
            "diff": "@@ -3830,10 +3830,6 @@ def round_(a, decimals=0, out=None):\n     --------\n     around : equivalent function; see for details.\n     \"\"\"\n-    if not overrides.ARRAY_FUNCTION_ENABLED:\n-        # call dispatch helper explicitly, as it emits a deprecation warning\n-        _round__dispatcher(a, decimals=decimals, out=out)\n-\n     return around(a, decimals=decimals, out=out)\n \n \n@@ -3859,10 +3855,6 @@ def product(*args, **kwargs):\n     --------\n     prod : equivalent function; see for details.\n     \"\"\"\n-    if not overrides.ARRAY_FUNCTION_ENABLED:\n-        # call dispatch helper explicitly, as it emits a deprecation warning\n-        _product_dispatcher(*args, **kwargs)\n-\n     return prod(*args, **kwargs)\n \n \n@@ -3887,10 +3879,6 @@ def cumproduct(*args, **kwargs):\n     --------\n     cumprod : equivalent function; see for details.\n     \"\"\"\n-    if not overrides.ARRAY_FUNCTION_ENABLED:\n-        # call dispatch helper explicitly, as it emits a deprecation warning\n-        _cumproduct_dispatcher(*args, **kwargs)\n-\n     return cumprod(*args, **kwargs)\n \n \n@@ -3918,10 +3906,6 @@ def sometrue(*args, **kwargs):\n     --------\n     any : equivalent function; see for details.\n     \"\"\"\n-    if not overrides.ARRAY_FUNCTION_ENABLED:\n-        # call dispatch helper explicitly, as it emits a deprecation warning\n-        _sometrue_dispatcher(*args, **kwargs)\n-\n     return any(*args, **kwargs)\n \n \n@@ -3946,8 +3930,4 @@ def alltrue(*args, **kwargs):\n     --------\n     numpy.all : Equivalent function; see for details.\n     \"\"\"\n-    if not overrides.ARRAY_FUNCTION_ENABLED:\n-        # call dispatch helper explicitly, as it emits a deprecation warning\n-        _alltrue_dispatcher(*args, **kwargs)\n-\n     return all(*args, **kwargs)\n",
            "comment_added_diff": {},
            "comment_deleted_diff": {
                "3834": "        # call dispatch helper explicitly, as it emits a deprecation warning",
                "3863": "        # call dispatch helper explicitly, as it emits a deprecation warning",
                "3891": "        # call dispatch helper explicitly, as it emits a deprecation warning",
                "3922": "        # call dispatch helper explicitly, as it emits a deprecation warning",
                "3950": "        # call dispatch helper explicitly, as it emits a deprecation warning"
            },
            "comment_modified_diff": {}
        },
        {
            "commit": "b50568d9e758b489c2a3c409ef4e57b67820f090",
            "timestamp": "2023-03-30T22:38:54-07:00",
            "author": "Pratyay Banerjee",
            "commit_message": "DOC: Fix typos & grammer in docstrings and comments (#23503)",
            "additions": 1,
            "deletions": 1,
            "change_type": "MODIFY",
            "diff": "@@ -3802,7 +3802,7 @@ def var(a, axis=None, dtype=None, out=None, ddof=0, keepdims=np._NoValue, *,\n \n \n # Aliases of other functions. Provided unique docstrings \n-# are reference purposes only. Wherever possible,\n+# are for reference purposes only. Wherever possible,\n # avoid using them.\n \n \n",
            "comment_added_diff": {
                "3805": "# are for reference purposes only. Wherever possible,"
            },
            "comment_deleted_diff": {
                "3805": "# are reference purposes only. Wherever possible,"
            },
            "comment_modified_diff": {
                "3805": "# are reference purposes only. Wherever possible,"
            }
        },
        {
            "commit": "700211a7af88afa225ef0d8332c575c8aae813de",
            "timestamp": "2023-03-31T16:38:15+02:00",
            "author": "JoryKlaverstijn",
            "commit_message": "DOC: Removed `.shape` setting note from reshape (#23491)\n\n* DOC: Changed the example for modifying the shape without modifying the initial object\r\n\r\n* DOC: Removed the example for directly assigning a tuple to the shape attribute of a numpy array\r\n\r\n* DOC: Re-added note about copying data when reshaping an array to numpy.reshape docs\r\n\r\n* DOC: reformat for linting\r\n\r\n---------\r\n\r\nCo-authored-by: Jory Klaverstijn <j.klaverstijn@student.rug.nl>\r\nCo-authored-by: Matti Picus <matti.picus@gmail.com>",
            "additions": 3,
            "deletions": 18,
            "change_type": "MODIFY",
            "diff": "@@ -238,24 +238,9 @@ def reshape(a, newshape, order='C'):\n \n     Notes\n     -----\n-    It is not always possible to change the shape of an array without\n-    copying the data. If you want an error to be raised when the data is copied,\n-    you should assign the new shape to the shape attribute of the array::\n-\n-     >>> a = np.zeros((10, 2))\n-\n-     # A transpose makes the array non-contiguous\n-     >>> b = a.T\n-\n-     # Taking a view makes it possible to modify the shape without modifying\n-     # the initial object.\n-     >>> c = b.view()\n-     >>> c.shape = (20)\n-     Traceback (most recent call last):\n-        ...\n-     AttributeError: Incompatible shape for in-place modification. Use\n-     `.reshape()` to make a copy with the desired shape.\n-\n+    It is not always possible to change the shape of an array without copying\n+    the data.\n+    \n     The `order` keyword gives the index ordering both for *fetching* the values\n     from `a`, and then *placing* the values into the output array.\n     For example, let's say you have an array:\n",
            "comment_added_diff": {},
            "comment_deleted_diff": {
                "247": "     # A transpose makes the array non-contiguous",
                "250": "     # Taking a view makes it possible to modify the shape without modifying",
                "251": "     # the initial object."
            },
            "comment_modified_diff": {}
        },
        {
            "commit": "ab2178b47c0ee834180c318db196976623710691",
            "timestamp": "2023-07-07T08:57:37+03:00",
            "author": "Ronald van Elburg",
            "commit_message": "ENH: add mean keyword to std and var (#24126)\n\n* Add mean keyword to std and var functions.\r\n\r\n* Add releae note for mean keyword to std and var functions.\r\n\r\n* Update release note with PR number\r\n\r\n* Address lint issue.\r\n\r\n* Align nan signatures with new signatures.\r\n\r\n* Address lint issue.\r\n\r\n* Correct version numbers on keywords.\r\n\r\n* Put backticks on keyword argument in documentation string.\r\n\r\n* Cleanuup assert statements in tests\r\n\r\n* Move comparison of in and out arrays closer to the function call.\r\n\r\n* Remove clutter from example code in release note.\r\n\r\n* Add test for nanstd and fix error in nanvar\r\n\r\n* haqndle \"mean\" keyword for var and std on MaskedArrays.\r\n\r\n* Address lint issues.\r\n\r\n* update the dispatchers according to suggestions by Marten van Kerkwijk:\r\n\r\n(https://github.com/numpy/numpy/pull/24126#discussion_r1254314302) The dispatcher returns all arguments that can, in principle, contain something that is an array and hence that, if not a regular ndarray, can allow the function to be dealt with another package (say, dask). Since mean can be an array, it should be added (most similar to where).\r\n\r\n* Move and adjust example from release note to doc-strings. Reflow doc-string.\r\n\r\n* Improve doc-string. Shorter sentences and add type and label mean argument\r\n\r\n* Remove some of these pesky trailing white spaces\r\n\r\n* Make extra white lines more consistent.\r\n\r\n* Make sure code examples execute without Jupyter magic.\r\n\r\n* Fold lines to pass linter.\r\n\r\n* Update doc-string nanstd and nanvar.\r\n\r\n* Try to satisfy linter and apple requirements at the same time. Making the example code ugly, alas!\r\n\r\n* Make doctest skip resource dependent output",
            "additions": 61,
            "deletions": 13,
            "change_type": "MODIFY",
            "diff": "@@ -240,7 +240,7 @@ def reshape(a, newshape, order='C'):\n     -----\n     It is not always possible to change the shape of an array without copying\n     the data.\n-    \n+\n     The `order` keyword gives the index ordering both for *fetching* the values\n     from `a`, and then *placing* the values into the output array.\n     For example, let's say you have an array:\n@@ -2741,7 +2741,7 @@ def max(a, axis=None, out=None, keepdims=np._NoValue, initial=np._NoValue,\n     max : ndarray or scalar\n         Maximum of `a`. If `axis` is None, the result is a scalar value.\n         If `axis` is an int, the result is an array of dimension\n-        ``a.ndim - 1``. If `axis` is a tuple, the result is an array of \n+        ``a.ndim - 1``. If `axis` is a tuple, the result is an array of\n         dimension ``a.ndim - len(axis)``.\n \n     See Also\n@@ -2884,7 +2884,7 @@ def min(a, axis=None, out=None, keepdims=np._NoValue, initial=np._NoValue,\n     min : ndarray or scalar\n         Minimum of `a`. If `axis` is None, the result is a scalar value.\n         If `axis` is an int, the result is an array of dimension\n-        ``a.ndim - 1``.  If `axis` is a tuple, the result is an array of \n+        ``a.ndim - 1``.  If `axis` is a tuple, the result is an array of\n         dimension ``a.ndim - len(axis)``.\n \n     See Also\n@@ -3072,7 +3072,7 @@ def prod(a, axis=None, dtype=None, out=None, keepdims=np._NoValue,\n     array([  2.,  12.])\n     >>> np.prod(a, axis=0)\n     array([3., 8.])\n-    \n+\n     Or select specific elements to include:\n \n     >>> np.prod([1., np.nan, 3.], where=[True, False, True])\n@@ -3506,13 +3506,13 @@ def mean(a, axis=None, dtype=None, out=None, keepdims=np._NoValue, *,\n \n \n def _std_dispatcher(a, axis=None, dtype=None, out=None, ddof=None,\n-                    keepdims=None, *, where=None):\n-    return (a, where, out)\n+                    keepdims=None, *, where=None, mean=None):\n+    return (a, where, out, mean)\n \n \n @array_function_dispatch(_std_dispatcher)\n def std(a, axis=None, dtype=None, out=None, ddof=0, keepdims=np._NoValue, *,\n-        where=np._NoValue):\n+        where=np._NoValue, mean=np._NoValue):\n     \"\"\"\n     Compute the standard deviation along the specified axis.\n \n@@ -3554,13 +3554,20 @@ def std(a, axis=None, dtype=None, out=None, ddof=0, keepdims=np._NoValue, *,\n         `ndarray`, however any non-default value will be.  If the\n         sub-class' method does not implement `keepdims` any\n         exceptions will be raised.\n-\n     where : array_like of bool, optional\n         Elements to include in the standard deviation.\n         See `~numpy.ufunc.reduce` for details.\n \n         .. versionadded:: 1.20.0\n \n+    mean : array like, optional\n+        Provide the mean to prevent its recalculation. The mean should have\n+        a shape as if it was calculated with ``keepdims=True``.\n+        The axis for the calculation of the mean should be the same as used in\n+        the call to this std function.\n+\n+        .. versionadded:: 1.26.0\n+\n     Returns\n     -------\n     standard_deviation : ndarray, see dtype parameter above.\n@@ -3628,12 +3635,29 @@ def std(a, axis=None, dtype=None, out=None, ddof=0, keepdims=np._NoValue, *,\n     >>> np.std(a, where=[[True], [True], [False]])\n     2.0\n \n+    Using the mean keyword to save computation time:\n+    >>> import numpy as np\n+    >>> from timeit import timeit\n+    >>> a = np.array([[14, 8, 11, 10], [7, 9, 10, 11], [10, 15, 5, 10]])\n+    >>> mean = np.mean(a, axis=1, keepdims=True)\n+    >>>\n+    >>> g = globals()\n+    >>> n = 10000\n+    >>> t1 = timeit(\"std = np.std(a, axis=1, mean=mean)\", globals=g, number=n)\n+    >>> t2 = timeit(\"std = np.std(a, axis=1)\", globals=g, number=n)\n+    >>> print(f'Percentage execution time saved {100*(t2-t1)/t2:.0f}%')\n+    #doctest: +SKIP\n+    Percentage execution time saved 30%\n+\n     \"\"\"\n     kwargs = {}\n     if keepdims is not np._NoValue:\n         kwargs['keepdims'] = keepdims\n     if where is not np._NoValue:\n         kwargs['where'] = where\n+    if mean is not np._NoValue:\n+        kwargs['mean'] = mean\n+\n     if type(a) is not mu.ndarray:\n         try:\n             std = a.std\n@@ -3647,13 +3671,13 @@ def std(a, axis=None, dtype=None, out=None, ddof=0, keepdims=np._NoValue, *,\n \n \n def _var_dispatcher(a, axis=None, dtype=None, out=None, ddof=None,\n-                    keepdims=None, *, where=None):\n-    return (a, where, out)\n+                    keepdims=None, *, where=None, mean=None):\n+    return (a, where, out, mean)\n \n \n @array_function_dispatch(_var_dispatcher)\n def var(a, axis=None, dtype=None, out=None, ddof=0, keepdims=np._NoValue, *,\n-        where=np._NoValue):\n+        where=np._NoValue, mean=np._NoValue):\n     \"\"\"\n     Compute the variance along the specified axis.\n \n@@ -3696,13 +3720,20 @@ def var(a, axis=None, dtype=None, out=None, ddof=0, keepdims=np._NoValue, *,\n         `ndarray`, however any non-default value will be.  If the\n         sub-class' method does not implement `keepdims` any\n         exceptions will be raised.\n-\n     where : array_like of bool, optional\n         Elements to include in the variance. See `~numpy.ufunc.reduce` for\n         details.\n \n         .. versionadded:: 1.20.0\n \n+    mean : array like, optional\n+        Provide the mean to prevent its recalculation. The mean should have\n+        a shape as if it was calculated with ``keepdims=True``.\n+        The axis for the calculation of the mean should be the same as used in\n+        the call to this var function.\n+\n+        .. versionadded:: 1.26.0\n+\n     Returns\n     -------\n     variance : ndarray, see dtype parameter above\n@@ -3768,12 +3799,29 @@ def var(a, axis=None, dtype=None, out=None, ddof=0, keepdims=np._NoValue, *,\n     >>> np.var(a, where=[[True], [True], [False]])\n     4.0\n \n+    Using the mean keyword to save computation time:\n+    >>> import numpy as np\n+    >>> from timeit import timeit\n+    >>>\n+    >>> a = np.array([[14, 8, 11, 10], [7, 9, 10, 11], [10, 15, 5, 10]])\n+    >>> mean = np.mean(a, axis=1, keepdims=True)\n+    >>>\n+    >>> g = globals()\n+    >>> n = 10000\n+    >>> t1 = timeit(\"var = np.var(a, axis=1, mean=mean)\", globals=g, number=n)\n+    >>> t2 = timeit(\"var = np.var(a, axis=1)\", globals=g, number=n)\n+    >>> print(f'Percentage execution time saved {100*(t2-t1)/t2:.0f}%')\n+    #doctest: +SKIP\n+    Percentage execution time saved 32%\n+\n     \"\"\"\n     kwargs = {}\n     if keepdims is not np._NoValue:\n         kwargs['keepdims'] = keepdims\n     if where is not np._NoValue:\n         kwargs['where'] = where\n+    if mean is not np._NoValue:\n+        kwargs['mean'] = mean\n \n     if type(a) is not mu.ndarray:\n         try:\n@@ -3788,7 +3836,7 @@ def var(a, axis=None, dtype=None, out=None, ddof=0, keepdims=np._NoValue, *,\n                          **kwargs)\n \n \n-# Aliases of other functions. Provided unique docstrings \n+# Aliases of other functions. Provided unique docstrings\n # are for reference purposes only. Wherever possible,\n # avoid using them.\n \n",
            "comment_added_diff": {
                "3649": "    #doctest: +SKIP",
                "3814": "    #doctest: +SKIP",
                "3839": "# Aliases of other functions. Provided unique docstrings"
            },
            "comment_deleted_diff": {
                "3791": "# Aliases of other functions. Provided unique docstrings"
            },
            "comment_modified_diff": {}
        },
        {
            "commit": "b43384e8f9f7242c59985c4a3d687c95a2a9dbf4",
            "timestamp": "2023-08-30T09:34:12+02:00",
            "author": "Mateusz Sok\u00f3\u0142",
            "commit_message": "MAINT: Remove deprecated functions [NEP 52] (#24477)",
            "additions": 1,
            "deletions": 28,
            "change_type": "MODIFY",
            "diff": "@@ -23,7 +23,7 @@\n     'compress', 'cumprod', 'cumproduct', 'cumsum', 'diagonal', 'mean',\n     'max', 'min',\n     'ndim', 'nonzero', 'partition', 'prod', 'product', 'ptp', 'put',\n-    'ravel', 'repeat', 'reshape', 'resize', 'round', 'round_',\n+    'ravel', 'repeat', 'reshape', 'resize', 'round',\n     'searchsorted', 'shape', 'size', 'sometrue', 'sort', 'squeeze',\n     'std', 'sum', 'swapaxes', 'take', 'trace', 'transpose', 'var',\n ]\n@@ -3843,33 +3843,6 @@ def var(a, axis=None, dtype=None, out=None, ddof=0, keepdims=np._NoValue, *,\n # avoid using them.\n \n \n-def _round__dispatcher(a, decimals=None, out=None):\n-    # 2023-02-28, 1.25.0\n-    warnings.warn(\"`round_` is deprecated as of NumPy 1.25.0, and will be \"\n-                  \"removed in NumPy 2.0. Please use `round` instead.\",\n-                  DeprecationWarning, stacklevel=3)\n-    return (a, out)\n-\n-\n-@array_function_dispatch(_round__dispatcher)\n-def round_(a, decimals=0, out=None):\n-    \"\"\"\n-    Round an array to the given number of decimals.\n-\n-    `~numpy.round_` is a disrecommended backwards-compatibility\n-    alias of `~numpy.around` and `~numpy.round`.\n-\n-    .. deprecated:: 1.25.0\n-        ``round_`` is deprecated as of NumPy 1.25.0, and will be\n-        removed in NumPy 2.0. Please use `round` instead.\n-\n-    See Also\n-    --------\n-    around : equivalent function; see for details.\n-    \"\"\"\n-    return around(a, decimals=decimals, out=out)\n-\n-\n def _product_dispatcher(a, axis=None, dtype=None, out=None, keepdims=None,\n                         initial=None, where=None):\n     # 2023-03-02, 1.25.0\n",
            "comment_added_diff": {},
            "comment_deleted_diff": {
                "3847": "    # 2023-02-28, 1.25.0"
            },
            "comment_modified_diff": {}
        },
        {
            "commit": "cb176c1f8b453a923e65ddde9245cb6f1c8da47e",
            "timestamp": "2023-09-18T10:35:06-07:00",
            "author": "Raghuveer Devulapalli",
            "commit_message": "Add may vary to output of np.partition and np.argpartition in docs",
            "additions": 4,
            "deletions": 4,
            "change_type": "MODIFY",
            "diff": "@@ -740,7 +740,7 @@ def partition(a, kth, axis=-1, kind='introselect', order=None):\n     >>> a = np.array([7, 1, 7, 7, 1, 5, 7, 2, 3, 2, 6, 2, 3, 0])\n     >>> p = np.partition(a, 4)\n     >>> p\n-    array([0, 1, 2, 1, 2, 5, 2, 3, 3, 6, 7, 7, 7, 7])\n+    array([0, 1, 2, 1, 2, 5, 2, 3, 3, 6, 7, 7, 7, 7]) # may vary\n \n     ``p[4]`` is 2;  all elements in ``p[:4]`` are less than or equal\n     to ``p[4]``, and all elements in ``p[5:]`` are greater than or\n@@ -838,13 +838,13 @@ def argpartition(a, kth, axis=-1, kind='introselect', order=None):\n \n     >>> x = np.array([3, 4, 2, 1])\n     >>> x[np.argpartition(x, 3)]\n-    array([2, 1, 3, 4])\n+    array([2, 1, 3, 4]) # may vary\n     >>> x[np.argpartition(x, (1, 3))]\n-    array([1, 2, 3, 4])\n+    array([1, 2, 3, 4]) # may vary\n \n     >>> x = [3, 4, 2, 1]\n     >>> np.array(x)[np.argpartition(x, 3)]\n-    array([2, 1, 3, 4])\n+    array([2, 1, 3, 4]) # may vary\n \n     Multi-dimensional array:\n \n",
            "comment_added_diff": {
                "743": "    array([0, 1, 2, 1, 2, 5, 2, 3, 3, 6, 7, 7, 7, 7]) # may vary",
                "841": "    array([2, 1, 3, 4]) # may vary",
                "843": "    array([1, 2, 3, 4]) # may vary",
                "847": "    array([2, 1, 3, 4]) # may vary"
            },
            "comment_deleted_diff": {},
            "comment_modified_diff": {
                "743": "    array([0, 1, 2, 1, 2, 5, 2, 3, 3, 6, 7, 7, 7, 7])",
                "841": "    array([2, 1, 3, 4])",
                "843": "    array([1, 2, 3, 4])",
                "847": "    array([2, 1, 3, 4])"
            }
        }
    ],
    "setup.py": [
        {
            "commit": "4730b6f33f5c699ae1a49471e7e60f81ed5494b5",
            "timestamp": "2022-10-19T15:55:59-07:00",
            "author": "Developer-Ecosystem-Engineering",
            "commit_message": "review feedback, make lint happy",
            "additions": 17,
            "deletions": 20,
            "change_type": "MODIFY",
            "diff": "@@ -173,6 +173,16 @@ def check_funcs(funcs_name, headers=[\"feature_detection_math.h\"]):\n         else:\n             return 1\n \n+    # GH-14787: Work around GCC<8.4 bug when compiling with AVX512\n+    # support on Windows-based platforms\n+    def check_gh14787(fn):\n+        if fn == 'attribute_target_avx512f':\n+            if (sys.platform in ('win32', 'cygwin') and\n+                    config.check_compiler_gcc() and\n+                    not config.check_gcc_version_at_least(8, 4)):\n+                ext.extra_compile_args.extend(\n+                        ['-ffixed-xmm%s' % n for n in range(16, 32)])\n+\n     #use_msvc = config.check_decl(\"_MSC_VER\")\n     if not check_funcs_once(MANDATORY_FUNCS, add_to_moredefs=False):\n         raise SystemError(\"One of the required function to build numpy is not\"\n@@ -223,31 +233,18 @@ def check_funcs(funcs_name, headers=[\"feature_detection_math.h\"]):\n     for dec, fn in OPTIONAL_FUNCTION_ATTRIBUTES:\n         if config.check_gcc_function_attribute(dec, fn):\n             moredefs.append((fname2def(fn), 1))\n-            if fn == 'attribute_target_avx512f':\n-                # GH-14787: Work around GCC<8.4 bug when compiling with AVX512\n-                # support on Windows-based platforms\n-                if (sys.platform in ('win32', 'cygwin') and\n-                        config.check_compiler_gcc() and\n-                        not config.check_gcc_version_at_least(8, 4)):\n-                    ext.extra_compile_args.extend(\n-                            ['-ffixed-xmm%s' % n for n in range(16, 32)])\n+            check_gh14787(fn)\n \n     platform = sysconfig.get_platform()\n-    if(\"x86_64\" in platform):\n+    if (\"x86_64\" in platform):\n         for dec, fn in OPTIONAL_FUNCTION_ATTRIBUTES_AVX:\n             if config.check_gcc_function_attribute(dec, fn):\n                 moredefs.append((fname2def(fn), 1))\n-                if fn == 'attribute_target_avx512f':\n-                    # GH-14787: Work around GCC<8.4 bug when compiling with AVX512\n-                    # support on Windows-based platforms\n-                    if (sys.platform in ('win32', 'cygwin') and\n-                            config.check_compiler_gcc() and\n-                            not config.check_gcc_version_at_least(8, 4)):\n-                        ext.extra_compile_args.extend(\n-                                ['-ffixed-xmm%s' % n for n in range(16, 32)])\n-        for dec, fn, code, header in OPTIONAL_FUNCTION_ATTRIBUTES_WITH_INTRINSICS_AVX:\n-            if config.check_gcc_function_attribute_with_intrinsics(dec, fn, code,\n-                                                                   header):\n+                check_gh14787(fn)\n+        for dec, fn, code, header in (\n+        OPTIONAL_FUNCTION_ATTRIBUTES_WITH_INTRINSICS_AVX):\n+            if config.check_gcc_function_attribute_with_intrinsics(\n+                    dec, fn, code, header):\n                 moredefs.append((fname2def(fn), 1))\n \n     for fn in OPTIONAL_VARIABLE_ATTRIBUTES:\n",
            "comment_added_diff": {
                "176": "    # GH-14787: Work around GCC<8.4 bug when compiling with AVX512",
                "177": "    # support on Windows-based platforms"
            },
            "comment_deleted_diff": {
                "227": "                # GH-14787: Work around GCC<8.4 bug when compiling with AVX512",
                "228": "                # support on Windows-based platforms",
                "241": "                    # GH-14787: Work around GCC<8.4 bug when compiling with AVX512",
                "242": "                    # support on Windows-based platforms"
            },
            "comment_modified_diff": {}
        },
        {
            "commit": "4002a7d421ff10780c28a3643683af7a9754f87f",
            "timestamp": "2022-11-25T12:37:46+01:00",
            "author": "Ralf Gommers",
            "commit_message": "BLD: enable building NumPy with Meson\n\nThis enables building with NumPy on Linux and macOS. Windows support\nshould be complete to, but is untested as of now and may need a few\ntweaks. This contains:\n- A set of `meson.build` files and related code generation script\n  tweaks, header templates, etc.\n- One CI job on Linux\n- Basic docs on using Meson to build NumPy (not yet integrated in the\n  html docs, it's too early for that - this is for early adopters right\n  now).\n\nThe build should be complete, with the major exception of SIMD support.\nThe full test suite passes. See gh-22546 for the tracking issue with\ndetailed notes on the plan for switching NumPy to Meson as its build\nsystem.\n\nCo-authored-by: Stefan van der Walt <stefanv@berkeley.edu>",
            "additions": 6,
            "deletions": 0,
            "change_type": "MODIFY",
            "diff": "@@ -45,6 +45,8 @@\n # in time is only in build. -- Charles Harris, 2013-03-30\n \n class CallOnceOnly:\n+    # NOTE: we don't need any of this in the Meson build,\n+    # it takes care of caching\n     def __init__(self):\n         self._check_types = None\n         self._check_ieee_macros = None\n@@ -177,6 +179,8 @@ def check_funcs(\n         else:\n             return 1\n \n+    # NOTE: not needed in Meson build, we set the minimum\n+    #       compiler version to 8.4 to avoid this bug\n     # GH-14787: Work around GCC<8.4 bug when compiling with AVX512\n     # support on Windows-based platforms\n     def check_gh14787(fn):\n@@ -427,6 +431,8 @@ def check_types(config_cmd, ext, build_dir):\n \n     return private_defines, public_defines\n \n+# NOTE: this isn't needed in the Meson build,\n+#       and we won't support a MATHLIB env var\n def check_mathlib(config_cmd):\n     # Testing the C math library\n     mathlibs = []\n",
            "comment_added_diff": {
                "48": "    # NOTE: we don't need any of this in the Meson build,",
                "49": "    # it takes care of caching",
                "182": "    # NOTE: not needed in Meson build, we set the minimum",
                "183": "    #       compiler version to 8.4 to avoid this bug",
                "434": "# NOTE: this isn't needed in the Meson build,",
                "435": "#       and we won't support a MATHLIB env var"
            },
            "comment_deleted_diff": {},
            "comment_modified_diff": {}
        },
        {
            "commit": "b64727c0ccfe60a2ed668eb1923d60f433558580",
            "timestamp": "2022-11-27T12:15:46+02:00",
            "author": "mattip",
            "commit_message": "BUILD: revert function() -> #define for 3 npymath functions",
            "additions": 5,
            "deletions": 1,
            "change_type": "MODIFY",
            "diff": "@@ -770,7 +770,11 @@ def get_mathlib_info(*args):\n                        # join('src', 'npymath', 'ieee754.cpp'),\n                        join('src', 'npymath', 'ieee754.c.src'),\n                        join('src', 'npymath', 'npy_math_complex.c.src'),\n-                       join('src', 'npymath', 'halffloat.c')\n+                       join('src', 'npymath', 'halffloat.c'),\n+                       # Remove this once scipy macos arm64 build correctly\n+                       # links to the arm64 npymath library,\n+                       # see gh-22673\n+                       join('src', 'npymath', 'arm64_exports.c'),\n                        ]\n \n     config.add_installed_library('npymath',\n",
            "comment_added_diff": {
                "774": "                       # Remove this once scipy macos arm64 build correctly",
                "775": "                       # links to the arm64 npymath library,",
                "776": "                       # see gh-22673"
            },
            "comment_deleted_diff": {},
            "comment_modified_diff": {}
        },
        {
            "commit": "c687f2d16f8ba965828fee3a001844b4952474f5",
            "timestamp": "2022-11-28T17:06:11+01:00",
            "author": "Matti Picus",
            "commit_message": "MAINT: npymath cleanups for isnan, isinf, isinfinite, signbit, nextafter (#22684)\n\n* make isnan, isinf, isfinite, signbit, nextafter aliases\r\n\r\n* fixes from review\r\n\r\nCo-authored-by: Sebastian Berg <sebastianb@nvidia.com>",
            "additions": 1,
            "deletions": 49,
            "change_type": "MODIFY",
            "diff": "@@ -60,14 +60,6 @@ def check_types(self, *a, **kw):\n             out = copy.deepcopy(pickle.loads(self._check_types))\n         return out\n \n-    def check_ieee_macros(self, *a, **kw):\n-        if self._check_ieee_macros is None:\n-            out = check_ieee_macros(*a, **kw)\n-            self._check_ieee_macros = pickle.dumps(out)\n-        else:\n-            out = copy.deepcopy(pickle.loads(self._check_ieee_macros))\n-        return out\n-\n     def check_complex(self, *a, **kw):\n         if self._check_complex is None:\n             out = check_complex(*a, **kw)\n@@ -293,43 +285,6 @@ def check_prec(prec):\n \n     return priv, pub\n \n-def check_ieee_macros(config):\n-    priv = []\n-    pub = []\n-\n-    macros = []\n-\n-    def _add_decl(f):\n-        priv.append(fname2def(\"decl_%s\" % f))\n-        pub.append('NPY_%s' % fname2def(\"decl_%s\" % f))\n-\n-    # XXX: hack to circumvent cpp pollution from python: python put its\n-    # config.h in the public namespace, so we have a clash for the common\n-    # functions we test. We remove every function tested by python's\n-    # autoconf, hoping their own test are correct\n-    _macros = [\"isnan\", \"isinf\", \"signbit\", \"isfinite\"]\n-    for f in _macros:\n-        py_symbol = fname2def(\"decl_%s\" % f)\n-        already_declared = config.check_decl(py_symbol,\n-                headers=[\"Python.h\", \"math.h\"])\n-        if already_declared:\n-            if config.check_macro_true(py_symbol,\n-                    headers=[\"Python.h\", \"math.h\"]):\n-                pub.append('NPY_%s' % fname2def(\"decl_%s\" % f))\n-        else:\n-            macros.append(f)\n-    # Normally, isnan and isinf are macro (C99), but some platforms only have\n-    # func, or both func and macro version. Check for macro only, and define\n-    # replacement ones if not found.\n-    # Note: including Python.h is necessary because it modifies some math.h\n-    # definitions\n-    for f in macros:\n-        st = config.check_decl(f, headers=[\"Python.h\", \"math.h\"])\n-        if st:\n-            _add_decl(f)\n-\n-    return priv, pub\n-\n def check_types(config_cmd, ext, build_dir):\n     private_defines = []\n     public_defines = []\n@@ -510,7 +465,6 @@ def generate_config_h(ext, build_dir):\n             moredefs.append(('MATHLIB', ','.join(mathlibs)))\n \n             check_math_capabilities(config_cmd, ext, moredefs, mathlibs)\n-            moredefs.extend(cocache.check_ieee_macros(config_cmd)[0])\n             moredefs.extend(cocache.check_complex(config_cmd, mathlibs)[0])\n \n             # Signal check\n@@ -629,7 +583,6 @@ def generate_numpyconfig_h(ext, build_dir):\n                 moredefs.append(('NPY_NO_SMP', 0))\n \n             mathlibs = check_mathlib(config_cmd)\n-            moredefs.extend(cocache.check_ieee_macros(config_cmd)[1])\n             moredefs.extend(cocache.check_complex(config_cmd, mathlibs)[1])\n \n             if NPY_RELAXED_STRIDES_DEBUG:\n@@ -710,8 +663,7 @@ def generate_api(ext, build_dir):\n \n     config.numpy_include_dirs.extend(config.paths('include'))\n \n-    deps = [join('src', 'npymath', '_signbit.c'),\n-            join('include', 'numpy', '*object.h'),\n+    deps = [join('include', 'numpy', '*object.h'),\n             join(codegen_dir, 'genapi.py'),\n             ]\n \n",
            "comment_added_diff": {},
            "comment_deleted_diff": {
                "306": "    # XXX: hack to circumvent cpp pollution from python: python put its",
                "307": "    # config.h in the public namespace, so we have a clash for the common",
                "308": "    # functions we test. We remove every function tested by python's",
                "309": "    # autoconf, hoping their own test are correct",
                "321": "    # Normally, isnan and isinf are macro (C99), but some platforms only have",
                "322": "    # func, or both func and macro version. Check for macro only, and define",
                "323": "    # replacement ones if not found.",
                "324": "    # Note: including Python.h is necessary because it modifies some math.h",
                "325": "    # definitions"
            },
            "comment_modified_diff": {}
        },
        {
            "commit": "bf148bf9cd1e5cde05353bfdbe11124523555f5c",
            "timestamp": "2022-11-29T12:08:46+01:00",
            "author": "Ralf Gommers",
            "commit_message": "BLD: add workaround in setup.py for newer setuptools",
            "additions": 2,
            "deletions": 0,
            "change_type": "MODIFY",
            "diff": "@@ -463,6 +463,8 @@ def setup_package():\n     else:\n         #from numpy.distutils.core import setup\n         from setuptools import setup\n+        # workaround for broken --no-build-isolation with newer setuptools, see gh-21288\n+        metadata[\"packages\"] = []\n \n     try:\n         setup(**metadata)\n",
            "comment_added_diff": {
                "466": "        # workaround for broken --no-build-isolation with newer setuptools, see gh-21288"
            },
            "comment_deleted_diff": {},
            "comment_modified_diff": {}
        },
        {
            "commit": "fe31f6b309eb9239a91c6179badb4776fd40427e",
            "timestamp": "2022-11-29T10:38:57-08:00",
            "author": "Stefan van der Walt",
            "commit_message": "STY: satisfy linter",
            "additions": 2,
            "deletions": 1,
            "change_type": "MODIFY",
            "diff": "@@ -463,7 +463,8 @@ def setup_package():\n     else:\n         #from numpy.distutils.core import setup\n         from setuptools import setup\n-        # workaround for broken --no-build-isolation with newer setuptools, see gh-21288\n+        # workaround for broken --no-build-isolation with newer setuptools,\n+        # see gh-21288\n         metadata[\"packages\"] = []\n \n     try:\n",
            "comment_added_diff": {
                "466": "        # workaround for broken --no-build-isolation with newer setuptools,",
                "467": "        # see gh-21288"
            },
            "comment_deleted_diff": {
                "466": "        # workaround for broken --no-build-isolation with newer setuptools, see gh-21288"
            },
            "comment_modified_diff": {
                "466": "        # workaround for broken --no-build-isolation with newer setuptools, see gh-21288"
            }
        },
        {
            "commit": "d0a613cc50967d2f723111bb3b3ec83df606ddfd",
            "timestamp": "2023-01-05T14:48:54+01:00",
            "author": "Sebastian Berg",
            "commit_message": "MAINT: Move export for scipy arm64 helper into main module\n\nThis is a follow up to gh-22679 which addressed gh-22673.\n\nThe main thing is that we want the functions to be available after\nimporting NumPy, so they need to be part of multiarray.\nHowever, `npymath` is a static library, so the symbols are not really\nexported there.  The former PR did actually work in practice but this\nseems like it is technically the right place?\n\nFor some reason, I had to add nextafter to be able to do:\n\n    from scipy.spatial.distance import euclidean\n\nwith the SciPy 1.9.3 wheels.  SciPy test collection works with this for\nthe 1.9.3 wheel, so this should be all the symbols hopefully.",
            "additions": 4,
            "deletions": 4,
            "change_type": "MODIFY",
            "diff": "@@ -729,10 +729,6 @@ def get_mathlib_info(*args):\n                        join('src', 'npymath', 'ieee754.c.src'),\n                        join('src', 'npymath', 'npy_math_complex.c.src'),\n                        join('src', 'npymath', 'halffloat.c'),\n-                       # Remove this once scipy macos arm64 build correctly\n-                       # links to the arm64 npymath library,\n-                       # see gh-22673\n-                       join('src', 'npymath', 'arm64_exports.c'),\n                        ]\n \n     config.add_installed_library('npymath',\n@@ -964,6 +960,10 @@ def get_mathlib_info(*args):\n             join('src', 'multiarray', 'textreading', 'stream_pyobject.c'),\n             join('src', 'multiarray', 'textreading', 'str_to_int.c'),\n             join('src', 'multiarray', 'textreading', 'tokenize.cpp'),\n+            # Remove this once scipy macos arm64 build correctly\n+            # links to the arm64 npymath library,\n+            # see gh-22673\n+            join('src', 'npymath', 'arm64_exports.c'),\n             ]\n \n     #######################################################################\n",
            "comment_added_diff": {
                "963": "            # Remove this once scipy macos arm64 build correctly",
                "964": "            # links to the arm64 npymath library,",
                "965": "            # see gh-22673"
            },
            "comment_deleted_diff": {
                "732": "                       # Remove this once scipy macos arm64 build correctly",
                "733": "                       # links to the arm64 npymath library,",
                "734": "                       # see gh-22673"
            },
            "comment_modified_diff": {}
        },
        {
            "commit": "92bd9902d4233d9f5befe05fd47bfb8b2d4e102a",
            "timestamp": "2023-01-30T13:38:39-08:00",
            "author": "Raghuveer Devulapalli",
            "commit_message": "MAINT: Disable AVX-512 qsort on macOS and WIN32",
            "additions": 17,
            "deletions": 1,
            "change_type": "MODIFY",
            "diff": "@@ -68,6 +68,15 @@ def check_complex(self, *a, **kw):\n             out = copy.deepcopy(pickle.loads(self._check_complex))\n         return out\n \n+# Temporarily disable AVX512 sorting on WIN32 and macOS until we can figure\n+# out why the build fails\n+def enable_avx512_qsort():\n+    enable = True\n+    platform = sysconfig.get_platform()\n+    if \"win32\" in platform or \"macos\" in platform:\n+        enable = False\n+    return enable\n+\n def can_link_svml():\n     \"\"\"SVML library is supported only on x86_64 architecture and currently\n     only on linux\n@@ -484,6 +493,9 @@ def generate_config_h(ext, build_dir):\n             if can_link_svml():\n                 moredefs.append(('NPY_CAN_LINK_SVML', 1))\n \n+            if enable_avx512_qsort():\n+                moredefs.append(('NPY_ENABLE_AVX512_QSORT', 1))\n+\n             # Use bogus stride debug aid to flush out bugs where users use\n             # strides of dimensions with length 1 to index a full contiguous\n             # array.\n@@ -943,7 +955,6 @@ def get_mathlib_info(*args):\n             join('src', 'multiarray', 'usertypes.c'),\n             join('src', 'multiarray', 'vdot.c'),\n             join('src', 'common', 'npy_sort.h.src'),\n-            join('src', 'npysort', 'x86-qsort-skx.dispatch.cpp'),\n             join('src', 'npysort', 'quicksort.cpp'),\n             join('src', 'npysort', 'mergesort.cpp'),\n             join('src', 'npysort', 'timsort.cpp'),\n@@ -967,6 +978,11 @@ def get_mathlib_info(*args):\n             join('src', 'npymath', 'arm64_exports.c'),\n             ]\n \n+    if enable_avx512_qsort():\n+        multiarray_src += [\n+                join('src', 'npysort', 'x86-qsort-skx.dispatch.cpp'),\n+                ]\n+\n     #######################################################################\n     #             _multiarray_umath module - umath part                   #\n     #######################################################################\n",
            "comment_added_diff": {
                "71": "# Temporarily disable AVX512 sorting on WIN32 and macOS until we can figure",
                "72": "# out why the build fails"
            },
            "comment_deleted_diff": {},
            "comment_modified_diff": {}
        },
        {
            "commit": "c71352232164ab7ddc4142ebc1db694493b34ff9",
            "timestamp": "2023-01-30T13:38:39-08:00",
            "author": "Raghuveer Devulapalli",
            "commit_message": "MAINT: Fix comment",
            "additions": 3,
            "deletions": 4,
            "change_type": "MODIFY",
            "diff": "@@ -68,12 +68,11 @@ def check_complex(self, *a, **kw):\n             out = copy.deepcopy(pickle.loads(self._check_complex))\n         return out\n \n-# Temporarily disable AVX512 sorting on WIN32 and macOS until we can figure\n-# out why the build fails\n+# Temporarily disable AVX512 sorting on WIN32 until we can figure\n+# out why it has test failures\n def enable_avx512_qsort():\n     enable = True\n-    platform = sysconfig.get_platform()\n-    if \"win32\" in platform:\n+    if \"win32\" in sysconfig.get_platform():\n         enable = False\n     return enable\n \n",
            "comment_added_diff": {
                "71": "# Temporarily disable AVX512 sorting on WIN32 until we can figure",
                "72": "# out why it has test failures"
            },
            "comment_deleted_diff": {
                "71": "# Temporarily disable AVX512 sorting on WIN32 and macOS until we can figure",
                "72": "# out why the build fails"
            },
            "comment_modified_diff": {
                "71": "# Temporarily disable AVX512 sorting on WIN32 and macOS until we can figure",
                "72": "# out why the build fails"
            }
        },
        {
            "commit": "7ddb5daa866984caa78e3fa4b5cd4869f4ee94cf",
            "timestamp": "2023-02-07T21:07:27+02:00",
            "author": "Sayed Adel",
            "commit_message": "ENH, SIMD: removes #NPY_ENABLE_AVX512_QSORT and use #directives instead",
            "additions": 2,
            "deletions": 17,
            "change_type": "MODIFY",
            "diff": "@@ -68,14 +68,6 @@ def check_complex(self, *a, **kw):\n             out = copy.deepcopy(pickle.loads(self._check_complex))\n         return out\n \n-# Temporarily disable AVX512 sorting on WIN32 until we can figure\n-# out why it has test failures\n-def enable_avx512_qsort():\n-    enable = True\n-    if \"win32\" in sysconfig.get_platform():\n-        enable = False\n-    return enable\n-\n def can_link_svml():\n     \"\"\"SVML library is supported only on x86_64 architecture and currently\n     only on linux\n@@ -492,9 +484,6 @@ def generate_config_h(ext, build_dir):\n             if can_link_svml():\n                 moredefs.append(('NPY_CAN_LINK_SVML', 1))\n \n-            if enable_avx512_qsort():\n-                moredefs.append(('NPY_ENABLE_AVX512_QSORT', 1))\n-\n             # Use bogus stride debug aid to flush out bugs where users use\n             # strides of dimensions with length 1 to index a full contiguous\n             # array.\n@@ -975,14 +964,10 @@ def get_mathlib_info(*args):\n             # links to the arm64 npymath library,\n             # see gh-22673\n             join('src', 'npymath', 'arm64_exports.c'),\n+            join('src', 'npysort', 'simd_qsort.dispatch.cpp'),\n+            join('src', 'npysort', 'simd_qsort_16bit.dispatch.cpp'),\n             ]\n \n-    if enable_avx512_qsort():\n-        multiarray_src += [\n-                join('src', 'npysort', 'simd_qsort.dispatch.cpp'),\n-                join('src', 'npysort', 'simd_qsort_16bit.dispatch.cpp'),\n-                ]\n-\n     #######################################################################\n     #             _multiarray_umath module - umath part                   #\n     #######################################################################\n",
            "comment_added_diff": {},
            "comment_deleted_diff": {
                "71": "# Temporarily disable AVX512 sorting on WIN32 until we can figure",
                "72": "# out why it has test failures"
            },
            "comment_modified_diff": {}
        },
        {
            "commit": "866f41a85bddfa3ea6de551bb27f335b0f8a6a52",
            "timestamp": "2023-02-20T04:07:15+02:00",
            "author": "Sayed Adel",
            "commit_message": "MAINT, SIMD: Removes compiler definitions of attribute-based CPU dispatching",
            "additions": 1,
            "deletions": 24,
            "change_type": "MODIFY",
            "diff": "@@ -171,18 +171,6 @@ def check_funcs(\n         else:\n             return 1\n \n-    # NOTE: not needed in Meson build, we set the minimum\n-    #       compiler version to 8.4 to avoid this bug\n-    # GH-14787: Work around GCC<8.4 bug when compiling with AVX512\n-    # support on Windows-based platforms\n-    def check_gh14787(fn):\n-        if fn == 'attribute_target_avx512f':\n-            if (sys.platform in ('win32', 'cygwin') and\n-                    config.check_compiler_gcc() and\n-                    not config.check_gcc_version_at_least(8, 4)):\n-                ext.extra_compile_args.extend(\n-                        ['-ffixed-xmm%s' % n for n in range(16, 32)])\n-\n     #use_msvc = config.check_decl(\"_MSC_VER\")\n     if not check_funcs_once(MANDATORY_FUNCS, add_to_moredefs=False):\n         raise SystemError(\"One of the required function to build numpy is not\"\n@@ -233,20 +221,8 @@ def check_gh14787(fn):\n     for dec, fn in OPTIONAL_FUNCTION_ATTRIBUTES:\n         if config.check_gcc_function_attribute(dec, fn):\n             moredefs.append((fname2def(fn), 1))\n-            check_gh14787(fn)\n \n     platform = sysconfig.get_platform()\n-    if (\"x86_64\" in platform):\n-        for dec, fn in OPTIONAL_FUNCTION_ATTRIBUTES_AVX:\n-            if config.check_gcc_function_attribute(dec, fn):\n-                moredefs.append((fname2def(fn), 1))\n-                check_gh14787(fn)\n-        for dec, fn, code, header in (\n-        OPTIONAL_FUNCTION_ATTRIBUTES_WITH_INTRINSICS_AVX):\n-            if config.check_gcc_function_attribute_with_intrinsics(\n-                    dec, fn, code, header):\n-                moredefs.append((fname2def(fn), 1))\n-\n     for fn in OPTIONAL_VARIABLE_ATTRIBUTES:\n         if config.check_gcc_variable_attribute(fn):\n             m = fn.replace(\"(\", \"_\").replace(\")\", \"_\")\n@@ -1019,6 +995,7 @@ def generate_umath_doc_header(ext, build_dir):\n             join('src', 'umath', 'loops_modulo.dispatch.c.src'),\n             join('src', 'umath', 'loops_comparison.dispatch.c.src'),\n             join('src', 'umath', 'loops_unary_complex.dispatch.c.src'),\n+            join('src', 'umath', 'loops_autovec_int.dispatch.c.src'),\n             join('src', 'umath', 'matmul.h.src'),\n             join('src', 'umath', 'matmul.c.src'),\n             join('src', 'umath', 'clip.h'),\n",
            "comment_added_diff": {},
            "comment_deleted_diff": {
                "174": "    # NOTE: not needed in Meson build, we set the minimum",
                "175": "    #       compiler version to 8.4 to avoid this bug",
                "176": "    # GH-14787: Work around GCC<8.4 bug when compiling with AVX512",
                "177": "    # support on Windows-based platforms"
            },
            "comment_modified_diff": {}
        },
        {
            "commit": "d183edf54e3c74c52471c694068ec7fcc5f7aa34",
            "timestamp": "2023-04-04T04:13:01+02:00",
            "author": "Sayed Adel",
            "commit_message": "ENH: Raise C++ standard to C++17",
            "additions": 1,
            "deletions": 41,
            "change_type": "MODIFY",
            "diff": "@@ -405,7 +405,6 @@ def configuration(parent_package='',top_path=None):\n                                            exec_mod_from_location)\n     from numpy.distutils.system_info import (get_info, blas_opt_info,\n                                              lapack_opt_info)\n-    from numpy.distutils.ccompiler_opt import NPY_CXX_FLAGS\n     from numpy.version import release as is_released\n \n     config = Configuration('core', parent_package, top_path)\n@@ -658,44 +657,6 @@ def get_mathlib_info(*args):\n         # but we cannot use add_installed_pkg_config here either, so we only\n         # update the substitution dictionary during npymath build\n         config_cmd = config.get_config_cmd()\n-        # Check that the toolchain works, to fail early if it doesn't\n-        # (avoid late errors with MATHLIB which are confusing if the\n-        # compiler does not work).\n-        for lang, test_code, note in (\n-            ('c', 'int main(void) { return 0;}', ''),\n-            ('c++', (\n-                    'int main(void)'\n-                    '{ auto x = 0.0; return static_cast<int>(x); }'\n-                ), (\n-                    'note: A compiler with support for C++11 language '\n-                    'features is required.'\n-                )\n-             ),\n-        ):\n-            is_cpp = lang == 'c++'\n-            if is_cpp:\n-                # this a workaround to get rid of invalid c++ flags\n-                # without doing big changes to config.\n-                # c tested first, compiler should be here\n-                bk_c = config_cmd.compiler\n-                config_cmd.compiler = bk_c.cxx_compiler()\n-\n-                # Check that Linux compiler actually support the default flags\n-                if hasattr(config_cmd.compiler, 'compiler'):\n-                    config_cmd.compiler.compiler.extend(NPY_CXX_FLAGS)\n-                    config_cmd.compiler.compiler_so.extend(NPY_CXX_FLAGS)\n-\n-            st = config_cmd.try_link(test_code, lang=lang)\n-            if not st:\n-                # rerun the failing command in verbose mode\n-                config_cmd.compiler.verbose = True\n-                config_cmd.try_link(test_code, lang=lang)\n-                raise RuntimeError(\n-                    f\"Broken toolchain: cannot link a simple {lang.upper()} \"\n-                    f\"program. {note}\"\n-                )\n-            if is_cpp:\n-                config_cmd.compiler = bk_c\n         mlibs = check_mathlib(config_cmd)\n \n         posix_mlib = ' '.join(['-l%s' % l for l in mlibs])\n@@ -1067,8 +1028,7 @@ def generate_umath_doc_header(ext, build_dir):\n                                 common_deps,\n                          libraries=['npymath'],\n                          extra_objects=svml_objs,\n-                         extra_info=extra_info,\n-                         extra_cxx_compile_args=NPY_CXX_FLAGS)\n+                         extra_info=extra_info)\n \n     #######################################################################\n     #                        umath_tests module                           #\n",
            "comment_added_diff": {},
            "comment_deleted_diff": {
                "661": "        # Check that the toolchain works, to fail early if it doesn't",
                "662": "        # (avoid late errors with MATHLIB which are confusing if the",
                "663": "        # compiler does not work).",
                "677": "                # this a workaround to get rid of invalid c++ flags",
                "678": "                # without doing big changes to config.",
                "679": "                # c tested first, compiler should be here",
                "683": "                # Check that Linux compiler actually support the default flags",
                "690": "                # rerun the failing command in verbose mode"
            },
            "comment_modified_diff": {}
        },
        {
            "commit": "d183edf54e3c74c52471c694068ec7fcc5f7aa34",
            "timestamp": "2023-04-04T04:13:01+02:00",
            "author": "Sayed Adel",
            "commit_message": "ENH: Raise C++ standard to C++17",
            "additions": 115,
            "deletions": 23,
            "change_type": "MODIFY",
            "diff": "@@ -12,7 +12,9 @@\n import warnings\n import builtins\n import re\n+import tempfile\n \n+from distutils.errors import CompileError\n \n # Python supported version checks. Keep right after stdlib imports to ensure we\n # get a sensible error for older Python versions\n@@ -184,45 +186,135 @@ def run(self):\n \n def get_build_overrides():\n     \"\"\"\n-    Custom build commands to add `-std=c99` to compilation\n+    Custom build commands to add std flags if required to compilation\n     \"\"\"\n     from numpy.distutils.command.build_clib import build_clib\n     from numpy.distutils.command.build_ext import build_ext\n     from numpy._utils import _pep440\n \n-    def _needs_gcc_c99_flag(obj):\n-        if obj.compiler.compiler_type != 'unix':\n-            return False\n+    def try_compile(compiler, file, flags = [], verbose=False):\n+        # To bypass trapping warnings by Travis CI\n+        if getattr(compiler, 'compiler_type', '') == 'unix':\n+            flags = ['-Werror'] + flags\n+        bk_ver = getattr(compiler, 'verbose', False)\n+        compiler.verbose = verbose\n+        try:\n+            compiler.compile([file], extra_postargs=flags)\n+            return True, ''\n+        except CompileError as e:\n+            return False, str(e)\n+        finally:\n+            compiler.verbose = bk_ver\n+\n+    def flags_is_required(compiler, is_cpp, flags, code):\n+        if is_cpp:\n+            compiler = compiler.cxx_compiler()\n+            suf = '.cpp'\n+        else:\n+            suf = '.c'\n+        with tempfile.TemporaryDirectory() as temp_dir:\n+            tmp_file = os.path.join(temp_dir, \"test\" + suf)\n+            with open(tmp_file, \"w+\") as f:\n+                f.write(code)\n+            # without specify any flags in case of the required\n+            # standard already supported by default, then there's\n+            # no need for passing the flags\n+            comp = try_compile(compiler, tmp_file)\n+            if not comp[0]:\n+                comp = try_compile(compiler, tmp_file, flags)\n+                if not comp[0]:\n+                    # rerun to verbose the error\n+                    try_compile(compiler, tmp_file, flags, True)\n+                    if is_cpp:\n+                        raise RuntimeError(\n+                            \"Broken toolchain during testing C++ compiler. \\n\"\n+                            \"A compiler with support for C++17 language \"\n+                            \"features is required.\\n\"\n+                            f\"Triggered the following error: {comp[1]}.\"\n+                        )\n+                    else:\n+                        raise RuntimeError(\n+                            \"Broken toolchain during testing C compiler. \\n\"\n+                            \"A compiler with support for C99 language \"\n+                            \"features is required.\\n\"\n+                            f\"Triggered the following error: {comp[1]}.\"\n+                        )\n+                return True\n+        return False\n \n-        cc = obj.compiler.compiler[0]\n-        if \"gcc\" not in cc:\n-            return False\n-\n-        # will print something like '4.2.1\\n'\n-        out = subprocess.run([cc, '-dumpversion'],\n-                             capture_output=True, text=True)\n-        # -std=c99 is default from this version on\n-        if _pep440.parse(out.stdout) >= _pep440.Version('5.0'):\n-            return False\n-        return True\n+    def std_cxx_flags(cmd):\n+        compiler = cmd.compiler\n+        flags = getattr(compiler, '__np_cache_cpp_flags', None)\n+        if flags is not None:\n+            return flags\n+        flags = dict(\n+            msvc = ['/std:c++17']\n+        ).get(compiler.compiler_type, ['-std=c++17'])\n+        # These flags are used to compile any C++ source within Numpy.\n+        # They are chosen to have very few runtime dependencies.\n+        extra_flags = dict(\n+            # to update #def __cplusplus with enabled C++ version\n+            msvc = ['/Zc:__cplusplus']\n+        ).get(compiler.compiler_type, [\n+            # The following flag is used to avoid emit any extra code\n+            # from STL since extensions are build by C linker and\n+            # without C++ runtime dependencies.\n+            '-fno-threadsafe-statics',\n+            '-D__STDC_VERSION__=0',  # for compatibility with C headers\n+            '-fno-exceptions',  # no exception support\n+            '-fno-rtti'  # no runtime type information\n+        ])\n+        if not flags_is_required(compiler, True, flags, textwrap.dedent('''\n+            #include <type_traits>\n+            template<typename ...T>\n+            constexpr bool test_fold = (... && std::is_const_v<T>);\n+            int main()\n+            {\n+                if constexpr (test_fold<int, const int>) {\n+                    return 0;\n+                }\n+                else {\n+                    return -1;\n+                }\n+            }\n+        ''')):\n+            flags.clear()\n+        flags += extra_flags\n+        setattr(compiler, '__np_cache_cpp_flags', flags)\n+        return flags\n+\n+    def std_c_flags(cmd):\n+        compiler = cmd.compiler\n+        flags = getattr(compiler, '__np_cache_c_flags', None)\n+        if flags is not None:\n+            return flags\n+        flags = dict(\n+            msvc = []\n+        ).get(compiler.compiler_type, ['-std=c99'])\n+\n+        if not flags_is_required(compiler, False, flags, textwrap.dedent('''\n+            inline int test_inline() { return 0; }\n+            int main(void)\n+            { return test_inline(); }\n+        ''')):\n+            flags.clear()\n+\n+        setattr(compiler, '__np_cache_c_flags', flags)\n+        return flags\n \n     class new_build_clib(build_clib):\n         def build_a_library(self, build_info, lib_name, libraries):\n-            from numpy.distutils.ccompiler_opt import NPY_CXX_FLAGS\n-            if _needs_gcc_c99_flag(self):\n-                build_info['extra_cflags'] = ['-std=c99']\n-            build_info['extra_cxxflags'] = NPY_CXX_FLAGS\n+            build_info['extra_cflags'] = std_c_flags(self)\n+            build_info['extra_cxxflags'] = std_cxx_flags(self)\n             build_clib.build_a_library(self, build_info, lib_name, libraries)\n \n     class new_build_ext(build_ext):\n         def build_extension(self, ext):\n-            if _needs_gcc_c99_flag(self):\n-                if '-std=c99' not in ext.extra_compile_args:\n-                    ext.extra_compile_args.append('-std=c99')\n+            ext.extra_c_compile_args += std_c_flags(self)\n+            ext.extra_cxx_compile_args += std_cxx_flags(self)\n             build_ext.build_extension(self, ext)\n     return new_build_clib, new_build_ext\n \n-\n def generate_cython():\n     # Check Cython version\n     from numpy._utils import _pep440\n",
            "comment_added_diff": {
                "196": "        # To bypass trapping warnings by Travis CI",
                "219": "            # without specify any flags in case of the required",
                "220": "            # standard already supported by default, then there's",
                "221": "            # no need for passing the flags",
                "226": "                    # rerun to verbose the error",
                "253": "        # These flags are used to compile any C++ source within Numpy.",
                "254": "        # They are chosen to have very few runtime dependencies.",
                "256": "            # to update #def __cplusplus with enabled C++ version",
                "259": "            # The following flag is used to avoid emit any extra code",
                "260": "            # from STL since extensions are build by C linker and",
                "261": "            # without C++ runtime dependencies.",
                "263": "            '-D__STDC_VERSION__=0',  # for compatibility with C headers",
                "264": "            '-fno-exceptions',  # no exception support",
                "265": "            '-fno-rtti'  # no runtime type information",
                "268": "            #include <type_traits>"
            },
            "comment_deleted_diff": {
                "201": "        # will print something like '4.2.1\\n'",
                "204": "        # -std=c99 is default from this version on"
            },
            "comment_modified_diff": {
                "219": "            if _needs_gcc_c99_flag(self):",
                "220": "                if '-std=c99' not in ext.extra_compile_args:",
                "221": "                    ext.extra_compile_args.append('-std=c99')"
            }
        },
        {
            "commit": "0a0240bcdad5daa0b84781719b3f8a002ef0f82b",
            "timestamp": "2023-04-16T22:36:03+01:00",
            "author": "Ralf Gommers",
            "commit_message": "BLD: use the C++ linker to link `_multiarray_umath.so`\n\nThis gets rid of undefined symbol issues for `assert`.\n\nCloses gh-23122\nCloses gh-23595",
            "additions": 0,
            "deletions": 3,
            "change_type": "MODIFY",
            "diff": "@@ -1010,9 +1010,6 @@ def generate_umath_doc_header(ext, build_dir):\n         svml_objs.sort()\n \n     config.add_extension('_multiarray_umath',\n-                         # Forcing C language even though we have C++ sources.\n-                         # It forces the C linker and don't link C++ runtime.\n-                         language = 'c',\n                          sources=multiarray_src + umath_src +\n                                  common_src +\n                                  [generate_config_h,\n",
            "comment_added_diff": {},
            "comment_deleted_diff": {
                "1013": "                         # Forcing C language even though we have C++ sources.",
                "1014": "                         # It forces the C linker and don't link C++ runtime."
            },
            "comment_modified_diff": {}
        },
        {
            "commit": "bcc9c51c167a7bfaa8475b146a5bebc03213687f",
            "timestamp": "2023-04-24T15:11:54+02:00",
            "author": "Sayed Adel",
            "commit_message": "BUG: Avoid uses -Werror during tests default C/C++ standards\n\n  This may break the test if any unsupported\n  compiler options are specified.\n\n  This patch also removes the testing of constexpr and keeps only fold\n  expressions and inline variables to avoid triggering the constexpr\n  warning warning: \u2018if constexpr\u2019 only available with \u2018-std=c++17\u2019 or\n  \u2018-std=gnu++17 by Travis CI, which can cause the build to fail due to warning trapping.",
            "additions": 1,
            "deletions": 4,
            "change_type": "MODIFY",
            "diff": "@@ -193,9 +193,6 @@ def get_build_overrides():\n     from numpy._utils import _pep440\n \n     def try_compile(compiler, file, flags = [], verbose=False):\n-        # To bypass trapping warnings by Travis CI\n-        if getattr(compiler, 'compiler_type', '') == 'unix':\n-            flags = ['-Werror'] + flags\n         bk_ver = getattr(compiler, 'verbose', False)\n         compiler.verbose = verbose\n         try:\n@@ -270,7 +267,7 @@ def std_cxx_flags(cmd):\n             constexpr bool test_fold = (... && std::is_const_v<T>);\n             int main()\n             {\n-                if constexpr (test_fold<int, const int>) {\n+                if (test_fold<int, const int>) {\n                     return 0;\n                 }\n                 else {\n",
            "comment_added_diff": {},
            "comment_deleted_diff": {
                "196": "        # To bypass trapping warnings by Travis CI"
            },
            "comment_modified_diff": {}
        },
        {
            "commit": "372771b9658f33d75c02c974edca498f64b2b0a9",
            "timestamp": "2023-05-16T11:38:31+02:00",
            "author": "Sebastian Berg",
            "commit_message": "TST: Make sure cython tests define NPY_TARGET_VERSION to >=1.25",
            "additions": 5,
            "deletions": 1,
            "change_type": "MODIFY",
            "diff": "@@ -9,7 +9,11 @@\n from setuptools.extension import Extension\n import os\n \n-macros = [(\"NPY_NO_DEPRECATED_API\", 0)]\n+macros = [\n+    (\"NPY_NO_DEPRECATED_API\", 0),\n+    # Require 1.25+ to test datetime additions\n+    (\"NPY_TARGET_VERSION\", \"NPY_1_25_API_VERSION\"),\n+]\n \n checks = Extension(\n     \"checks\",\n",
            "comment_added_diff": {
                "14": "    # Require 1.25+ to test datetime additions"
            },
            "comment_deleted_diff": {},
            "comment_modified_diff": {}
        },
        {
            "commit": "200fa4b3c9e1475a52aa783ab227f75914680531",
            "timestamp": "2023-05-25T10:42:48-07:00",
            "author": "Brigitta Sip\u0151cz",
            "commit_message": "MAINT: removing all mentions of umath_tests",
            "additions": 1,
            "deletions": 1,
            "change_type": "MODIFY",
            "diff": "@@ -1029,7 +1029,7 @@ def generate_umath_doc_header(ext, build_dir):\n                          extra_info=extra_info)\n \n     #######################################################################\n-    #                        umath_tests module                           #\n+    #                       _umath_tests module                           #\n     #######################################################################\n \n     config.add_extension('_umath_tests', sources=[\n",
            "comment_added_diff": {
                "1032": "    #                       _umath_tests module                           #"
            },
            "comment_deleted_diff": {
                "1032": "    #                        umath_tests module                           #"
            },
            "comment_modified_diff": {
                "1032": "    #                        umath_tests module                           #"
            }
        },
        {
            "commit": "43c32e65d9534815435d56ddea93e60af9f25204",
            "timestamp": "2023-07-14T17:39:25+03:00",
            "author": "mattip",
            "commit_message": "fixes from review",
            "additions": 0,
            "deletions": 46,
            "change_type": "DELETE",
            "diff": "@@ -1,46 +0,0 @@\n-#!/usr/bin/env python3\n-\"\"\"\n-Build the Cython demonstrations of low-level access to NumPy random\n-\n-Usage: python setup.py build_ext -i\n-\"\"\"\n-from os.path import dirname, join, abspath\n-\n-from setuptools import setup\n-from setuptools.extension import Extension\n-\n-import numpy as np\n-from Cython.Build import cythonize\n-\n-\n-path = dirname(__file__)\n-src_dir = join(dirname(path), '..', 'src')\n-defs = [('NPY_NO_DEPRECATED_API', 0)]\n-inc_path = np.get_include()\n-# Add paths for npyrandom and npymath libraries:\n-lib_path = [\n-    abspath(join(np.get_include(), '..', '..', 'random', 'lib')),\n-    abspath(join(np.get_include(), '..', 'lib'))\n-]\n-\n-extending = Extension(\"extending\",\n-                      sources=[join('.', 'extending.pyx')],\n-                      include_dirs=[\n-                            np.get_include(),\n-                            join(path, '..', '..')\n-                        ],\n-                      define_macros=defs,\n-                      )\n-distributions = Extension(\"extending_distributions\",\n-                          sources=[join('.', 'extending_distributions.pyx')],\n-                          include_dirs=[inc_path],\n-                          library_dirs=lib_path,\n-                          libraries=['npyrandom', 'npymath'],\n-                          define_macros=defs,\n-                          )\n-\n-extensions = [extending, distributions]\n-\n-setup(\n-    ext_modules=cythonize(extensions)\n-)\n",
            "comment_added_diff": {},
            "comment_deleted_diff": {
                "1": "#!/usr/bin/env python3",
                "20": "# Add paths for npyrandom and npymath libraries:"
            },
            "comment_modified_diff": {}
        },
        {
            "commit": "be896c9c652c010093d26bd0ff732dbfabcd6361",
            "timestamp": "2023-08-11T22:40:16+02:00",
            "author": "Stefan van der Walt",
            "commit_message": "MAINT: Remove versioneer (#24196)\n\nCo-authored-by: Ralf Gommers <ralf.gommers@gmail.com>",
            "additions": 14,
            "deletions": 13,
            "change_type": "MODIFY",
            "diff": "@@ -22,9 +22,6 @@\n     raise RuntimeError(\"Python version >= 3.9 required.\")\n \n \n-import versioneer\n-\n-\n # This is a bit hackish: we are setting a global variable so that the main\n # numpy __init__ can detect if it is being loaded by the setup routine, to\n # avoid attempting to load components that aren't built yet.  While ugly, it's\n@@ -34,7 +31,16 @@\n # Needed for backwards code compatibility below and in some CI scripts.\n # The version components are changed from ints to strings, but only VERSION\n # seems to matter outside of this module and it was already a str.\n-FULLVERSION = versioneer.get_version()\n+FULLVERSION = subprocess.check_output([\n+    sys.executable,\n+    'numpy/_build_utils/gitversion.py'\n+]).strip().decode('ascii')\n+\n+# Write git version to disk\n+subprocess.check_output([\n+    sys.executable,\n+    'numpy/_build_utils/gitversion.py', '--write', 'numpy/version.py'\n+])\n \n # Capture the version string:\n # 1.22.0.dev0+ ... -> ISRELEASED == False, VERSION == 1.22.0\n@@ -80,10 +86,6 @@\n             raise RuntimeError(\"setuptools versions >= '60.0.0' require \"\n                     \"SETUPTOOLS_USE_DISTUTILS=stdlib in the environment\")\n \n-# Initialize cmdclass from versioneer\n-from numpy.distutils.core import numpy_cmdclass\n-cmdclass = versioneer.get_cmdclass(numpy_cmdclass)\n-\n CLASSIFIERS = \"\"\"\\\n Development Status :: 5 - Production/Stable\n Intended Audience :: Science/Research\n@@ -106,7 +108,6 @@\n Operating System :: MacOS\n \"\"\"\n \n-\n def configuration(parent_package='', top_path=None):\n     from numpy.distutils.misc_util import Configuration\n \n@@ -173,10 +174,9 @@ def __exit__(self, exception_type, exception_value, traceback):\n         with open(self.f1, 'w') as f:\n             f.write(self.bsd_text)\n \n-\n # Need to inherit from versioneer version of sdist to get the encoded\n # version information.\n-class sdist_checked(cmdclass['sdist']):\n+class sdist_checked:\n     \"\"\" check submodules on sdist to prevent incomplete tarballs \"\"\"\n     def run(self):\n         check_submodules()\n@@ -480,6 +480,8 @@ def get_docs_url():\n         return \"https://numpy.org/doc/{}.{}\".format(MAJOR, MINOR)\n \n \n+from numpy.distutils.core import numpy_cmdclass as cmdclass\n+\n def setup_package():\n     src_path = os.path.dirname(os.path.abspath(__file__))\n     old_path = os.getcwd()\n@@ -498,7 +500,6 @@ def setup_package():\n             'f2py%s.%s = numpy.f2py.f2py2e:main' % sys.version_info[:2],\n             ]\n \n-    cmdclass[\"sdist\"] = sdist_checked\n     metadata = dict(\n         name='numpy',\n         maintainer=\"NumPy Developers\",\n@@ -518,7 +519,7 @@ def setup_package():\n         classifiers=[_f for _f in CLASSIFIERS.split('\\n') if _f],\n         platforms=[\"Windows\", \"Linux\", \"Solaris\", \"Mac OS-X\", \"Unix\"],\n         test_suite='pytest',\n-        version=versioneer.get_version(),\n+        version=VERSION,\n         cmdclass=cmdclass,\n         python_requires='>=3.9',\n         zip_safe=False,\n",
            "comment_added_diff": {
                "39": "# Write git version to disk"
            },
            "comment_deleted_diff": {
                "83": "# Initialize cmdclass from versioneer"
            },
            "comment_modified_diff": {}
        },
        {
            "commit": "63a1fee8d1c1e442aa8ca08ac7cefb195f99b2a9",
            "timestamp": "2023-09-13T22:52:14+02:00",
            "author": "Ralf Gommers",
            "commit_message": "MAINT: remove all `setup.py`'s and related files for distutils builds",
            "additions": 0,
            "deletions": 1105,
            "change_type": "DELETE",
            "diff": "@@ -1,1105 +0,0 @@\n-import os\n-import sys\n-import sysconfig\n-import pickle\n-import copy\n-import warnings\n-import textwrap\n-import glob\n-from os.path import join\n-\n-from numpy.distutils import log\n-from numpy.distutils.msvccompiler import lib_opts_if_msvc\n-from distutils.dep_util import newer\n-from sysconfig import get_config_var\n-from setup_common import *  # noqa: F403\n-\n-# Set to True to enable relaxed strides checking. This (mostly) means\n-# that `strides[dim]` is ignored if `shape[dim] == 1` when setting flags.\n-NPY_RELAXED_STRIDES_CHECKING = (os.environ.get('NPY_RELAXED_STRIDES_CHECKING', \"1\") != \"0\")\n-if not NPY_RELAXED_STRIDES_CHECKING:\n-    raise SystemError(\n-        \"Support for NPY_RELAXED_STRIDES_CHECKING=0 has been removed as of \"\n-        \"NumPy 1.23.  This error will eventually be removed entirely.\")\n-\n-# Put NPY_RELAXED_STRIDES_DEBUG=1 in the environment if you want numpy to use a\n-# bogus value for affected strides in order to help smoke out bad stride usage\n-# when relaxed stride checking is enabled.\n-NPY_RELAXED_STRIDES_DEBUG = (os.environ.get('NPY_RELAXED_STRIDES_DEBUG', \"0\") != \"0\")\n-NPY_RELAXED_STRIDES_DEBUG = NPY_RELAXED_STRIDES_DEBUG and NPY_RELAXED_STRIDES_CHECKING\n-\n-# Set NPY_DISABLE_SVML=1 in the environment to disable the vendored SVML\n-# library. This option only has significance on a Linux x86_64 host and is most\n-# useful to avoid improperly requiring SVML when cross compiling.\n-NPY_DISABLE_SVML = (os.environ.get('NPY_DISABLE_SVML', \"0\") == \"1\")\n-\n-# XXX: ugly, we use a class to avoid calling twice some expensive functions in\n-# config.h/numpyconfig.h. I don't see a better way because distutils force\n-# config.h generation inside an Extension class, and as such sharing\n-# configuration information between extensions is not easy.\n-# Using a pickled-based memoize does not work because config_cmd is an instance\n-# method, which cPickle does not like.\n-#\n-# Use pickle in all cases, as cPickle is gone in python3 and the difference\n-# in time is only in build. -- Charles Harris, 2013-03-30\n-\n-class CallOnceOnly:\n-    # NOTE: we don't need any of this in the Meson build,\n-    # it takes care of caching\n-    def __init__(self):\n-        self._check_types = None\n-        self._check_ieee_macros = None\n-        self._check_complex = None\n-\n-    def check_types(self, *a, **kw):\n-        if self._check_types is None:\n-            out = check_types(*a, **kw)\n-            self._check_types = pickle.dumps(out)\n-        else:\n-            out = copy.deepcopy(pickle.loads(self._check_types))\n-        return out\n-\n-    def check_complex(self, *a, **kw):\n-        if self._check_complex is None:\n-            out = check_complex(*a, **kw)\n-            self._check_complex = pickle.dumps(out)\n-        else:\n-            out = copy.deepcopy(pickle.loads(self._check_complex))\n-        return out\n-\n-def can_link_svml():\n-    \"\"\"SVML library is supported only on x86_64 architecture and currently\n-    only on linux\n-    \"\"\"\n-    if NPY_DISABLE_SVML:\n-        return False\n-    platform = sysconfig.get_platform()\n-    return (\"x86_64\" in platform\n-            and \"linux\" in platform\n-            and sys.maxsize > 2**31)\n-\n-def can_link_svml_fp16():\n-    \"\"\"SVML FP16 requires binutils >= 2.38 for an updated assembler\n-    \"\"\"\n-    if can_link_svml():\n-        binutils_ver = os.popen(\"ld -v\").readlines()[0].strip()[-4:]\n-        return float(binutils_ver) >= 2.38\n-\n-def check_git_submodules():\n-    out = os.popen(\"git submodule status\")\n-    modules = out.readlines()\n-    for submodule in modules:\n-        if (submodule.strip()[0] == \"-\"):\n-            raise RuntimeError(\"git submodules are not initialized.\"\n-                    \"Please run `git submodule update --init` to fix this.\")\n-\n-def pythonlib_dir():\n-    \"\"\"return path where libpython* is.\"\"\"\n-    if sys.platform == 'win32':\n-        return os.path.join(sys.prefix, \"libs\")\n-    else:\n-        return get_config_var('LIBDIR')\n-\n-def is_npy_no_signal():\n-    \"\"\"Return True if the NPY_NO_SIGNAL symbol must be defined in configuration\n-    header.\"\"\"\n-    return sys.platform == 'win32'\n-\n-def is_npy_no_smp():\n-    \"\"\"Return True if the NPY_NO_SMP symbol must be defined in public\n-    header (when SMP support cannot be reliably enabled).\"\"\"\n-    # Perhaps a fancier check is in order here.\n-    #  so that threads are only enabled if there\n-    #  are actually multiple CPUS? -- but\n-    #  threaded code can be nice even on a single\n-    #  CPU so that long-calculating code doesn't\n-    #  block.\n-    return 'NPY_NOSMP' in os.environ\n-\n-def win32_checks(deflist):\n-    from numpy.distutils.misc_util import get_build_architecture\n-    a = get_build_architecture()\n-\n-    # Distutils hack on AMD64 on windows\n-    print('BUILD_ARCHITECTURE: %r, os.name=%r, sys.platform=%r' %\n-          (a, os.name, sys.platform))\n-    if a == 'AMD64':\n-        deflist.append('DISTUTILS_USE_SDK')\n-\n-    # On win32, force long double format string to be 'g', not\n-    # 'Lg', since the MS runtime does not support long double whose\n-    # size is > sizeof(double)\n-    if a == \"Intel\" or a == \"AMD64\":\n-        deflist.append('FORCE_NO_LONG_DOUBLE_FORMATTING')\n-\n-def check_math_capabilities(config, ext, moredefs, mathlibs):\n-    def check_func(\n-        func_name,\n-        decl=False,\n-        headers=[\"feature_detection_math.h\", \"feature_detection_cmath.h\"],\n-    ):\n-        return config.check_func(\n-            func_name,\n-            libraries=mathlibs,\n-            decl=decl,\n-            call=True,\n-            call_args=FUNC_CALL_ARGS[func_name],\n-            headers=headers,\n-        )\n-\n-    def check_funcs_once(\n-            funcs_name,\n-            headers=[\"feature_detection_math.h\", \"feature_detection_cmath.h\"],\n-            add_to_moredefs=True):\n-        call = dict([(f, True) for f in funcs_name])\n-        call_args = dict([(f, FUNC_CALL_ARGS[f]) for f in funcs_name])\n-        st = config.check_funcs_once(\n-            funcs_name,\n-            libraries=mathlibs,\n-            decl=False,\n-            call=call,\n-            call_args=call_args,\n-            headers=headers,\n-        )\n-        if st and add_to_moredefs:\n-            moredefs.extend([(fname2def(f), 1) for f in funcs_name])\n-        return st\n-\n-    def check_funcs(\n-            funcs_name,\n-            headers=[\"feature_detection_math.h\", \"feature_detection_cmath.h\"]):\n-        # Use check_funcs_once first, and if it does not work, test func per\n-        # func. Return success only if all the functions are available\n-        if not check_funcs_once(funcs_name, headers=headers):\n-            # Global check failed, check func per func\n-            for f in funcs_name:\n-                if check_func(f, headers=headers):\n-                    moredefs.append((fname2def(f), 1))\n-            return 0\n-        else:\n-            return 1\n-\n-    #use_msvc = config.check_decl(\"_MSC_VER\")\n-    if not check_funcs_once(MANDATORY_FUNCS, add_to_moredefs=False):\n-        raise SystemError(\"One of the required function to build numpy is not\"\n-                \" available (the list is %s).\" % str(MANDATORY_FUNCS))\n-\n-    # Standard functions which may not be available and for which we have a\n-    # replacement implementation. Note that some of these are C99 functions.\n-\n-    # XXX: hack to circumvent cpp pollution from python: python put its\n-    # config.h in the public namespace, so we have a clash for the common\n-    # functions we test. We remove every function tested by python's\n-    # autoconf, hoping their own test are correct\n-    for f in OPTIONAL_FUNCS_MAYBE:\n-        if config.check_decl(fname2def(f), headers=[\"Python.h\"]):\n-            OPTIONAL_FILE_FUNCS.remove(f)\n-\n-    check_funcs(OPTIONAL_FILE_FUNCS, headers=[\"feature_detection_stdio.h\"])\n-    check_funcs(OPTIONAL_MISC_FUNCS, headers=[\"feature_detection_misc.h\"])\n-\n-    for h in OPTIONAL_HEADERS:\n-        if config.check_func(\"\", decl=False, call=False, headers=[h]):\n-            h = h.replace(\".\", \"_\").replace(os.path.sep, \"_\")\n-            moredefs.append((fname2def(h), 1))\n-\n-    # Try with both \"locale.h\" and \"xlocale.h\"\n-    locale_headers = [\n-        \"stdlib.h\",\n-        \"xlocale.h\",\n-        \"feature_detection_locale.h\",\n-    ]\n-    if not check_funcs(OPTIONAL_LOCALE_FUNCS, headers=locale_headers):\n-        # It didn't work with xlocale.h, maybe it will work with locale.h?\n-        locale_headers[1] = \"locale.h\"\n-        check_funcs(OPTIONAL_LOCALE_FUNCS, headers=locale_headers)\n-\n-    for tup in OPTIONAL_INTRINSICS:\n-        headers = None\n-        if len(tup) == 2:\n-            f, args, m = tup[0], tup[1], fname2def(tup[0])\n-        elif len(tup) == 3:\n-            f, args, headers, m = tup[0], tup[1], [tup[2]], fname2def(tup[0])\n-        else:\n-            f, args, headers, m = tup[0], tup[1], [tup[2]], fname2def(tup[3])\n-        if config.check_func(f, decl=False, call=True, call_args=args,\n-                             headers=headers):\n-            moredefs.append((m, 1))\n-\n-    for dec, fn in OPTIONAL_FUNCTION_ATTRIBUTES:\n-        if config.check_gcc_function_attribute(dec, fn):\n-            moredefs.append((fname2def(fn), 1))\n-\n-    platform = sysconfig.get_platform()\n-    for fn in OPTIONAL_VARIABLE_ATTRIBUTES:\n-        if config.check_gcc_variable_attribute(fn):\n-            m = fn.replace(\"(\", \"_\").replace(\")\", \"_\")\n-            moredefs.append((fname2def(m), 1))\n-\n-def check_complex(config, mathlibs):\n-    priv = []\n-    pub = []\n-\n-    # Check for complex support\n-    st = config.check_header('complex.h')\n-    if not st:\n-        raise SystemError(\"'complex.h' header is not available\")\n-\n-    for t in C99_COMPLEX_TYPES:\n-        st = config.check_type(t, headers=[\"complex.h\"])\n-        if not st:\n-            raise SystemError(\n-                    f\"'complex.h' header does include complex type {t}\")\n-\n-    def check_prec(prec):\n-        flist = [f + prec for f in C99_COMPLEX_FUNCS]\n-        decl = dict([(f, True) for f in flist])\n-        if not config.check_funcs_once(flist, call=decl, decl=decl,\n-                                        libraries=mathlibs):\n-            for f in flist:\n-                if config.check_func(f, call=True, decl=True,\n-                                        libraries=mathlibs):\n-                    priv.append((fname2def(f), 1))\n-        else:\n-            priv.extend([(fname2def(f), 1) for f in flist])\n-\n-    check_prec('')\n-    check_prec('f')\n-    check_prec('l')\n-\n-    return priv, pub\n-\n-def check_types(config_cmd, ext, build_dir):\n-    private_defines = []\n-    public_defines = []\n-\n-    # Expected size (in number of bytes) for each type. This is an\n-    # optimization: those are only hints, and an exhaustive search for the size\n-    # is done if the hints are wrong.\n-    expected = {'short': [2], 'int': [4], 'long': [8, 4],\n-                'float': [4], 'double': [8], 'long double': [16, 12, 8],\n-                'Py_intptr_t': [8, 4], 'PY_LONG_LONG': [8], 'long long': [8],\n-                'off_t': [8, 4]}\n-\n-    # Check we have the python header (-dev* packages on Linux)\n-    result = config_cmd.check_header('Python.h')\n-    if not result:\n-        python = 'python'\n-        if '__pypy__' in sys.builtin_module_names:\n-            python = 'pypy'\n-        raise SystemError(\n-                \"Cannot compile 'Python.h'. Perhaps you need to \"\n-                \"install {0}-dev|{0}-devel.\".format(python))\n-    res = config_cmd.check_header(\"endian.h\")\n-    if res:\n-        private_defines.append(('HAVE_ENDIAN_H', 1))\n-        public_defines.append(('NPY_HAVE_ENDIAN_H', 1))\n-    res = config_cmd.check_header(\"sys/endian.h\")\n-    if res:\n-        private_defines.append(('HAVE_SYS_ENDIAN_H', 1))\n-        public_defines.append(('NPY_HAVE_SYS_ENDIAN_H', 1))\n-\n-    # Check basic types sizes\n-    for type in ('short', 'int', 'long'):\n-        res = config_cmd.check_decl(\"SIZEOF_%s\" % sym2def(type), headers=[\"Python.h\"])\n-        if res:\n-            public_defines.append(('NPY_SIZEOF_%s' % sym2def(type), \"SIZEOF_%s\" % sym2def(type)))\n-        else:\n-            res = config_cmd.check_type_size(type, expected=expected[type])\n-            if res >= 0:\n-                public_defines.append(('NPY_SIZEOF_%s' % sym2def(type), '%d' % res))\n-            else:\n-                raise SystemError(\"Checking sizeof (%s) failed !\" % type)\n-\n-    for type in ('float', 'double', 'long double'):\n-        already_declared = config_cmd.check_decl(\"SIZEOF_%s\" % sym2def(type),\n-                                                 headers=[\"Python.h\"])\n-        res = config_cmd.check_type_size(type, expected=expected[type])\n-        if res >= 0:\n-            public_defines.append(('NPY_SIZEOF_%s' % sym2def(type), '%d' % res))\n-            if not already_declared and not type == 'long double':\n-                private_defines.append(('SIZEOF_%s' % sym2def(type), '%d' % res))\n-        else:\n-            raise SystemError(\"Checking sizeof (%s) failed !\" % type)\n-\n-        # Compute size of corresponding complex type: used to check that our\n-        # definition is binary compatible with C99 complex type (check done at\n-        # build time in npy_common.h)\n-        complex_def = \"struct {%s __x; %s __y;}\" % (type, type)\n-        res = config_cmd.check_type_size(complex_def,\n-                                         expected=[2 * x for x in expected[type]])\n-        if res >= 0:\n-            public_defines.append(('NPY_SIZEOF_COMPLEX_%s' % sym2def(type), '%d' % res))\n-        else:\n-            raise SystemError(\"Checking sizeof (%s) failed !\" % complex_def)\n-\n-    for type in ('Py_intptr_t', 'off_t'):\n-        res = config_cmd.check_type_size(type, headers=[\"Python.h\"],\n-                library_dirs=[pythonlib_dir()],\n-                expected=expected[type])\n-\n-        if res >= 0:\n-            private_defines.append(('SIZEOF_%s' % sym2def(type), '%d' % res))\n-            public_defines.append(('NPY_SIZEOF_%s' % sym2def(type), '%d' % res))\n-        else:\n-            raise SystemError(\"Checking sizeof (%s) failed !\" % type)\n-\n-    # We check declaration AND type because that's how distutils does it.\n-    if config_cmd.check_decl('PY_LONG_LONG', headers=['Python.h']):\n-        res = config_cmd.check_type_size('PY_LONG_LONG',  headers=['Python.h'],\n-                library_dirs=[pythonlib_dir()],\n-                expected=expected['PY_LONG_LONG'])\n-        if res >= 0:\n-            private_defines.append(('SIZEOF_%s' % sym2def('PY_LONG_LONG'), '%d' % res))\n-            public_defines.append(('NPY_SIZEOF_%s' % sym2def('PY_LONG_LONG'), '%d' % res))\n-        else:\n-            raise SystemError(\"Checking sizeof (%s) failed !\" % 'PY_LONG_LONG')\n-\n-        res = config_cmd.check_type_size('long long',\n-                expected=expected['long long'])\n-        if res >= 0:\n-            #private_defines.append(('SIZEOF_%s' % sym2def('long long'), '%d' % res))\n-            public_defines.append(('NPY_SIZEOF_%s' % sym2def('long long'), '%d' % res))\n-        else:\n-            raise SystemError(\"Checking sizeof (%s) failed !\" % 'long long')\n-\n-    if not config_cmd.check_decl('CHAR_BIT', headers=['Python.h']):\n-        raise RuntimeError(\n-            \"Config wo CHAR_BIT is not supported\"\n-            \", please contact the maintainers\")\n-\n-    return private_defines, public_defines\n-\n-# NOTE: this isn't needed in the Meson build,\n-#       and we won't support a MATHLIB env var\n-def check_mathlib(config_cmd):\n-    # Testing the C math library\n-    mathlibs = []\n-    mathlibs_choices = [[], [\"m\"], [\"cpml\"]]\n-    mathlib = os.environ.get(\"MATHLIB\")\n-    if mathlib:\n-        mathlibs_choices.insert(0, mathlib.split(\",\"))\n-    for libs in mathlibs_choices:\n-        if config_cmd.check_func(\n-            \"log\",\n-            libraries=libs,\n-            call_args=\"0\",\n-            decl=\"double log(double);\",\n-            call=True\n-        ):\n-            mathlibs = libs\n-            break\n-    else:\n-        raise RuntimeError(\n-            \"math library missing; rerun setup.py after setting the \"\n-            \"MATHLIB env variable\"\n-        )\n-    return mathlibs\n-\n-\n-def visibility_define(config):\n-    \"\"\"Return the define value to use for NPY_VISIBILITY_HIDDEN (may be empty\n-    string).\"\"\"\n-    hide = '__attribute__((visibility(\"hidden\")))'\n-    if config.check_gcc_function_attribute(hide, 'hideme'):\n-        return hide\n-    else:\n-        return ''\n-\n-def configuration(parent_package='',top_path=None):\n-    from numpy.distutils.misc_util import (Configuration, dot_join,\n-                                           exec_mod_from_location)\n-    from numpy.distutils.system_info import (get_info, blas_opt_info,\n-                                             lapack_opt_info)\n-\n-    config = Configuration('core', parent_package, top_path)\n-    local_dir = config.local_path\n-    codegen_dir = join(local_dir, 'code_generators')\n-\n-    # Check whether we have a mismatch between the set C API VERSION and the\n-    # actual C API VERSION. Will raise a MismatchCAPIError if so.\n-    check_api_version(C_API_VERSION, codegen_dir)\n-\n-    check_git_submodules()\n-\n-    generate_umath_py = join(codegen_dir, 'generate_umath.py')\n-    n = dot_join(config.name, 'generate_umath')\n-    generate_umath = exec_mod_from_location('_'.join(n.split('.')),\n-                                            generate_umath_py)\n-\n-    header_dir = 'include/numpy'  # this is relative to config.path_in_package\n-\n-    cocache = CallOnceOnly()\n-\n-    def generate_config_h(ext, build_dir):\n-        target = join(build_dir, header_dir, 'config.h')\n-        d = os.path.dirname(target)\n-        if not os.path.exists(d):\n-            os.makedirs(d)\n-\n-        if newer(__file__, target):\n-            config_cmd = config.get_config_cmd()\n-            log.info('Generating %s', target)\n-\n-            # Check sizeof\n-            moredefs, ignored = cocache.check_types(config_cmd, ext, build_dir)\n-\n-            # Check math library and C99 math funcs availability\n-            mathlibs = check_mathlib(config_cmd)\n-            moredefs.append(('MATHLIB', ','.join(mathlibs)))\n-\n-            check_math_capabilities(config_cmd, ext, moredefs, mathlibs)\n-            moredefs.extend(cocache.check_complex(config_cmd, mathlibs)[0])\n-\n-            # Signal check\n-            if is_npy_no_signal():\n-                moredefs.append('NPY_NO_SIGNAL')\n-\n-            # Windows checks\n-            if sys.platform == 'win32' or os.name == 'nt':\n-                win32_checks(moredefs)\n-\n-            # C99 restrict keyword\n-            moredefs.append(('NPY_RESTRICT', config_cmd.check_restrict()))\n-\n-            # Inline check\n-            inline = config_cmd.check_inline()\n-\n-            if can_link_svml():\n-                moredefs.append(('NPY_CAN_LINK_SVML', 1))\n-\n-            # Use bogus stride debug aid to flush out bugs where users use\n-            # strides of dimensions with length 1 to index a full contiguous\n-            # array.\n-            if NPY_RELAXED_STRIDES_DEBUG:\n-                moredefs.append(('NPY_RELAXED_STRIDES_DEBUG', 1))\n-            else:\n-                moredefs.append(('NPY_RELAXED_STRIDES_DEBUG', 0))\n-\n-            # Get long double representation\n-            rep = check_long_double_representation(config_cmd)\n-            moredefs.append(('HAVE_LDOUBLE_%s' % rep, 1))\n-\n-            if check_for_right_shift_internal_compiler_error(config_cmd):\n-                moredefs.append('NPY_DO_NOT_OPTIMIZE_LONG_right_shift')\n-                moredefs.append('NPY_DO_NOT_OPTIMIZE_ULONG_right_shift')\n-                moredefs.append('NPY_DO_NOT_OPTIMIZE_LONGLONG_right_shift')\n-                moredefs.append('NPY_DO_NOT_OPTIMIZE_ULONGLONG_right_shift')\n-\n-            # Generate the config.h file from moredefs\n-            with open(target, 'w') as target_f:\n-                if sys.platform == 'darwin':\n-                    target_f.write(\n-                        \"/* may be overridden by numpyconfig.h on darwin */\\n\"\n-                    )\n-                for d in moredefs:\n-                    if isinstance(d, str):\n-                        target_f.write('#define %s\\n' % (d))\n-                    else:\n-                        target_f.write('#define %s %s\\n' % (d[0], d[1]))\n-\n-                # define inline to our keyword, or nothing\n-                target_f.write('#ifndef __cplusplus\\n')\n-                if inline == 'inline':\n-                    target_f.write('/* #undef inline */\\n')\n-                else:\n-                    target_f.write('#define inline %s\\n' % inline)\n-                target_f.write('#endif\\n')\n-\n-                # add the guard to make sure config.h is never included directly,\n-                # but always through npy_config.h\n-                target_f.write(textwrap.dedent(\"\"\"\n-                    #ifndef NUMPY_CORE_SRC_COMMON_NPY_CONFIG_H_\n-                    #error config.h should never be included directly, include npy_config.h instead\n-                    #endif\n-                    \"\"\"))\n-\n-            log.info('File: %s' % target)\n-            with open(target) as target_f:\n-                log.info(target_f.read())\n-            log.info('EOF')\n-        else:\n-            mathlibs = []\n-            with open(target) as target_f:\n-                for line in target_f:\n-                    s = '#define MATHLIB'\n-                    if line.startswith(s):\n-                        value = line[len(s):].strip()\n-                        if value:\n-                            mathlibs.extend(value.split(','))\n-\n-        # Ugly: this can be called within a library and not an extension,\n-        # in which case there is no libraries attributes (and none is\n-        # needed).\n-        if hasattr(ext, 'libraries'):\n-            ext.libraries.extend(mathlibs)\n-\n-        incl_dir = os.path.dirname(target)\n-        if incl_dir not in config.numpy_include_dirs:\n-            config.numpy_include_dirs.append(incl_dir)\n-\n-        return target\n-\n-    def generate_numpyconfig_h(ext, build_dir):\n-        \"\"\"Depends on config.h: generate_config_h has to be called before !\"\"\"\n-        # put common include directory in build_dir on search path\n-        # allows using code generation in headers\n-        config.add_include_dirs(join(build_dir, \"src\", \"common\"))\n-        config.add_include_dirs(join(build_dir, \"src\", \"npymath\"))\n-\n-        target = join(build_dir, header_dir, '_numpyconfig.h')\n-        d = os.path.dirname(target)\n-        if not os.path.exists(d):\n-            os.makedirs(d)\n-        if newer(__file__, target):\n-            config_cmd = config.get_config_cmd()\n-            log.info('Generating %s', target)\n-\n-            # Check sizeof\n-            ignored, moredefs = cocache.check_types(config_cmd, ext, build_dir)\n-\n-            if is_npy_no_signal():\n-                moredefs.append(('NPY_NO_SIGNAL', 1))\n-\n-            if is_npy_no_smp():\n-                moredefs.append(('NPY_NO_SMP', 1))\n-            else:\n-                moredefs.append(('NPY_NO_SMP', 0))\n-\n-            mathlibs = check_mathlib(config_cmd)\n-            moredefs.extend(cocache.check_complex(config_cmd, mathlibs)[1])\n-\n-            if NPY_RELAXED_STRIDES_DEBUG:\n-                moredefs.append(('NPY_RELAXED_STRIDES_DEBUG', 1))\n-\n-            # Check whether we can use inttypes (C99) formats\n-            if config_cmd.check_decl('PRIdPTR', headers=['inttypes.h']):\n-                moredefs.append(('NPY_USE_C99_FORMATS', 1))\n-\n-            # visibility check\n-            hidden_visibility = visibility_define(config_cmd)\n-            moredefs.append(('NPY_VISIBILITY_HIDDEN', hidden_visibility))\n-\n-            # Add the C API/ABI versions\n-            moredefs.append(('NPY_ABI_VERSION', '0x%.8X' % C_ABI_VERSION))\n-            moredefs.append(('NPY_API_VERSION', '0x%.8X' % C_API_VERSION))\n-\n-            # Add moredefs to header\n-            with open(target, 'w') as target_f:\n-                for d in moredefs:\n-                    if isinstance(d, str):\n-                        target_f.write('#define %s\\n' % (d))\n-                    else:\n-                        target_f.write('#define %s %s\\n' % (d[0], d[1]))\n-\n-                # Define __STDC_FORMAT_MACROS\n-                target_f.write(textwrap.dedent(\"\"\"\n-                    #ifndef __STDC_FORMAT_MACROS\n-                    #define __STDC_FORMAT_MACROS 1\n-                    #endif\n-                    \"\"\"))\n-\n-            # Dump the numpyconfig.h header to stdout\n-            log.info('File: %s' % target)\n-            with open(target) as target_f:\n-                log.info(target_f.read())\n-            log.info('EOF')\n-        config.add_data_files((header_dir, target))\n-        return target\n-\n-    def generate_api_func(module_name):\n-        def generate_api(ext, build_dir):\n-            script = join(codegen_dir, module_name + '.py')\n-            sys.path.insert(0, codegen_dir)\n-            try:\n-                m = __import__(module_name)\n-                log.info('executing %s', script)\n-                h_file, c_file = m.generate_api(os.path.join(build_dir, header_dir))\n-            finally:\n-                del sys.path[0]\n-            config.add_data_files((header_dir, h_file),\n-                                 )\n-            return (h_file,)\n-        return generate_api\n-\n-    generate_numpy_api = generate_api_func('generate_numpy_api')\n-    generate_ufunc_api = generate_api_func('generate_ufunc_api')\n-\n-    config.add_include_dirs(join(local_dir, \"src\", \"common\"))\n-    config.add_include_dirs(join(local_dir, \"src\"))\n-    config.add_include_dirs(join(local_dir))\n-\n-    config.add_data_dir('include/numpy')\n-    config.add_include_dirs(join('src', 'npymath'))\n-    config.add_include_dirs(join('src', 'multiarray'))\n-    config.add_include_dirs(join('src', 'umath'))\n-    config.add_include_dirs(join('src', 'npysort'))\n-    config.add_include_dirs(join('src', '_simd'))\n-\n-    config.add_define_macros([(\"NPY_INTERNAL_BUILD\", \"1\")]) # this macro indicates that Numpy build is in process\n-    config.add_define_macros([(\"HAVE_NPY_CONFIG_H\", \"1\")])\n-    if sys.platform[:3] == \"aix\":\n-        config.add_define_macros([(\"_LARGE_FILES\", None)])\n-    else:\n-        config.add_define_macros([(\"_FILE_OFFSET_BITS\", \"64\")])\n-        config.add_define_macros([('_LARGEFILE_SOURCE', '1')])\n-        config.add_define_macros([('_LARGEFILE64_SOURCE', '1')])\n-\n-    config.numpy_include_dirs.extend(config.paths('include'))\n-\n-    deps = [join('include', 'numpy', '*object.h'),\n-            join(codegen_dir, 'genapi.py'),\n-            ]\n-\n-    #######################################################################\n-    #                          npymath library                            #\n-    #######################################################################\n-\n-    subst_dict = dict([(\"sep\", os.path.sep), (\"pkgname\", \"numpy.core\")])\n-\n-    def get_mathlib_info(*args):\n-        # Another ugly hack: the mathlib info is known once build_src is run,\n-        # but we cannot use add_installed_pkg_config here either, so we only\n-        # update the substitution dictionary during npymath build\n-        config_cmd = config.get_config_cmd()\n-        mlibs = check_mathlib(config_cmd)\n-\n-        posix_mlib = ' '.join(['-l%s' % l for l in mlibs])\n-        msvc_mlib = ' '.join(['%s.lib' % l for l in mlibs])\n-        subst_dict[\"posix_mathlib\"] = posix_mlib\n-        subst_dict[\"msvc_mathlib\"] = msvc_mlib\n-\n-    npymath_sources = [join('src', 'npymath', 'npy_math_internal.h.src'),\n-                       join('src', 'npymath', 'npy_math.c'),\n-                       # join('src', 'npymath', 'ieee754.cpp'),\n-                       join('src', 'npymath', 'ieee754.c.src'),\n-                       join('src', 'npymath', 'npy_math_complex.c.src'),\n-                       join('src', 'npymath', 'halffloat.cpp'),\n-                       ]\n-\n-    config.add_installed_library('npymath',\n-            sources=npymath_sources + [get_mathlib_info],\n-            install_dir='lib',\n-            build_info={\n-                'include_dirs' : [],  # empty list required for creating npy_math_internal.h\n-                'extra_compiler_args': [lib_opts_if_msvc],\n-            })\n-    config.add_npy_pkg_config(\"npymath.ini.in\", \"lib/npy-pkg-config\",\n-            subst_dict)\n-    config.add_npy_pkg_config(\"mlib.ini.in\", \"lib/npy-pkg-config\",\n-            subst_dict)\n-\n-    #######################################################################\n-    #                     multiarray_tests module                         #\n-    #######################################################################\n-\n-    config.add_extension('_multiarray_tests',\n-                    sources=[join('src', 'multiarray', '_multiarray_tests.c.src'),\n-                             join('src', 'common', 'mem_overlap.c'),\n-                             join('src', 'common', 'npy_argparse.c'),\n-                             join('src', 'common', 'npy_hashtable.c')],\n-                    depends=[join('src', 'common', 'mem_overlap.h'),\n-                             join('src', 'common', 'npy_argparse.h'),\n-                             join('src', 'common', 'npy_hashtable.h'),\n-                             join('src', 'common', 'npy_extint128.h')],\n-                    libraries=['npymath'])\n-\n-    #######################################################################\n-    #             _multiarray_umath module - common part                  #\n-    #######################################################################\n-\n-    common_deps = [\n-            join('src', 'common', 'dlpack', 'dlpack.h'),\n-            join('src', 'common', 'array_assign.h'),\n-            join('src', 'common', 'binop_override.h'),\n-            join('src', 'common', 'cblasfuncs.h'),\n-            join('src', 'common', 'lowlevel_strided_loops.h'),\n-            join('src', 'common', 'mem_overlap.h'),\n-            join('src', 'common', 'npy_argparse.h'),\n-            join('src', 'common', 'npy_cblas.h'),\n-            join('src', 'common', 'npy_config.h'),\n-            join('src', 'common', 'npy_ctypes.h'),\n-            join('src', 'common', 'npy_dlpack.h'),\n-            join('src', 'common', 'npy_extint128.h'),\n-            join('src', 'common', 'npy_import.h'),\n-            join('src', 'common', 'npy_hashtable.h'),\n-            join('src', 'common', 'npy_longdouble.h'),\n-            join('src', 'common', 'npy_svml.h'),\n-            join('src', 'common', 'templ_common.h.src'),\n-            join('src', 'common', 'ucsnarrow.h'),\n-            join('src', 'common', 'ufunc_override.h'),\n-            join('src', 'common', 'umathmodule.h'),\n-            join('src', 'common', 'numpyos.h'),\n-            join('src', 'common', 'npy_cpu_dispatch.h'),\n-            join('src', 'common', 'simd', 'simd.h'),\n-            join('src', 'common', 'common.hpp'),\n-        ]\n-\n-    common_src = [\n-            join('src', 'common', 'array_assign.c'),\n-            join('src', 'common', 'mem_overlap.c'),\n-            join('src', 'common', 'npy_argparse.c'),\n-            join('src', 'common', 'npy_hashtable.c'),\n-            join('src', 'common', 'npy_longdouble.c'),\n-            join('src', 'common', 'templ_common.h.src'),\n-            join('src', 'common', 'ucsnarrow.c'),\n-            join('src', 'common', 'ufunc_override.c'),\n-            join('src', 'common', 'numpyos.c'),\n-            join('src', 'common', 'npy_cpu_features.c'),\n-            join('src', 'common', 'npy_cpu_dispatch.c'),\n-            ]\n-\n-    if os.environ.get('NPY_USE_BLAS_ILP64', \"0\") != \"0\":\n-        blas_info = get_info('blas_ilp64_opt', 2)\n-    else:\n-        blas_info = get_info('blas_opt', 0)\n-\n-    have_blas = blas_info and ('HAVE_CBLAS', None) in blas_info.get('define_macros', [])\n-\n-    if have_blas:\n-        extra_info = blas_info\n-        # These files are also in MANIFEST.in so that they are always in\n-        # the source distribution independently of HAVE_CBLAS.\n-        common_src.extend([join('src', 'common', 'cblasfuncs.c'),\n-                           join('src', 'common', 'python_xerbla.c'),\n-                          ])\n-    else:\n-        extra_info = {}\n-\n-    #######################################################################\n-    #             _multiarray_umath module - multiarray part              #\n-    #######################################################################\n-\n-    multiarray_deps = [\n-            join('src', 'multiarray', 'abstractdtypes.h'),\n-            join('src', 'multiarray', 'arrayobject.h'),\n-            join('src', 'multiarray', 'arraytypes.h.src'),\n-            join('src', 'multiarray', 'arrayfunction_override.h'),\n-            join('src', 'multiarray', 'array_coercion.h'),\n-            join('src', 'multiarray', 'array_method.h'),\n-            join('src', 'multiarray', 'npy_buffer.h'),\n-            join('src', 'multiarray', 'calculation.h'),\n-            join('src', 'multiarray', 'common.h'),\n-            join('src', 'multiarray', 'common_dtype.h'),\n-            join('src', 'multiarray', 'convert_datatype.h'),\n-            join('src', 'multiarray', 'convert.h'),\n-            join('src', 'multiarray', 'conversion_utils.h'),\n-            join('src', 'multiarray', 'ctors.h'),\n-            join('src', 'multiarray', 'descriptor.h'),\n-            join('src', 'multiarray', 'dtypemeta.h'),\n-            join('src', 'multiarray', 'dtype_transfer.h'),\n-            join('src', 'multiarray', 'dtype_traversal.h'),\n-            join('src', 'multiarray', 'dragon4.h'),\n-            join('src', 'multiarray', 'einsum_debug.h'),\n-            join('src', 'multiarray', 'einsum_sumprod.h'),\n-            join('src', 'multiarray', 'experimental_public_dtype_api.h'),\n-            join('src', 'multiarray', 'getset.h'),\n-            join('src', 'multiarray', 'hashdescr.h'),\n-            join('src', 'multiarray', 'iterators.h'),\n-            join('src', 'multiarray', 'legacy_dtype_implementation.h'),\n-            join('src', 'multiarray', 'mapping.h'),\n-            join('src', 'multiarray', 'methods.h'),\n-            join('src', 'multiarray', 'multiarraymodule.h'),\n-            join('src', 'multiarray', 'nditer_impl.h'),\n-            join('src', 'multiarray', 'number.h'),\n-            join('src', 'multiarray', 'refcount.h'),\n-            join('src', 'multiarray', 'scalartypes.h'),\n-            join('src', 'multiarray', 'sequence.h'),\n-            join('src', 'multiarray', 'shape.h'),\n-            join('src', 'multiarray', 'strfuncs.h'),\n-            join('src', 'multiarray', 'typeinfo.h'),\n-            join('src', 'multiarray', 'usertypes.h'),\n-            join('src', 'multiarray', 'vdot.h'),\n-            join('src', 'multiarray', 'textreading', 'readtext.h'),\n-            join('include', 'numpy', 'arrayobject.h'),\n-            join('include', 'numpy', '_neighborhood_iterator_imp.h'),\n-            join('include', 'numpy', 'npy_endian.h'),\n-            join('include', 'numpy', 'arrayscalars.h'),\n-            join('include', 'numpy', 'npy_3kcompat.h'),\n-            join('include', 'numpy', 'npy_math.h'),\n-            join('include', 'numpy', 'halffloat.h'),\n-            join('include', 'numpy', 'npy_common.h'),\n-            join('include', 'numpy', 'npy_os.h'),\n-            join('include', 'numpy', 'utils.h'),\n-            join('include', 'numpy', 'ndarrayobject.h'),\n-            join('include', 'numpy', 'npy_cpu.h'),\n-            join('include', 'numpy', 'numpyconfig.h'),\n-            join('include', 'numpy', 'ndarraytypes.h'),\n-            join('include', 'numpy', 'npy_1_7_deprecated_api.h'),\n-            # add library sources as distuils does not consider libraries\n-            # dependencies\n-            ] + npymath_sources\n-\n-    multiarray_src = [\n-            join('src', 'multiarray', 'abstractdtypes.c'),\n-            join('src', 'multiarray', 'alloc.c'),\n-            join('src', 'multiarray', 'arrayobject.c'),\n-            join('src', 'multiarray', 'arraytypes.h.src'),\n-            join('src', 'multiarray', 'arraytypes.c.src'),\n-            join('src', 'multiarray', 'argfunc.dispatch.c.src'),\n-            join('src', 'multiarray', 'array_coercion.c'),\n-            join('src', 'multiarray', 'array_method.c'),\n-            join('src', 'multiarray', 'array_assign_scalar.c'),\n-            join('src', 'multiarray', 'array_assign_array.c'),\n-            join('src', 'multiarray', 'arrayfunction_override.c'),\n-            join('src', 'multiarray', 'buffer.c'),\n-            join('src', 'multiarray', 'calculation.c'),\n-            join('src', 'multiarray', 'compiled_base.c'),\n-            join('src', 'multiarray', 'common.c'),\n-            join('src', 'multiarray', 'common_dtype.c'),\n-            join('src', 'multiarray', 'convert.c'),\n-            join('src', 'multiarray', 'convert_datatype.c'),\n-            join('src', 'multiarray', 'conversion_utils.c'),\n-            join('src', 'multiarray', 'ctors.c'),\n-            join('src', 'multiarray', 'datetime.c'),\n-            join('src', 'multiarray', 'datetime_strings.c'),\n-            join('src', 'multiarray', 'datetime_busday.c'),\n-            join('src', 'multiarray', 'datetime_busdaycal.c'),\n-            join('src', 'multiarray', 'descriptor.c'),\n-            join('src', 'multiarray', 'dlpack.c'),\n-            join('src', 'multiarray', 'dtypemeta.c'),\n-            join('src', 'multiarray', 'dragon4.c'),\n-            join('src', 'multiarray', 'dtype_transfer.c'),\n-            join('src', 'multiarray', 'dtype_traversal.c'),\n-            join('src', 'multiarray', 'einsum.c.src'),\n-            join('src', 'multiarray', 'einsum_sumprod.c.src'),\n-            join('src', 'multiarray', 'experimental_public_dtype_api.c'),\n-            join('src', 'multiarray', 'flagsobject.c'),\n-            join('src', 'multiarray', 'getset.c'),\n-            join('src', 'multiarray', 'hashdescr.c'),\n-            join('src', 'multiarray', 'item_selection.c'),\n-            join('src', 'multiarray', 'iterators.c'),\n-            join('src', 'multiarray', 'legacy_dtype_implementation.c'),\n-            join('src', 'multiarray', 'lowlevel_strided_loops.c.src'),\n-            join('src', 'multiarray', 'mapping.c'),\n-            join('src', 'multiarray', 'methods.c'),\n-            join('src', 'multiarray', 'multiarraymodule.c'),\n-            join('src', 'multiarray', 'nditer_templ.c.src'),\n-            join('src', 'multiarray', 'nditer_api.c'),\n-            join('src', 'multiarray', 'nditer_constr.c'),\n-            join('src', 'multiarray', 'nditer_pywrap.c'),\n-            join('src', 'multiarray', 'number.c'),\n-            join('src', 'multiarray', 'refcount.c'),\n-            join('src', 'multiarray', 'sequence.c'),\n-            join('src', 'multiarray', 'shape.c'),\n-            join('src', 'multiarray', 'scalarapi.c'),\n-            join('src', 'multiarray', 'scalartypes.c.src'),\n-            join('src', 'multiarray', 'strfuncs.c'),\n-            join('src', 'multiarray', 'temp_elide.c'),\n-            join('src', 'multiarray', 'typeinfo.c'),\n-            join('src', 'multiarray', 'usertypes.c'),\n-            join('src', 'multiarray', 'vdot.c'),\n-            join('src', 'common', 'npy_sort.h.src'),\n-            join('src', 'npysort', 'quicksort.cpp'),\n-            join('src', 'npysort', 'mergesort.cpp'),\n-            join('src', 'npysort', 'timsort.cpp'),\n-            join('src', 'npysort', 'heapsort.cpp'),\n-            join('src', 'npysort', 'radixsort.cpp'),\n-            join('src', 'common', 'npy_partition.h'),\n-            join('src', 'npysort', 'selection.cpp'),\n-            join('src', 'common', 'npy_binsearch.h'),\n-            join('src', 'npysort', 'binsearch.cpp'),\n-            join('src', 'multiarray', 'textreading', 'conversions.c'),\n-            join('src', 'multiarray', 'textreading', 'field_types.c'),\n-            join('src', 'multiarray', 'textreading', 'growth.c'),\n-            join('src', 'multiarray', 'textreading', 'readtext.c'),\n-            join('src', 'multiarray', 'textreading', 'rows.c'),\n-            join('src', 'multiarray', 'textreading', 'stream_pyobject.c'),\n-            join('src', 'multiarray', 'textreading', 'str_to_int.c'),\n-            join('src', 'multiarray', 'textreading', 'tokenize.cpp'),\n-            # Remove this once scipy macos arm64 build correctly\n-            # links to the arm64 npymath library,\n-            # see gh-22673\n-            join('src', 'npymath', 'arm64_exports.c'),\n-            join('src', 'npysort', 'simd_qsort.dispatch.cpp'),\n-            join('src', 'npysort', 'simd_qsort_16bit.dispatch.cpp'),\n-            ]\n-\n-    #######################################################################\n-    #             _multiarray_umath module - umath part                   #\n-    #######################################################################\n-\n-    def generate_umath_c(ext, build_dir):\n-        target = join(build_dir, header_dir, '__umath_generated.c')\n-        dir = os.path.dirname(target)\n-        if not os.path.exists(dir):\n-            os.makedirs(dir)\n-        script = generate_umath_py\n-        if newer(script, target):\n-            with open(target, 'w') as f:\n-                f.write(generate_umath.make_code(generate_umath.defdict,\n-                                                 generate_umath.__file__))\n-        return []\n-\n-    def generate_umath_doc_header(ext, build_dir):\n-        from numpy.distutils.misc_util import exec_mod_from_location\n-\n-        target = join(build_dir, header_dir, '_umath_doc_generated.h')\n-        dir = os.path.dirname(target)\n-        if not os.path.exists(dir):\n-            os.makedirs(dir)\n-\n-        generate_umath_doc_py = join(codegen_dir, 'generate_umath_doc.py')\n-        if newer(generate_umath_doc_py, target):\n-            n = dot_join(config.name, 'generate_umath_doc')\n-            generate_umath_doc = exec_mod_from_location(\n-                '_'.join(n.split('.')), generate_umath_doc_py)\n-            generate_umath_doc.write_code(target)\n-\n-    umath_src = [\n-            join('src', 'umath', 'umathmodule.c'),\n-            join('src', 'umath', 'reduction.c'),\n-            join('src', 'umath', 'funcs.inc.src'),\n-            join('src', 'umath', 'loops.h.src'),\n-            join('src', 'umath', 'loops_utils.h.src'),\n-            join('src', 'umath', 'loops.c.src'),\n-            join('src', 'umath', 'loops_unary.dispatch.c.src'),\n-            join('src', 'umath', 'loops_unary_fp.dispatch.c.src'),\n-            join('src', 'umath', 'loops_unary_fp_le.dispatch.c.src'),\n-            join('src', 'umath', 'loops_arithm_fp.dispatch.c.src'),\n-            join('src', 'umath', 'loops_arithmetic.dispatch.c.src'),\n-            join('src', 'umath', 'loops_logical.dispatch.c.src'),\n-            join('src', 'umath', 'loops_minmax.dispatch.c.src'),\n-            join('src', 'umath', 'loops_trigonometric.dispatch.c.src'),\n-            join('src', 'umath', 'loops_umath_fp.dispatch.c.src'),\n-            join('src', 'umath', 'loops_exponent_log.dispatch.c.src'),\n-            join('src', 'umath', 'loops_hyperbolic.dispatch.c.src'),\n-            join('src', 'umath', 'loops_modulo.dispatch.c.src'),\n-            join('src', 'umath', 'loops_comparison.dispatch.c.src'),\n-            join('src', 'umath', 'loops_unary_complex.dispatch.c.src'),\n-            join('src', 'umath', 'loops_autovec.dispatch.c.src'),\n-            join('src', 'umath', 'matmul.h.src'),\n-            join('src', 'umath', 'matmul.c.src'),\n-            join('src', 'umath', 'clip.h'),\n-            join('src', 'umath', 'clip.cpp'),\n-            join('src', 'umath', 'dispatching.c'),\n-            join('src', 'umath', 'legacy_array_method.c'),\n-            join('src', 'umath', 'wrapping_array_method.c'),\n-            join('src', 'umath', 'ufunc_object.c'),\n-            join('src', 'umath', 'extobj.c'),\n-            join('src', 'umath', 'scalarmath.c.src'),\n-            join('src', 'umath', 'ufunc_type_resolution.c'),\n-            join('src', 'umath', 'override.c'),\n-            join('src', 'umath', 'string_ufuncs.cpp'),\n-            # For testing. Eventually, should use public API and be separate:\n-            join('src', 'umath', '_scaled_float_dtype.c'),\n-            ]\n-\n-    umath_deps = [\n-            generate_umath_py,\n-            join('include', 'numpy', 'npy_math.h'),\n-            join('include', 'numpy', 'halffloat.h'),\n-            join('src', 'multiarray', 'common.h'),\n-            join('src', 'multiarray', 'number.h'),\n-            join('src', 'common', 'templ_common.h.src'),\n-            join('src', 'umath', 'override.h'),\n-            join(codegen_dir, 'generate_ufunc_api.py'),\n-            join(codegen_dir, 'ufunc_docstrings.py'),\n-            ]\n-\n-    svml_path = join('numpy', 'core', 'src', 'umath', 'svml')\n-    svml_objs = []\n-    # we have converted the following into universal intrinsics\n-    # so we can bring the benefits of performance for all platforms\n-    # not just for avx512 on linux without performance/accuracy regression,\n-    # actually the other way around, better performance and\n-    # after all maintainable code.\n-    svml_filter = (\n-    )\n-    if can_link_svml():\n-        svml_objs = glob.glob(svml_path + '/**/*.s', recursive=True)\n-        svml_objs = [o for o in svml_objs if not o.endswith(svml_filter)]\n-\n-        # The ordering of names returned by glob is undefined, so we sort\n-        # to make builds reproducible.\n-        svml_objs.sort()\n-        if not can_link_svml_fp16():\n-            svml_objs = [o for o in svml_objs if not o.endswith('_h_la.s')]\n-\n-    config.add_extension('_multiarray_umath',\n-                         sources=multiarray_src + umath_src +\n-                                 common_src +\n-                                 [generate_config_h,\n-                                  generate_numpyconfig_h,\n-                                  generate_numpy_api,\n-                                  join(codegen_dir, 'generate_numpy_api.py'),\n-                                  join('*.py'),\n-                                  generate_umath_c,\n-                                  generate_umath_doc_header,\n-                                  generate_ufunc_api,\n-                                 ],\n-                         depends=deps + multiarray_deps + umath_deps +\n-                                common_deps,\n-                         libraries=['npymath'],\n-                         extra_objects=svml_objs,\n-                         extra_info=extra_info)\n-\n-    #######################################################################\n-    #                       _umath_tests module                           #\n-    #######################################################################\n-\n-    config.add_extension('_umath_tests', sources=[\n-        join('src', 'umath', '_umath_tests.c.src'),\n-        join('src', 'umath', '_umath_tests.dispatch.c'),\n-        join('src', 'common', 'npy_cpu_features.c'),\n-    ])\n-\n-    #######################################################################\n-    #                   custom rational dtype module                      #\n-    #######################################################################\n-\n-    config.add_extension('_rational_tests',\n-                    sources=[join('src', 'umath', '_rational_tests.c')])\n-\n-    #######################################################################\n-    #                        struct_ufunc_test module                     #\n-    #######################################################################\n-\n-    config.add_extension('_struct_ufunc_tests',\n-                    sources=[join('src', 'umath', '_struct_ufunc_tests.c')])\n-\n-\n-    #######################################################################\n-    #                        operand_flag_tests module                    #\n-    #######################################################################\n-\n-    config.add_extension('_operand_flag_tests',\n-                    sources=[join('src', 'umath', '_operand_flag_tests.c')])\n-\n-    #######################################################################\n-    #                        SIMD module                                  #\n-    #######################################################################\n-\n-    config.add_extension('_simd',\n-        sources=[\n-            join('src', 'common', 'npy_cpu_features.c'),\n-            join('src', '_simd', '_simd.c'),\n-            join('src', '_simd', '_simd_inc.h.src'),\n-            join('src', '_simd', '_simd_data.inc.src'),\n-            join('src', '_simd', '_simd.dispatch.c.src'),\n-        ], depends=[\n-            join('src', 'common', 'npy_cpu_dispatch.h'),\n-            join('src', 'common', 'simd', 'simd.h'),\n-            join('src', '_simd', '_simd.h'),\n-            join('src', '_simd', '_simd_inc.h.src'),\n-            join('src', '_simd', '_simd_data.inc.src'),\n-            join('src', '_simd', '_simd_arg.inc'),\n-            join('src', '_simd', '_simd_convert.inc'),\n-            join('src', '_simd', '_simd_easyintrin.inc'),\n-            join('src', '_simd', '_simd_vector.inc'),\n-        ],\n-        libraries=['npymath']\n-    )\n-\n-    config.add_subpackage('tests')\n-    config.add_data_dir('tests/data')\n-    config.add_data_dir('tests/examples')\n-    config.add_data_files('*.pyi')\n-\n-    config.make_svn_version_py()\n-\n-    return config\n-\n-if __name__ == '__main__':\n-    from numpy.distutils.core import setup\n-    setup(configuration=configuration)\n",
            "comment_added_diff": {},
            "comment_deleted_diff": {
                "15": "from setup_common import *  # noqa: F403",
                "17": "# Set to True to enable relaxed strides checking. This (mostly) means",
                "18": "# that `strides[dim]` is ignored if `shape[dim] == 1` when setting flags.",
                "25": "# Put NPY_RELAXED_STRIDES_DEBUG=1 in the environment if you want numpy to use a",
                "26": "# bogus value for affected strides in order to help smoke out bad stride usage",
                "27": "# when relaxed stride checking is enabled.",
                "31": "# Set NPY_DISABLE_SVML=1 in the environment to disable the vendored SVML",
                "32": "# library. This option only has significance on a Linux x86_64 host and is most",
                "33": "# useful to avoid improperly requiring SVML when cross compiling.",
                "36": "# XXX: ugly, we use a class to avoid calling twice some expensive functions in",
                "37": "# config.h/numpyconfig.h. I don't see a better way because distutils force",
                "38": "# config.h generation inside an Extension class, and as such sharing",
                "39": "# configuration information between extensions is not easy.",
                "40": "# Using a pickled-based memoize does not work because config_cmd is an instance",
                "41": "# method, which cPickle does not like.",
                "42": "#",
                "43": "# Use pickle in all cases, as cPickle is gone in python3 and the difference",
                "44": "# in time is only in build. -- Charles Harris, 2013-03-30",
                "47": "    # NOTE: we don't need any of this in the Meson build,",
                "48": "    # it takes care of caching",
                "111": "    # Perhaps a fancier check is in order here.",
                "112": "    #  so that threads are only enabled if there",
                "113": "    #  are actually multiple CPUS? -- but",
                "114": "    #  threaded code can be nice even on a single",
                "115": "    #  CPU so that long-calculating code doesn't",
                "116": "    #  block.",
                "123": "    # Distutils hack on AMD64 on windows",
                "129": "    # On win32, force long double format string to be 'g', not",
                "130": "    # 'Lg', since the MS runtime does not support long double whose",
                "131": "    # size is > sizeof(double)",
                "171": "        # Use check_funcs_once first, and if it does not work, test func per",
                "172": "        # func. Return success only if all the functions are available",
                "174": "            # Global check failed, check func per func",
                "182": "    #use_msvc = config.check_decl(\"_MSC_VER\")",
                "187": "    # Standard functions which may not be available and for which we have a",
                "188": "    # replacement implementation. Note that some of these are C99 functions.",
                "190": "    # XXX: hack to circumvent cpp pollution from python: python put its",
                "191": "    # config.h in the public namespace, so we have a clash for the common",
                "192": "    # functions we test. We remove every function tested by python's",
                "193": "    # autoconf, hoping their own test are correct",
                "206": "    # Try with both \"locale.h\" and \"xlocale.h\"",
                "213": "        # It didn't work with xlocale.h, maybe it will work with locale.h?",
                "243": "    # Check for complex support",
                "276": "    # Expected size (in number of bytes) for each type. This is an",
                "277": "    # optimization: those are only hints, and an exhaustive search for the size",
                "278": "    # is done if the hints are wrong.",
                "284": "    # Check we have the python header (-dev* packages on Linux)",
                "302": "    # Check basic types sizes",
                "325": "        # Compute size of corresponding complex type: used to check that our",
                "326": "        # definition is binary compatible with C99 complex type (check done at",
                "327": "        # build time in npy_common.h)",
                "347": "    # We check declaration AND type because that's how distutils does it.",
                "361": "            #private_defines.append(('SIZEOF_%s' % sym2def('long long'), '%d' % res))",
                "373": "# NOTE: this isn't needed in the Meson build,",
                "374": "#       and we won't support a MATHLIB env var",
                "376": "    # Testing the C math library",
                "419": "    # Check whether we have a mismatch between the set C API VERSION and the",
                "420": "    # actual C API VERSION. Will raise a MismatchCAPIError if so.",
                "430": "    header_dir = 'include/numpy'  # this is relative to config.path_in_package",
                "444": "            # Check sizeof",
                "447": "            # Check math library and C99 math funcs availability",
                "454": "            # Signal check",
                "458": "            # Windows checks",
                "462": "            # C99 restrict keyword",
                "465": "            # Inline check",
                "471": "            # Use bogus stride debug aid to flush out bugs where users use",
                "472": "            # strides of dimensions with length 1 to index a full contiguous",
                "473": "            # array.",
                "479": "            # Get long double representation",
                "489": "            # Generate the config.h file from moredefs",
                "497": "                        target_f.write('#define %s\\n' % (d))",
                "499": "                        target_f.write('#define %s %s\\n' % (d[0], d[1]))",
                "501": "                # define inline to our keyword, or nothing",
                "502": "                target_f.write('#ifndef __cplusplus\\n')",
                "504": "                    target_f.write('/* #undef inline */\\n')",
                "506": "                    target_f.write('#define inline %s\\n' % inline)",
                "507": "                target_f.write('#endif\\n')",
                "509": "                # add the guard to make sure config.h is never included directly,",
                "510": "                # but always through npy_config.h",
                "512": "                    #ifndef NUMPY_CORE_SRC_COMMON_NPY_CONFIG_H_",
                "513": "                    #error config.h should never be included directly, include npy_config.h instead",
                "514": "                    #endif",
                "525": "                    s = '#define MATHLIB'",
                "531": "        # Ugly: this can be called within a library and not an extension,",
                "532": "        # in which case there is no libraries attributes (and none is",
                "533": "        # needed).",
                "545": "        # put common include directory in build_dir on search path",
                "546": "        # allows using code generation in headers",
                "558": "            # Check sizeof",
                "575": "            # Check whether we can use inttypes (C99) formats",
                "579": "            # visibility check",
                "583": "            # Add the C API/ABI versions",
                "587": "            # Add moredefs to header",
                "591": "                        target_f.write('#define %s\\n' % (d))",
                "593": "                        target_f.write('#define %s %s\\n' % (d[0], d[1]))",
                "595": "                # Define __STDC_FORMAT_MACROS",
                "597": "                    #ifndef __STDC_FORMAT_MACROS",
                "598": "                    #define __STDC_FORMAT_MACROS 1",
                "599": "                    #endif",
                "602": "            # Dump the numpyconfig.h header to stdout",
                "639": "    config.add_define_macros([(\"NPY_INTERNAL_BUILD\", \"1\")]) # this macro indicates that Numpy build is in process",
                "654": "    #######################################################################",
                "655": "    #                          npymath library                            #",
                "656": "    #######################################################################",
                "661": "        # Another ugly hack: the mathlib info is known once build_src is run,",
                "662": "        # but we cannot use add_installed_pkg_config here either, so we only",
                "663": "        # update the substitution dictionary during npymath build",
                "674": "                       # join('src', 'npymath', 'ieee754.cpp'),",
                "684": "                'include_dirs' : [],  # empty list required for creating npy_math_internal.h",
                "692": "    #######################################################################",
                "693": "    #                     multiarray_tests module                         #",
                "694": "    #######################################################################",
                "707": "    #######################################################################",
                "708": "    #             _multiarray_umath module - common part                  #",
                "709": "    #######################################################################",
                "761": "        # These files are also in MANIFEST.in so that they are always in",
                "762": "        # the source distribution independently of HAVE_CBLAS.",
                "769": "    #######################################################################",
                "770": "    #             _multiarray_umath module - multiarray part              #",
                "771": "    #######################################################################",
                "829": "            # add library sources as distuils does not consider libraries",
                "830": "            # dependencies",
                "910": "            # Remove this once scipy macos arm64 build correctly",
                "911": "            # links to the arm64 npymath library,",
                "912": "            # see gh-22673",
                "918": "    #######################################################################",
                "919": "    #             _multiarray_umath module - umath part                   #",
                "920": "    #######################################################################",
                "984": "            # For testing. Eventually, should use public API and be separate:",
                "1002": "    # we have converted the following into universal intrinsics",
                "1003": "    # so we can bring the benefits of performance for all platforms",
                "1004": "    # not just for avx512 on linux without performance/accuracy regression,",
                "1005": "    # actually the other way around, better performance and",
                "1006": "    # after all maintainable code.",
                "1013": "        # The ordering of names returned by glob is undefined, so we sort",
                "1014": "        # to make builds reproducible.",
                "1037": "    #######################################################################",
                "1038": "    #                       _umath_tests module                           #",
                "1039": "    #######################################################################",
                "1047": "    #######################################################################",
                "1048": "    #                   custom rational dtype module                      #",
                "1049": "    #######################################################################",
                "1054": "    #######################################################################",
                "1055": "    #                        struct_ufunc_test module                     #",
                "1056": "    #######################################################################",
                "1062": "    #######################################################################",
                "1063": "    #                        operand_flag_tests module                    #",
                "1064": "    #######################################################################",
                "1069": "    #######################################################################",
                "1070": "    #                        SIMD module                                  #",
                "1071": "    #######################################################################"
            },
            "comment_modified_diff": {}
        },
        {
            "commit": "63a1fee8d1c1e442aa8ca08ac7cefb195f99b2a9",
            "timestamp": "2023-09-13T22:52:14+02:00",
            "author": "Ralf Gommers",
            "commit_message": "MAINT: remove all `setup.py`'s and related files for distutils builds",
            "additions": 0,
            "deletions": 17,
            "change_type": "DELETE",
            "diff": "@@ -1,17 +0,0 @@\n-#!/usr/bin/env python3\n-def configuration(parent_package='',top_path=None):\n-    from numpy.distutils.misc_util import Configuration\n-    config = Configuration('distutils', parent_package, top_path)\n-    config.add_subpackage('command')\n-    config.add_subpackage('fcompiler')\n-    config.add_subpackage('tests')\n-    config.add_data_files('site.cfg')\n-    config.add_data_files('mingw/gfortran_vs2003_hack.c')\n-    config.add_data_dir('checks')\n-    config.add_data_files('*.pyi')\n-    config.make_config_py()\n-    return config\n-\n-if __name__ == '__main__':\n-    from numpy.distutils.core      import setup\n-    setup(configuration=configuration)\n",
            "comment_added_diff": {},
            "comment_deleted_diff": {
                "1": "#!/usr/bin/env python3"
            },
            "comment_modified_diff": {}
        },
        {
            "commit": "63a1fee8d1c1e442aa8ca08ac7cefb195f99b2a9",
            "timestamp": "2023-09-13T22:52:14+02:00",
            "author": "Ralf Gommers",
            "commit_message": "MAINT: remove all `setup.py`'s and related files for distutils builds",
            "additions": 0,
            "deletions": 74,
            "change_type": "DELETE",
            "diff": "@@ -1,74 +0,0 @@\n-#!/usr/bin/env python3\n-\"\"\"\n-setup.py for installing F2PY\n-\n-Usage:\n-   pip install .\n-\n-Copyright 2001-2005 Pearu Peterson all rights reserved,\n-Pearu Peterson <pearu@cens.ioc.ee>\n-Permission to use, modify, and distribute this software is given under the\n-terms of the NumPy License.\n-\n-NO WARRANTY IS EXPRESSED OR IMPLIED.  USE AT YOUR OWN RISK.\n-$Revision: 1.32 $\n-$Date: 2005/01/30 17:22:14 $\n-Pearu Peterson\n-\n-\"\"\"\n-from numpy.distutils.core import setup\n-from numpy.distutils.misc_util import Configuration\n-\n-\n-from __version__ import version\n-\n-\n-def configuration(parent_package='', top_path=None):\n-    config = Configuration('f2py', parent_package, top_path)\n-    config.add_subpackage('tests')\n-    config.add_subpackage('_backends')\n-    config.add_data_dir('tests/src')\n-    config.add_data_files(\n-        'src/fortranobject.c',\n-        'src/fortranobject.h',\n-        'backends/meson.build.template',\n-    )\n-    config.add_data_files('*.pyi')\n-    return config\n-\n-\n-if __name__ == \"__main__\":\n-\n-    config = configuration(top_path='')\n-    config = config.todict()\n-\n-    config['classifiers'] = [\n-        'Development Status :: 5 - Production/Stable',\n-        'Intended Audience :: Developers',\n-        'Intended Audience :: Science/Research',\n-        'License :: OSI Approved :: NumPy License',\n-        'Natural Language :: English',\n-        'Operating System :: OS Independent',\n-        'Programming Language :: C',\n-        'Programming Language :: Fortran',\n-        'Programming Language :: Python',\n-        'Topic :: Scientific/Engineering',\n-        'Topic :: Software Development :: Code Generators',\n-    ]\n-    setup(version=version,\n-          description=\"F2PY - Fortran to Python Interface Generator\",\n-          author=\"Pearu Peterson\",\n-          author_email=\"pearu@cens.ioc.ee\",\n-          maintainer=\"Pearu Peterson\",\n-          maintainer_email=\"pearu@cens.ioc.ee\",\n-          license=\"BSD\",\n-          platforms=\"Unix, Windows (mingw|cygwin), Mac OSX\",\n-          long_description=\"\"\"\\\n-The Fortran to Python Interface Generator, or F2PY for short, is a\n-command line tool (f2py) for generating Python C/API modules for\n-wrapping Fortran 77/90/95 subroutines, accessing common blocks from\n-Python, and calling Python functions from Fortran (call-backs).\n-Interfacing subroutines/data from Fortran 90/95 modules is supported.\"\"\",\n-          url=\"https://numpy.org/doc/stable/f2py/\",\n-          keywords=['Fortran', 'f2py'],\n-          **config)\n",
            "comment_added_diff": {},
            "comment_deleted_diff": {
                "1": "#!/usr/bin/env python3"
            },
            "comment_modified_diff": {}
        },
        {
            "commit": "63a1fee8d1c1e442aa8ca08ac7cefb195f99b2a9",
            "timestamp": "2023-09-13T22:52:14+02:00",
            "author": "Ralf Gommers",
            "commit_message": "MAINT: remove all `setup.py`'s and related files for distutils builds",
            "additions": 0,
            "deletions": 22,
            "change_type": "DELETE",
            "diff": "@@ -1,22 +0,0 @@\n-import sys\n-\n-def configuration(parent_package='',top_path=None):\n-    from numpy.distutils.misc_util import Configuration\n-    config = Configuration('fft', parent_package, top_path)\n-\n-    config.add_subpackage('tests')\n-\n-    # AIX needs to be told to use large file support - at all times\n-    defs = [('_LARGE_FILES', None)] if sys.platform[:3] == \"aix\" else []\n-    # Configure pocketfft_internal\n-    config.add_extension('_pocketfft_internal',\n-                         sources=['_pocketfft.c'],\n-                         define_macros=defs,\n-                         )\n-\n-    config.add_data_files('*.pyi')\n-    return config\n-\n-if __name__ == '__main__':\n-    from numpy.distutils.core import setup\n-    setup(configuration=configuration)\n",
            "comment_added_diff": {},
            "comment_deleted_diff": {
                "9": "    # AIX needs to be told to use large file support - at all times",
                "11": "    # Configure pocketfft_internal"
            },
            "comment_modified_diff": {}
        },
        {
            "commit": "63a1fee8d1c1e442aa8ca08ac7cefb195f99b2a9",
            "timestamp": "2023-09-13T22:52:14+02:00",
            "author": "Ralf Gommers",
            "commit_message": "MAINT: remove all `setup.py`'s and related files for distutils builds",
            "additions": 0,
            "deletions": 90,
            "change_type": "DELETE",
            "diff": "@@ -1,90 +0,0 @@\n-import os\n-import sys\n-import sysconfig\n-\n-def configuration(parent_package='', top_path=None):\n-    from numpy.distutils.misc_util import Configuration\n-    from numpy.distutils.system_info import get_info, system_info\n-    config = Configuration('linalg', parent_package, top_path)\n-\n-    config.add_subpackage('tests')\n-\n-    # Configure lapack_lite\n-\n-    src_dir = 'lapack_lite'\n-    lapack_lite_src = [\n-        os.path.join(src_dir, 'python_xerbla.c'),\n-        os.path.join(src_dir, 'f2c_z_lapack.c'),\n-        os.path.join(src_dir, 'f2c_c_lapack.c'),\n-        os.path.join(src_dir, 'f2c_d_lapack.c'),\n-        os.path.join(src_dir, 'f2c_s_lapack.c'),\n-        os.path.join(src_dir, 'f2c_lapack.c'),\n-        os.path.join(src_dir, 'f2c_blas.c'),\n-        os.path.join(src_dir, 'f2c_config.c'),\n-        os.path.join(src_dir, 'f2c.c'),\n-    ]\n-    all_sources = config.paths(lapack_lite_src)\n-\n-    if os.environ.get('NPY_USE_BLAS_ILP64', \"0\") != \"0\":\n-        lapack_info = get_info('lapack_ilp64_opt', 2)\n-    else:\n-        lapack_info = get_info('lapack_opt', 0)  # and {}\n-\n-    use_lapack_lite = not lapack_info\n-\n-    if use_lapack_lite:\n-        # This makes numpy.distutils write the fact that lapack_lite\n-        # is being used to numpy.__config__\n-        class numpy_linalg_lapack_lite(system_info):\n-            def calc_info(self):\n-                info = {'language': 'c'}\n-                size_t_size = sysconfig.get_config_var(\"SIZEOF_SIZE_T\")\n-                if size_t_size:\n-                    maxsize = 2**(size_t_size - 1) - 1\n-                else:\n-                    # We prefer using sysconfig as it allows cross-compilation\n-                    # but the information may be missing (e.g. on windows).\n-                    maxsize = sys.maxsize\n-                if maxsize > 2**32:\n-                    # Build lapack-lite in 64-bit integer mode.\n-                    # The suffix is arbitrary (lapack_lite symbols follow it),\n-                    # but use the \"64_\" convention here.\n-                    info['define_macros'] = [\n-                        ('HAVE_BLAS_ILP64', None),\n-                        ('BLAS_SYMBOL_SUFFIX', '64_')\n-                    ]\n-                self.set_info(**info)\n-\n-        lapack_info = numpy_linalg_lapack_lite().get_info(2)\n-\n-    def get_lapack_lite_sources(ext, build_dir):\n-        if use_lapack_lite:\n-            print(\"### Warning:  Using unoptimized lapack ###\")\n-            return all_sources\n-        else:\n-            if sys.platform == 'win32':\n-                print(\"### Warning:  python_xerbla.c is disabled ###\")\n-                return []\n-            return [all_sources[0]]\n-\n-    config.add_extension(\n-        'lapack_lite',\n-        sources=['lapack_litemodule.c', get_lapack_lite_sources],\n-        depends=['lapack_lite/f2c.h'],\n-        extra_info=lapack_info,\n-    )\n-\n-    # umath_linalg module\n-    config.add_extension(\n-        '_umath_linalg',\n-        sources=['umath_linalg.cpp', get_lapack_lite_sources],\n-        depends=['lapack_lite/f2c.h'],\n-        extra_info=lapack_info,\n-        libraries=['npymath'],\n-    )\n-    config.add_data_files('*.pyi')\n-    return config\n-\n-if __name__ == '__main__':\n-    from numpy.distutils.core import setup\n-    setup(configuration=configuration)\n",
            "comment_added_diff": {},
            "comment_deleted_diff": {
                "12": "    # Configure lapack_lite",
                "31": "        lapack_info = get_info('lapack_opt', 0)  # and {}",
                "36": "        # This makes numpy.distutils write the fact that lapack_lite",
                "37": "        # is being used to numpy.__config__",
                "45": "                    # We prefer using sysconfig as it allows cross-compilation",
                "46": "                    # but the information may be missing (e.g. on windows).",
                "49": "                    # Build lapack-lite in 64-bit integer mode.",
                "50": "                    # The suffix is arbitrary (lapack_lite symbols follow it),",
                "51": "                    # but use the \"64_\" convention here.",
                "62": "            print(\"### Warning:  Using unoptimized lapack ###\")",
                "66": "                print(\"### Warning:  python_xerbla.c is disabled ###\")",
                "77": "    # umath_linalg module"
            },
            "comment_modified_diff": {}
        },
        {
            "commit": "63a1fee8d1c1e442aa8ca08ac7cefb195f99b2a9",
            "timestamp": "2023-09-13T22:52:14+02:00",
            "author": "Ralf Gommers",
            "commit_message": "MAINT: remove all `setup.py`'s and related files for distutils builds",
            "additions": 0,
            "deletions": 12,
            "change_type": "DELETE",
            "diff": "@@ -1,12 +0,0 @@\n-#!/usr/bin/env python3\n-def configuration(parent_package='',top_path=None):\n-    from numpy.distutils.misc_util import Configuration\n-    config = Configuration('ma', parent_package, top_path)\n-    config.add_subpackage('tests')\n-    config.add_data_files('*.pyi')\n-    return config\n-\n-if __name__ == \"__main__\":\n-    from numpy.distutils.core import setup\n-    config = configuration(top_path='').todict()\n-    setup(**config)\n",
            "comment_added_diff": {},
            "comment_deleted_diff": {
                "1": "#!/usr/bin/env python3"
            },
            "comment_modified_diff": {}
        },
        {
            "commit": "63a1fee8d1c1e442aa8ca08ac7cefb195f99b2a9",
            "timestamp": "2023-09-13T22:52:14+02:00",
            "author": "Ralf Gommers",
            "commit_message": "MAINT: remove all `setup.py`'s and related files for distutils builds",
            "additions": 0,
            "deletions": 12,
            "change_type": "DELETE",
            "diff": "@@ -1,12 +0,0 @@\n-#!/usr/bin/env python3\n-def configuration(parent_package='', top_path=None):\n-    from numpy.distutils.misc_util import Configuration\n-    config = Configuration('matrixlib', parent_package, top_path)\n-    config.add_subpackage('tests')\n-    config.add_data_files('*.pyi')\n-    return config\n-\n-if __name__ == \"__main__\":\n-    from numpy.distutils.core import setup\n-    config = configuration(top_path='').todict()\n-    setup(**config)\n",
            "comment_added_diff": {},
            "comment_deleted_diff": {
                "1": "#!/usr/bin/env python3"
            },
            "comment_modified_diff": {}
        },
        {
            "commit": "63a1fee8d1c1e442aa8ca08ac7cefb195f99b2a9",
            "timestamp": "2023-09-13T22:52:14+02:00",
            "author": "Ralf Gommers",
            "commit_message": "MAINT: remove all `setup.py`'s and related files for distutils builds",
            "additions": 0,
            "deletions": 159,
            "change_type": "DELETE",
            "diff": "@@ -1,159 +0,0 @@\n-import os\n-import sys\n-from os.path import join\n-\n-from numpy.distutils.system_info import platform_bits\n-from numpy.distutils.msvccompiler import lib_opts_if_msvc\n-\n-\n-def configuration(parent_package='', top_path=None):\n-    from numpy.distutils.misc_util import Configuration, get_mathlibs\n-    config = Configuration('random', parent_package, top_path)\n-\n-    def generate_libraries(ext, build_dir):\n-        config_cmd = config.get_config_cmd()\n-        libs = get_mathlibs()\n-        if sys.platform == 'win32':\n-            libs.extend(['Advapi32', 'Kernel32'])\n-        ext.libraries.extend(libs)\n-        return None\n-\n-    # enable unix large file support on 32 bit systems\n-    # (64 bit off_t, lseek -> lseek64 etc.)\n-    if sys.platform[:3] == 'aix':\n-        defs = [('_LARGE_FILES', None)]\n-    else:\n-        defs = [('_FILE_OFFSET_BITS', '64'),\n-                ('_LARGEFILE_SOURCE', '1'),\n-                ('_LARGEFILE64_SOURCE', '1')]\n-\n-    defs.append(('NPY_NO_DEPRECATED_API', 0))\n-    config.add_subpackage('tests')\n-    config.add_data_dir('tests/data')\n-    config.add_data_dir('_examples')\n-\n-    EXTRA_LINK_ARGS = []\n-    EXTRA_LIBRARIES = ['npyrandom']\n-    if os.name != 'nt':\n-        # Math lib\n-        EXTRA_LIBRARIES.append('m')\n-    # Some bit generators exclude GCC inlining\n-    EXTRA_COMPILE_ARGS = ['-U__GNUC_GNU_INLINE__']\n-\n-    if sys.platform == 'cygwin':\n-        # Export symbols without __declspec(dllexport) for using by cython.\n-        # Using __declspec(dllexport) does not export other necessary symbols\n-        # in Cygwin package's Cython environment, making it impossible to\n-        # import modules.\n-        EXTRA_LINK_ARGS += ['-Wl,--export-all-symbols']\n-\n-    # Use legacy integer variable sizes\n-    LEGACY_DEFS = [('NP_RANDOM_LEGACY', '1')]\n-    PCG64_DEFS = []\n-    # One can force emulated 128-bit arithmetic if one wants.\n-    #PCG64_DEFS += [('PCG_FORCE_EMULATED_128BIT_MATH', '1')]\n-    depends = ['__init__.pxd', 'c_distributions.pxd', 'bit_generator.pxd']\n-\n-    # npyrandom - a library like npymath\n-    npyrandom_sources = [\n-        'src/distributions/logfactorial.c',\n-        'src/distributions/distributions.c',\n-        'src/distributions/random_mvhg_count.c',\n-        'src/distributions/random_mvhg_marginals.c',\n-        'src/distributions/random_hypergeometric.c',\n-    ]\n-\n-    def lib_opts(build_cmd):\n-        \"\"\" Add flags that depend on the compiler.\n-\n-        We can't see which compiler we are using in our scope, because we have\n-        not initialized the distutils build command, so use this deferred\n-        calculation to run when we are building the library.\n-        \"\"\"\n-        opts = lib_opts_if_msvc(build_cmd)\n-        if build_cmd.compiler.compiler_type != 'msvc':\n-            # Some bit generators require c99\n-            opts.append('-std=c99')\n-        return opts\n-\n-    config.add_installed_library('npyrandom',\n-        sources=npyrandom_sources,\n-        install_dir='lib',\n-        build_info={\n-            'include_dirs' : [],  # empty list required for creating npyrandom.h\n-            'extra_compiler_args': [lib_opts],\n-        })\n-\n-    for gen in ['mt19937']:\n-        # gen.pyx, src/gen/gen.c, src/gen/gen-jump.c\n-        config.add_extension(f'_{gen}',\n-                             sources=[f'_{gen}.c',\n-                                      f'src/{gen}/{gen}.c',\n-                                      f'src/{gen}/{gen}-jump.c'],\n-                             include_dirs=['.', 'src', join('src', gen)],\n-                             libraries=EXTRA_LIBRARIES,\n-                             extra_compile_args=EXTRA_COMPILE_ARGS,\n-                             extra_link_args=EXTRA_LINK_ARGS,\n-                             depends=depends + [f'_{gen}.pyx'],\n-                             define_macros=defs,\n-                             )\n-    for gen in ['philox', 'pcg64', 'sfc64']:\n-        # gen.pyx, src/gen/gen.c\n-        _defs = defs + PCG64_DEFS if gen == 'pcg64' else defs\n-        config.add_extension(f'_{gen}',\n-                             sources=[f'_{gen}.c',\n-                                      f'src/{gen}/{gen}.c'],\n-                             include_dirs=['.', 'src', join('src', gen)],\n-                             libraries=EXTRA_LIBRARIES,\n-                             extra_compile_args=EXTRA_COMPILE_ARGS,\n-                             extra_link_args=EXTRA_LINK_ARGS,\n-                             depends=depends + [f'_{gen}.pyx',\n-                                   'bit_generator.pyx', 'bit_generator.pxd'],\n-                             define_macros=_defs,\n-                             )\n-    for gen in ['_common', 'bit_generator']:\n-        # gen.pyx\n-        config.add_extension(gen,\n-                             sources=[f'{gen}.c'],\n-                             libraries=EXTRA_LIBRARIES,\n-                             extra_compile_args=EXTRA_COMPILE_ARGS,\n-                             extra_link_args=EXTRA_LINK_ARGS,\n-                             include_dirs=['.', 'src'],\n-                             depends=depends + [f'{gen}.pyx', f'{gen}.pxd',],\n-                             define_macros=defs,\n-                             )\n-        config.add_data_files(f'{gen}.pxd')\n-    for gen in ['_generator', '_bounded_integers']:\n-        # gen.pyx, src/distributions/distributions.c\n-        config.add_extension(gen,\n-                             sources=[f'{gen}.c'],\n-                             libraries=EXTRA_LIBRARIES + ['npymath'],\n-                             extra_compile_args=EXTRA_COMPILE_ARGS,\n-                             include_dirs=['.', 'src'],\n-                             extra_link_args=EXTRA_LINK_ARGS,\n-                             depends=depends + [f'{gen}.pyx'],\n-                             define_macros=defs,\n-                             )\n-    config.add_data_files('_bounded_integers.pxd')\n-    mtrand_libs = ['m', 'npymath'] if os.name != 'nt' else ['npymath']\n-    config.add_extension('mtrand',\n-                         sources=['mtrand.c',\n-                                  'src/legacy/legacy-distributions.c',\n-                                  'src/distributions/distributions.c',\n-                                 ],\n-                         include_dirs=['.', 'src', 'src/legacy'],\n-                         libraries=mtrand_libs,\n-                         extra_compile_args=EXTRA_COMPILE_ARGS,\n-                         extra_link_args=EXTRA_LINK_ARGS,\n-                         depends=depends + ['mtrand.pyx'],\n-                         define_macros=defs + LEGACY_DEFS,\n-                         )\n-    config.add_data_files(*depends)\n-    config.add_data_files('*.pyi')\n-    return config\n-\n-\n-if __name__ == '__main__':\n-    from numpy.distutils.core import setup\n-\n-    setup(configuration=configuration)\n",
            "comment_added_diff": {},
            "comment_deleted_diff": {
                "21": "    # enable unix large file support on 32 bit systems",
                "22": "    # (64 bit off_t, lseek -> lseek64 etc.)",
                "38": "        # Math lib",
                "40": "    # Some bit generators exclude GCC inlining",
                "44": "        # Export symbols without __declspec(dllexport) for using by cython.",
                "45": "        # Using __declspec(dllexport) does not export other necessary symbols",
                "46": "        # in Cygwin package's Cython environment, making it impossible to",
                "47": "        # import modules.",
                "50": "    # Use legacy integer variable sizes",
                "53": "    # One can force emulated 128-bit arithmetic if one wants.",
                "54": "    #PCG64_DEFS += [('PCG_FORCE_EMULATED_128BIT_MATH', '1')]",
                "57": "    # npyrandom - a library like npymath",
                "75": "            # Some bit generators require c99",
                "83": "            'include_dirs' : [],  # empty list required for creating npyrandom.h",
                "88": "        # gen.pyx, src/gen/gen.c, src/gen/gen-jump.c",
                "101": "        # gen.pyx, src/gen/gen.c",
                "115": "        # gen.pyx",
                "127": "        # gen.pyx, src/distributions/distributions.c"
            },
            "comment_modified_diff": {}
        },
        {
            "commit": "63a1fee8d1c1e442aa8ca08ac7cefb195f99b2a9",
            "timestamp": "2023-09-13T22:52:14+02:00",
            "author": "Ralf Gommers",
            "commit_message": "MAINT: remove all `setup.py`'s and related files for distutils builds",
            "additions": 0,
            "deletions": 33,
            "change_type": "DELETE",
            "diff": "@@ -1,33 +0,0 @@\n-#!/usr/bin/env python3\n-\n-def configuration(parent_package='',top_path=None):\n-    from numpy.distutils.misc_util import Configuration\n-    config = Configuration('numpy', parent_package, top_path)\n-\n-    config.add_subpackage('array_api')\n-    config.add_subpackage('compat')\n-    config.add_subpackage('core')\n-    config.add_subpackage('distutils')\n-    config.add_subpackage('doc')\n-    config.add_subpackage('f2py')\n-    config.add_subpackage('fft')\n-    config.add_subpackage('lib')\n-    config.add_subpackage('linalg')\n-    config.add_subpackage('ma')\n-    config.add_subpackage('matrixlib')\n-    config.add_subpackage('polynomial')\n-    config.add_subpackage('random')\n-    config.add_subpackage('testing')\n-    config.add_subpackage('typing')\n-    config.add_subpackage('_typing')\n-    config.add_subpackage('_utils')\n-    config.add_data_dir('doc')\n-    config.add_data_files('py.typed')\n-    config.add_data_files('*.pyi')\n-    config.add_subpackage('tests')\n-    config.add_subpackage('_pyinstaller')\n-    config.make_config_py() # installs __config__.py\n-    return config\n-\n-if __name__ == '__main__':\n-    print('This is the wrong setup.py file to run')\n",
            "comment_added_diff": {},
            "comment_deleted_diff": {
                "1": "#!/usr/bin/env python3",
                "29": "    config.make_config_py() # installs __config__.py"
            },
            "comment_modified_diff": {}
        },
        {
            "commit": "63a1fee8d1c1e442aa8ca08ac7cefb195f99b2a9",
            "timestamp": "2023-09-13T22:52:14+02:00",
            "author": "Ralf Gommers",
            "commit_message": "MAINT: remove all `setup.py`'s and related files for distutils builds",
            "additions": 0,
            "deletions": 21,
            "change_type": "DELETE",
            "diff": "@@ -1,21 +0,0 @@\n-#!/usr/bin/env python3\n-\n-def configuration(parent_package='',top_path=None):\n-    from numpy.distutils.misc_util import Configuration\n-    config = Configuration('testing', parent_package, top_path)\n-\n-    config.add_subpackage('_private')\n-    config.add_subpackage('tests')\n-    config.add_data_files('*.pyi')\n-    config.add_data_files('_private/*.pyi')\n-    return config\n-\n-if __name__ == '__main__':\n-    from numpy.distutils.core import setup\n-    setup(maintainer=\"NumPy Developers\",\n-          maintainer_email=\"numpy-dev@numpy.org\",\n-          description=\"NumPy test module\",\n-          url=\"https://www.numpy.org\",\n-          license=\"NumPy License (BSD Style)\",\n-          configuration=configuration,\n-          )\n",
            "comment_added_diff": {},
            "comment_deleted_diff": {
                "1": "#!/usr/bin/env python3"
            },
            "comment_modified_diff": {}
        },
        {
            "commit": "63a1fee8d1c1e442aa8ca08ac7cefb195f99b2a9",
            "timestamp": "2023-09-13T22:52:14+02:00",
            "author": "Ralf Gommers",
            "commit_message": "MAINT: remove all `setup.py`'s and related files for distutils builds",
            "additions": 0,
            "deletions": 573,
            "change_type": "DELETE",
            "diff": "@@ -1,573 +0,0 @@\n-#!/usr/bin/env python3\n-\"\"\"\n-Numpy build options can be modified with a site.cfg file.\n-See site.cfg.example for a template and more information.\n-\"\"\"\n-\n-import os\n-from pathlib import Path\n-import sys\n-import subprocess\n-import textwrap\n-import warnings\n-import builtins\n-import re\n-import tempfile\n-\n-from distutils.errors import CompileError\n-\n-# Python supported version checks. Keep right after stdlib imports to ensure we\n-# get a sensible error for older Python versions\n-if sys.version_info[:2] < (3, 9):\n-    raise RuntimeError(\"Python version >= 3.9 required.\")\n-\n-\n-# This is a bit hackish: we are setting a global variable so that the main\n-# numpy __init__ can detect if it is being loaded by the setup routine, to\n-# avoid attempting to load components that aren't built yet.  While ugly, it's\n-# a lot more robust than what was previously being used.\n-builtins.__NUMPY_SETUP__ = True\n-\n-# Needed for backwards code compatibility below and in some CI scripts.\n-# The version components are changed from ints to strings, but only VERSION\n-# seems to matter outside of this module and it was already a str.\n-FULLVERSION = subprocess.check_output([\n-    sys.executable,\n-    'numpy/_build_utils/gitversion.py'\n-]).strip().decode('ascii')\n-\n-# Write git version to disk\n-subprocess.check_output([\n-    sys.executable,\n-    'numpy/_build_utils/gitversion.py', '--write', 'numpy/version.py'\n-])\n-\n-# Capture the version string:\n-# 1.22.0.dev0+ ... -> ISRELEASED == False, VERSION == 1.22.0\n-# 1.22.0rc1+ ... -> ISRELEASED == False, VERSION == 1.22.0\n-# 1.22.0 ... -> ISRELEASED == True, VERSION == 1.22.0\n-# 1.22.0rc1 ... -> ISRELEASED == True, VERSION == 1.22.0\n-ISRELEASED = re.search(r'(dev|\\+)', FULLVERSION) is None\n-_V_MATCH = re.match(r'(\\d+)\\.(\\d+)\\.(\\d+)', FULLVERSION)\n-if _V_MATCH is None:\n-    raise RuntimeError(f'Cannot parse version {FULLVERSION}')\n-MAJOR, MINOR, MICRO = _V_MATCH.groups()\n-VERSION = '{}.{}.{}'.format(MAJOR, MINOR, MICRO)\n-\n-# The first version not in the `Programming Language :: Python :: ...` classifiers below\n-if sys.version_info >= (3, 12):\n-    fmt = \"NumPy {} may not yet support Python {}.{}.\"\n-    warnings.warn(\n-        fmt.format(VERSION, *sys.version_info[:2]),\n-        RuntimeWarning)\n-    del fmt\n-\n-# BEFORE importing setuptools, remove MANIFEST. Otherwise it may not be\n-# properly updated when the contents of directories change (true for distutils,\n-# not sure about setuptools).\n-if os.path.exists('MANIFEST'):\n-    os.remove('MANIFEST')\n-\n-# We need to import setuptools here in order for it to persist in sys.modules.\n-# Its presence/absence is used in subclassing setup in numpy/distutils/core.py.\n-# However, we need to run the distutils version of sdist, so import that first\n-# so that it is in sys.modules\n-import numpy.distutils.command.sdist\n-import setuptools\n-if int(setuptools.__version__.split('.')[0]) >= 60:\n-    # setuptools >= 60 switches to vendored distutils by default; this\n-    # may break the numpy build, so make sure the stdlib version is used\n-    try:\n-        setuptools_use_distutils = os.environ['SETUPTOOLS_USE_DISTUTILS']\n-    except KeyError:\n-        os.environ['SETUPTOOLS_USE_DISTUTILS'] = \"stdlib\"\n-    else:\n-        if setuptools_use_distutils != \"stdlib\":\n-            raise RuntimeError(\"setuptools versions >= '60.0.0' require \"\n-                    \"SETUPTOOLS_USE_DISTUTILS=stdlib in the environment\")\n-\n-CLASSIFIERS = \"\"\"\\\n-Development Status :: 5 - Production/Stable\n-Intended Audience :: Science/Research\n-Intended Audience :: Developers\n-License :: OSI Approved :: BSD License\n-Programming Language :: C\n-Programming Language :: Python\n-Programming Language :: Python :: 3\n-Programming Language :: Python :: 3.9\n-Programming Language :: Python :: 3.10\n-Programming Language :: Python :: 3.11\n-Programming Language :: Python :: 3 :: Only\n-Programming Language :: Python :: Implementation :: CPython\n-Topic :: Software Development\n-Topic :: Scientific/Engineering\n-Typing :: Typed\n-Operating System :: Microsoft :: Windows\n-Operating System :: POSIX\n-Operating System :: Unix\n-Operating System :: MacOS\n-\"\"\"\n-\n-def configuration(parent_package='', top_path=None):\n-    from numpy.distutils.misc_util import Configuration\n-\n-    config = Configuration(None, parent_package, top_path)\n-    config.set_options(ignore_setup_xxx_py=True,\n-                       assume_default_configuration=True,\n-                       delegate_options_to_subpackages=True,\n-                       quiet=True)\n-\n-    config.add_subpackage('numpy')\n-    config.add_data_files(('numpy', 'LICENSE.txt'))\n-    config.add_data_files(('numpy', 'numpy/*.pxd'))\n-\n-    config.get_version('numpy/version.py')  # sets config.version\n-\n-    return config\n-\n-\n-def check_submodules():\n-    \"\"\" verify that the submodules are checked out and clean\n-        use `git submodule update --init`; on failure\n-    \"\"\"\n-    if not os.path.exists('.git'):\n-        return\n-    with open('.gitmodules') as f:\n-        for line in f:\n-            if 'path' in line:\n-                p = line.split('=')[-1].strip()\n-                if not os.path.exists(p):\n-                    raise ValueError('Submodule {} missing'.format(p))\n-\n-    proc = subprocess.Popen(['git', 'submodule', 'status'],\n-                            stdout=subprocess.PIPE)\n-    status, _ = proc.communicate()\n-    status = status.decode(\"ascii\", \"replace\")\n-    for line in status.splitlines():\n-        if line.startswith('-') or line.startswith('+'):\n-            raise ValueError('Submodule not clean: {}'.format(line))\n-\n-\n-class concat_license_files():\n-    \"\"\"Merge LICENSE.txt and LICENSES_bundled.txt for sdist creation\n-\n-    Done this way to keep LICENSE.txt in repo as exact BSD 3-clause (see\n-    gh-13447).  This makes GitHub state correctly how NumPy is licensed.\n-    \"\"\"\n-    def __init__(self):\n-        self.f1 = 'LICENSE.txt'\n-        self.f2 = 'LICENSES_bundled.txt'\n-\n-    def __enter__(self):\n-        \"\"\"Concatenate files and remove LICENSES_bundled.txt\"\"\"\n-        with open(self.f1) as f1:\n-            self.bsd_text = f1.read()\n-\n-        with open(self.f1, 'a') as f1:\n-            with open(self.f2) as f2:\n-                self.bundled_text = f2.read()\n-                f1.write('\\n\\n')\n-                f1.write(self.bundled_text)\n-\n-    def __exit__(self, exception_type, exception_value, traceback):\n-        \"\"\"Restore content of both files\"\"\"\n-        with open(self.f1, 'w') as f:\n-            f.write(self.bsd_text)\n-\n-# Need to inherit from versioneer version of sdist to get the encoded\n-# version information.\n-class sdist_checked:\n-    \"\"\" check submodules on sdist to prevent incomplete tarballs \"\"\"\n-    def run(self):\n-        check_submodules()\n-        with concat_license_files():\n-            super().run()\n-\n-\n-def get_build_overrides():\n-    \"\"\"\n-    Custom build commands to add std flags if required to compilation\n-    \"\"\"\n-    from numpy.distutils.command.build_clib import build_clib\n-    from numpy.distutils.command.build_ext import build_ext\n-    from numpy._utils import _pep440\n-\n-    def try_compile(compiler, file, flags = [], verbose=False):\n-        bk_ver = getattr(compiler, 'verbose', False)\n-        compiler.verbose = verbose\n-        try:\n-            compiler.compile([file], extra_postargs=flags)\n-            return True, ''\n-        except CompileError as e:\n-            return False, str(e)\n-        finally:\n-            compiler.verbose = bk_ver\n-\n-    def flags_is_required(compiler, is_cpp, flags, code):\n-        if is_cpp:\n-            compiler = compiler.cxx_compiler()\n-            suf = '.cpp'\n-        else:\n-            suf = '.c'\n-        with tempfile.TemporaryDirectory() as temp_dir:\n-            tmp_file = os.path.join(temp_dir, \"test\" + suf)\n-            with open(tmp_file, \"w+\") as f:\n-                f.write(code)\n-            # without specify any flags in case of the required\n-            # standard already supported by default, then there's\n-            # no need for passing the flags\n-            comp = try_compile(compiler, tmp_file)\n-            if not comp[0]:\n-                comp = try_compile(compiler, tmp_file, flags)\n-                if not comp[0]:\n-                    # rerun to verbose the error\n-                    try_compile(compiler, tmp_file, flags, True)\n-                    if is_cpp:\n-                        raise RuntimeError(\n-                            \"Broken toolchain during testing C++ compiler. \\n\"\n-                            \"A compiler with support for C++17 language \"\n-                            \"features is required.\\n\"\n-                            f\"Triggered the following error: {comp[1]}.\"\n-                        )\n-                    else:\n-                        raise RuntimeError(\n-                            \"Broken toolchain during testing C compiler. \\n\"\n-                            \"A compiler with support for C99 language \"\n-                            \"features is required.\\n\"\n-                            f\"Triggered the following error: {comp[1]}.\"\n-                        )\n-                return True\n-        return False\n-\n-    def std_cxx_flags(cmd):\n-        compiler = cmd.compiler\n-        flags = getattr(compiler, '__np_cache_cpp_flags', None)\n-        if flags is not None:\n-            return flags\n-        flags = dict(\n-            msvc = ['/std:c++17']\n-        ).get(compiler.compiler_type, ['-std=c++17'])\n-        # These flags are used to compile any C++ source within Numpy.\n-        # They are chosen to have very few runtime dependencies.\n-        extra_flags = dict(\n-            # to update #def __cplusplus with enabled C++ version\n-            msvc = ['/Zc:__cplusplus']\n-        ).get(compiler.compiler_type, [\n-            # The following flag is used to avoid emit any extra code\n-            # from STL since extensions are build by C linker and\n-            # without C++ runtime dependencies.\n-            '-fno-threadsafe-statics',\n-            '-D__STDC_VERSION__=0',  # for compatibility with C headers\n-            '-fno-exceptions',  # no exception support\n-            '-fno-rtti'  # no runtime type information\n-        ])\n-        if not flags_is_required(compiler, True, flags, textwrap.dedent('''\n-            #include <type_traits>\n-            template<typename ...T>\n-            constexpr bool test_fold = (... && std::is_const_v<T>);\n-            int main()\n-            {\n-                if (test_fold<int, const int>) {\n-                    return 0;\n-                }\n-                else {\n-                    return -1;\n-                }\n-            }\n-        ''')):\n-            flags.clear()\n-        flags += extra_flags\n-        setattr(compiler, '__np_cache_cpp_flags', flags)\n-        return flags\n-\n-    def std_c_flags(cmd):\n-        compiler = cmd.compiler\n-        flags = getattr(compiler, '__np_cache_c_flags', None)\n-        if flags is not None:\n-            return flags\n-        flags = dict(\n-            msvc = []\n-        ).get(compiler.compiler_type, ['-std=c99'])\n-\n-        if not flags_is_required(compiler, False, flags, textwrap.dedent('''\n-            inline static int test_inline() { return 0; }\n-            int main(void)\n-            { return test_inline(); }\n-        ''')):\n-            flags.clear()\n-\n-        setattr(compiler, '__np_cache_c_flags', flags)\n-        return flags\n-\n-    class new_build_clib(build_clib):\n-        def build_a_library(self, build_info, lib_name, libraries):\n-            build_info['extra_cflags'] = std_c_flags(self)\n-            build_info['extra_cxxflags'] = std_cxx_flags(self)\n-            build_clib.build_a_library(self, build_info, lib_name, libraries)\n-\n-    class new_build_ext(build_ext):\n-        def build_extension(self, ext):\n-            ext.extra_c_compile_args += std_c_flags(self)\n-            ext.extra_cxx_compile_args += std_cxx_flags(self)\n-            build_ext.build_extension(self, ext)\n-    return new_build_clib, new_build_ext\n-\n-def generate_cython():\n-    # Check Cython version\n-    from numpy._utils import _pep440\n-    try:\n-        # try the cython in the installed python first (somewhat related to\n-        # scipy/scipy#2397)\n-        import Cython\n-        from Cython.Compiler.Version import version as cython_version\n-    except ImportError as e:\n-        # The `cython` command need not point to the version installed in the\n-        # Python running this script, so raise an error to avoid the chance of\n-        # using the wrong version of Cython.\n-        msg = 'Cython needs to be installed in Python as a module'\n-        raise OSError(msg) from e\n-    else:\n-        # Note: keep in sync with that in pyproject.toml\n-        # Update for Python 3.11\n-        required_version = '0.29.30'\n-\n-        if _pep440.parse(cython_version) < _pep440.Version(required_version):\n-            cython_path = Cython.__file__\n-            msg = 'Building NumPy requires Cython >= {}, found {} at {}'\n-            msg = msg.format(required_version, cython_version, cython_path)\n-            raise RuntimeError(msg)\n-\n-    # Process files\n-    cwd = os.path.abspath(os.path.dirname(__file__))\n-    print(\"Cythonizing sources\")\n-    for d in ('random',):\n-        p = subprocess.call([sys.executable,\n-                             os.path.join(cwd, 'tools', 'cythonize.py'),\n-                             'numpy/{0}'.format(d)],\n-                            cwd=cwd)\n-        if p != 0:\n-            raise RuntimeError(\"Running cythonize failed!\")\n-\n-\n-def parse_setuppy_commands():\n-    \"\"\"Check the commands and respond appropriately.  Disable broken commands.\n-\n-    Return a boolean value for whether or not to run the build or not (avoid\n-    parsing Cython and template files if False).\n-    \"\"\"\n-    args = sys.argv[1:]\n-\n-    if not args:\n-        # User forgot to give an argument probably, let setuptools handle that.\n-        return True\n-\n-    info_commands = ['--help-commands', '--name', '--version', '-V',\n-                     '--fullname', '--author', '--author-email',\n-                     '--maintainer', '--maintainer-email', '--contact',\n-                     '--contact-email', '--url', '--license', '--description',\n-                     '--long-description', '--platforms', '--classifiers',\n-                     '--keywords', '--provides', '--requires', '--obsoletes',\n-                     'version',]\n-\n-    for command in info_commands:\n-        if command in args:\n-            return False\n-\n-    # Note that 'alias', 'saveopts' and 'setopt' commands also seem to work\n-    # fine as they are, but are usually used together with one of the commands\n-    # below and not standalone.  Hence they're not added to good_commands.\n-    good_commands = ('develop', 'sdist', 'build', 'build_ext', 'build_py',\n-                     'build_clib', 'build_scripts', 'bdist_wheel', 'bdist_rpm',\n-                     'bdist_wininst', 'bdist_msi', 'bdist_mpkg', 'build_src',\n-                     'bdist_egg')\n-\n-    for command in good_commands:\n-        if command in args:\n-            return True\n-\n-    # The following commands are supported, but we need to show more\n-    # useful messages to the user\n-    if 'install' in args:\n-        print(textwrap.dedent(\"\"\"\n-            Note: if you need reliable uninstall behavior, then install\n-            with pip instead of using `setup.py install`:\n-\n-              - `pip install .`       (from a git repo or downloaded source\n-                                       release)\n-              - `pip install numpy`   (last NumPy release on PyPI)\n-\n-            \"\"\"))\n-        return True\n-\n-    if '--help' in args or '-h' in sys.argv[1]:\n-        print(textwrap.dedent(\"\"\"\n-            NumPy-specific help\n-            -------------------\n-\n-            To install NumPy from here with reliable uninstall, we recommend\n-            that you use `pip install .`. To install the latest NumPy release\n-            from PyPI, use `pip install numpy`.\n-\n-            For help with build/installation issues, please ask on the\n-            numpy-discussion mailing list.  If you are sure that you have run\n-            into a bug, please report it at https://github.com/numpy/numpy/issues.\n-\n-            Setuptools commands help\n-            ------------------------\n-            \"\"\"))\n-        return False\n-\n-    # The following commands aren't supported.  They can only be executed when\n-    # the user explicitly adds a --force command-line argument.\n-    bad_commands = dict(\n-        test=\"\"\"\n-            `setup.py test` is not supported.  Use one of the following\n-            instead:\n-\n-              - `python runtests.py`              (to build and test)\n-              - `python runtests.py --no-build`   (to test installed numpy)\n-              - `>>> numpy.test()`           (run tests for installed numpy\n-                                              from within an interpreter)\n-            \"\"\",\n-        upload=\"\"\"\n-            `setup.py upload` is not supported, because it's insecure.\n-            Instead, build what you want to upload and upload those files\n-            with `twine upload -s <filenames>` instead.\n-            \"\"\",\n-        clean=\"\"\"\n-            `setup.py clean` is not supported, use one of the following instead:\n-\n-              - `git clean -xdf` (cleans all files)\n-              - `git clean -Xdf` (cleans all versioned files, doesn't touch\n-                                  files that aren't checked into the git repo)\n-            \"\"\",\n-        build_sphinx=\"\"\"\n-            `setup.py build_sphinx` is not supported, use the\n-            Makefile under doc/\"\"\",\n-        flake8=\"`setup.py flake8` is not supported, use flake8 standalone\",\n-        )\n-    bad_commands['nosetests'] = bad_commands['test']\n-    for command in ('upload_docs', 'easy_install', 'bdist', 'bdist_dumb',\n-                    'register', 'check', 'install_data', 'install_headers',\n-                    'install_lib', 'install_scripts', ):\n-        bad_commands[command] = \"`setup.py %s` is not supported\" % command\n-\n-    for command in bad_commands.keys():\n-        if command in args:\n-            print(textwrap.dedent(bad_commands[command]) +\n-                  \"\\nAdd `--force` to your command to use it anyway if you \"\n-                  \"must (unsupported).\\n\")\n-            sys.exit(1)\n-\n-    # Commands that do more than print info, but also don't need Cython and\n-    # template parsing.\n-    other_commands = ['egg_info', 'install_egg_info', 'rotate', 'dist_info']\n-    for command in other_commands:\n-        if command in args:\n-            return False\n-\n-    # If we got here, we didn't detect what setup.py command was given\n-    raise RuntimeError(\"Unrecognized setuptools command: {}\".format(args))\n-\n-\n-def get_docs_url():\n-    if 'dev' in VERSION:\n-        return \"https://numpy.org/devdocs\"\n-    else:\n-        # For releases, this URL ends up on PyPI.\n-        # By pinning the version, users looking at old PyPI releases can get\n-        # to the associated docs easily.\n-        return \"https://numpy.org/doc/{}.{}\".format(MAJOR, MINOR)\n-\n-\n-from numpy.distutils.core import numpy_cmdclass as cmdclass\n-\n-def setup_package():\n-    src_path = os.path.dirname(os.path.abspath(__file__))\n-    old_path = os.getcwd()\n-    os.chdir(src_path)\n-    sys.path.insert(0, src_path)\n-\n-    # The f2py scripts that will be installed\n-    if sys.platform == 'win32':\n-        f2py_cmds = [\n-            'f2py = numpy.f2py.f2py2e:main',\n-            ]\n-    else:\n-        f2py_cmds = [\n-            'f2py = numpy.f2py.f2py2e:main',\n-            'f2py%s = numpy.f2py.f2py2e:main' % sys.version_info[:1],\n-            'f2py%s.%s = numpy.f2py.f2py2e:main' % sys.version_info[:2],\n-            ]\n-\n-    metadata = dict(\n-        name='numpy',\n-        maintainer=\"NumPy Developers\",\n-        maintainer_email=\"numpy-discussion@python.org\",\n-        description=\"Fundamental package for array computing in Python\",\n-        long_description=Path(\"README.md\").read_text(encoding=\"utf-8\"),\n-        long_description_content_type=\"text/markdown\",\n-        url=\"https://www.numpy.org\",\n-        author=\"Travis E. Oliphant et al.\",\n-        download_url=\"https://pypi.python.org/pypi/numpy\",\n-        project_urls={\n-            \"Bug Tracker\": \"https://github.com/numpy/numpy/issues\",\n-            \"Documentation\": get_docs_url(),\n-            \"Source Code\": \"https://github.com/numpy/numpy\",\n-        },\n-        license='BSD-3-Clause',\n-        classifiers=[_f for _f in CLASSIFIERS.split('\\n') if _f],\n-        platforms=[\"Windows\", \"Linux\", \"Solaris\", \"Mac OS-X\", \"Unix\"],\n-        test_suite='pytest',\n-        version=VERSION,\n-        cmdclass=cmdclass,\n-        python_requires='>=3.9',\n-        zip_safe=False,\n-        entry_points={\n-            'console_scripts': f2py_cmds,\n-            'array_api': ['numpy = numpy.array_api'],\n-            'pyinstaller40': ['hook-dirs = numpy:_pyinstaller_hooks_dir'],\n-        },\n-    )\n-\n-    if \"--force\" in sys.argv:\n-        run_build = True\n-        sys.argv.remove('--force')\n-    else:\n-        # Raise errors for unsupported commands, improve help output, etc.\n-        run_build = parse_setuppy_commands()\n-\n-    if run_build:\n-        # patches distutils, even though we don't use it\n-        #from setuptools import setup\n-        from numpy.distutils.core import setup\n-\n-        if 'sdist' not in sys.argv:\n-            # Generate Cython sources, unless we're generating an sdist\n-            generate_cython()\n-\n-        metadata['configuration'] = configuration\n-        # Customize extension building\n-        cmdclass['build_clib'], cmdclass['build_ext'] = get_build_overrides()\n-    else:\n-        #from numpy.distutils.core import setup\n-        from setuptools import setup\n-        # workaround for broken --no-build-isolation with newer setuptools,\n-        # see gh-21288\n-        metadata[\"packages\"] = []\n-\n-    try:\n-        setup(**metadata)\n-    finally:\n-        del sys.path[0]\n-        os.chdir(old_path)\n-    return\n-\n-\n-if __name__ == '__main__':\n-    setup_package()\n-    # This may avoid problems where numpy is installed via ``*_requires`` by\n-    # setuptools, the global namespace isn't reset properly, and then numpy is\n-    # imported later (which will then fail to load numpy extension modules).\n-    # See gh-7956 for details\n-    del builtins.__NUMPY_SETUP__\n",
            "comment_added_diff": {},
            "comment_deleted_diff": {
                "1": "#!/usr/bin/env python3",
                "19": "# Python supported version checks. Keep right after stdlib imports to ensure we",
                "20": "# get a sensible error for older Python versions",
                "25": "# This is a bit hackish: we are setting a global variable so that the main",
                "26": "# numpy __init__ can detect if it is being loaded by the setup routine, to",
                "27": "# avoid attempting to load components that aren't built yet.  While ugly, it's",
                "28": "# a lot more robust than what was previously being used.",
                "31": "# Needed for backwards code compatibility below and in some CI scripts.",
                "32": "# The version components are changed from ints to strings, but only VERSION",
                "33": "# seems to matter outside of this module and it was already a str.",
                "39": "# Write git version to disk",
                "45": "# Capture the version string:",
                "46": "# 1.22.0.dev0+ ... -> ISRELEASED == False, VERSION == 1.22.0",
                "47": "# 1.22.0rc1+ ... -> ISRELEASED == False, VERSION == 1.22.0",
                "48": "# 1.22.0 ... -> ISRELEASED == True, VERSION == 1.22.0",
                "49": "# 1.22.0rc1 ... -> ISRELEASED == True, VERSION == 1.22.0",
                "57": "# The first version not in the `Programming Language :: Python :: ...` classifiers below",
                "65": "# BEFORE importing setuptools, remove MANIFEST. Otherwise it may not be",
                "66": "# properly updated when the contents of directories change (true for distutils,",
                "67": "# not sure about setuptools).",
                "71": "# We need to import setuptools here in order for it to persist in sys.modules.",
                "72": "# Its presence/absence is used in subclassing setup in numpy/distutils/core.py.",
                "73": "# However, we need to run the distutils version of sdist, so import that first",
                "74": "# so that it is in sys.modules",
                "78": "    # setuptools >= 60 switches to vendored distutils by default; this",
                "79": "    # may break the numpy build, so make sure the stdlib version is used",
                "124": "    config.get_version('numpy/version.py')  # sets config.version",
                "177": "# Need to inherit from versioneer version of sdist to get the encoded",
                "178": "# version information.",
                "216": "            # without specify any flags in case of the required",
                "217": "            # standard already supported by default, then there's",
                "218": "            # no need for passing the flags",
                "223": "                    # rerun to verbose the error",
                "250": "        # These flags are used to compile any C++ source within Numpy.",
                "251": "        # They are chosen to have very few runtime dependencies.",
                "253": "            # to update #def __cplusplus with enabled C++ version",
                "256": "            # The following flag is used to avoid emit any extra code",
                "257": "            # from STL since extensions are build by C linker and",
                "258": "            # without C++ runtime dependencies.",
                "260": "            '-D__STDC_VERSION__=0',  # for compatibility with C headers",
                "261": "            '-fno-exceptions',  # no exception support",
                "262": "            '-fno-rtti'  # no runtime type information",
                "265": "            #include <type_traits>",
                "316": "    # Check Cython version",
                "319": "        # try the cython in the installed python first (somewhat related to",
                "320": "        # scipy/scipy#2397)",
                "324": "        # The `cython` command need not point to the version installed in the",
                "325": "        # Python running this script, so raise an error to avoid the chance of",
                "326": "        # using the wrong version of Cython.",
                "330": "        # Note: keep in sync with that in pyproject.toml",
                "331": "        # Update for Python 3.11",
                "340": "    # Process files",
                "361": "        # User forgot to give an argument probably, let setuptools handle that.",
                "376": "    # Note that 'alias', 'saveopts' and 'setopt' commands also seem to work",
                "377": "    # fine as they are, but are usually used together with one of the commands",
                "378": "    # below and not standalone.  Hence they're not added to good_commands.",
                "388": "    # The following commands are supported, but we need to show more",
                "389": "    # useful messages to the user",
                "420": "    # The following commands aren't supported.  They can only be executed when",
                "421": "    # the user explicitly adds a --force command-line argument.",
                "462": "    # Commands that do more than print info, but also don't need Cython and",
                "463": "    # template parsing.",
                "469": "    # If we got here, we didn't detect what setup.py command was given",
                "477": "        # For releases, this URL ends up on PyPI.",
                "478": "        # By pinning the version, users looking at old PyPI releases can get",
                "479": "        # to the associated docs easily.",
                "491": "    # The f2py scripts that will be installed",
                "537": "        # Raise errors for unsupported commands, improve help output, etc.",
                "541": "        # patches distutils, even though we don't use it",
                "542": "        #from setuptools import setup",
                "546": "            # Generate Cython sources, unless we're generating an sdist",
                "550": "        # Customize extension building",
                "553": "        #from numpy.distutils.core import setup",
                "555": "        # workaround for broken --no-build-isolation with newer setuptools,",
                "556": "        # see gh-21288",
                "569": "    # This may avoid problems where numpy is installed via ``*_requires`` by",
                "570": "    # setuptools, the global namespace isn't reset properly, and then numpy is",
                "571": "    # imported later (which will then fail to load numpy extension modules).",
                "572": "    # See gh-7956 for details"
            },
            "comment_modified_diff": {}
        },
        {
            "commit": "4727fb1e794d8d42cb389ee5a54b7d9b3da54ae4",
            "timestamp": "2023-09-17T19:36:15+02:00",
            "author": "Mateusz Sok\u00f3\u0142",
            "commit_message": "Update numpy/setup.py",
            "additions": 35,
            "deletions": 0,
            "change_type": "ADD",
            "diff": "@@ -0,0 +1,35 @@\n+#!/usr/bin/env python3\n+\n+def configuration(parent_package='',top_path=None):\n+    from numpy.distutils.misc_util import Configuration\n+    config = Configuration('numpy', parent_package, top_path)\n+\n+    config.add_subpackage('array_api')\n+    config.add_subpackage('compat')\n+    config.add_subpackage('core')\n+    config.add_subpackage('distutils')\n+    config.add_subpackage('doc')\n+    config.add_subpackage('f2py')\n+    config.add_subpackage('fft')\n+    config.add_subpackage('lib')\n+    config.add_subpackage('linalg')\n+    config.add_subpackage('ma')\n+    config.add_subpackage('matrixlib')\n+    config.add_subpackage('polynomial')\n+    config.add_subpackage('random')\n+    config.add_subpackage('rec')\n+    config.add_subpackage('char')\n+    config.add_subpackage('testing')\n+    config.add_subpackage('typing')\n+    config.add_subpackage('_typing')\n+    config.add_subpackage('_utils')\n+    config.add_data_dir('doc')\n+    config.add_data_files('py.typed')\n+    config.add_data_files('*.pyi')\n+    config.add_subpackage('tests')\n+    config.add_subpackage('_pyinstaller')\n+    config.make_config_py() # installs __config__.py\n+    return config\n+\n+if __name__ == '__main__':\n+    print('This is the wrong setup.py file to run')\n",
            "comment_added_diff": {
                "1": "#!/usr/bin/env python3",
                "31": "    config.make_config_py() # installs __config__.py"
            },
            "comment_deleted_diff": {},
            "comment_modified_diff": {}
        },
        {
            "commit": "eb6a10a2294defc48a93fad1f57d73628e04db2a",
            "timestamp": "2023-09-17T19:38:58+02:00",
            "author": "Mateusz Sok\u00f3\u0142",
            "commit_message": "Apply review comments",
            "additions": 0,
            "deletions": 35,
            "change_type": "DELETE",
            "diff": "@@ -1,35 +0,0 @@\n-#!/usr/bin/env python3\n-\n-def configuration(parent_package='',top_path=None):\n-    from numpy.distutils.misc_util import Configuration\n-    config = Configuration('numpy', parent_package, top_path)\n-\n-    config.add_subpackage('array_api')\n-    config.add_subpackage('compat')\n-    config.add_subpackage('core')\n-    config.add_subpackage('distutils')\n-    config.add_subpackage('doc')\n-    config.add_subpackage('f2py')\n-    config.add_subpackage('fft')\n-    config.add_subpackage('lib')\n-    config.add_subpackage('linalg')\n-    config.add_subpackage('ma')\n-    config.add_subpackage('matrixlib')\n-    config.add_subpackage('polynomial')\n-    config.add_subpackage('random')\n-    config.add_subpackage('rec')\n-    config.add_subpackage('char')\n-    config.add_subpackage('testing')\n-    config.add_subpackage('typing')\n-    config.add_subpackage('_typing')\n-    config.add_subpackage('_utils')\n-    config.add_data_dir('doc')\n-    config.add_data_files('py.typed')\n-    config.add_data_files('*.pyi')\n-    config.add_subpackage('tests')\n-    config.add_subpackage('_pyinstaller')\n-    config.make_config_py() # installs __config__.py\n-    return config\n-\n-if __name__ == '__main__':\n-    print('This is the wrong setup.py file to run')\n",
            "comment_added_diff": {},
            "comment_deleted_diff": {
                "1": "#!/usr/bin/env python3",
                "31": "    config.make_config_py() # installs __config__.py"
            },
            "comment_modified_diff": {}
        }
    ],
    "setup.py.orig": [],
    "setup_common.py": [
        {
            "commit": "9bde3880d7de3db54e44f29071462bf2dbbee1e7",
            "timestamp": "2022-10-20T17:56:10+03:00",
            "author": "mattip",
            "commit_message": "MAINT: refactor mandatory npymath functions to #define macros",
            "additions": 2,
            "deletions": 0,
            "change_type": "MODIFY",
            "diff": "@@ -122,6 +122,8 @@ def set_sig(sig):\n             set_sig(line)\n \n # Mandatory functions: if not found, fail the build\n+# Some of these can still be blocklisted if the C99 implementation\n+# is buggy, see numpy/core/src/common/npy_config.h\n MANDATORY_FUNCS = [\n     \"sin\", \"cos\", \"tan\", \"sinh\", \"cosh\", \"tanh\", \"fabs\",\n     \"floor\", \"ceil\", \"sqrt\", \"log10\", \"log\", \"exp\", \"asin\",\n",
            "comment_added_diff": {
                "125": "# Some of these can still be blocklisted if the C99 implementation",
                "126": "# is buggy, see numpy/core/src/common/npy_config.h"
            },
            "comment_deleted_diff": {},
            "comment_modified_diff": {}
        },
        {
            "commit": "c3cc6814ed47623bc9d2303922c99177ed1e2bc1",
            "timestamp": "2022-12-21T13:38:11+01:00",
            "author": "Sebastian Berg",
            "commit_message": "BLD: Help raspian arm + clang 13 about `__builtin_mul_overflow`\n\nIt seems on raspian arm with clang 13 `__builtin_mul_overflow` is\ndefined for `int` but doesn't work for `ptrdiff_t` (and maybe others).\nThis checks for `ptrdiff_t` instead of int, which was reported to\nwork-around the issue.\n\nCloses gh-22811",
            "additions": 4,
            "deletions": 2,
            "change_type": "MODIFY",
            "diff": "@@ -129,7 +129,7 @@ def set_sig(sig):\n     \"floor\", \"ceil\", \"sqrt\", \"log10\", \"log\", \"exp\", \"asin\",\n     \"acos\", \"atan\", \"fmod\", 'modf', 'frexp', 'ldexp',\n     \"expm1\", \"log1p\", \"acosh\", \"asinh\", \"atanh\",\n-    \"rint\", \"trunc\", \"exp2\", \n+    \"rint\", \"trunc\", \"exp2\",\n     \"copysign\", \"nextafter\", \"strtoll\", \"strtoull\", \"cbrt\",\n     \"log2\", \"pow\", \"hypot\", \"atan2\",\n     \"csin\", \"csinh\", \"ccos\", \"ccosh\", \"ctan\", \"ctanh\",\n@@ -178,7 +178,9 @@ def set_sig(sig):\n                        (\"__builtin_bswap32\", '5u'),\n                        (\"__builtin_bswap64\", '5u'),\n                        (\"__builtin_expect\", '5, 0'),\n-                       (\"__builtin_mul_overflow\", '5, 5, (int*)5'),\n+                       # Test `long long` for arm+clang 13 (gh-22811,\n+                       # but we use all versions of __builtin_mul_overflow):\n+                       (\"__builtin_mul_overflow\", '(long long)5, 5, (int*)5'),\n                        # MMX only needed for icc, but some clangs don't have it\n                        (\"_m_from_int64\", '0', \"emmintrin.h\"),\n                        (\"_mm_load_ps\", '(float*)0', \"xmmintrin.h\"),  # SSE\n",
            "comment_added_diff": {
                "181": "                       # Test `long long` for arm+clang 13 (gh-22811,",
                "182": "                       # but we use all versions of __builtin_mul_overflow):"
            },
            "comment_deleted_diff": {},
            "comment_modified_diff": {
                "181": "                       (\"__builtin_mul_overflow\", '5, 5, (int*)5'),"
            }
        },
        {
            "commit": "ed7efc7bda3d6d69fc1ca246a82bd79e4934b530",
            "timestamp": "2022-12-25T15:17:18+02:00",
            "author": "mattip",
            "commit_message": "MAINT: restore npymath implementations needed for freebsd",
            "additions": 3,
            "deletions": 1,
            "change_type": "MODIFY",
            "diff": "@@ -132,7 +132,6 @@ def set_sig(sig):\n     \"rint\", \"trunc\", \"exp2\",\n     \"copysign\", \"nextafter\", \"strtoll\", \"strtoull\", \"cbrt\",\n     \"log2\", \"pow\", \"hypot\", \"atan2\",\n-    \"csin\", \"csinh\", \"ccos\", \"ccosh\", \"ctan\", \"ctanh\",\n     \"creal\", \"cimag\", \"conj\"\n ]\n \n@@ -154,6 +153,9 @@ def set_sig(sig):\n C99_COMPLEX_FUNCS = [\n     \"cabs\", \"cacos\", \"cacosh\", \"carg\", \"casin\", \"casinh\", \"catan\",\n     \"catanh\", \"cexp\", \"clog\", \"cpow\", \"csqrt\",\n+    # The long double variants (like csinl)  should be mandatory on C11,\n+    # but are missing in FreeBSD. Issue gh-22850\n+    \"csin\", \"csinh\", \"ccos\", \"ccosh\", \"ctan\", \"ctanh\",\n     ]\n \n OPTIONAL_HEADERS = [\n",
            "comment_added_diff": {
                "156": "    # The long double variants (like csinl)  should be mandatory on C11,",
                "157": "    # but are missing in FreeBSD. Issue gh-22850"
            },
            "comment_deleted_diff": {},
            "comment_modified_diff": {}
        },
        {
            "commit": "866f41a85bddfa3ea6de551bb27f335b0f8a6a52",
            "timestamp": "2023-02-20T04:07:15+02:00",
            "author": "Sayed Adel",
            "commit_message": "MAINT, SIMD: Removes compiler definitions of attribute-based CPU dispatching",
            "additions": 0,
            "deletions": 56,
            "change_type": "MODIFY",
            "diff": "@@ -183,25 +183,7 @@ def set_sig(sig):\n                        # Test `long long` for arm+clang 13 (gh-22811,\n                        # but we use all versions of __builtin_mul_overflow):\n                        (\"__builtin_mul_overflow\", '(long long)5, 5, (int*)5'),\n-                       # MMX only needed for icc, but some clangs don't have it\n-                       (\"_m_from_int64\", '0', \"emmintrin.h\"),\n-                       (\"_mm_load_ps\", '(float*)0', \"xmmintrin.h\"),  # SSE\n-                       (\"_mm_prefetch\", '(float*)0, _MM_HINT_NTA',\n-                        \"xmmintrin.h\"),  # SSE\n-                       (\"_mm_load_pd\", '(double*)0', \"emmintrin.h\"),  # SSE2\n                        (\"__builtin_prefetch\", \"(float*)0, 0, 3\"),\n-                       # check that the linker can handle avx\n-                       (\"__asm__ volatile\", '\"vpand %xmm1, %xmm2, %xmm3\"',\n-                        \"stdio.h\", \"LINK_AVX\"),\n-                       (\"__asm__ volatile\", '\"vpand %ymm1, %ymm2, %ymm3\"',\n-                        \"stdio.h\", \"LINK_AVX2\"),\n-                       (\"__asm__ volatile\", '\"vpaddd %zmm1, %zmm2, %zmm3\"',\n-                        \"stdio.h\", \"LINK_AVX512F\"),\n-                       (\"__asm__ volatile\", '\"vfpclasspd $0x40, %zmm15, %k6\\\\n\"\\\n-                                             \"vmovdqu8 %xmm0, %xmm1\\\\n\"\\\n-                                             \"vpbroadcastmb2q %k0, %xmm0\\\\n\"',\n-                        \"stdio.h\", \"LINK_AVX512_SKX\"),\n-                       (\"__asm__ volatile\", '\"xgetbv\"', \"stdio.h\", \"XGETBV\"),\n                        ]\n \n # function attributes\n@@ -216,44 +198,6 @@ def set_sig(sig):\n                                 ('__attribute__((nonnull (1)))',\n                                  'attribute_nonnull'),\n                                 ]\n-\n-OPTIONAL_FUNCTION_ATTRIBUTES_AVX = [('__attribute__((target (\"avx\")))',\n-    'attribute_target_avx'),\n-    ('__attribute__((target (\"avx2\")))',\n-    'attribute_target_avx2'),\n-    ('__attribute__((target (\"avx512f\")))',\n-    'attribute_target_avx512f'),\n-    ('__attribute__((target (\"avx512f,avx512dq,avx512bw,avx512vl,avx512cd\")))',\n-    'attribute_target_avx512_skx'),\n-    ]\n-\n-# function attributes with intrinsics\n-# To ensure your compiler can compile avx intrinsics with just the attributes\n-# gcc 4.8.4 support attributes but not with intrisics\n-# tested via \"#include<%s> int %s %s(void *){code; return 0;};\" % (header, attribute, name, code)\n-# function name will be converted to HAVE_<upper-case-name> preprocessor macro\n-# The _mm512_castps_si512 instruction is specific check for AVX-512F support\n-# in gcc-4.9 which is missing a subset of intrinsics. See\n-# https://gcc.gnu.org/bugzilla/show_bug.cgi?id=61878\n-OPTIONAL_FUNCTION_ATTRIBUTES_WITH_INTRINSICS_AVX = [\n-    ('__attribute__((target(\"avx2,fma\")))',\n-    'attribute_target_avx2_with_intrinsics',\n-    '__m256 temp = _mm256_set1_ps(1.0); temp = \\\n-    _mm256_fmadd_ps(temp, temp, temp)',\n-    'immintrin.h'),\n-    ('__attribute__((target(\"avx512f\")))',\n-    'attribute_target_avx512f_with_intrinsics',\n-    '__m512i temp = _mm512_castps_si512(_mm512_set1_ps(1.0))',\n-    'immintrin.h'),\n-    ('__attribute__((target (\"avx512f,avx512dq,avx512bw,avx512vl,avx512cd\")))',\n-    'attribute_target_avx512_skx_with_intrinsics',\n-    '__mmask8 temp = _mm512_fpclass_pd_mask(_mm512_set1_pd(1.0), 0x01);\\\n-    __m512i unused_temp = \\\n-        _mm512_castps_si512(_mm512_set1_ps(1.0));\\\n-    _mm_mask_storeu_epi8(NULL, 0xFF, _mm_broadcastmb_epi64(temp))',\n-    'immintrin.h'),\n-    ]\n-\n def fname2def(name):\n     return \"HAVE_%s\" % name.upper()\n \n",
            "comment_added_diff": {},
            "comment_deleted_diff": {
                "186": "                       # MMX only needed for icc, but some clangs don't have it",
                "188": "                       (\"_mm_load_ps\", '(float*)0', \"xmmintrin.h\"),  # SSE",
                "190": "                        \"xmmintrin.h\"),  # SSE",
                "191": "                       (\"_mm_load_pd\", '(double*)0', \"emmintrin.h\"),  # SSE2",
                "193": "                       # check that the linker can handle avx",
                "230": "# function attributes with intrinsics",
                "231": "# To ensure your compiler can compile avx intrinsics with just the attributes",
                "232": "# gcc 4.8.4 support attributes but not with intrisics",
                "233": "# tested via \"#include<%s> int %s %s(void *){code; return 0;};\" % (header, attribute, name, code)",
                "234": "# function name will be converted to HAVE_<upper-case-name> preprocessor macro",
                "235": "# The _mm512_castps_si512 instruction is specific check for AVX-512F support",
                "236": "# in gcc-4.9 which is missing a subset of intrinsics. See",
                "237": "# https://gcc.gnu.org/bugzilla/show_bug.cgi?id=61878"
            },
            "comment_modified_diff": {}
        },
        {
            "commit": "e3beb2e78689560e11491df54c72acff18ed4afa",
            "timestamp": "2023-02-28T11:16:53-08:00",
            "author": "Brock",
            "commit_message": "update C_API_VERSION",
            "additions": 2,
            "deletions": 1,
            "change_type": "MODIFY",
            "diff": "@@ -48,7 +48,8 @@\n # 0x0000000f - 1.22.x\n # 0x00000010 - 1.23.x\n # 0x00000010 - 1.24.x\n-C_API_VERSION = 0x00000010\n+# 0x00000011 - 1.25.x\n+C_API_VERSION = 0x00000011\n \n class MismatchCAPIError(ValueError):\n     pass\n",
            "comment_added_diff": {
                "51": "# 0x00000011 - 1.25.x"
            },
            "comment_deleted_diff": {},
            "comment_modified_diff": {
                "51": "C_API_VERSION = 0x00000010"
            }
        },
        {
            "commit": "6309cf291ca806f554723d659777acca46cdad1f",
            "timestamp": "2023-04-04T17:17:17+02:00",
            "author": "Sebastian Berg",
            "commit_message": "BLD: Add support for NPY_TARGET_VERSION macro\n\nThis is a way for downstream users to specify which NumPy version\nthey wish to be compaible with.\n\nNote that we provide a conservative default here (because almost nobody\nactually uses new API as they would lose backwards compatibility).\n\nInitially I had thought we should just redefine it so that the target\nversion uses the same scheme as the Python hex version (and limited API),\nbut since we have `NPY_1_15_API_VERSION` defines, use those...\n\nThis commit does not include any actual use of this!",
            "additions": 5,
            "deletions": 0,
            "change_type": "MODIFY",
            "diff": "@@ -50,6 +50,11 @@\n # 0x00000010 - 1.24.x\n C_API_VERSION = 0x00000010\n \n+# When compiling against NumPy (downstream libraries), NumPy will by default\n+# pick an older feature version.  For example, for 1.25.x we default to the\n+# 1.17 API and support going back all the way to 1.15.x (if so desired).\n+# This is set up in `numpyconfig.h`.\n+\n class MismatchCAPIError(ValueError):\n     pass\n \n",
            "comment_added_diff": {
                "53": "# When compiling against NumPy (downstream libraries), NumPy will by default",
                "54": "# pick an older feature version.  For example, for 1.25.x we default to the",
                "55": "# 1.17 API and support going back all the way to 1.15.x (if so desired).",
                "56": "# This is set up in `numpyconfig.h`."
            },
            "comment_deleted_diff": {},
            "comment_modified_diff": {}
        },
        {
            "commit": "3a811358830c324b7b6819b88dec4e4bcd91444a",
            "timestamp": "2023-04-04T17:17:17+02:00",
            "author": "Sebastian Berg",
            "commit_message": "ENH: Allow compiling compatibly to old NumPy versions\n\nThe default compiles compatibly with 1.17.x, we allow going back to\n1.15 (mainly because it is easy).\n\nThere were few additions in this time, a few structs grew and very\nfew API functions were added.  Added a way to mark API functions\nas requiring a specific target version.\n\nIf a user wishes to use the *new* API, they have to add the definition:\n\n    #define NPY_TARGET_VERSION NPY_1_22_API_VERSION\n\nBefore importing NumPy.  (Our version numbering is a bit funny\nI first thought to use a hex version of the main NumPy version,\nbut since we already have the `NPY_1_22_API_VERSION` defines...)",
            "additions": 6,
            "deletions": 1,
            "change_type": "MODIFY",
            "diff": "@@ -48,7 +48,8 @@\n # 0x0000000f - 1.22.x\n # 0x00000010 - 1.23.x\n # 0x00000010 - 1.24.x\n-C_API_VERSION = 0x00000010\n+# 0x00000011 - 1.25.x\n+C_API_VERSION = 0x00000011\n \n # When compiling against NumPy (downstream libraries), NumPy will by default\n # pick an older feature version.  For example, for 1.25.x we default to the\n@@ -96,6 +97,10 @@ def check_api_version(apiversion, codegen_dir):\n                f\"checksum in core/codegen_dir/cversions.txt is {api_hash}. If \"\n                \"functions were added in the C API, you have to update \"\n                f\"C_API_VERSION in {__file__}.\"\n+               \"\\n\"\n+               \"Please make sure that new additions are guarded with \"\n+               \"MinVersion() to make them unavailable when wishing support \"\n+               \"for older NumPy versions.\"\n                )\n         raise MismatchCAPIError(msg)\n \n",
            "comment_added_diff": {
                "51": "# 0x00000011 - 1.25.x"
            },
            "comment_deleted_diff": {},
            "comment_modified_diff": {
                "51": "C_API_VERSION = 0x00000010"
            }
        },
        {
            "commit": "6ce690dc5584666beb13b41650110d6a601160b1",
            "timestamp": "2023-04-28T08:01:51+02:00",
            "author": "Sebastian Berg",
            "commit_message": "Apply suggestions from code review\n\nCo-authored-by: Matti Picus <matti.picus@gmail.com>",
            "additions": 2,
            "deletions": 2,
            "change_type": "MODIFY",
            "diff": "@@ -51,9 +51,9 @@\n # 0x00000011 - 1.25.x\n C_API_VERSION = 0x00000011\n \n-# When compiling against NumPy (downstream libraries), NumPy will by default\n+# By default, when compiling downstream libraries against NumPy,```\n # pick an older feature version.  For example, for 1.25.x we default to the\n-# 1.17 API and support going back all the way to 1.15.x (if so desired).\n+# 1.19 API and support going back all the way to 1.15.x (if so desired).\n # This is set up in `numpyconfig.h`.\n \n class MismatchCAPIError(ValueError):\n",
            "comment_added_diff": {
                "54": "# By default, when compiling downstream libraries against NumPy,```",
                "56": "# 1.19 API and support going back all the way to 1.15.x (if so desired)."
            },
            "comment_deleted_diff": {
                "54": "# When compiling against NumPy (downstream libraries), NumPy will by default",
                "56": "# 1.17 API and support going back all the way to 1.15.x (if so desired)."
            },
            "comment_modified_diff": {
                "54": "# When compiling against NumPy (downstream libraries), NumPy will by default",
                "56": "# 1.17 API and support going back all the way to 1.15.x (if so desired)."
            }
        },
        {
            "commit": "c007eb27a22751b4f66647c9f4d718384cb96ab0",
            "timestamp": "2023-06-11T16:34:19+02:00",
            "author": "Sebastian Berg",
            "commit_message": "ABI: Bump C-ABI to 2 but accept older NumPy if compiled against new one\n\nAs of now we can do this, because only _removing_ features we will\nremain truly ABI compatible.\n\nNEP 53 outlines a plan to introduce `numpy2_compat` which would allow\nmore extensive changes, but lets start with this much simpler one.",
            "additions": 5,
            "deletions": 2,
            "change_type": "MODIFY",
            "diff": "@@ -21,7 +21,10 @@\n # Binary compatibility version number. This number is increased whenever the\n # C-API is changed such that binary compatibility is broken, i.e. whenever a\n # recompile of extension modules is needed.\n-C_ABI_VERSION = 0x01000009\n+# NOTE: This is the version against which an extension module was compiled.\n+#       As of now compiling against version 2 (0x02000000) yields version 1\n+#       compatible binaries (subset).  See also NEP 53.\n+C_ABI_VERSION = 0x02000000\n \n # Minor API version.  This number is increased whenever a change is made to the\n # C-API -- whether it breaks binary compatibility or not.  Some changes, such\n@@ -49,7 +52,7 @@\n # 0x00000010 - 1.23.x\n # 0x00000010 - 1.24.x\n # 0x00000011 - 1.25.x\n-C_API_VERSION = 0x00000011\n+C_API_VERSION = 0x00000012\n \n # By default, when compiling downstream libraries against NumPy,```\n # pick an older feature version.  For example, for 1.25.x we default to the\n",
            "comment_added_diff": {
                "24": "# NOTE: This is the version against which an extension module was compiled.",
                "25": "#       As of now compiling against version 2 (0x02000000) yields version 1",
                "26": "#       compatible binaries (subset).  See also NEP 53."
            },
            "comment_deleted_diff": {},
            "comment_modified_diff": {
                "24": "C_ABI_VERSION = 0x01000009"
            }
        },
        {
            "commit": "dc93de39fc7928484197ea729afd3a75404af183",
            "timestamp": "2023-06-11T21:47:30+02:00",
            "author": "Sebastian Berg",
            "commit_message": "MAINT: Make sure meson version and NPY_2_0_API_VERSION are defined",
            "additions": 1,
            "deletions": 0,
            "change_type": "MODIFY",
            "diff": "@@ -52,6 +52,7 @@\n # 0x00000010 - 1.23.x\n # 0x00000010 - 1.24.x\n # 0x00000011 - 1.25.x\n+# 0x00000012 - 2.0.x\n C_API_VERSION = 0x00000012\n \n # By default, when compiling downstream libraries against NumPy,```\n",
            "comment_added_diff": {
                "55": "# 0x00000012 - 2.0.x"
            },
            "comment_deleted_diff": {},
            "comment_modified_diff": {}
        },
        {
            "commit": "63a1fee8d1c1e442aa8ca08ac7cefb195f99b2a9",
            "timestamp": "2023-09-13T22:52:14+02:00",
            "author": "Ralf Gommers",
            "commit_message": "MAINT: remove all `setup.py`'s and related files for distutils builds",
            "additions": 0,
            "deletions": 434,
            "change_type": "DELETE",
            "diff": "@@ -1,434 +0,0 @@\n-# Code common to build tools\n-import copy\n-import pathlib\n-import sys\n-import textwrap\n-\n-from numpy.distutils.misc_util import mingw32\n-\n-\n-#-------------------\n-# Versioning support\n-#-------------------\n-# How to change C_API_VERSION ?\n-#   - increase C_API_VERSION value\n-#   - record the hash for the new C API with the cversions.py script\n-#   and add the hash to cversions.txt\n-# The hash values are used to remind developers when the C API number was not\n-# updated - generates a MismatchCAPIWarning warning which is turned into an\n-# exception for released version.\n-\n-# Binary compatibility version number. This number is increased whenever the\n-# C-API is changed such that binary compatibility is broken, i.e. whenever a\n-# recompile of extension modules is needed.\n-# NOTE: This is the version against which an extension module was compiled.\n-#       As of now compiling against version 2 (0x02000000) yields version 1\n-#       compatible binaries (subset).  See also NEP 53.\n-C_ABI_VERSION = 0x02000000\n-\n-# Minor API version.  This number is increased whenever a change is made to the\n-# C-API -- whether it breaks binary compatibility or not.  Some changes, such\n-# as adding a function pointer to the end of the function table, can be made\n-# without breaking binary compatibility.  In this case, only the C_API_VERSION\n-# (*not* C_ABI_VERSION) would be increased.  Whenever binary compatibility is\n-# broken, both C_API_VERSION and C_ABI_VERSION should be increased.\n-#\n-# The version needs to be kept in sync with that in cversions.txt.\n-#\n-# 0x00000008 - 1.7.x\n-# 0x00000009 - 1.8.x\n-# 0x00000009 - 1.9.x\n-# 0x0000000a - 1.10.x\n-# 0x0000000a - 1.11.x\n-# 0x0000000a - 1.12.x\n-# 0x0000000b - 1.13.x\n-# 0x0000000c - 1.14.x\n-# 0x0000000c - 1.15.x\n-# 0x0000000d - 1.16.x\n-# 0x0000000d - 1.19.x\n-# 0x0000000e - 1.20.x\n-# 0x0000000e - 1.21.x\n-# 0x0000000f - 1.22.x\n-# 0x00000010 - 1.23.x\n-# 0x00000010 - 1.24.x\n-# 0x00000011 - 1.25.x\n-# 0x00000012 - 2.0.x\n-C_API_VERSION = 0x00000012\n-\n-# By default, when compiling downstream libraries against NumPy,\n-# pick an older feature version.  For example, for 1.25.x we default to the\n-# 1.19 API and support going back all the way to 1.15.x (if so desired).\n-# This is set up in `numpyconfig.h`.\n-\n-class MismatchCAPIError(ValueError):\n-    pass\n-\n-\n-def get_api_versions(apiversion, codegen_dir):\n-    \"\"\"\n-    Return current C API checksum and the recorded checksum.\n-\n-    Return current C API checksum and the recorded checksum for the given\n-    version of the C API version.\n-\n-    \"\"\"\n-    # Compute the hash of the current API as defined in the .txt files in\n-    # code_generators\n-    sys.path.insert(0, codegen_dir)\n-    try:\n-        m = __import__('genapi')\n-        numpy_api = __import__('numpy_api')\n-        curapi_hash = m.fullapi_hash(numpy_api.full_api)\n-        apis_hash = m.get_versions_hash()\n-    finally:\n-        del sys.path[0]\n-\n-    return curapi_hash, apis_hash[apiversion]\n-\n-def check_api_version(apiversion, codegen_dir):\n-    \"\"\"Emits a MismatchCAPIWarning if the C API version needs updating.\"\"\"\n-    curapi_hash, api_hash = get_api_versions(apiversion, codegen_dir)\n-\n-    # If different hash, it means that the api .txt files in\n-    # codegen_dir have been updated without the API version being\n-    # updated. Any modification in those .txt files should be reflected\n-    # in the api and eventually abi versions.\n-    # To compute the checksum of the current API, use numpy/core/cversions.py\n-    if not curapi_hash == api_hash:\n-        msg = (\"API mismatch detected, the C API version \"\n-               \"numbers have to be updated. Current C api version is \"\n-               f\"{apiversion}, with checksum {curapi_hash}, but recorded \"\n-               f\"checksum in core/codegen_dir/cversions.txt is {api_hash}. If \"\n-               \"functions were added in the C API, you have to update \"\n-               f\"C_API_VERSION in {__file__}.\"\n-               \"\\n\"\n-               \"Please make sure that new additions are guarded with \"\n-               \"MinVersion() to make them unavailable when wishing support \"\n-               \"for older NumPy versions.\"\n-               )\n-        raise MismatchCAPIError(msg)\n-\n-\n-FUNC_CALL_ARGS = {}\n-\n-def set_sig(sig):\n-    prefix, _, args = sig.partition(\"(\")\n-    args = args.rpartition(\")\")[0]\n-    funcname = prefix.rpartition(\" \")[-1]\n-    args = [arg.strip() for arg in args.split(\",\")]\n-    # We use {0} because 0 alone cannot be cast to complex on MSVC in C:\n-    FUNC_CALL_ARGS[funcname] = \", \".join(\"(%s){0}\" % arg for arg in args)\n-\n-\n-for file in [\n-    \"feature_detection_locale.h\",\n-    \"feature_detection_math.h\",\n-    \"feature_detection_cmath.h\",\n-    \"feature_detection_misc.h\",\n-    \"feature_detection_stdio.h\",\n-]:\n-    with open(pathlib.Path(__file__).parent / file) as f:\n-        for line in f:\n-            if line.startswith(\"#\"):\n-                continue\n-            if not line.strip():\n-                continue\n-            set_sig(line)\n-\n-# Mandatory functions: if not found, fail the build\n-# Some of these can still be blocklisted if the C99 implementation\n-# is buggy, see numpy/core/src/common/npy_config.h\n-MANDATORY_FUNCS = [\n-    \"sin\", \"cos\", \"tan\", \"sinh\", \"cosh\", \"tanh\", \"fabs\",\n-    \"floor\", \"ceil\", \"sqrt\", \"log10\", \"log\", \"exp\", \"asin\",\n-    \"acos\", \"atan\", \"fmod\", 'modf', 'frexp', 'ldexp',\n-    \"expm1\", \"log1p\", \"acosh\", \"asinh\", \"atanh\",\n-    \"rint\", \"trunc\", \"exp2\",\n-    \"copysign\", \"nextafter\", \"strtoll\", \"strtoull\", \"cbrt\",\n-    \"log2\", \"pow\", \"hypot\", \"atan2\",\n-    \"creal\", \"cimag\", \"conj\"\n-]\n-\n-OPTIONAL_LOCALE_FUNCS = [\"strtold_l\"]\n-OPTIONAL_FILE_FUNCS = [\"ftello\", \"fseeko\", \"fallocate\"]\n-OPTIONAL_MISC_FUNCS = [\"backtrace\", \"madvise\"]\n-\n-# variable attributes tested via \"int %s a\" % attribute\n-OPTIONAL_VARIABLE_ATTRIBUTES = [\"__thread\", \"__declspec(thread)\"]\n-\n-# Subset of OPTIONAL_*_FUNCS which may already have HAVE_* defined by Python.h\n-OPTIONAL_FUNCS_MAYBE = [\n-    \"ftello\", \"fseeko\"\n-    ]\n-\n-C99_COMPLEX_TYPES = [\n-    'complex double', 'complex float', 'complex long double'\n-    ]\n-C99_COMPLEX_FUNCS = [\n-    \"cabs\", \"cacos\", \"cacosh\", \"carg\", \"casin\", \"casinh\", \"catan\",\n-    \"catanh\", \"cexp\", \"clog\", \"cpow\", \"csqrt\",\n-    # The long double variants (like csinl)  should be mandatory on C11,\n-    # but are missing in FreeBSD. Issue gh-22850\n-    \"csin\", \"csinh\", \"ccos\", \"ccosh\", \"ctan\", \"ctanh\",\n-    ]\n-\n-OPTIONAL_HEADERS = [\n-# sse headers only enabled automatically on amd64/x32 builds\n-                \"xmmintrin.h\",  # SSE\n-                \"emmintrin.h\",  # SSE2\n-                \"immintrin.h\",  # AVX\n-                \"features.h\",  # for glibc version linux\n-                \"xlocale.h\",  # see GH#8367\n-                \"dlfcn.h\",  # dladdr\n-                \"execinfo.h\",  # backtrace\n-                \"libunwind.h\",  # backtrace for LLVM/Clang using libunwind\n-                \"sys/mman.h\", #madvise\n-]\n-\n-# optional gcc compiler builtins and their call arguments and optional a\n-# required header and definition name (HAVE_ prepended)\n-# call arguments are required as the compiler will do strict signature checking\n-OPTIONAL_INTRINSICS = [(\"__builtin_isnan\", '5.'),\n-                       (\"__builtin_isinf\", '5.'),\n-                       (\"__builtin_isfinite\", '5.'),\n-                       (\"__builtin_bswap32\", '5u'),\n-                       (\"__builtin_bswap64\", '5u'),\n-                       (\"__builtin_expect\", '5, 0'),\n-                       # Test `long long` for arm+clang 13 (gh-22811,\n-                       # but we use all versions of __builtin_mul_overflow):\n-                       (\"__builtin_mul_overflow\", '(long long)5, 5, (int*)5'),\n-                       (\"__builtin_prefetch\", \"(float*)0, 0, 3\"),\n-                       ]\n-\n-# function attributes\n-# tested via \"int %s %s(void *);\" % (attribute, name)\n-# function name will be converted to HAVE_<upper-case-name> preprocessor macro\n-OPTIONAL_FUNCTION_ATTRIBUTES = [('__attribute__((optimize(\"unroll-loops\")))',\n-                                'attribute_optimize_unroll_loops'),\n-                                ('__attribute__((optimize(\"O3\")))',\n-                                 'attribute_optimize_opt_3'),\n-                                ('__attribute__((optimize(\"O2\")))',\n-                                 'attribute_optimize_opt_2'),\n-                                ('__attribute__((nonnull (1)))',\n-                                 'attribute_nonnull'),\n-                                ]\n-def fname2def(name):\n-    return \"HAVE_%s\" % name.upper()\n-\n-def sym2def(symbol):\n-    define = symbol.replace(' ', '')\n-    return define.upper()\n-\n-def type2def(symbol):\n-    define = symbol.replace(' ', '_')\n-    return define.upper()\n-\n-# Code to detect long double representation taken from MPFR m4 macro\n-def check_long_double_representation(cmd):\n-    cmd._check_compiler()\n-    body = LONG_DOUBLE_REPRESENTATION_SRC % {'type': 'long double'}\n-\n-    # Disable whole program optimization (the default on vs2015, with python 3.5+)\n-    # which generates intermediary object files and prevents checking the\n-    # float representation.\n-    if sys.platform == \"win32\" and not mingw32():\n-        try:\n-            cmd.compiler.compile_options.remove(\"/GL\")\n-        except (AttributeError, ValueError):\n-            pass\n-\n-    # Disable multi-file interprocedural optimization in the Intel compiler on Linux\n-    # which generates intermediary object files and prevents checking the\n-    # float representation.\n-    elif (sys.platform != \"win32\"\n-            and cmd.compiler.compiler_type.startswith('intel')\n-            and '-ipo' in cmd.compiler.cc_exe):\n-        newcompiler = cmd.compiler.cc_exe.replace(' -ipo', '')\n-        cmd.compiler.set_executables(\n-            compiler=newcompiler,\n-            compiler_so=newcompiler,\n-            compiler_cxx=newcompiler,\n-            linker_exe=newcompiler,\n-            linker_so=newcompiler + ' -shared'\n-        )\n-\n-    # We need to use _compile because we need the object filename\n-    src, obj = cmd._compile(body, None, None, 'c')\n-    try:\n-        ltype = long_double_representation(pyod(obj))\n-        return ltype\n-    except ValueError:\n-        # try linking to support CC=\"gcc -flto\" or icc -ipo\n-        # struct needs to be volatile so it isn't optimized away\n-        # additionally \"clang -flto\" requires the foo struct to be used\n-        body = body.replace('struct', 'volatile struct')\n-        body += \"int main(void) { return foo.before[0]; }\\n\"\n-        src, obj = cmd._compile(body, None, None, 'c')\n-        cmd.temp_files.append(\"_configtest\")\n-        cmd.compiler.link_executable([obj], \"_configtest\")\n-        ltype = long_double_representation(pyod(\"_configtest\"))\n-        return ltype\n-    finally:\n-        cmd._clean()\n-\n-LONG_DOUBLE_REPRESENTATION_SRC = r\"\"\"\n-/* \"before\" is 16 bytes to ensure there's no padding between it and \"x\".\n- *    We're not expecting any \"long double\" bigger than 16 bytes or with\n- *       alignment requirements stricter than 16 bytes.  */\n-typedef %(type)s test_type;\n-\n-struct {\n-        char         before[16];\n-        test_type    x;\n-        char         after[8];\n-} foo = {\n-        { '\\0', '\\0', '\\0', '\\0', '\\0', '\\0', '\\0', '\\0',\n-          '\\001', '\\043', '\\105', '\\147', '\\211', '\\253', '\\315', '\\357' },\n-        -123456789.0,\n-        { '\\376', '\\334', '\\272', '\\230', '\\166', '\\124', '\\062', '\\020' }\n-};\n-\"\"\"\n-\n-def pyod(filename):\n-    \"\"\"Python implementation of the od UNIX utility (od -b, more exactly).\n-\n-    Parameters\n-    ----------\n-    filename : str\n-        name of the file to get the dump from.\n-\n-    Returns\n-    -------\n-    out : seq\n-        list of lines of od output\n-\n-    Notes\n-    -----\n-    We only implement enough to get the necessary information for long double\n-    representation, this is not intended as a compatible replacement for od.\n-    \"\"\"\n-    out = []\n-    with open(filename, 'rb') as fid:\n-        yo2 = [oct(o)[2:] for o in fid.read()]\n-    for i in range(0, len(yo2), 16):\n-        line = ['%07d' % int(oct(i)[2:])]\n-        line.extend(['%03d' % int(c) for c in yo2[i:i+16]])\n-        out.append(\" \".join(line))\n-    return out\n-\n-\n-_BEFORE_SEQ = ['000', '000', '000', '000', '000', '000', '000', '000',\n-              '001', '043', '105', '147', '211', '253', '315', '357']\n-_AFTER_SEQ = ['376', '334', '272', '230', '166', '124', '062', '020']\n-\n-_IEEE_DOUBLE_BE = ['301', '235', '157', '064', '124', '000', '000', '000']\n-_IEEE_DOUBLE_LE = _IEEE_DOUBLE_BE[::-1]\n-_INTEL_EXTENDED_12B = ['000', '000', '000', '000', '240', '242', '171', '353',\n-                       '031', '300', '000', '000']\n-_INTEL_EXTENDED_16B = ['000', '000', '000', '000', '240', '242', '171', '353',\n-                       '031', '300', '000', '000', '000', '000', '000', '000']\n-_MOTOROLA_EXTENDED_12B = ['300', '031', '000', '000', '353', '171',\n-                          '242', '240', '000', '000', '000', '000']\n-_IEEE_QUAD_PREC_BE = ['300', '031', '326', '363', '105', '100', '000', '000',\n-                      '000', '000', '000', '000', '000', '000', '000', '000']\n-_IEEE_QUAD_PREC_LE = _IEEE_QUAD_PREC_BE[::-1]\n-_IBM_DOUBLE_DOUBLE_BE = (['301', '235', '157', '064', '124', '000', '000', '000'] +\n-                     ['000'] * 8)\n-_IBM_DOUBLE_DOUBLE_LE = (['000', '000', '000', '124', '064', '157', '235', '301'] +\n-                     ['000'] * 8)\n-\n-def long_double_representation(lines):\n-    \"\"\"Given a binary dump as given by GNU od -b, look for long double\n-    representation.\"\"\"\n-\n-    # Read contains a list of 32 items, each item is a byte (in octal\n-    # representation, as a string). We 'slide' over the output until read is of\n-    # the form before_seq + content + after_sequence, where content is the long double\n-    # representation:\n-    #  - content is 12 bytes: 80 bits Intel representation\n-    #  - content is 16 bytes: 80 bits Intel representation (64 bits) or quad precision\n-    #  - content is 8 bytes: same as double (not implemented yet)\n-    read = [''] * 32\n-    saw = None\n-    for line in lines:\n-        # we skip the first word, as od -b output an index at the beginning of\n-        # each line\n-        for w in line.split()[1:]:\n-            read.pop(0)\n-            read.append(w)\n-\n-            # If the end of read is equal to the after_sequence, read contains\n-            # the long double\n-            if read[-8:] == _AFTER_SEQ:\n-                saw = copy.copy(read)\n-                # if the content was 12 bytes, we only have 32 - 8 - 12 = 12\n-                # \"before\" bytes. In other words the first 4 \"before\" bytes went\n-                # past the sliding window.\n-                if read[:12] == _BEFORE_SEQ[4:]:\n-                    if read[12:-8] == _INTEL_EXTENDED_12B:\n-                        return 'INTEL_EXTENDED_12_BYTES_LE'\n-                    if read[12:-8] == _MOTOROLA_EXTENDED_12B:\n-                        return 'MOTOROLA_EXTENDED_12_BYTES_BE'\n-                # if the content was 16 bytes, we are left with 32-8-16 = 16\n-                # \"before\" bytes, so 8 went past the sliding window.\n-                elif read[:8] == _BEFORE_SEQ[8:]:\n-                    if read[8:-8] == _INTEL_EXTENDED_16B:\n-                        return 'INTEL_EXTENDED_16_BYTES_LE'\n-                    elif read[8:-8] == _IEEE_QUAD_PREC_BE:\n-                        return 'IEEE_QUAD_BE'\n-                    elif read[8:-8] == _IEEE_QUAD_PREC_LE:\n-                        return 'IEEE_QUAD_LE'\n-                    elif read[8:-8] == _IBM_DOUBLE_DOUBLE_LE:\n-                        return 'IBM_DOUBLE_DOUBLE_LE'\n-                    elif read[8:-8] == _IBM_DOUBLE_DOUBLE_BE:\n-                        return 'IBM_DOUBLE_DOUBLE_BE'\n-                # if the content was 8 bytes, left with 32-8-8 = 16 bytes\n-                elif read[:16] == _BEFORE_SEQ:\n-                    if read[16:-8] == _IEEE_DOUBLE_LE:\n-                        return 'IEEE_DOUBLE_LE'\n-                    elif read[16:-8] == _IEEE_DOUBLE_BE:\n-                        return 'IEEE_DOUBLE_BE'\n-\n-    if saw is not None:\n-        raise ValueError(\"Unrecognized format (%s)\" % saw)\n-    else:\n-        # We never detected the after_sequence\n-        raise ValueError(\"Could not lock sequences (%s)\" % saw)\n-\n-\n-def check_for_right_shift_internal_compiler_error(cmd):\n-    \"\"\"\n-    On our arm CI, this fails with an internal compilation error\n-\n-    The failure looks like the following, and can be reproduced on ARM64 GCC 5.4:\n-\n-        <source>: In function 'right_shift':\n-        <source>:4:20: internal compiler error: in expand_shift_1, at expmed.c:2349\n-               ip1[i] = ip1[i] >> in2;\n-                      ^\n-        Please submit a full bug report,\n-        with preprocessed source if appropriate.\n-        See <http://gcc.gnu.org/bugs.html> for instructions.\n-        Compiler returned: 1\n-\n-    This function returns True if this compiler bug is present, and we need to\n-    turn off optimization for the function\n-    \"\"\"\n-    cmd._check_compiler()\n-    has_optimize = cmd.try_compile(textwrap.dedent(\"\"\"\\\n-        __attribute__((optimize(\"O3\"))) void right_shift() {}\n-        \"\"\"), None, None)\n-    if not has_optimize:\n-        return False\n-\n-    no_err = cmd.try_compile(textwrap.dedent(\"\"\"\\\n-        typedef long the_type;  /* fails also for unsigned and long long */\n-        __attribute__((optimize(\"O3\"))) void right_shift(the_type in2, the_type *ip1, int n) {\n-            for (int i = 0; i < n; i++) {\n-                if (in2 < (the_type)sizeof(the_type) * 8) {\n-                    ip1[i] = ip1[i] >> in2;\n-                }\n-            }\n-        }\n-        \"\"\"), None, None)\n-    return not no_err\n",
            "comment_added_diff": {},
            "comment_deleted_diff": {
                "1": "# Code common to build tools",
                "10": "#-------------------",
                "11": "# Versioning support",
                "12": "#-------------------",
                "13": "# How to change C_API_VERSION ?",
                "14": "#   - increase C_API_VERSION value",
                "15": "#   - record the hash for the new C API with the cversions.py script",
                "16": "#   and add the hash to cversions.txt",
                "17": "# The hash values are used to remind developers when the C API number was not",
                "18": "# updated - generates a MismatchCAPIWarning warning which is turned into an",
                "19": "# exception for released version.",
                "21": "# Binary compatibility version number. This number is increased whenever the",
                "22": "# C-API is changed such that binary compatibility is broken, i.e. whenever a",
                "23": "# recompile of extension modules is needed.",
                "24": "# NOTE: This is the version against which an extension module was compiled.",
                "25": "#       As of now compiling against version 2 (0x02000000) yields version 1",
                "26": "#       compatible binaries (subset).  See also NEP 53.",
                "29": "# Minor API version.  This number is increased whenever a change is made to the",
                "30": "# C-API -- whether it breaks binary compatibility or not.  Some changes, such",
                "31": "# as adding a function pointer to the end of the function table, can be made",
                "32": "# without breaking binary compatibility.  In this case, only the C_API_VERSION",
                "33": "# (*not* C_ABI_VERSION) would be increased.  Whenever binary compatibility is",
                "34": "# broken, both C_API_VERSION and C_ABI_VERSION should be increased.",
                "35": "#",
                "36": "# The version needs to be kept in sync with that in cversions.txt.",
                "37": "#",
                "38": "# 0x00000008 - 1.7.x",
                "39": "# 0x00000009 - 1.8.x",
                "40": "# 0x00000009 - 1.9.x",
                "41": "# 0x0000000a - 1.10.x",
                "42": "# 0x0000000a - 1.11.x",
                "43": "# 0x0000000a - 1.12.x",
                "44": "# 0x0000000b - 1.13.x",
                "45": "# 0x0000000c - 1.14.x",
                "46": "# 0x0000000c - 1.15.x",
                "47": "# 0x0000000d - 1.16.x",
                "48": "# 0x0000000d - 1.19.x",
                "49": "# 0x0000000e - 1.20.x",
                "50": "# 0x0000000e - 1.21.x",
                "51": "# 0x0000000f - 1.22.x",
                "52": "# 0x00000010 - 1.23.x",
                "53": "# 0x00000010 - 1.24.x",
                "54": "# 0x00000011 - 1.25.x",
                "55": "# 0x00000012 - 2.0.x",
                "58": "# By default, when compiling downstream libraries against NumPy,",
                "59": "# pick an older feature version.  For example, for 1.25.x we default to the",
                "60": "# 1.19 API and support going back all the way to 1.15.x (if so desired).",
                "61": "# This is set up in `numpyconfig.h`.",
                "75": "    # Compute the hash of the current API as defined in the .txt files in",
                "76": "    # code_generators",
                "92": "    # If different hash, it means that the api .txt files in",
                "93": "    # codegen_dir have been updated without the API version being",
                "94": "    # updated. Any modification in those .txt files should be reflected",
                "95": "    # in the api and eventually abi versions.",
                "96": "    # To compute the checksum of the current API, use numpy/core/cversions.py",
                "119": "    # We use {0} because 0 alone cannot be cast to complex on MSVC in C:",
                "132": "            if line.startswith(\"#\"):",
                "138": "# Mandatory functions: if not found, fail the build",
                "139": "# Some of these can still be blocklisted if the C99 implementation",
                "140": "# is buggy, see numpy/core/src/common/npy_config.h",
                "156": "# variable attributes tested via \"int %s a\" % attribute",
                "159": "# Subset of OPTIONAL_*_FUNCS which may already have HAVE_* defined by Python.h",
                "170": "    # The long double variants (like csinl)  should be mandatory on C11,",
                "171": "    # but are missing in FreeBSD. Issue gh-22850",
                "176": "# sse headers only enabled automatically on amd64/x32 builds",
                "177": "                \"xmmintrin.h\",  # SSE",
                "178": "                \"emmintrin.h\",  # SSE2",
                "179": "                \"immintrin.h\",  # AVX",
                "180": "                \"features.h\",  # for glibc version linux",
                "181": "                \"xlocale.h\",  # see GH#8367",
                "182": "                \"dlfcn.h\",  # dladdr",
                "183": "                \"execinfo.h\",  # backtrace",
                "184": "                \"libunwind.h\",  # backtrace for LLVM/Clang using libunwind",
                "185": "                \"sys/mman.h\", #madvise",
                "188": "# optional gcc compiler builtins and their call arguments and optional a",
                "189": "# required header and definition name (HAVE_ prepended)",
                "190": "# call arguments are required as the compiler will do strict signature checking",
                "197": "                       # Test `long long` for arm+clang 13 (gh-22811,",
                "198": "                       # but we use all versions of __builtin_mul_overflow):",
                "203": "# function attributes",
                "204": "# tested via \"int %s %s(void *);\" % (attribute, name)",
                "205": "# function name will be converted to HAVE_<upper-case-name> preprocessor macro",
                "226": "# Code to detect long double representation taken from MPFR m4 macro",
                "231": "    # Disable whole program optimization (the default on vs2015, with python 3.5+)",
                "232": "    # which generates intermediary object files and prevents checking the",
                "233": "    # float representation.",
                "240": "    # Disable multi-file interprocedural optimization in the Intel compiler on Linux",
                "241": "    # which generates intermediary object files and prevents checking the",
                "242": "    # float representation.",
                "255": "    # We need to use _compile because we need the object filename",
                "261": "        # try linking to support CC=\"gcc -flto\" or icc -ipo",
                "262": "        # struct needs to be volatile so it isn't optimized away",
                "263": "        # additionally \"clang -flto\" requires the foo struct to be used",
                "344": "    # Read contains a list of 32 items, each item is a byte (in octal",
                "345": "    # representation, as a string). We 'slide' over the output until read is of",
                "346": "    # the form before_seq + content + after_sequence, where content is the long double",
                "347": "    # representation:",
                "348": "    #  - content is 12 bytes: 80 bits Intel representation",
                "349": "    #  - content is 16 bytes: 80 bits Intel representation (64 bits) or quad precision",
                "350": "    #  - content is 8 bytes: same as double (not implemented yet)",
                "354": "        # we skip the first word, as od -b output an index at the beginning of",
                "355": "        # each line",
                "360": "            # If the end of read is equal to the after_sequence, read contains",
                "361": "            # the long double",
                "364": "                # if the content was 12 bytes, we only have 32 - 8 - 12 = 12",
                "365": "                # \"before\" bytes. In other words the first 4 \"before\" bytes went",
                "366": "                # past the sliding window.",
                "372": "                # if the content was 16 bytes, we are left with 32-8-16 = 16",
                "373": "                # \"before\" bytes, so 8 went past the sliding window.",
                "385": "                # if the content was 8 bytes, left with 32-8-8 = 16 bytes",
                "395": "        # We never detected the after_sequence"
            },
            "comment_modified_diff": {}
        }
    ],
    "setup_common.py.orig": [],
    "loops.c.src": [],
    "test_function_base.py": [
        {
            "commit": "b3c0960a54c81a26bd07912dda96db9e356b34d1",
            "timestamp": "2022-12-08T13:01:59+01:00",
            "author": "Matteo Raso",
            "commit_message": "BUG: Quantile function on complex number now throws an error (#22652) (#22703)\n\nSince percentile is more or less identical to quantile, I also made it\r\nthrow an error if it receives a complex input. I also made nanquantile\r\nand nanpercentile throw errors as well.\r\n\r\n* Made the changes recommended by seberg\r\n\r\n* Fixed a test for PR 22703\r\n\r\n* Fixed tests for quantile\r\n\r\n* Shortened some more lines\r\n\r\n* Fixup more lines\r\n\r\nCo-authored-by: Sebastian Berg <sebastianb@nvidia.com>",
            "additions": 19,
            "deletions": 5,
            "change_type": "MODIFY",
            "diff": "@@ -2973,6 +2973,14 @@ def test_api(self):\n         o = np.ones((1,))\n         np.percentile(d, 5, None, o, False, 'linear')\n \n+    def test_complex(self):\n+        arr_c = np.array([0.5+3.0j, 2.1+0.5j, 1.6+2.3j], dtype='G')\n+        assert_raises(TypeError, np.percentile, arr_c, 0.5)\n+        arr_c = np.array([0.5+3.0j, 2.1+0.5j, 1.6+2.3j], dtype='D')\n+        assert_raises(TypeError, np.percentile, arr_c, 0.5)\n+        arr_c = np.array([0.5+3.0j, 2.1+0.5j, 1.6+2.3j], dtype='F')\n+        assert_raises(TypeError, np.percentile, arr_c, 0.5)\n+\n     def test_2D(self):\n         x = np.array([[1, 1, 1],\n                       [1, 1, 1],\n@@ -2981,7 +2989,7 @@ def test_2D(self):\n                       [1, 1, 1]])\n         assert_array_equal(np.percentile(x, 50, axis=0), [1, 1, 1])\n \n-    @pytest.mark.parametrize(\"dtype\", np.typecodes[\"AllFloat\"])\n+    @pytest.mark.parametrize(\"dtype\", np.typecodes[\"Float\"])\n     def test_linear_nan_1D(self, dtype):\n         # METHOD 1 of H&F\n         arr = np.asarray([15.0, np.NAN, 35.0, 40.0, 50.0], dtype=dtype)\n@@ -2998,9 +3006,6 @@ def test_linear_nan_1D(self, dtype):\n                            (np.float32, np.float32),\n                            (np.float64, np.float64),\n                            (np.longdouble, np.longdouble),\n-                           (np.complex64, np.complex64),\n-                           (np.complex128, np.complex128),\n-                           (np.clongdouble, np.clongdouble),\n                            (np.dtype(\"O\"), np.float64)]\n \n     @pytest.mark.parametrize([\"input_dtype\", \"expected_dtype\"], H_F_TYPE_CODES)\n@@ -3040,7 +3045,7 @@ def test_linear_interpolation(self,\n             np.testing.assert_equal(np.asarray(actual).dtype,\n                                     np.dtype(expected_dtype))\n \n-    TYPE_CODES = np.typecodes[\"AllInteger\"] + np.typecodes[\"AllFloat\"] + \"O\"\n+    TYPE_CODES = np.typecodes[\"AllInteger\"] + np.typecodes[\"Float\"] + \"O\"\n \n     @pytest.mark.parametrize(\"dtype\", TYPE_CODES)\n     def test_lower_higher(self, dtype):\n@@ -3517,6 +3522,15 @@ def test_fraction(self):\n         x = np.arange(8)\n         assert_equal(np.quantile(x, Fraction(1, 2)), Fraction(7, 2))\n \n+    def test_complex(self):\n+        #See gh-22652\n+        arr_c = np.array([0.5+3.0j, 2.1+0.5j, 1.6+2.3j], dtype='G')\n+        assert_raises(TypeError, np.quantile, arr_c, 0.5)\n+        arr_c = np.array([0.5+3.0j, 2.1+0.5j, 1.6+2.3j], dtype='D')\n+        assert_raises(TypeError, np.quantile, arr_c, 0.5)\n+        arr_c = np.array([0.5+3.0j, 2.1+0.5j, 1.6+2.3j], dtype='F')\n+        assert_raises(TypeError, np.quantile, arr_c, 0.5)\n+\n     def test_no_p_overwrite(self):\n         # this is worth retesting, because quantile does not make a copy\n         p0 = np.array([0, 0.75, 0.25, 0.5, 1.0])\n",
            "comment_added_diff": {
                "3526": "        #See gh-22652"
            },
            "comment_deleted_diff": {},
            "comment_modified_diff": {}
        },
        {
            "commit": "e63e8694966d4b9a48b441c1266465e86dd12843",
            "timestamp": "2022-12-30T08:51:36-06:00",
            "author": "Lee Johnston",
            "commit_message": "TST: Add linspace test case for any_step_zero and not _mult_inplace",
            "additions": 9,
            "deletions": 0,
            "change_type": "MODIFY",
            "diff": "@@ -407,3 +407,12 @@ def test_round_negative(self):\n         y = linspace(-1, 3, num=8, dtype=int)\n         t = array([-1, -1, 0, 0, 1, 1, 2, 3], dtype=int)\n         assert_array_equal(y, t)\n+\n+    def test_any_step_zero_and_not_mult_inplace(self):\n+        # any_step_zero is True, _mult_inplace is False\n+        start = array([0.0, 1.0])\n+        stop = array([2.0, 1.0])\n+        y = linspace(start, stop, 3)\n+        assert_array_equal(y, array([[0.0, 1.0], [1.0, 1.0], [2.0, 1.0]]))\n+    \n+\n",
            "comment_added_diff": {
                "412": "        # any_step_zero is True, _mult_inplace is False"
            },
            "comment_deleted_diff": {},
            "comment_modified_diff": {}
        },
        {
            "commit": "88cdaa21aea87ec7d56d1d583500ab2659a5e65e",
            "timestamp": "2023-01-18T02:25:00-05:00",
            "author": "Matteo Raso",
            "commit_message": "BUG: Added __name__ atribute to vectorize class (#23021)",
            "additions": 7,
            "deletions": 0,
            "change_type": "MODIFY",
            "diff": "@@ -1780,6 +1780,13 @@ class subclass(np.ndarray):\n         assert_equal(type(r), subclass)\n         assert_equal(r, m * v)\n \n+    def test_name(self):\n+        #See gh-23021\n+        @np.vectorize\n+        def f2(a, b):\n+            return a + b\n+\n+        assert f2.__name__ == 'f2'\n \n class TestLeaks:\n     class A:\n",
            "comment_added_diff": {
                "1784": "        #See gh-23021"
            },
            "comment_deleted_diff": {},
            "comment_modified_diff": {}
        },
        {
            "commit": "ade008bf5fba3cbc43ffbdf5ee261953a8a71a3a",
            "timestamp": "2023-01-21T20:20:30-05:00",
            "author": "Matteo Raso",
            "commit_message": "ENH: Enabled use of numpy.vectorize as decorator (#9477)\n\nMost of this code comes from PR-9593, but with the default value for pyfunc\nbeing numpy._NoValue instead of None, no override of __new__, and no effort\nto make subclassing possible.\n\nCo-Authored-By: Michael Lamparski <diagonaldevice@gmail.com>",
            "additions": 30,
            "deletions": 0,
            "change_type": "MODIFY",
            "diff": "@@ -1788,6 +1788,36 @@ def f2(a, b):\n \n         assert f2.__name__ == 'f2'\n \n+    def test_decorator(self):\n+        @vectorize\n+        def addsubtract(a, b):\n+            if a > b:\n+                return a - b\n+            else:\n+                return a + b\n+\n+        r = addsubtract([0, 3, 6, 9], [1, 3, 5, 7])\n+        assert_array_equal(r, [1, 6, 1, 2])\n+\n+    def test_signature_otypes_decorator(self):\n+        @vectorize(signature='(n)->(n)', otypes=['float64'])\n+        def f(x):\n+            return x\n+\n+        r = f([1, 2, 3])\n+        assert_equal(r.dtype, np.dtype('float64'))\n+        assert_array_equal(r, [1, 2, 3])\n+        assert f.__name__ == 'f'\n+\n+    def test_positional_regression_9477(self):\n+        # This supplies the first keyword argument as a positional,\n+        # to ensure that they are still properly forwarded after the\n+        # enhancement for #9477\n+        f = vectorize((lambda x: x), ['float64'])\n+        r = f([2])\n+        assert_equal(r.dtype, np.dtype('float64'))\n+\n+\n class TestLeaks:\n     class A:\n         iters = 20\n",
            "comment_added_diff": {
                "1813": "        # This supplies the first keyword argument as a positional,",
                "1814": "        # to ensure that they are still properly forwarded after the",
                "1815": "        # enhancement for #9477"
            },
            "comment_deleted_diff": {},
            "comment_modified_diff": {}
        },
        {
            "commit": "ed1732410f51293e4c5f63dcf162d9f1d417335a",
            "timestamp": "2023-03-27T09:36:25+03:00",
            "author": "Matti Picus",
            "commit_message": "Revert \"ENH: Enabled the use of numpy.vectorize as a decorator\"",
            "additions": 0,
            "deletions": 55,
            "change_type": "MODIFY",
            "diff": "@@ -1787,61 +1787,6 @@ class subclass(np.ndarray):\n         assert_equal(type(r), subclass)\n         assert_equal(r, m * v)\n \n-    def test_name(self):\n-        #See gh-23021\n-        @np.vectorize\n-        def f2(a, b):\n-            return a + b\n-\n-        assert f2.__name__ == 'f2'\n-\n-    def test_decorator(self):\n-        @vectorize\n-        def addsubtract(a, b):\n-            if a > b:\n-                return a - b\n-            else:\n-                return a + b\n-\n-        r = addsubtract([0, 3, 6, 9], [1, 3, 5, 7])\n-        assert_array_equal(r, [1, 6, 1, 2])\n-\n-    def test_docstring(self):\n-        @vectorize\n-        def f(x):\n-            \"\"\"Docstring\"\"\"\n-            return x\n-\n-        assert f.__doc__ == \"Docstring\"\n-\n-    def test_signature_otypes_decorator(self):\n-        @vectorize(signature='(n)->(n)', otypes=['float64'])\n-        def f(x):\n-            return x\n-\n-        r = f([1, 2, 3])\n-        assert_equal(r.dtype, np.dtype('float64'))\n-        assert_array_equal(r, [1, 2, 3])\n-        assert f.__name__ == 'f'\n-\n-    def test_bad_input(self):\n-        with assert_raises(TypeError):\n-            A = np.vectorize(pyfunc = 3)\n-\n-    def test_no_keywords(self):\n-        with assert_raises(TypeError):\n-            @np.vectorize(\"string\")\n-            def foo():\n-                return \"bar\"\n-\n-    def test_positional_regression_9477(self):\n-        # This supplies the first keyword argument as a positional,\n-        # to ensure that they are still properly forwarded after the\n-        # enhancement for #9477\n-        f = vectorize((lambda x: x), ['float64'])\n-        r = f([2])\n-        assert_equal(r.dtype, np.dtype('float64'))\n-\n \n class TestLeaks:\n     class A:\n",
            "comment_added_diff": {},
            "comment_deleted_diff": {
                "1791": "        #See gh-23021",
                "1838": "        # This supplies the first keyword argument as a positional,",
                "1839": "        # to ensure that they are still properly forwarded after the",
                "1840": "        # enhancement for #9477"
            },
            "comment_modified_diff": {}
        },
        {
            "commit": "dfaa72d72453b8738ec711180e03da824651e46b",
            "timestamp": "2023-04-01T21:45:16-04:00",
            "author": "Matteo Raso",
            "commit_message": "Fixed edge case where pyfunc has no attribute `__name__`",
            "additions": 68,
            "deletions": 12,
            "change_type": "MODIFY",
            "diff": "@@ -8,7 +8,7 @@\n import hypothesis\n from hypothesis.extra.numpy import arrays\n import hypothesis.strategies as st\n-\n+from functools import partial\n \n import numpy as np\n from numpy import ma\n@@ -229,8 +229,8 @@ def test_basic(self):\n     def test_nd(self):\n         y1 = [[0, 0, 0], [0, 1, 0], [1, 1, 0]]\n         assert_(np.any(y1))\n-        assert_array_equal(np.any(y1, axis=0), [1, 1, 0])\n-        assert_array_equal(np.any(y1, axis=1), [0, 1, 1])\n+        assert_array_equal(np.sometrue(y1, axis=0), [1, 1, 0])\n+        assert_array_equal(np.sometrue(y1, axis=1), [0, 1, 1])\n \n \n class TestAll:\n@@ -247,8 +247,8 @@ def test_basic(self):\n     def test_nd(self):\n         y1 = [[0, 0, 1], [0, 1, 1], [1, 1, 1]]\n         assert_(not np.all(y1))\n-        assert_array_equal(np.all(y1, axis=0), [0, 0, 1])\n-        assert_array_equal(np.all(y1, axis=1), [0, 0, 1])\n+        assert_array_equal(np.alltrue(y1, axis=0), [0, 0, 1])\n+        assert_array_equal(np.alltrue(y1, axis=1), [0, 0, 1])\n \n \n class TestCopy:\n@@ -1217,13 +1217,6 @@ def test_x_signed_int_big_jump(self, x_dtype):\n         dfdx = gradient(f, x)\n         assert_array_equal(dfdx, [0.5, 0.5])\n \n-    def test_return_type(self):\n-        res = np.gradient(([1, 2], [2, 3]))\n-        if np._using_numpy2_behavior():\n-            assert type(res) is tuple\n-        else:\n-            assert type(res) is list\n-\n \n class TestAngle:\n \n@@ -1787,6 +1780,69 @@ class subclass(np.ndarray):\n         assert_equal(type(r), subclass)\n         assert_equal(r, m * v)\n \n+    def test_name(self):\n+        #See gh-23021\n+        @np.vectorize\n+        def f2(a, b):\n+            return a + b\n+\n+        assert f2.__name__ == 'f2'\n+\n+    def test_decorator(self):\n+        @vectorize\n+        def addsubtract(a, b):\n+            if a > b:\n+                return a - b\n+            else:\n+                return a + b\n+\n+        r = addsubtract([0, 3, 6, 9], [1, 3, 5, 7])\n+        assert_array_equal(r, [1, 6, 1, 2])\n+\n+    def test_docstring(self):\n+        @vectorize\n+        def f(x):\n+            \"\"\"Docstring\"\"\"\n+            return x\n+\n+        assert f.__doc__ == \"Docstring\"\n+\n+    def test_partial(self):\n+        def foo(x, y):\n+            return x + y\n+\n+        bar = partial(foo, 3)\n+        vbar = np.vectorize(bar)\n+        assert vbar(1) == 4\n+\n+    def test_signature_otypes_decorator(self):\n+        @vectorize(signature='(n)->(n)', otypes=['float64'])\n+        def f(x):\n+            return x\n+\n+        r = f([1, 2, 3])\n+        assert_equal(r.dtype, np.dtype('float64'))\n+        assert_array_equal(r, [1, 2, 3])\n+        assert f.__name__ == 'f'\n+\n+    def test_bad_input(self):\n+        with assert_raises(TypeError):\n+            A = np.vectorize(pyfunc = 3)\n+\n+    def test_no_keywords(self):\n+        with assert_raises(TypeError):\n+            @np.vectorize(\"string\")\n+            def foo():\n+                return \"bar\"\n+\n+    def test_positional_regression_9477(self):\n+        # This supplies the first keyword argument as a positional,\n+        # to ensure that they are still properly forwarded after the\n+        # enhancement for #9477\n+        f = vectorize((lambda x: x), ['float64'])\n+        r = f([2])\n+        assert_equal(r.dtype, np.dtype('float64'))\n+\n \n class TestLeaks:\n     class A:\n",
            "comment_added_diff": {
                "1784": "        #See gh-23021",
                "1839": "        # This supplies the first keyword argument as a positional,",
                "1840": "        # to ensure that they are still properly forwarded after the",
                "1841": "        # enhancement for #9477"
            },
            "comment_deleted_diff": {},
            "comment_modified_diff": {}
        },
        {
            "commit": "30c047cdb2e0c0b233e5dc3b61c081ce1f6df3d3",
            "timestamp": "2023-04-28T13:34:53+02:00",
            "author": "Christian Lorentzen",
            "commit_message": "TST: add tests for numpy.quantile (#23129)\n\nThis PR adds additional tests for quantiles:\r\n\r\n1. Identification equation $E[V(q, Y)] = 0$\r\n2. Adding a constant $c > 0$: $q(c + Y) = c + q(Y)$\r\n3. Multiplying by a constant $c > 0$: $q(c \\cdot Y) = c \\cdot q(y)$\r\n4. Multiplying by $-1$: $q_{\\alpha}(-Y) = -q_{1-\\alpha}(Y)$\r\n\r\n(Note by seberg as reviewer: These tests are fairly complex, but may be useful for future development.  But if they seem too complicated, they are probably not really vital and could be shortened or removed.)",
            "additions": 100,
            "deletions": 5,
            "change_type": "MODIFY",
            "diff": "@@ -3538,9 +3538,20 @@ def test_nan_q(self):\n             np.percentile([1, 2, 3, 4.0], q)\n \n \n+quantile_methods = [\n+    'inverted_cdf', 'averaged_inverted_cdf', 'closest_observation',\n+    'interpolated_inverted_cdf', 'hazen', 'weibull', 'linear',\n+    'median_unbiased', 'normal_unbiased', 'nearest', 'lower', 'higher',\n+    'midpoint']\n+\n+\n class TestQuantile:\n     # most of this is already tested by TestPercentile\n \n+    def V(self, x, y, alpha):\n+        # Identification function used in several tests.\n+        return (x >= y) - alpha\n+\n     def test_max_ulp(self):\n         x = [0.0, 0.2, 0.4]\n         a = np.quantile(x, 0.45)\n@@ -3619,11 +3630,7 @@ def test_quantile_preserve_int_type(self, dtype):\n                           method=\"nearest\")\n         assert res.dtype == dtype\n \n-    @pytest.mark.parametrize(\"method\",\n-             ['inverted_cdf', 'averaged_inverted_cdf', 'closest_observation',\n-              'interpolated_inverted_cdf', 'hazen', 'weibull', 'linear',\n-              'median_unbiased', 'normal_unbiased',\n-              'nearest', 'lower', 'higher', 'midpoint'])\n+    @pytest.mark.parametrize(\"method\", quantile_methods)\n     def test_quantile_monotonic(self, method):\n         # GH 14685\n         # test that the return value of quantile is monotonic if p0 is ordered\n@@ -3654,6 +3661,94 @@ def test_quantile_scalar_nan(self):\n         assert np.isscalar(actual)\n         assert_equal(np.quantile(a, 0.5), np.nan)\n \n+    @pytest.mark.parametrize(\"method\", quantile_methods)\n+    @pytest.mark.parametrize(\"alpha\", [0.2, 0.5, 0.9])\n+    def test_quantile_identification_equation(self, method, alpha):\n+        # Test that the identification equation holds for the empirical\n+        # CDF:\n+        #   E[V(x, Y)] = 0  <=>  x is quantile\n+        # with Y the random variable for which we have observed values and\n+        # V(x, y) the canonical identification function for the quantile (at\n+        # level alpha), see\n+        # https://doi.org/10.48550/arXiv.0912.0902        \n+        rng = np.random.default_rng(4321)\n+        # We choose n and alpha such that we cover 3 cases:\n+        #  - n * alpha is an integer\n+        #  - n * alpha is a float that gets rounded down\n+        #  - n * alpha is a float that gest rounded up\n+        n = 102  # n * alpha = 20.4, 51. , 91.8\n+        y = rng.random(n)\n+        x = np.quantile(y, alpha, method=method)\n+        if method in (\"higher\",):\n+            # These methods do not fulfill the identification equation.\n+            assert np.abs(np.mean(self.V(x, y, alpha))) > 0.1 / n\n+        elif int(n * alpha) == n * alpha:\n+            # We can expect exact results, up to machine precision.\n+            assert_allclose(np.mean(self.V(x, y, alpha)), 0, atol=1e-14)\n+        else:\n+            # V = (x >= y) - alpha cannot sum to zero exactly but within\n+            # \"sample precision\".\n+            assert_allclose(np.mean(self.V(x, y, alpha)), 0,\n+                atol=1 / n / np.amin([alpha, 1 - alpha]))\n+\n+    @pytest.mark.parametrize(\"method\", quantile_methods)\n+    @pytest.mark.parametrize(\"alpha\", [0.2, 0.5, 0.9])\n+    def test_quantile_add_and_multiply_constant(self, method, alpha):\n+        # Test that\n+        #  1. quantile(c + x) = c + quantile(x)\n+        #  2. quantile(c * x) = c * quantile(x)\n+        #  3. quantile(-x) = -quantile(x, 1 - alpha)\n+        #     On empirical quantiles, this equation does not hold exactly.\n+        # Koenker (2005) \"Quantile Regression\" Chapter 2.2.3 calls these\n+        # properties equivariance.\n+        rng = np.random.default_rng(4321)\n+        # We choose n and alpha such that we have cases for\n+        #  - n * alpha is an integer\n+        #  - n * alpha is a float that gets rounded down\n+        #  - n * alpha is a float that gest rounded up\n+        n = 102  # n * alpha = 20.4, 51. , 91.8\n+        y = rng.random(n)\n+        q = np.quantile(y, alpha, method=method)\n+        c = 13.5\n+\n+        # 1\n+        assert_allclose(np.quantile(c + y, alpha, method=method), c + q)\n+        # 2\n+        assert_allclose(np.quantile(c * y, alpha, method=method), c * q)\n+        # 3\n+        q = -np.quantile(-y, 1 - alpha, method=method)\n+        if method == \"inverted_cdf\":\n+            if (\n+                n * alpha == int(n * alpha)\n+                or np.round(n * alpha) == int(n * alpha) + 1\n+            ):\n+                assert_allclose(q, np.quantile(y, alpha, method=\"higher\"))\n+            else:\n+                assert_allclose(q, np.quantile(y, alpha, method=\"lower\"))\n+        elif method == \"closest_observation\":\n+            if n * alpha == int(n * alpha):\n+                assert_allclose(q, np.quantile(y, alpha, method=\"higher\"))\n+            elif np.round(n * alpha) == int(n * alpha) + 1:\n+                assert_allclose(\n+                    q, np.quantile(y, alpha + 1/n, method=\"higher\"))\n+            else:\n+                assert_allclose(q, np.quantile(y, alpha, method=\"lower\"))\n+        elif method == \"interpolated_inverted_cdf\":\n+            assert_allclose(q, np.quantile(y, alpha + 1/n, method=method))\n+        elif method == \"nearest\":\n+            if n * alpha == int(n * alpha):\n+                assert_allclose(q, np.quantile(y, alpha + 1/n, method=method))\n+            else:\n+                assert_allclose(q, np.quantile(y, alpha, method=method))\n+        elif method == \"lower\":\n+            assert_allclose(q, np.quantile(y, alpha, method=\"higher\"))\n+        elif method == \"higher\":\n+            assert_allclose(q, np.quantile(y, alpha, method=\"lower\"))\n+        else:\n+            # \"averaged_inverted_cdf\", \"hazen\", \"weibull\", \"linear\",\n+            # \"median_unbiased\", \"normal_unbiased\", \"midpoint\"\n+            assert_allclose(q, np.quantile(y, alpha, method=method))\n+\n \n class TestLerp:\n     @hypothesis.given(t0=st.floats(allow_nan=False, allow_infinity=False,\n",
            "comment_added_diff": {
                "3552": "        # Identification function used in several tests.",
                "3667": "        # Test that the identification equation holds for the empirical",
                "3668": "        # CDF:",
                "3669": "        #   E[V(x, Y)] = 0  <=>  x is quantile",
                "3670": "        # with Y the random variable for which we have observed values and",
                "3671": "        # V(x, y) the canonical identification function for the quantile (at",
                "3672": "        # level alpha), see",
                "3673": "        # https://doi.org/10.48550/arXiv.0912.0902",
                "3675": "        # We choose n and alpha such that we cover 3 cases:",
                "3676": "        #  - n * alpha is an integer",
                "3677": "        #  - n * alpha is a float that gets rounded down",
                "3678": "        #  - n * alpha is a float that gest rounded up",
                "3679": "        n = 102  # n * alpha = 20.4, 51. , 91.8",
                "3683": "            # These methods do not fulfill the identification equation.",
                "3686": "            # We can expect exact results, up to machine precision.",
                "3689": "            # V = (x >= y) - alpha cannot sum to zero exactly but within",
                "3690": "            # \"sample precision\".",
                "3697": "        # Test that",
                "3698": "        #  1. quantile(c + x) = c + quantile(x)",
                "3699": "        #  2. quantile(c * x) = c * quantile(x)",
                "3700": "        #  3. quantile(-x) = -quantile(x, 1 - alpha)",
                "3701": "        #     On empirical quantiles, this equation does not hold exactly.",
                "3702": "        # Koenker (2005) \"Quantile Regression\" Chapter 2.2.3 calls these",
                "3703": "        # properties equivariance.",
                "3705": "        # We choose n and alpha such that we have cases for",
                "3706": "        #  - n * alpha is an integer",
                "3707": "        #  - n * alpha is a float that gets rounded down",
                "3708": "        #  - n * alpha is a float that gest rounded up",
                "3709": "        n = 102  # n * alpha = 20.4, 51. , 91.8",
                "3714": "        # 1",
                "3716": "        # 2",
                "3718": "        # 3",
                "3748": "            # \"averaged_inverted_cdf\", \"hazen\", \"weibull\", \"linear\",",
                "3749": "            # \"median_unbiased\", \"normal_unbiased\", \"midpoint\""
            },
            "comment_deleted_diff": {},
            "comment_modified_diff": {}
        },
        {
            "commit": "6ac4d6ded2dded576a5af12820fa49a961130468",
            "timestamp": "2023-05-17T14:03:39+02:00",
            "author": "Sebastian Berg",
            "commit_message": "BUG: Fix median and quantile NaT handling\n\nNote that this doesn't mean that rounding is correct at least for\nquantiles, so there is some dubious about it being a good idea to\nuse this.\n\nBut, it does fix the issue, and I the `copyto` solution seems rather\ngood to me, the only thing that isn't ideal is the `supports_nan`\ndefinition itself.\n\nCloses gh-20376",
            "additions": 38,
            "deletions": 0,
            "change_type": "MODIFY",
            "diff": "@@ -3537,6 +3537,25 @@ def test_nan_q(self):\n         with pytest.raises(ValueError, match=\"Percentiles must be in\"):\n             np.percentile([1, 2, 3, 4.0], q)\n \n+    @pytest.mark.parametrize(\"dtype\", [\"m8[D]\", \"M8[s]\"])\n+    @pytest.mark.parametrize(\"pos\", [0, 23, 10])\n+    def test_nat_basic(self, dtype, pos):\n+        # TODO: Note that times have dubious rounding as of fixing NaTs!\n+        # NaT and NaN should behave the same, do basic tests for NaT:\n+        a = np.arange(0, 24, dtype=dtype)\n+        a[pos] = \"NaT\"\n+        res = np.percentile(a, 30)\n+        assert res.dtype == dtype\n+        assert np.isnat(res)\n+        res = np.percentile(a, [30, 60])\n+        assert res.dtype == dtype\n+        assert np.isnat(res).all()\n+\n+        a = np.arange(0, 24*3, dtype=dtype).reshape(-1, 3)\n+        a[pos, 1] = \"NaT\"\n+        res = np.percentile(a, 30, axis=0)\n+        assert_array_equal(np.isnat(res), [False, True, False])\n+\n \n quantile_methods = [\n     'inverted_cdf', 'averaged_inverted_cdf', 'closest_observation',\n@@ -4072,6 +4091,25 @@ def test_keepdims_out(self, axis):\n         assert result is out\n         assert_equal(result.shape, shape_out)\n \n+    @pytest.mark.parametrize(\"dtype\", [\"m8[s]\"])\n+    @pytest.mark.parametrize(\"pos\", [0, 23, 10])\n+    def test_nat_behavior(self, dtype, pos):\n+        # TODO: Median does not support Datetime, due to `mean`.\n+        # NaT and NaN should behave the same, do basic tests for NaT.\n+        a = np.arange(0, 24, dtype=dtype)\n+        a[pos] = \"NaT\"\n+        res = np.median(a)\n+        assert res.dtype == dtype\n+        assert np.isnat(res)\n+        res = np.percentile(a, [30, 60])\n+        assert res.dtype == dtype\n+        assert np.isnat(res).all()\n+\n+        a = np.arange(0, 24*3, dtype=dtype).reshape(-1, 3)\n+        a[pos, 1] = \"NaT\"\n+        res = np.median(a, axis=0)\n+        assert_array_equal(np.isnat(res), [False, True, False])\n+\n \n class TestAdd_newdoc_ufunc:\n \n",
            "comment_added_diff": {
                "3543": "        # TODO: Note that times have dubious rounding as of fixing NaTs!",
                "3544": "        # NaT and NaN should behave the same, do basic tests for NaT:",
                "4097": "        # TODO: Median does not support Datetime, due to `mean`.",
                "4098": "        # NaT and NaN should behave the same, do basic tests for NaT."
            },
            "comment_deleted_diff": {},
            "comment_modified_diff": {}
        },
        {
            "commit": "250e1479ce342d9d7ab8a592508f6ce892d4c98b",
            "timestamp": "2023-08-12T22:01:16+02:00",
            "author": "Mateusz Sok\u00f3\u0142",
            "commit_message": "API: Cleaning `numpy/__init__.py` and main namespace - Part 2 [NEP 52] (#24357)\n\n[skip ci]",
            "additions": 24,
            "deletions": 0,
            "change_type": "MODIFY",
            "diff": "@@ -1,10 +1,16 @@\n+import sys\n+\n import pytest\n+\n+import numpy as np\n from numpy import (\n     logspace, linspace, geomspace, dtype, array, sctypes, arange, isnan,\n     ndarray, sqrt, nextafter, stack, errstate\n     )\n+from numpy.core.function_base import add_newdoc\n from numpy.testing import (\n     assert_, assert_equal, assert_raises, assert_array_equal, assert_allclose,\n+    IS_PYPY\n     )\n \n \n@@ -444,3 +450,21 @@ def test_any_step_zero_and_not_mult_inplace(self):\n         assert_array_equal(y, array([[0.0, 1.0], [1.0, 1.0], [2.0, 1.0]]))\n     \n \n+class TestAdd_newdoc:\n+\n+    @pytest.mark.skipif(sys.flags.optimize == 2, reason=\"Python running -OO\")\n+    @pytest.mark.xfail(IS_PYPY, reason=\"PyPy does not modify tp_doc\")\n+    def test_add_doc(self):\n+        # test that np.add_newdoc did attach a docstring successfully:\n+        tgt = \"Current flat index into the array.\"\n+        assert_equal(np.core.flatiter.index.__doc__[:len(tgt)], tgt)\n+        assert_(len(np.core.ufunc.identity.__doc__) > 300)\n+        assert_(len(np.lib.index_tricks.mgrid.__doc__) > 300)\n+\n+    @pytest.mark.skipif(sys.flags.optimize == 2, reason=\"Python running -OO\")\n+    def test_errors_are_ignored(self):\n+        prev_doc = np.core.flatiter.index.__doc__\n+        # nothing changed, but error ignored, this should probably\n+        # give a warning (or even error) in the future.\n+        add_newdoc(\"numpy.core\", \"flatiter\", (\"index\", \"bad docstring\"))\n+        assert prev_doc == np.core.flatiter.index.__doc__\n",
            "comment_added_diff": {
                "458": "        # test that np.add_newdoc did attach a docstring successfully:",
                "467": "        # nothing changed, but error ignored, this should probably",
                "468": "        # give a warning (or even error) in the future."
            },
            "comment_deleted_diff": {},
            "comment_modified_diff": {}
        },
        {
            "commit": "250e1479ce342d9d7ab8a592508f6ce892d4c98b",
            "timestamp": "2023-08-12T22:01:16+02:00",
            "author": "Mateusz Sok\u00f3\u0142",
            "commit_message": "API: Cleaning `numpy/__init__.py` and main namespace - Part 2 [NEP 52] (#24357)\n\n[skip ci]",
            "additions": 3,
            "deletions": 62,
            "change_type": "MODIFY",
            "diff": "@@ -21,7 +21,7 @@\n import numpy.lib.function_base as nfb\n from numpy.random import rand\n from numpy.lib import (\n-    add_newdoc_ufunc, angle, average, bartlett, blackman, corrcoef, cov,\n+    angle, average, bartlett, blackman, corrcoef, cov,\n     delete, diff, digitize, extract, flipud, gradient, hamming, hanning,\n     i0, insert, interp, kaiser, meshgrid, msort, piecewise, place, rot90,\n     select, setxor1d, sinc, trapz, trim_zeros, unwrap, unique, vectorize\n@@ -3061,12 +3061,12 @@ def test_2D(self):\n     @pytest.mark.parametrize(\"dtype\", np.typecodes[\"Float\"])\n     def test_linear_nan_1D(self, dtype):\n         # METHOD 1 of H&F\n-        arr = np.asarray([15.0, np.NAN, 35.0, 40.0, 50.0], dtype=dtype)\n+        arr = np.asarray([15.0, np.nan, 35.0, 40.0, 50.0], dtype=dtype)\n         res = np.percentile(\n             arr,\n             40.0,\n             method=\"linear\")\n-        np.testing.assert_equal(res, np.NAN)\n+        np.testing.assert_equal(res, np.nan)\n         np.testing.assert_equal(res.dtype, arr.dtype)\n \n     H_F_TYPE_CODES = [(int_type, np.float64)\n@@ -4109,65 +4109,6 @@ def test_nat_behavior(self, dtype, pos):\n         assert_array_equal(np.isnat(res), [False, True, False])\n \n \n-class TestAdd_newdoc_ufunc:\n-\n-    def test_ufunc_arg(self):\n-        assert_raises(TypeError, add_newdoc_ufunc, 2, \"blah\")\n-        assert_raises(ValueError, add_newdoc_ufunc, np.add, \"blah\")\n-\n-    def test_string_arg(self):\n-        assert_raises(TypeError, add_newdoc_ufunc, np.add, 3)\n-\n-\n-class TestAdd_newdoc:\n-\n-    @pytest.mark.skipif(sys.flags.optimize == 2, reason=\"Python running -OO\")\n-    @pytest.mark.xfail(IS_PYPY, reason=\"PyPy does not modify tp_doc\")\n-    def test_add_doc(self):\n-        # test that np.add_newdoc did attach a docstring successfully:\n-        tgt = \"Current flat index into the array.\"\n-        assert_equal(np.core.flatiter.index.__doc__[:len(tgt)], tgt)\n-        assert_(len(np.core.ufunc.identity.__doc__) > 300)\n-        assert_(len(np.lib.index_tricks.mgrid.__doc__) > 300)\n-\n-    @pytest.mark.skipif(sys.flags.optimize == 2, reason=\"Python running -OO\")\n-    def test_errors_are_ignored(self):\n-        prev_doc = np.core.flatiter.index.__doc__\n-        # nothing changed, but error ignored, this should probably\n-        # give a warning (or even error) in the future.\n-        np.add_newdoc(\"numpy.core\", \"flatiter\", (\"index\", \"bad docstring\"))\n-        assert prev_doc == np.core.flatiter.index.__doc__\n-\n-\n-class TestAddDocstring():\n-    # Test should possibly be moved, but it also fits to be close to\n-    # the newdoc tests...\n-    @pytest.mark.skipif(sys.flags.optimize == 2, reason=\"Python running -OO\")\n-    @pytest.mark.skipif(IS_PYPY, reason=\"PyPy does not modify tp_doc\")\n-    def test_add_same_docstring(self):\n-        # test for attributes (which are C-level defined)\n-        np.add_docstring(np.ndarray.flat, np.ndarray.flat.__doc__)\n-        # And typical functions:\n-        def func():\n-            \"\"\"docstring\"\"\"\n-            return\n-\n-        np.add_docstring(func, func.__doc__)\n-\n-    @pytest.mark.skipif(sys.flags.optimize == 2, reason=\"Python running -OO\")\n-    def test_different_docstring_fails(self):\n-        # test for attributes (which are C-level defined)\n-        with assert_raises(RuntimeError):\n-            np.add_docstring(np.ndarray.flat, \"different docstring\")\n-        # And typical functions:\n-        def func():\n-            \"\"\"docstring\"\"\"\n-            return\n-\n-        with assert_raises(RuntimeError):\n-            np.add_docstring(func, \"different docstring\")\n-\n-\n class TestSortComplex:\n \n     @pytest.mark.parametrize(\"type_in, type_out\", [\n",
            "comment_added_diff": {},
            "comment_deleted_diff": {
                "4127": "        # test that np.add_newdoc did attach a docstring successfully:",
                "4136": "        # nothing changed, but error ignored, this should probably",
                "4137": "        # give a warning (or even error) in the future.",
                "4143": "    # Test should possibly be moved, but it also fits to be close to",
                "4144": "    # the newdoc tests...",
                "4148": "        # test for attributes (which are C-level defined)",
                "4150": "        # And typical functions:",
                "4159": "        # test for attributes (which are C-level defined)",
                "4162": "        # And typical functions:"
            },
            "comment_modified_diff": {}
        }
    ],
    "22457.change.rst": [],
    "nditer_constr.c": [],
    "test_nditer.py": [
        {
            "commit": "9a5c2f5c56c75f823f96615786819d6d52500899",
            "timestamp": "2022-10-20T09:53:48+02:00",
            "author": "Sebastian Berg",
            "commit_message": "ENH: Allow all allocated operands in nditer/NpyIter\n\nThis allows all operands to be allocated,  it is necessary to\nprovide the dtype in this case, if missing the error changes\nfrom ``ValueError`` to ``TypeError``\n\nCloses gh-13934, gh-15140",
            "additions": 13,
            "deletions": 3,
            "change_type": "MODIFY",
            "diff": "@@ -1594,11 +1594,12 @@ def test_iter_allocate_output_errors():\n     # Allocated output can't have buffering without delayed bufalloc\n     assert_raises(ValueError, nditer, [a, None], ['buffered'],\n                                             ['allocate', 'readwrite'])\n-    # Must specify at least one input\n-    assert_raises(ValueError, nditer, [None, None], [],\n+    # Must specify dtype if there are no inputs (cannot promote existing ones;\n+    # maybe this should use the 'f4' here, but it does not historically.)\n+    assert_raises(TypeError, nditer, [None, None], [],\n                         [['writeonly', 'allocate'],\n                          ['writeonly', 'allocate']],\n-                        op_dtypes=[np.dtype('f4'), np.dtype('f4')])\n+                        op_dtypes=[None, np.dtype('f4')])\n     # If using op_axes, must specify all the axes\n     a = arange(24, dtype='i4').reshape(2, 3, 4)\n     assert_raises(ValueError, nditer, [a, None], [],\n@@ -1623,6 +1624,15 @@ def test_iter_allocate_output_errors():\n                         op_dtypes=[None, np.dtype('f4')],\n                         op_axes=[None, [0, np.newaxis, 2]])\n \n+def test_all_allocated():\n+    # When no output and no shape is given, `()` is used as shape.\n+    i = np.nditer([None], op_dtypes=[\"int64\"])\n+    assert i.operands[0].shape == ()\n+    assert i.dtypes == (np.dtype(\"int64\"),)\n+\n+    i = np.nditer([None], op_dtypes=[\"int64\"], itershape=(2, 3, 4))\n+    assert i.operands[0].shape == (2, 3, 4)\n+\n def test_iter_remove_axis():\n     a = arange(24).reshape(2, 3, 4)\n \n",
            "comment_added_diff": {
                "1597": "    # Must specify dtype if there are no inputs (cannot promote existing ones;",
                "1598": "    # maybe this should use the 'f4' here, but it does not historically.)",
                "1628": "    # When no output and no shape is given, `()` is used as shape."
            },
            "comment_deleted_diff": {
                "1597": "    # Must specify at least one input"
            },
            "comment_modified_diff": {
                "1597": "    # Must specify at least one input",
                "1598": "    assert_raises(ValueError, nditer, [None, None], [],"
            }
        },
        {
            "commit": "18a10e62ad39c6430517cc0802a1efdc2075fc1a",
            "timestamp": "2022-10-27T13:03:05+02:00",
            "author": "Sebastian Berg",
            "commit_message": "TST: Make test_partial_iteration_cleanup robust but require leak checker\n\nThis makes sure the test is not flaky, but the test now requires\na leak checker (both valgrind or reference count based should work\nin CPython at least).\n\nCloses gh-21169",
            "additions": 9,
            "deletions": 13,
            "change_type": "MODIFY",
            "diff": "@@ -3169,7 +3169,6 @@ def test_warn_noclose():\n \n @pytest.mark.skipif(sys.version_info[:2] == (3, 9) and sys.platform == \"win32\",\n                     reason=\"Errors with Python 3.9 on Windows\")\n-@pytest.mark.skipif(not HAS_REFCOUNT, reason=\"Python lacks refcounts\")\n @pytest.mark.parametrize([\"in_dtype\", \"buf_dtype\"],\n         [(\"i\", \"O\"), (\"O\", \"i\"),  # most simple cases\n          (\"i,O\", \"O,O\"),  # structured partially only copying O\n@@ -3177,9 +3176,14 @@ def test_warn_noclose():\n          ])\n @pytest.mark.parametrize(\"steps\", [1, 2, 3])\n def test_partial_iteration_cleanup(in_dtype, buf_dtype, steps):\n-    value = 123  # relies on python cache (leak-check will still find it)\n+    \"\"\"\n+    Checks for reference counting leaks during cleanup.  Using explicit\n+    reference counts lead to occasional false positives (at least in parallel\n+    test setups).  This test now should still test leaks correctly when\n+    run e.g. with pytest-valgrind or pytest-leaks\n+    \"\"\"\n+    value = 2**30 + 1  # just a random value that Python won't intern\n     arr = np.full(int(np.BUFSIZE * 2.5), value).astype(in_dtype)\n-    count = sys.getrefcount(value)\n \n     it = np.nditer(arr, op_dtypes=[np.dtype(buf_dtype)],\n             flags=[\"buffered\", \"external_loop\", \"refs_ok\"], casting=\"unsafe\")\n@@ -3187,11 +3191,7 @@ def test_partial_iteration_cleanup(in_dtype, buf_dtype, steps):\n         # The iteration finishes in 3 steps, the first two are partial\n         next(it)\n \n-    # Note that resetting does not free references\n-    del it\n-    break_cycles()\n-    break_cycles()\n-    assert count == sys.getrefcount(value)\n+    del it  # not necessary, but we test the cleanup\n \n     # Repeat the test with `iternext`\n     it = np.nditer(arr, op_dtypes=[np.dtype(buf_dtype)],\n@@ -3199,11 +3199,7 @@ def test_partial_iteration_cleanup(in_dtype, buf_dtype, steps):\n     for step in range(steps):\n         it.iternext()\n \n-    del it  # should ensure cleanup\n-    break_cycles()\n-    break_cycles()\n-    assert count == sys.getrefcount(value)\n-\n+    del it  # not necessary, but we test the cleanup\n \n @pytest.mark.skipif(not HAS_REFCOUNT, reason=\"Python lacks refcounts\")\n @pytest.mark.parametrize([\"in_dtype\", \"buf_dtype\"],\n",
            "comment_added_diff": {
                "3185": "    value = 2**30 + 1  # just a random value that Python won't intern",
                "3194": "    del it  # not necessary, but we test the cleanup",
                "3202": "    del it  # not necessary, but we test the cleanup"
            },
            "comment_deleted_diff": {
                "3180": "    value = 123  # relies on python cache (leak-check will still find it)",
                "3190": "    # Note that resetting does not free references",
                "3202": "    del it  # should ensure cleanup"
            },
            "comment_modified_diff": {
                "3194": "    assert count == sys.getrefcount(value)",
                "3202": "    del it  # should ensure cleanup"
            }
        },
        {
            "commit": "998cbb521a1f0d845b3266276fb97483552c9419",
            "timestamp": "2023-02-19T21:00:35+01:00",
            "author": "Sebastian Berg",
            "commit_message": "TST: Add test and comment about path that seems uncoverable",
            "additions": 30,
            "deletions": 0,
            "change_type": "MODIFY",
            "diff": "@@ -1457,6 +1457,36 @@ def test_iter_copy_casts_structured():\n         assert_array_equal(res2[\"b\"][field], expected)\n \n \n+def test_iter_copy_casts_structured2():\n+    # Similar to the above, this is a fairly arcane test to cover internals\n+    in_dtype = np.dtype([(\"a\", np.dtype(\"O,O\")),\n+                         (\"b\", np.dtype(\"(5)O,(3)O,(1,)O\"))])\n+    out_dtype = np.dtype([(\"a\", np.dtype(\"O\")),\n+                          (\"b\", np.dtype(\"O,(3)i,(4)O\"))])\n+\n+    arr = np.ones(1, dtype=in_dtype)\n+    it = np.nditer((arr,), [\"buffered\", \"external_loop\", \"refs_ok\"],\n+                   op_dtypes=[out_dtype], casting=\"unsafe\")\n+    it_copy = it.copy()\n+\n+    res1 = next(it)\n+    del it\n+    res2 = next(it_copy)\n+    del it_copy\n+\n+    # Array of two structured scalars:\n+    for res in res1, res2:\n+        # Cast to tuple by getitem, which may be weird and changable?:\n+        assert type(res[\"a\"][0]) == tuple\n+        assert res[\"a\"][0] == (1, 1)\n+\n+    for res in res1, res2:\n+        assert_array_equal(res[\"b\"][\"f0\"][0], np.ones(5, dtype=object))\n+        assert_array_equal(res[\"b\"][\"f1\"], np.ones((1, 3), dtype=\"i\"))\n+        assert res[\"b\"][\"f2\"].shape == (1, 4)\n+        assert_array_equal(res[\"b\"][\"f2\"][0], np.ones(4, dtype=object))\n+\n+\n def test_iter_allocate_output_simple():\n     # Check that the iterator will properly allocate outputs\n \n",
            "comment_added_diff": {
                "1461": "    # Similar to the above, this is a fairly arcane test to cover internals",
                "1477": "    # Array of two structured scalars:",
                "1479": "        # Cast to tuple by getitem, which may be weird and changable?:"
            },
            "comment_deleted_diff": {},
            "comment_modified_diff": {}
        },
        {
            "commit": "f9ff49e9daf20c2f1acf716b07d9c8d340240317",
            "timestamp": "2023-06-18T18:30:24+02:00",
            "author": "Dimitri Papadopoulos",
            "commit_message": "MAINT: Fix typos found by codespell",
            "additions": 1,
            "deletions": 1,
            "change_type": "MODIFY",
            "diff": "@@ -1476,7 +1476,7 @@ def test_iter_copy_casts_structured2():\n \n     # Array of two structured scalars:\n     for res in res1, res2:\n-        # Cast to tuple by getitem, which may be weird and changable?:\n+        # Cast to tuple by getitem, which may be weird and changeable?:\n         assert type(res[\"a\"][0]) == tuple\n         assert res[\"a\"][0] == (1, 1)\n \n",
            "comment_added_diff": {
                "1479": "        # Cast to tuple by getitem, which may be weird and changeable?:"
            },
            "comment_deleted_diff": {
                "1479": "        # Cast to tuple by getitem, which may be weird and changable?:"
            },
            "comment_modified_diff": {
                "1479": "        # Cast to tuple by getitem, which may be weird and changable?:"
            }
        }
    ],
    "npy_math.h": [],
    "ieee754.cpp": [],
    "npy_math_internal.h.src": [],
    "__init__.py": [
        {
            "commit": "075859216fae0509c52a54cb5c96c217f23026ca",
            "timestamp": "2022-11-17T14:21:54+01:00",
            "author": "Sebastian Berg",
            "commit_message": "DEP: Next step in scalar type alias deprecations/futurewarnings\n\nFinalizes the scalar type alias deprecations making them an error.\nHowever, at the same time adds a `FutureWarning` that new aliases\nwill be introduced in the future.\n(They would eventually be preferred over the `str_`, etc. version.)\n\nIt may make sense, that this FutureWarning is already propelled soon\nsince it interacts with things such as changing the representation of\nstrings to `np.str_(\"\")` if the preferred alias becomes `np.str`.\n\nIt also introduces a new deprecation to remove the 0 sized bit-aliases\nand the bitsize `bool8` alias.  (Unfortunately, these are here still allowed\nas part of the `np.sctypeDict`).",
            "additions": 28,
            "deletions": 58,
            "change_type": "MODIFY",
            "diff": "@@ -155,70 +155,32 @@\n     from . import matrixlib as _mat\n     from .matrixlib import *\n \n-    # Deprecations introduced in NumPy 1.20.0, 2020-06-06\n-    import builtins as _builtins\n-\n+    # Future warning introduced in NumPy 1.24.0, 2022-11-17\n     _msg = (\n-        \"`np.{n}` is a deprecated alias for the builtin `{n}`. \"\n-        \"To silence this warning, use `{n}` by itself. Doing this will not \"\n-        \"modify any behavior and is safe. {extended_msg}\\n\"\n-        \"Deprecated in NumPy 1.20; for more details and guidance: \"\n-        \"https://numpy.org/devdocs/release/1.20.0-notes.html#deprecations\")\n-\n-    _specific_msg = (\n-        \"If you specifically wanted the numpy scalar type, use `np.{}` here.\")\n-\n-    _int_extended_msg = (\n-        \"When replacing `np.{}`, you may wish to use e.g. `np.int64` \"\n-        \"or `np.int32` to specify the precision. If you wish to review \"\n-        \"your current use, check the release note link for \"\n-        \"additional information.\")\n+        \"`np.{n}` is a deprecated alias for `{an}`.  (Deprecated NumPy 1.24)\")\n \n+    # Some of these are awkard (since `np.str` may be preferable in the long\n+    # term), but overall the names ending in 0 seem undesireable\n     _type_info = [\n-        (\"object\", \"\"),  # The NumPy scalar only exists by name.\n-        (\"bool\", _specific_msg.format(\"bool_\")),\n-        (\"float\", _specific_msg.format(\"float64\")),\n-        (\"complex\", _specific_msg.format(\"complex128\")),\n-        (\"str\", _specific_msg.format(\"str_\")),\n-        (\"int\", _int_extended_msg.format(\"int\"))]\n+        (\"bool8\", bool_, \"np.bool_\"),\n+        (\"int0\", intp, \"np.intp\"),\n+        (\"uint0\", uintp, \"np.uintp\"),\n+        (\"str0\", str_, \"np.str_\"),\n+        (\"bytes0\", bytes_, \"np.bytes_\"),\n+        (\"void0\", void, \"np.void\"),\n+        (\"object0\", object_,\n+            \"`np.object0` is a deprecated alias for `np.object_`. \"\n+            \"`object` can be used instead.  (Deprecated NumPy 1.24)\")]\n+\n+    # Some of these could be defined right away, but most were aliases to\n+    # the Python objects before.  When defined, these should possibly not\n+    # be added to `__all__` to avoid import with `from numpy import *`.\n+    __future_scalars__ = {\"bool\", \"long\", \"ulong\", \"str\", \"bytes\", \"object\"}\n \n     __deprecated_attrs__.update({\n-        n: (getattr(_builtins, n), _msg.format(n=n, extended_msg=extended_msg))\n-        for n, extended_msg in _type_info\n-    })\n-\n-    # Numpy 1.20.0, 2020-10-19\n-    __deprecated_attrs__[\"typeDict\"] = (\n-        core.numerictypes.typeDict,\n-        \"`np.typeDict` is a deprecated alias for `np.sctypeDict`.\"\n-    )\n-\n-    # NumPy 1.22, 2021-10-20\n-    __deprecated_attrs__[\"MachAr\"] = (\n-        core._machar.MachAr,\n-        \"`np.MachAr` is deprecated (NumPy 1.22).\"\n-    )\n+        n: (alias, _msg.format(n=n, an=an)) for n, alias, an in _type_info})\n \n-    _msg = (\n-        \"`np.{n}` is a deprecated alias for `np.compat.{n}`. \"\n-        \"To silence this warning, use `np.compat.{n}` by itself, or \"\n-        \"the builtin `{n2}` for which `np.compat.{n}` is itself an \"\n-        \"alias. Doing this will not modify any behaviour and is safe. \"\n-        \"{extended_msg}\\n\"\n-        \"Deprecated in NumPy 1.20; for more details and guidance: \"\n-        \"https://numpy.org/devdocs/release/1.20.0-notes.html#deprecations\")\n-\n-    __deprecated_attrs__[\"long\"] = (\n-        getattr(compat, \"long\"),\n-        _msg.format(n=\"long\", n2=\"int\",\n-                    extended_msg=_int_extended_msg.format(\"long\")))\n-\n-    __deprecated_attrs__[\"unicode\"] = (\n-        getattr(compat, \"unicode\"),\n-        _msg.format(n=\"unicode\", n2=\"str\",\n-                    extended_msg=_specific_msg.format(\"str_\")))\n-\n-    del _msg, _specific_msg, _int_extended_msg, _type_info, _builtins\n+    del _msg, _type_info\n \n     from .core import round, abs, max, min\n     # now that numpy modules are imported, can initialize limits\n@@ -296,6 +258,14 @@ def _expired(*args, **kwds):\n             warnings.warn(msg, DeprecationWarning, stacklevel=2)\n             return val\n \n+        if attr in __future_scalars__:\n+            # And future warnings for those that will change, but also give\n+            # the AttributeError\n+            warnings.warn(\n+                \"In the future the attribute `%s` will be defined as the \"\n+                \"corresponding NumPy scalar.  (This may have returned Python \"\n+                \"scalars in past versions.\", FutureWarning)\n+\n         # Importing Tester requires importing all of UnitTest which is not a\n         # cheap import Since it is mainly used in test suits, we lazy import it\n         # here to save on the order of 10 ms of import time for most users\n",
            "comment_added_diff": {
                "158": "    # Future warning introduced in NumPy 1.24.0, 2022-11-17",
                "162": "    # Some of these are awkard (since `np.str` may be preferable in the long",
                "163": "    # term), but overall the names ending in 0 seem undesireable",
                "175": "    # Some of these could be defined right away, but most were aliases to",
                "176": "    # the Python objects before.  When defined, these should possibly not",
                "177": "    # be added to `__all__` to avoid import with `from numpy import *`.",
                "262": "            # And future warnings for those that will change, but also give",
                "263": "            # the AttributeError"
            },
            "comment_deleted_diff": {
                "158": "    # Deprecations introduced in NumPy 1.20.0, 2020-06-06",
                "166": "        \"https://numpy.org/devdocs/release/1.20.0-notes.html#deprecations\")",
                "178": "        (\"object\", \"\"),  # The NumPy scalar only exists by name.",
                "190": "    # Numpy 1.20.0, 2020-10-19",
                "196": "    # NumPy 1.22, 2021-10-20",
                "209": "        \"https://numpy.org/devdocs/release/1.20.0-notes.html#deprecations\")"
            },
            "comment_modified_diff": {
                "158": "    # Deprecations introduced in NumPy 1.20.0, 2020-06-06",
                "162": "        \"`np.{n}` is a deprecated alias for the builtin `{n}`. \"",
                "163": "        \"To silence this warning, use `{n}` by itself. Doing this will not \"",
                "175": "        \"additional information.\")"
            }
        },
        {
            "commit": "d41658143d15dfe2f17ad7fd11467dcdba9c3070",
            "timestamp": "2022-11-21T09:10:02+01:00",
            "author": "Sebastian Berg",
            "commit_message": "DOC: Adjust comments on deprecated/future scalar alias based on review",
            "additions": 5,
            "deletions": 3,
            "change_type": "MODIFY",
            "diff": "@@ -159,7 +159,7 @@\n     _msg = (\n         \"`np.{n}` is a deprecated alias for `{an}`.  (Deprecated NumPy 1.24)\")\n \n-    # Some of these are awkard (since `np.str` may be preferable in the long\n+    # Some of these are awkward (since `np.str` may be preferable in the long\n     # term), but overall the names ending in 0 seem undesireable\n     _type_info = [\n         (\"bool8\", bool_, \"np.bool_\"),\n@@ -173,8 +173,10 @@\n             \"`object` can be used instead.  (Deprecated NumPy 1.24)\")]\n \n     # Some of these could be defined right away, but most were aliases to\n-    # the Python objects before.  When defined, these should possibly not\n-    # be added to `__all__` to avoid import with `from numpy import *`.\n+    # the Python objects and only removed in NumPy 1.24.  Defining them should\n+    # probably wait for NumPy 1.26 or 2.0.\n+    # When defined, these should possibly not be added to `__all__` to avoid\n+    # import with `from numpy import *`.\n     __future_scalars__ = {\"bool\", \"long\", \"ulong\", \"str\", \"bytes\", \"object\"}\n \n     __deprecated_attrs__.update({\n",
            "comment_added_diff": {
                "162": "    # Some of these are awkward (since `np.str` may be preferable in the long",
                "176": "    # the Python objects and only removed in NumPy 1.24.  Defining them should",
                "177": "    # probably wait for NumPy 1.26 or 2.0.",
                "178": "    # When defined, these should possibly not be added to `__all__` to avoid",
                "179": "    # import with `from numpy import *`."
            },
            "comment_deleted_diff": {
                "162": "    # Some of these are awkard (since `np.str` may be preferable in the long",
                "176": "    # the Python objects before.  When defined, these should possibly not",
                "177": "    # be added to `__all__` to avoid import with `from numpy import *`."
            },
            "comment_modified_diff": {
                "162": "    # Some of these are awkard (since `np.str` may be preferable in the long",
                "176": "    # the Python objects before.  When defined, these should possibly not",
                "177": "    # be added to `__all__` to avoid import with `from numpy import *`."
            }
        },
        {
            "commit": "8b9b0efbc08a502627f455ec59656fce68eb10d7",
            "timestamp": "2022-11-22T17:38:33+01:00",
            "author": "Sebastian Berg",
            "commit_message": "DEP: Finalize MachAr and machar deprecations\n\nThis removes the attributes on finfo and the \"public\" module.  It also\ndeprecates `np.core.MachAr`.  We should be able to get away with just\ndeleting it, but there seems little reason to not just deprecate it for now.",
            "additions": 4,
            "deletions": 5,
            "change_type": "MODIFY",
            "diff": "@@ -84,7 +84,6 @@\n from . import function_base\n from .function_base import *\n from . import _machar\n-from ._machar import *\n from . import getlimits\n from .getlimits import *\n from . import shape_base\n@@ -153,13 +152,13 @@ def _DType_reduce(DType):\n \n \n def __getattr__(name):\n-    # Deprecated 2021-10-20, NumPy 1.22\n-    if name == \"machar\":\n+    # Deprecated 2022-11-22, NumPy 1.25.\n+    if name == \"MachAr\":\n         warnings.warn(\n-            \"The `np.core.machar` module is deprecated (NumPy 1.22)\",\n+            \"The `np.core.MachAr` is considered private API (NumPy 1.24)\",\n             DeprecationWarning, stacklevel=2,\n         )\n-        return _machar\n+        return _machar.MachAr\n     raise AttributeError(f\"Module {__name__!r} has no attribute {name!r}\")\n \n \n",
            "comment_added_diff": {
                "155": "    # Deprecated 2022-11-22, NumPy 1.25."
            },
            "comment_deleted_diff": {
                "156": "    # Deprecated 2021-10-20, NumPy 1.22"
            },
            "comment_modified_diff": {}
        },
        {
            "commit": "4002a7d421ff10780c28a3643683af7a9754f87f",
            "timestamp": "2022-11-25T12:37:46+01:00",
            "author": "Ralf Gommers",
            "commit_message": "BLD: enable building NumPy with Meson\n\nThis enables building with NumPy on Linux and macOS. Windows support\nshould be complete to, but is untested as of now and may need a few\ntweaks. This contains:\n- A set of `meson.build` files and related code generation script\n  tweaks, header templates, etc.\n- One CI job on Linux\n- Basic docs on using Meson to build NumPy (not yet integrated in the\n  html docs, it's too early for that - this is for early adopters right\n  now).\n\nThe build should be complete, with the major exception of SIMD support.\nThe full test suite passes. See gh-22546 for the tracking issue with\ndetailed notes on the plan for switching NumPy to Meson as its build\nsystem.\n\nCo-authored-by: Stefan van der Walt <stefanv@berkeley.edu>",
            "additions": 22,
            "deletions": 0,
            "change_type": "ADD",
            "diff": "@@ -0,0 +1,22 @@\n+# Don't use the deprecated NumPy C API. Define this to a fixed version\n+# instead of NPY_API_VERSION in order not to break compilation for\n+# released SciPy versions when NumPy introduces a new deprecation. Use\n+# in setup.py::\n+#\n+#   config.add_extension('_name', sources=['source_fname'], **numpy_nodepr_api)\n+#\n+numpy_nodepr_api = dict(\n+    define_macros=[(\"NPY_NO_DEPRECATED_API\", \"NPY_1_9_API_VERSION\")]\n+)\n+\n+\n+def import_file(folder, module_name):\n+    \"\"\"Import a file directly, avoiding importing scipy\"\"\"\n+    import importlib\n+    import pathlib\n+\n+    fname = pathlib.Path(folder) / f'{module_name}.py'\n+    spec = importlib.util.spec_from_file_location(module_name, str(fname))\n+    module = importlib.util.module_from_spec(spec)\n+    spec.loader.exec_module(module)\n+    return module\n",
            "comment_added_diff": {
                "1": "# Don't use the deprecated NumPy C API. Define this to a fixed version",
                "2": "# instead of NPY_API_VERSION in order not to break compilation for",
                "3": "# released SciPy versions when NumPy introduces a new deprecation. Use",
                "4": "# in setup.py::",
                "5": "#",
                "6": "#   config.add_extension('_name', sources=['source_fname'], **numpy_nodepr_api)",
                "7": "#"
            },
            "comment_deleted_diff": {},
            "comment_modified_diff": {}
        },
        {
            "commit": "ea2f2d65b0df09a04681148930806800909d6473",
            "timestamp": "2022-12-01T12:03:49+01:00",
            "author": "Sebastian Berg",
            "commit_message": "ENH: Add an InvalidPromotion exception",
            "additions": 3,
            "deletions": 1,
            "change_type": "MODIFY",
            "diff": "@@ -107,6 +107,8 @@\n import warnings\n \n from ._globals import _NoValue, _CopyMode\n+from . import exceptions\n+# Note that the following names are imported explicitly for backcompat:\n from .exceptions import (\n     ComplexWarning, ModuleDeprecationWarning, VisibleDeprecationWarning,\n     TooHardError, AxisError)\n@@ -130,7 +132,7 @@\n         raise ImportError(msg) from e\n \n     __all__ = [\n-        'ModuleDeprecationWarning', 'VisibleDeprecationWarning',\n+        'exceptions', 'ModuleDeprecationWarning', 'VisibleDeprecationWarning',\n         'ComplexWarning', 'TooHardError', 'AxisError']\n \n     # mapping of {name: (value, deprecation_msg)}\n",
            "comment_added_diff": {
                "111": "# Note that the following names are imported explicitly for backcompat:"
            },
            "comment_deleted_diff": {},
            "comment_modified_diff": {}
        },
        {
            "commit": "928a7b40a40941c0c282cfad78c3a6021422a71c",
            "timestamp": "2022-12-06T12:00:21+01:00",
            "author": "Sebastian Berg",
            "commit_message": "API: Hide exceptions from the main namespace\n\nI wasn't sure if we should already start deprecating the exceptions\nso opted to follow up with only hiding them from `__dir__()` but\nstill having them in `__all__` and available.\n\nThis also changes their module to `numpy.exceptions`, which matters\nbecause that is how they will be pickled (it would not be possible\nto unpickle such an exception in an older NumPy version).\n\nDue to pickling, we could put off changing the module.",
            "additions": 5,
            "deletions": 2,
            "change_type": "MODIFY",
            "diff": "@@ -107,8 +107,7 @@\n import warnings\n \n from ._globals import _NoValue, _CopyMode\n-from . import exceptions\n-# Note that the following names are imported explicitly for backcompat:\n+# These exceptions were moved in 1.25 and are hidden from __dir__()\n from .exceptions import (\n     ComplexWarning, ModuleDeprecationWarning, VisibleDeprecationWarning,\n     TooHardError, AxisError)\n@@ -144,6 +143,7 @@\n     from . import core\n     from .core import *\n     from . import compat\n+    from . import exceptions\n     from . import lib\n     # NOTE: to be revisited following future namespace cleanup.\n     # See gh-14454 and gh-15672 for discussion.\n@@ -291,6 +291,9 @@ def __dir__():\n         public_symbols = globals().keys() | {'Tester', 'testing'}\n         public_symbols -= {\n             \"core\", \"matrixlib\",\n+            # These were moved in 1.25 and may be deprecated eventually:\n+            \"ModuleDeprecationWarning\", \"VisibleDeprecationWarning\",\n+            \"ComplexWarning\", \"TooHardError\", \"AxisError\"\n         }\n         return list(public_symbols)\n \n",
            "comment_added_diff": {
                "110": "# These exceptions were moved in 1.25 and are hidden from __dir__()",
                "294": "            # These were moved in 1.25 and may be deprecated eventually:"
            },
            "comment_deleted_diff": {
                "111": "# Note that the following names are imported explicitly for backcompat:"
            },
            "comment_modified_diff": {
                "110": "from . import exceptions"
            }
        },
        {
            "commit": "ceccabebc444df4be7a2d744bd5c76854799df38",
            "timestamp": "2023-01-05T17:15:51+01:00",
            "author": "Sebastian Berg",
            "commit_message": "MAINT: Add additional information to missing scalar AttributeError\n\nThis is a followup on gh-22607 which removed them.  Since it appears some\nusers missed the DeprecationWarning entirely, it may help them to\ninclude the old information as an attribute error.\n\nAn example is:\n```\nIn [1]: np.int\n\nAttributeError: module 'numpy' has no attribute 'int'.\n`np.int` was a deprecated alias for the builtin `int`. To avoid this error in existing code, use `int` by itself.\nDoing this will not modify any behavior and is safe. When replacing `np.int`, you may wish to use e.g. `np.int64`\nor `np.int32` to specify the precision. If you wish to review your current use, check the release note link for\nadditional information.\nThe aliases was originally deprecated in NumPy 1.20; for more details and guidance see the original release note\nat:\n    https://numpy.org/devdocs/release/1.20.0-notes.html#deprecations\n```\n\nYes, that is very verbose...\n\nyour changes. Lines starting",
            "additions": 38,
            "deletions": 2,
            "change_type": "MODIFY",
            "diff": "@@ -158,6 +158,40 @@\n     from . import matrixlib as _mat\n     from .matrixlib import *\n \n+    # Deprecations introduced in NumPy 1.20.0, 2020-06-06\n+    import builtins as _builtins\n+\n+    _msg = (\n+        \"module 'numpy' has no attribute '{n}'.\\n\"\n+        \"`np.{n}` was a deprecated alias for the builtin `{n}`. \"\n+        \"To avoid this error in existing code, use `{n}` by itself. \"\n+        \"Doing this will not modify any behavior and is safe. {extended_msg}\\n\"\n+        \"The aliases was originally deprecated in NumPy 1.20; for more \"\n+        \"details and guidance see the original release note at:\\n\"\n+        \"    https://numpy.org/devdocs/release/1.20.0-notes.html#deprecations\")\n+\n+    _specific_msg = (\n+        \"If you specifically wanted the numpy scalar type, use `np.{}` here.\")\n+\n+    _int_extended_msg = (\n+        \"When replacing `np.{}`, you may wish to use e.g. `np.int64` \"\n+        \"or `np.int32` to specify the precision. If you wish to review \"\n+        \"your current use, check the release note link for \"\n+        \"additional information.\")\n+\n+    _type_info = [\n+        (\"object\", \"\"),  # The NumPy scalar only exists by name.\n+        (\"bool\", _specific_msg.format(\"bool_\")),\n+        (\"float\", _specific_msg.format(\"float64\")),\n+        (\"complex\", _specific_msg.format(\"complex128\")),\n+        (\"str\", _specific_msg.format(\"str_\")),\n+        (\"int\", _int_extended_msg.format(\"int\"))]\n+\n+    __former_attrs__ = {\n+         n: _msg.format(n=n, extended_msg=extended_msg)\n+         for n, extended_msg in _type_info\n+     }\n+\n     # Future warning introduced in NumPy 1.24.0, 2022-11-17\n     _msg = (\n         \"`np.{n}` is a deprecated alias for `{an}`.  (Deprecated NumPy 1.24)\")\n@@ -268,8 +302,10 @@ def _expired(*args, **kwds):\n             # the AttributeError\n             warnings.warn(\n                 f\"In the future `np.{attr}` will be defined as the \"\n-                \"corresponding NumPy scalar.  (This may have returned Python \"\n-                \"scalars in past versions.\", FutureWarning, stacklevel=2)\n+                \"corresponding NumPy scalar.\", FutureWarning, stacklevel=2)\n+\n+        if attr in __former_attrs__:\n+            raise AttributeError(__former_attrs__[attr])\n \n         # Importing Tester requires importing all of UnitTest which is not a\n         # cheap import Since it is mainly used in test suits, we lazy import it\n",
            "comment_added_diff": {
                "161": "    # Deprecations introduced in NumPy 1.20.0, 2020-06-06",
                "171": "        \"    https://numpy.org/devdocs/release/1.20.0-notes.html#deprecations\")",
                "183": "        (\"object\", \"\"),  # The NumPy scalar only exists by name."
            },
            "comment_deleted_diff": {},
            "comment_modified_diff": {}
        },
        {
            "commit": "f75bb0edb0e6eec2564de4bf798242984860a19b",
            "timestamp": "2023-01-19T14:35:44-07:00",
            "author": "Charles Harris",
            "commit_message": "MAINT: Remove all nose testing support.\n\nNumPy switched to using pytest in 2018 and nose has been unmaintained\nfor many years. We have kept NumPy's nose support to avoid breaking\ndownstream projects who might have been using it and not yet switched to\npytest or some other testing framework. With the arrival of Python 3.12,\nunpatched nose will raise an error. It it time to move on.\n\nDecorators removed\n\n- raises\n- slow\n- setastest\n- skipif\n- knownfailif\n- deprecated\n- parametrize\n- _needs_refcount\n\nThese are not to be confused with pytest versions with similar names,\ne.g., pytest.mark.slow, pytest.mark.skipif, pytest.mark.parametrize.\n\nFunctions removed\n\n- Tester\n- import_nose\n- run_module_suite",
            "additions": 3,
            "deletions": 9,
            "change_type": "MODIFY",
            "diff": "@@ -307,24 +307,18 @@ def _expired(*args, **kwds):\n         if attr in __former_attrs__:\n             raise AttributeError(__former_attrs__[attr])\n \n-        # Importing Tester requires importing all of UnitTest which is not a\n-        # cheap import Since it is mainly used in test suits, we lazy import it\n-        # here to save on the order of 10 ms of import time for most users\n-        #\n-        # The previous way Tester was imported also had a side effect of adding\n-        # the full `numpy.testing` namespace\n         if attr == 'testing':\n             import numpy.testing as testing\n             return testing\n         elif attr == 'Tester':\n-            from .testing import Tester\n-            return Tester\n+            \"Removed in NumPy 1.25.0\"\n+            raise RuntimeError(\"Tester was removed in NumPy 1.25.\")\n \n         raise AttributeError(\"module {!r} has no attribute \"\n                              \"{!r}\".format(__name__, attr))\n \n     def __dir__():\n-        public_symbols = globals().keys() | {'Tester', 'testing'}\n+        public_symbols = globals().keys() | {'testing'}\n         public_symbols -= {\n             \"core\", \"matrixlib\",\n             # These were moved in 1.25 and may be deprecated eventually:\n",
            "comment_added_diff": {},
            "comment_deleted_diff": {
                "310": "        # Importing Tester requires importing all of UnitTest which is not a",
                "311": "        # cheap import Since it is mainly used in test suits, we lazy import it",
                "312": "        # here to save on the order of 10 ms of import time for most users",
                "313": "        #",
                "314": "        # The previous way Tester was imported also had a side effect of adding",
                "315": "        # the full `numpy.testing` namespace"
            },
            "comment_modified_diff": {}
        },
        {
            "commit": "96389f69e298729a33c2c6ad6bf994d338638d60",
            "timestamp": "2023-02-02T01:54:27+01:00",
            "author": "Sebastian Berg",
            "commit_message": "ENH: Allow trivial pickling of user DType (classes)\n\nThis also adds a `_legac` attribute, I am happy to rename it, but\nI suspect having something like it is useful.\n\nArguably, the best solution would be to give our DTypes a working\nmodule+name, but given that we are not there, this seems easy.\nWe could probably check for `__reduce__`, but I am not certain that\nwouldn't pre-empt successfully already, so this just restores the\ndefault for now.",
            "additions": 4,
            "deletions": 2,
            "change_type": "MODIFY",
            "diff": "@@ -145,8 +145,10 @@ def _DType_reconstruct(scalar_type):\n def _DType_reduce(DType):\n     # To pickle a DType without having to add top-level names, pickle the\n     # scalar type for now (and assume that reconstruction will be possible).\n-    if DType is dtype:\n-        return \"dtype\"  # must pickle `np.dtype` as a singleton.\n+    if not DType._legacy:\n+        # If we don't have a legacy DType, we should have a valid top level\n+        # name available, so use it (i.e. `np.dtype` itself!)\n+        return DType.__name__\n     scalar_type = DType.type  # pickle the scalar type for reconstruction\n     return _DType_reconstruct, (scalar_type,)\n \n",
            "comment_added_diff": {
                "149": "        # If we don't have a legacy DType, we should have a valid top level",
                "150": "        # name available, so use it (i.e. `np.dtype` itself!)"
            },
            "comment_deleted_diff": {
                "149": "        return \"dtype\"  # must pickle `np.dtype` as a singleton."
            },
            "comment_modified_diff": {
                "149": "        return \"dtype\"  # must pickle `np.dtype` as a singleton."
            }
        },
        {
            "commit": "624b18090ae567f3cfd528a8ae156b2ae7db6d82",
            "timestamp": "2023-02-10T14:49:16+01:00",
            "author": "Sebastian Berg",
            "commit_message": "API: Add environment variable for behavior planned in a 2.0\n\nThe idea of the flag is not to allow to change it right now, since\nthere may be some things where that is hard to do in general, and\nit doesn't seem relevant:  nobody is supposed to use it besides for\ntesting.",
            "additions": 8,
            "deletions": 2,
            "change_type": "MODIFY",
            "diff": "@@ -122,6 +122,10 @@\n if __NUMPY_SETUP__:\n     sys.stderr.write('Running from numpy source directory.\\n')\n else:\n+    # Make variable available during multiarray/C initialization\n+    import os\n+    _numpy2_behavior = os.environ.get(\"NPY_NUMPY_2_BEHAVIOR\", \"0\") != \"0\"\n+\n     try:\n         from numpy.__config__ import show as show_config\n     except ImportError as e:\n@@ -392,7 +396,6 @@ def _mac_os_check():\n     # is slow and thus better avoided.\n     # Specifically kernel version 4.6 had a bug fix which probably fixed this:\n     # https://github.com/torvalds/linux/commit/7cf91a98e607c2f935dbcc177d70011e95b8faff\n-    import os\n     use_hugepage = os.environ.get(\"NUMPY_MADVISE_HUGEPAGE\", None)\n     if sys.platform == \"linux\" and use_hugepage is None:\n         # If there is an issue with parsing the kernel version,\n@@ -415,13 +418,16 @@ def _mac_os_check():\n \n     # Note that this will currently only make a difference on Linux\n     core.multiarray._set_madvise_hugepage(use_hugepage)\n+    del use_hugepage\n \n     # Give a warning if NumPy is reloaded or imported on a sub-interpreter\n     # We do this from python, since the C-module may not be reloaded and\n     # it is tidier organized.\n     core.multiarray._multiarray_umath._reload_guard()\n \n-    core._set_promotion_state(os.environ.get(\"NPY_PROMOTION_STATE\", \"legacy\"))\n+    # default to \"weak\" promotion for \"NumPy 2\".\n+    core._set_promotion_state(os.environ.get(\n+            \"NPY_PROMOTION_STATE\", \"weak\" if _numpy2_behavior else \"legacy\"))\n \n     # Tell PyInstaller where to find hook-numpy.py\n     def _pyinstaller_hooks_dir():\n",
            "comment_added_diff": {
                "125": "    # Make variable available during multiarray/C initialization",
                "428": "    # default to \"weak\" promotion for \"NumPy 2\"."
            },
            "comment_deleted_diff": {},
            "comment_modified_diff": {}
        },
        {
            "commit": "9d5eafe596e75e30a85c01ed62bb5bea9389adc8",
            "timestamp": "2023-02-10T15:51:57+01:00",
            "author": "Sebastian Berg",
            "commit_message": "MAINT: Use `np._using_numpy2_behavior()` and initialize it in C",
            "additions": 4,
            "deletions": 6,
            "change_type": "MODIFY",
            "diff": "@@ -122,10 +122,6 @@\n if __NUMPY_SETUP__:\n     sys.stderr.write('Running from numpy source directory.\\n')\n else:\n-    # Make variable available during multiarray/C initialization\n-    import os\n-    _numpy2_behavior = os.environ.get(\"NPY_NUMPY_2_BEHAVIOR\", \"0\") != \"0\"\n-\n     try:\n         from numpy.__config__ import show as show_config\n     except ImportError as e:\n@@ -396,6 +392,7 @@ def _mac_os_check():\n     # is slow and thus better avoided.\n     # Specifically kernel version 4.6 had a bug fix which probably fixed this:\n     # https://github.com/torvalds/linux/commit/7cf91a98e607c2f935dbcc177d70011e95b8faff\n+    import os\n     use_hugepage = os.environ.get(\"NUMPY_MADVISE_HUGEPAGE\", None)\n     if sys.platform == \"linux\" and use_hugepage is None:\n         # If there is an issue with parsing the kernel version,\n@@ -426,8 +423,9 @@ def _mac_os_check():\n     core.multiarray._multiarray_umath._reload_guard()\n \n     # default to \"weak\" promotion for \"NumPy 2\".\n-    core._set_promotion_state(os.environ.get(\n-            \"NPY_PROMOTION_STATE\", \"weak\" if _numpy2_behavior else \"legacy\"))\n+    core._set_promotion_state(\n+        os.environ.get(\"NPY_PROMOTION_STATE\",\n+                       \"weak\" if _using_numpy2_behavior() else \"legacy\"))\n \n     # Tell PyInstaller where to find hook-numpy.py\n     def _pyinstaller_hooks_dir():\n",
            "comment_added_diff": {},
            "comment_deleted_diff": {
                "125": "    # Make variable available during multiarray/C initialization"
            },
            "comment_modified_diff": {}
        },
        {
            "commit": "e1e487acf1d820cbab8a6f97986bf2fb451dfa8e",
            "timestamp": "2023-02-11T22:46:28+01:00",
            "author": "Dimitri Papadopoulos",
            "commit_message": "Fix typos found by copdespell",
            "additions": 1,
            "deletions": 1,
            "change_type": "MODIFY",
            "diff": "@@ -197,7 +197,7 @@\n         \"`np.{n}` is a deprecated alias for `{an}`.  (Deprecated NumPy 1.24)\")\n \n     # Some of these are awkward (since `np.str` may be preferable in the long\n-    # term), but overall the names ending in 0 seem undesireable\n+    # term), but overall the names ending in 0 seem undesirable\n     _type_info = [\n         (\"bool8\", bool_, \"np.bool_\"),\n         (\"int0\", intp, \"np.intp\"),\n",
            "comment_added_diff": {
                "200": "    # term), but overall the names ending in 0 seem undesirable"
            },
            "comment_deleted_diff": {
                "200": "    # term), but overall the names ending in 0 seem undesireable"
            },
            "comment_modified_diff": {
                "200": "    # term), but overall the names ending in 0 seem undesireable"
            }
        },
        {
            "commit": "40eaa55f6439772e25d2d73d6edf96b149694987",
            "timestamp": "2023-02-20T21:07:30+00:00",
            "author": "Matti Picus",
            "commit_message": "BUILD: fixes for MSVC",
            "additions": 3,
            "deletions": 3,
            "change_type": "MODIFY",
            "diff": "@@ -112,6 +112,9 @@\n     ComplexWarning, ModuleDeprecationWarning, VisibleDeprecationWarning,\n     TooHardError, AxisError)\n \n+# Allow distributors to run custom init code before importing numpy.core\n+from . import _distributor_init\n+\n # We first need to detect if we're being called as part of the numpy setup\n # procedure itself in a reliable manner.\n try:\n@@ -137,9 +140,6 @@\n     # mapping of {name: (value, deprecation_msg)}\n     __deprecated_attrs__ = {}\n \n-    # Allow distributors to run custom init code\n-    from . import _distributor_init\n-\n     from . import core\n     from .core import *\n     from . import compat\n",
            "comment_added_diff": {
                "115": "# Allow distributors to run custom init code before importing numpy.core"
            },
            "comment_deleted_diff": {
                "140": "    # Allow distributors to run custom init code"
            },
            "comment_modified_diff": {}
        },
        {
            "commit": "dacd6dcb3c3a9832ae9b3189439bd2df840994e5",
            "timestamp": "2023-02-21T11:08:36+02:00",
            "author": "mattip",
            "commit_message": "BLD: fix review comments",
            "additions": 3,
            "deletions": 3,
            "change_type": "MODIFY",
            "diff": "@@ -112,9 +112,6 @@\n     ComplexWarning, ModuleDeprecationWarning, VisibleDeprecationWarning,\n     TooHardError, AxisError)\n \n-# Allow distributors to run custom init code before importing numpy.core\n-from . import _distributor_init\n-\n # We first need to detect if we're being called as part of the numpy setup\n # procedure itself in a reliable manner.\n try:\n@@ -125,6 +122,9 @@\n if __NUMPY_SETUP__:\n     sys.stderr.write('Running from numpy source directory.\\n')\n else:\n+    # Allow distributors to run custom init code before importing numpy.core\n+    from . import _distributor_init\n+\n     try:\n         from numpy.__config__ import show as show_config\n     except ImportError as e:\n",
            "comment_added_diff": {
                "125": "    # Allow distributors to run custom init code before importing numpy.core"
            },
            "comment_deleted_diff": {
                "115": "# Allow distributors to run custom init code before importing numpy.core"
            },
            "comment_modified_diff": {}
        },
        {
            "commit": "b50568d9e758b489c2a3c409ef4e57b67820f090",
            "timestamp": "2023-03-30T22:38:54-07:00",
            "author": "Pratyay Banerjee",
            "commit_message": "DOC: Fix typos & grammer in docstrings and comments (#23503)",
            "additions": 1,
            "deletions": 1,
            "change_type": "MODIFY",
            "diff": "@@ -26,7 +26,7 @@ def dirty_lock(lock_name, lock_on_count=1):\n     lock_path = os.path.abspath(os.path.join(\n         os.path.dirname(__file__), \"..\", \"env\", lock_name)\n     )\n-    # ASV load the 'benchmark_dir' to discovering the available benchmarks\n+    # ASV loads the 'benchmark_dir' to discover the available benchmarks\n     # the issue here is ASV doesn't capture any strings from stdout or stderr\n     # during this stage so we escape it and lock on the second increment\n     try:\n",
            "comment_added_diff": {
                "29": "    # ASV loads the 'benchmark_dir' to discover the available benchmarks"
            },
            "comment_deleted_diff": {
                "29": "    # ASV load the 'benchmark_dir' to discovering the available benchmarks"
            },
            "comment_modified_diff": {
                "29": "    # ASV load the 'benchmark_dir' to discovering the available benchmarks"
            }
        },
        {
            "commit": "79a292bb9ba6cc0a5de77c84e961f87194f17fcb",
            "timestamp": "2023-04-07T13:29:45-06:00",
            "author": "Nathan Goldbaum",
            "commit_message": "DEP: deprecate np.math and np.lib.math",
            "additions": 17,
            "deletions": 2,
            "change_type": "MODIFY",
            "diff": "@@ -11,7 +11,6 @@\n useful to have in the main name-space.\n \n \"\"\"\n-import math\n \n from numpy.version import version as __version__\n \n@@ -58,7 +57,7 @@\n from ._version import *\n from numpy.core._multiarray_umath import tracemalloc_domain\n \n-__all__ = ['emath', 'math', 'tracemalloc_domain', 'Arrayterator']\n+__all__ = ['emath', 'tracemalloc_domain', 'Arrayterator']\n __all__ += type_check.__all__\n __all__ += index_tricks.__all__\n __all__ += function_base.__all__\n@@ -77,3 +76,19 @@\n from numpy._pytesttester import PytestTester\n test = PytestTester(__name__)\n del PytestTester\n+\n+def __getattr__(attr):\n+    # Warn for reprecated attributes\n+    import math\n+    import warnings\n+\n+    if attr == 'math':\n+        warnings.warn(\n+            \"`np.lib.math` is a deprecated alias for the standard library \"\n+            \"`math` module (Deprecated Numpy 1.25). Replace usages of \"\n+            \"`numpy.lib.math` with `math`\", DeprecationWarning, stacklevel=2)\n+        return math\n+    else:\n+        raise AttributeError(\"module {!r} has no attribute \"\n+                             \"{!r}\".format(__name__, attr))\n+        \n",
            "comment_added_diff": {
                "81": "    # Warn for reprecated attributes"
            },
            "comment_deleted_diff": {},
            "comment_modified_diff": {}
        },
        {
            "commit": "03e5cf0b5697957c3f8345ed09a3662494633e7e",
            "timestamp": "2023-04-12T12:21:41+02:00",
            "author": "Sebastian Berg",
            "commit_message": "API: Add `numpy.types` module and fill it with DType classes",
            "additions": 7,
            "deletions": 6,
            "change_type": "MODIFY",
            "diff": "@@ -142,13 +142,14 @@ def _DType_reconstruct(scalar_type):\n \n \n def _DType_reduce(DType):\n-    # To pickle a DType without having to add top-level names, pickle the\n-    # scalar type for now (and assume that reconstruction will be possible).\n-    if not DType._legacy:\n-        # If we don't have a legacy DType, we should have a valid top level\n-        # name available, so use it (i.e. `np.dtype` itself!)\n+    # As types/classes, most DTypes can simply be pickled by their name:\n+    if not DType._legacy or DType.__module__ == \"numpy.types\":\n         return DType.__name__\n-    scalar_type = DType.type  # pickle the scalar type for reconstruction\n+\n+    # However, user defined legacy dtypes (like rational) do not end up in\n+    # `numpy.types` as module and do not have a public class at all.\n+    # For these, we pickle them by reconstructing them from the scalar type:\n+    scalar_type = DType.type\n     return _DType_reconstruct, (scalar_type,)\n \n \n",
            "comment_added_diff": {
                "145": "    # As types/classes, most DTypes can simply be pickled by their name:",
                "149": "    # However, user defined legacy dtypes (like rational) do not end up in",
                "150": "    # `numpy.types` as module and do not have a public class at all.",
                "151": "    # For these, we pickle them by reconstructing them from the scalar type:"
            },
            "comment_deleted_diff": {
                "145": "    # To pickle a DType without having to add top-level names, pickle the",
                "146": "    # scalar type for now (and assume that reconstruction will be possible).",
                "148": "        # If we don't have a legacy DType, we should have a valid top level",
                "149": "        # name available, so use it (i.e. `np.dtype` itself!)",
                "151": "    scalar_type = DType.type  # pickle the scalar type for reconstruction"
            },
            "comment_modified_diff": {
                "145": "    # To pickle a DType without having to add top-level names, pickle the",
                "149": "        # name available, so use it (i.e. `np.dtype` itself!)",
                "151": "    scalar_type = DType.type  # pickle the scalar type for reconstruction"
            }
        },
        {
            "commit": "569f7bc692aab41014ddc683c06e84e1bc276305",
            "timestamp": "2023-04-12T12:26:48+02:00",
            "author": "Sebastian Berg",
            "commit_message": "MAINT: Move module to be `np.dtypes` and add release note",
            "additions": 2,
            "deletions": 2,
            "change_type": "MODIFY",
            "diff": "@@ -143,11 +143,11 @@ def _DType_reconstruct(scalar_type):\n \n def _DType_reduce(DType):\n     # As types/classes, most DTypes can simply be pickled by their name:\n-    if not DType._legacy or DType.__module__ == \"numpy.types\":\n+    if not DType._legacy or DType.__module__ == \"numpy.dtypes\":\n         return DType.__name__\n \n     # However, user defined legacy dtypes (like rational) do not end up in\n-    # `numpy.types` as module and do not have a public class at all.\n+    # `numpy.dtypes` as module and do not have a public class at all.\n     # For these, we pickle them by reconstructing them from the scalar type:\n     scalar_type = DType.type\n     return _DType_reconstruct, (scalar_type,)\n",
            "comment_added_diff": {
                "150": "    # `numpy.dtypes` as module and do not have a public class at all."
            },
            "comment_deleted_diff": {
                "150": "    # `numpy.types` as module and do not have a public class at all."
            },
            "comment_modified_diff": {
                "150": "    # `numpy.types` as module and do not have a public class at all."
            }
        },
        {
            "commit": "494197741cda94017a710192c54f8fab97e0cbe2",
            "timestamp": "2023-06-01T11:23:43+02:00",
            "author": "Sebastian Berg",
            "commit_message": "MAINT: Remove all \"NumPy 2\" as that should be main now\n\nThis effectively reverts most of gh-23089, the array function disabling\nhas been removed in the meantime, so the test is now not needed anymore\n(the test set it to 0 before).\n\nI am sad that there weren't actually any changes, but it doesn't really\nmatter.",
            "additions": 2,
            "deletions": 3,
            "change_type": "MODIFY",
            "diff": "@@ -427,10 +427,9 @@ def _mac_os_check():\n     # it is tidier organized.\n     core.multiarray._multiarray_umath._reload_guard()\n \n-    # default to \"weak\" promotion for \"NumPy 2\".\n+    # TODO: Switch to defaulting to \"weak\".\n     core._set_promotion_state(\n-        os.environ.get(\"NPY_PROMOTION_STATE\",\n-                       \"weak\" if _using_numpy2_behavior() else \"legacy\"))\n+        os.environ.get(\"NPY_PROMOTION_STATE\", \"legacy\"))\n \n     # Tell PyInstaller where to find hook-numpy.py\n     def _pyinstaller_hooks_dir():\n",
            "comment_added_diff": {
                "430": "    # TODO: Switch to defaulting to \"weak\"."
            },
            "comment_deleted_diff": {
                "430": "    # default to \"weak\" promotion for \"NumPy 2\"."
            },
            "comment_modified_diff": {
                "430": "    # default to \"weak\" promotion for \"NumPy 2\"."
            }
        },
        {
            "commit": "c8e2343d22bc886dd08775589bc2405016349f37",
            "timestamp": "2023-08-07T22:02:46+02:00",
            "author": "Mateusz Sok\u00f3\u0142",
            "commit_message": "API: Cleaning `numpy/__init__.py` and main namespace - Part 1 [NEP 52] (#24316)",
            "additions": 41,
            "deletions": 137,
            "change_type": "MODIFY",
            "diff": "@@ -43,13 +43,6 @@\n   >>> np.lookfor('keyword')\n   ... # doctest: +SKIP\n \n-General-purpose documents like a glossary and help on the basic concepts\n-of numpy are available under the ``doc`` sub-module::\n-\n-  >>> from numpy import doc\n-  >>> help(doc)\n-  ... # doctest: +SKIP\n-\n Available subpackages\n ---------------------\n lib\n@@ -74,8 +67,6 @@\n     Run numpy unittests\n show_config\n     Show numpy build configuration\n-matlib\n-    Make everything matrices.\n __version__\n     NumPy version string\n \n@@ -99,14 +90,11 @@\n Exceptions to this rule are documented.\n \n \"\"\"\n+import os\n import sys\n import warnings\n \n from ._globals import _NoValue, _CopyMode\n-# These exceptions were moved in 1.25 and are hidden from __dir__()\n-from .exceptions import (\n-    ComplexWarning, ModuleDeprecationWarning, VisibleDeprecationWarning,\n-    TooHardError, AxisError)\n \n # We first need to detect if we're being called as part of the numpy setup\n # procedure itself in a reliable manner.\n@@ -128,14 +116,8 @@\n         its source directory; please exit the numpy source tree, and relaunch\n         your python interpreter from there.\"\"\"\n         raise ImportError(msg) from e\n-\n-    __all__ = [\n-        'exceptions', 'ModuleDeprecationWarning', 'VisibleDeprecationWarning',\n-        'ComplexWarning', 'TooHardError', 'AxisError']\n-\n-    # mapping of {name: (value, deprecation_msg)}\n-    __deprecated_attrs__ = {}\n-    __expired_functions__ = {}\n+    \n+    __all__ = ['exceptions']\n \n     from . import core\n     from .core import *\n@@ -155,9 +137,7 @@\n     from . import matrixlib as _mat\n     from .matrixlib import *\n \n-    # Deprecations introduced in NumPy 1.20.0, 2020-06-06\n-    import builtins as _builtins\n-\n+    # We build warning messages for former attributes\n     _msg = (\n         \"module 'numpy' has no attribute '{n}'.\\n\"\n         \"`np.{n}` was a deprecated alias for the builtin `{n}`. \"\n@@ -189,22 +169,6 @@\n          for n, extended_msg in _type_info\n      }\n \n-    # Future warning introduced in NumPy 1.24.0, 2022-11-17\n-    _msg = (\n-        \"`np.{n}` is a deprecated alias for `{an}`.  (Deprecated NumPy 1.24)\")\n-\n-    # Some of these are awkward (since `np.str` may be preferable in the long\n-    # term), but overall the names ending in 0 seem undesirable\n-    _type_info = [\n-        (\"bool8\", bool_, \"np.bool_\"),\n-        (\"int0\", intp, \"np.intp\"),\n-        (\"uint0\", uintp, \"np.uintp\"),\n-        (\"str0\", str_, \"np.str_\"),\n-        (\"bytes0\", bytes_, \"np.bytes_\"),\n-        (\"void0\", void, \"np.void\"),\n-        (\"object0\", object_,\n-            \"`np.object0` is a deprecated alias for `np.object_`. \"\n-            \"`object` can be used instead.  (Deprecated NumPy 1.24)\")]\n \n     # Some of these could be defined right away, but most were aliases to\n     # the Python objects and only removed in NumPy 1.24.  Defining them should\n@@ -213,20 +177,8 @@\n     # import with `from numpy import *`.\n     __future_scalars__ = {\"bool\", \"long\", \"ulong\", \"str\", \"bytes\", \"object\"}\n \n-    __deprecated_attrs__.update({\n-        n: (alias, _msg.format(n=n, an=an)) for n, alias, an in _type_info})\n-\n-    import math\n-\n-    __deprecated_attrs__['math'] = (math,\n-        \"`np.math` is a deprecated alias for the standard library `math` \"\n-        \"module (Deprecated Numpy 1.25). Replace usages of `np.math` with \"\n-        \"`math`\")\n-\n-    del math, _msg, _type_info\n-\n     from .core import abs\n-    # now that numpy modules are imported, can initialize limits\n+    # now that numpy core module is imported, can initialize limits\n     core.getlimits._register_known_types()\n \n     __all__.extend(['__version__', 'show_config'])\n@@ -247,61 +199,22 @@\n     __all__.remove('unicode')\n \n     # Remove things that are in the numpy.lib but not in the numpy namespace\n-    # Note that there is a test (numpy/tests/test_public_api.py:test_numpy_namespace)\n+    # Note that there is a test under path: \n+    # `numpy/tests/test_public_api.py:test_numpy_namespace`\n     # that prevents adding more things to the main namespace by accident.\n     # The list below will grow until the `from .lib import *` fixme above is\n     # taken care of\n     __all__.remove('Arrayterator')\n     del Arrayterator\n \n-    # These names were removed in NumPy 1.20.  For at least one release,\n-    # attempts to access these names in the numpy namespace will trigger\n-    # a warning, and calling the function will raise an exception.\n-    _financial_names = ['fv', 'ipmt', 'irr', 'mirr', 'nper', 'npv', 'pmt',\n-                        'ppmt', 'pv', 'rate']\n-    for name in _financial_names:\n-        __expired_functions__[name] = (\n-            f'In accordance with NEP 32, the function {name} was removed '\n-            'from NumPy version 1.20.  A replacement for this function '\n-            'is available in the numpy_financial library: '\n-            'https://pypi.org/project/numpy-financial'\n-        )\n-\n     # Filter out Cython harmless warnings\n     warnings.filterwarnings(\"ignore\", message=\"numpy.dtype size changed\")\n     warnings.filterwarnings(\"ignore\", message=\"numpy.ufunc size changed\")\n     warnings.filterwarnings(\"ignore\", message=\"numpy.ndarray size changed\")\n \n-    # oldnumeric and numarray were removed in 1.9. In case some packages import\n-    # but do not use them, we define them here for backward compatibility.\n-    oldnumeric = 'removed'\n-    numarray = 'removed'\n-\n     def __getattr__(attr):\n-        # Warn for expired attributes, and return a dummy function\n-        # that always raises an exception.\n+        # Warn for expired attributes\n         import warnings\n-        import math\n-        try:\n-            msg = __expired_functions__[attr]\n-        except KeyError:\n-            pass\n-        else:\n-            warnings.warn(msg, DeprecationWarning, stacklevel=2)\n-\n-            def _expired(*args, **kwds):\n-                raise RuntimeError(msg)\n-\n-            return _expired\n-\n-        # Emit warnings for deprecated attributes\n-        try:\n-            val, msg = __deprecated_attrs__[attr]\n-        except KeyError:\n-            pass\n-        else:\n-            warnings.warn(msg, DeprecationWarning, stacklevel=2)\n-            return val\n \n         if attr in __future_scalars__:\n             # And future warnings for those that will change, but also give\n@@ -316,24 +229,13 @@ def _expired(*args, **kwds):\n         if attr == 'testing':\n             import numpy.testing as testing\n             return testing\n-        elif attr == 'Tester':\n-            \"Removed in NumPy 1.25.0\"\n-            raise RuntimeError(\"Tester was removed in NumPy 1.25.\")\n-        elif attr == \"compat\":\n-            import numpy.compat as compat\n-            return compat\n \n         raise AttributeError(\"module {!r} has no attribute \"\n                              \"{!r}\".format(__name__, attr))\n \n     def __dir__():\n         public_symbols = globals().keys() | {'testing'}\n-        public_symbols -= {\n-            \"core\", \"matrixlib\",\n-            # These were moved in 1.25 and may be deprecated eventually:\n-            \"ModuleDeprecationWarning\", \"VisibleDeprecationWarning\",\n-            \"ComplexWarning\", \"TooHardError\", \"AxisError\"\n-        }\n+        public_symbols -= {\"core\", \"matrixlib\"}\n         return list(public_symbols)\n \n     # Pytest testing\n@@ -384,7 +286,6 @@ def _mac_os_check():\n         with warnings.catch_warnings(record=True) as w:\n             _mac_os_check()\n             # Throw runtime error, if the test failed Check for warning and error_message\n-            error_message = \"\"\n             if len(w) > 0:\n                 error_message = \"{}: {}\".format(w[-1].category.__name__, str(w[-1].message))\n                 msg = (\n@@ -395,36 +296,42 @@ def _mac_os_check():\n                     \"\\nOtherwise report this to the vendor \"\n                     \"that provided NumPy.\\n{}\\n\".format(error_message))\n                 raise RuntimeError(msg)\n+        del w\n     del _mac_os_check\n \n-    # We usually use madvise hugepages support, but on some old kernels it\n-    # is slow and thus better avoided.\n-    # Specifically kernel version 4.6 had a bug fix which probably fixed this:\n-    # https://github.com/torvalds/linux/commit/7cf91a98e607c2f935dbcc177d70011e95b8faff\n-    import os\n-    use_hugepage = os.environ.get(\"NUMPY_MADVISE_HUGEPAGE\", None)\n-    if sys.platform == \"linux\" and use_hugepage is None:\n-        # If there is an issue with parsing the kernel version,\n-        # set use_hugepages to 0. Usage of LooseVersion will handle\n-        # the kernel version parsing better, but avoided since it\n-        # will increase the import time. See: #16679 for related discussion.\n-        try:\n-            use_hugepage = 1\n-            kernel_version = os.uname().release.split(\".\")[:2]\n-            kernel_version = tuple(int(v) for v in kernel_version)\n-            if kernel_version < (4, 6):\n+    def hugepage_setup():\n+        \"\"\"\n+        We usually use madvise hugepages support, but on some old kernels it\n+        is slow and thus better avoided. Specifically kernel version 4.6 \n+        had a bug fix which probably fixed this:\n+        https://github.com/torvalds/linux/commit/7cf91a98e607c2f935dbcc177d70011e95b8faff\n+        \"\"\"\n+        use_hugepage = os.environ.get(\"NUMPY_MADVISE_HUGEPAGE\", None)\n+        if sys.platform == \"linux\" and use_hugepage is None:\n+            # If there is an issue with parsing the kernel version,\n+            # set use_hugepage to 0. Usage of LooseVersion will handle\n+            # the kernel version parsing better, but avoided since it\n+            # will increase the import time. \n+            # See: #16679 for related discussion.\n+            try:\n+                use_hugepage = 1\n+                kernel_version = os.uname().release.split(\".\")[:2]\n+                kernel_version = tuple(int(v) for v in kernel_version)\n+                if kernel_version < (4, 6):\n+                    use_hugepage = 0\n+            except ValueError:\n                 use_hugepage = 0\n-        except ValueError:\n-            use_hugepages = 0\n-    elif use_hugepage is None:\n-        # This is not Linux, so it should not matter, just enable anyway\n-        use_hugepage = 1\n-    else:\n-        use_hugepage = int(use_hugepage)\n+            finally:\n+                del kernel_version\n+        elif use_hugepage is None:\n+            # This is not Linux, so it should not matter, just enable anyway\n+            use_hugepage = 1\n+        else:\n+            use_hugepage = int(use_hugepage)\n \n     # Note that this will currently only make a difference on Linux\n-    core.multiarray._set_madvise_hugepage(use_hugepage)\n-    del use_hugepage\n+    core.multiarray._set_madvise_hugepage(hugepage_setup())\n+    del hugepage_setup\n \n     # Give a warning if NumPy is reloaded or imported on a sub-interpreter\n     # We do this from python, since the C-module may not be reloaded and\n@@ -440,12 +347,9 @@ def _pyinstaller_hooks_dir():\n         from pathlib import Path\n         return [str(Path(__file__).with_name(\"_pyinstaller\").resolve())]\n \n-    # Remove symbols imported for internal use\n-    del os\n-\n \n # get the version using versioneer\n from .version import __version__, git_revision as __git_version__\n \n # Remove symbols imported for internal use\n-del sys, warnings\n+del os, sys, warnings\n",
            "comment_added_diff": {
                "140": "    # We build warning messages for former attributes",
                "181": "    # now that numpy core module is imported, can initialize limits",
                "202": "    # Note that there is a test under path:",
                "203": "    # `numpy/tests/test_public_api.py:test_numpy_namespace`",
                "216": "        # Warn for expired attributes",
                "311": "            # If there is an issue with parsing the kernel version,",
                "312": "            # set use_hugepage to 0. Usage of LooseVersion will handle",
                "313": "            # the kernel version parsing better, but avoided since it",
                "314": "            # will increase the import time.",
                "315": "            # See: #16679 for related discussion.",
                "327": "            # This is not Linux, so it should not matter, just enable anyway"
            },
            "comment_deleted_diff": {
                "51": "  ... # doctest: +SKIP",
                "106": "# These exceptions were moved in 1.25 and are hidden from __dir__()",
                "136": "    # mapping of {name: (value, deprecation_msg)}",
                "158": "    # Deprecations introduced in NumPy 1.20.0, 2020-06-06",
                "192": "    # Future warning introduced in NumPy 1.24.0, 2022-11-17",
                "196": "    # Some of these are awkward (since `np.str` may be preferable in the long",
                "197": "    # term), but overall the names ending in 0 seem undesirable",
                "229": "    # now that numpy modules are imported, can initialize limits",
                "250": "    # Note that there is a test (numpy/tests/test_public_api.py:test_numpy_namespace)",
                "257": "    # These names were removed in NumPy 1.20.  For at least one release,",
                "258": "    # attempts to access these names in the numpy namespace will trigger",
                "259": "    # a warning, and calling the function will raise an exception.",
                "275": "    # oldnumeric and numarray were removed in 1.9. In case some packages import",
                "276": "    # but do not use them, we define them here for backward compatibility.",
                "281": "        # Warn for expired attributes, and return a dummy function",
                "282": "        # that always raises an exception.",
                "297": "        # Emit warnings for deprecated attributes",
                "333": "            # These were moved in 1.25 and may be deprecated eventually:",
                "400": "    # We usually use madvise hugepages support, but on some old kernels it",
                "401": "    # is slow and thus better avoided.",
                "402": "    # Specifically kernel version 4.6 had a bug fix which probably fixed this:",
                "403": "    # https://github.com/torvalds/linux/commit/7cf91a98e607c2f935dbcc177d70011e95b8faff",
                "407": "        # If there is an issue with parsing the kernel version,",
                "408": "        # set use_hugepages to 0. Usage of LooseVersion will handle",
                "409": "        # the kernel version parsing better, but avoided since it",
                "410": "        # will increase the import time. See: #16679 for related discussion.",
                "420": "        # This is not Linux, so it should not matter, just enable anyway",
                "443": "    # Remove symbols imported for internal use"
            },
            "comment_modified_diff": {
                "202": "        (\"str0\", str_, \"np.str_\"),",
                "203": "        (\"bytes0\", bytes_, \"np.bytes_\"),",
                "216": "    __deprecated_attrs__.update({"
            }
        },
        {
            "commit": "4d05cd1ef67f6c9b4f0bb63d69d388f7651c62bf",
            "timestamp": "2023-08-10T12:43:44+02:00",
            "author": "Ralf Gommers",
            "commit_message": "BLD: build with our numpy fork of meson",
            "additions": 27,
            "deletions": 0,
            "change_type": "ADD",
            "diff": "@@ -0,0 +1,27 @@\n+import os\n+import sys\n+import pathlib\n+\n+from mesonpy import (\n+    build_sdist,\n+    build_wheel,\n+    build_editable,\n+    get_requires_for_build_sdist,\n+    get_requires_for_build_wheel,\n+    get_requires_for_build_editable,\n+)\n+\n+\n+# The numpy-vendored version of Meson. Put the directory that the executable\n+# `meson` is in at the front of the PATH.\n+curdir = pathlib.Path(__file__).parent.resolve()\n+meson_executable_dir = str(curdir.parent.parent / 'entrypoint')\n+os.environ['PATH'] = meson_executable_dir + os.pathsep + os.environ['PATH']\n+\n+# Check that the meson git submodule is present\n+meson_import_dir = curdir.parent.parent / 'meson' / 'mesonbuild'\n+if not meson_import_dir.exists():\n+    raise RuntimeError(\n+        'The `vendored-meson/meson` git submodule does not exist! ' +\n+        'Run `git submodule update --init` to fix this problem.'\n+    )\n",
            "comment_added_diff": {
                "15": "# The numpy-vendored version of Meson. Put the directory that the executable",
                "16": "# `meson` is in at the front of the PATH.",
                "21": "# Check that the meson git submodule is present"
            },
            "comment_deleted_diff": {},
            "comment_modified_diff": {}
        },
        {
            "commit": "be896c9c652c010093d26bd0ff732dbfabcd6361",
            "timestamp": "2023-08-11T22:40:16+02:00",
            "author": "Stefan van der Walt",
            "commit_message": "MAINT: Remove versioneer (#24196)\n\nCo-authored-by: Ralf Gommers <ralf.gommers@gmail.com>",
            "additions": 5,
            "deletions": 3,
            "change_type": "MODIFY",
            "diff": "@@ -96,6 +96,11 @@\n \n from ._globals import _NoValue, _CopyMode\n \n+\n+# If a version with git hash was stored, use that instead\n+from . import version\n+from .version import __version__\n+\n # We first need to detect if we're being called as part of the numpy setup\n # procedure itself in a reliable manner.\n try:\n@@ -348,8 +353,5 @@ def _pyinstaller_hooks_dir():\n         return [str(Path(__file__).with_name(\"_pyinstaller\").resolve())]\n \n \n-# get the version using versioneer\n-from .version import __version__, git_revision as __git_version__\n-\n # Remove symbols imported for internal use\n del os, sys, warnings\n",
            "comment_added_diff": {
                "100": "# If a version with git hash was stored, use that instead"
            },
            "comment_deleted_diff": {
                "351": "# get the version using versioneer"
            },
            "comment_modified_diff": {}
        },
        {
            "commit": "250e1479ce342d9d7ab8a592508f6ce892d4c98b",
            "timestamp": "2023-08-12T22:01:16+02:00",
            "author": "Mateusz Sok\u00f3\u0142",
            "commit_message": "API: Cleaning `numpy/__init__.py` and main namespace - Part 2 [NEP 52] (#24357)\n\n[skip ci]",
            "additions": 160,
            "deletions": 48,
            "change_type": "MODIFY",
            "diff": "@@ -121,26 +121,112 @@\n         its source directory; please exit the numpy source tree, and relaunch\n         your python interpreter from there.\"\"\"\n         raise ImportError(msg) from e\n-    \n-    __all__ = ['exceptions']\n \n     from . import core\n-    from .core import *\n-    from . import exceptions\n-    from . import dtypes\n+    from .core import (\n+        _no_nep50_warning, memmap, Inf, Infinity, NaN, iinfo, finfo,\n+        False_, ScalarType, True_, abs, absolute, add, all, allclose, alltrue,\n+        amax, amin, any, arange, arccos, arccosh, arcsin, arcsinh, arctan,\n+        arctan2, arctanh, argmax, argmin, argpartition, argsort, argwhere,\n+        around, array, array2string, array_equal, array_equiv, array_repr,\n+        array_str, asanyarray, asarray, ascontiguousarray, asfortranarray,\n+        atleast_1d, atleast_2d, atleast_3d, base_repr, binary_repr, \n+        bitwise_and, bitwise_not, bitwise_or, bitwise_xor, block, bool_,\n+        broadcast, busday_count, busday_offset, busdaycalendar, byte, bytes_,\n+        can_cast, cbrt, cdouble, ceil, cfloat, char, character, chararray,\n+        choose, clip, clongdouble, clongfloat, compare_chararrays, complex_,\n+        complexfloating, compress, concatenate, conj, conjugate, convolve,\n+        copysign, copyto, correlate, cos, cosh, count_nonzero, cross, csingle,\n+        cumprod, cumproduct, cumsum, datetime64, datetime_as_string, \n+        datetime_data, deg2rad, degrees, diagonal, divide, divmod, dot, \n+        double, dtype, e, einsum, einsum_path, empty, empty_like, equal,\n+        errstate, euler_gamma, exp, exp2, expm1, fabs, find_common_type, \n+        flatiter, flatnonzero, flexible, \n+        float_, float_power, floating, floor, floor_divide, fmax, fmin, fmod, \n+        format_float_positional, format_float_scientific, format_parser, \n+        frexp, from_dlpack, frombuffer, fromfile, fromfunction, fromiter, \n+        frompyfunc, fromstring, full, full_like, gcd, generic, geomspace, \n+        get_printoptions, getbufsize, geterr, geterrcall, greater, \n+        greater_equal, half, heaviside, hstack, hypot, identity, iinfo, \n+        indices, inexact, inf, infty, inner, int_,\n+        intc, integer, invert, is_busday, isclose, isfinite, isfortran,\n+        isinf, isnan, isnat, isscalar, issctype, issubdtype, lcm, ldexp,\n+        left_shift, less, less_equal, lexsort, linspace, little_endian, log, \n+        log10, log1p, log2, logaddexp, logaddexp2, logical_and, logical_not, \n+        logical_or, logical_xor, logspace, longcomplex, longdouble, \n+        longfloat, longlong, matmul, max, maximum, maximum_sctype, \n+        may_share_memory, mean, min, min_scalar_type, minimum, mod, \n+        modf, moveaxis, multiply, nan, nbytes, ndarray, ndim, nditer, \n+        negative, nested_iters, newaxis, nextafter, nonzero, not_equal,\n+        number, obj2sctype, object_, ones, ones_like, outer, partition,\n+        pi, positive, power, printoptions, prod, product, promote_types, \n+        ptp, put, putmask, rad2deg, radians, ravel, rec, recarray, reciprocal,\n+        record, remainder, repeat, require, reshape, resize, result_type, \n+        right_shift, rint, roll, rollaxis, round, round_, sctype2char, \n+        sctypeDict, sctypes, searchsorted, set_printoptions,\n+        set_string_function, setbufsize, seterr, seterrcall, shape,\n+        shares_memory, short, sign, signbit, signedinteger, sin, single, \n+        singlecomplex, sinh, size, sometrue, sort, spacing, sqrt, square, \n+        squeeze, stack, std, str_, string_, subtract, sum, swapaxes, take,\n+        tan, tanh, tensordot, timedelta64, trace, transpose, \n+        true_divide, trunc, typecodes, ubyte, ufunc, uint, uintc, ulonglong, \n+        unicode_, unsignedinteger, ushort, var, vdot, void, vstack, where, \n+        zeros, zeros_like, _get_promotion_state, _set_promotion_state,\n+        int8, int16, int32, int64, intp, uint8, uint16, uint32, uint64, uintp,\n+        float16, float32, float64, complex64, complex128\n+    )\n+\n+    # NOTE: It's still under discussion whether these aliases \n+    # should be removed.\n+    for ta in [\"float96\", \"float128\", \"complex192\", \"complex256\"]:\n+        try:\n+            globals()[ta] = getattr(core, ta)\n+        except AttributeError:\n+            pass\n+    del ta\n+\n     from . import lib\n-    # NOTE: to be revisited following future namespace cleanup.\n-    # See gh-14454 and gh-15672 for discussion.\n-    from .lib import *\n-\n-    from . import linalg\n-    from . import fft\n-    from . import polynomial\n-    from . import random\n-    from . import ctypeslib\n-    from . import ma\n+    from .lib import (\n+        DataSource, angle, append, apply_along_axis, apply_over_axes,\n+        array_split, asarray_chkfinite, asfarray, average, bartlett,\n+        bincount, blackman, broadcast_arrays, broadcast_shapes,\n+        broadcast_to, byte_bounds, c_, column_stack, common_type,\n+        copy, corrcoef, cov, delete, diag, diag_indices,\n+        diag_indices_from, diagflat, diff, digitize, dsplit, dstack,\n+        ediff1d, emath, expand_dims, extract, eye, fill_diagonal, fix,\n+        flip, fliplr, flipud, fromregex, get_array_wrap, genfromtxt,\n+        get_include, gradient, hamming, hanning, histogram, histogram2d,\n+        histogram_bin_edges, histogramdd, hsplit, i0, imag, in1d,\n+        index_exp, info, insert, interp, intersect1d, iscomplex,\n+        iscomplexobj, isin, isneginf, isreal, isrealobj, issubclass_,\n+        issubsctype, iterable, ix_, kaiser, kron, load, loadtxt, mask_indices,\n+        median, meshgrid, mgrid, mintypecode, msort, nan_to_num, \n+        nanargmax, nanargmin, nancumprod, nancumsum, nanmax, nanmean,\n+        nanmedian, nanmin, nanpercentile, nanprod, nanquantile, nanstd,\n+        nansum, nanvar, ndenumerate, ndindex, ogrid, packbits, pad,\n+        percentile, piecewise, place, poly, poly1d, polyadd, polyder,\n+        polydiv, polyfit, polyint, polymul, polysub, polyval,\n+        put_along_axis, quantile, r_, ravel_multi_index, real, real_if_close,\n+        roots, rot90, row_stack, s_, save, savetxt, savez, savez_compressed,\n+        select, setdiff1d, setxor1d, show_runtime, sinc, sort_complex, split,\n+        take_along_axis, tile, tracemalloc_domain, trapz, tri, tril,\n+        tril_indices, tril_indices_from, typename, union1d, unique, unpackbits,\n+        unravel_index, unwrap, vander, vectorize, vsplit, trim_zeros,\n+        triu, triu_indices, triu_indices_from, isposinf, RankWarning, disp,\n+        deprecate, deprecate_with_doc, who, safe_eval, recfromtxt, recfromcsv\n+    )\n     from . import matrixlib as _mat\n-    from .matrixlib import *\n+    from .matrixlib import (\n+        asmatrix, bmat, mat, matrix\n+    )\n+\n+    # public submodules are imported lazily, \n+    # therefore are accessible from __getattr__\n+    __numpy_submodules__ = {\n+        \"linalg\", \"fft\", \"dtypes\", \"random\", \"polynomial\", \"ma\", \n+        \"exceptions\", \"lib\", \"ctypeslib\", \"testing\", \"typing\",\n+        \"array_api\", \"f2py\", \"distutils\", \"test\"\n+    }\n \n     # We build warning messages for former attributes\n     _msg = (\n@@ -182,35 +268,13 @@\n     # import with `from numpy import *`.\n     __future_scalars__ = {\"bool\", \"long\", \"ulong\", \"str\", \"bytes\", \"object\"}\n \n-    from .core import abs\n     # now that numpy core module is imported, can initialize limits\n     core.getlimits._register_known_types()\n \n-    __all__.extend(['__version__', 'show_config'])\n-    __all__.extend(core.__all__)\n-    __all__.extend(_mat.__all__)\n-    __all__.extend(lib.__all__)\n-    __all__.extend(['linalg', 'fft', 'random', 'ctypeslib', 'ma'])\n-\n-    # Remove one of the two occurrences of `issubdtype`, which is exposed as\n-    # both `numpy.core.issubdtype` and `numpy.lib.issubdtype`.\n-    __all__.remove('issubdtype')\n-\n-    # These are exported by np.core, but are replaced by the builtins below\n-    # remove them to ensure that we don't end up with `np.long == np.int_`,\n-    # which would be a breaking change.\n-    del long, unicode\n-    __all__.remove('long')\n-    __all__.remove('unicode')\n-\n-    # Remove things that are in the numpy.lib but not in the numpy namespace\n-    # Note that there is a test under path: \n-    # `numpy/tests/test_public_api.py:test_numpy_namespace`\n-    # that prevents adding more things to the main namespace by accident.\n-    # The list below will grow until the `from .lib import *` fixme above is\n-    # taken care of\n-    __all__.remove('Arrayterator')\n-    del Arrayterator\n+    __all__ = list(\n+        __numpy_submodules__ | set(core.__all__) | set(lib.__all__) | \n+        set(_mat.__all__) | {\"show_config\", \"__version__\"}\n+    )\n \n     # Filter out Cython harmless warnings\n     warnings.filterwarnings(\"ignore\", message=\"numpy.dtype size changed\")\n@@ -221,6 +285,49 @@ def __getattr__(attr):\n         # Warn for expired attributes\n         import warnings\n \n+        if attr == \"linalg\":\n+            import numpy.linalg as linalg\n+            return linalg\n+        elif attr == \"fft\":\n+            import numpy.fft as fft\n+            return fft\n+        elif attr == \"dtypes\":\n+            import numpy.dtypes as dtypes\n+            return dtypes\n+        elif attr == \"random\":\n+            import numpy.random as random\n+            return random\n+        elif attr == \"polynomial\":\n+            import numpy.polynomial as polynomial\n+            return polynomial\n+        elif attr == \"ma\":\n+            import numpy.ma as ma\n+            return ma\n+        elif attr == \"ctypeslib\":\n+            import numpy.ctypeslib as ctypeslib\n+            return ctypeslib\n+        elif attr == \"exceptions\":\n+            import numpy.exceptions as exceptions\n+            return exceptions\n+        elif attr == \"testing\":\n+            import numpy.testing as testing\n+            return testing\n+        elif attr == \"matlib\":\n+            import numpy.matlib as matlib\n+            return matlib\n+        elif attr == \"f2py\":\n+            import numpy.f2py as f2py\n+            return f2py\n+        elif attr == \"typing\":\n+            import numpy.typing as typing\n+            return typing\n+        elif attr == \"array_api\":\n+            import numpy.array_api as array_api\n+            return array_api\n+        elif attr == \"distutils\":\n+            import numpy.distutils as distutils\n+            return distutils\n+\n         if attr in __future_scalars__:\n             # And future warnings for those that will change, but also give\n             # the AttributeError\n@@ -231,16 +338,21 @@ def __getattr__(attr):\n         if attr in __former_attrs__:\n             raise AttributeError(__former_attrs__[attr])\n \n-        if attr == 'testing':\n-            import numpy.testing as testing\n-            return testing\n-\n         raise AttributeError(\"module {!r} has no attribute \"\n                              \"{!r}\".format(__name__, attr))\n \n     def __dir__():\n-        public_symbols = globals().keys() | {'testing'}\n-        public_symbols -= {\"core\", \"matrixlib\"}\n+        # TODO: move away from using `globals` to a statically defined \n+        # list. With `globals`, when running in a testing context \n+        # a bunch of random names fall into global scope, such as\n+        # `conftest` or `distutils`.\n+        public_symbols = (\n+            globals().keys() | __numpy_submodules__\n+        )\n+        public_symbols -= {\n+            \"core\", \"matrixlib\", \"matlib\", \"tests\", \"conftest\", \"version\", \n+            \"compat\"\n+            }\n         return list(public_symbols)\n \n     # Pytest testing\n",
            "comment_added_diff": {
                "179": "    # NOTE: It's still under discussion whether these aliases",
                "180": "    # should be removed.",
                "223": "    # public submodules are imported lazily,",
                "224": "    # therefore are accessible from __getattr__",
                "345": "        # TODO: move away from using `globals` to a statically defined",
                "346": "        # list. With `globals`, when running in a testing context",
                "347": "        # a bunch of random names fall into global scope, such as",
                "348": "        # `conftest` or `distutils`."
            },
            "comment_deleted_diff": {
                "132": "    # NOTE: to be revisited following future namespace cleanup.",
                "133": "    # See gh-14454 and gh-15672 for discussion.",
                "195": "    # Remove one of the two occurrences of `issubdtype`, which is exposed as",
                "196": "    # both `numpy.core.issubdtype` and `numpy.lib.issubdtype`.",
                "199": "    # These are exported by np.core, but are replaced by the builtins below",
                "200": "    # remove them to ensure that we don't end up with `np.long == np.int_`,",
                "201": "    # which would be a breaking change.",
                "206": "    # Remove things that are in the numpy.lib but not in the numpy namespace",
                "207": "    # Note that there is a test under path:",
                "208": "    # `numpy/tests/test_public_api.py:test_numpy_namespace`",
                "209": "    # that prevents adding more things to the main namespace by accident.",
                "210": "    # The list below will grow until the `from .lib import *` fixme above is",
                "211": "    # taken care of"
            },
            "comment_modified_diff": {}
        },
        {
            "commit": "250e1479ce342d9d7ab8a592508f6ce892d4c98b",
            "timestamp": "2023-08-12T22:01:16+02:00",
            "author": "Mateusz Sok\u00f3\u0142",
            "commit_message": "API: Cleaning `numpy/__init__.py` and main namespace - Part 2 [NEP 52] (#24357)\n\n[skip ci]",
            "additions": 0,
            "deletions": 26,
            "change_type": "DELETE",
            "diff": "@@ -1,26 +0,0 @@\n-import os\n-\n-ref_dir = os.path.join(os.path.dirname(__file__))\n-\n-__all__ = sorted(f[:-3] for f in os.listdir(ref_dir) if f.endswith('.py') and\n-           not f.startswith('__'))\n-\n-for f in __all__:\n-    __import__(__name__ + '.' + f)\n-\n-del f, ref_dir\n-\n-__doc__ = \"\"\"\\\n-Topical documentation\n-=====================\n-\n-The following topics are available:\n-%s\n-\n-You can view them by\n-\n->>> help(np.doc.TOPIC)                                      #doctest: +SKIP\n-\n-\"\"\" % '\\n- '.join([''] + __all__)\n-\n-__all__.extend(['__doc__'])\n",
            "comment_added_diff": {},
            "comment_deleted_diff": {
                "22": ">>> help(np.doc.TOPIC)                                      #doctest: +SKIP"
            },
            "comment_modified_diff": {}
        },
        {
            "commit": "320de475a2829ad2bfeca41adff072c5dcdf8a73",
            "timestamp": "2023-08-16T11:57:00+02:00",
            "author": "Ralf Gommers",
            "commit_message": "MAINT: revert adding `distutils` and `array_api` to `np.__all__`\n\nThis has turned out to be too disruptive; `from numpy import *`\nhas to remain warning-free and these two modules emit a warning\non import rather than when they're actually used.\n\n`array_api` we have to decide on for 2.0 (either remove the experimental\nwarning or remove it), I'll open a separate issue to follow up on that.",
            "additions": 6,
            "deletions": 6,
            "change_type": "MODIFY",
            "diff": "@@ -220,15 +220,15 @@\n         asmatrix, bmat, mat, matrix\n     )\n \n-    # public submodules are imported lazily, \n-    # therefore are accessible from __getattr__\n+    # public submodules are imported lazily, therefore are accessible from\n+    # __getattr__. Note that `distutils` (deprecated) and `array_api`\n+    # (experimental label) are not added here, because `from numpy import *`\n+    # must not raise any warnings - that's too disruptive.\n     __numpy_submodules__ = {\n         \"linalg\", \"fft\", \"dtypes\", \"random\", \"polynomial\", \"ma\", \n         \"exceptions\", \"lib\", \"ctypeslib\", \"testing\", \"typing\",\n-        \"array_api\", \"f2py\", \"test\"\n+        \"f2py\", \"test\"\n     }\n-    if sys.version_info < (3, 12):\n-        __numpy_submodules__.add('distutils')\n \n     # We build warning messages for former attributes\n     _msg = (\n@@ -357,7 +357,7 @@ def __dir__():\n         )\n         public_symbols -= {\n             \"core\", \"matrixlib\", \"matlib\", \"tests\", \"conftest\", \"version\", \n-            \"compat\"\n+            \"compat\", \"distutils\", \"array_api\"\n             }\n         return list(public_symbols)\n \n",
            "comment_added_diff": {
                "223": "    # public submodules are imported lazily, therefore are accessible from",
                "224": "    # __getattr__. Note that `distutils` (deprecated) and `array_api`",
                "225": "    # (experimental label) are not added here, because `from numpy import *`",
                "226": "    # must not raise any warnings - that's too disruptive."
            },
            "comment_deleted_diff": {
                "223": "    # public submodules are imported lazily,",
                "224": "    # therefore are accessible from __getattr__"
            },
            "comment_modified_diff": {
                "223": "    # public submodules are imported lazily,",
                "224": "    # therefore are accessible from __getattr__"
            }
        },
        {
            "commit": "d047b4296b1f158c70d287ecd1c4a49fbd784c7e",
            "timestamp": "2023-08-24T22:50:13+02:00",
            "author": "Mateusz Sok\u00f3\u0142",
            "commit_message": "API: Finalize third batch of changes",
            "additions": 0,
            "deletions": 5,
            "change_type": "MODIFY",
            "diff": "@@ -38,11 +38,6 @@\n The native Python help() does not know how to view their help, but our\n np.info() function does.\n \n-To search for documents containing a keyword, do::\n-\n-  >>> np.lookfor('keyword')\n-  ... # doctest: +SKIP\n-\n Available subpackages\n ---------------------\n lib\n",
            "comment_added_diff": {},
            "comment_deleted_diff": {
                "44": "  ... # doctest: +SKIP"
            },
            "comment_modified_diff": {}
        },
        {
            "commit": "63a1fee8d1c1e442aa8ca08ac7cefb195f99b2a9",
            "timestamp": "2023-09-13T22:52:14+02:00",
            "author": "Ralf Gommers",
            "commit_message": "MAINT: remove all `setup.py`'s and related files for distutils builds",
            "additions": 0,
            "deletions": 1205,
            "change_type": "DELETE",
            "diff": "@@ -1,1205 +0,0 @@\n-\"\"\"\n-A small templating language\n-\n-This implements a small templating language.  This language implements\n-if/elif/else, for/continue/break, expressions, and blocks of Python\n-code.  The syntax is::\n-\n-  {{any expression (function calls etc)}}\n-  {{any expression | filter}}\n-  {{for x in y}}...{{endfor}}\n-  {{if x}}x{{elif y}}y{{else}}z{{endif}}\n-  {{py:x=1}}\n-  {{py:\n-  def foo(bar):\n-      return 'baz'\n-  }}\n-  {{default var = default_value}}\n-  {{# comment}}\n-\n-You use this with the ``Template`` class or the ``sub`` shortcut.\n-The ``Template`` class takes the template string and the name of\n-the template (for errors) and a default namespace.  Then (like\n-``string.Template``) you can call the ``tmpl.substitute(**kw)``\n-method to make a substitution (or ``tmpl.substitute(a_dict)``).\n-\n-``sub(content, **kw)`` substitutes the template immediately.  You\n-can use ``__name='tmpl.html'`` to set the name of the template.\n-\n-If there are syntax errors ``TemplateError`` will be raised.\n-\n-This copy of tempita was taken from https://github.com/gjhiggins/tempita\n-with a few changes to remove the six dependency.\n-\n-\"\"\"\n-import re\n-import sys\n-try:\n-    from urllib.parse import quote as url_quote\n-    from io import StringIO\n-    from html import escape as html_escape\n-except ImportError:\n-    from urllib import quote as url_quote\n-    from cStringIO import StringIO\n-    from cgi import escape as html_escape\n-import os\n-import tokenize\n-from ._looper import looper\n-from .compat3 import (\n-    bytes, basestring_, next, is_unicode, coerce_text, iteritems)\n-\n-\n-__all__ = ['TemplateError', 'Template', 'sub', 'HTMLTemplate',\n-           'sub_html', 'html', 'bunch']\n-\n-in_re = re.compile(r'\\s+in\\s+')\n-var_re = re.compile(r'^[a-z_][a-z0-9_]*$', re.I)\n-\n-\n-class TemplateError(Exception):\n-    \"\"\"Exception raised while parsing a template\n-    \"\"\"\n-\n-    def __init__(self, message, position, name=None):\n-        Exception.__init__(self, message)\n-        self.position = position\n-        self.name = name\n-\n-    def __str__(self):\n-        msg = ' '.join(self.args)\n-        if self.position:\n-            msg = '%s at line %s column %s' % (\n-                msg, self.position[0], self.position[1])\n-        if self.name:\n-            msg += ' in %s' % self.name\n-        return msg\n-\n-\n-class _TemplateContinue(Exception):\n-    pass\n-\n-\n-class _TemplateBreak(Exception):\n-    pass\n-\n-\n-def get_file_template(name, from_template):\n-    path = os.path.join(os.path.dirname(from_template.name), name)\n-    return from_template.__class__.from_filename(\n-        path, namespace=from_template.namespace,\n-        get_template=from_template.get_template)\n-\n-\n-class Template:\n-\n-    default_namespace = {\n-        'start_braces': '{{',\n-        'end_braces': '}}',\n-        'looper': looper,\n-    }\n-\n-    default_encoding = 'utf8'\n-    default_inherit = None\n-\n-    def __init__(self, content, name=None, namespace=None, stacklevel=None,\n-                 get_template=None, default_inherit=None, line_offset=0,\n-                 delimiters=None):\n-        self.content = content\n-\n-        # set delimiters\n-        if delimiters is None:\n-            delimiters = (self.default_namespace['start_braces'],\n-                          self.default_namespace['end_braces'])\n-        else:\n-            assert len(delimiters) == 2 and all(\n-                [isinstance(delimiter, basestring_)\n-                    for delimiter in delimiters])\n-            self.default_namespace = self.__class__.default_namespace.copy()\n-            self.default_namespace['start_braces'] = delimiters[0]\n-            self.default_namespace['end_braces'] = delimiters[1]\n-        self.delimiters = delimiters\n-\n-        self._unicode = is_unicode(content)\n-        if name is None and stacklevel is not None:\n-            try:\n-                caller = sys._getframe(stacklevel)\n-            except ValueError:\n-                pass\n-            else:\n-                globals = caller.f_globals\n-                lineno = caller.f_lineno\n-                if '__file__' in globals:\n-                    name = globals['__file__']\n-                    if name.endswith('.pyc') or name.endswith('.pyo'):\n-                        name = name[:-1]\n-                elif '__name__' in globals:\n-                    name = globals['__name__']\n-                else:\n-                    name = '<string>'\n-                if lineno:\n-                    name += ':%s' % lineno\n-        self.name = name\n-        self._parsed = parse(\n-            content, name=name, line_offset=line_offset,\n-            delimiters=self.delimiters)\n-        if namespace is None:\n-            namespace = {}\n-        self.namespace = namespace\n-        self.get_template = get_template\n-        if default_inherit is not None:\n-            self.default_inherit = default_inherit\n-\n-    def from_filename(cls, filename, namespace=None, encoding=None,\n-                      default_inherit=None, get_template=get_file_template):\n-        with open(filename, 'rb') as f:\n-            c = f.read()\n-        if encoding:\n-            c = c.decode(encoding)\n-        else:\n-            c = c.decode('latin-1')\n-        return cls(content=c, name=filename, namespace=namespace,\n-                   default_inherit=default_inherit, get_template=get_template)\n-\n-    from_filename = classmethod(from_filename)\n-\n-    def __repr__(self):\n-        return '<%s %s name=%r>' % (\n-            self.__class__.__name__,\n-            hex(id(self))[2:], self.name)\n-\n-    def substitute(self, *args, **kw):\n-        if args:\n-            if kw:\n-                raise TypeError(\n-                    \"You can only give positional *or* keyword arguments\")\n-            if len(args) > 1:\n-                raise TypeError(\n-                    \"You can only give one positional argument\")\n-            if not hasattr(args[0], 'items'):\n-                raise TypeError(\n-                    (\"If you pass in a single argument, you must pass in a \",\n-                     \"dict-like object (with a .items() method); you gave %r\")\n-                    % (args[0],))\n-            kw = args[0]\n-        ns = kw\n-        ns['__template_name__'] = self.name\n-        if self.namespace:\n-            ns.update(self.namespace)\n-        result, defs, inherit = self._interpret(ns)\n-        if not inherit:\n-            inherit = self.default_inherit\n-        if inherit:\n-            result = self._interpret_inherit(result, defs, inherit, ns)\n-        return result\n-\n-    def _interpret(self, ns):\n-        # __traceback_hide__ = True\n-        parts = []\n-        defs = {}\n-        self._interpret_codes(self._parsed, ns, out=parts, defs=defs)\n-        if '__inherit__' in defs:\n-            inherit = defs.pop('__inherit__')\n-        else:\n-            inherit = None\n-        return ''.join(parts), defs, inherit\n-\n-    def _interpret_inherit(self, body, defs, inherit_template, ns):\n-        # __traceback_hide__ = True\n-        if not self.get_template:\n-            raise TemplateError(\n-                'You cannot use inheritance without passing in get_template',\n-                position=None, name=self.name)\n-        templ = self.get_template(inherit_template, self)\n-        self_ = TemplateObject(self.name)\n-        for name, value in iteritems(defs):\n-            setattr(self_, name, value)\n-        self_.body = body\n-        ns = ns.copy()\n-        ns['self'] = self_\n-        return templ.substitute(ns)\n-\n-    def _interpret_codes(self, codes, ns, out, defs):\n-        # __traceback_hide__ = True\n-        for item in codes:\n-            if isinstance(item, basestring_):\n-                out.append(item)\n-            else:\n-                self._interpret_code(item, ns, out, defs)\n-\n-    def _interpret_code(self, code, ns, out, defs):\n-        # __traceback_hide__ = True\n-        name, pos = code[0], code[1]\n-        if name == 'py':\n-            self._exec(code[2], ns, pos)\n-        elif name == 'continue':\n-            raise _TemplateContinue()\n-        elif name == 'break':\n-            raise _TemplateBreak()\n-        elif name == 'for':\n-            vars, expr, content = code[2], code[3], code[4]\n-            expr = self._eval(expr, ns, pos)\n-            self._interpret_for(vars, expr, content, ns, out, defs)\n-        elif name == 'cond':\n-            parts = code[2:]\n-            self._interpret_if(parts, ns, out, defs)\n-        elif name == 'expr':\n-            parts = code[2].split('|')\n-            base = self._eval(parts[0], ns, pos)\n-            for part in parts[1:]:\n-                func = self._eval(part, ns, pos)\n-                base = func(base)\n-            out.append(self._repr(base, pos))\n-        elif name == 'default':\n-            var, expr = code[2], code[3]\n-            if var not in ns:\n-                result = self._eval(expr, ns, pos)\n-                ns[var] = result\n-        elif name == 'inherit':\n-            expr = code[2]\n-            value = self._eval(expr, ns, pos)\n-            defs['__inherit__'] = value\n-        elif name == 'def':\n-            name = code[2]\n-            signature = code[3]\n-            parts = code[4]\n-            ns[name] = defs[name] = TemplateDef(\n-                self, name, signature, body=parts, ns=ns, pos=pos)\n-        elif name == 'comment':\n-            return\n-        else:\n-            assert 0, \"Unknown code: %r\" % name\n-\n-    def _interpret_for(self, vars, expr, content, ns, out, defs):\n-        # __traceback_hide__ = True\n-        for item in expr:\n-            if len(vars) == 1:\n-                ns[vars[0]] = item\n-            else:\n-                if len(vars) != len(item):\n-                    raise ValueError(\n-                        'Need %i items to unpack (got %i items)'\n-                        % (len(vars), len(item)))\n-                for name, value in zip(vars, item):\n-                    ns[name] = value\n-            try:\n-                self._interpret_codes(content, ns, out, defs)\n-            except _TemplateContinue:\n-                continue\n-            except _TemplateBreak:\n-                break\n-\n-    def _interpret_if(self, parts, ns, out, defs):\n-        # __traceback_hide__ = True\n-        # @@: if/else/else gets through\n-        for part in parts:\n-            assert not isinstance(part, basestring_)\n-            name, pos = part[0], part[1]\n-            if name == 'else':\n-                result = True\n-            else:\n-                result = self._eval(part[2], ns, pos)\n-            if result:\n-                self._interpret_codes(part[3], ns, out, defs)\n-                break\n-\n-    def _eval(self, code, ns, pos):\n-        # __traceback_hide__ = True\n-        try:\n-            try:\n-                value = eval(code, self.default_namespace, ns)\n-            except SyntaxError as e:\n-                raise SyntaxError(\n-                    'invalid syntax in expression: %s' % code)\n-            return value\n-        except:\n-            e_type, e_value, e_traceback = sys.exc_info()\n-            if getattr(e_value, 'args', None):\n-                arg0 = e_value.args[0]\n-            else:\n-                arg0 = coerce_text(e_value)\n-            e_value.args = (self._add_line_info(arg0, pos),)\n-            raise e_value\n-\n-    def _exec(self, code, ns, pos):\n-        # __traceback_hide__ = True\n-        try:\n-            exec(code, self.default_namespace, ns)\n-        except:\n-            e_type, e_value, e_traceback = sys.exc_info()\n-            if e_value.args:\n-                e_value.args = (self._add_line_info(e_value.args[0], pos),)\n-            else:\n-                e_value.args = (self._add_line_info(None, pos),)\n-            raise e_value\n-\n-    def _repr(self, value, pos):\n-        # __traceback_hide__ = True\n-        try:\n-            if value is None:\n-                return ''\n-            if self._unicode:\n-                value = str(value)\n-                if not is_unicode(value):\n-                    value = value.decode('utf-8')\n-            else:\n-                if not isinstance(value, basestring_):\n-                    value = coerce_text(value)\n-                if (is_unicode(value) and self.default_encoding):\n-                    value = value.encode(self.default_encoding)\n-        except:\n-            e_type, e_value, e_traceback = sys.exc_info()\n-            e_value.args = (self._add_line_info(e_value.args[0], pos),)\n-            raise e_value\n-        else:\n-            if self._unicode and isinstance(value, bytes):\n-                if not self.default_encoding:\n-                    raise UnicodeDecodeError(\n-                        'Cannot decode bytes value %r into unicode '\n-                        '(no default_encoding provided)' % value)\n-                try:\n-                    value = value.decode(self.default_encoding)\n-                except UnicodeDecodeError as e:\n-                    raise UnicodeDecodeError(\n-                        e.encoding,\n-                        e.object,\n-                        e.start,\n-                        e.end,\n-                        e.reason + ' in string %r' % value)\n-            elif not self._unicode and is_unicode(value):\n-                if not self.default_encoding:\n-                    raise UnicodeEncodeError(\n-                        'Cannot encode unicode value %r into bytes '\n-                        '(no default_encoding provided)' % value)\n-                value = value.encode(self.default_encoding)\n-            return value\n-\n-    def _add_line_info(self, msg, pos):\n-        msg = \"%s at line %s column %s\" % (\n-            msg, pos[0], pos[1])\n-        if self.name:\n-            msg += \" in file %s\" % self.name\n-        return msg\n-\n-\n-def sub(content, delimiters=None, **kw):\n-    name = kw.get('__name')\n-    tmpl = Template(content, name=name, delimiters=delimiters)\n-    return tmpl.substitute(kw)\n-\n-\n-def paste_script_template_renderer(content, vars, filename=None):\n-    tmpl = Template(content, name=filename)\n-    return tmpl.substitute(vars)\n-\n-\n-class bunch(dict):\n-\n-    def __init__(self, **kw):\n-        for name, value in iteritems(kw):\n-            setattr(self, name, value)\n-\n-    def __setattr__(self, name, value):\n-        self[name] = value\n-\n-    def __getattr__(self, name):\n-        try:\n-            return self[name]\n-        except KeyError:\n-            raise AttributeError(name)\n-\n-    def __getitem__(self, key):\n-        if 'default' in self:\n-            try:\n-                return dict.__getitem__(self, key)\n-            except KeyError:\n-                return dict.__getitem__(self, 'default')\n-        else:\n-            return dict.__getitem__(self, key)\n-\n-    def __repr__(self):\n-        items = [\n-            (k, v) for k, v in iteritems(self)]\n-        items.sort()\n-        return '<%s %s>' % (\n-            self.__class__.__name__,\n-            ' '.join(['%s=%r' % (k, v) for k, v in items]))\n-\n-############################################################\n-# HTML Templating\n-############################################################\n-\n-\n-class html:\n-\n-    def __init__(self, value):\n-        self.value = value\n-\n-    def __str__(self):\n-        return self.value\n-\n-    def __html__(self):\n-        return self.value\n-\n-    def __repr__(self):\n-        return '<%s %r>' % (\n-            self.__class__.__name__, self.value)\n-\n-\n-def html_quote(value, force=True):\n-    if not force and hasattr(value, '__html__'):\n-        return value.__html__()\n-    if value is None:\n-        return ''\n-    if not isinstance(value, basestring_):\n-        value = coerce_text(value)\n-    if isinstance(value, bytes):\n-        value = html_escape(value.decode('latin1'), 1)\n-        value = value.encode('latin1')\n-    else:\n-        value = html_escape(value, 1)\n-    return value\n-\n-\n-def url(v):\n-    v = coerce_text(v)\n-    if is_unicode(v):\n-        v = v.encode('utf8')\n-    return url_quote(v)\n-\n-\n-def attr(**kw):\n-    kw = list(iteritems(kw))\n-    kw.sort()\n-    parts = []\n-    for name, value in kw:\n-        if value is None:\n-            continue\n-        if name.endswith('_'):\n-            name = name[:-1]\n-        parts.append('%s=\"%s\"' % (html_quote(name), html_quote(value)))\n-    return html(' '.join(parts))\n-\n-\n-class HTMLTemplate(Template):\n-\n-    default_namespace = Template.default_namespace.copy()\n-    default_namespace.update(dict(\n-        html=html,\n-        attr=attr,\n-        url=url,\n-        html_quote=html_quote))\n-\n-    def _repr(self, value, pos):\n-        if hasattr(value, '__html__'):\n-            value = value.__html__()\n-            quote = False\n-        else:\n-            quote = True\n-        plain = Template._repr(self, value, pos)\n-        if quote:\n-            return html_quote(plain)\n-        else:\n-            return plain\n-\n-\n-def sub_html(content, **kw):\n-    name = kw.get('__name')\n-    tmpl = HTMLTemplate(content, name=name)\n-    return tmpl.substitute(kw)\n-\n-\n-class TemplateDef:\n-    def __init__(self, template, func_name, func_signature,\n-                 body, ns, pos, bound_self=None):\n-        self._template = template\n-        self._func_name = func_name\n-        self._func_signature = func_signature\n-        self._body = body\n-        self._ns = ns\n-        self._pos = pos\n-        self._bound_self = bound_self\n-\n-    def __repr__(self):\n-        return '<tempita function %s(%s) at %s:%s>' % (\n-            self._func_name, self._func_signature,\n-            self._template.name, self._pos)\n-\n-    def __str__(self):\n-        return self()\n-\n-    def __call__(self, *args, **kw):\n-        values = self._parse_signature(args, kw)\n-        ns = self._ns.copy()\n-        ns.update(values)\n-        if self._bound_self is not None:\n-            ns['self'] = self._bound_self\n-        out = []\n-        subdefs = {}\n-        self._template._interpret_codes(self._body, ns, out, subdefs)\n-        return ''.join(out)\n-\n-    def __get__(self, obj, type=None):\n-        if obj is None:\n-            return self\n-        return self.__class__(\n-            self._template, self._func_name, self._func_signature,\n-            self._body, self._ns, self._pos, bound_self=obj)\n-\n-    def _parse_signature(self, args, kw):\n-        values = {}\n-        sig_args, var_args, var_kw, defaults = self._func_signature\n-        extra_kw = {}\n-        for name, value in iteritems(kw):\n-            if not var_kw and name not in sig_args:\n-                raise TypeError(\n-                    'Unexpected argument %s' % name)\n-            if name in sig_args:\n-                values[sig_args] = value\n-            else:\n-                extra_kw[name] = value\n-        args = list(args)\n-        sig_args = list(sig_args)\n-        while args:\n-            while sig_args and sig_args[0] in values:\n-                sig_args.pop(0)\n-            if sig_args:\n-                name = sig_args.pop(0)\n-                values[name] = args.pop(0)\n-            elif var_args:\n-                values[var_args] = tuple(args)\n-                break\n-            else:\n-                raise TypeError(\n-                    'Extra position arguments: %s'\n-                    % ', '.join(repr(v) for v in args))\n-        for name, value_expr in iteritems(defaults):\n-            if name not in values:\n-                values[name] = self._template._eval(\n-                    value_expr, self._ns, self._pos)\n-        for name in sig_args:\n-            if name not in values:\n-                raise TypeError(\n-                    'Missing argument: %s' % name)\n-        if var_kw:\n-            values[var_kw] = extra_kw\n-        return values\n-\n-\n-class TemplateObject:\n-\n-    def __init__(self, name):\n-        self.__name = name\n-        self.get = TemplateObjectGetter(self)\n-\n-    def __repr__(self):\n-        return '<%s %s>' % (self.__class__.__name__, self.__name)\n-\n-\n-class TemplateObjectGetter:\n-\n-    def __init__(self, template_obj):\n-        self.__template_obj = template_obj\n-\n-    def __getattr__(self, attr):\n-        return getattr(self.__template_obj, attr, Empty)\n-\n-    def __repr__(self):\n-        return '<%s around %r>' % (\n-            self.__class__.__name__, self.__template_obj)\n-\n-\n-class _Empty:\n-    def __call__(self, *args, **kw):\n-        return self\n-\n-    def __str__(self):\n-        return ''\n-\n-    def __repr__(self):\n-        return 'Empty'\n-\n-    def __unicode__(self):\n-        return ''\n-\n-    def __iter__(self):\n-        return iter(())\n-\n-    def __bool__(self):\n-        return False\n-\n-Empty = _Empty()\n-del _Empty\n-\n-############################################################\n-# Lexing and Parsing\n-############################################################\n-\n-\n-def lex(s, name=None, trim_whitespace=True, line_offset=0, delimiters=None):\n-    \"\"\"\n-    Lex a string into chunks:\n-\n-        >>> lex('hey')\n-        ['hey']\n-        >>> lex('hey {{you}}')\n-        ['hey ', ('you', (1, 7))]\n-        >>> lex('hey {{')\n-        Traceback (most recent call last):\n-            ...\n-        tempita.TemplateError: No }} to finish last expression at line 1 column 7\n-        >>> lex('hey }}')\n-        Traceback (most recent call last):\n-            ...\n-        tempita.TemplateError: }} outside expression at line 1 column 7\n-        >>> lex('hey {{ {{')\n-        Traceback (most recent call last):\n-            ...\n-        tempita.TemplateError: {{ inside expression at line 1 column 10\n-    \"\"\"\n-\n-    if delimiters is None:\n-        delimiters = (Template.default_namespace['start_braces'],\n-                      Template.default_namespace['end_braces'])\n-    in_expr = False\n-    chunks = []\n-    last = 0\n-    last_pos = (line_offset + 1, 1)\n-    token_re = re.compile(r'%s|%s' % (re.escape(delimiters[0]),\n-                                      re.escape(delimiters[1])))\n-    for match in token_re.finditer(s):\n-        expr = match.group(0)\n-        pos = find_position(s, match.end(), last, last_pos)\n-        if expr == delimiters[0] and in_expr:\n-            raise TemplateError('%s inside expression' % delimiters[0],\n-                                position=pos,\n-                                name=name)\n-        elif expr == delimiters[1] and not in_expr:\n-            raise TemplateError('%s outside expression' % delimiters[1],\n-                                position=pos,\n-                                name=name)\n-        if expr == delimiters[0]:\n-            part = s[last:match.start()]\n-            if part:\n-                chunks.append(part)\n-            in_expr = True\n-        else:\n-            chunks.append((s[last:match.start()], last_pos))\n-            in_expr = False\n-        last = match.end()\n-        last_pos = pos\n-    if in_expr:\n-        raise TemplateError('No %s to finish last expression' % delimiters[1],\n-                            name=name, position=last_pos)\n-    part = s[last:]\n-    if part:\n-        chunks.append(part)\n-    if trim_whitespace:\n-        chunks = trim_lex(chunks)\n-    return chunks\n-\n-statement_re = re.compile(r'^(?:if |elif |for |def |inherit |default |py:)')\n-single_statements = ['else', 'endif', 'endfor', 'enddef', 'continue', 'break']\n-trail_whitespace_re = re.compile(r'\\n\\r?[\\t ]*$')\n-lead_whitespace_re = re.compile(r'^[\\t ]*\\n')\n-\n-\n-def trim_lex(tokens):\n-    r\"\"\"\n-    Takes a lexed list of tokens, and removes whitespace when there is\n-    a directive on a line by itself:\n-\n-       >>> tokens = lex('{{if x}}\\nx\\n{{endif}}\\ny', trim_whitespace=False)\n-       >>> tokens\n-       [('if x', (1, 3)), '\\nx\\n', ('endif', (3, 3)), '\\ny']\n-       >>> trim_lex(tokens)\n-       [('if x', (1, 3)), 'x\\n', ('endif', (3, 3)), 'y']\n-    \"\"\"\n-    last_trim = None\n-    for i in range(len(tokens)):\n-        current = tokens[i]\n-        if isinstance(tokens[i], basestring_):\n-            # we don't trim this\n-            continue\n-        item = current[0]\n-        if not statement_re.search(item) and item not in single_statements:\n-            continue\n-        if not i:\n-            prev = ''\n-        else:\n-            prev = tokens[i - 1]\n-        if i + 1 >= len(tokens):\n-            next_chunk = ''\n-        else:\n-            next_chunk = tokens[i + 1]\n-        if (not\n-                isinstance(next_chunk, basestring_) or\n-                not isinstance(prev, basestring_)):\n-            continue\n-        prev_ok = not prev or trail_whitespace_re.search(prev)\n-        if i == 1 and not prev.strip():\n-            prev_ok = True\n-        if last_trim is not None and last_trim + 2 == i and not prev.strip():\n-            prev_ok = 'last'\n-        if (prev_ok and (not next_chunk or lead_whitespace_re.search(\n-                         next_chunk) or (\n-                         i == len(tokens) - 2 and not next_chunk.strip()))):\n-            if prev:\n-                if ((i == 1 and not prev.strip()) or prev_ok == 'last'):\n-                    tokens[i - 1] = ''\n-                else:\n-                    m = trail_whitespace_re.search(prev)\n-                    # +1 to leave the leading \\n on:\n-                    prev = prev[:m.start() + 1]\n-                    tokens[i - 1] = prev\n-            if next_chunk:\n-                last_trim = i\n-                if i == len(tokens) - 2 and not next_chunk.strip():\n-                    tokens[i + 1] = ''\n-                else:\n-                    m = lead_whitespace_re.search(next_chunk)\n-                    next_chunk = next_chunk[m.end():]\n-                    tokens[i + 1] = next_chunk\n-    return tokens\n-\n-\n-def find_position(string, index, last_index, last_pos):\n-    \"\"\"\n-    Given a string and index, return (line, column)\n-    \"\"\"\n-    lines = string.count('\\n', last_index, index)\n-    if lines > 0:\n-        column = index - string.rfind('\\n', last_index, index)\n-    else:\n-        column = last_pos[1] + (index - last_index)\n-    return (last_pos[0] + lines, column)\n-\n-\n-def parse(s, name=None, line_offset=0, delimiters=None):\n-    r\"\"\"\n-    Parses a string into a kind of AST\n-\n-        >>> parse('{{x}}')\n-        [('expr', (1, 3), 'x')]\n-        >>> parse('foo')\n-        ['foo']\n-        >>> parse('{{if x}}test{{endif}}')\n-        [('cond', (1, 3), ('if', (1, 3), 'x', ['test']))]\n-        >>> parse(\n-        ...    'series->{{for x in y}}x={{x}}{{endfor}}'\n-        ... )  #doctest: +NORMALIZE_WHITESPACE\n-        ['series->',\n-            ('for', (1, 11), ('x',), 'y', ['x=', ('expr', (1, 27), 'x')])]\n-        >>> parse('{{for x, y in z:}}{{continue}}{{endfor}}')\n-        [('for', (1, 3), ('x', 'y'), 'z', [('continue', (1, 21))])]\n-        >>> parse('{{py:x=1}}')\n-        [('py', (1, 3), 'x=1')]\n-        >>> parse(\n-        ...    '{{if x}}a{{elif y}}b{{else}}c{{endif}}'\n-        ... )  #doctest: +NORMALIZE_WHITESPACE\n-        [('cond', (1, 3), ('if', (1, 3), 'x', ['a']),\n-            ('elif', (1, 12), 'y', ['b']), ('else', (1, 23), None, ['c']))]\n-\n-    Some exceptions::\n-\n-        >>> parse('{{continue}}')\n-        Traceback (most recent call last):\n-            ...\n-        tempita.TemplateError: continue outside of for loop at line 1 column 3\n-        >>> parse('{{if x}}foo')\n-        Traceback (most recent call last):\n-            ...\n-        tempita.TemplateError: No {{endif}} at line 1 column 3\n-        >>> parse('{{else}}')\n-        Traceback (most recent call last):\n-            ...\n-        tempita.TemplateError: else outside of an if block at line 1 column 3\n-        >>> parse('{{if x}}{{for x in y}}{{endif}}{{endfor}}')\n-        Traceback (most recent call last):\n-            ...\n-        tempita.TemplateError: Unexpected endif at line 1 column 25\n-        >>> parse('{{if}}{{endif}}')\n-        Traceback (most recent call last):\n-            ...\n-        tempita.TemplateError: if with no expression at line 1 column 3\n-        >>> parse('{{for x y}}{{endfor}}')\n-        Traceback (most recent call last):\n-            ...\n-        tempita.TemplateError: Bad for (no \"in\") in 'x y' at line 1 column 3\n-        >>> parse('{{py:x=1\\ny=2}}')  #doctest: +NORMALIZE_WHITESPACE\n-        Traceback (most recent call last):\n-            ...\n-        tempita.TemplateError: Multi-line py blocks must start\n-            with a newline at line 1 column 3\n-    \"\"\"\n-\n-    if delimiters is None:\n-        delimiters = (Template.default_namespace['start_braces'],\n-                      Template.default_namespace['end_braces'])\n-    tokens = lex(s, name=name, line_offset=line_offset, delimiters=delimiters)\n-    result = []\n-    while tokens:\n-        next_chunk, tokens = parse_expr(tokens, name)\n-        result.append(next_chunk)\n-    return result\n-\n-\n-def parse_expr(tokens, name, context=()):\n-    if isinstance(tokens[0], basestring_):\n-        return tokens[0], tokens[1:]\n-    expr, pos = tokens[0]\n-    expr = expr.strip()\n-    if expr.startswith('py:'):\n-        expr = expr[3:].lstrip(' \\t')\n-        if expr.startswith('\\n') or expr.startswith('\\r'):\n-            expr = expr.lstrip('\\r\\n')\n-            if '\\r' in expr:\n-                expr = expr.replace('\\r\\n', '\\n')\n-                expr = expr.replace('\\r', '')\n-            expr += '\\n'\n-        else:\n-            if '\\n' in expr:\n-                raise TemplateError(\n-                    'Multi-line py blocks must start with a newline',\n-                    position=pos, name=name)\n-        return ('py', pos, expr), tokens[1:]\n-    elif expr in ('continue', 'break'):\n-        if 'for' not in context:\n-            raise TemplateError(\n-                'continue outside of for loop',\n-                position=pos, name=name)\n-        return (expr, pos), tokens[1:]\n-    elif expr.startswith('if '):\n-        return parse_cond(tokens, name, context)\n-    elif (expr.startswith('elif ') or expr == 'else'):\n-        raise TemplateError(\n-            '%s outside of an if block' % expr.split()[0],\n-            position=pos, name=name)\n-    elif expr in ('if', 'elif', 'for'):\n-        raise TemplateError(\n-            '%s with no expression' % expr,\n-            position=pos, name=name)\n-    elif expr in ('endif', 'endfor', 'enddef'):\n-        raise TemplateError(\n-            'Unexpected %s' % expr,\n-            position=pos, name=name)\n-    elif expr.startswith('for '):\n-        return parse_for(tokens, name, context)\n-    elif expr.startswith('default '):\n-        return parse_default(tokens, name, context)\n-    elif expr.startswith('inherit '):\n-        return parse_inherit(tokens, name, context)\n-    elif expr.startswith('def '):\n-        return parse_def(tokens, name, context)\n-    elif expr.startswith('#'):\n-        return ('comment', pos, tokens[0][0]), tokens[1:]\n-    return ('expr', pos, tokens[0][0]), tokens[1:]\n-\n-\n-def parse_cond(tokens, name, context):\n-    start = tokens[0][1]\n-    pieces = []\n-    context = context + ('if',)\n-    while 1:\n-        if not tokens:\n-            raise TemplateError(\n-                'Missing {{endif}}',\n-                position=start, name=name)\n-        if (isinstance(tokens[0], tuple) and tokens[0][0] == 'endif'):\n-            return ('cond', start) + tuple(pieces), tokens[1:]\n-        next_chunk, tokens = parse_one_cond(tokens, name, context)\n-        pieces.append(next_chunk)\n-\n-\n-def parse_one_cond(tokens, name, context):\n-    (first, pos), tokens = tokens[0], tokens[1:]\n-    content = []\n-    if first.endswith(':'):\n-        first = first[:-1]\n-    if first.startswith('if '):\n-        part = ('if', pos, first[3:].lstrip(), content)\n-    elif first.startswith('elif '):\n-        part = ('elif', pos, first[5:].lstrip(), content)\n-    elif first == 'else':\n-        part = ('else', pos, None, content)\n-    else:\n-        assert 0, \"Unexpected token %r at %s\" % (first, pos)\n-    while 1:\n-        if not tokens:\n-            raise TemplateError(\n-                'No {{endif}}',\n-                position=pos, name=name)\n-        if (isinstance(tokens[0], tuple) and (\n-                tokens[0][0] == 'endif' or tokens[0][0].startswith(\n-                    'elif ') or tokens[0][0] == 'else')):\n-            return part, tokens\n-        next_chunk, tokens = parse_expr(tokens, name, context)\n-        content.append(next_chunk)\n-\n-\n-def parse_for(tokens, name, context):\n-    first, pos = tokens[0]\n-    tokens = tokens[1:]\n-    context = ('for',) + context\n-    content = []\n-    assert first.startswith('for ')\n-    if first.endswith(':'):\n-        first = first[:-1]\n-    first = first[3:].strip()\n-    match = in_re.search(first)\n-    if not match:\n-        raise TemplateError(\n-            'Bad for (no \"in\") in %r' % first,\n-            position=pos, name=name)\n-    vars = first[:match.start()]\n-    if '(' in vars:\n-        raise TemplateError(\n-            'You cannot have () in the variable section of a for loop (%r)'\n-            % vars, position=pos, name=name)\n-    vars = tuple([\n-        v.strip() for v in first[:match.start()].split(',')\n-        if v.strip()])\n-    expr = first[match.end():]\n-    while 1:\n-        if not tokens:\n-            raise TemplateError(\n-                'No {{endfor}}',\n-                position=pos, name=name)\n-        if (isinstance(tokens[0], tuple) and tokens[0][0] == 'endfor'):\n-            return ('for', pos, vars, expr, content), tokens[1:]\n-        next_chunk, tokens = parse_expr(tokens, name, context)\n-        content.append(next_chunk)\n-\n-\n-def parse_default(tokens, name, context):\n-    first, pos = tokens[0]\n-    assert first.startswith('default ')\n-    first = first.split(None, 1)[1]\n-    parts = first.split('=', 1)\n-    if len(parts) == 1:\n-        raise TemplateError(\n-            \"Expression must be {{default var=value}}; no = found in %r\" %\n-            first, position=pos, name=name)\n-    var = parts[0].strip()\n-    if ',' in var:\n-        raise TemplateError(\n-            \"{{default x, y = ...}} is not supported\",\n-            position=pos, name=name)\n-    if not var_re.search(var):\n-        raise TemplateError(\n-            \"Not a valid variable name for {{default}}: %r\"\n-            % var, position=pos, name=name)\n-    expr = parts[1].strip()\n-    return ('default', pos, var, expr), tokens[1:]\n-\n-\n-def parse_inherit(tokens, name, context):\n-    first, pos = tokens[0]\n-    assert first.startswith('inherit ')\n-    expr = first.split(None, 1)[1]\n-    return ('inherit', pos, expr), tokens[1:]\n-\n-\n-def parse_def(tokens, name, context):\n-    first, start = tokens[0]\n-    tokens = tokens[1:]\n-    assert first.startswith('def ')\n-    first = first.split(None, 1)[1]\n-    if first.endswith(':'):\n-        first = first[:-1]\n-    if '(' not in first:\n-        func_name = first\n-        sig = ((), None, None, {})\n-    elif not first.endswith(')'):\n-        raise TemplateError(\"Function definition doesn't end with ): %s\" %\n-                            first, position=start, name=name)\n-    else:\n-        first = first[:-1]\n-        func_name, sig_text = first.split('(', 1)\n-        sig = parse_signature(sig_text, name, start)\n-    context = context + ('def',)\n-    content = []\n-    while 1:\n-        if not tokens:\n-            raise TemplateError(\n-                'Missing {{enddef}}',\n-                position=start, name=name)\n-        if (isinstance(tokens[0], tuple) and tokens[0][0] == 'enddef'):\n-            return ('def', start, func_name, sig, content), tokens[1:]\n-        next_chunk, tokens = parse_expr(tokens, name, context)\n-        content.append(next_chunk)\n-\n-\n-def parse_signature(sig_text, name, pos):\n-    tokens = tokenize.generate_tokens(StringIO(sig_text).readline)\n-    sig_args = []\n-    var_arg = None\n-    var_kw = None\n-    defaults = {}\n-\n-    def get_token(pos=False):\n-        try:\n-            tok_type, tok_string, (srow, scol), (erow, ecol), line = next(\n-                tokens)\n-        except StopIteration:\n-            return tokenize.ENDMARKER, ''\n-        if pos:\n-            return tok_type, tok_string, (srow, scol), (erow, ecol)\n-        else:\n-            return tok_type, tok_string\n-    while 1:\n-        var_arg_type = None\n-        tok_type, tok_string = get_token()\n-        if tok_type == tokenize.ENDMARKER:\n-            break\n-        if tok_type == tokenize.OP and (\n-                tok_string == '*' or tok_string == '**'):\n-            var_arg_type = tok_string\n-            tok_type, tok_string = get_token()\n-        if tok_type != tokenize.NAME:\n-            raise TemplateError('Invalid signature: (%s)' % sig_text,\n-                                position=pos, name=name)\n-        var_name = tok_string\n-        tok_type, tok_string = get_token()\n-        if tok_type == tokenize.ENDMARKER or (\n-                tok_type == tokenize.OP and tok_string == ','):\n-            if var_arg_type == '*':\n-                var_arg = var_name\n-            elif var_arg_type == '**':\n-                var_kw = var_name\n-            else:\n-                sig_args.append(var_name)\n-            if tok_type == tokenize.ENDMARKER:\n-                break\n-            continue\n-        if var_arg_type is not None:\n-            raise TemplateError('Invalid signature: (%s)' % sig_text,\n-                                position=pos, name=name)\n-        if tok_type == tokenize.OP and tok_string == '=':\n-            nest_type = None\n-            unnest_type = None\n-            nest_count = 0\n-            start_pos = end_pos = None\n-            parts = []\n-            while 1:\n-                tok_type, tok_string, s, e = get_token(True)\n-                if start_pos is None:\n-                    start_pos = s\n-                end_pos = e\n-                if tok_type == tokenize.ENDMARKER and nest_count:\n-                    raise TemplateError('Invalid signature: (%s)' % sig_text,\n-                                        position=pos, name=name)\n-                if (not nest_count and\n-                    (tok_type == tokenize.ENDMARKER or\n-                        (tok_type == tokenize.OP and tok_string == ','))):\n-                    default_expr = isolate_expression(\n-                        sig_text, start_pos, end_pos)\n-                    defaults[var_name] = default_expr\n-                    sig_args.append(var_name)\n-                    break\n-                parts.append((tok_type, tok_string))\n-                if nest_count \\\n-                        and tok_type == tokenize.OP \\\n-                        and tok_string == nest_type:\n-                    nest_count += 1\n-                elif nest_count \\\n-                        and tok_type == tokenize.OP \\\n-                        and tok_string == unnest_type:\n-                    nest_count -= 1\n-                    if not nest_count:\n-                        nest_type = unnest_type = None\n-                elif not nest_count \\\n-                        and tok_type == tokenize.OP \\\n-                        and tok_string in ('(', '[', '{'):\n-                    nest_type = tok_string\n-                    nest_count = 1\n-                    unnest_type = {'(': ')', '[': ']', '{': '}'}[nest_type]\n-    return sig_args, var_arg, var_kw, defaults\n-\n-\n-def isolate_expression(string, start_pos, end_pos):\n-    srow, scol = start_pos\n-    srow -= 1\n-    erow, ecol = end_pos\n-    erow -= 1\n-    lines = string.splitlines(True)\n-    if srow == erow:\n-        return lines[srow][scol:ecol]\n-    parts = [lines[srow][scol:]]\n-    parts.extend(lines[srow + 1:erow])\n-    if erow < len(lines):\n-        # It'll sometimes give (end_row_past_finish, 0)\n-        parts.append(lines[erow][:ecol])\n-    return ''.join(parts)\n-\n-_fill_command_usage = \"\"\"\\\n-%prog [OPTIONS] TEMPLATE arg=value\n-\n-Use py:arg=value to set a Python value; otherwise all values are\n-strings.\n-\"\"\"\n-\n-\n-def fill_command(args=None):\n-    import sys\n-    import optparse\n-    import pkg_resources\n-    import os\n-    if args is None:\n-        args = sys.argv[1:]\n-    dist = pkg_resources.get_distribution('Paste')\n-    parser = optparse.OptionParser(\n-        version=coerce_text(dist),\n-        usage=_fill_command_usage)\n-    parser.add_option(\n-        '-o', '--output',\n-        dest='output',\n-        metavar=\"FILENAME\",\n-        help=\"File to write output to (default stdout)\")\n-    parser.add_option(\n-        '--html',\n-        dest='use_html',\n-        action='store_true',\n-        help=\"Use HTML style filling (including automatic HTML quoting)\")\n-    parser.add_option(\n-        '--env',\n-        dest='use_env',\n-        action='store_true',\n-        help=\"Put the environment in as top-level variables\")\n-    options, args = parser.parse_args(args)\n-    if len(args) < 1:\n-        print('You must give a template filename')\n-        sys.exit(2)\n-    template_name = args[0]\n-    args = args[1:]\n-    vars = {}\n-    if options.use_env:\n-        vars.update(os.environ)\n-    for value in args:\n-        if '=' not in value:\n-            print('Bad argument: %r' % value)\n-            sys.exit(2)\n-        name, value = value.split('=', 1)\n-        if name.startswith('py:'):\n-            name = name[:3]\n-            value = eval(value)\n-        vars[name] = value\n-    if template_name == '-':\n-        template_content = sys.stdin.read()\n-        template_name = '<stdin>'\n-    else:\n-        with open(template_name, 'rb', encoding=\"latin-1\") as f:\n-            template_content = f.read()\n-    if options.use_html:\n-        TemplateClass = HTMLTemplate\n-    else:\n-        TemplateClass = Template\n-    template = TemplateClass(template_content, name=template_name)\n-    result = template.substitute(vars)\n-    if options.output:\n-        with open(options.output, 'wb') as f:\n-            f.write(result)\n-    else:\n-        sys.stdout.write(result)\n-\n-if __name__ == '__main__':\n-    fill_command()\n",
            "comment_added_diff": {},
            "comment_deleted_diff": {
                "18": "  {{# comment}}",
                "109": "        # set delimiters",
                "196": "        # __traceback_hide__ = True",
                "207": "        # __traceback_hide__ = True",
                "222": "        # __traceback_hide__ = True",
                "230": "        # __traceback_hide__ = True",
                "273": "        # __traceback_hide__ = True",
                "292": "        # __traceback_hide__ = True",
                "293": "        # @@: if/else/else gets through",
                "306": "        # __traceback_hide__ = True",
                "324": "        # __traceback_hide__ = True",
                "336": "        # __traceback_hide__ = True",
                "427": "############################################################",
                "428": "# HTML Templating",
                "429": "############################################################",
                "633": "############################################################",
                "634": "# Lexing and Parsing",
                "635": "############################################################",
                "721": "            # we don't trim this",
                "751": "                    # +1 to leave the leading \\n on:",
                "789": "        ... )  #doctest: +NORMALIZE_WHITESPACE",
                "798": "        ... )  #doctest: +NORMALIZE_WHITESPACE",
                "828": "        >>> parse('{{py:x=1\\ny=2}}')  #doctest: +NORMALIZE_WHITESPACE",
                "893": "    elif expr.startswith('#'):",
                "1130": "        # It'll sometimes give (end_row_past_finish, 0)"
            },
            "comment_modified_diff": {}
        }
    ],
    "generate_umath.py": [
        {
            "commit": "4002a7d421ff10780c28a3643683af7a9754f87f",
            "timestamp": "2022-11-25T12:37:46+01:00",
            "author": "Ralf Gommers",
            "commit_message": "BLD: enable building NumPy with Meson\n\nThis enables building with NumPy on Linux and macOS. Windows support\nshould be complete to, but is untested as of now and may need a few\ntweaks. This contains:\n- A set of `meson.build` files and related code generation script\n  tweaks, header templates, etc.\n- One CI job on Linux\n- Basic docs on using Meson to build NumPy (not yet integrated in the\n  html docs, it's too early for that - this is for early adopters right\n  now).\n\nThe build should be complete, with the major exception of SIMD support.\nThe full test suite passes. See gh-22546 for the tracking issue with\ndetailed notes on the plan for switching NumPy to Meson as its build\nsystem.\n\nCo-authored-by: Stefan van der Walt <stefanv@berkeley.edu>",
            "additions": 25,
            "deletions": 3,
            "change_type": "MODIFY",
            "diff": "@@ -3,6 +3,7 @@\n import struct\n import sys\n import textwrap\n+import argparse\n \n Zero = \"PyLong_FromLong(0)\"\n One = \"PyLong_FromLong(1)\"\n@@ -1224,8 +1225,29 @@ def make_code(funcdict, filename):\n     return code\n \n \n-if __name__ == \"__main__\":\n+def main():\n+    parser = argparse.ArgumentParser()\n+    parser.add_argument(\n+        \"-o\",\n+        \"--outfile\",\n+        type=str,\n+        help=\"Path to the output directory\"\n+    )\n+    args = parser.parse_args()\n+\n+    # used to insert the name of this file into the generated file\n     filename = __file__\n     code = make_code(defdict, filename)\n-    with open('__umath_generated.c', 'w') as fid:\n-        fid.write(code)\n+\n+    if not args.outfile:\n+        # This is the distutils-based build\n+        outfile = '__umath_generated.c'\n+    else:\n+        outfile = os.path.join(os.getcwd(), args.outfile)\n+\n+    with open(outfile, 'w') as f:\n+        f.write(code)\n+\n+\n+if __name__ == \"__main__\":\n+    main()\n",
            "comment_added_diff": {
                "1238": "    # used to insert the name of this file into the generated file",
                "1243": "        # This is the distutils-based build"
            },
            "comment_deleted_diff": {},
            "comment_modified_diff": {}
        },
        {
            "commit": "f809fdd8e9b20b993a9a694f4c27f8c6daa07708",
            "timestamp": "2023-01-10T16:19:58+01:00",
            "author": "Sebastian Berg",
            "commit_message": "BUG: Ensure correct loop order in sin, cos, and arctan2\n\nThese were incorrect afer being vectorized.  The commit additional\ntests these (not arctan2 admittedly) and adds a check to generate_umath\nto make it a bit less likely that future additions add this type of thing.\n\nNote that the check allows duplicated loops so long they are correctly\nordered the *first* time.  This makes results correct, but duplicated\nloops are not nice anyways and it would be nice to remove them.\n\nWe could drop them manually in hindsight even?  In any case, that should\nnot be backported, so it is not includedhere.\n\nCloses gh-22984",
            "additions": 46,
            "deletions": 5,
            "change_type": "MODIFY",
            "diff": "@@ -90,6 +90,41 @@ def finish_signature(self, nin, nout):\n         assert len(self.out) == nout\n         self.astype = self.astype_dict.get(self.type, None)\n \n+\n+def _check_order(types1, types2):\n+    dtype_order = allP + \"O\"\n+    for t1, t2 in zip(types1, types2):\n+        # We have no opinion on object or time ordering for now:\n+        if t1 in \"OP\" or t2 in \"OP\":\n+            return True\n+        if t1 in \"mM\" or t2 in \"mM\":\n+            return True\n+\n+        t1i = dtype_order.index(t1)\n+        t2i = dtype_order.index(t2)\n+        if t1i < t2i:\n+            return\n+        if t2i > t1i:\n+            break\n+\n+    raise TypeError(\n+            f\"Input dtypes are unsorted or duplicate: {types1} and {types2}\")\n+\n+\n+def check_td_order(tds):\n+    # A quick check for whether the signatures make sense, it happened too\n+    # often that SIMD additions added loops that do not even make some sense.\n+    # TODO: This should likely be a test and it would be nice if it rejected\n+    #       duplicate entries as well (but we have many as of writing this).\n+    signatures = [t.in_+t.out for t in tds]\n+\n+    for prev_i, sign in enumerate(signatures[1:]):\n+        if sign in signatures[:prev_i+1]:\n+            continue  # allow duplicates...\n+        \n+        _check_order(signatures[prev_i], sign)\n+\n+\n _fdata_map = dict(\n     e='npy_%sf',\n     f='npy_%sf',\n@@ -173,6 +208,9 @@ def __init__(self, nin, nout, identity, docstring, typereso,\n         for td in self.type_descriptions:\n             td.finish_signature(self.nin, self.nout)\n \n+        check_td_order(self.type_descriptions)\n+\n+\n # String-handling utilities to avoid locale-dependence.\n \n import string\n@@ -719,18 +757,20 @@ def english_upper(s):\n     Ufunc(1, 1, None,\n           docstrings.get('numpy.core.umath.cos'),\n           None,\n+          TD('e', dispatch=[('loops_umath_fp', 'e')]),\n           TD('f', dispatch=[('loops_trigonometric', 'f')]),\n-          TD('ed', dispatch=[('loops_umath_fp', 'ed')]),\n-          TD('fdg' + cmplx, f='cos'),\n+          TD('d', dispatch=[('loops_umath_fp', 'd')]),\n+          TD('g' + cmplx, f='cos'),\n           TD(P, f='cos'),\n           ),\n 'sin':\n     Ufunc(1, 1, None,\n           docstrings.get('numpy.core.umath.sin'),\n           None,\n+          TD('e', dispatch=[('loops_umath_fp', 'e')]),\n           TD('f', dispatch=[('loops_trigonometric', 'f')]),\n-          TD('ed', dispatch=[('loops_umath_fp', 'ed')]),\n-          TD('fdg' + cmplx, f='sin'),\n+          TD('d', dispatch=[('loops_umath_fp', 'd')]),\n+          TD('g' + cmplx, f='sin'),\n           TD(P, f='sin'),\n           ),\n 'tan':\n@@ -888,8 +928,9 @@ def english_upper(s):\n     Ufunc(2, 1, None,\n           docstrings.get('numpy.core.umath.arctan2'),\n           None,\n+          TD('e', f='atan2', astype={'e': 'f'}),\n           TD('fd', dispatch=[('loops_umath_fp', 'fd')]),\n-          TD(flts, f='atan2', astype={'e': 'f'}),\n+          TD('g', f='atan2', astype={'e': 'f'}),\n           TD(P, f='arctan2'),\n           ),\n 'remainder':\n",
            "comment_added_diff": {
                "97": "        # We have no opinion on object or time ordering for now:",
                "115": "    # A quick check for whether the signatures make sense, it happened too",
                "116": "    # often that SIMD additions added loops that do not even make some sense.",
                "117": "    # TODO: This should likely be a test and it would be nice if it rejected",
                "118": "    #       duplicate entries as well (but we have many as of writing this).",
                "123": "            continue  # allow duplicates..."
            },
            "comment_deleted_diff": {},
            "comment_modified_diff": {}
        },
        {
            "commit": "7910e2bf14b8bef95c3180fbf667458e29426a0e",
            "timestamp": "2023-01-30T21:32:35+02:00",
            "author": "mattip",
            "commit_message": "ENH: add indexed lower loops and connect them to ufuncs",
            "additions": 93,
            "deletions": 5,
            "change_type": "MODIFY",
            "diff": "@@ -1,3 +1,8 @@\n+\"\"\"\n+Generate the code to build all the internal ufuncs. At the base is the defdict:\n+a dictionary ofUfunc classes. This is fed to make_code to generate\n+__umath_generated.c\n+\"\"\"\n import os\n import re\n import struct\n@@ -5,6 +10,7 @@\n import textwrap\n import argparse\n \n+# identity objects\n Zero = \"PyLong_FromLong(0)\"\n One = \"PyLong_FromLong(1)\"\n True_ = \"(Py_INCREF(Py_True), Py_True)\"\n@@ -125,7 +131,7 @@ def check_td_order(tds):\n         _check_order(signatures[prev_i], sign)\n \n \n-_fdata_map = dict(\n+_floatformat_map = dict(\n     e='npy_%sf',\n     f='npy_%sf',\n     d='npy_%s',\n@@ -136,11 +142,13 @@ def check_td_order(tds):\n )\n \n def build_func_data(types, f):\n-    func_data = [_fdata_map.get(t, '%s') % (f,) for t in types]\n+    func_data = [_floatformat_map.get(t, '%s') % (f,) for t in types]\n     return func_data\n \n def TD(types, f=None, astype=None, in_=None, out=None, cfunc_alias=None,\n        simd=None, dispatch=None):\n+    \"\"\"Generate a TypeDescription instance for each item in types\n+    \"\"\"\n     if f is not None:\n         if isinstance(f, str):\n             func_data = build_func_data(types, f)\n@@ -188,12 +196,15 @@ class Ufunc:\n     ----------\n     nin : number of input arguments\n     nout : number of output arguments\n-    identity : identity element for a two-argument function\n+    identity : identity element for a two-argument function (like Zero)\n     docstring : docstring for the ufunc\n-    type_descriptions : list of TypeDescription objects\n+    typereso: type resolver function of type PyUFunc_TypeResolutionFunc\n+    type_descriptions : TypeDescription objects\n+    signature: a generalized ufunc signature (like for matmul)\n+    indexed: add indexed loops (ufunc.at) for these type characters\n     \"\"\"\n     def __init__(self, nin, nout, identity, docstring, typereso,\n-                 *type_descriptions, signature=None):\n+                 *type_descriptions, signature=None, indexed=''):\n         self.nin = nin\n         self.nout = nout\n         if identity is None:\n@@ -203,6 +214,7 @@ def __init__(self, nin, nout, identity, docstring, typereso,\n         self.typereso = typereso\n         self.type_descriptions = []\n         self.signature = signature\n+        self.indexed = indexed\n         for td in type_descriptions:\n             self.type_descriptions.extend(td)\n         for td in self.type_descriptions:\n@@ -347,6 +359,7 @@ def english_upper(s):\n            TypeDescription('M', FullTypeDescr, 'mM', 'M'),\n           ],\n           TD(O, f='PyNumber_Add'),\n+          indexed=flts + ints\n           ),\n 'subtract':\n     Ufunc(2, 1, None, # Zero is only a unit to the right, not the left\n@@ -359,6 +372,7 @@ def english_upper(s):\n            TypeDescription('M', FullTypeDescr, 'MM', 'm'),\n           ],\n           TD(O, f='PyNumber_Subtract'),\n+          indexed=flts + ints\n           ),\n 'multiply':\n     Ufunc(2, 1, One,\n@@ -374,6 +388,7 @@ def english_upper(s):\n            TypeDescription('m', FullTypeDescr, 'dm', 'm'),\n           ],\n           TD(O, f='PyNumber_Multiply'),\n+          indexed=flts + ints\n           ),\n #'true_divide' : aliased to divide in umathmodule.c:initumath\n 'floor_divide':\n@@ -388,6 +403,7 @@ def english_upper(s):\n            TypeDescription('m', FullTypeDescr, 'mm', 'q'),\n           ],\n           TD(O, f='PyNumber_FloorDivide'),\n+          indexed=flts + ints\n           ),\n 'divide':\n     Ufunc(2, 1, None, # One is only a unit to the right, not the left\n@@ -1255,6 +1271,40 @@ def make_ufuncs(funcdict):\n         if uf.typereso is not None:\n             mlist.append(\n                 r\"((PyUFuncObject *)f)->type_resolver = &%s;\" % uf.typereso)\n+        for c in uf.indexed:\n+            # Handle indexed loops by getting the underlying ArrayMethodObject\n+            # from the list in f._loops and setting its field appropriately\n+            fmt = textwrap.dedent(\"\"\"\n+            {{\n+                PyArray_DTypeMeta *dtype = PyArray_DTypeFromTypeNum({typenum});\n+                PyObject *info = get_info_no_cast((PyUFuncObject *)f, dtype, {count});\n+                if (info == NULL) {{\n+                    return -1;\n+                }}\n+                if (info == Py_None) {{\n+                    PyErr_SetString(PyExc_RuntimeError,\n+                        \"cannot add indexed loop to ufunc {name} with {typenum}\");\n+                    return -1;\n+                }}\n+                if (!PyObject_TypeCheck(info, &PyArrayMethod_Type)) {{\n+                    PyErr_SetString(PyExc_RuntimeError,\n+                        \"Not a PyArrayMethodObject in ufunc {name} with {typenum}\");\n+                }}\n+                ((PyArrayMethodObject*)info)->contiguous_indexed_loop = {funcname};\n+                /* info is borrowed, no need to decref*/\n+            }}    \n+            \"\"\")\n+            cname = name\n+            if cname == \"floor_divide\":\n+                # XXX there must be a better way...\n+                cname = \"divide\"\n+            mlist.append(fmt.format(\n+                typenum=f\"NPY_{english_upper(chartoname[c])}\",\n+                count=uf.nin+uf.nout,\n+                name=name,\n+                funcname = f\"{english_upper(chartoname[c])}_{cname}_indexed\",\n+            ))\n+            \n         mlist.append(r\"\"\"PyDict_SetItemString(dictionary, \"%s\", f);\"\"\" % name)\n         mlist.append(r\"\"\"Py_DECREF(f);\"\"\")\n         code3list.append('\\n'.join(mlist))\n@@ -1277,8 +1327,46 @@ def make_code(funcdict, filename):\n     #include \"loops.h\"\n     #include \"matmul.h\"\n     #include \"clip.h\"\n+    #include \"dtypemeta.h\"\n     #include \"_umath_doc_generated.h\"\n+\n     %s\n+    /*\n+     * Return the PyArrayMethodObject or PyCapsule that matches a registered\n+     * tuple of identical dtypes. Return a borrowed ref of the first match.\n+     */\n+    static PyObject *\n+    get_info_no_cast(PyUFuncObject *ufunc, PyArray_DTypeMeta *op_dtype, int ndtypes)\n+    {\n+        \n+        PyObject *t_dtypes = PyTuple_New(ndtypes);\n+        if (t_dtypes == NULL) {\n+            return NULL;\n+        }\n+        for (int i=0; i < ndtypes; i++) {\n+            PyTuple_SetItem(t_dtypes, i, (PyObject *)op_dtype);\n+        }\n+        PyObject *loops = ufunc->_loops;\n+        Py_ssize_t length = PyList_Size(loops);\n+        for (Py_ssize_t i = 0; i < length; i++) {\n+            PyObject *item = PyList_GetItem(loops, i);\n+            PyObject *cur_DType_tuple = PyTuple_GetItem(item, 0);\n+            int cmp = PyObject_RichCompareBool(cur_DType_tuple, t_dtypes, Py_EQ);\n+            if (cmp < 0) {\n+                Py_DECREF(t_dtypes);\n+                return NULL;\n+            }\n+            if (cmp == 0) {\n+                continue;\n+            }\n+            /* Got the match */\n+            Py_DECREF(t_dtypes);\n+            return PyTuple_GetItem(item, 1); \n+        }\n+        Py_DECREF(t_dtypes);\n+        Py_RETURN_NONE;\n+    }\n+\n \n     static int\n     InitOperators(PyObject *dictionary) {\n",
            "comment_added_diff": {
                "13": "# identity objects",
                "1275": "            # Handle indexed loops by getting the underlying ArrayMethodObject",
                "1276": "            # from the list in f._loops and setting its field appropriately",
                "1299": "                # XXX there must be a better way...",
                "1330": "    #include \"dtypemeta.h\""
            },
            "comment_deleted_diff": {},
            "comment_modified_diff": {}
        },
        {
            "commit": "28706afcbe1bf35413049d0283e6e01ef9abcb1a",
            "timestamp": "2023-02-01T16:14:23+02:00",
            "author": "mattip",
            "commit_message": "MAINT, BUG: fixes from review and testing",
            "additions": 5,
            "deletions": 42,
            "change_type": "MODIFY",
            "diff": "@@ -415,6 +415,7 @@ def english_upper(s):\n            TypeDescription('m', FullTypeDescr, 'mm', 'd', cfunc_alias='divide'),\n           ],\n           TD(O, f='PyNumber_TrueDivide'),\n+          indexed=flts\n           ),\n 'conjugate':\n     Ufunc(1, 1, None,\n@@ -1298,15 +1299,11 @@ def make_ufuncs(funcdict):\n                 /* info is borrowed, no need to decref*/\n             }}\n             \"\"\")\n-            cname = name\n-            if cname == \"floor_divide\":\n-                # XXX there must be a better way...\n-                cname = \"divide\"\n             mlist.append(fmt.format(\n                 typenum=f\"NPY_{english_upper(chartoname[c])}\",\n                 count=uf.nin+uf.nout,\n                 name=name,\n-                funcname = f\"{english_upper(chartoname[c])}_{cname}_indexed\",\n+                funcname = f\"{english_upper(chartoname[c])}_{name}_indexed\",\n             ))\n \n         mlist.append(r\"\"\"PyDict_SetItemString(dictionary, \"%s\", f);\"\"\" % name)\n@@ -1335,44 +1332,10 @@ def make_code(funcdict, filename):\n     #include \"_umath_doc_generated.h\"\n \n     %s\n-    /*\n-     * Return the PyArrayMethodObject or PyCapsule that matches a registered\n-     * tuple of identical dtypes. Return a borrowed ref of the first match.\n-     */\n-    static PyObject *\n+    /* Returns a borrowed ref of the second value in the matching info tuple */\n+    PyObject *\n     get_info_no_cast(PyUFuncObject *ufunc, PyArray_DTypeMeta *op_dtype,\n-                     int ndtypes)\n-    {\n-\n-        PyObject *t_dtypes = PyTuple_New(ndtypes);\n-        if (t_dtypes == NULL) {\n-            return NULL;\n-        }\n-        for (int i=0; i < ndtypes; i++) {\n-            PyTuple_SetItem(t_dtypes, i, (PyObject *)op_dtype);\n-        }\n-        PyObject *loops = ufunc->_loops;\n-        Py_ssize_t length = PyList_Size(loops);\n-        for (Py_ssize_t i = 0; i < length; i++) {\n-            PyObject *item = PyList_GetItem(loops, i);\n-            PyObject *cur_DType_tuple = PyTuple_GetItem(item, 0);\n-            int cmp = PyObject_RichCompareBool(cur_DType_tuple,\n-                                               t_dtypes, Py_EQ);\n-            if (cmp < 0) {\n-                Py_DECREF(t_dtypes);\n-                return NULL;\n-            }\n-            if (cmp == 0) {\n-                continue;\n-            }\n-            /* Got the match */\n-            Py_DECREF(t_dtypes);\n-            return PyTuple_GetItem(item, 1);\n-        }\n-        Py_DECREF(t_dtypes);\n-        Py_RETURN_NONE;\n-    }\n-\n+                     int ndtypes);\n \n     static int\n     InitOperators(PyObject *dictionary) {\n",
            "comment_added_diff": {},
            "comment_deleted_diff": {
                "1303": "                # XXX there must be a better way..."
            },
            "comment_modified_diff": {}
        },
        {
            "commit": "866f41a85bddfa3ea6de551bb27f335b0f8a6a52",
            "timestamp": "2023-02-20T04:07:15+02:00",
            "author": "Sayed Adel",
            "commit_message": "MAINT, SIMD: Removes compiler definitions of attribute-based CPU dispatching",
            "additions": 48,
            "deletions": 49,
            "change_type": "MODIFY",
            "diff": "@@ -61,9 +61,6 @@ class TypeDescription:\n     cfunc_alias : str or none, optional\n         Appended to inner loop C function name, e.g., FLOAT_{cfunc_alias}. See make_arrays.\n         NOTE: it doesn't support 'astype'\n-    simd : list\n-        Available SIMD ufunc loops, dispatched at runtime in specified order\n-        Currently only supported for simples types (see make_arrays)\n     dispatch : str or None, optional\n         Dispatch-able source name without its extension '.dispatch.c' that\n         contains the definition of ufunc, dispatched at runtime depending on the\n@@ -71,7 +68,7 @@ class TypeDescription:\n         NOTE: it doesn't support 'astype'\n     \"\"\"\n     def __init__(self, type, f=None, in_=None, out=None, astype=None, cfunc_alias=None,\n-                 simd=None, dispatch=None):\n+                 dispatch=None):\n         self.type = type\n         self.func_data = f\n         if astype is None:\n@@ -84,7 +81,6 @@ def __init__(self, type, f=None, in_=None, out=None, astype=None, cfunc_alias=No\n             out = out.replace('P', type)\n         self.out = out\n         self.cfunc_alias = cfunc_alias\n-        self.simd = simd\n         self.dispatch = dispatch\n \n     def finish_signature(self, nin, nout):\n@@ -146,8 +142,9 @@ def build_func_data(types, f):\n     return func_data\n \n def TD(types, f=None, astype=None, in_=None, out=None, cfunc_alias=None,\n-       simd=None, dispatch=None):\n-    \"\"\"Generate a TypeDescription instance for each item in types\n+       dispatch=None):\n+    \"\"\"\n+    Generate a TypeDescription instance for each item in types\n     \"\"\"\n     if f is not None:\n         if isinstance(f, str):\n@@ -172,12 +169,6 @@ def TD(types, f=None, astype=None, in_=None, out=None, cfunc_alias=None,\n         raise ValueError(\"Number of types and outputs do not match\")\n     tds = []\n     for t, fd, i, o in zip(types, func_data, in_, out):\n-        # [(simd-name, list of types)]\n-        if simd is not None:\n-            simdt = [k for k, v in simd if t in v]\n-        else:\n-            simdt = []\n-\n         # [(dispatch file name without extension '.dispatch.c*', list of types)]\n         if dispatch:\n             dispt = ([k for k, v in dispatch if t in v]+[None])[0]\n@@ -185,7 +176,7 @@ def TD(types, f=None, astype=None, in_=None, out=None, cfunc_alias=None,\n             dispt = None\n         tds.append(TypeDescription(\n             t, f=fd, in_=i, out=o, astype=astype, cfunc_alias=cfunc_alias,\n-            simd=simdt, dispatch=dispt\n+            dispatch=dispt\n         ))\n     return tds\n \n@@ -352,8 +343,10 @@ def english_upper(s):\n           docstrings.get('numpy.core.umath.add'),\n           'PyUFunc_AdditionTypeResolver',\n           TD('?', cfunc_alias='logical_or', dispatch=[('loops_logical', '?')]),\n-          TD(no_bool_times_obj, simd=[('avx2', ints)],\n-                                dispatch=[('loops_arithm_fp', 'fdFD')]),\n+          TD(no_bool_times_obj, dispatch=[\n+              ('loops_arithm_fp', 'fdFD'),\n+              ('loops_autovec_int', ints),\n+          ]),\n           [TypeDescription('M', FullTypeDescr, 'Mm', 'M'),\n            TypeDescription('m', FullTypeDescr, 'mm', 'm'),\n            TypeDescription('M', FullTypeDescr, 'mM', 'M'),\n@@ -365,8 +358,10 @@ def english_upper(s):\n     Ufunc(2, 1, None, # Zero is only a unit to the right, not the left\n           docstrings.get('numpy.core.umath.subtract'),\n           'PyUFunc_SubtractionTypeResolver',\n-          TD(no_bool_times_obj, simd=[('avx2', ints)],\n-                                dispatch=[('loops_arithm_fp', 'fdFD')]),\n+          TD(no_bool_times_obj, dispatch=[\n+              ('loops_arithm_fp', 'fdFD'),\n+              ('loops_autovec_int', ints),\n+          ]),\n           [TypeDescription('M', FullTypeDescr, 'Mm', 'M'),\n            TypeDescription('m', FullTypeDescr, 'mm', 'm'),\n            TypeDescription('M', FullTypeDescr, 'MM', 'm'),\n@@ -380,8 +375,10 @@ def english_upper(s):\n           'PyUFunc_MultiplicationTypeResolver',\n           TD('?', cfunc_alias='logical_and',\n                   dispatch=[('loops_logical', '?')]),\n-          TD(no_bool_times_obj, simd=[('avx2', ints)],\n-                                dispatch=[('loops_arithm_fp', 'fdFD')]),\n+          TD(no_bool_times_obj, dispatch=[\n+              ('loops_arithm_fp', 'fdFD'),\n+              ('loops_autovec_int', ints),\n+          ]),\n           [TypeDescription('m', FullTypeDescr, 'mq', 'm'),\n            TypeDescription('m', FullTypeDescr, 'qm', 'm'),\n            TypeDescription('m', FullTypeDescr, 'md', 'm'),\n@@ -421,8 +418,10 @@ def english_upper(s):\n     Ufunc(1, 1, None,\n           docstrings.get('numpy.core.umath.conjugate'),\n           None,\n-          TD(ints+flts+cmplx, simd=[('avx2', ints)],\n-            dispatch=[('loops_arithm_fp', 'FD')]),\n+          TD(ints+flts+cmplx, dispatch=[\n+              ('loops_arithm_fp', 'FD'),\n+              ('loops_autovec_int', ints),\n+          ]),\n           TD(P, f='conjugate'),\n           ),\n 'fmod':\n@@ -437,15 +436,21 @@ def english_upper(s):\n     Ufunc(1, 1, None,\n           docstrings.get('numpy.core.umath.square'),\n           None,\n-          TD(ints+inexact, simd=[('avx2', ints)],\n-             dispatch=[('loops_unary_fp', 'fd'), ('loops_arithm_fp', 'FD')]),\n+          TD(ints+inexact, dispatch=[\n+              ('loops_unary_fp', 'fd'),\n+              ('loops_arithm_fp', 'FD'),\n+              ('loops_autovec_int', ints),\n+          ]),\n           TD(O, f='Py_square'),\n           ),\n 'reciprocal':\n     Ufunc(1, 1, None,\n           docstrings.get('numpy.core.umath.reciprocal'),\n           None,\n-          TD(ints+inexact, simd=[('avx2', ints)], dispatch=[('loops_unary_fp', 'fd')]),\n+          TD(ints+inexact, dispatch=[\n+              ('loops_unary_fp', 'fd'),\n+              ('loops_autovec_int', ints),\n+          ]),\n           TD(O, f='Py_reciprocal'),\n           ),\n # This is no longer used as numpy.ones_like, however it is\n@@ -563,24 +568,30 @@ def english_upper(s):\n     Ufunc(2, 1, True_,\n           docstrings.get('numpy.core.umath.logical_and'),\n           'PyUFunc_SimpleBinaryComparisonTypeResolver',\n-          TD(nodatetime_or_obj, out='?', simd=[('avx2', ints)],\n-                                dispatch=[('loops_logical', '?')]),\n+          TD(nodatetime_or_obj, out='?', dispatch=[\n+              ('loops_logical', '?'),\n+              ('loops_autovec_int', ints),\n+          ]),\n           TD(O, f='npy_ObjectLogicalAnd'),\n           ),\n 'logical_not':\n     Ufunc(1, 1, None,\n           docstrings.get('numpy.core.umath.logical_not'),\n           None,\n-          TD(nodatetime_or_obj, out='?', simd=[('avx2', ints)],\n-                                dispatch=[('loops_logical', '?')]),\n+          TD(nodatetime_or_obj, out='?', dispatch=[\n+              ('loops_logical', '?'),\n+              ('loops_autovec_int', ints),\n+          ]),\n           TD(O, f='npy_ObjectLogicalNot'),\n           ),\n 'logical_or':\n     Ufunc(2, 1, False_,\n           docstrings.get('numpy.core.umath.logical_or'),\n           'PyUFunc_SimpleBinaryComparisonTypeResolver',\n-          TD(nodatetime_or_obj, out='?', simd=[('avx2', ints)],\n-                                dispatch=[('loops_logical', '?')]),\n+          TD(nodatetime_or_obj, out='?', dispatch=[\n+              ('loops_logical', '?'),\n+              ('loops_autovec_int', ints),\n+          ]),\n           TD(O, f='npy_ObjectLogicalOr'),\n           ),\n 'logical_xor':\n@@ -656,7 +667,7 @@ def english_upper(s):\n           None,\n           TD('?', cfunc_alias='logical_and',\n                   dispatch=[('loops_logical', '?')]),\n-          TD(ints, simd=[('avx2', ints)]),\n+          TD(ints, dispatch=[('loops_autovec_int', ints)]),\n           TD(O, f='PyNumber_And'),\n           ),\n 'bitwise_or':\n@@ -664,7 +675,7 @@ def english_upper(s):\n           docstrings.get('numpy.core.umath.bitwise_or'),\n           None,\n           TD('?', cfunc_alias='logical_or', dispatch=[('loops_logical', '?')]),\n-          TD(ints, simd=[('avx2', ints)]),\n+          TD(ints, dispatch=[('loops_autovec_int', ints)]),\n           TD(O, f='PyNumber_Or'),\n           ),\n 'bitwise_xor':\n@@ -673,7 +684,7 @@ def english_upper(s):\n           None,\n           TD('?', cfunc_alias='not_equal',\n                   dispatch=[('loops_comparison', '?')]),\n-          TD(ints, simd=[('avx2', ints)]),\n+          TD(ints, dispatch=[('loops_autovec_int', ints)]),\n           TD(O, f='PyNumber_Xor'),\n           ),\n 'invert':\n@@ -682,21 +693,21 @@ def english_upper(s):\n           None,\n           TD('?', cfunc_alias='logical_not',\n                   dispatch=[('loops_logical', '?')]),\n-          TD(ints, simd=[('avx2', ints)]),\n+          TD(ints, dispatch=[('loops_autovec_int', ints)]),\n           TD(O, f='PyNumber_Invert'),\n           ),\n 'left_shift':\n     Ufunc(2, 1, None,\n           docstrings.get('numpy.core.umath.left_shift'),\n           None,\n-          TD(ints, simd=[('avx2', ints)]),\n+          TD(ints, dispatch=[('loops_autovec_int', ints)]),\n           TD(O, f='PyNumber_Lshift'),\n           ),\n 'right_shift':\n     Ufunc(2, 1, None,\n           docstrings.get('numpy.core.umath.right_shift'),\n           None,\n-          TD(ints, simd=[('avx2', ints)]),\n+          TD(ints, dispatch=[('loops_autovec_int', ints)]),\n           TD(O, f='PyNumber_Rshift'),\n           ),\n 'heaviside':\n@@ -1156,18 +1167,6 @@ def make_arrays(funcdict):\n                 datalist.append('(void *)NULL')\n                 tname = english_upper(chartoname[t.type])\n                 cfunc_fname = f\"{tname}_{cfunc_alias}\"\n-                if t.simd is not None:\n-                    for vt in t.simd:\n-                        code2list.append(textwrap.dedent(\"\"\"\\\n-                        #ifdef HAVE_ATTRIBUTE_TARGET_{ISA}\n-                        if (NPY_CPU_HAVE({ISA})) {{\n-                            {fname}_functions[{idx}] = {cname}_{isa};\n-                        }}\n-                        #endif\n-                        \"\"\").format(\n-                            ISA=vt.upper(), isa=vt,\n-                            fname=name, cname=cfunc_fname, idx=k\n-                        ))\n             else:\n                 try:\n                     thedict = arity_lookup[uf.nin, uf.nout]\n",
            "comment_added_diff": {},
            "comment_deleted_diff": {
                "175": "        # [(simd-name, list of types)]",
                "1162": "                        #ifdef HAVE_ATTRIBUTE_TARGET_{ISA}",
                "1166": "                        #endif"
            },
            "comment_modified_diff": {}
        },
        {
            "commit": "ec8d5db302c0e8597feb058f58863d5e9a6554c1",
            "timestamp": "2023-05-04T16:33:27+02:00",
            "author": "Sebastian Berg",
            "commit_message": "ENH: Make signed/unsigned integer comparisons exact\n\nThis makes comparisons between signed and unsigned integers exact\nby special-casing promotion in comparison to never promote integers\nto floats, but rather promote them to uint64 or int64 and use a\nspecific loop for that purpose.\n\nThis is a bit lazy, it doesn't make the scalar paths fast (they never were\nthough) nor does it try to vectorize the loop.\nThus, for cases that are not int64/uint64 already and require a cast in\neither case, it should be a bit slower.  OTOH, it was never really fast\nand the int64/uint64 mix is probably faster since it avoids casting.\n\n---\n\nNow... the reason I was looking into this was, that I had hoped\nit would help with NEP 50/weak scalar typing to allow:\n\n    uint64(1) < -1  # annoying that it fails with NEP 50\n\nbut, it doesn't actually, because if I use int64 for the -1 then very\nlarge numbers would be a problem...\nI could probably(?) add a *specific* \"Python integer\" ArrayMethod for comparisons\nand that could pick `object` dtype and thus get the original Python object\n(the loop could then in practice assume a scalar value).\n\n---\n\nIn either case, this works, and unless we worry about keeping the behavior\nwe probably might as well do this.\n(Potentially with follow-ups to speed it up.)",
            "additions": 39,
            "deletions": 13,
            "change_type": "MODIFY",
            "diff": "@@ -109,6 +109,11 @@ def _check_order(types1, types2):\n         if t2i > t1i:\n             break\n \n+    if types1 == \"QQ?\" and types2 == \"qQ?\":\n+        # Explicitly allow this mixed case, rather than figure out what order\n+        # is nicer or how to encode it.\n+        return\n+\n     raise TypeError(\n             f\"Input dtypes are unsorted or duplicate: {types1} and {types2}\")\n \n@@ -523,49 +528,67 @@ def english_upper(s):\n     Ufunc(2, 1, None,\n           docstrings.get('numpy.core.umath.greater'),\n           'PyUFunc_SimpleBinaryComparisonTypeResolver',\n-          TD(all, out='?', dispatch=[('loops_comparison', bints+'fd')]),\n-          [TypeDescription('O', FullTypeDescr, 'OO', 'O')],\n+          TD(bints, out='?'),\n+          [TypeDescription('q', FullTypeDescr, 'qQ', '?'),\n+           TypeDescription('q', FullTypeDescr, 'Qq', '?')],\n+          TD(inexact+times, out='?', dispatch=[('loops_comparison', bints+'fd')]),\n           TD('O', out='?'),\n+          [TypeDescription('O', FullTypeDescr, 'OO', 'O')],\n           ),\n 'greater_equal':\n     Ufunc(2, 1, None,\n           docstrings.get('numpy.core.umath.greater_equal'),\n           'PyUFunc_SimpleBinaryComparisonTypeResolver',\n-          TD(all, out='?', dispatch=[('loops_comparison', bints+'fd')]),\n-          [TypeDescription('O', FullTypeDescr, 'OO', 'O')],\n+          TD(bints, out='?'),\n+          [TypeDescription('q', FullTypeDescr, 'qQ', '?'),\n+           TypeDescription('q', FullTypeDescr, 'Qq', '?')],\n+          TD(inexact+times, out='?', dispatch=[('loops_comparison', bints+'fd')]),\n           TD('O', out='?'),\n+          [TypeDescription('O', FullTypeDescr, 'OO', 'O')],\n           ),\n 'less':\n     Ufunc(2, 1, None,\n           docstrings.get('numpy.core.umath.less'),\n           'PyUFunc_SimpleBinaryComparisonTypeResolver',\n-          TD(all, out='?', dispatch=[('loops_comparison', bints+'fd')]),\n-          [TypeDescription('O', FullTypeDescr, 'OO', 'O')],\n+          TD(bints, out='?'),\n+          [TypeDescription('q', FullTypeDescr, 'qQ', '?'),\n+           TypeDescription('q', FullTypeDescr, 'Qq', '?')],\n+          TD(inexact+times, out='?', dispatch=[('loops_comparison', bints+'fd')]),\n           TD('O', out='?'),\n+          [TypeDescription('O', FullTypeDescr, 'OO', 'O')],\n           ),\n 'less_equal':\n     Ufunc(2, 1, None,\n           docstrings.get('numpy.core.umath.less_equal'),\n           'PyUFunc_SimpleBinaryComparisonTypeResolver',\n-          TD(all, out='?', dispatch=[('loops_comparison', bints+'fd')]),\n-          [TypeDescription('O', FullTypeDescr, 'OO', 'O')],\n+          TD(bints, out='?'),\n+          [TypeDescription('q', FullTypeDescr, 'qQ', '?'),\n+           TypeDescription('q', FullTypeDescr, 'Qq', '?')],\n+          TD(inexact+times, out='?', dispatch=[('loops_comparison', bints+'fd')]),\n           TD('O', out='?'),\n+          [TypeDescription('O', FullTypeDescr, 'OO', 'O')],\n           ),\n 'equal':\n     Ufunc(2, 1, None,\n           docstrings.get('numpy.core.umath.equal'),\n           'PyUFunc_SimpleBinaryComparisonTypeResolver',\n-          TD(all, out='?', dispatch=[('loops_comparison', bints+'fd')]),\n-          [TypeDescription('O', FullTypeDescr, 'OO', 'O')],\n+          TD(bints, out='?'),\n+          [TypeDescription('q', FullTypeDescr, 'qQ', '?'),\n+           TypeDescription('q', FullTypeDescr, 'Qq', '?')],\n+          TD(inexact+times, out='?', dispatch=[('loops_comparison', bints+'fd')]),\n           TD('O', out='?'),\n+          [TypeDescription('O', FullTypeDescr, 'OO', 'O')],\n           ),\n 'not_equal':\n     Ufunc(2, 1, None,\n           docstrings.get('numpy.core.umath.not_equal'),\n           'PyUFunc_SimpleBinaryComparisonTypeResolver',\n-          TD(all, out='?', dispatch=[('loops_comparison', bints+'fd')]),\n-          [TypeDescription('O', FullTypeDescr, 'OO', 'O')],\n+          TD(bints, out='?'),\n+          [TypeDescription('q', FullTypeDescr, 'qQ', '?'),\n+           TypeDescription('q', FullTypeDescr, 'Qq', '?')],\n+          TD(inexact+times, out='?', dispatch=[('loops_comparison', bints+'fd')]),\n           TD('O', out='?'),\n+          [TypeDescription('O', FullTypeDescr, 'OO', 'O')],\n           ),\n 'logical_and':\n     Ufunc(2, 1, True_,\n@@ -1172,7 +1195,10 @@ def make_arrays(funcdict):\n             if t.func_data is FullTypeDescr:\n                 tname = english_upper(chartoname[t.type])\n                 datalist.append('(void *)NULL')\n-                cfunc_fname = f\"{tname}_{t.in_}_{t.out}_{cfunc_alias}\"\n+                if t.out == \"?\":\n+                    cfunc_fname = f\"{tname}_{t.in_}_bool_{cfunc_alias}\"\n+                else:\n+                    cfunc_fname = f\"{tname}_{t.in_}_{t.out}_{cfunc_alias}\"\n             elif isinstance(t.func_data, FuncNameSuffix):\n                 datalist.append('(void *)NULL')\n                 tname = english_upper(chartoname[t.type])\n",
            "comment_added_diff": {
                "113": "        # Explicitly allow this mixed case, rather than figure out what order",
                "114": "        # is nicer or how to encode it."
            },
            "comment_deleted_diff": {},
            "comment_modified_diff": {}
        }
    ],
    "_globals.py": [
        {
            "commit": "7a98e6c18c2985dceca0667693f570864e85c159",
            "timestamp": "2022-10-24T19:23:15+02:00",
            "author": "Dimitri Papadopoulos",
            "commit_message": "MAINT: remove code specific to Python 2",
            "additions": 0,
            "deletions": 4,
            "change_type": "MODIFY",
            "diff": "@@ -83,10 +83,6 @@ def __new__(cls):\n             cls.__instance = super().__new__(cls)\n         return cls.__instance\n \n-    # needed for python 2 to preserve identity through a pickle\n-    def __reduce__(self):\n-        return (self.__class__, ())\n-\n     def __repr__(self):\n         return \"<no value>\"\n \n",
            "comment_added_diff": {},
            "comment_deleted_diff": {
                "86": "    # needed for python 2 to preserve identity through a pickle"
            },
            "comment_modified_diff": {}
        }
    ],
    "bench_io.py": [
        {
            "commit": "13d55a3c2f016a58a6e9d6b8086f338e07c7478f",
            "timestamp": "2022-10-26T10:02:37-07:00",
            "author": "Mike Taves",
            "commit_message": "MAINT: remove u-prefix for former Unicode strings (#22479)",
            "additions": 16,
            "deletions": 16,
            "change_type": "MODIFY",
            "diff": "@@ -75,12 +75,12 @@ class LoadtxtCSVComments(Benchmark):\n     param_names = ['num_lines']\n \n     def setup(self, num_lines):\n-        data = [u'1,2,3 # comment'] * num_lines\n+        data = ['1,2,3 # comment'] * num_lines\n         # unfortunately, timeit will only run setup()\n         # between repeat events, but not for iterations\n         # within repeats, so the StringIO object\n         # will have to be rewinded in the benchmark proper\n-        self.data_comments = StringIO(u'\\n'.join(data))\n+        self.data_comments = StringIO('\\n'.join(data))\n \n     def time_comment_loadtxt_csv(self, num_lines):\n         # benchmark handling of lines with comments\n@@ -93,7 +93,7 @@ def time_comment_loadtxt_csv(self, num_lines):\n         # confounding timing result somewhat) for every\n         # call to timing test proper\n         np.loadtxt(self.data_comments,\n-                   delimiter=u',')\n+                   delimiter=',')\n         self.data_comments.seek(0)\n \n class LoadtxtCSVdtypes(Benchmark):\n@@ -106,8 +106,8 @@ class LoadtxtCSVdtypes(Benchmark):\n     param_names = ['dtype', 'num_lines']\n \n     def setup(self, dtype, num_lines):\n-        data = [u'5, 7, 888'] * num_lines\n-        self.csv_data = StringIO(u'\\n'.join(data))\n+        data = ['5, 7, 888'] * num_lines\n+        self.csv_data = StringIO('\\n'.join(data))\n \n     def time_loadtxt_dtypes_csv(self, dtype, num_lines):\n         # benchmark loading arrays of various dtypes\n@@ -117,7 +117,7 @@ def time_loadtxt_dtypes_csv(self, dtype, num_lines):\n         # rewind of StringIO object\n \n         np.loadtxt(self.csv_data,\n-                   delimiter=u',',\n+                   delimiter=',',\n                    dtype=dtype)\n         self.csv_data.seek(0)\n \n@@ -127,15 +127,15 @@ class LoadtxtCSVStructured(Benchmark):\n \n     def setup(self):\n         num_lines = 50000\n-        data = [u\"M, 21, 72, X, 155\"] * num_lines\n-        self.csv_data = StringIO(u'\\n'.join(data))\n+        data = [\"M, 21, 72, X, 155\"] * num_lines\n+        self.csv_data = StringIO('\\n'.join(data))\n \n     def time_loadtxt_csv_struct_dtype(self):\n         # obligate rewind of StringIO object\n         # between iterations of a repeat:\n \n         np.loadtxt(self.csv_data,\n-                   delimiter=u',',\n+                   delimiter=',',\n                    dtype=[('category_1', 'S1'),\n                           ('category_2', 'i4'),\n                           ('category_3', 'f8'),\n@@ -174,10 +174,10 @@ class LoadtxtReadUint64Integers(Benchmark):\n \n     def setup(self, size):\n         arr = np.arange(size).astype('uint64') + 2**63\n-        self.data1 = StringIO(u'\\n'.join(arr.astype(str).tolist()))\n+        self.data1 = StringIO('\\n'.join(arr.astype(str).tolist()))\n         arr = arr.astype(object)\n         arr[500] = -1\n-        self.data2 = StringIO(u'\\n'.join(arr.astype(str).tolist()))\n+        self.data2 = StringIO('\\n'.join(arr.astype(str).tolist()))\n \n     def time_read_uint64(self, size):\n         # mandatory rewind of StringIO object\n@@ -200,14 +200,14 @@ class LoadtxtUseColsCSV(Benchmark):\n \n     def setup(self, usecols):\n         num_lines = 5000\n-        data = [u'0, 1, 2, 3, 4, 5, 6, 7, 8, 9'] * num_lines\n-        self.csv_data = StringIO(u'\\n'.join(data))\n+        data = ['0, 1, 2, 3, 4, 5, 6, 7, 8, 9'] * num_lines\n+        self.csv_data = StringIO('\\n'.join(data))\n \n     def time_loadtxt_usecols_csv(self, usecols):\n         # must rewind StringIO because of state\n         # dependence of file reading\n         np.loadtxt(self.csv_data,\n-                   delimiter=u',',\n+                   delimiter=',',\n                    usecols=usecols)\n         self.csv_data.seek(0)\n \n@@ -225,7 +225,7 @@ def setup(self, num_lines):\n         dates = np.arange('today', 20, dtype=np.datetime64)\n         np.random.seed(123)\n         values = np.random.rand(20)\n-        date_line = u''\n+        date_line = ''\n \n         for date, value in zip(dates, values):\n             date_line += (str(date) + ',' + str(value) + '\\n')\n@@ -238,7 +238,7 @@ def time_loadtxt_csv_datetime(self, num_lines):\n         # rewind StringIO object -- the timing iterations\n         # are state-dependent\n         X = np.loadtxt(self.csv_data,\n-                       delimiter=u',',\n+                       delimiter=',',\n                        dtype=([('dates', 'M8[us]'),\n                                ('values', 'float64')]))\n         self.csv_data.seek(0)\n",
            "comment_added_diff": {
                "78": "        data = ['1,2,3 # comment'] * num_lines"
            },
            "comment_deleted_diff": {
                "78": "        data = [u'1,2,3 # comment'] * num_lines"
            },
            "comment_modified_diff": {
                "78": "        data = [u'1,2,3 # comment'] * num_lines"
            }
        }
    ],
    "conf.py": [
        {
            "commit": "4121134b21e39fa1bc488b58d4324b3f9ba05a04",
            "timestamp": "2023-01-18T21:07:19+01:00",
            "author": "Sebastian Berg",
            "commit_message": "DOC: Ignore attribute FutureWarning in doc build\n\nThe docs somehow seem to access `numpy.str`, etc. during the autodoc\ngeneration probably.\nThis adds a specific warning filter to hide it.\n\nCloses gh-22987",
            "additions": 7,
            "deletions": 0,
            "change_type": "MODIFY",
            "diff": "@@ -62,6 +62,13 @@ class PyTypeObject(ctypes.Structure):\n \n replace_scalar_type_names()\n \n+\n+# As of NumPy 1.25, a deprecation of `str`/`bytes` attributes happens.\n+# For some reasons, the doc build accesses these, so ignore them.\n+import warnings\n+warnings.filterwarnings(\"ignore\", \"In the future.*NumPy scalar\", FutureWarning)\n+\n+\n # -----------------------------------------------------------------------------\n # General configuration\n # -----------------------------------------------------------------------------\n",
            "comment_added_diff": {
                "66": "# As of NumPy 1.25, a deprecation of `str`/`bytes` attributes happens.",
                "67": "# For some reasons, the doc build accesses these, so ignore them."
            },
            "comment_deleted_diff": {},
            "comment_modified_diff": {}
        }
    ],
    "test_api.py": [
        {
            "commit": "f57879ebf622dc5b8ae0f06bdca041893904114e",
            "timestamp": "2023-01-04T02:19:18-08:00",
            "author": "Developer-Ecosystem-Engineering",
            "commit_message": "resolve additional platform test failures\n\nSpecial case SSE\nFix PPC64 build\nOnly use vqtbl4q_u8 on A64\nStop trying to use optimizations on s390x",
            "additions": 10,
            "deletions": 0,
            "change_type": "MODIFY",
            "diff": "@@ -102,6 +102,16 @@ def test_array_array():\n     assert_raises(ValueError, np.array, [nested], dtype=np.float64)\n \n     # Try with lists...\n+    # float32\n+    assert_equal(np.array([None] * 10, dtype=np.float32),\n+                 np.full((10,), np.nan, dtype=np.float32))\n+    assert_equal(np.array([[None]] * 10, dtype=np.float32),\n+                 np.full((10, 1), np.nan, dtype=np.float32))\n+    assert_equal(np.array([[None] * 10], dtype=np.float32),\n+                 np.full((1, 10), np.nan, dtype=np.float32))\n+    assert_equal(np.array([[None] * 10] * 10, dtype=np.float32),\n+                 np.full((10, 10), np.nan, dtype=np.float32))\n+    # float64\n     assert_equal(np.array([None] * 10, dtype=np.float64),\n                  np.full((10,), np.nan, dtype=np.float64))\n     assert_equal(np.array([[None]] * 10, dtype=np.float64),\n",
            "comment_added_diff": {
                "105": "    # float32",
                "114": "    # float64"
            },
            "comment_deleted_diff": {},
            "comment_modified_diff": {}
        },
        {
            "commit": "03443bd69d43d7b7a39f466b6ddee78db38a751b",
            "timestamp": "2023-06-08T08:45:25+02:00",
            "author": "Sebastian Berg",
            "commit_message": "API: Change string to bool conversions to be consistent with Python\n\nThis changes casts from strings to bool to use the length (and not\ngo via integers).\nFurther, `nonzero` behavior was taking into account spaces previously.\n\nThere is for users to silently always getting `False`\nnow if they used the integer conversion logic (making their code\nwrong).",
            "additions": 13,
            "deletions": 22,
            "change_type": "MODIFY",
            "diff": "@@ -321,30 +321,21 @@ def test_array_astype_warning(t):\n @pytest.mark.parametrize([\"dtype\", \"out_dtype\"],\n         [(np.bytes_, np.bool_),\n          (np.str_, np.bool_),\n-         (np.dtype(\"S10,S9\"), np.dtype(\"?,?\"))])\n+         (np.dtype(\"S10,S9\"), np.dtype(\"?,?\")),\n+         # The following also checks unaligned unicode access:\n+         (np.dtype(\"S7,U9\"), np.dtype(\"?,?\"))])\n def test_string_to_boolean_cast(dtype, out_dtype):\n-    \"\"\"\n-    Currently, for `astype` strings are cast to booleans effectively by\n-    calling `bool(int(string)`. This is not consistent (see gh-9875) and\n-    will eventually be deprecated.\n-    \"\"\"\n-    arr = np.array([\"10\", \"10\\0\\0\\0\", \"0\\0\\0\", \"0\"], dtype=dtype)\n-    expected = np.array([True, True, False, False], dtype=out_dtype)\n+    # Only the last two (empty) strings are falsy (the `\\0` is stripped):\n+    arr = np.array(\n+            [\"10\", \"10\\0\\0\\0\", \"0\\0\\0\", \"0\", \"False\", \" \", \"\", \"\\0\"],\n+            dtype=dtype)\n+    expected = np.array(\n+            [True, True, True, True, True, True, False, False],\n+            dtype=out_dtype)\n     assert_array_equal(arr.astype(out_dtype), expected)\n-\n-@pytest.mark.parametrize([\"dtype\", \"out_dtype\"],\n-        [(np.bytes_, np.bool_),\n-         (np.str_, np.bool_),\n-         (np.dtype(\"S10,S9\"), np.dtype(\"?,?\"))])\n-def test_string_to_boolean_cast_errors(dtype, out_dtype):\n-    \"\"\"\n-    These currently error out, since cast to integers fails, but should not\n-    error out in the future.\n-    \"\"\"\n-    for invalid in [\"False\", \"True\", \"\", \"\\0\", \"non-empty\"]:\n-        arr = np.array([invalid], dtype=dtype)\n-        with assert_raises(ValueError):\n-            arr.astype(out_dtype)\n+    # As it's similar, check that nonzero behaves the same (structs are\n+    # nonzero if all entries are)\n+    assert_array_equal(np.nonzero(arr), np.nonzero(expected))\n \n @pytest.mark.parametrize(\"str_type\", [str, bytes, np.str_, np.unicode_])\n @pytest.mark.parametrize(\"scalar_type\",\n",
            "comment_added_diff": {
                "325": "         # The following also checks unaligned unicode access:",
                "328": "    # Only the last two (empty) strings are falsy (the `\\0` is stripped):",
                "336": "    # As it's similar, check that nonzero behaves the same (structs are",
                "337": "    # nonzero if all entries are)"
            },
            "comment_deleted_diff": {},
            "comment_modified_diff": {
                "328": "    calling `bool(int(string)`. This is not consistent (see gh-9875) and",
                "336": "        [(np.bytes_, np.bool_),",
                "337": "         (np.str_, np.bool_),"
            }
        },
        {
            "commit": "2a914572bb35d065a798d1e186358b5d51edd2e5",
            "timestamp": "2023-06-23T10:18:56-06:00",
            "author": "Nathan Goldbaum",
            "commit_message": "TST: add test for empty astype call",
            "additions": 3,
            "deletions": 0,
            "change_type": "MODIFY",
            "diff": "@@ -284,6 +284,9 @@ class MyNDArray(np.ndarray):\n     a = np.array(1000, dtype='i4')\n     assert_raises(TypeError, a.astype, 'U1', casting='safe')\n \n+    # gh-24023\n+    assert_raises(TypeError, a.astype)\n+\n @pytest.mark.parametrize(\"dt\", [\"S\", \"U\"])\n def test_array_astype_to_string_discovery_empty(dt):\n     # See also gh-19085\n",
            "comment_added_diff": {
                "287": "    # gh-24023"
            },
            "comment_deleted_diff": {},
            "comment_modified_diff": {}
        },
        {
            "commit": "b7dc58d89b04c46b5f4873fef35a277591a3bbf0",
            "timestamp": "2023-08-01T12:23:04+02:00",
            "author": "Sebastian Berg",
            "commit_message": "DEP: Expire copyandtranspose (C and Python) and ScalarFromObject",
            "additions": 0,
            "deletions": 15,
            "change_type": "MODIFY",
            "diff": "@@ -160,21 +160,6 @@ def test_array_impossible_casts(array):\n         np.array(rt, dtype=\"M8\")\n \n \n-# TODO: remove when fastCopyAndTranspose deprecation expires\n-@pytest.mark.parametrize(\"a\",\n-    (\n-        np.array(2),  # 0D array\n-        np.array([3, 2, 7, 0]),  # 1D array\n-        np.arange(6).reshape(2, 3)  # 2D array\n-    ),\n-)\n-def test_fastCopyAndTranspose(a):\n-    with pytest.deprecated_call():\n-        b = np.fastCopyAndTranspose(a)\n-        assert_equal(b, a.T)\n-        assert b.flags.owndata\n-\n-\n def test_array_astype():\n     a = np.arange(6, dtype='f4').reshape(2, 3)\n     # Default behavior: allows unsafe casts, keeps memory layout,\n",
            "comment_added_diff": {},
            "comment_deleted_diff": {
                "163": "# TODO: remove when fastCopyAndTranspose deprecation expires",
                "166": "        np.array(2),  # 0D array",
                "167": "        np.array([3, 2, 7, 0]),  # 1D array",
                "168": "        np.arange(6).reshape(2, 3)  # 2D array"
            },
            "comment_modified_diff": {}
        }
    ],
    "test_arrayprint.py": [
        {
            "commit": "071388f957c13c1a4f03bc811e3d128335ac686e",
            "timestamp": "2023-03-14T13:57:27+01:00",
            "author": "molsonkiko",
            "commit_message": "ENH: show dtype in array repr when endianness is non-native (#23295)\n\nFx problem where, for example,\r\n\r\nnp.array([1], dtype='>u2') and np.array([1], dtype='<u2')\r\n\r\nboth got represented as np.array([1], dtype=uint16), or the dtype is not shown for the default ones (float64, default int).",
            "additions": 42,
            "deletions": 0,
            "change_type": "MODIFY",
            "diff": "@@ -9,6 +9,7 @@\n     assert_, assert_equal, assert_raises, assert_warns, HAS_REFCOUNT,\n     assert_raises_regex,\n     )\n+from numpy.core.arrayprint import _typelessdata\n import textwrap\n \n class TestArrayRepr:\n@@ -796,6 +797,47 @@ def test_dtype_linewidth_wrapping(self):\n             array(['1', '1', '1', '1', '1', '1', '1', '1', '1', '1', '1', '1'],\n                   dtype='{}')\"\"\".format(styp)))\n \n+    @pytest.mark.parametrize(\n+        ['native'],\n+        [\n+            ('bool',),\n+            ('uint8',),\n+            ('uint16',),\n+            ('uint32',),\n+            ('uint64',),\n+            ('int8',),\n+            ('int16',),\n+            ('int32',),\n+            ('int64',),\n+            ('float16',),\n+            ('float32',),\n+            ('float64',),\n+            ('U1',),     # 4-byte width string\n+        ],\n+    )\n+    def test_dtype_endianness_repr(self, native):\n+        '''\n+        there was an issue where \n+        repr(array([0], dtype='<u2')) and repr(array([0], dtype='>u2'))\n+        both returned the same thing:\n+        array([0], dtype=uint16)\n+        even though their dtypes have different endianness.\n+        '''\n+        native_dtype = np.dtype(native)\n+        non_native_dtype = native_dtype.newbyteorder()\n+        non_native_repr = repr(np.array([1], non_native_dtype))\n+        native_repr = repr(np.array([1], native_dtype))\n+        # preserve the sensible default of only showing dtype if nonstandard\n+        assert ('dtype' in native_repr) ^ (native_dtype in _typelessdata),\\\n+                (\"an array's repr should show dtype if and only if the type \"\n+                 'of the array is NOT one of the standard types '\n+                 '(e.g., int32, bool, float64).')\n+        if non_native_dtype.itemsize > 1:\n+            # if the type is >1 byte, the non-native endian version\n+            # must show endianness.\n+            assert non_native_repr != native_repr\n+            assert f\"dtype='{non_native_dtype.byteorder}\" in non_native_repr\n+\n     def test_linewidth_repr(self):\n         a = np.full(7, fill_value=2)\n         np.set_printoptions(linewidth=17)\n",
            "comment_added_diff": {
                "815": "            ('U1',),     # 4-byte width string",
                "830": "        # preserve the sensible default of only showing dtype if nonstandard",
                "836": "            # if the type is >1 byte, the non-native endian version",
                "837": "            # must show endianness."
            },
            "comment_deleted_diff": {},
            "comment_modified_diff": {}
        },
        {
            "commit": "46785ee50101de9ac13ebf1231e195212cc6cdd6",
            "timestamp": "2023-07-18T11:50:24+02:00",
            "author": "Sebastian Berg",
            "commit_message": "Fixup tests, repr, and guard against using full repr in array printing",
            "additions": 8,
            "deletions": 7,
            "change_type": "MODIFY",
            "diff": "@@ -326,7 +326,7 @@ def test_unstructured_void_repr(self):\n             r\"       b'\\x1B\\x5B\\x33\\x31\\x6D\\x52\\x65\\x64'], dtype='|V8')\")\n \n         assert_equal(eval(repr(a), vars(np)), a)\n-        assert_equal(eval(repr(a[0]), vars(np)), a[0])\n+        assert_equal(eval(repr(a[0]), dict(np=np)), a[0])\n \n     def test_edgeitems_kwarg(self):\n         # previously the global print options would be taken over the kwarg\n@@ -505,6 +505,7 @@ def test_any_text(self, text):\n         a = np.array([text, text, text])\n         # casting a list of them to an array does not e.g. truncate the value\n         assert_equal(a[0], text)\n+        text = text.item()  # use raw python strings for repr below\n         # and that np.array2string puts a newline in the expected location\n         expected_repr = \"[{0!r} {0!r}\\n {0!r}]\".format(text)\n         result = np.array2string(a, max_line_width=len(repr(text)) * 2 + 3)\n@@ -966,12 +967,12 @@ def test_edgeitems(self):\n             repr(a),\n             textwrap.dedent(\"\"\"\\\n             array([[[ 0, ...,  2],\n-                    ...,\n+                    ..., \n                     [ 6, ...,  8]],\n \n-                   ...,\n+                   ..., \n                    [[18, ..., 20],\n-                    ...,\n+                    ..., \n                     [24, ..., 26]]])\"\"\")\n         )\n \n@@ -980,14 +981,14 @@ def test_edgeitems(self):\n             textwrap.dedent(\"\"\"\\\n             array([[[[ 0.]],\n \n-                    ...,\n+                    ..., \n                     [[ 0.]]],\n \n \n-                   ...,\n+                   ..., \n                    [[[ 0.]],\n \n-                    ...,\n+                    ..., \n                     [[ 0.]]]])\"\"\")\n         )\n \n",
            "comment_added_diff": {
                "508": "        text = text.item()  # use raw python strings for repr below"
            },
            "comment_deleted_diff": {},
            "comment_modified_diff": {}
        },
        {
            "commit": "768fcf0b9d588648270d841397c6cf81648c9320",
            "timestamp": "2023-07-18T11:50:24+02:00",
            "author": "Sebastian Berg",
            "commit_message": "TST,MAINT: Add tests for both new and old scalar repr and fix complex",
            "additions": 45,
            "deletions": 0,
            "change_type": "MODIFY",
            "diff": "@@ -1049,3 +1049,48 @@ def test_ctx_mgr_as_smth(self):\n         with np.printoptions(**opts) as ctx:\n             saved_opts = ctx.copy()\n         assert_equal({k: saved_opts[k] for k in opts}, opts)\n+\n+\n+@pytest.mark.parametrize(\"dtype\", \"bhilqpBHILQPefdgFDG\")\n+@pytest.mark.parametrize(\"value\", [0, 1])\n+def test_scalar_repr_numbers(dtype, value):\n+    # Test NEP 51 scalar repr (and legacy option) for numeric types\n+    dtype = np.dtype(dtype)\n+    scalar = np.array(value, dtype=dtype)[()]\n+    assert isinstance(scalar, np.generic)\n+\n+    string = str(scalar)\n+    repr_string = string.strip(\"()\")  # complex may have extra brackets\n+    representation = repr(scalar)\n+    if dtype.char == \"g\":\n+        assert representation == f\"np.longdouble('{repr_string}')\"\n+    elif dtype.char == 'G':\n+        assert representation == f\"np.clongdouble('{repr_string}')\"\n+    else:\n+        normalized_name = np.dtype(f\"{dtype.kind}{dtype.itemsize}\").type.__name__\n+        assert representation == f\"np.{normalized_name}({repr_string})\"\n+\n+    np.set_printoptions(legacy=\"1.25\")\n+    assert repr(scalar) == string\n+    np.set_printoptions(legacy=False)\n+\n+\n+@pytest.mark.parametrize(\"scalar, legacy_repr,representation\", [\n+        (np.True_, \"True\", \"np.True_\"),\n+        (np.bytes_(b'a'), \"b'a'\", \"np.bytes_(b'a')\"),\n+        (np.str_('a'), \"'a'\", \"np.str_('a')\"),\n+        (np.datetime64(\"2012\"),\n+            \"numpy.datetime64('2012')\", \"np.datetime64('2012')\"),\n+        (np.timedelta64(1), \"numpy.timedelta64(1)\", \"np.timedelta64(1)\"),\n+        (np.void((True, 2), dtype=\"?,i8\"),\n+            \"(True, 2)\",\n+            \"np.void((True, 2), dtype=[('f0', '?'), ('f1', '<i8')])\"),\n+        (np.void(b'a'), r\"void(b'\\x61')\", r\"np.void(b'\\x61')\"),\n+    ])\n+def test_scalar_repr_special(scalar, legacy_repr, representation):\n+    # Test NEP 51 scalar repr (and legacy option) for numeric types\n+    assert repr(scalar) == representation\n+\n+    np.set_printoptions(legacy=\"1.25\")\n+    assert repr(scalar) == legacy_repr\n+    np.set_printoptions(legacy=False)\n",
            "comment_added_diff": {
                "1057": "    # Test NEP 51 scalar repr (and legacy option) for numeric types",
                "1063": "    repr_string = string.strip(\"()\")  # complex may have extra brackets",
                "1091": "    # Test NEP 51 scalar repr (and legacy option) for numeric types"
            },
            "comment_deleted_diff": {},
            "comment_modified_diff": {}
        },
        {
            "commit": "d624b62750032d6735984aded26a9b7d8dbda706",
            "timestamp": "2023-07-27T09:16:55+02:00",
            "author": "Sebastian Berg",
            "commit_message": "BUG: Move legacy check for void printing\n\nThe check needs to be in the python path, because also the printing\nfor `str()` subtly changed.\n\nTo be the same as tuples, we should actually print `(np.float32(3.), np.int8(1))`\nfor `str()` (tuples include the repr), while for `repr()` we include the dtype\nso we would print similar (but ideally with full precision) to arrays.\n\nThis isn't quite ideal, I would be happy to print the full repr in the `str()`\nbut I guess that might be a bit annoying in practice, so maybe our Numeric types\nare special enough for now.",
            "additions": 10,
            "deletions": 0,
            "change_type": "MODIFY",
            "diff": "@@ -1084,6 +1084,9 @@ def test_scalar_repr_numbers(dtype, value):\n         (np.void((True, 2), dtype=\"?,<i8\"),\n             \"(True, 2)\",\n             \"np.void((True, 2), dtype=[('f0', '?'), ('f1', '<i8')])\"),\n+        (np.void((1, 2), dtype=\"<f8,>f4\"),\n+            \"(1., 2.)\",\n+            \"np.void((1.0, 2.0), dtype=[('f0', '<f8'), ('f1', '>f4')])\"),\n         (np.void(b'a'), r\"void(b'\\x61')\", r\"np.void(b'\\x61')\"),\n     ])\n def test_scalar_repr_special(scalar, legacy_repr, representation):\n@@ -1092,3 +1095,10 @@ def test_scalar_repr_special(scalar, legacy_repr, representation):\n \n     with np.printoptions(legacy=\"1.25\"):\n         assert repr(scalar) == legacy_repr\n+\n+def test_scalar_void_float_str():\n+    # Note that based on this currently we do not print the same as a tuple\n+    # would, since the tuple would include the repr() inside for floats, but\n+    # we do not do that.\n+    scalar = np.void((1.0, 2.0), dtype=[('f0', '<f8'), ('f1', '>f4')])\n+    assert str(scalar) == \"(1.0, 2.0)\"\n",
            "comment_added_diff": {
                "1100": "    # Note that based on this currently we do not print the same as a tuple",
                "1101": "    # would, since the tuple would include the repr() inside for floats, but",
                "1102": "    # we do not do that."
            },
            "comment_deleted_diff": {},
            "comment_modified_diff": {}
        },
        {
            "commit": "db083e779b7a08e994778134428510fee04fa31e",
            "timestamp": "2023-07-27T13:15:21-04:00",
            "author": "paulreece",
            "commit_message": "BUG: array2string does not add signs for positive integers. Fixes #24181.",
            "additions": 87,
            "deletions": 0,
            "change_type": "MODIFY",
            "diff": "@@ -524,6 +524,93 @@ def test_refcount(self):\n         gc.enable()\n         assert_(r1 == r2)\n \n+    def test_with_sign(self):\n+        # mixed negative and positive value array\n+        a = np.array([-2, 0, 3])\n+        assert_equal(\n+            np.array2string(a, sign='+'),\n+            '[-2 +0 +3]'\n+        )\n+        assert_equal(\n+            np.array2string(a, sign='-'),\n+            '[-2  0  3]'\n+        )\n+        assert_equal(\n+            np.array2string(a, sign=' '),\n+            '[-2  0  3]'\n+        )\n+        # all non-negative array\n+        a = np.array([2, 0, 3])\n+        assert_equal(\n+            np.array2string(a, sign='+'),\n+            '[+2 +0 +3]'\n+        )\n+        assert_equal(\n+            np.array2string(a, sign='-'),\n+            '[2 0 3]'\n+        )\n+        assert_equal(\n+            np.array2string(a, sign=' '),\n+            '[ 2  0  3]'\n+        )\n+        # all negative array\n+        a = np.array([-2, -1, -3])\n+        assert_equal(\n+            np.array2string(a, sign='+'),\n+            '[-2 -1 -3]'\n+        )\n+        assert_equal(\n+            np.array2string(a, sign='-'),\n+            '[-2 -1 -3]'\n+        )\n+        assert_equal(\n+            np.array2string(a, sign=' '),\n+            '[-2 -1 -3]'\n+        )\n+        # 2d array mixed negative and positive\n+        a = np.array([[10, -1, 1, 1], [10, 10, 10, 10]])\n+        assert_equal(\n+            np.array2string(a, sign='+'),\n+            '[[+10  -1  +1  +1]\\n [+10 +10 +10 +10]]'\n+        )\n+        assert_equal(\n+            np.array2string(a, sign='-'),\n+            '[[10 -1  1  1]\\n [10 10 10 10]]'\n+        )\n+        assert_equal(\n+            np.array2string(a, sign=' '),\n+            '[[10 -1  1  1]\\n [10 10 10 10]]'\n+        )\n+        # 2d array all positive\n+        a = np.array([[10, 0, 1, 1], [10, 10, 10, 10]])\n+        assert_equal(\n+            np.array2string(a, sign='+'),\n+            '[[+10  +0  +1  +1]\\n [+10 +10 +10 +10]]'\n+        )\n+        assert_equal(\n+            np.array2string(a, sign='-'),\n+            '[[10  0  1  1]\\n [10 10 10 10]]'\n+        )\n+        assert_equal(\n+            np.array2string(a, sign=' '),\n+            '[[ 10   0   1   1]\\n [ 10  10  10  10]]'\n+        )\n+        # 2d array all negative\n+        a = np.array([[-10, -1, -1, -1], [-10, -10, -10, -10]])\n+        assert_equal(\n+            np.array2string(a, sign='+'),\n+            '[[-10  -1  -1  -1]\\n [-10 -10 -10 -10]]'\n+        )\n+        assert_equal(\n+            np.array2string(a, sign='-'),\n+            '[[-10  -1  -1  -1]\\n [-10 -10 -10 -10]]'\n+        )\n+        assert_equal(\n+            np.array2string(a, sign=' '),\n+            '[[-10  -1  -1  -1]\\n [-10 -10 -10 -10]]'\n+        )\n+\n+\n class TestPrintOptions:\n     \"\"\"Test getting and setting global print options.\"\"\"\n \n",
            "comment_added_diff": {
                "528": "        # mixed negative and positive value array",
                "542": "        # all non-negative array",
                "556": "        # all negative array",
                "570": "        # 2d array mixed negative and positive",
                "584": "        # 2d array all positive",
                "598": "        # 2d array all negative"
            },
            "comment_deleted_diff": {},
            "comment_modified_diff": {}
        }
    ],
    "test_defchararray.py": [
        {
            "commit": "737b0647e712853d2b7defd944dde7e4b10cd224",
            "timestamp": "2023-01-10T17:55:51+01:00",
            "author": "Nathan Goldbaum",
            "commit_message": "ENH: allow NEP 42 dtypes to work with np.char (#22863)\n\nThis makes it possible for new-style NEP 42 string dtypes like ASCIIDType to work with the functions in np.char, this has leads to some mild modification (stricter behavior in bad paths).\r\n\r\nIt will only work with dtypes with a scalar that subclasses str or bytes. I also assume that you can create instances of the user dtype from python like dtype_instance = CustomDType(size_in_bytes). This is a pretty big assumption about the API of the dtype, I'm not sure offhand how I can do this more portably or more safely.\r\n\r\nI also added a new macro, NPY_DT_is_user_defined, which checks dtype->type_num == -1, which is currently true for all custom dtypes using the experimental dtype API. This new macro is needed because NPY_DT_is_legacy will return false for np.void.\r\n\r\nThis is only tested via the user dtypes currently.",
            "additions": 14,
            "deletions": 0,
            "change_type": "MODIFY",
            "diff": "@@ -1,3 +1,5 @@\n+import pytest\n+\n import numpy as np\n from numpy.core.multiarray import _vec_string\n from numpy.testing import (\n@@ -670,3 +672,15 @@ def test_empty_indexing():\n     # empty chararray instead of a chararray with a single empty string in it.\n     s = np.chararray((4,))\n     assert_(s[[]].size == 0)\n+\n+\n+@pytest.mark.parametrize([\"dt1\", \"dt2\"],\n+        [(\"S\", \"U\"), (\"U\", \"S\"), (\"S\", \"O\"), (\"U\", \"O\"),\n+         (\"S\", \"d\"), (\"S\", \"V\")])\n+def test_add_types(dt1, dt2):\n+    arr1 = np.array([1234234], dtype=dt1)\n+    # If the following fails, e.g. use a number and test \"V\" explicitly\n+    arr2 = np.array([b\"423\"], dtype=dt2)\n+    with pytest.raises(TypeError,\n+            match=f\".*same dtype kind.*{arr1.dtype}.*{arr2.dtype}\"):\n+        np.char.add(arr1, arr2)\n",
            "comment_added_diff": {
                "682": "    # If the following fails, e.g. use a number and test \"V\" explicitly"
            },
            "comment_deleted_diff": {},
            "comment_modified_diff": {}
        },
        {
            "commit": "b49a6df5c5bff39caade0dfc56915597cd107136",
            "timestamp": "2023-10-06T12:56:17+02:00",
            "author": "Lysandros Nikolaou",
            "commit_message": "Address feedback",
            "additions": 0,
            "deletions": 12,
            "change_type": "MODIFY",
            "diff": "@@ -678,15 +678,3 @@ def test_empty_indexing():\n     # empty chararray instead of a chararray with a single empty string in it.\n     s = np.char.chararray((4,))\n     assert_(s[[]].size == 0)\n-\n-\n-@pytest.mark.parametrize([\"dt1\", \"dt2\"],\n-        [(\"S\", \"U\"), (\"U\", \"S\"), (\"S\", \"O\"), (\"U\", \"O\"),\n-         (\"S\", \"d\"), (\"S\", \"V\")])\n-def test_add_types(dt1, dt2):\n-    arr1 = np.array([1234234], dtype=dt1)\n-    # If the following fails, e.g. use a number and test \"V\" explicitly\n-    arr2 = np.array([b\"423\"], dtype=dt2)\n-    with pytest.raises(TypeError,\n-            match=f\".*same dtype kind.*{arr1.dtype}.*{arr2.dtype}\"):\n-        np.char.add(arr1, arr2)\n",
            "comment_added_diff": {},
            "comment_deleted_diff": {
                "688": "    # If the following fails, e.g. use a number and test \"V\" explicitly"
            },
            "comment_modified_diff": {}
        }
    ],
    "test_dtype.py": [
        {
            "commit": "71a924ee7f71f1ea14af3aee0a6be29353a996d4",
            "timestamp": "2022-11-25T16:58:13+01:00",
            "author": "Sebastian Berg",
            "commit_message": "BUG: Ensure string aliases `\"int0\"`, etc. remain valid for now\n\nint0 and uint0 were accidentally dropped, the others just added as\na test.\nWe did successfully remove many Numeric types before, so these could\nprobably be deprecated (it is a bit more annoying to do).\n\nThese could probably just be removed, scikit-learn only noticed it in\na single test (scipy probably not at all).  But maybe not in 1.24",
            "additions": 9,
            "deletions": 0,
            "change_type": "MODIFY",
            "diff": "@@ -126,6 +126,15 @@ def test_numeric_style_types_are_invalid(self, dtype):\n         with assert_raises(TypeError):\n             np.dtype(dtype)\n \n+    def test_remaining_dtypes_with_bad_bytesize(self):\n+        # These should probably deprecated as well, the np.<name> aliases were\n+        assert np.dtype(\"int0\") is np.dtype(\"intp\")\n+        assert np.dtype(\"uint0\") is np.dtype(\"uintp\")\n+        assert np.dtype(\"bool8\") is np.dtype(\"bool\")\n+        assert np.dtype(\"bytes0\") is np.dtype(\"bytes\")\n+        assert np.dtype(\"str0\") is np.dtype(\"str\")\n+        assert np.dtype(\"object0\") is np.dtype(\"object\")\n+\n     @pytest.mark.parametrize(\n         'value',\n         ['m8', 'M8', 'datetime64', 'timedelta64',\n",
            "comment_added_diff": {
                "130": "        # These should probably deprecated as well, the np.<name> aliases were"
            },
            "comment_deleted_diff": {},
            "comment_modified_diff": {}
        },
        {
            "commit": "6b606389b2747a455ba8c23bdc3ed6b1ed8c000b",
            "timestamp": "2022-11-27T18:42:05+01:00",
            "author": "Sebastian Berg",
            "commit_message": "Update numpy/core/tests/test_dtype.py\n\nCo-authored-by: Matti Picus <matti.picus@gmail.com>",
            "additions": 1,
            "deletions": 1,
            "change_type": "MODIFY",
            "diff": "@@ -127,7 +127,7 @@ def test_numeric_style_types_are_invalid(self, dtype):\n             np.dtype(dtype)\n \n     def test_remaining_dtypes_with_bad_bytesize(self):\n-        # These should probably deprecated as well, the np.<name> aliases were\n+        # The np.<name> aliases were deprecated, these probably should be too \n         assert np.dtype(\"int0\") is np.dtype(\"intp\")\n         assert np.dtype(\"uint0\") is np.dtype(\"uintp\")\n         assert np.dtype(\"bool8\") is np.dtype(\"bool\")\n",
            "comment_added_diff": {
                "130": "        # The np.<name> aliases were deprecated, these probably should be too"
            },
            "comment_deleted_diff": {
                "130": "        # These should probably deprecated as well, the np.<name> aliases were"
            },
            "comment_modified_diff": {
                "130": "        # These should probably deprecated as well, the np.<name> aliases were"
            }
        },
        {
            "commit": "3b5ba53b645f486319ec181871525b76393e5b75",
            "timestamp": "2023-01-05T10:58:30+01:00",
            "author": "Nathan Goldbaum",
            "commit_message": "ENH: Create string dtype instances from the abstract dtype (#22923)\n\nFollowing up from #22863 (comment), this makes it possible to create string dtype instances from an abstract string DTypeMeta.\r\n\r\nCo-authored-by: Sebastian Berg <sebastianb@nvidia.com>",
            "additions": 28,
            "deletions": 0,
            "change_type": "MODIFY",
            "diff": "@@ -195,6 +195,34 @@ def test_field_order_equality(self):\n         # This is an safe cast (not equiv) due to the different names:\n         assert np.can_cast(x, y, casting=\"safe\")\n \n+    @pytest.mark.parametrize(\n+        [\"type_char\", \"char_size\", \"scalar_type\"],\n+        [[\"U\", 4, np.str_],\n+         [\"S\", 1, np.bytes_]])\n+    def test_create_string_dtypes_directly(\n+            self, type_char, char_size, scalar_type):\n+        dtype_class = type(np.dtype(type_char))\n+\n+        dtype = dtype_class(8)\n+        assert dtype.type is scalar_type\n+        assert dtype.itemsize == 8*char_size\n+\n+    def test_create_invalid_string_errors(self):\n+        one_too_big = np.iinfo(np.intc).max + 1\n+        with pytest.raises(TypeError):\n+            type(np.dtype(\"U\"))(one_too_big // 4)\n+\n+        with pytest.raises(TypeError):\n+            # Code coverage for very large numbers:\n+            type(np.dtype(\"U\"))(np.iinfo(np.intp).max // 4 + 1)\n+\n+        if one_too_big < sys.maxsize:\n+            with pytest.raises(TypeError):\n+                type(np.dtype(\"S\"))(one_too_big)\n+\n+        with pytest.raises(ValueError):\n+            type(np.dtype(\"U\"))(-1)\n+\n \n class TestRecord:\n     def test_equivalent_record(self):\n",
            "comment_added_diff": {
                "216": "            # Code coverage for very large numbers:"
            },
            "comment_deleted_diff": {},
            "comment_modified_diff": {}
        },
        {
            "commit": "770fe81cc2969d36af4bd1576ce5b69fcb03ffbf",
            "timestamp": "2023-02-25T14:28:33+01:00",
            "author": "Sebastian Berg",
            "commit_message": "BUG: Allow no-op clearing of void dtypes\n\nSome void dtypes think they contain objects but don't.  Instead\nof playing whack-a-mole to see if that can be fixed, simply make\nthe clearing a no-op here for them.\nUser dtypes are weirder, it should be OK to pass through.\n\nFixes the error message and use write-unraisable.\n\nCloses gh-23277",
            "additions": 8,
            "deletions": 0,
            "change_type": "MODIFY",
            "diff": "@@ -551,6 +551,14 @@ def test_fieldless_views(self):\n         assert_equal(np.zeros((1, 2), dtype=[]) == a,\n                      np.ones((1, 2), dtype=bool))\n \n+    def test_nonstructured_with_object(self):\n+        # See gh-23277, the dtype here thinks it contain objects, if the\n+        # assert about that fails, the test becomes meaningless (which is OK)\n+        arr = np.recarray((0,), dtype=\"O\") \n+        assert arr.dtype.names is None  # no fields\n+        assert arr.dtype.hasobject  # but claims to contain objects\n+        del arr  # the deletion failed previously.\n+\n \n class TestSubarray:\n     def test_single_subarray(self):\n",
            "comment_added_diff": {
                "555": "        # See gh-23277, the dtype here thinks it contain objects, if the",
                "556": "        # assert about that fails, the test becomes meaningless (which is OK)",
                "558": "        assert arr.dtype.names is None  # no fields",
                "559": "        assert arr.dtype.hasobject  # but claims to contain objects",
                "560": "        del arr  # the deletion failed previously."
            },
            "comment_deleted_diff": {},
            "comment_modified_diff": {}
        },
        {
            "commit": "95da78dcf89aee1aec0db01e127314511b7dc282",
            "timestamp": "2023-04-12T12:21:41+02:00",
            "author": "Sebastian Berg",
            "commit_message": "TST: Update test due to windows Int vs intc name difference",
            "additions": 6,
            "deletions": 2,
            "change_type": "MODIFY",
            "diff": "@@ -1565,9 +1565,13 @@ def test_basic_dtypes_subclass_properties(self, dtype):\n         assert isinstance(dtype, np.dtype)\n         assert type(dtype) is not np.dtype\n         if dtype.type.__name__ != \"rational\":\n-            dt_name = type(dtype).__name__\n+            dt_name = type(dtype).__name__.lower().removesuffix(\"dtype\")\n+            if dt_name == \"uint\" or dt_name == \"int\":\n+                # The scalar names has a `c` attached because \"int\" is Python\n+                # int and that is long...\n+                dt_name += \"c\"\n             sc_name = dtype.type.__name__\n-            assert dt_name.lower().removesuffix(\"dtype\") == sc_name.strip(\"_\")\n+            assert dt_name == sc_name.strip(\"_\")\n             assert type(dtype).__module__ == \"numpy.types\"\n \n             assert getattr(numpy.types, type(dtype).__name__) is type(dtype)\n",
            "comment_added_diff": {
                "1570": "                # The scalar names has a `c` attached because \"int\" is Python",
                "1571": "                # int and that is long..."
            },
            "comment_deleted_diff": {},
            "comment_modified_diff": {
                "1570": "            assert dt_name.lower().removesuffix(\"dtype\") == sc_name.strip(\"_\")"
            }
        },
        {
            "commit": "c30e4b511ae601d9e8e1b172d77175ca9b557a69",
            "timestamp": "2023-07-02T22:26:31-07:00",
            "author": "Victor Tang",
            "commit_message": "BUG: Fix empty structured array dtype alignment\n\nThis change ensures that dtypes are given an alignment of at least 1.\nPrior to this change, dtypes of empty structured arrays with `align` set\nto True would have an alignment of 0.\n\nFor instance, `np.dtype([], align=True).alignment` returns 0. Empty\nstructured array dtypes should have an alignment of 1, meaning that they\ndo not need padding. An alignment of 0 is problematic because the\nalignment is used to determine offsets, which involves dividing by the\nalignment. If the alignment is 0, then a division by zero occurs.\n\nThe following code raises a ZeroDivisionError when converting the dtype\nto a string representation:\n\n```python\nrepr(np.dtype([('f0', [])], align=True))\n```\n\nThe following code crashes Python because a division by zero occurs\nin the C code:\n\n```python\nnp.dtype({'names':['f0'], 'formats':[[]], 'offsets': [0]}, align=True)\n```",
            "additions": 15,
            "deletions": 0,
            "change_type": "MODIFY",
            "diff": "@@ -348,6 +348,21 @@ def test_aligned_size(self):\n                                  ('b', [('f0', '<i2'), ('', '|V2'),\n                                  ('f1', '<f4')], (2,))])\n \n+    def test_empty_struct_alignment(self):\n+        # Empty dtypes should have an alignment of 1\n+        dt = np.dtype([], align=True)\n+        assert_equal(dt.alignment, 1)\n+        dt = np.dtype([('f0', [])], align=True)\n+        assert_equal(dt.alignment, 1)\n+        dt = np.dtype({'names': [],\n+                       'formats': [],\n+                       'offsets': []}, align=True)\n+        assert_equal(dt.alignment, 1)\n+        dt = np.dtype({'names': ['f0'],\n+                       'formats': [[]],\n+                       'offsets': [0]}, align=True)\n+        assert_equal(dt.alignment, 1)\n+\n     def test_union_struct(self):\n         # Should be able to create union dtypes\n         dt = np.dtype({'names':['f0', 'f1', 'f2'], 'formats':['<u4', '<u2', '<u2'],\n",
            "comment_added_diff": {
                "352": "        # Empty dtypes should have an alignment of 1"
            },
            "comment_deleted_diff": {},
            "comment_modified_diff": {}
        },
        {
            "commit": "241d67678189872a25ebc0e56a07ad4254bcef9b",
            "timestamp": "2023-09-29T15:47:11+02:00",
            "author": "Mateusz Sok\u00f3\u0142",
            "commit_message": "API: Remove zero names from dtype aliases (#24807)",
            "additions": 16,
            "deletions": 8,
            "change_type": "MODIFY",
            "diff": "@@ -126,14 +126,22 @@ def test_numeric_style_types_are_invalid(self, dtype):\n         with assert_raises(TypeError):\n             np.dtype(dtype)\n \n-    def test_remaining_dtypes_with_bad_bytesize(self):\n-        # The np.<name> aliases were deprecated, these probably should be too \n-        assert np.dtype(\"int0\") is np.dtype(\"intp\")\n-        assert np.dtype(\"uint0\") is np.dtype(\"uintp\")\n-        assert np.dtype(\"bool8\") is np.dtype(\"bool\")\n-        assert np.dtype(\"bytes0\") is np.dtype(\"bytes\")\n-        assert np.dtype(\"str0\") is np.dtype(\"str\")\n-        assert np.dtype(\"object0\") is np.dtype(\"object\")\n+    def test_expired_dtypes_with_bad_bytesize(self):\n+        match: str = r\".*removed in NumPy 2.0.*\"\n+        with pytest.raises(TypeError, match=match):\n+            np.dtype(\"int0\")\n+        with pytest.raises(TypeError, match=match):\n+            np.dtype(\"uint0\")\n+        with pytest.raises(TypeError, match=match):\n+            np.dtype(\"bool8\")\n+        with pytest.raises(TypeError, match=match):\n+            np.dtype(\"bytes0\")\n+        with pytest.raises(TypeError, match=match):\n+            np.dtype(\"str0\")\n+        with pytest.raises(TypeError, match=match):\n+            np.dtype(\"object0\")\n+        with pytest.raises(TypeError, match=match):\n+            np.dtype(\"void0\")\n \n     @pytest.mark.parametrize(\n         'value',\n",
            "comment_added_diff": {},
            "comment_deleted_diff": {
                "130": "        # The np.<name> aliases were deprecated, these probably should be too"
            },
            "comment_modified_diff": {}
        }
    ],
    "test_einsum.py": [
        {
            "commit": "e1e487acf1d820cbab8a6f97986bf2fb451dfa8e",
            "timestamp": "2023-02-11T22:46:28+01:00",
            "author": "Dimitri Papadopoulos",
            "commit_message": "Fix typos found by copdespell",
            "additions": 1,
            "deletions": 1,
            "change_type": "MODIFY",
            "diff": "@@ -755,7 +755,7 @@ def test_different_paths(self, dtype):\n         # Test originally added to cover broken float16 path: gh-20305\n         # Likely most are covered elsewhere, at least partially.\n         dtype = np.dtype(dtype)\n-        # Simple test, designed to excersize most specialized code paths,\n+        # Simple test, designed to exercise most specialized code paths,\n         # note the +0.5 for floats.  This makes sure we use a float value\n         # where the results must be exact.\n         arr = (np.arange(7) + 0.5).astype(dtype)\n",
            "comment_added_diff": {
                "758": "        # Simple test, designed to exercise most specialized code paths,"
            },
            "comment_deleted_diff": {
                "758": "        # Simple test, designed to excersize most specialized code paths,"
            },
            "comment_modified_diff": {
                "758": "        # Simple test, designed to excersize most specialized code paths,"
            }
        },
        {
            "commit": "53f7b55cefa7207240e8bf5b8ec0f661c7b36491",
            "timestamp": "2023-03-29T14:10:51+03:00",
            "author": "iamsoto",
            "commit_message": "PR_fixes_1",
            "additions": 94,
            "deletions": 24,
            "change_type": "MODIFY",
            "diff": "@@ -100,6 +100,69 @@ def test_einsum_errors(self):\n             assert_raises(ValueError, np.einsum, \"i->i\", np.arange(6).reshape(-1, 1),\n                           optimize=do_opt, order='d')\n \n+    def test_einsum_object_errors(self):\n+        # Exceptions created by object arithmetic should\n+        # successfully propogate\n+\n+        class CustomException(Exception):\n+            pass\n+\n+        class DestructoBox:\n+\n+            def __init__(self, value, destruct):\n+                self._val = value\n+                self._destruct = destruct\n+\n+            def __add__(self, other):\n+                tmp = self._val + other._val\n+                if tmp >= self._destruct:\n+                    raise CustomException\n+                else:\n+                    self._val = tmp\n+                    return self\n+\n+            def __radd__(self, other):\n+                if other == 0:\n+                    return self\n+                else:\n+                    return self.__add__(other)\n+\n+            def __mul__(self, other):\n+                tmp = self._val * other._val\n+                if tmp >= self._destruct:\n+                    raise CustomException\n+                else:\n+                    self._val = tmp\n+                    return self\n+\n+            def __rmul__(self, other):\n+                if other == 0:\n+                    return self\n+                else:\n+                    return self.__mul__(other)\n+\n+        a = np.array([DestructoBox(i, 5) for i in range(1, 10)],\n+                     dtype='object').reshape(3, 3)\n+\n+        # raised from unbuffered_loop_nop1_ndim2\n+        assert_raises(CustomException, np.einsum, \"ij->i\", a)\n+\n+        # raised from unbuffered_loop_nop1_ndim3\n+        b = np.array([DestructoBox(i, 100) for i in range(0, 27)],\n+                     dtype='object').reshape(3, 3, 3)\n+        assert_raises(CustomException, np.einsum, \"i...k->...\", b)\n+\n+        # raised from unbuffered_loop_nop2_ndim2\n+        b = np.array([DestructoBox(i, 55) for i in range(1, 4)],\n+                     dtype='object')\n+        assert_raises(CustomException, np.einsum, \"ij, j\", a, b)\n+\n+        # raised from unbuffered_loop_nop2_ndim3\n+        assert_raises(CustomException, np.einsum, \"ij, jh\", a, a)\n+\n+        # raised from PyArray_EinsteinSum\n+        assert_raises(CustomException, np.einsum, \"ij->\", a)\n+\n     def test_einsum_views(self):\n         # pass-through\n         for do_opt in [True, False]:\n@@ -247,47 +310,50 @@ def check_einsum_sums(self, dtype, do_opt=False):\n         # sum(a, axis=-1)\n         for n in range(1, 17):\n             a = np.arange(n, dtype=dtype)\n-            assert_equal(np.einsum(\"i->\", a, optimize=do_opt),\n-                         np.sum(a, axis=-1).astype(dtype))\n-            assert_equal(np.einsum(a, [0], [], optimize=do_opt),\n-                         np.sum(a, axis=-1).astype(dtype))\n+            b = np.sum(a, axis=-1)\n+            if hasattr(b, 'astype'):\n+                b = b.astype(dtype)\n+            assert_equal(np.einsum(\"i->\", a, optimize=do_opt), b)\n+            assert_equal(np.einsum(a, [0], [], optimize=do_opt), b)\n \n         for n in range(1, 17):\n             a = np.arange(2*3*n, dtype=dtype).reshape(2, 3, n)\n-            assert_equal(np.einsum(\"...i->...\", a, optimize=do_opt),\n-                         np.sum(a, axis=-1).astype(dtype))\n-            assert_equal(np.einsum(a, [Ellipsis, 0], [Ellipsis], optimize=do_opt),\n-                         np.sum(a, axis=-1).astype(dtype))\n+            b = np.sum(a, axis=-1)\n+            if hasattr(b, 'astype'):\n+                b = b.astype(dtype)\n+            assert_equal(np.einsum(\"...i->...\", a, optimize=do_opt), b)\n+            assert_equal(np.einsum(a, [Ellipsis, 0], [Ellipsis], optimize=do_opt), b)\n \n         # sum(a, axis=0)\n         for n in range(1, 17):\n             a = np.arange(2*n, dtype=dtype).reshape(2, n)\n-            assert_equal(np.einsum(\"i...->...\", a, optimize=do_opt),\n-                         np.sum(a, axis=0).astype(dtype))\n-            assert_equal(np.einsum(a, [0, Ellipsis], [Ellipsis], optimize=do_opt),\n-                         np.sum(a, axis=0).astype(dtype))\n+            b = np.sum(a, axis=0)\n+            if hasattr(b, 'astype'):\n+                b = b.astype(dtype)\n+            assert_equal(np.einsum(\"i...->...\", a, optimize=do_opt), b)\n+            assert_equal(np.einsum(a, [0, Ellipsis], [Ellipsis], optimize=do_opt), b)\n \n         for n in range(1, 17):\n             a = np.arange(2*3*n, dtype=dtype).reshape(2, 3, n)\n-            assert_equal(np.einsum(\"i...->...\", a, optimize=do_opt),\n-                         np.sum(a, axis=0).astype(dtype))\n-            assert_equal(np.einsum(a, [0, Ellipsis], [Ellipsis], optimize=do_opt),\n-                         np.sum(a, axis=0).astype(dtype))\n+            b = np.sum(a, axis=0)\n+            if hasattr(b, 'astype'):\n+                b = b.astype(dtype)\n+            assert_equal(np.einsum(\"i...->...\", a, optimize=do_opt), b)\n+            assert_equal(np.einsum(a, [0, Ellipsis], [Ellipsis], optimize=do_opt), b)\n \n         # trace(a)\n         for n in range(1, 17):\n             a = np.arange(n*n, dtype=dtype).reshape(n, n)\n-            assert_equal(np.einsum(\"ii\", a, optimize=do_opt),\n-                         np.trace(a).astype(dtype))\n-            assert_equal(np.einsum(a, [0, 0], optimize=do_opt),\n-                         np.trace(a).astype(dtype))\n+            b = np.trace(a)\n+            if hasattr(b, 'astype'):\n+                b = b.astype(dtype)\n+            assert_equal(np.einsum(\"ii\", a, optimize=do_opt), b)\n+            assert_equal(np.einsum(a, [0, 0], optimize=do_opt), b)\n \n             # gh-15961: should accept numpy int64 type in subscript list\n             np_array = np.asarray([0, 0])\n-            assert_equal(np.einsum(a, np_array, optimize=do_opt),\n-                         np.trace(a).astype(dtype))\n-            assert_equal(np.einsum(a, list(np_array), optimize=do_opt),\n-                         np.trace(a).astype(dtype))\n+            assert_equal(np.einsum(a, np_array, optimize=do_opt), b)\n+            assert_equal(np.einsum(a, list(np_array), optimize=do_opt), b)\n \n         # multiply(a, b)\n         assert_equal(np.einsum(\"..., ...\", 3, 4), 12)  # scalar case\n@@ -587,6 +653,10 @@ def test_einsum_sums_cfloat128(self):\n     def test_einsum_sums_clongdouble(self):\n         self.check_einsum_sums(np.clongdouble)\n \n+    def test_einsum_sums_object(self):\n+        self.check_einsum_sums('object')\n+        self.check_einsum_sums('object', True)\n+\n     def test_einsum_misc(self):\n         # This call used to crash because of a bug in\n         # PyArray_AssignZero\n",
            "comment_added_diff": {
                "104": "        # Exceptions created by object arithmetic should",
                "105": "        # successfully propogate",
                "147": "        # raised from unbuffered_loop_nop1_ndim2",
                "150": "        # raised from unbuffered_loop_nop1_ndim3",
                "155": "        # raised from unbuffered_loop_nop2_ndim2",
                "160": "        # raised from unbuffered_loop_nop2_ndim3",
                "163": "        # raised from PyArray_EinsteinSum"
            },
            "comment_deleted_diff": {},
            "comment_modified_diff": {}
        },
        {
            "commit": "2538eb7e67ac7fed0bb7cf1a862194ffb9471a0e",
            "timestamp": "2023-03-30T10:55:55+03:00",
            "author": "mattip",
            "commit_message": "TST: fix tests for einsum returning a python object",
            "additions": 6,
            "deletions": 2,
            "change_type": "MODIFY",
            "diff": "@@ -555,11 +555,15 @@ def check_einsum_sums(self, dtype, do_opt=False):\n \n         b = np.einsum(\"i->\", a, dtype=dtype, casting='unsafe')\n         assert_equal(b, np.sum(a))\n-        assert_equal(b.dtype, np.dtype(dtype))\n+        if hasattr(b, \"dtype\"):\n+            # Can be a python object when dtype is object\n+            assert_equal(b.dtype, np.dtype(dtype))\n \n         b = np.einsum(a, [0], [], dtype=dtype, casting='unsafe')\n         assert_equal(b, np.sum(a))\n-        assert_equal(b.dtype, np.dtype(dtype))\n+        if hasattr(b, \"dtype\"):\n+            # Can be a python object when dtype is object\n+            assert_equal(b.dtype, np.dtype(dtype))\n \n         # A case which was failing (ticket #1885)\n         p = np.arange(2) + 1\n",
            "comment_added_diff": {
                "559": "            # Can be a python object when dtype is object",
                "565": "            # Can be a python object when dtype is object"
            },
            "comment_deleted_diff": {},
            "comment_modified_diff": {}
        },
        {
            "commit": "f9ff49e9daf20c2f1acf716b07d9c8d340240317",
            "timestamp": "2023-06-18T18:30:24+02:00",
            "author": "Dimitri Papadopoulos",
            "commit_message": "MAINT: Fix typos found by codespell",
            "additions": 1,
            "deletions": 1,
            "change_type": "MODIFY",
            "diff": "@@ -102,7 +102,7 @@ def test_einsum_errors(self):\n \n     def test_einsum_object_errors(self):\n         # Exceptions created by object arithmetic should\n-        # successfully propogate\n+        # successfully propagate\n \n         class CustomException(Exception):\n             pass\n",
            "comment_added_diff": {
                "105": "        # successfully propagate"
            },
            "comment_deleted_diff": {
                "105": "        # successfully propogate"
            },
            "comment_modified_diff": {
                "105": "        # successfully propogate"
            }
        }
    ],
    "test_numerictypes.py": [
        {
            "commit": "075859216fae0509c52a54cb5c96c217f23026ca",
            "timestamp": "2022-11-17T14:21:54+01:00",
            "author": "Sebastian Berg",
            "commit_message": "DEP: Next step in scalar type alias deprecations/futurewarnings\n\nFinalizes the scalar type alias deprecations making them an error.\nHowever, at the same time adds a `FutureWarning` that new aliases\nwill be introduced in the future.\n(They would eventually be preferred over the `str_`, etc. version.)\n\nIt may make sense, that this FutureWarning is already propelled soon\nsince it interacts with things such as changing the representation of\nstrings to `np.str_(\"\")` if the preferred alias becomes `np.str`.\n\nIt also introduces a new deprecation to remove the 0 sized bit-aliases\nand the bitsize `bool8` alias.  (Unfortunately, these are here still allowed\nas part of the `np.sctypeDict`).",
            "additions": 3,
            "deletions": 2,
            "change_type": "MODIFY",
            "diff": "@@ -443,8 +443,9 @@ def test_ulong(self):\n         # alias for np.int_, but np.long is not supported for historical\n         # reasons (gh-21063)\n         assert_(np.sctypeDict['ulong'] is np.uint)\n-        assert_(not hasattr(np, 'ulong'))\n-\n+        with pytest.warns(FutureWarning):\n+            # We will probably allow this in the future:\n+            assert not hasattr(np, 'ulong')\n \n class TestBitName:\n     def test_abstract(self):\n",
            "comment_added_diff": {
                "447": "            # We will probably allow this in the future:"
            },
            "comment_deleted_diff": {},
            "comment_modified_diff": {
                "447": ""
            }
        },
        {
            "commit": "6cb136ee909b2b3909a202ac94b09c89c8dae4dd",
            "timestamp": "2023-08-24T22:48:39+02:00",
            "author": "Mateusz Sok\u00f3\u0142",
            "commit_message": "API: Remove other scalar aliases [skip ci]",
            "additions": 9,
            "deletions": 9,
            "change_type": "MODIFY",
            "diff": "@@ -440,14 +440,14 @@ def test_nondtype_nonscalartype(self):\n \n class TestSctypeDict:\n     def test_longdouble(self):\n-        assert_(np.sctypeDict['f8'] is not np.longdouble)\n-        assert_(np.sctypeDict['c16'] is not np.clongdouble)\n+        assert_(np.core.sctypeDict['f8'] is not np.longdouble)\n+        assert_(np.core.sctypeDict['c16'] is not np.clongdouble)\n \n     def test_ulong(self):\n-        # Test that 'ulong' behaves like 'long'. np.sctypeDict['long'] is an\n-        # alias for np.int_, but np.long is not supported for historical\n+        # Test that 'ulong' behaves like 'long'. np.core.sctypeDict['long'] \n+        # is an alias for np.int_, but np.long is not supported for historical\n         # reasons (gh-21063)\n-        assert_(np.sctypeDict['ulong'] is np.uint)\n+        assert_(np.core.sctypeDict['ulong'] is np.uint)\n         with pytest.warns(FutureWarning):\n             # We will probably allow this in the future:\n             assert not hasattr(np, 'ulong')\n@@ -466,19 +466,19 @@ class TestMaximumSctype:\n \n     @pytest.mark.parametrize('t', [np.byte, np.short, np.intc, np.int_, np.longlong])\n     def test_int(self, t):\n-        assert_equal(np.maximum_sctype(t), np.sctypes['int'][-1])\n+        assert_equal(np.maximum_sctype(t), np.core.sctypes['int'][-1])\n \n     @pytest.mark.parametrize('t', [np.ubyte, np.ushort, np.uintc, np.uint, np.ulonglong])\n     def test_uint(self, t):\n-        assert_equal(np.maximum_sctype(t), np.sctypes['uint'][-1])\n+        assert_equal(np.maximum_sctype(t), np.core.sctypes['uint'][-1])\n \n     @pytest.mark.parametrize('t', [np.half, np.single, np.double, np.longdouble])\n     def test_float(self, t):\n-        assert_equal(np.maximum_sctype(t), np.sctypes['float'][-1])\n+        assert_equal(np.maximum_sctype(t), np.core.sctypes['float'][-1])\n \n     @pytest.mark.parametrize('t', [np.csingle, np.cdouble, np.clongdouble])\n     def test_complex(self, t):\n-        assert_equal(np.maximum_sctype(t), np.sctypes['complex'][-1])\n+        assert_equal(np.maximum_sctype(t), np.core.sctypes['complex'][-1])\n \n     @pytest.mark.parametrize('t', [np.bool_, np.object_, np.str_, np.bytes_,\n                                    np.void])\n",
            "comment_added_diff": {
                "447": "        # Test that 'ulong' behaves like 'long'. np.core.sctypeDict['long']",
                "448": "        # is an alias for np.int_, but np.long is not supported for historical"
            },
            "comment_deleted_diff": {
                "447": "        # Test that 'ulong' behaves like 'long'. np.sctypeDict['long'] is an",
                "448": "        # alias for np.int_, but np.long is not supported for historical"
            },
            "comment_modified_diff": {
                "447": "        # Test that 'ulong' behaves like 'long'. np.sctypeDict['long'] is an",
                "448": "        # alias for np.int_, but np.long is not supported for historical"
            }
        },
        {
            "commit": "b43384e8f9f7242c59985c4a3d687c95a2a9dbf4",
            "timestamp": "2023-08-30T09:34:12+02:00",
            "author": "Mateusz Sok\u00f3\u0142",
            "commit_message": "MAINT: Remove deprecated functions [NEP 52] (#24477)",
            "additions": 0,
            "deletions": 25,
            "change_type": "MODIFY",
            "diff": "@@ -338,31 +338,6 @@ def test_assign(self):\n         assert_(a['int'].shape == (5, 0))\n         assert_(a['float'].shape == (5, 2))\n \n-class TestCommonType:\n-    def test_scalar_loses1(self):\n-        with pytest.warns(DeprecationWarning, match=\"np.find_common_type\"):\n-            res = np.find_common_type(['f4', 'f4', 'i2'], ['f8'])\n-        assert_(res == 'f4')\n-\n-    def test_scalar_loses2(self):\n-        with pytest.warns(DeprecationWarning, match=\"np.find_common_type\"):\n-            res = np.find_common_type(['f4', 'f4'], ['i8'])\n-        assert_(res == 'f4')\n-\n-    def test_scalar_wins(self):\n-        with pytest.warns(DeprecationWarning, match=\"np.find_common_type\"):\n-            res = np.find_common_type(['f4', 'f4', 'i2'], ['c8'])\n-        assert_(res == 'c8')\n-\n-    def test_scalar_wins2(self):\n-        with pytest.warns(DeprecationWarning, match=\"np.find_common_type\"):\n-            res = np.find_common_type(['u4', 'i4', 'i4'], ['f4'])\n-        assert_(res == 'f8')\n-\n-    def test_scalar_wins3(self):  # doesn't go up to 'f16' on purpose\n-        with pytest.warns(DeprecationWarning, match=\"np.find_common_type\"):\n-            res = np.find_common_type(['u8', 'i8', 'i8'], ['f8'])\n-        assert_(res == 'f8')\n \n class TestMultipleFields:\n     def setup_method(self):\n",
            "comment_added_diff": {},
            "comment_deleted_diff": {
                "362": "    def test_scalar_wins3(self):  # doesn't go up to 'f16' on purpose"
            },
            "comment_modified_diff": {}
        }
    ],
    "test_regression.py": [
        {
            "commit": "84e4fff7df0fcc6bf6417074937e7c080fd96678",
            "timestamp": "2022-12-01T14:38:28+01:00",
            "author": "Sebastian Berg",
            "commit_message": "TST: Adapt test for modification to == and != deprecation/futurewarning",
            "additions": 1,
            "deletions": 4,
            "change_type": "MODIFY",
            "diff": "@@ -128,10 +128,7 @@ def test_scalar_compare(self):\n         assert_(a[1] == 'auto')\n         assert_(a[0] != 'auto')\n         b = np.linspace(0, 10, 11)\n-        # This should return true for now, but will eventually raise an error:\n-        with suppress_warnings() as sup:\n-            sup.filter(FutureWarning)\n-            assert_(b != 'auto')\n+        assert_array_equal(b != 'auto', np.ones(11, dtype=bool))\n         assert_(b[0] != 'auto')\n \n     def test_unicode_swapping(self):\n",
            "comment_added_diff": {},
            "comment_deleted_diff": {
                "131": "        # This should return true for now, but will eventually raise an error:"
            },
            "comment_modified_diff": {}
        },
        {
            "commit": "b0919fe2bd9b0e85f65c2ace7c364e6e408f0a73",
            "timestamp": "2022-12-21T15:10:05-08:00",
            "author": "Developer-Ecosystem-Engineering",
            "commit_message": "Add regression test for gh-22845",
            "additions": 11,
            "deletions": 0,
            "change_type": "MODIFY",
            "diff": "@@ -2553,3 +2553,14 @@ def get_idx(string, str_lst):\n                 f\"Unexpected types order of ufunc in {operation}\"\n                 f\"for {order}. Possible fix: Use signed before unsigned\"\n                 \"in generate_umath.py\")\n+\n+    def test_nonbool_logical(self):\n+        # gh-22845\n+        # create two arrays with bit patterns that do not overlap.\n+        # needs to be large enough to test both SIMD and scalar paths\n+        size = 100\n+        a = np.frombuffer(b'\\x01' * size, dtype=np.bool_)\n+        b = np.frombuffer(b'\\x80' * size, dtype=np.bool_)\n+        expected = np.ones(size, dtype=np.bool_)\n+        assert_array_equal(np.logical_and(a, b), expected)\n+\n",
            "comment_added_diff": {
                "2558": "        # gh-22845",
                "2559": "        # create two arrays with bit patterns that do not overlap.",
                "2560": "        # needs to be large enough to test both SIMD and scalar paths"
            },
            "comment_deleted_diff": {},
            "comment_modified_diff": {}
        },
        {
            "commit": "9fe3554b33ed1c3e7c8c78a8ac97322c0a16f7cb",
            "timestamp": "2023-03-03T16:16:08-07:00",
            "author": "Tyler Reddy",
            "commit_message": "BUG: ma with structured dtype\n\nFixes #22041\n\n* add regression test and fix for creating a masked array with a\nstructured dtype; the test is simply for lack of error in the repoducer\n\n* the concern expressed by core team in matching issue was that\n`astropy` might be negatively affected; I ran full `astropy` (hash: `c9ad7c56`)\ntest suite locally with this feature branch and it seemed \"ok,\"\njust 1 unrelated network failure in the network-requiring tests\n(`test_ftp_tls_auto`):\n\n```1 failed, 21430 passed, 3490 skipped, 176 xfailed, 23275 warnings in\n430.18s (0:07:10)```",
            "additions": 6,
            "deletions": 0,
            "change_type": "MODIFY",
            "diff": "@@ -89,3 +89,9 @@ def test_empty_list_on_structured(self):\n     def test_masked_array_tobytes_fortran(self):\n         ma = np.ma.arange(4).reshape((2,2))\n         assert_array_equal(ma.tobytes(order='F'), ma.T.tobytes())\n+\n+    def test_structured_array(self):\n+        # see gh-22041\n+        np.ma.array((1, (b\"\", b\"\")),\n+                    dtype=[(\"x\", np.int_),\n+                          (\"y\", [(\"i\", np.void), (\"j\", np.void)])])\n",
            "comment_added_diff": {
                "94": "        # see gh-22041"
            },
            "comment_deleted_diff": {},
            "comment_modified_diff": {}
        },
        {
            "commit": "ceb78f2220def0c62419615cd1a544bd5d7285d3",
            "timestamp": "2023-08-16T11:02:42+02:00",
            "author": "Pieter Eendebak",
            "commit_message": "DEP: Remove deprecated numpy.who (#24321)\n\n[skip ci]",
            "additions": 0,
            "deletions": 17,
            "change_type": "MODIFY",
            "diff": "@@ -158,23 +158,6 @@ def test_void_coercion(self):\n         x = np.zeros((1,), dt)\n         assert_(np.r_[x, x].dtype == dt)\n \n-    @pytest.mark.filterwarnings(\"ignore:.*who.*:DeprecationWarning\")\n-    def test_who_with_0dim_array(self):\n-        # ticket #1243\n-        import os\n-        import sys\n-\n-        oldstdout = sys.stdout\n-        sys.stdout = open(os.devnull, 'w')\n-        try:\n-            try:\n-                np.who({'foo': np.array(1)})\n-            except Exception:\n-                raise AssertionError(\"ticket #1243\")\n-        finally:\n-            sys.stdout.close()\n-            sys.stdout = oldstdout\n-\n     def test_include_dirs(self):\n         # As a sanity check, just test that get_include\n         # includes something reasonable.  Somewhat\n",
            "comment_added_diff": {},
            "comment_deleted_diff": {
                "163": "        # ticket #1243",
                "173": "                raise AssertionError(\"ticket #1243\")"
            },
            "comment_modified_diff": {}
        },
        {
            "commit": "6cb136ee909b2b3909a202ac94b09c89c8dae4dd",
            "timestamp": "2023-08-24T22:48:39+02:00",
            "author": "Mateusz Sok\u00f3\u0142",
            "commit_message": "API: Remove other scalar aliases [skip ci]",
            "additions": 17,
            "deletions": 4,
            "change_type": "MODIFY",
            "diff": "@@ -129,10 +129,6 @@ def test_ndenumerate_crash(self):\n         # Shouldn't crash:\n         list(np.ndenumerate(np.array([[]])))\n \n-    def test_asfarray_none(self):\n-        # Test for changeset r5065\n-        assert_array_equal(np.array([np.nan]), np.asfarray([None]))\n-\n     def test_large_fancy_indexing(self):\n         # Large enough to fail on 64-bit.\n         nbits = np.dtype(np.intp).itemsize * 8\n@@ -158,6 +154,23 @@ def test_void_coercion(self):\n         x = np.zeros((1,), dt)\n         assert_(np.r_[x, x].dtype == dt)\n \n+    @pytest.mark.filterwarnings(\"ignore:.*who.*:DeprecationWarning\")\n+    def test_who_with_0dim_array(self):\n+        # ticket #1243\n+        import os\n+        import sys\n+\n+        oldstdout = sys.stdout\n+        sys.stdout = open(os.devnull, 'w')\n+        try:\n+            try:\n+                np.lib.utils.who({'foo': np.array(1)})\n+            except Exception:\n+                raise AssertionError(\"ticket #1243\")\n+        finally:\n+            sys.stdout.close()\n+            sys.stdout = oldstdout\n+\n     def test_include_dirs(self):\n         # As a sanity check, just test that get_include\n         # includes something reasonable.  Somewhat\n",
            "comment_added_diff": {
                "159": "        # ticket #1243",
                "169": "                raise AssertionError(\"ticket #1243\")"
            },
            "comment_deleted_diff": {
                "133": "        # Test for changeset r5065"
            },
            "comment_modified_diff": {}
        },
        {
            "commit": "67d8d2c38ac7e59b28495b7fff8389691909a0b4",
            "timestamp": "2023-08-24T22:52:53+02:00",
            "author": "Mateusz Sok\u00f3\u0142",
            "commit_message": "API: Remove cfloat usage",
            "additions": 0,
            "deletions": 17,
            "change_type": "MODIFY",
            "diff": "@@ -154,23 +154,6 @@ def test_void_coercion(self):\n         x = np.zeros((1,), dt)\n         assert_(np.r_[x, x].dtype == dt)\n \n-    @pytest.mark.filterwarnings(\"ignore:.*who.*:DeprecationWarning\")\n-    def test_who_with_0dim_array(self):\n-        # ticket #1243\n-        import os\n-        import sys\n-\n-        oldstdout = sys.stdout\n-        sys.stdout = open(os.devnull, 'w')\n-        try:\n-            try:\n-                np.lib.utils.who({'foo': np.array(1)})\n-            except Exception:\n-                raise AssertionError(\"ticket #1243\")\n-        finally:\n-            sys.stdout.close()\n-            sys.stdout = oldstdout\n-\n     def test_include_dirs(self):\n         # As a sanity check, just test that get_include\n         # includes something reasonable.  Somewhat\n",
            "comment_added_diff": {},
            "comment_deleted_diff": {
                "159": "        # ticket #1243",
                "169": "                raise AssertionError(\"ticket #1243\")"
            },
            "comment_modified_diff": {}
        },
        {
            "commit": "b43384e8f9f7242c59985c4a3d687c95a2a9dbf4",
            "timestamp": "2023-08-30T09:34:12+02:00",
            "author": "Mateusz Sok\u00f3\u0142",
            "commit_message": "MAINT: Remove deprecated functions [NEP 52] (#24477)",
            "additions": 0,
            "deletions": 6,
            "change_type": "MODIFY",
            "diff": "@@ -1671,12 +1671,6 @@ def test_nonzero_byteswap(self):\n         a = a.byteswap().newbyteorder()\n         assert_equal(a.nonzero()[0], [1])  # [0] if nonzero() ignores swap\n \n-    def test_find_common_type_boolean(self):\n-        # Ticket #1695\n-        with pytest.warns(DeprecationWarning, match=\"np.find_common_type\"):\n-            res = np.find_common_type([], ['?', '?'])\n-        assert res == '?'\n-\n     def test_empty_mul(self):\n         a = np.array([1.])\n         a[1:1] *= 2\n",
            "comment_added_diff": {},
            "comment_deleted_diff": {
                "1675": "        # Ticket #1695"
            },
            "comment_modified_diff": {}
        },
        {
            "commit": "65bb8ddd03763b8a931de2cd472d8ce8bd2f9fbd",
            "timestamp": "2023-09-11T18:51:42+02:00",
            "author": "Mateusz Sok\u00f3\u0142",
            "commit_message": "API: Remove ptp, setitem and newbyteorder from np.ndarray class",
            "additions": 3,
            "deletions": 3,
            "change_type": "MODIFY",
            "diff": "@@ -138,7 +138,7 @@ def test_unicode_swapping(self):\n         ulen = 1\n         ucs_value = '\\U0010FFFF'\n         ua = np.array([[[ucs_value*ulen]*2]*3]*4, dtype='U%s' % ulen)\n-        ua.newbyteorder()  # Should succeed.\n+        ua.view(np.dtype.byteorder())  # Should succeed.\n \n     def test_object_array_fill(self):\n         # Ticket #86\n@@ -1158,7 +1158,6 @@ def __array_finalize__(self, obj):\n         assert_(dat.max(1).info == 'jubba')\n         assert_(dat.mean(1).info == 'jubba')\n         assert_(dat.min(1).info == 'jubba')\n-        assert_(dat.newbyteorder().info == 'jubba')\n         assert_(dat.prod(1).info == 'jubba')\n         assert_(dat.ptp(1).info == 'jubba')\n         assert_(dat.ravel().info == 'jubba')\n@@ -1668,7 +1667,8 @@ def test_nonzero_byteswap(self):\n         a = np.array([0x80000000, 0x00000080, 0], dtype=np.uint32)\n         a.dtype = np.float32\n         assert_equal(a.nonzero()[0], [1])\n-        a = a.byteswap().newbyteorder()\n+        a = a.byteswap()\n+        a = a.view(a.dtype.newbyteorder())\n         assert_equal(a.nonzero()[0], [1])  # [0] if nonzero() ignores swap\n \n     def test_empty_mul(self):\n",
            "comment_added_diff": {
                "141": "        ua.view(np.dtype.byteorder())  # Should succeed."
            },
            "comment_deleted_diff": {
                "141": "        ua.newbyteorder()  # Should succeed."
            },
            "comment_modified_diff": {
                "141": "        ua.newbyteorder()  # Should succeed."
            }
        },
        {
            "commit": "1b249633ec171f4ccb95ab561e47068a574a4155",
            "timestamp": "2023-09-12T13:08:33+02:00",
            "author": "Mateusz Sok\u00f3\u0142",
            "commit_message": "Remove remaining .ptp() and .itemset() calls",
            "additions": 2,
            "deletions": 3,
            "change_type": "MODIFY",
            "diff": "@@ -138,7 +138,7 @@ def test_unicode_swapping(self):\n         ulen = 1\n         ucs_value = '\\U0010FFFF'\n         ua = np.array([[[ucs_value*ulen]*2]*3]*4, dtype='U%s' % ulen)\n-        ua.view(np.dtype.byteorder())  # Should succeed.\n+        ua.view(ua.dtype.newbyteorder())  # Should succeed.\n \n     def test_object_array_fill(self):\n         # Ticket #86\n@@ -519,7 +519,7 @@ def test_method_args(self):\n         # Make sure methods and functions have same default axis\n         # keyword and arguments\n         funcs1 = ['argmax', 'argmin', 'sum', 'any', 'all', 'cumsum',\n-                  'ptp', 'cumprod', 'prod', 'std', 'var', 'mean',\n+                  'cumprod', 'prod', 'std', 'var', 'mean',\n                   'round', 'min', 'max', 'argsort', 'sort']\n         funcs2 = ['compress', 'take', 'repeat']\n \n@@ -1159,7 +1159,6 @@ def __array_finalize__(self, obj):\n         assert_(dat.mean(1).info == 'jubba')\n         assert_(dat.min(1).info == 'jubba')\n         assert_(dat.prod(1).info == 'jubba')\n-        assert_(dat.ptp(1).info == 'jubba')\n         assert_(dat.ravel().info == 'jubba')\n         assert_(dat.real.info == 'jubba')\n         assert_(dat.repeat(2).info == 'jubba')\n",
            "comment_added_diff": {
                "141": "        ua.view(ua.dtype.newbyteorder())  # Should succeed."
            },
            "comment_deleted_diff": {
                "141": "        ua.view(np.dtype.byteorder())  # Should succeed."
            },
            "comment_modified_diff": {
                "141": "        ua.view(np.dtype.byteorder())  # Should succeed."
            }
        },
        {
            "commit": "94b9f2a78733d322cea8fd02c76c39beecb11aec",
            "timestamp": "2023-09-15T16:19:38+02:00",
            "author": "Ralf Gommers",
            "commit_message": "MAINT: remove relaxed strides debug build setting\n\nThis follows up on gh-21039. As noted there, this doesn't really\nserve any purpose anymore and historically seems to have resulted in\nonly 2 bug reports.",
            "additions": 0,
            "deletions": 15,
            "change_type": "MODIFY",
            "diff": "@@ -644,10 +644,6 @@ def test_reshape_zero_size(self):\n         a = np.ones((0, 2))\n         a.shape = (-1, 2)\n \n-    # Cannot test if NPY_RELAXED_STRIDES_DEBUG changes the strides.\n-    # With NPY_RELAXED_STRIDES_DEBUG the test becomes superfluous.\n-    @pytest.mark.skipif(np.ones(1).strides[0] == np.iinfo(np.intp).max,\n-                        reason=\"Using relaxed stride debug\")\n     def test_reshape_trailing_ones_strides(self):\n         # GitHub issue gh-2949, bad strides for trailing ones of new shape\n         a = np.zeros(12, dtype=np.int32)[::2]  # not contiguous\n@@ -904,17 +900,6 @@ def test_copy_detection_corner_case(self):\n         # Ticket #658\n         np.indices((0, 3, 4)).T.reshape(-1, 3)\n \n-    # Cannot test if NPY_RELAXED_STRIDES_DEBUG changes the strides.\n-    # With NPY_RELAXED_STRIDES_DEBUG the test becomes superfluous,\n-    # 0-sized reshape itself is tested elsewhere.\n-    @pytest.mark.skipif(np.ones(1).strides[0] == np.iinfo(np.intp).max,\n-                        reason=\"Using relaxed stride debug\")\n-    def test_copy_detection_corner_case2(self):\n-        # Ticket #771: strides are not set correctly when reshaping 0-sized\n-        # arrays\n-        b = np.indices((0, 3, 4)).T.reshape(-1, 3)\n-        assert_equal(b.strides, (3 * b.itemsize, b.itemsize))\n-\n     def test_object_array_refcounting(self):\n         # Ticket #633\n         if not hasattr(sys, 'getrefcount'):\n",
            "comment_added_diff": {},
            "comment_deleted_diff": {
                "647": "    # Cannot test if NPY_RELAXED_STRIDES_DEBUG changes the strides.",
                "648": "    # With NPY_RELAXED_STRIDES_DEBUG the test becomes superfluous.",
                "907": "    # Cannot test if NPY_RELAXED_STRIDES_DEBUG changes the strides.",
                "908": "    # With NPY_RELAXED_STRIDES_DEBUG the test becomes superfluous,",
                "909": "    # 0-sized reshape itself is tested elsewhere.",
                "913": "        # Ticket #771: strides are not set correctly when reshaping 0-sized",
                "914": "        # arrays"
            },
            "comment_modified_diff": {}
        }
    ],
    "test_scalarinherit.py": [],
    "test_unicode.py": [],
    "test_arraypad.py": [],
    "test_format.py": [],
    "test_io.py": [
        {
            "commit": "13d55a3c2f016a58a6e9d6b8086f338e07c7478f",
            "timestamp": "2022-10-26T10:02:37-07:00",
            "author": "Mike Taves",
            "commit_message": "MAINT: remove u-prefix for former Unicode strings (#22479)",
            "additions": 14,
            "deletions": 14,
            "change_type": "MODIFY",
            "diff": "@@ -560,7 +560,7 @@ def test_unicode_stringstream(self):\n         s.seek(0)\n         assert_equal(s.read(), utf8 + '\\n')\n \n-    @pytest.mark.parametrize(\"fmt\", [u\"%f\", b\"%f\"])\n+    @pytest.mark.parametrize(\"fmt\", [\"%f\", b\"%f\"])\n     @pytest.mark.parametrize(\"iotype\", [StringIO, BytesIO])\n     def test_unicode_and_bytes_fmt(self, fmt, iotype):\n         # string type of fmt should not matter, see also gh-4053\n@@ -569,7 +569,7 @@ def test_unicode_and_bytes_fmt(self, fmt, iotype):\n         np.savetxt(s, a, fmt=fmt)\n         s.seek(0)\n         if iotype is StringIO:\n-            assert_equal(s.read(), u\"%f\\n\" % 1.)\n+            assert_equal(s.read(), \"%f\\n\" % 1.)\n         else:\n             assert_equal(s.read(), b\"%f\\n\" % 1.)\n \n@@ -763,7 +763,7 @@ def test_comments_unicode(self):\n         c.write('# comment\\n1,2,3,5\\n')\n         c.seek(0)\n         x = np.loadtxt(c, dtype=int, delimiter=',',\n-                       comments=u'#')\n+                       comments='#')\n         a = np.array([1, 2, 3, 5], int)\n         assert_array_equal(x, a)\n \n@@ -1514,7 +1514,7 @@ def test_file_is_closed_on_error(self):\n         with tempdir() as tmpdir:\n             fpath = os.path.join(tmpdir, \"test.csv\")\n             with open(fpath, \"wb\") as f:\n-                f.write(u'\\N{GREEK PI SYMBOL}'.encode('utf8'))\n+                f.write('\\N{GREEK PI SYMBOL}'.encode())\n \n             # ResourceWarnings are emitted from a destructor, so won't be\n             # detected by regular propagation to errors.\n@@ -2174,9 +2174,9 @@ def test_latin1(self):\n         test = np.genfromtxt(TextIO(s),\n                              dtype=None, comments=None, delimiter=',',\n                              encoding='latin1')\n-        assert_equal(test[1, 0], u\"test1\")\n-        assert_equal(test[1, 1], u\"testNonethe\" + latin1.decode('latin1'))\n-        assert_equal(test[1, 2], u\"test3\")\n+        assert_equal(test[1, 0], \"test1\")\n+        assert_equal(test[1, 1], \"testNonethe\" + latin1.decode('latin1'))\n+        assert_equal(test[1, 2], \"test3\")\n \n         with warnings.catch_warnings(record=True) as w:\n             warnings.filterwarnings('always', '', np.VisibleDeprecationWarning)\n@@ -2230,8 +2230,8 @@ def test_utf8_file(self):\n \n     def test_utf8_file_nodtype_unicode(self):\n         # bytes encoding with non-latin1 -> unicode upcast\n-        utf8 = u'\\u03d6'\n-        latin1 = u'\\xf6\\xfc\\xf6'\n+        utf8 = '\\u03d6'\n+        latin1 = '\\xf6\\xfc\\xf6'\n \n         # skip test if cannot encode utf8 test string with preferred\n         # encoding. The preferred encoding is assumed to be the default\n@@ -2246,9 +2246,9 @@ def test_utf8_file_nodtype_unicode(self):\n \n         with temppath() as path:\n             with io.open(path, \"wt\") as f:\n-                f.write(u\"norm1,norm2,norm3\\n\")\n-                f.write(u\"norm1,\" + latin1 + u\",norm3\\n\")\n-                f.write(u\"test1,testNonethe\" + utf8 + u\",test3\\n\")\n+                f.write(\"norm1,norm2,norm3\\n\")\n+                f.write(\"norm1,\" + latin1 + \",norm3\\n\")\n+                f.write(\"test1,testNonethe\" + utf8 + \",test3\\n\")\n             with warnings.catch_warnings(record=True) as w:\n                 warnings.filterwarnings('always', '',\n                                         np.VisibleDeprecationWarning)\n@@ -2583,7 +2583,7 @@ def test_recfromtxt(self):\n         with temppath(suffix='.txt') as path:\n             path = Path(path)\n             with path.open('w') as f:\n-                f.write(u'A,B\\n0,1\\n2,3')\n+                f.write('A,B\\n0,1\\n2,3')\n \n             kwargs = dict(delimiter=\",\", missing_values=\"N/A\", names=True)\n             test = np.recfromtxt(path, **kwargs)\n@@ -2596,7 +2596,7 @@ def test_recfromcsv(self):\n         with temppath(suffix='.txt') as path:\n             path = Path(path)\n             with path.open('w') as f:\n-                f.write(u'A,B\\n0,1\\n2,3')\n+                f.write('A,B\\n0,1\\n2,3')\n \n             kwargs = dict(missing_values=\"N/A\", names=True, case_sensitive=True)\n             test = np.recfromcsv(path, dtype=None, **kwargs)\n",
            "comment_added_diff": {
                "766": "                       comments='#')"
            },
            "comment_deleted_diff": {
                "766": "                       comments=u'#')"
            },
            "comment_modified_diff": {
                "766": "                       comments=u'#')"
            }
        },
        {
            "commit": "be0e502e47b72db8fbd52ab40124fcb07d45936f",
            "timestamp": "2023-04-06T12:34:33+02:00",
            "author": "Ganesh Kathiresan",
            "commit_message": "ENH: ``__repr__`` for NpzFile object (#23357)\n\nImproves the repr to include information about the arrays contained, e.g.:\r\n\r\n   >>> npzfile = np.load('arr.npz')\r\n   >>> npzfile\r\n   NpzFile 'arr.npz' with keys arr_0, arr_1, arr_2, arr_3, arr_4...\r\n\r\ncloses #23319\r\n\r\nCo-authored-by: Ross Barnowski <rossbar@berkeley.edu>",
            "additions": 17,
            "deletions": 2,
            "change_type": "MODIFY",
            "diff": "@@ -321,6 +321,21 @@ def test_closing_zipfile_after_load(self):\n             data.close()\n             assert_(fp.closed)\n \n+    @pytest.mark.parametrize(\"count, expected_repr\", [\n+        (1, \"NpzFile {fname!r} with keys: arr_0\"),\n+        (5, \"NpzFile {fname!r} with keys: arr_0, arr_1, arr_2, arr_3, arr_4\"),\n+        # _MAX_REPR_ARRAY_COUNT is 5, so files with more than 5 keys are\n+        # expected to end in '...'\n+        (6, \"NpzFile {fname!r} with keys: arr_0, arr_1, arr_2, arr_3, arr_4...\"),\n+    ])\n+    def test_repr_lists_keys(self, count, expected_repr):\n+        a = np.array([[1, 2], [3, 4]], float)\n+        with temppath(suffix='.npz') as tmp:\n+            np.savez(tmp, *[a]*count)\n+            l = np.load(tmp)\n+            assert repr(l) == expected_repr.format(fname=tmp)\n+            l.close()\n+\n \n class TestSaveTxt:\n     def test_array(self):\n@@ -597,8 +612,8 @@ def check_large_zip(memoryerror_raised):\n         # in our process if needed, see gh-16889\n         memoryerror_raised = Value(c_bool)\n \n-        # Since Python 3.8, the default start method for multiprocessing has \n-        # been changed from 'fork' to 'spawn' on macOS, causing inconsistency \n+        # Since Python 3.8, the default start method for multiprocessing has\n+        # been changed from 'fork' to 'spawn' on macOS, causing inconsistency\n         # on memory sharing model, lead to failed test for check_large_zip\n         ctx = get_context('fork')\n         p = ctx.Process(target=check_large_zip, args=(memoryerror_raised,))\n",
            "comment_added_diff": {
                "327": "        # _MAX_REPR_ARRAY_COUNT is 5, so files with more than 5 keys are",
                "328": "        # expected to end in '...'",
                "615": "        # Since Python 3.8, the default start method for multiprocessing has",
                "616": "        # been changed from 'fork' to 'spawn' on macOS, causing inconsistency"
            },
            "comment_deleted_diff": {
                "600": "        # Since Python 3.8, the default start method for multiprocessing has",
                "601": "        # been changed from 'fork' to 'spawn' on macOS, causing inconsistency"
            },
            "comment_modified_diff": {}
        },
        {
            "commit": "25908cacd19915bf3ddd659c28be28a41bd97a54",
            "timestamp": "2023-05-12T23:05:07-07:00",
            "author": "Nathan Goldbaum",
            "commit_message": "BUG: properly handle tuple keys in NpZFile.__getitem__ (#23757)\n\n* BUG: properly handle tuple keys in NpZFile.__getitem__\r\n\r\n* TST: test tuple rendering specifically.\r\n\r\n---------\r\n\r\nCo-authored-by: Ross Barnowski <rossbar@berkeley.edu>",
            "additions": 11,
            "deletions": 0,
            "change_type": "MODIFY",
            "diff": "@@ -232,6 +232,17 @@ def test_named_arrays(self):\n         assert_equal(a, l['file_a'])\n         assert_equal(b, l['file_b'])\n \n+\n+    def test_tuple_getitem_raises(self):\n+        # gh-23748\n+        a = np.array([1, 2, 3])\n+        f = BytesIO()\n+        np.savez(f, a=a)\n+        f.seek(0)\n+        l = np.load(f)\n+        with pytest.raises(KeyError, match=\"(1, 2)\"):\n+            l[1, 2]\n+\n     def test_BagObj(self):\n         a = np.array([[1, 2], [3, 4]], float)\n         b = np.array([[1 + 2j, 2 + 7j], [3 - 6j, 4 + 12j]], complex)\n",
            "comment_added_diff": {
                "237": "        # gh-23748"
            },
            "comment_deleted_diff": {},
            "comment_modified_diff": {}
        },
        {
            "commit": "74074231b681a322341143fa90c44b8d96f719a2",
            "timestamp": "2023-05-27T19:19:31+05:30",
            "author": "ganesh-k13",
            "commit_message": "TST: Added `pathlib.Path` tests for io functions",
            "additions": 24,
            "deletions": 20,
            "change_type": "MODIFY",
            "diff": "@@ -471,11 +471,12 @@ def test_header_footer(self):\n         assert_equal(c.read(),\n                      asbytes('1 2\\n3 4\\n' + commentstr + test_header_footer + '\\n'))\n \n-    def test_file_roundtrip(self):\n+    @pytest.mark.parametrize(\"filename_type\", [Path, str])\n+    def test_file_roundtrip(self, filename_type):\n         with temppath() as name:\n             a = np.array([(1, 2), (3, 4)])\n-            np.savetxt(name, a)\n-            b = np.loadtxt(name)\n+            np.savetxt(filename_type(name), a)\n+            b = np.loadtxt(filename_type(name))\n             assert_array_equal(a, b)\n \n     def test_complex_arrays(self):\n@@ -2567,10 +2568,10 @@ def test_save_load_memmap(self):\n                 break_cycles()\n \n     @pytest.mark.xfail(IS_WASM, reason=\"memmap doesn't work correctly\")\n-    def test_save_load_memmap_readwrite(self):\n-        # Test that pathlib.Path instances can be written mem-mapped.\n+    @pytest.mark.parametrize(\"filename_type\", [Path, str])\n+    def test_save_load_memmap_readwrite(self, filename_type):\n         with temppath(suffix='.npy') as path:\n-            path = Path(path)\n+            path = filename_type(path)\n             a = np.array([[1, 2], [3, 4]], int)\n             np.save(path, a)\n             b = np.load(path, mmap_mode='r+')\n@@ -2583,35 +2584,37 @@ def test_save_load_memmap_readwrite(self):\n             data = np.load(path)\n             assert_array_equal(data, a)\n \n-    def test_savez_load(self):\n-        # Test that pathlib.Path instances can be used with savez.\n+    @pytest.mark.parametrize(\"filename_type\", [Path, str])\n+    def test_savez_load(self, filename_type):\n         with temppath(suffix='.npz') as path:\n-            path = Path(path)\n+            path = filename_type(path)\n             np.savez(path, lab='place holder')\n             with np.load(path) as data:\n                 assert_array_equal(data['lab'], 'place holder')\n \n-    def test_savez_compressed_load(self):\n-        # Test that pathlib.Path instances can be used with savez.\n+    @pytest.mark.parametrize(\"filename_type\", [Path, str])\n+    def test_savez_compressed_load(self, filename_type):\n         with temppath(suffix='.npz') as path:\n-            path = Path(path)\n+            path = filename_type(path)\n             np.savez_compressed(path, lab='place holder')\n             data = np.load(path)\n             assert_array_equal(data['lab'], 'place holder')\n             data.close()\n \n-    def test_genfromtxt(self):\n+    @pytest.mark.parametrize(\"filename_type\", [Path, str])\n+    def test_genfromtxt(self, filename_type):\n         with temppath(suffix='.txt') as path:\n-            path = Path(path)\n+            path = filename_type(path)\n             a = np.array([(1, 2), (3, 4)])\n             np.savetxt(path, a)\n             data = np.genfromtxt(path)\n             assert_array_equal(a, data)\n \n-    def test_recfromtxt(self):\n+    @pytest.mark.parametrize(\"filename_type\", [Path, str])\n+    def test_recfromtxt(self, filename_type):\n         with temppath(suffix='.txt') as path:\n-            path = Path(path)\n-            with path.open('w') as f:\n+            path = filename_type(path)\n+            with open(path, 'w') as f:\n                 f.write('A,B\\n0,1\\n2,3')\n \n             kwargs = dict(delimiter=\",\", missing_values=\"N/A\", names=True)\n@@ -2621,10 +2624,11 @@ def test_recfromtxt(self):\n             assert_(isinstance(test, np.recarray))\n             assert_equal(test, control)\n \n-    def test_recfromcsv(self):\n+    @pytest.mark.parametrize(\"filename_type\", [Path, str])\n+    def test_recfromcsv(self, filename_type):\n         with temppath(suffix='.txt') as path:\n-            path = Path(path)\n-            with path.open('w') as f:\n+            path = filename_type(path)\n+            with open(path, 'w') as f:\n                 f.write('A,B\\n0,1\\n2,3')\n \n             kwargs = dict(missing_values=\"N/A\", names=True, case_sensitive=True)\n",
            "comment_added_diff": {},
            "comment_deleted_diff": {
                "2571": "        # Test that pathlib.Path instances can be written mem-mapped.",
                "2587": "        # Test that pathlib.Path instances can be used with savez.",
                "2595": "        # Test that pathlib.Path instances can be used with savez."
            },
            "comment_modified_diff": {}
        },
        {
            "commit": "7ca502e2b0e02a1bfd9db4affef60155c0d68f5f",
            "timestamp": "2023-05-28T12:27:30+02:00",
            "author": "Dimitri Papadopoulos",
            "commit_message": "MAINT: io.open \u2192 open\n\nIn Python 3, io.open() is an alias for the builtin open() function:\nhttps://docs.python.org/3/library/io.html#io.open",
            "additions": 3,
            "deletions": 4,
            "change_type": "MODIFY",
            "diff": "@@ -5,7 +5,6 @@\n import threading\n import time\n import warnings\n-import io\n import re\n import pytest\n from pathlib import Path\n@@ -701,7 +700,7 @@ def test_converters_nodecode(self):\n         # test native string converters enabled by setting an encoding\n         utf8 = b'\\xcf\\x96'.decode('UTF-8')\n         with temppath() as path:\n-            with io.open(path, 'wt', encoding='UTF-8') as f:\n+            with open(path, 'wt', encoding='UTF-8') as f:\n                 f.write(utf8)\n             x = self.loadfunc(path, dtype=np.str_,\n                               converters={0: lambda x: x + 't'},\n@@ -2264,7 +2263,7 @@ def test_utf8_file_nodtype_unicode(self):\n \n         # skip test if cannot encode utf8 test string with preferred\n         # encoding. The preferred encoding is assumed to be the default\n-        # encoding of io.open. Will need to change this for PyTest, maybe\n+        # encoding of open. Will need to change this for PyTest, maybe\n         # using pytest.mark.xfail(raises=***).\n         try:\n             encoding = locale.getpreferredencoding()\n@@ -2274,7 +2273,7 @@ def test_utf8_file_nodtype_unicode(self):\n                         'unable to encode utf8 in preferred encoding')\n \n         with temppath() as path:\n-            with io.open(path, \"wt\") as f:\n+            with open(path, \"wt\") as f:\n                 f.write(\"norm1,norm2,norm3\\n\")\n                 f.write(\"norm1,\" + latin1 + \",norm3\\n\")\n                 f.write(\"test1,testNonethe\" + utf8 + \",test3\\n\")\n",
            "comment_added_diff": {
                "2266": "        # encoding of open. Will need to change this for PyTest, maybe"
            },
            "comment_deleted_diff": {
                "2267": "        # encoding of io.open. Will need to change this for PyTest, maybe"
            },
            "comment_modified_diff": {}
        }
    ],
    "test_core.py": [
        {
            "commit": "45cb03723b863b0814092631fe9cf17df67add4e",
            "timestamp": "2022-12-19T20:22:20+01:00",
            "author": "Sebastian Berg",
            "commit_message": "BUG: Do not use getdata() in np.ma.masked_invalid\n\nThis is the minimal solution to fix gh-22826 with as little change\nas possible.\nWe should fix `getdata()` but I don't want to do that in a bug-fix\nrelease really.\n\nIMO the alternative is to revert gh-22046 which would also revert\nthe behavior noticed in gh-22720  (which seems less harmful though).\n\nCloses gh-22826",
            "additions": 14,
            "deletions": 0,
            "change_type": "MODIFY",
            "diff": "@@ -4507,6 +4507,20 @@ def test_masked_invalid_error(self):\n                            match=\"not supported for the input types\"):\n             np.ma.masked_invalid(a)\n \n+    def test_masked_invalid_pandas(self):\n+        # getdata() used to be bad for pandas series due to its _data\n+        # attribute.  This test is a regression test mainly and may be\n+        # removed if getdata() is adjusted.\n+        class Series():\n+            _data = \"nonsense\"\n+\n+            def __array__(self):\n+                return np.array([5, np.nan, np.inf])\n+\n+        arr = np.ma.masked_invalid(Series())\n+        assert_array_equal(arr._data, np.array(Series()))\n+        assert_array_equal(arr._mask, [False, True, True])\n+\n     def test_choose(self):\n         # Test choose\n         choices = [[0, 1, 2, 3], [10, 11, 12, 13],\n",
            "comment_added_diff": {
                "4511": "        # getdata() used to be bad for pandas series due to its _data",
                "4512": "        # attribute.  This test is a regression test mainly and may be",
                "4513": "        # removed if getdata() is adjusted."
            },
            "comment_deleted_diff": {},
            "comment_modified_diff": {}
        },
        {
            "commit": "954aee7ab016d6f76d263bfe4c70de114eed63eb",
            "timestamp": "2022-12-21T09:41:19+01:00",
            "author": "Sebastian Berg",
            "commit_message": "API: Ensure a full mask is returned for masked_invalid\n\nMatplotlib relies on this, so we don't seem to have much of a choice.\nI am surprised that we were not notified of the issue before release\ntime.\n\nCloses gh-22720, gh-22720",
            "additions": 12,
            "deletions": 0,
            "change_type": "MODIFY",
            "diff": "@@ -4521,6 +4521,18 @@ def __array__(self):\n         assert_array_equal(arr._data, np.array(Series()))\n         assert_array_equal(arr._mask, [False, True, True])\n \n+    @pytest.mark.parametrize(\"copy\", [True, False])\n+    def test_masked_invalid_full_mask(self, copy):\n+        # Matplotlib relied on masked_invalid always returning a full mask\n+        # (Also astropy projects, but were ok with it gh-22720 and gh-22842)\n+        a = np.ma.array([1, 2, 3, 4])\n+        assert a._mask is nomask\n+        res = np.ma.masked_invalid(a, copy=copy)\n+        assert res.mask is not nomask\n+        # mask of a should not be mutated\n+        assert a.mask is nomask\n+        assert np.may_share_memory(a._data, res._data) != copy\n+\n     def test_choose(self):\n         # Test choose\n         choices = [[0, 1, 2, 3], [10, 11, 12, 13],\n",
            "comment_added_diff": {
                "4526": "        # Matplotlib relied on masked_invalid always returning a full mask",
                "4527": "        # (Also astropy projects, but were ok with it gh-22720 and gh-22842)",
                "4532": "        # mask of a should not be mutated"
            },
            "comment_deleted_diff": {},
            "comment_modified_diff": {}
        },
        {
            "commit": "720cabc2331a0f003b708e55eb9bf0df204a085a",
            "timestamp": "2023-01-19T00:40:34+01:00",
            "author": "Marko Pacak",
            "commit_message": "BUG: fix ma.diff not preserving mask when using append/prepend (#22776)\n\nPort CORE diff relevant code to MA and adapt docstrings examples and add tsts.\r\n\r\nCloses gh-22465",
            "additions": 40,
            "deletions": 0,
            "change_type": "MODIFY",
            "diff": "@@ -4093,6 +4093,46 @@ def test_mean_overflow(self):\n                          mask=np.zeros((10000, 10000)))\n         assert_equal(a.mean(), 65535.0)\n \n+    def test_diff_with_prepend(self):\n+        # GH 22465\n+        x = np.array([1, 2, 2, 3, 4, 2, 1, 1])\n+\n+        a = np.ma.masked_equal(x[3:], value=2)\n+        a_prep = np.ma.masked_equal(x[:3], value=2)\n+        diff1 = np.ma.diff(a, prepend=a_prep, axis=0)\n+\n+        b = np.ma.masked_equal(x, value=2)\n+        diff2 = np.ma.diff(b, axis=0)\n+\n+        assert_(np.ma.allequal(diff1, diff2))\n+\n+    def test_diff_with_append(self):\n+        # GH 22465\n+        x = np.array([1, 2, 2, 3, 4, 2, 1, 1])\n+\n+        a = np.ma.masked_equal(x[:3], value=2)\n+        a_app = np.ma.masked_equal(x[3:], value=2)\n+        diff1 = np.ma.diff(a, append=a_app, axis=0)\n+\n+        b = np.ma.masked_equal(x, value=2)\n+        diff2 = np.ma.diff(b, axis=0)\n+\n+        assert_(np.ma.allequal(diff1, diff2))\n+\n+    def test_diff_with_dim_0(self):\n+        with pytest.raises(\n+            ValueError,\n+            match=\"diff requires input that is at least one dimensional\"\n+            ):\n+            np.ma.diff(np.array(1))\n+\n+    def test_diff_with_n_0(self):\n+        a = np.ma.masked_equal([1, 2, 2, 3, 4, 2, 1, 1], value=2)\n+        diff = np.ma.diff(a, n=0, axis=0)\n+\n+        assert_(np.ma.allequal(a, diff))\n+\n+\n class TestMaskedArrayMathMethodsComplex:\n     # Test class for miscellaneous MaskedArrays methods.\n     def setup_method(self):\n",
            "comment_added_diff": {
                "4097": "        # GH 22465",
                "4110": "        # GH 22465"
            },
            "comment_deleted_diff": {},
            "comment_modified_diff": {}
        },
        {
            "commit": "c61900d5675d4a33a893da5c8e343b6c0c1666dd",
            "timestamp": "2023-02-23T17:32:01-07:00",
            "author": "Tyler Reddy",
            "commit_message": "BUG: masked array proper deepcopies\n\nFixes #22556\nFixes #21022\n\n* add regression test and fix for gh-22556, where\nwe were relying on the array `copy` arg to deepcopy\na compound object type; I thought about performance issues\nhere, but if you are already in the land of `object` and\nyou are explicitly opting in to `deepcopy`, it seems like\nperformance might be wishful thinking anyway\n\n* add regression test and fix for gh-21022--this one was\nweirder but seems possible to sidestep by not trying\nto assign a shape of `()` to something that already has\nshape `()` and a non-writeable `shape` attribute",
            "additions": 20,
            "deletions": 0,
            "change_type": "MODIFY",
            "diff": "@@ -8,6 +8,7 @@\n \n import sys\n import warnings\n+import copy\n import operator\n import itertools\n import textwrap\n@@ -5570,3 +5571,22 @@ def method(self):\n original note\"\"\"\n \n     assert_equal(np.ma.core.doc_note(method.__doc__, \"note\"), expected_doc)\n+\n+\n+def test_gh_22556():\n+    source = np.ma.array([0, [0, 1, 2]], dtype=object)\n+    deepcopy = copy.deepcopy(source)\n+    deepcopy[1].append('this should not appear in source')\n+    assert len(source[1]) == 3\n+\n+\n+def test_gh_21022():\n+    # testing for absence of reported error\n+    source = np.ma.masked_array(data=[-1, -1], mask=True, dtype=np.float64)\n+    axis = np.array(0)\n+    result = np.prod(source, axis=axis, keepdims=False)\n+    result = np.ma.masked_array(result,\n+                                mask=np.ones(result.shape, dtype=np.bool_))\n+    array = np.ma.masked_array(data=-1, mask=True, dtype=np.float64)\n+    copy.deepcopy(array)\n+    copy.deepcopy(result)\n",
            "comment_added_diff": {
                "5584": "    # testing for absence of reported error"
            },
            "comment_deleted_diff": {},
            "comment_modified_diff": {}
        },
        {
            "commit": "77a5aab54e10e063773e9aaf0e3730bf0cfefa9b",
            "timestamp": "2023-04-21T11:52:06+02:00",
            "author": "Sebastian Berg",
            "commit_message": "BUG: Fix masked array raveling when `order=\"A\"` or `order=\"K\"`\n\nThis transitively fixes gh-22912.  I had alooked a bit into whether\nit is worthwhile to preserve the mask order, but TBH, we seem to not\ndo so in so many places, that I don't think it really is worthwhile.\n\nApplying `order=\"K\"` or `order=\"A\"` to the data and mask separately\nis an big bug though.\n\nCloses gh-22912",
            "additions": 16,
            "deletions": 0,
            "change_type": "MODIFY",
            "diff": "@@ -3411,6 +3411,22 @@ def test_ravel(self):\n         assert_equal(a.ravel(order='C'), [1, 2, 3, 4])\n         assert_equal(a.ravel(order='F'), [1, 3, 2, 4])\n \n+    @pytest.mark.parametrize(\"order\", \"AKCF\")\n+    @pytest.mark.parametrize(\"data_order\", \"CF\")\n+    def test_ravel_order(self, order, data_order):\n+        # Ravelling must ravel mask and data in the same order always to avoid\n+        # misaligning the two in the ravel result.\n+        arr = np.ones((5, 10), order=data_order)\n+        arr[0, :] = 0\n+        mask = np.ones((10, 5), dtype=bool, order=data_order).T\n+        mask[0, :] = False\n+        x = array(arr, mask=mask)\n+        assert x._data.flags.fnc != x._mask.flags.fnc\n+        assert (x.filled(0) == 0).all()\n+        raveled = x.ravel(order)\n+        assert (raveled.filled(0) == 0).all()\n+\n+\n     def test_reshape(self):\n         # Tests reshape\n         x = arange(4)\n",
            "comment_added_diff": {
                "3417": "        # Ravelling must ravel mask and data in the same order always to avoid",
                "3418": "        # misaligning the two in the ravel result."
            },
            "comment_deleted_diff": {},
            "comment_modified_diff": {}
        },
        {
            "commit": "0655cb9e24cff97642c70a98c5258bcbe7a6c3d3",
            "timestamp": "2023-04-21T12:23:23+02:00",
            "author": "Sebastian Berg",
            "commit_message": "BUG: Ignore invalid and overflow warnings in masked setitem\n\nGiving a warning for invalid/overflow in settitem/casts is right\n(IMO), however for masked arrays it can be surprising since the\nwarning is not useful if the value is invalid but also masked.\n\nSo, simply blanket ignore the relevant warnings in setitem via errstate.\n(There may be some other cases like `.astype()` where it might be\nhelpful to MA users to just blanket opt-out of these new warnings.)\n\nCloses gh-23000",
            "additions": 18,
            "deletions": 0,
            "change_type": "MODIFY",
            "diff": "@@ -377,6 +377,24 @@ def test_indexing(self):\n         assert_equal(s1, s2)\n         assert_(x1[1:1].shape == (0,))\n \n+    def test_setitem_no_warning(self):\n+        # Setitem shouldn't warn, because the assignment might be masked\n+        # and warning for a masked assignment is weird (see gh-23000)\n+        # (When the value is masked, otherwise a warning would be acceptable\n+        # but is not given currently.)\n+        x = np.ma.arange(60).reshape((6, 10))\n+        index = (slice(1, 5, 2), [7, 5])\n+        value = np.ma.masked_all((2, 2))\n+        value._data[...] = np.inf  # not a valid integer...\n+        x[index] = value\n+        # The masked scalar is special cased, but test anyway (it's NaN):\n+        x[...] = np.ma.masked\n+        # Finally, a large value that cannot be cast to the float32 `x`\n+        x = np.ma.arange(3., dtype=np.float32)\n+        value = np.ma.array([2e234, 1, 1], mask=[True, False, False])\n+        x[...] = value\n+        x[[0, 1, 2]] = value\n+\n     @suppress_copy_mask_on_assignment\n     def test_copy(self):\n         # Tests of some subtle points of copying and sizing.\n",
            "comment_added_diff": {
                "381": "        # Setitem shouldn't warn, because the assignment might be masked",
                "382": "        # and warning for a masked assignment is weird (see gh-23000)",
                "383": "        # (When the value is masked, otherwise a warning would be acceptable",
                "384": "        # but is not given currently.)",
                "388": "        value._data[...] = np.inf  # not a valid integer...",
                "390": "        # The masked scalar is special cased, but test anyway (it's NaN):",
                "392": "        # Finally, a large value that cannot be cast to the float32 `x`"
            },
            "comment_deleted_diff": {},
            "comment_modified_diff": {}
        },
        {
            "commit": "e8920038ade22a8966fb977e77aa00a82142901b",
            "timestamp": "2023-04-28T14:41:00+02:00",
            "author": "Sebastian Berg",
            "commit_message": "BUG: Fix masked array ravel order for A (and somewhat K)\n\nSwaps the order to the correct thing and thus\n\ncloses gh-23651",
            "additions": 2,
            "deletions": 0,
            "change_type": "MODIFY",
            "diff": "@@ -3444,6 +3444,8 @@ def test_ravel_order(self, order, data_order):\n         raveled = x.ravel(order)\n         assert (raveled.filled(0) == 0).all()\n \n+        # NOTE: Can be wrong if arr order is neither C nor F and `order=\"K\"` \n+        assert_array_equal(arr.ravel(order), x.ravel(order)._data)\n \n     def test_reshape(self):\n         # Tests reshape\n",
            "comment_added_diff": {
                "3447": "        # NOTE: Can be wrong if arr order is neither C nor F and `order=\"K\"`"
            },
            "comment_deleted_diff": {},
            "comment_modified_diff": {}
        },
        {
            "commit": "9a07a5efe8ab20cacee9235d9a8fc4de02433581",
            "timestamp": "2023-08-26T17:33:03+02:00",
            "author": "Marten van Kerkwijk",
            "commit_message": "BUG: fix comparisons between masked and unmasked structured arrays",
            "additions": 19,
            "deletions": 2,
            "change_type": "MODIFY",
            "diff": "@@ -1310,7 +1310,7 @@ def test_minmax_dtypes(self):\n         m1 = [1, 0, 0, 0, 0, 0, 1, 0, 0, 0, 0, 0]\n         xm = masked_array(x, mask=m1)\n         xm.set_fill_value(1e+20)\n-        float_dtypes = [np.float16, np.float32, np.float64, np.longdouble, \n+        float_dtypes = [np.float16, np.float32, np.float64, np.longdouble,\n                         np.complex64, np.complex128, np.clongdouble]\n         for float_dtype in float_dtypes:\n             assert_equal(masked_array(x, mask=m1, dtype=float_dtype).max(),\n@@ -1614,6 +1614,23 @@ def test_ne_on_structured(self):\n         assert_equal(test.mask, [[False, False], [False, True]])\n         assert_(test.fill_value == True)\n \n+    def test_eq_ne_structured_with_non_masked(self):\n+        a = array([(1, 1), (2, 2), (3, 4)],\n+                  mask=[(0, 1), (0, 0), (1, 1)], dtype='i4,i4')\n+        eq = a == a.data\n+        ne = a.data != a\n+        # Test the obvious.\n+        assert_(np.all(eq))\n+        assert_(not np.any(ne))\n+        # Expect the mask set only for items with all fields masked.\n+        expected_mask = a.mask == np.ones((), a.mask.dtype)\n+        assert_array_equal(eq.mask, expected_mask)\n+        assert_array_equal(ne.mask, expected_mask)\n+        # The masked element will indicated not equal, because the\n+        # masks did not match.\n+        assert_equal(eq.data, [True, True, False])\n+        assert_array_equal(eq.data, ~ne.data)\n+\n     def test_eq_ne_structured_extra(self):\n         # ensure simple examples are symmetric and make sense.\n         # from https://github.com/numpy/numpy/pull/8590#discussion_r101126465\n@@ -3444,7 +3461,7 @@ def test_ravel_order(self, order, data_order):\n         raveled = x.ravel(order)\n         assert (raveled.filled(0) == 0).all()\n \n-        # NOTE: Can be wrong if arr order is neither C nor F and `order=\"K\"` \n+        # NOTE: Can be wrong if arr order is neither C nor F and `order=\"K\"`\n         assert_array_equal(arr.ravel(order), x.ravel(order)._data)\n \n     def test_reshape(self):\n",
            "comment_added_diff": {
                "1622": "        # Test the obvious.",
                "1625": "        # Expect the mask set only for items with all fields masked.",
                "1629": "        # The masked element will indicated not equal, because the",
                "1630": "        # masks did not match.",
                "3464": "        # NOTE: Can be wrong if arr order is neither C nor F and `order=\"K\"`"
            },
            "comment_deleted_diff": {
                "3447": "        # NOTE: Can be wrong if arr order is neither C nor F and `order=\"K\"`"
            },
            "comment_modified_diff": {}
        },
        {
            "commit": "17440d692dd648159c304676772ac1d5c6e40288",
            "timestamp": "2023-08-27T12:46:44+02:00",
            "author": "Marten van Kerkwijk",
            "commit_message": "BUG: ensure nomask in comparison result is not broadcast",
            "additions": 9,
            "deletions": 0,
            "change_type": "MODIFY",
            "diff": "@@ -1770,6 +1770,15 @@ def test_eq_broadcast_with_unmasked(self, op):\n         assert_(result.mask.shape == b.shape)\n         assert_equal(result.mask, np.zeros(b.shape, bool) | a.mask)\n \n+    @pytest.mark.parametrize(\"op\", [operator.eq, operator.gt])\n+    def test_comp_no_mask_not_broadcast(self, op):\n+        # Regression test for failing doctest in MaskedArray.nonzero\n+        # after gh-24556.\n+        a = array([[1, 2, 3], [4, 5, 6], [7, 8, 9]])\n+        result = op(a, 3)\n+        assert_(not result.mask.shape)\n+        assert_(result.mask is nomask)\n+\n     @pytest.mark.parametrize('dt1', num_dts, ids=num_ids)\n     @pytest.mark.parametrize('dt2', num_dts, ids=num_ids)\n     @pytest.mark.parametrize('fill', [None, 1])\n",
            "comment_added_diff": {
                "1775": "        # Regression test for failing doctest in MaskedArray.nonzero",
                "1776": "        # after gh-24556."
            },
            "comment_deleted_diff": {},
            "comment_modified_diff": {}
        }
    ],
    "asv_pip_nopep517.py": [],
    "ccompiler_opt.py": [
        {
            "commit": "d183edf54e3c74c52471c694068ec7fcc5f7aa34",
            "timestamp": "2023-04-04T04:13:01+02:00",
            "author": "Sayed Adel",
            "commit_message": "ENH: Raise C++ standard to C++17",
            "additions": 1,
            "deletions": 10,
            "change_type": "MODIFY",
            "diff": "@@ -16,15 +16,6 @@\n import subprocess\n import textwrap\n \n-# These flags are used to compile any C++ source within Numpy.\n-# They are chosen to have very few runtime dependencies.\n-NPY_CXX_FLAGS = [\n-    '-std=c++11',  # Minimal standard version\n-    '-D__STDC_VERSION__=0',  # for compatibility with C headers\n-    '-fno-exceptions',  # no exception support\n-    '-fno-rtti']  # no runtime type information\n-\n-\n class _Config:\n     \"\"\"An abstract class holds all configurable attributes of `CCompilerOpt`,\n     these class attributes can be used to change the default behavior\n@@ -1000,7 +991,7 @@ def __init__(self):\n         )\n         detect_args = (\n            (\"cc_has_debug\",  \".*(O0|Od|ggdb|coverage|debug:full).*\", \"\"),\n-           (\"cc_has_native\", \n+           (\"cc_has_native\",\n                 \".*(-march=native|-xHost|/QxHost|-mcpu=a64fx).*\", \"\"),\n            # in case if the class run with -DNPY_DISABLE_OPTIMIZATION\n            (\"cc_noopt\", \".*DISABLE_OPT.*\", \"\"),\n",
            "comment_added_diff": {},
            "comment_deleted_diff": {
                "19": "# These flags are used to compile any C++ source within Numpy.",
                "20": "# They are chosen to have very few runtime dependencies.",
                "22": "    '-std=c++11',  # Minimal standard version",
                "23": "    '-D__STDC_VERSION__=0',  # for compatibility with C headers",
                "24": "    '-fno-exceptions',  # no exception support",
                "25": "    '-fno-rtti']  # no runtime type information"
            },
            "comment_modified_diff": {}
        }
    ],
    "exec_command.py": [
        {
            "commit": "69a39461de4da2bfc0e6aeeff02c73c0972a8614",
            "timestamp": "2022-10-27T10:38:56+13:00",
            "author": "Mike Taves",
            "commit_message": "MAINT: change subprocess arguments from Python>=3.7",
            "additions": 3,
            "deletions": 4,
            "change_type": "MODIFY",
            "diff": "@@ -276,14 +276,13 @@ def _exec_command(command, use_shell=None, use_tee = None, **env):\n     # Inherit environment by default\n     env = env or None\n     try:\n-        # universal_newlines is set to False so that communicate()\n+        # text is set to False so that communicate()\n         # will return bytes. We need to decode the output ourselves\n         # so that Python will not raise a UnicodeDecodeError when\n         # it encounters an invalid character; rather, we simply replace it\n-        proc = subprocess.Popen(command, shell=use_shell, env=env,\n+        proc = subprocess.Popen(command, shell=use_shell, env=env, text=False,\n                                 stdout=subprocess.PIPE,\n-                                stderr=subprocess.STDOUT,\n-                                universal_newlines=False)\n+                                stderr=subprocess.STDOUT)\n     except OSError:\n         # Return 127, as os.spawn*() and /bin/sh do\n         return 127, ''\n",
            "comment_added_diff": {
                "279": "        # text is set to False so that communicate()"
            },
            "comment_deleted_diff": {
                "279": "        # universal_newlines is set to False so that communicate()"
            },
            "comment_modified_diff": {
                "279": "        # universal_newlines is set to False so that communicate()"
            }
        }
    ],
    "lib2def.py": [],
    "misc_util.py": [],
    "test_scalarmath.py": [
        {
            "commit": "343cbafffb68e3dc2c1dca71eea0644751ae924f",
            "timestamp": "2022-10-27T12:03:43+02:00",
            "author": "Sebastian Berg",
            "commit_message": "BUG: `-unsigned_int(0)` no overflow warning",
            "additions": 2,
            "deletions": 0,
            "change_type": "MODIFY",
            "diff": "@@ -923,6 +923,8 @@ def test_scalar_unsigned_integer_overflow(dtype):\n     with pytest.warns(RuntimeWarning, match=\"overflow encountered\"):\n         -val\n \n+    zero = np.dtype(dtype).type(0)\n+    -zero  # does not warn\n \n @pytest.mark.parametrize(\"dtype\", np.typecodes[\"AllInteger\"])\n @pytest.mark.parametrize(\"operation\", [\n",
            "comment_added_diff": {
                "927": "    -zero  # does not warn"
            },
            "comment_deleted_diff": {},
            "comment_modified_diff": {}
        },
        {
            "commit": "5df68d2ff0dec50d8b0f55d39e49f83ead13c497",
            "timestamp": "2022-12-13T18:55:16+01:00",
            "author": "Sebastian Berg",
            "commit_message": "BUG: Fix infinite recursion in longdouble/large integer scalar ops\n\nA smal bug snuck in when implementing NEP 50 weak-scalar logic,\nand unfortunately the tests didn't cover that specific path :/.\n\ncloses gh-22787",
            "additions": 10,
            "deletions": 8,
            "change_type": "MODIFY",
            "diff": "@@ -860,27 +860,29 @@ def test_operator_scalars(op, type1, type2):\n \n \n @pytest.mark.parametrize(\"op\", reasonable_operators_for_scalars)\n-def test_longdouble_inf_loop(op):\n+@pytest.mark.parametrize(\"val\", [None, 2**64])\n+def test_longdouble_inf_loop(op, val):\n+    # Note: The 2**64 value will pass once NEP 50 is adopted.\n     try:\n-        op(np.longdouble(3), None)\n+        op(np.longdouble(3), val)\n     except TypeError:\n         pass\n     try:\n-        op(None, np.longdouble(3))\n+        op(val, np.longdouble(3))\n     except TypeError:\n         pass\n \n \n @pytest.mark.parametrize(\"op\", reasonable_operators_for_scalars)\n-def test_clongdouble_inf_loop(op):\n-    if op in {operator.mod} and False:\n-        pytest.xfail(\"The modulo operator is known to be broken\")\n+@pytest.mark.parametrize(\"val\", [None, 2**64])\n+def test_clongdouble_inf_loop(op, val):\n+    # Note: The 2**64 value will pass once NEP 50 is adopted.\n     try:\n-        op(np.clongdouble(3), None)\n+        op(np.clongdouble(3), val)\n     except TypeError:\n         pass\n     try:\n-        op(None, np.longdouble(3))\n+        op(val, np.longdouble(3))\n     except TypeError:\n         pass\n \n",
            "comment_added_diff": {
                "865": "    # Note: The 2**64 value will pass once NEP 50 is adopted.",
                "879": "    # Note: The 2**64 value will pass once NEP 50 is adopted."
            },
            "comment_deleted_diff": {},
            "comment_modified_diff": {
                "865": "        op(np.longdouble(3), None)",
                "879": "        op(np.clongdouble(3), None)"
            }
        },
        {
            "commit": "d3501206677a4bccdf4b5ab6c9365a84ef26096e",
            "timestamp": "2023-01-24T12:56:52+01:00",
            "author": "Sebastian Berg",
            "commit_message": "TST: Expand scalar/umath comparison tests especially for dtypes",
            "additions": 42,
            "deletions": 12,
            "change_type": "MODIFY",
            "diff": "@@ -75,17 +75,7 @@ def test_leak(self):\n             np.add(1, 1)\n \n \n-@pytest.mark.slow\n-@settings(max_examples=10000, deadline=2000)\n-@given(sampled_from(reasonable_operators_for_scalars),\n-       hynp.arrays(dtype=hynp.scalar_dtypes(), shape=()),\n-       hynp.arrays(dtype=hynp.scalar_dtypes(), shape=()))\n-def test_array_scalar_ufunc_equivalence(op, arr1, arr2):\n-    \"\"\"\n-    This is a thorough test attempting to cover important promotion paths\n-    and ensuring that arrays and scalars stay as aligned as possible.\n-    However, if it creates troubles, it should maybe just be removed.\n-    \"\"\"\n+def check_ufunc_scalar_equivalence(op, arr1, arr2):\n     scalar1 = arr1[()]\n     scalar2 = arr2[()]\n     assert isinstance(scalar1, np.generic)\n@@ -107,7 +97,47 @@ def test_array_scalar_ufunc_equivalence(op, arr1, arr2):\n                 op(scalar1, scalar2)\n         else:\n             scalar_res = op(scalar1, scalar2)\n-            assert_array_equal(scalar_res, res)\n+            assert_array_equal(scalar_res, res, strict=True)\n+\n+\n+@pytest.mark.slow\n+@settings(max_examples=10000, deadline=2000)\n+@given(sampled_from(reasonable_operators_for_scalars),\n+       hynp.arrays(dtype=hynp.scalar_dtypes(), shape=()),\n+       hynp.arrays(dtype=hynp.scalar_dtypes(), shape=()))\n+def test_array_scalar_ufunc_equivalence(op, arr1, arr2):\n+    \"\"\"\n+    This is a thorough test attempting to cover important promotion paths\n+    and ensuring that arrays and scalars stay as aligned as possible.\n+    However, if it creates troubles, it should maybe just be removed.\n+    \"\"\"\n+    check_ufunc_scalar_equivalence(op, arr1, arr2)\n+\n+\n+@pytest.mark.slow\n+@given(sampled_from(reasonable_operators_for_scalars),\n+       hynp.scalar_dtypes(), hynp.scalar_dtypes())\n+def test_array_scalar_ufunc_dtypes(op, dt1, dt2):\n+    # Same as above, but don't worry about sampling weird values so that we\n+    # do not have to sample as much\n+    arr1 = np.array(2, dtype=dt1)\n+    arr2 = np.array(1, dtype=dt2)  # power of 2 does weird things for arrays\n+\n+    check_ufunc_scalar_equivalence(op, arr1, arr2)\n+\n+\n+@pytest.mark.parametrize(\"fscalar\", [np.float16, np.float32])\n+def test_int_float_promotion_truediv(fscalar):\n+    # Promotion for mixed int and float32/float16 must not go to float64\n+    i = np.int8(1)\n+    f = fscalar(1)\n+    expected = np.result_type(i, f)\n+    assert (i / f).dtype == expected\n+    assert (f / i).dtype == expected\n+    # But normal int / int true division goes to float64:\n+    assert (i / i).dtype == np.dtype(\"float64\")\n+    # For int16, result has to be ast least float32 (takes ufunc path):\n+    assert (np.int16(1) / f).dtype == np.dtype(\"float32\")\n \n \n class TestBaseMath:\n",
            "comment_added_diff": {
                "121": "    # Same as above, but don't worry about sampling weird values so that we",
                "122": "    # do not have to sample as much",
                "124": "    arr2 = np.array(1, dtype=dt2)  # power of 2 does weird things for arrays",
                "131": "    # Promotion for mixed int and float32/float16 must not go to float64",
                "137": "    # But normal int / int true division goes to float64:",
                "139": "    # For int16, result has to be ast least float32 (takes ufunc path):"
            },
            "comment_deleted_diff": {},
            "comment_modified_diff": {}
        },
        {
            "commit": "30b62f80b18414d340d421b56f5698f920fa4526",
            "timestamp": "2023-01-24T15:35:04+01:00",
            "author": "Sebastian Berg",
            "commit_message": "TST: Explicitly ignore promotion issues for array**2 in hypothesis test",
            "additions": 6,
            "deletions": 1,
            "change_type": "MODIFY",
            "diff": "@@ -85,6 +85,11 @@ def check_ufunc_scalar_equivalence(op, arr1, arr2):\n         comp_ops = {operator.ge, operator.gt, operator.le, operator.lt}\n         if op in comp_ops and (np.isnan(scalar1) or np.isnan(scalar2)):\n             pytest.xfail(\"complex comp ufuncs use sort-order, scalars do not.\")\n+    if op == operator.pow and arr2.item() in [-1, 0, 0.5, 1, 2]:\n+        # array**scalar special case can have different result dtype\n+        # (Other powers may have issues also, but are not hit here.)\n+        # TODO: It would be nice to resolve this issue.\n+        pytest.skip(\"array**2 can have incorrect/weird result dtype\")\n \n     # ignore fpe's since they may just mismatch for integers anyway.\n     with warnings.catch_warnings(), np.errstate(all=\"ignore\"):\n@@ -121,7 +126,7 @@ def test_array_scalar_ufunc_dtypes(op, dt1, dt2):\n     # Same as above, but don't worry about sampling weird values so that we\n     # do not have to sample as much\n     arr1 = np.array(2, dtype=dt1)\n-    arr2 = np.array(1, dtype=dt2)  # power of 2 does weird things for arrays\n+    arr2 = np.array(3, dtype=dt2)  # some power do weird things.\n \n     check_ufunc_scalar_equivalence(op, arr1, arr2)\n \n",
            "comment_added_diff": {
                "89": "        # array**scalar special case can have different result dtype",
                "90": "        # (Other powers may have issues also, but are not hit here.)",
                "91": "        # TODO: It would be nice to resolve this issue.",
                "129": "    arr2 = np.array(3, dtype=dt2)  # some power do weird things."
            },
            "comment_deleted_diff": {
                "124": "    arr2 = np.array(1, dtype=dt2)  # power of 2 does weird things for arrays"
            },
            "comment_modified_diff": {}
        }
    ],
    "test_deprecations.py": [
        {
            "commit": "2c72fbf88f81ba7a7dc7fd83d737fe4139ec7133",
            "timestamp": "2022-11-07T16:05:17+01:00",
            "author": "Sebastian Berg",
            "commit_message": "DEP: Expire deprecation of dtype/signature allowing instances\n\nWe never really allowed instances here and deprecated it since\nNumPy 1.21 (it just failed completely in 1.21.0).",
            "additions": 0,
            "deletions": 33,
            "change_type": "MODIFY",
            "diff": "@@ -1044,39 +1044,6 @@ def test_not_deprecated(self, name: str) -> None:\n         self.assert_not_deprecated(lambda: getattr(self.ctypes, name))\n \n \n-class TestUFuncForcedDTypeWarning(_DeprecationTestCase):\n-    message = \"The `dtype` and `signature` arguments to ufuncs only select the\"\n-\n-    def test_not_deprecated(self):\n-        import pickle\n-        # does not warn (test relies on bad pickling behaviour, simply remove\n-        # it if the `assert int64 is not int64_2` should start failing.\n-        int64 = np.dtype(\"int64\")\n-        int64_2 = pickle.loads(pickle.dumps(int64))\n-        assert int64 is not int64_2\n-        self.assert_not_deprecated(lambda: np.add(3, 4, dtype=int64_2))\n-\n-    def test_deprecation(self):\n-        int64 = np.dtype(\"int64\")\n-        self.assert_deprecated(lambda: np.add(3, 5, dtype=int64.newbyteorder()))\n-        self.assert_deprecated(lambda: np.add(3, 5, dtype=\"m8[ns]\"))\n-\n-    def test_behaviour(self):\n-        int64 = np.dtype(\"int64\")\n-        arr = np.arange(10, dtype=\"m8[s]\")\n-\n-        with pytest.warns(DeprecationWarning, match=self.message):\n-            np.add(3, 5, dtype=int64.newbyteorder())\n-        with pytest.warns(DeprecationWarning, match=self.message):\n-            np.add(3, 5, dtype=\"m8[ns]\")  # previously used the \"ns\"\n-        with pytest.warns(DeprecationWarning, match=self.message):\n-            np.add(arr, arr, dtype=\"m8[ns]\")  # never preserved the \"ns\"\n-        with pytest.warns(DeprecationWarning, match=self.message):\n-            np.maximum(arr, arr, dtype=\"m8[ns]\")  # previously used the \"ns\"\n-        with pytest.warns(DeprecationWarning, match=self.message):\n-            np.maximum.reduce(arr, dtype=\"m8[ns]\")  # never preserved the \"ns\"\n-\n-\n PARTITION_DICT = {\n     \"partition method\": np.arange(10).partition,\n     \"argpartition method\": np.arange(10).argpartition,\n",
            "comment_added_diff": {},
            "comment_deleted_diff": {
                "1052": "        # does not warn (test relies on bad pickling behaviour, simply remove",
                "1053": "        # it if the `assert int64 is not int64_2` should start failing.",
                "1071": "            np.add(3, 5, dtype=\"m8[ns]\")  # previously used the \"ns\"",
                "1073": "            np.add(arr, arr, dtype=\"m8[ns]\")  # never preserved the \"ns\"",
                "1075": "            np.maximum(arr, arr, dtype=\"m8[ns]\")  # previously used the \"ns\"",
                "1077": "            np.maximum.reduce(arr, dtype=\"m8[ns]\")  # never preserved the \"ns\""
            },
            "comment_modified_diff": {}
        },
        {
            "commit": "10908c5fd147572c9ea02cef0c0d5eae6892e4cb",
            "timestamp": "2022-11-07T14:42:32-08:00",
            "author": "Sebastian Berg",
            "commit_message": "DEP: Expire deprecation to ignore bad dtype= in logical ufuncs (#22541)\n\n* DEP: Expire deprecation to ignore bad dtype= in logical ufuncs\r\n\r\nBasically only `bool` and `object dtype make sense, but they did\r\nnot work correctly.  The dtype argument was rather ignored often.\r\n\r\nThe offending behavior was deprecated in 1.20 and is now removed.\r\n\r\nCo-authored-by: Sebastian Berg <sebastianb@nvidia.com>\r\nCo-authored-by: Charles Harris <charlesr.harris@gmail.com>",
            "additions": 0,
            "deletions": 25,
            "change_type": "MODIFY",
            "diff": "@@ -1000,31 +1000,6 @@ def test_deprecated(self):\n         self.assert_deprecated(lambda: np.add(1, 2, sig=(np.dtype(\"l\"),)))\n \n \n-class TestComparisonBadDType(_DeprecationTestCase):\n-    # Deprecated 2021-04-01, NumPy 1.21\n-    message = r\"using `dtype=` in comparisons is only useful for\"\n-\n-    def test_deprecated(self):\n-        self.assert_deprecated(lambda: np.equal(1, 1, dtype=np.int64))\n-        # Not an error only for the transition\n-        self.assert_deprecated(lambda: np.equal(1, 1, sig=(None, None, \"l\")))\n-\n-    def test_not_deprecated(self):\n-        np.equal(True, False, dtype=bool)\n-        np.equal(3, 5, dtype=bool, casting=\"unsafe\")\n-        np.equal([None], [4], dtype=object)\n-\n-class TestComparisonBadObjectDType(_DeprecationTestCase):\n-    # Deprecated 2021-04-01, NumPy 1.21  (different branch of the above one)\n-    message = r\"using `dtype=object` \\(or equivalent signature\\) will\"\n-    warning_cls = FutureWarning\n-\n-    def test_deprecated(self):\n-        self.assert_deprecated(lambda: np.equal(1, 1, dtype=object))\n-        self.assert_deprecated(\n-                lambda: np.equal(1, 1, sig=(None, None, object)))\n-\n-\n class TestCtypesGetter(_DeprecationTestCase):\n     # Deprecated 2021-05-18, Numpy 1.21.0\n     warning_cls = DeprecationWarning\n",
            "comment_added_diff": {},
            "comment_deleted_diff": {
                "1004": "    # Deprecated 2021-04-01, NumPy 1.21",
                "1009": "        # Not an error only for the transition",
                "1018": "    # Deprecated 2021-04-01, NumPy 1.21  (different branch of the above one)"
            },
            "comment_modified_diff": {}
        },
        {
            "commit": "075859216fae0509c52a54cb5c96c217f23026ca",
            "timestamp": "2022-11-17T14:21:54+01:00",
            "author": "Sebastian Berg",
            "commit_message": "DEP: Next step in scalar type alias deprecations/futurewarnings\n\nFinalizes the scalar type alias deprecations making them an error.\nHowever, at the same time adds a `FutureWarning` that new aliases\nwill be introduced in the future.\n(They would eventually be preferred over the `str_`, etc. version.)\n\nIt may make sense, that this FutureWarning is already propelled soon\nsince it interacts with things such as changing the representation of\nstrings to `np.str_(\"\")` if the preferred alias becomes `np.str`.\n\nIt also introduces a new deprecation to remove the 0 sized bit-aliases\nand the bitsize `bool8` alias.  (Unfortunately, these are here still allowed\nas part of the `np.sctypeDict`).",
            "additions": 25,
            "deletions": 22,
            "change_type": "MODIFY",
            "diff": "@@ -583,25 +583,6 @@ def test_non_exact_match(self):\n         self.assert_deprecated(lambda: np.searchsorted(arr[0], 4, side='Random'))\n \n \n-class TestDeprecatedGlobals(_DeprecationTestCase):\n-    # 2020-06-06\n-    def test_type_aliases(self):\n-        # from builtins\n-        self.assert_deprecated(lambda: np.bool(True))\n-        self.assert_deprecated(lambda: np.int(1))\n-        self.assert_deprecated(lambda: np.float(1))\n-        self.assert_deprecated(lambda: np.complex(1))\n-        self.assert_deprecated(lambda: np.object())\n-        self.assert_deprecated(lambda: np.str('abc'))\n-\n-        # from np.compat\n-        self.assert_deprecated(lambda: np.long(1))\n-        self.assert_deprecated(lambda: np.unicode('abc'))\n-\n-        # from np.core.numerictypes\n-        self.assert_deprecated(lambda: np.typeDict)\n-\n-\n class TestMatrixInOuter(_DeprecationTestCase):\n     # 2020-05-13 NumPy 1.20.0\n     message = (r\"add.outer\\(\\) was passed a numpy matrix as \"\n@@ -1046,9 +1027,6 @@ class TestMachAr(_DeprecationTestCase):\n     # Deprecated 2021-10-19, NumPy 1.22\n     warning_cls = DeprecationWarning\n \n-    def test_deprecated(self):\n-        self.assert_deprecated(lambda: np.MachAr)\n-\n     def test_deprecated_module(self):\n         self.assert_deprecated(lambda: getattr(np.core, \"machar\"))\n \n@@ -1172,3 +1150,28 @@ def create(value, dtype):\n                         lambda: creation_func(info.max + 1, dtype))\n             except OverflowError:\n                 pass  # OverflowErrors always happened also before and are OK.\n+\n+\n+class TestDeprecatedGlobals(_DeprecationTestCase):\n+    # Deprecated 2022-11-17, NumPy 1.24\n+    def test_type_aliases(self):\n+        # from builtins\n+        self.assert_deprecated(lambda: np.bool8)\n+        self.assert_deprecated(lambda: np.int0)\n+        self.assert_deprecated(lambda: np.uint0)\n+        self.assert_deprecated(lambda: np.bytes0)\n+        self.assert_deprecated(lambda: np.str0)\n+        self.assert_deprecated(lambda: np.object0)\n+\n+\n+@pytest.mark.parametrize(\"name\",\n+        [\"bool\", \"long\", \"ulong\", \"str\", \"bytes\", \"object\"])\n+def test_future_scalar_attributes(name):\n+    # FutureWarning added 2022-11-17, NumPy 1.24,\n+    assert name not in dir(np)  # we may want to not add them\n+    with pytest.warns(FutureWarning):\n+        assert not hasattr(np, name)\n+\n+    # Unfortunately, they are currently still valid via `np.dtype()`\n+    np.dtype(name)\n+    name in np.sctypeDict\n",
            "comment_added_diff": {
                "1156": "    # Deprecated 2022-11-17, NumPy 1.24",
                "1158": "        # from builtins",
                "1170": "    # FutureWarning added 2022-11-17, NumPy 1.24,",
                "1171": "    assert name not in dir(np)  # we may want to not add them",
                "1175": "    # Unfortunately, they are currently still valid via `np.dtype()`"
            },
            "comment_deleted_diff": {
                "587": "    # 2020-06-06",
                "589": "        # from builtins",
                "597": "        # from np.compat",
                "601": "        # from np.core.numerictypes"
            },
            "comment_modified_diff": {}
        },
        {
            "commit": "8b9b0efbc08a502627f455ec59656fce68eb10d7",
            "timestamp": "2022-11-22T17:38:33+01:00",
            "author": "Sebastian Berg",
            "commit_message": "DEP: Finalize MachAr and machar deprecations\n\nThis removes the attributes on finfo and the \"public\" module.  It also\ndeprecates `np.core.MachAr`.  We should be able to get away with just\ndeleting it, but there seems little reason to not just deprecate it for now.",
            "additions": 2,
            "deletions": 6,
            "change_type": "MODIFY",
            "diff": "@@ -1024,15 +1024,11 @@ def test_not_deprecated(self, func):\n \n \n class TestMachAr(_DeprecationTestCase):\n-    # Deprecated 2021-10-19, NumPy 1.22\n+    # Deprecated 2022-11-22, NumPy 1.25\n     warning_cls = DeprecationWarning\n \n     def test_deprecated_module(self):\n-        self.assert_deprecated(lambda: getattr(np.core, \"machar\"))\n-\n-    def test_deprecated_attr(self):\n-        finfo = np.finfo(float)\n-        self.assert_deprecated(lambda: getattr(finfo, \"machar\"))\n+        self.assert_deprecated(lambda: getattr(np.core, \"MachAr\"))\n \n \n class TestQuantileInterpolationDeprecation(_DeprecationTestCase):\n",
            "comment_added_diff": {
                "1027": "    # Deprecated 2022-11-22, NumPy 1.25"
            },
            "comment_deleted_diff": {
                "1027": "    # Deprecated 2021-10-19, NumPy 1.22"
            },
            "comment_modified_diff": {
                "1027": "    # Deprecated 2021-10-19, NumPy 1.22"
            }
        },
        {
            "commit": "4929777626bd7263141228de23b32c17dfbfd2b6",
            "timestamp": "2022-12-01T17:58:04+01:00",
            "author": "Sebastian Berg",
            "commit_message": "TST: Remove old deprecation test and convert/add new ones",
            "additions": 0,
            "deletions": 74,
            "change_type": "MODIFY",
            "diff": "@@ -138,80 +138,6 @@ class _VisibleDeprecationTestCase(_DeprecationTestCase):\n     warning_cls = np.VisibleDeprecationWarning\n \n \n-class TestComparisonDeprecations(_DeprecationTestCase):\n-    \"\"\"This tests the deprecation, for non-element-wise comparison logic.\n-    This used to mean that when an error occurred during element-wise comparison\n-    (i.e. broadcasting) NotImplemented was returned, but also in the comparison\n-    itself, False was given instead of the error.\n-\n-    Also test FutureWarning for the None comparison.\n-    \"\"\"\n-\n-    message = \"elementwise.* comparison failed; .*\"\n-\n-    def test_normal_types(self):\n-        for op in (operator.eq, operator.ne):\n-            # Broadcasting errors:\n-            self.assert_deprecated(op, args=(np.zeros(3), []))\n-            a = np.zeros(3, dtype='i,i')\n-            # (warning is issued a couple of times here)\n-            self.assert_deprecated(op, args=(a, a[:-1]), num=None)\n-\n-            # ragged array comparison returns True/False\n-            a = np.array([1, np.array([1,2,3])], dtype=object)\n-            b = np.array([1, np.array([1,2,3])], dtype=object)\n-            self.assert_deprecated(op, args=(a, b), num=None)\n-\n-    def test_string(self):\n-        # For two string arrays, strings always raised the broadcasting error:\n-        a = np.array(['a', 'b'])\n-        b = np.array(['a', 'b', 'c'])\n-        assert_warns(FutureWarning, lambda x, y: x == y, a, b)\n-\n-        # The empty list is not cast to string, and this used to pass due\n-        # to dtype mismatch; now (2018-06-21) it correctly leads to a\n-        # FutureWarning.\n-        assert_warns(FutureWarning, lambda: a == [])\n-\n-    def test_void_dtype_equality_failures(self):\n-        class NotArray:\n-            def __array__(self):\n-                raise TypeError\n-\n-            # Needed so Python 3 does not raise DeprecationWarning twice.\n-            def __ne__(self, other):\n-                return NotImplemented\n-\n-        self.assert_deprecated(lambda: np.arange(2) == NotArray())\n-        self.assert_deprecated(lambda: np.arange(2) != NotArray())\n-\n-    def test_array_richcompare_legacy_weirdness(self):\n-        # It doesn't really work to use assert_deprecated here, b/c part of\n-        # the point of assert_deprecated is to check that when warnings are\n-        # set to \"error\" mode then the error is propagated -- which is good!\n-        # But here we are testing a bunch of code that is deprecated *because*\n-        # it has the habit of swallowing up errors and converting them into\n-        # different warnings. So assert_warns will have to be sufficient.\n-        assert_warns(FutureWarning, lambda: np.arange(2) == \"a\")\n-        assert_warns(FutureWarning, lambda: np.arange(2) != \"a\")\n-        # No warning for scalar comparisons\n-        with warnings.catch_warnings():\n-            warnings.filterwarnings(\"error\")\n-            assert_(not (np.array(0) == \"a\"))\n-            assert_(np.array(0) != \"a\")\n-            assert_(not (np.int16(0) == \"a\"))\n-            assert_(np.int16(0) != \"a\")\n-\n-        for arg1 in [np.asarray(0), np.int16(0)]:\n-            struct = np.zeros(2, dtype=\"i4,i4\")\n-            for arg2 in [struct, \"a\"]:\n-                for f in [operator.lt, operator.le, operator.gt, operator.ge]:\n-                    with warnings.catch_warnings() as l:\n-                        warnings.filterwarnings(\"always\")\n-                        assert_raises(TypeError, f, arg1, arg2)\n-                        assert_(not l)\n-\n-\n class TestDatetime64Timezone(_DeprecationTestCase):\n     \"\"\"Parsing of datetime64 with timezones deprecated in 1.11.0, because\n     datetime64 is now timezone naive rather than UTC only.\n",
            "comment_added_diff": {},
            "comment_deleted_diff": {
                "154": "            # Broadcasting errors:",
                "157": "            # (warning is issued a couple of times here)",
                "160": "            # ragged array comparison returns True/False",
                "166": "        # For two string arrays, strings always raised the broadcasting error:",
                "171": "        # The empty list is not cast to string, and this used to pass due",
                "172": "        # to dtype mismatch; now (2018-06-21) it correctly leads to a",
                "173": "        # FutureWarning.",
                "181": "            # Needed so Python 3 does not raise DeprecationWarning twice.",
                "189": "        # It doesn't really work to use assert_deprecated here, b/c part of",
                "190": "        # the point of assert_deprecated is to check that when warnings are",
                "191": "        # set to \"error\" mode then the error is propagated -- which is good!",
                "192": "        # But here we are testing a bunch of code that is deprecated *because*",
                "193": "        # it has the habit of swallowing up errors and converting them into",
                "194": "        # different warnings. So assert_warns will have to be sufficient.",
                "197": "        # No warning for scalar comparisons"
            },
            "comment_modified_diff": {}
        },
        {
            "commit": "ceccabebc444df4be7a2d744bd5c76854799df38",
            "timestamp": "2023-01-05T17:15:51+01:00",
            "author": "Sebastian Berg",
            "commit_message": "MAINT: Add additional information to missing scalar AttributeError\n\nThis is a followup on gh-22607 which removed them.  Since it appears some\nusers missed the DeprecationWarning entirely, it may help them to\ninclude the old information as an attribute error.\n\nAn example is:\n```\nIn [1]: np.int\n\nAttributeError: module 'numpy' has no attribute 'int'.\n`np.int` was a deprecated alias for the builtin `int`. To avoid this error in existing code, use `int` by itself.\nDoing this will not modify any behavior and is safe. When replacing `np.int`, you may wish to use e.g. `np.int64`\nor `np.int32` to specify the precision. If you wish to review your current use, check the release note link for\nadditional information.\nThe aliases was originally deprecated in NumPy 1.20; for more details and guidance see the original release note\nat:\n    https://numpy.org/devdocs/release/1.20.0-notes.html#deprecations\n```\n\nYes, that is very verbose...\n\nyour changes. Lines starting",
            "additions": 15,
            "deletions": 0,
            "change_type": "MODIFY",
            "diff": "@@ -1098,3 +1098,18 @@ def test_future_scalar_attributes(name):\n     # Unfortunately, they are currently still valid via `np.dtype()`\n     np.dtype(name)\n     name in np.sctypeDict\n+\n+\n+# Ignore the above future attribute warning for this test.\n+@pytest.mark.filterwarnings(\"ignore:In the future:FutureWarning\")\n+class TestRemovedGlobals:\n+    # Removed 2023-01-12, NumPy 1.24.0\n+    # Not a deprecation, but the large error was added to aid those who missed\n+    # the previous deprecation, and should be removed similarly to one\n+    # (or faster).\n+    @pytest.mark.parametrize(\"name\",\n+            [\"object\", \"bool\", \"float\", \"complex\", \"str\", \"int\"])\n+    def test_attributeerror_includes_info(self, name):\n+        msg = f\".*\\n`np.{name}` was a deprecated alias for the builtin\"\n+        with pytest.raises(AttributeError, match=msg):\n+            getattr(np, name)\n",
            "comment_added_diff": {
                "1103": "# Ignore the above future attribute warning for this test.",
                "1106": "    # Removed 2023-01-12, NumPy 1.24.0",
                "1107": "    # Not a deprecation, but the large error was added to aid those who missed",
                "1108": "    # the previous deprecation, and should be removed similarly to one",
                "1109": "    # (or faster)."
            },
            "comment_deleted_diff": {},
            "comment_modified_diff": {}
        },
        {
            "commit": "9e98e33417621171dc84bdaafbb1b337b8bd92bd",
            "timestamp": "2023-01-11T22:24:10+01:00",
            "author": "Sebastian Berg",
            "commit_message": "DEP: Finalize `+arr` returning a copy e.g. for string arrays\n\nThis was deprecated 4-5 years ago in NumPy 1.16.  Pandas stumbled\nover it cleaning up their warning filters, so I decided to just\nexpire it.",
            "additions": 0,
            "deletions": 6,
            "change_type": "MODIFY",
            "diff": "@@ -310,12 +310,6 @@ def test_generator_sum(self):\n         self.assert_deprecated(np.sum, args=((i for i in range(5)),))\n \n \n-class TestPositiveOnNonNumerical(_DeprecationTestCase):\n-    # 2018-06-28, 1.16.0\n-    def test_positive_on_non_number(self):\n-        self.assert_deprecated(operator.pos, args=(np.array('foo'),))\n-\n-\n class TestFromstring(_DeprecationTestCase):\n     # 2017-10-19, 1.14\n     def test_fromstring(self):\n",
            "comment_added_diff": {},
            "comment_deleted_diff": {
                "314": "    # 2018-06-28, 1.16.0"
            },
            "comment_modified_diff": {}
        },
        {
            "commit": "0cfbd3c4646aa867b89da2aef16c3a0c693d0303",
            "timestamp": "2023-01-19T18:20:49+01:00",
            "author": "Daiki Shintani",
            "commit_message": "DEP: deprecate np.finfo(None) (#23011)\n\nDeprecate np.finfo(None), it may be that we should more generally deprecate `np.dtype(None)` but this is a start and particularly weird maybe.\r\n\r\nCloses gh-14684",
            "additions": 6,
            "deletions": 0,
            "change_type": "MODIFY",
            "diff": "@@ -1107,3 +1107,9 @@ def test_attributeerror_includes_info(self, name):\n         msg = f\".*\\n`np.{name}` was a deprecated alias for the builtin\"\n         with pytest.raises(AttributeError, match=msg):\n             getattr(np, name)\n+\n+\n+class TestDeprecatedFinfo(_DeprecationTestCase):\n+    # Deprecated in NumPy 1.25, 2023-01-16\n+    def test_deprecated_none(self):\n+        self.assert_deprecated(np.finfo, args=(None,))\n",
            "comment_added_diff": {
                "1113": "    # Deprecated in NumPy 1.25, 2023-01-16"
            },
            "comment_deleted_diff": {},
            "comment_modified_diff": {}
        },
        {
            "commit": "f75bb0edb0e6eec2564de4bf798242984860a19b",
            "timestamp": "2023-01-19T14:35:44-07:00",
            "author": "Charles Harris",
            "commit_message": "MAINT: Remove all nose testing support.\n\nNumPy switched to using pytest in 2018 and nose has been unmaintained\nfor many years. We have kept NumPy's nose support to avoid breaking\ndownstream projects who might have been using it and not yet switched to\npytest or some other testing framework. With the arrival of Python 3.12,\nunpatched nose will raise an error. It it time to move on.\n\nDecorators removed\n\n- raises\n- slow\n- setastest\n- skipif\n- knownfailif\n- deprecated\n- parametrize\n- _needs_refcount\n\nThese are not to be confused with pytest versions with similar names,\ne.g., pytest.mark.slow, pytest.mark.skipif, pytest.mark.parametrize.\n\nFunctions removed\n\n- Tester\n- import_nose\n- run_module_suite",
            "additions": 0,
            "deletions": 212,
            "change_type": "MODIFY",
            "diff": "@@ -679,218 +679,6 @@ def test_deprecated(self):\n         ctor = np.core.multiarray.scalar\n         self.assert_deprecated(lambda: ctor(np.dtype(\"O\"), 1))\n \n-try:\n-    with warnings.catch_warnings():\n-        warnings.simplefilter(\"always\")\n-        import nose  # noqa: F401\n-except ImportError:\n-    HAVE_NOSE = False\n-else:\n-    HAVE_NOSE = True\n-\n-\n-@pytest.mark.skipif(not HAVE_NOSE, reason=\"Needs nose\")\n-class TestNoseDecoratorsDeprecated(_DeprecationTestCase):\n-    class DidntSkipException(Exception):\n-        pass\n-\n-    def test_slow(self):\n-        def _test_slow():\n-            @np.testing.dec.slow\n-            def slow_func(x, y, z):\n-                pass\n-\n-            assert_(slow_func.slow)\n-        self.assert_deprecated(_test_slow)\n-\n-    def test_setastest(self):\n-        def _test_setastest():\n-            @np.testing.dec.setastest()\n-            def f_default(a):\n-                pass\n-\n-            @np.testing.dec.setastest(True)\n-            def f_istest(a):\n-                pass\n-\n-            @np.testing.dec.setastest(False)\n-            def f_isnottest(a):\n-                pass\n-\n-            assert_(f_default.__test__)\n-            assert_(f_istest.__test__)\n-            assert_(not f_isnottest.__test__)\n-        self.assert_deprecated(_test_setastest, num=3)\n-\n-    def test_skip_functions_hardcoded(self):\n-        def _test_skip_functions_hardcoded():\n-            @np.testing.dec.skipif(True)\n-            def f1(x):\n-                raise self.DidntSkipException\n-\n-            try:\n-                f1('a')\n-            except self.DidntSkipException:\n-                raise Exception('Failed to skip')\n-            except SkipTest().__class__:\n-                pass\n-\n-            @np.testing.dec.skipif(False)\n-            def f2(x):\n-                raise self.DidntSkipException\n-\n-            try:\n-                f2('a')\n-            except self.DidntSkipException:\n-                pass\n-            except SkipTest().__class__:\n-                raise Exception('Skipped when not expected to')\n-        self.assert_deprecated(_test_skip_functions_hardcoded, num=2)\n-\n-    def test_skip_functions_callable(self):\n-        def _test_skip_functions_callable():\n-            def skip_tester():\n-                return skip_flag == 'skip me!'\n-\n-            @np.testing.dec.skipif(skip_tester)\n-            def f1(x):\n-                raise self.DidntSkipException\n-\n-            try:\n-                skip_flag = 'skip me!'\n-                f1('a')\n-            except self.DidntSkipException:\n-                raise Exception('Failed to skip')\n-            except SkipTest().__class__:\n-                pass\n-\n-            @np.testing.dec.skipif(skip_tester)\n-            def f2(x):\n-                raise self.DidntSkipException\n-\n-            try:\n-                skip_flag = 'five is right out!'\n-                f2('a')\n-            except self.DidntSkipException:\n-                pass\n-            except SkipTest().__class__:\n-                raise Exception('Skipped when not expected to')\n-        self.assert_deprecated(_test_skip_functions_callable, num=2)\n-\n-    def test_skip_generators_hardcoded(self):\n-        def _test_skip_generators_hardcoded():\n-            @np.testing.dec.knownfailureif(True, \"This test is known to fail\")\n-            def g1(x):\n-                yield from range(x)\n-\n-            try:\n-                for j in g1(10):\n-                    pass\n-            except KnownFailureException().__class__:\n-                pass\n-            else:\n-                raise Exception('Failed to mark as known failure')\n-\n-            @np.testing.dec.knownfailureif(False, \"This test is NOT known to fail\")\n-            def g2(x):\n-                yield from range(x)\n-                raise self.DidntSkipException('FAIL')\n-\n-            try:\n-                for j in g2(10):\n-                    pass\n-            except KnownFailureException().__class__:\n-                raise Exception('Marked incorrectly as known failure')\n-            except self.DidntSkipException:\n-                pass\n-        self.assert_deprecated(_test_skip_generators_hardcoded, num=2)\n-\n-    def test_skip_generators_callable(self):\n-        def _test_skip_generators_callable():\n-            def skip_tester():\n-                return skip_flag == 'skip me!'\n-\n-            @np.testing.dec.knownfailureif(skip_tester, \"This test is known to fail\")\n-            def g1(x):\n-                yield from range(x)\n-\n-            try:\n-                skip_flag = 'skip me!'\n-                for j in g1(10):\n-                    pass\n-            except KnownFailureException().__class__:\n-                pass\n-            else:\n-                raise Exception('Failed to mark as known failure')\n-\n-            @np.testing.dec.knownfailureif(skip_tester, \"This test is NOT known to fail\")\n-            def g2(x):\n-                yield from range(x)\n-                raise self.DidntSkipException('FAIL')\n-\n-            try:\n-                skip_flag = 'do not skip'\n-                for j in g2(10):\n-                    pass\n-            except KnownFailureException().__class__:\n-                raise Exception('Marked incorrectly as known failure')\n-            except self.DidntSkipException:\n-                pass\n-        self.assert_deprecated(_test_skip_generators_callable, num=2)\n-\n-    def test_deprecated(self):\n-        def _test_deprecated():\n-            @np.testing.dec.deprecated(True)\n-            def non_deprecated_func():\n-                pass\n-\n-            @np.testing.dec.deprecated()\n-            def deprecated_func():\n-                import warnings\n-                warnings.warn(\"TEST: deprecated func\", DeprecationWarning, stacklevel=1)\n-\n-            @np.testing.dec.deprecated()\n-            def deprecated_func2():\n-                import warnings\n-                warnings.warn(\"AHHHH\", stacklevel=1)\n-                raise ValueError\n-\n-            @np.testing.dec.deprecated()\n-            def deprecated_func3():\n-                import warnings\n-                warnings.warn(\"AHHHH\", stacklevel=1)\n-\n-            # marked as deprecated, but does not raise DeprecationWarning\n-            assert_raises(AssertionError, non_deprecated_func)\n-            # should be silent\n-            deprecated_func()\n-            with warnings.catch_warnings(record=True):\n-                warnings.simplefilter(\"always\")  # do not propagate unrelated warnings\n-                # fails if deprecated decorator just disables test. See #1453.\n-                assert_raises(ValueError, deprecated_func2)\n-                # warning is not a DeprecationWarning\n-                assert_raises(AssertionError, deprecated_func3)\n-        self.assert_deprecated(_test_deprecated, num=4)\n-\n-    def test_parametrize(self):\n-        def _test_parametrize():\n-            # dec.parametrize assumes that it is being run by nose. Because\n-            # we are running under pytest, we need to explicitly check the\n-            # results.\n-            @np.testing.dec.parametrize('base, power, expected',\n-                    [(1, 1, 1),\n-                    (2, 1, 2),\n-                    (2, 2, 4)])\n-            def check_parametrize(base, power, expected):\n-                assert_(base**power == expected)\n-\n-            count = 0\n-            for test in check_parametrize():\n-                test[0](*test[1:])\n-                count += 1\n-            assert_(count == 3)\n-        self.assert_deprecated(_test_parametrize)\n-\n \n class TestSingleElementSignature(_DeprecationTestCase):\n     # Deprecated 2021-04-01, NumPy 1.21\n",
            "comment_added_diff": {},
            "comment_deleted_diff": {
                "685": "        import nose  # noqa: F401",
                "863": "            # marked as deprecated, but does not raise DeprecationWarning",
                "865": "            # should be silent",
                "868": "                warnings.simplefilter(\"always\")  # do not propagate unrelated warnings",
                "869": "                # fails if deprecated decorator just disables test. See #1453.",
                "871": "                # warning is not a DeprecationWarning",
                "877": "            # dec.parametrize assumes that it is being run by nose. Because",
                "878": "            # we are running under pytest, we need to explicitly check the",
                "879": "            # results."
            },
            "comment_modified_diff": {}
        },
        {
            "commit": "768bbec24fff74cb59a173accf7fbeda6aca89a1",
            "timestamp": "2023-02-28T23:15:40+00:00",
            "author": "Ralf Gommers",
            "commit_message": "DEP: deprecate `np.round_`\n\nCloses gh-22617",
            "additions": 6,
            "deletions": 0,
            "change_type": "MODIFY",
            "diff": "@@ -901,3 +901,9 @@ class TestDeprecatedFinfo(_DeprecationTestCase):\n     # Deprecated in NumPy 1.25, 2023-01-16\n     def test_deprecated_none(self):\n         self.assert_deprecated(np.finfo, args=(None,))\n+\n+\n+class TestRound_(_DeprecationTestCase):\n+    # 2023-02-28, 1.25.0\n+    def test_round_(self):\n+        self.assert_deprecated(lambda: np.round_(np.array([1.5, 2.5, 3.5])))\n",
            "comment_added_diff": {
                "907": "    # 2023-02-28, 1.25.0"
            },
            "comment_deleted_diff": {},
            "comment_modified_diff": {}
        },
        {
            "commit": "9990f98f2e9e1a55f5d42206cbc6709a3efff418",
            "timestamp": "2023-03-02T15:10:41+00:00",
            "author": "Ralf Gommers",
            "commit_message": "DEP: deprecate `product`, `cumproduct`, `sometrue`, `alltrue`\n\n[skip cirrus]",
            "additions": 17,
            "deletions": 2,
            "change_type": "MODIFY",
            "diff": "@@ -902,8 +902,23 @@ class TestDeprecatedFinfo(_DeprecationTestCase):\n     def test_deprecated_none(self):\n         self.assert_deprecated(np.finfo, args=(None,))\n \n-\n-class TestRound_(_DeprecationTestCase):\n+class TestFromnumeric(_DeprecationTestCase):\n     # 2023-02-28, 1.25.0\n     def test_round_(self):\n         self.assert_deprecated(lambda: np.round_(np.array([1.5, 2.5, 3.5])))\n+\n+    # 2023-03-02, 1.25.0\n+    def test_cumproduct(self):\n+        self.assert_deprecated(lambda: np.cumproduct(np.array([1, 2, 3])))\n+\n+    # 2023-03-02, 1.25.0\n+    def test_product(self):\n+        self.assert_deprecated(lambda: np.product(np.array([1, 2, 3])))\n+\n+    # 2023-03-02, 1.25.0\n+    def test_sometrue(self):\n+        self.assert_deprecated(lambda: np.sometrue(np.array([True, False])))\n+\n+    # 2023-03-02, 1.25.0\n+    def test_alltrue(self):\n+        self.assert_deprecated(lambda: np.alltrue(np.array([True, False])))\n",
            "comment_added_diff": {
                "910": "    # 2023-03-02, 1.25.0",
                "914": "    # 2023-03-02, 1.25.0",
                "918": "    # 2023-03-02, 1.25.0",
                "922": "    # 2023-03-02, 1.25.0"
            },
            "comment_deleted_diff": {},
            "comment_modified_diff": {}
        },
        {
            "commit": "79a292bb9ba6cc0a5de77c84e961f87194f17fcb",
            "timestamp": "2023-04-07T13:29:45-06:00",
            "author": "Nathan Goldbaum",
            "commit_message": "DEP: deprecate np.math and np.lib.math",
            "additions": 9,
            "deletions": 0,
            "change_type": "MODIFY",
            "diff": "@@ -922,3 +922,12 @@ def test_sometrue(self):\n     # 2023-03-02, 1.25.0\n     def test_alltrue(self):\n         self.assert_deprecated(lambda: np.alltrue(np.array([True, False])))\n+\n+\n+class TestMathAlias(_DeprecationTestCase):\n+    # Deprecated in Numpy 1.25, 2023-04-06\n+    def test_deprecated_np_math(self):\n+        self.assert_deprecated(lambda: np.math)\n+\n+    def test_deprecated_np_lib_math(self):\n+        self.assert_deprecated(lambda: np.lib.math)\n",
            "comment_added_diff": {
                "928": "    # Deprecated in Numpy 1.25, 2023-04-06"
            },
            "comment_deleted_diff": {},
            "comment_modified_diff": {}
        },
        {
            "commit": "1acac891f99075128450aacf2a4538de3ff9d028",
            "timestamp": "2023-04-20T18:00:59+02:00",
            "author": "Nico Schl\u00f6mer",
            "commit_message": "DEP: deprecate scalar conversions for arrays with ndim > 0 (#10615)\n\nThis PR reflects some of the progress achieved in issue #10404 and is used to asses the impact of the changes. \r\n\r\nWith the changes in this PR, `float(numpy.array([1.0])` now gives a warning; likewise some other things:\r\n```python\r\nimport numpy\r\n\r\na = numpy.random.rand(10, 1)\r\na[0] = numpy.array([1.0])       # okay\r\na[0] = numpy.array(1.0)         # okay\r\na[0] = 1.0                      # okay\r\n\r\nb = numpy.random.rand(10)\r\nb[0] = numpy.array([1.0])       # ValueError: setting an array element with a sequence.\r\nb[0, ...] = numpy.array([1.0])  # okay\r\nb[0] = numpy.array(1.0)         # okay\r\nb[0] = 1.0                      # okay\r\n```\r\nThis aligns the behavior of numpy arrays with that of lists:\r\n```python\r\nfloat([3.14]) \r\n```\r\n```\r\nTypeError: float() argument must be a string or a number, not 'list'\r\n```\r\n```python\r\nimport numpy as np\r\n\r\na = np.random.rand(5)\r\na[0] = [3.14]\r\n```\r\n```\r\nValueError: setting an array element with a sequence.\r\n```\r\n\r\nFixes #10404.",
            "additions": 12,
            "deletions": 0,
            "change_type": "MODIFY",
            "diff": "@@ -822,6 +822,18 @@ def test_deprecated_raised(self, dtype):\n                 assert isinstance(e.__cause__, DeprecationWarning)\n \n \n+class TestScalarConversion(_DeprecationTestCase):\n+    # 2023-01-02, 1.25.0\n+    def test_float_conversion(self):\n+        self.assert_deprecated(float, args=(np.array([3.14]),))\n+\n+    def test_behaviour(self):\n+        b = np.array([[3.14]])\n+        c = np.zeros(5)\n+        with pytest.warns(DeprecationWarning):\n+            c[0] = b\n+\n+\n class TestPyIntConversion(_DeprecationTestCase):\n     message = r\".*stop allowing conversion of out-of-bound.*\"\n \n",
            "comment_added_diff": {
                "826": "    # 2023-01-02, 1.25.0"
            },
            "comment_deleted_diff": {},
            "comment_modified_diff": {}
        },
        {
            "commit": "a8face152ced3e951d067ee7276ff1fcb6be61b8",
            "timestamp": "2023-04-25T14:15:59+02:00",
            "author": "Sebastian Berg",
            "commit_message": "DEP: Finalize checking for sequence-like if something is array-like\n\nThis was always just a stop-gap for shapely basically, so there is\nno harm finalizing things.",
            "additions": 0,
            "deletions": 86,
            "change_type": "MODIFY",
            "diff": "@@ -580,92 +580,6 @@ def check():\n         self.assert_deprecated(check)\n \n \n-class TestFutureWarningArrayLikeNotIterable(_DeprecationTestCase):\n-    # Deprecated 2020-12-09, NumPy 1.20\n-    warning_cls = FutureWarning\n-    message = \"The input object of type.*but not a sequence\"\n-\n-    @pytest.mark.parametrize(\"protocol\",\n-            [\"__array__\", \"__array_interface__\", \"__array_struct__\"])\n-    def test_deprecated(self, protocol):\n-        \"\"\"Test that these objects give a warning since they are not 0-D,\n-        not coerced at the top level `np.array(obj)`, but nested, and do\n-        *not* define the sequence protocol.\n-\n-        NOTE: Tests for the versions including __len__ and __getitem__ exist\n-              in `test_array_coercion.py` and they can be modified or amended\n-              when this deprecation expired.\n-        \"\"\"\n-        blueprint = np.arange(10)\n-        MyArr = type(\"MyArr\", (), {protocol: getattr(blueprint, protocol)})\n-        self.assert_deprecated(lambda: np.array([MyArr()], dtype=object))\n-\n-    @pytest.mark.parametrize(\"protocol\",\n-             [\"__array__\", \"__array_interface__\", \"__array_struct__\"])\n-    def test_0d_not_deprecated(self, protocol):\n-        # 0-D always worked (albeit it would use __float__ or similar for the\n-        # conversion, which may not happen anymore)\n-        blueprint = np.array(1.)\n-        MyArr = type(\"MyArr\", (), {protocol: getattr(blueprint, protocol)})\n-        myarr = MyArr()\n-\n-        self.assert_not_deprecated(lambda: np.array([myarr], dtype=object))\n-        res = np.array([myarr], dtype=object)\n-        expected = np.empty(1, dtype=object)\n-        expected[0] = myarr\n-        assert_array_equal(res, expected)\n-\n-    @pytest.mark.parametrize(\"protocol\",\n-             [\"__array__\", \"__array_interface__\", \"__array_struct__\"])\n-    def test_unnested_not_deprecated(self, protocol):\n-        blueprint = np.arange(10)\n-        MyArr = type(\"MyArr\", (), {protocol: getattr(blueprint, protocol)})\n-        myarr = MyArr()\n-\n-        self.assert_not_deprecated(lambda: np.array(myarr))\n-        res = np.array(myarr)\n-        assert_array_equal(res, blueprint)\n-\n-    @pytest.mark.parametrize(\"protocol\",\n-             [\"__array__\", \"__array_interface__\", \"__array_struct__\"])\n-    def test_strange_dtype_handling(self, protocol):\n-        \"\"\"The old code would actually use the dtype from the array, but\n-        then end up not using the array (for dimension discovery)\n-        \"\"\"\n-        blueprint = np.arange(10).astype(\"f4\")\n-        MyArr = type(\"MyArr\", (), {protocol: getattr(blueprint, protocol),\n-                                   \"__float__\": lambda _: 0.5})\n-        myarr = MyArr()\n-\n-        # Make sure we warn (and capture the FutureWarning)\n-        with pytest.warns(FutureWarning, match=self.message):\n-            res = np.array([[myarr]])\n-\n-        assert res.shape == (1, 1)\n-        assert res.dtype == \"f4\"\n-        assert res[0, 0] == 0.5\n-\n-    @pytest.mark.parametrize(\"protocol\",\n-             [\"__array__\", \"__array_interface__\", \"__array_struct__\"])\n-    def test_assignment_not_deprecated(self, protocol):\n-        # If the result is dtype=object we do not unpack a nested array or\n-        # array-like, if it is nested at exactly the right depth.\n-        # NOTE: We actually do still call __array__, etc. but ignore the result\n-        #       in the end. For `dtype=object` we could optimize that away.\n-        blueprint = np.arange(10).astype(\"f4\")\n-        MyArr = type(\"MyArr\", (), {protocol: getattr(blueprint, protocol),\n-                                   \"__float__\": lambda _: 0.5})\n-        myarr = MyArr()\n-\n-        res = np.empty(3, dtype=object)\n-        def set():\n-            res[:] = [myarr, myarr, myarr]\n-        self.assert_not_deprecated(set)\n-        assert res[0] is myarr\n-        assert res[1] is myarr\n-        assert res[2] is myarr\n-\n-\n class TestDeprecatedUnpickleObjectScalar(_DeprecationTestCase):\n     # Deprecated 2020-11-24, NumPy 1.20\n     \"\"\"\n",
            "comment_added_diff": {},
            "comment_deleted_diff": {
                "584": "    # Deprecated 2020-12-09, NumPy 1.20",
                "606": "        # 0-D always worked (albeit it would use __float__ or similar for the",
                "607": "        # conversion, which may not happen anymore)",
                "640": "        # Make sure we warn (and capture the FutureWarning)",
                "651": "        # If the result is dtype=object we do not unpack a nested array or",
                "652": "        # array-like, if it is nested at exactly the right depth.",
                "653": "        # NOTE: We actually do still call __array__, etc. but ignore the result",
                "654": "        #       in the end. For `dtype=object` we could optimize that away."
            },
            "comment_modified_diff": {}
        },
        {
            "commit": "23a6d794c000eeb069e59ac164afed18f7ea37cb",
            "timestamp": "2023-04-26T14:58:41+02:00",
            "author": "Sebastian Berg",
            "commit_message": "DEP: Finalize subarray from non-subarray creation futurewarning\n\nThis unfortunately switches over the C-only path when FromArray\nis called with a subarray dtype.\n\nTogether with the previous commit (which is very simple but in a sense\ndoes the heavy lifting):\n\nCloses gh-23083",
            "additions": 0,
            "deletions": 42,
            "change_type": "MODIFY",
            "diff": "@@ -538,48 +538,6 @@ def test_not_deprecated(self):\n                            casting=\"same_kind\")\n \n \n-class TestDeprecateSubarrayDTypeDuringArrayCoercion(_DeprecationTestCase):\n-    warning_cls = FutureWarning\n-    message = \"(creating|casting) an array (with|to) a subarray dtype\"\n-\n-    def test_deprecated_array(self):\n-        # Arrays are more complex, since they \"broadcast\" on success:\n-        arr = np.array([1, 2])\n-\n-        self.assert_deprecated(lambda: arr.astype(\"(2)i,\"))\n-        with pytest.warns(FutureWarning):\n-            res = arr.astype(\"(2)i,\")\n-\n-        assert_array_equal(res, [[1, 2], [1, 2]])\n-\n-        self.assert_deprecated(lambda: np.array(arr, dtype=\"(2)i,\"))\n-        with pytest.warns(FutureWarning):\n-            res = np.array(arr, dtype=\"(2)i,\")\n-\n-        assert_array_equal(res, [[1, 2], [1, 2]])\n-\n-        with pytest.warns(FutureWarning):\n-            res = np.array([[(1,), (2,)], arr], dtype=\"(2)i,\")\n-\n-        assert_array_equal(res, [[[1, 1], [2, 2]], [[1, 2], [1, 2]]])\n-\n-    def test_deprecated_and_error(self):\n-        # These error paths do not give a warning, but will succeed in the\n-        # future.\n-        arr = np.arange(5 * 2).reshape(5, 2)\n-        def check():\n-            with pytest.raises(ValueError):\n-                arr.astype(\"(2,2)f\")\n-\n-        self.assert_deprecated(check)\n-\n-        def check():\n-            with pytest.raises(ValueError):\n-                np.array(arr, dtype=\"(2,2)f\")\n-\n-        self.assert_deprecated(check)\n-\n-\n class TestDeprecatedUnpickleObjectScalar(_DeprecationTestCase):\n     # Deprecated 2020-11-24, NumPy 1.20\n     \"\"\"\n",
            "comment_added_diff": {},
            "comment_deleted_diff": {
                "546": "        # Arrays are more complex, since they \"broadcast\" on success:",
                "567": "        # These error paths do not give a warning, but will succeed in the",
                "568": "        # future."
            },
            "comment_modified_diff": {}
        },
        {
            "commit": "87feaf72531979dd2eea965651b1c9d557ef37c2",
            "timestamp": "2023-06-12T17:57:36+02:00",
            "author": "Sebastian Berg",
            "commit_message": "DEP: Expire the `PyDataMem_SetEventHook` deprecation and remove it",
            "additions": 0,
            "deletions": 19,
            "change_type": "MODIFY",
            "diff": "@@ -619,25 +619,6 @@ def test_both_passed(self, func):\n                 func([0., 1.], 0., interpolation=\"nearest\", method=\"nearest\")\n \n \n-class TestMemEventHook(_DeprecationTestCase):\n-    # Deprecated 2021-11-18, NumPy 1.23\n-    def test_mem_seteventhook(self):\n-        # The actual tests are within the C code in\n-        # multiarray/_multiarray_tests.c.src\n-        import numpy.core._multiarray_tests as ma_tests\n-        with pytest.warns(DeprecationWarning,\n-                          match='PyDataMem_SetEventHook is deprecated'):\n-            ma_tests.test_pydatamem_seteventhook_start()\n-        # force an allocation and free of a numpy array\n-        # needs to be larger then limit of small memory cacher in ctors.c\n-        a = np.zeros(1000)\n-        del a\n-        break_cycles()\n-        with pytest.warns(DeprecationWarning,\n-                          match='PyDataMem_SetEventHook is deprecated'):\n-            ma_tests.test_pydatamem_seteventhook_end()\n-\n-\n class TestArrayFinalizeNone(_DeprecationTestCase):\n     message = \"Setting __array_finalize__ = None\"\n \n",
            "comment_added_diff": {},
            "comment_deleted_diff": {
                "623": "    # Deprecated 2021-11-18, NumPy 1.23",
                "625": "        # The actual tests are within the C code in",
                "626": "        # multiarray/_multiarray_tests.c.src",
                "631": "        # force an allocation and free of a numpy array",
                "632": "        # needs to be larger then limit of small memory cacher in ctors.c"
            },
            "comment_modified_diff": {}
        },
        {
            "commit": "12efa8eec31bd476e0110bb90de43d603d0e76d3",
            "timestamp": "2023-06-14T19:30:01+02:00",
            "author": "Mateusz Sok\u00f3\u0142",
            "commit_message": "DEP: deprecate compat and selected lib utils (#23830)\n\n[skip ci]",
            "additions": 9,
            "deletions": 0,
            "change_type": "MODIFY",
            "diff": "@@ -784,3 +784,12 @@ def test_deprecated_np_math(self):\n \n     def test_deprecated_np_lib_math(self):\n         self.assert_deprecated(lambda: np.lib.math)\n+\n+\n+class TestLibImports(_DeprecationTestCase):\n+    # Deprecated in Numpy 1.26.0, 2023-09\n+    def test_lib_functions_deprecation_call(self):\n+        from numpy.lib import byte_bounds, safe_eval, who\n+        self.assert_deprecated(lambda: byte_bounds(np.array([1])))\n+        self.assert_deprecated(lambda: safe_eval(\"None\"))\n+        self.assert_deprecated(lambda: who())\n",
            "comment_added_diff": {
                "790": "    # Deprecated in Numpy 1.26.0, 2023-09"
            },
            "comment_deleted_diff": {},
            "comment_modified_diff": {}
        },
        {
            "commit": "b6b5d977ea340f96ac5f746eff5907f1b4a66615",
            "timestamp": "2023-06-19T22:12:51+02:00",
            "author": "Sebastian Berg",
            "commit_message": "DEP: Expire `set_numeric_ops` and the corresponding C functions deprecation\n\nThis simply expires the deprecations because I saw they were one cause\nof doctest failures in the other PR, but its good anyway.",
            "additions": 0,
            "deletions": 12,
            "change_type": "MODIFY",
            "diff": "@@ -356,18 +356,6 @@ def test_deprecate_unparsable_string(self, invalid_str):\n             assert_array_equal(res, x)\n \n \n-class Test_GetSet_NumericOps(_DeprecationTestCase):\n-    # 2018-09-20, 1.16.0\n-    def test_get_numeric_ops(self):\n-        from numpy.core._multiarray_tests import getset_numericops\n-        self.assert_deprecated(getset_numericops, num=2)\n-\n-        # empty kwargs prevents any state actually changing which would break\n-        # other tests.\n-        self.assert_deprecated(np.set_numeric_ops, kwargs={})\n-        assert_raises(ValueError, np.set_numeric_ops, add='abc')\n-\n-\n class TestShape1Fields(_DeprecationTestCase):\n     warning_cls = FutureWarning\n \n",
            "comment_added_diff": {},
            "comment_deleted_diff": {
                "360": "    # 2018-09-20, 1.16.0",
                "365": "        # empty kwargs prevents any state actually changing which would break",
                "366": "        # other tests."
            },
            "comment_modified_diff": {}
        },
        {
            "commit": "c8e2343d22bc886dd08775589bc2405016349f37",
            "timestamp": "2023-08-07T22:02:46+02:00",
            "author": "Mateusz Sok\u00f3\u0142",
            "commit_message": "API: Cleaning `numpy/__init__.py` and main namespace - Part 1 [NEP 52] (#24316)",
            "additions": 1,
            "deletions": 17,
            "change_type": "MODIFY",
            "diff": "@@ -135,7 +135,7 @@ def assert_not_deprecated(self, function, args=(), kwargs={}):\n \n \n class _VisibleDeprecationTestCase(_DeprecationTestCase):\n-    warning_cls = np.VisibleDeprecationWarning\n+    warning_cls = np.exceptions.VisibleDeprecationWarning\n \n \n class TestDatetime64Timezone(_DeprecationTestCase):\n@@ -697,18 +697,6 @@ def create(value, dtype):\n                 pass  # OverflowErrors always happened also before and are OK.\n \n \n-class TestDeprecatedGlobals(_DeprecationTestCase):\n-    # Deprecated 2022-11-17, NumPy 1.24\n-    def test_type_aliases(self):\n-        # from builtins\n-        self.assert_deprecated(lambda: np.bool8)\n-        self.assert_deprecated(lambda: np.int0)\n-        self.assert_deprecated(lambda: np.uint0)\n-        self.assert_deprecated(lambda: np.bytes0)\n-        self.assert_deprecated(lambda: np.str0)\n-        self.assert_deprecated(lambda: np.object0)\n-\n-\n @pytest.mark.parametrize(\"name\",\n         [\"bool\", \"long\", \"ulong\", \"str\", \"bytes\", \"object\"])\n def test_future_scalar_attributes(name):\n@@ -766,10 +754,6 @@ def test_alltrue(self):\n \n \n class TestMathAlias(_DeprecationTestCase):\n-    # Deprecated in Numpy 1.25, 2023-04-06\n-    def test_deprecated_np_math(self):\n-        self.assert_deprecated(lambda: np.math)\n-\n     def test_deprecated_np_lib_math(self):\n         self.assert_deprecated(lambda: np.lib.math)\n \n",
            "comment_added_diff": {},
            "comment_deleted_diff": {
                "701": "    # Deprecated 2022-11-17, NumPy 1.24",
                "703": "        # from builtins",
                "769": "    # Deprecated in Numpy 1.25, 2023-04-06"
            },
            "comment_modified_diff": {}
        },
        {
            "commit": "250e1479ce342d9d7ab8a592508f6ce892d4c98b",
            "timestamp": "2023-08-12T22:01:16+02:00",
            "author": "Mateusz Sok\u00f3\u0142",
            "commit_message": "API: Cleaning `numpy/__init__.py` and main namespace - Part 2 [NEP 52] (#24357)\n\n[skip ci]",
            "additions": 0,
            "deletions": 8,
            "change_type": "MODIFY",
            "diff": "@@ -617,14 +617,6 @@ class NoFinalize(np.ndarray):\n \n         self.assert_deprecated(lambda: np.array(1).view(NoFinalize))\n \n-class TestAxisNotMAXDIMS(_DeprecationTestCase):\n-    # Deprecated 2022-01-08, NumPy 1.23\n-    message = r\"Using `axis=32` \\(MAXDIMS\\) is deprecated\"\n-\n-    def test_deprecated(self):\n-        a = np.zeros((1,)*32)\n-        self.assert_deprecated(lambda: np.repeat(a, 1, axis=np.MAXDIMS))\n-\n \n class TestLoadtxtParseIntsViaFloat(_DeprecationTestCase):\n     # Deprecated 2022-07-03, NumPy 1.23\n",
            "comment_added_diff": {},
            "comment_deleted_diff": {
                "621": "    # Deprecated 2022-01-08, NumPy 1.23"
            },
            "comment_modified_diff": {}
        },
        {
            "commit": "b43384e8f9f7242c59985c4a3d687c95a2a9dbf4",
            "timestamp": "2023-08-30T09:34:12+02:00",
            "author": "Mateusz Sok\u00f3\u0142",
            "commit_message": "MAINT: Remove deprecated functions [NEP 52] (#24477)",
            "additions": 0,
            "deletions": 4,
            "change_type": "MODIFY",
            "diff": "@@ -724,10 +724,6 @@ def test_deprecated_none(self):\n         self.assert_deprecated(np.finfo, args=(None,))\n \n class TestFromnumeric(_DeprecationTestCase):\n-    # 2023-02-28, 1.25.0\n-    def test_round_(self):\n-        self.assert_deprecated(lambda: np.round_(np.array([1.5, 2.5, 3.5])))\n-\n     # 2023-03-02, 1.25.0\n     def test_cumproduct(self):\n         self.assert_deprecated(lambda: np.cumproduct(np.array([1, 2, 3])))\n",
            "comment_added_diff": {},
            "comment_deleted_diff": {
                "727": "    # 2023-02-28, 1.25.0"
            },
            "comment_modified_diff": {}
        }
    ],
    "test_half.py": [],
    "test_indexing.py": [],
    "test_memmap.py": [],
    "test_numeric.py": [
        {
            "commit": "92e489eb3fea7ea4054e319b2d698e435e919704",
            "timestamp": "2023-03-16T22:15:10+11:00",
            "author": "Matti Picus",
            "commit_message": "DEP: remove deprecated casting in np.clip\n\nSigned-off-by: Matti Picus <matti.picus@gmail.com>",
            "additions": 16,
            "deletions": 45,
            "change_type": "MODIFY",
            "diff": "@@ -1824,20 +1824,11 @@ def setup_method(self):\n         self.nr = 5\n         self.nc = 3\n \n-    def fastclip(self, a, m, M, out=None, casting=None):\n-        if out is None:\n-            if casting is None:\n-                return a.clip(m, M)\n-            else:\n-                return a.clip(m, M, casting=casting)\n-        else:\n-            if casting is None:\n-                return a.clip(m, M, out)\n-            else:\n-                return a.clip(m, M, out, casting=casting)\n+    def fastclip(self, a, m, M, out=None, **kwargs):\n+        return a.clip(m, M, out=out, **kwargs)\n \n     def clip(self, a, m, M, out=None):\n-        # use slow-clip\n+        # use a.choose to verify fastclip result\n         selector = np.less(a, m) + 2*np.greater(a, M)\n         return selector.choose((a, m, M), out=out)\n \n@@ -1993,14 +1984,13 @@ def test_simple_int32_inout(self, casting):\n         ac = np.zeros(a.shape, dtype=np.int32)\n         act = ac.copy()\n         if casting is None:\n-            with assert_warns(DeprecationWarning):\n-                # NumPy 1.17.0, 2018-02-24 - casting is unsafe\n+            with pytest.raises(TypeError):\n                 self.fastclip(a, m, M, ac, casting=casting)\n         else:\n             # explicitly passing \"unsafe\" will silence warning\n             self.fastclip(a, m, M, ac, casting=casting)\n-        self.clip(a, m, M, act)\n-        assert_array_strict_equal(ac, act)\n+            self.clip(a, m, M, act)\n+            assert_array_strict_equal(ac, act)\n \n     def test_simple_int64_out(self):\n         # Test native int32 input with int32 scalar min/max and int64 out.\n@@ -2020,9 +2010,7 @@ def test_simple_int64_inout(self):\n         M = np.float64(1)\n         ac = np.zeros(a.shape, dtype=np.int32)\n         act = ac.copy()\n-        with assert_warns(DeprecationWarning):\n-            # NumPy 1.17.0, 2018-02-24 - casting is unsafe\n-            self.fastclip(a, m, M, ac)\n+        self.fastclip(a, m, M, out=ac, casting=\"unsafe\")\n         self.clip(a, m, M, act)\n         assert_array_strict_equal(ac, act)\n \n@@ -2033,9 +2021,7 @@ def test_simple_int32_out(self):\n         M = 2.0\n         ac = np.zeros(a.shape, dtype=np.int32)\n         act = ac.copy()\n-        with assert_warns(DeprecationWarning):\n-            # NumPy 1.17.0, 2018-02-24 - casting is unsafe\n-            self.fastclip(a, m, M, ac)\n+        self.fastclip(a, m, M, out=ac, casting=\"unsafe\")\n         self.clip(a, m, M, act)\n         assert_array_strict_equal(ac, act)\n \n@@ -2211,9 +2197,7 @@ def test_clip_with_out_simple2(self):\n         M = np.float64(2)\n         ac = np.zeros(a.shape, dtype=np.int32)\n         act = ac.copy()\n-        with assert_warns(DeprecationWarning):\n-            # NumPy 1.17.0, 2018-02-24 - casting is unsafe\n-            self.fastclip(a, m, M, ac)\n+        self.fastclip(a, m, M, out=ac, casting=\"unsafe\")\n         self.clip(a, m, M, act)\n         assert_array_strict_equal(ac, act)\n \n@@ -2235,9 +2219,7 @@ def test_clip_with_out_array_int32(self):\n         M = np.float64(1)\n         ac = np.zeros(a.shape, dtype=np.int32)\n         act = ac.copy()\n-        with assert_warns(DeprecationWarning):\n-            # NumPy 1.17.0, 2018-02-24 - casting is unsafe\n-            self.fastclip(a, m, M, ac)\n+        self.fastclip(a, m, M, out=ac, casting=\"unsafe\")\n         self.clip(a, m, M, act)\n         assert_array_strict_equal(ac, act)\n \n@@ -2248,9 +2230,7 @@ def test_clip_with_out_array_outint32(self):\n         M = 2.0\n         ac = np.zeros(a.shape, dtype=np.int32)\n         act = ac.copy()\n-        with assert_warns(DeprecationWarning):\n-            # NumPy 1.17.0, 2018-02-24 - casting is unsafe\n-            self.fastclip(a, m, M, ac)\n+        self.fastclip(a, m, M, out=ac, casting=\"unsafe\")\n         self.clip(a, m, M, act)\n         assert_array_strict_equal(ac, act)\n \n@@ -2303,16 +2283,11 @@ def test_clip_func_takes_out(self):\n \n     def test_clip_nan(self):\n         d = np.arange(7.)\n-        with assert_warns(DeprecationWarning):\n-            assert_equal(d.clip(min=np.nan), d)\n-        with assert_warns(DeprecationWarning):\n-            assert_equal(d.clip(max=np.nan), d)\n-        with assert_warns(DeprecationWarning):\n-            assert_equal(d.clip(min=np.nan, max=np.nan), d)\n-        with assert_warns(DeprecationWarning):\n-            assert_equal(d.clip(min=-2, max=np.nan), d)\n-        with assert_warns(DeprecationWarning):\n-            assert_equal(d.clip(min=np.nan, max=10), d)\n+        assert_equal(d.clip(min=np.nan), np.nan)\n+        assert_equal(d.clip(max=np.nan), np.nan)\n+        assert_equal(d.clip(min=np.nan, max=np.nan), np.nan)\n+        assert_equal(d.clip(min=-2, max=np.nan), np.nan)\n+        assert_equal(d.clip(min=np.nan, max=10), np.nan)\n \n     def test_object_clip(self):\n         a = np.arange(10, dtype=object)\n@@ -2364,16 +2339,12 @@ def test_clip_problem_cases(self, arr, amin, amax, exp):\n         actual = np.clip(arr, amin, amax)\n         assert_equal(actual, exp)\n \n-    @pytest.mark.xfail(reason=\"no scalar nan propagation yet\",\n-                       raises=AssertionError,\n-                       strict=True)\n     @pytest.mark.parametrize(\"arr, amin, amax\", [\n         # problematic scalar nan case from hypothesis\n         (np.zeros(10, dtype=np.int64),\n          np.array(np.nan),\n          np.zeros(10, dtype=np.int32)),\n     ])\n-    @pytest.mark.filterwarnings(\"ignore::DeprecationWarning\")\n     def test_clip_scalar_nan_propagation(self, arr, amin, amax):\n         # enforcement of scalar nan propagation for comparisons\n         # called through clip()\n",
            "comment_added_diff": {
                "1831": "        # use a.choose to verify fastclip result"
            },
            "comment_deleted_diff": {
                "1840": "        # use slow-clip",
                "1997": "                # NumPy 1.17.0, 2018-02-24 - casting is unsafe",
                "2024": "            # NumPy 1.17.0, 2018-02-24 - casting is unsafe",
                "2037": "            # NumPy 1.17.0, 2018-02-24 - casting is unsafe",
                "2215": "            # NumPy 1.17.0, 2018-02-24 - casting is unsafe",
                "2239": "            # NumPy 1.17.0, 2018-02-24 - casting is unsafe",
                "2252": "            # NumPy 1.17.0, 2018-02-24 - casting is unsafe"
            },
            "comment_modified_diff": {
                "1831": "            else:"
            }
        },
        {
            "commit": "95b8c249551eb54d07f05794e0ef21dc6c16d73d",
            "timestamp": "2023-06-13T09:26:30+02:00",
            "author": "Sebastian Berg",
            "commit_message": "API: Remove `np.geterrobj` and `np.seterrobj`\n\nThese are left for now as a private implementation detail.",
            "additions": 0,
            "deletions": 49,
            "change_type": "MODIFY",
            "diff": "@@ -568,55 +568,6 @@ def test_divide_err(self):\n             np.seterr(divide='ignore')\n             np.array([1.]) / np.array([0.])\n \n-    @pytest.mark.skipif(IS_WASM, reason=\"no wasm fp exception support\")\n-    def test_errobj(self):\n-        olderrobj = np.geterrobj()\n-        self.called = 0\n-        try:\n-            with warnings.catch_warnings(record=True) as w:\n-                warnings.simplefilter(\"always\")\n-                with np.errstate(divide='warn'):\n-                    np.seterrobj([20000, 1, None])\n-                    np.array([1.]) / np.array([0.])\n-                    assert_equal(len(w), 1)\n-\n-            def log_err(*args):\n-                self.called += 1\n-                extobj_err = args\n-                assert_(len(extobj_err) == 2)\n-                assert_(\"divide\" in extobj_err[0])\n-\n-            with np.errstate(divide='ignore'):\n-                np.seterrobj([20000, 3, log_err])\n-                np.array([1.]) / np.array([0.])\n-            assert_equal(self.called, 1)\n-\n-            np.seterrobj(olderrobj)\n-            with np.errstate(divide='ignore'):\n-                np.divide(1., 0., extobj=[20000, 3, log_err])\n-            assert_equal(self.called, 2)\n-        finally:\n-            np.seterrobj(olderrobj)\n-            del self.called\n-\n-    def test_errobj_noerrmask(self):\n-        # errmask = 0 has a special code path for the default\n-        olderrobj = np.geterrobj()\n-        try:\n-            # set errobj to something non default\n-            np.seterrobj([umath.UFUNC_BUFSIZE_DEFAULT,\n-                         umath.ERR_DEFAULT + 1, None])\n-            # call a ufunc\n-            np.isnan(np.array([6]))\n-            # same with the default, lots of times to get rid of possible\n-            # pre-existing stack in the code\n-            for i in range(10000):\n-                np.seterrobj([umath.UFUNC_BUFSIZE_DEFAULT, umath.ERR_DEFAULT,\n-                             None])\n-            np.isnan(np.array([6]))\n-        finally:\n-            np.seterrobj(olderrobj)\n-\n \n class TestFloatExceptions:\n     def assert_raises_fpe(self, fpeerr, flop, x, y):\n",
            "comment_added_diff": {},
            "comment_deleted_diff": {
                "603": "        # errmask = 0 has a special code path for the default",
                "606": "            # set errobj to something non default",
                "609": "            # call a ufunc",
                "611": "            # same with the default, lots of times to get rid of possible",
                "612": "            # pre-existing stack in the code"
            },
            "comment_modified_diff": {}
        },
        {
            "commit": "ab2178b47c0ee834180c318db196976623710691",
            "timestamp": "2023-07-07T08:57:37+03:00",
            "author": "Ronald van Elburg",
            "commit_message": "ENH: add mean keyword to std and var (#24126)\n\n* Add mean keyword to std and var functions.\r\n\r\n* Add releae note for mean keyword to std and var functions.\r\n\r\n* Update release note with PR number\r\n\r\n* Address lint issue.\r\n\r\n* Align nan signatures with new signatures.\r\n\r\n* Address lint issue.\r\n\r\n* Correct version numbers on keywords.\r\n\r\n* Put backticks on keyword argument in documentation string.\r\n\r\n* Cleanuup assert statements in tests\r\n\r\n* Move comparison of in and out arrays closer to the function call.\r\n\r\n* Remove clutter from example code in release note.\r\n\r\n* Add test for nanstd and fix error in nanvar\r\n\r\n* haqndle \"mean\" keyword for var and std on MaskedArrays.\r\n\r\n* Address lint issues.\r\n\r\n* update the dispatchers according to suggestions by Marten van Kerkwijk:\r\n\r\n(https://github.com/numpy/numpy/pull/24126#discussion_r1254314302) The dispatcher returns all arguments that can, in principle, contain something that is an array and hence that, if not a regular ndarray, can allow the function to be dealt with another package (say, dask). Since mean can be an array, it should be added (most similar to where).\r\n\r\n* Move and adjust example from release note to doc-strings. Reflow doc-string.\r\n\r\n* Improve doc-string. Shorter sentences and add type and label mean argument\r\n\r\n* Remove some of these pesky trailing white spaces\r\n\r\n* Make extra white lines more consistent.\r\n\r\n* Make sure code examples execute without Jupyter magic.\r\n\r\n* Fold lines to pass linter.\r\n\r\n* Update doc-string nanstd and nanvar.\r\n\r\n* Try to satisfy linter and apple requirements at the same time. Making the example code ugly, alas!\r\n\r\n* Make doctest skip resource dependent output",
            "additions": 317,
            "deletions": 0,
            "change_type": "MODIFY",
            "diff": "@@ -15,6 +15,7 @@\n     assert_warns, assert_array_max_ulp, HAS_REFCOUNT, IS_WASM\n     )\n from numpy.core._rational_tests import rational\n+from numpy import ma\n \n from hypothesis import given, strategies as st\n from hypothesis.extra import numpy as hynp\n@@ -298,6 +299,322 @@ def test_var(self):\n         B[0] = 1j\n         assert_almost_equal(np.var(B), 0.25)\n \n+    def test_std_with_mean_keyword(self):\n+        # Setting the seed to make the test reproducable\n+        rng = np.random.RandomState(1234)\n+        A = rng.randn(10, 20, 5) + 0.5\n+\n+        mean_out = np.zeros((10, 1, 5))\n+        std_out = np.zeros((10, 1, 5))\n+\n+        mean = np.mean(A,\n+                       out=mean_out,\n+                       axis=1,\n+                       keepdims=True)\n+\n+        # The returned  object should be the object specified during calling\n+        assert mean_out is mean\n+\n+        std = np.std(A,\n+                     out=std_out,\n+                     axis=1,\n+                     keepdims=True,\n+                     mean=mean)\n+\n+        # The returned  object should be the object specified during calling\n+        assert std_out is std\n+\n+        # Shape of returned mean and std should be same\n+        assert std.shape == mean.shape\n+        assert std.shape == (10, 1, 5)\n+\n+        # Output should be the same as from the individual algorithms\n+        std_old = np.std(A, axis=1, keepdims=True)\n+\n+        assert std_old.shape == mean.shape\n+        assert_almost_equal(std, std_old)\n+\n+    def test_var_with_mean_keyword(self):\n+        # Setting the seed to make the test reproducable\n+        rng = np.random.RandomState(1234)\n+        A = rng.randn(10, 20, 5) + 0.5\n+\n+        mean_out = np.zeros((10, 1, 5))\n+        var_out = np.zeros((10, 1, 5))\n+\n+        mean = np.mean(A,\n+                       out=mean_out,\n+                       axis=1,\n+                       keepdims=True)\n+\n+        # The returned  object should be the object specified during calling\n+        assert mean_out is mean\n+\n+        var = np.var(A,\n+                     out=var_out,\n+                     axis=1,\n+                     keepdims=True,\n+                     mean=mean)\n+\n+        # The returned  object should be the object specified during calling\n+        assert var_out is var\n+\n+        # Shape of returned mean and var should be same\n+        assert var.shape == mean.shape\n+        assert var.shape == (10, 1, 5)\n+\n+        # Output should be the same as from the individual algorithms\n+        var_old = np.var(A, axis=1, keepdims=True)\n+\n+        assert var_old.shape == mean.shape\n+        assert_almost_equal(var, var_old)\n+\n+    def test_std_with_mean_keyword_keepdims_false(self):\n+        rng = np.random.RandomState(1234)\n+        A = rng.randn(10, 20, 5) + 0.5\n+\n+        mean = np.mean(A,\n+                       axis=1,\n+                       keepdims=True)\n+\n+        std = np.std(A,\n+                     axis=1,\n+                     keepdims=False,\n+                     mean=mean)\n+\n+        # Shape of returned mean and std should be same\n+        assert std.shape == (10, 5)\n+\n+        # Output should be the same as from the individual algorithms\n+        std_old = np.std(A, axis=1, keepdims=False)\n+        mean_old = np.mean(A, axis=1, keepdims=False)\n+\n+        assert std_old.shape == mean_old.shape\n+        assert_equal(std, std_old)\n+\n+    def test_var_with_mean_keyword_keepdims_false(self):\n+        rng = np.random.RandomState(1234)\n+        A = rng.randn(10, 20, 5) + 0.5\n+\n+        mean = np.mean(A,\n+                       axis=1,\n+                       keepdims=True)\n+\n+        var = np.var(A,\n+                     axis=1,\n+                     keepdims=False,\n+                     mean=mean)\n+\n+        # Shape of returned mean and var should be same\n+        assert var.shape == (10, 5)\n+\n+        # Output should be the same as from the individual algorithms\n+        var_old = np.var(A, axis=1, keepdims=False)\n+        mean_old = np.mean(A, axis=1, keepdims=False)\n+\n+        assert var_old.shape == mean_old.shape\n+        assert_equal(var, var_old)\n+\n+    def test_std_with_mean_keyword_where_nontrivial(self):\n+        rng = np.random.RandomState(1234)\n+        A = rng.randn(10, 20, 5) + 0.5\n+\n+        where = A > 0.5\n+\n+        mean = np.mean(A,\n+                       axis=1,\n+                       keepdims=True,\n+                       where=where)\n+\n+        std = np.std(A,\n+                     axis=1,\n+                     keepdims=False,\n+                     mean=mean,\n+                     where=where)\n+\n+        # Shape of returned mean and std should be same\n+        assert std.shape == (10, 5)\n+\n+        # Output should be the same as from the individual algorithms\n+        std_old = np.std(A, axis=1, where=where)\n+        mean_old = np.mean(A, axis=1, where=where)\n+\n+        assert std_old.shape == mean_old.shape\n+        assert_equal(std, std_old)\n+\n+    def test_var_with_mean_keyword_where_nontrivial(self):\n+        rng = np.random.RandomState(1234)\n+        A = rng.randn(10, 20, 5) + 0.5\n+\n+        where = A > 0.5\n+\n+        mean = np.mean(A,\n+                       axis=1,\n+                       keepdims=True,\n+                       where=where)\n+\n+        var = np.var(A,\n+                     axis=1,\n+                     keepdims=False,\n+                     mean=mean,\n+                     where=where)\n+\n+        # Shape of returned mean and var should be same\n+        assert var.shape == (10, 5)\n+\n+        # Output should be the same as from the individual algorithms\n+        var_old = np.var(A, axis=1, where=where)\n+        mean_old = np.mean(A, axis=1, where=where)\n+\n+        assert var_old.shape == mean_old.shape\n+        assert_equal(var, var_old)\n+\n+    def test_std_with_mean_keyword_multiple_axis(self):\n+        # Setting the seed to make the test reproducable\n+        rng = np.random.RandomState(1234)\n+        A = rng.randn(10, 20, 5) + 0.5\n+\n+        axis = (0, 2)\n+\n+        mean = np.mean(A,\n+                       out=None,\n+                       axis=axis,\n+                       keepdims=True)\n+\n+        std = np.std(A,\n+                     out=None,\n+                     axis=axis,\n+                     keepdims=False,\n+                     mean=mean)\n+\n+        # Shape of returned mean and std should be same\n+        assert std.shape == (20,)\n+\n+        # Output should be the same as from the individual algorithms\n+        std_old = np.std(A, axis=axis, keepdims=False)\n+\n+        assert_almost_equal(std, std_old)\n+\n+    def test_std_with_mean_keyword_axis_None(self):\n+        # Setting the seed to make the test reproducable\n+        rng = np.random.RandomState(1234)\n+        A = rng.randn(10, 20, 5) + 0.5\n+\n+        axis = None\n+\n+        mean = np.mean(A,\n+                       out=None,\n+                       axis=axis,\n+                       keepdims=True)\n+\n+        std = np.std(A,\n+                     out=None,\n+                     axis=axis,\n+                     keepdims=False,\n+                     mean=mean)\n+\n+        # Shape of returned mean and std should be same\n+        assert std.shape == ()\n+\n+        # Output should be the same as from the individual algorithms\n+        std_old = np.std(A, axis=axis, keepdims=False)\n+\n+        assert_almost_equal(std, std_old)\n+\n+    def test_std_with_mean_keyword_keepdims_true_masked(self):\n+\n+        A = ma.array([[2., 3., 4., 5.],\n+                      [1., 2., 3., 4.]],\n+                     mask=[[True, False, True, False],\n+                           [True, False, True, False]])\n+\n+        B = ma.array([[100., 3., 104., 5.],\n+                      [101., 2., 103., 4.]],\n+                      mask=[[True, False, True, False],\n+                            [True, False, True, False]])\n+\n+        mean_out = ma.array([[0., 0., 0., 0.]],\n+                            mask=[[False, False, False, False]])\n+        std_out = ma.array([[0., 0., 0., 0.]],\n+                           mask=[[False, False, False, False]])\n+\n+        axis = 0\n+\n+        mean = np.mean(A, out=mean_out,\n+                       axis=axis, keepdims=True)\n+\n+        std = np.std(A, out=std_out,\n+                     axis=axis, keepdims=True,\n+                     mean=mean)\n+\n+        # Shape of returned mean and std should be same\n+        assert std.shape == mean.shape\n+        assert std.shape == (1, 4)\n+\n+        # Output should be the same as from the individual algorithms\n+        std_old = np.std(A, axis=axis, keepdims=True)\n+        mean_old = np.mean(A, axis=axis, keepdims=True)\n+\n+        assert std_old.shape == mean_old.shape\n+        assert_almost_equal(std, std_old)\n+        assert_almost_equal(mean, mean_old)\n+\n+        assert mean_out is mean\n+        assert std_out is std\n+\n+        # masked elements should be ignored\n+        mean_b = np.mean(B, axis=axis, keepdims=True)\n+        std_b = np.std(B, axis=axis, keepdims=True, mean=mean_b)\n+        assert_almost_equal(std, std_b)\n+        assert_almost_equal(mean, mean_b)\n+\n+    def test_var_with_mean_keyword_keepdims_true_masked(self):\n+\n+        A = ma.array([[2., 3., 4., 5.],\n+                      [1., 2., 3., 4.]],\n+                     mask=[[True, False, True, False],\n+                           [True, False, True, False]])\n+\n+        B = ma.array([[100., 3., 104., 5.],\n+                      [101., 2., 103., 4.]],\n+                      mask=[[True, False, True, False],\n+                            [True, False, True, False]])\n+\n+        mean_out = ma.array([[0., 0., 0., 0.]],\n+                            mask=[[False, False, False, False]])\n+        var_out = ma.array([[0., 0., 0., 0.]],\n+                           mask=[[False, False, False, False]])\n+\n+        axis = 0\n+\n+        mean = np.mean(A, out=mean_out,\n+                       axis=axis, keepdims=True)\n+\n+        var = np.var(A, out=var_out,\n+                     axis=axis, keepdims=True,\n+                     mean=mean)\n+\n+        # Shape of returned mean and var should be same\n+        assert var.shape == mean.shape\n+        assert var.shape == (1, 4)\n+\n+        # Output should be the same as from the individual algorithms\n+        var_old = np.var(A, axis=axis, keepdims=True)\n+        mean_old = np.mean(A, axis=axis, keepdims=True)\n+\n+        assert var_old.shape == mean_old.shape\n+        assert_almost_equal(var, var_old)\n+        assert_almost_equal(mean, mean_old)\n+\n+        assert mean_out is mean\n+        assert var_out is var\n+\n+        # masked elements should be ignored\n+        mean_b = np.mean(B, axis=axis, keepdims=True)\n+        var_b = np.var(B, axis=axis, keepdims=True, mean=mean_b)\n+        assert_almost_equal(var, var_b)\n+        assert_almost_equal(mean, mean_b)\n+\n \n class TestIsscalar:\n     def test_isscalar(self):\n",
            "comment_added_diff": {
                "303": "        # Setting the seed to make the test reproducable",
                "315": "        # The returned  object should be the object specified during calling",
                "324": "        # The returned  object should be the object specified during calling",
                "327": "        # Shape of returned mean and std should be same",
                "331": "        # Output should be the same as from the individual algorithms",
                "338": "        # Setting the seed to make the test reproducable",
                "350": "        # The returned  object should be the object specified during calling",
                "359": "        # The returned  object should be the object specified during calling",
                "362": "        # Shape of returned mean and var should be same",
                "366": "        # Output should be the same as from the individual algorithms",
                "385": "        # Shape of returned mean and std should be same",
                "388": "        # Output should be the same as from the individual algorithms",
                "408": "        # Shape of returned mean and var should be same",
                "411": "        # Output should be the same as from the individual algorithms",
                "435": "        # Shape of returned mean and std should be same",
                "438": "        # Output should be the same as from the individual algorithms",
                "462": "        # Shape of returned mean and var should be same",
                "465": "        # Output should be the same as from the individual algorithms",
                "473": "        # Setting the seed to make the test reproducable",
                "490": "        # Shape of returned mean and std should be same",
                "493": "        # Output should be the same as from the individual algorithms",
                "499": "        # Setting the seed to make the test reproducable",
                "516": "        # Shape of returned mean and std should be same",
                "519": "        # Output should be the same as from the individual algorithms",
                "550": "        # Shape of returned mean and std should be same",
                "554": "        # Output should be the same as from the individual algorithms",
                "565": "        # masked elements should be ignored",
                "597": "        # Shape of returned mean and var should be same",
                "601": "        # Output should be the same as from the individual algorithms",
                "612": "        # masked elements should be ignored"
            },
            "comment_deleted_diff": {},
            "comment_modified_diff": {}
        },
        {
            "commit": "a4c582d4b3c530176be393ac596dd00130367ddb",
            "timestamp": "2023-07-18T08:53:25+02:00",
            "author": "Dimitri Papadopoulos",
            "commit_message": "MAINT: Fix new or residual typos found by codespell",
            "additions": 4,
            "deletions": 4,
            "change_type": "MODIFY",
            "diff": "@@ -300,7 +300,7 @@ def test_var(self):\n         assert_almost_equal(np.var(B), 0.25)\n \n     def test_std_with_mean_keyword(self):\n-        # Setting the seed to make the test reproducable\n+        # Setting the seed to make the test reproducible\n         rng = np.random.RandomState(1234)\n         A = rng.randn(10, 20, 5) + 0.5\n \n@@ -335,7 +335,7 @@ def test_std_with_mean_keyword(self):\n         assert_almost_equal(std, std_old)\n \n     def test_var_with_mean_keyword(self):\n-        # Setting the seed to make the test reproducable\n+        # Setting the seed to make the test reproducible\n         rng = np.random.RandomState(1234)\n         A = rng.randn(10, 20, 5) + 0.5\n \n@@ -470,7 +470,7 @@ def test_var_with_mean_keyword_where_nontrivial(self):\n         assert_equal(var, var_old)\n \n     def test_std_with_mean_keyword_multiple_axis(self):\n-        # Setting the seed to make the test reproducable\n+        # Setting the seed to make the test reproducible\n         rng = np.random.RandomState(1234)\n         A = rng.randn(10, 20, 5) + 0.5\n \n@@ -496,7 +496,7 @@ def test_std_with_mean_keyword_multiple_axis(self):\n         assert_almost_equal(std, std_old)\n \n     def test_std_with_mean_keyword_axis_None(self):\n-        # Setting the seed to make the test reproducable\n+        # Setting the seed to make the test reproducible\n         rng = np.random.RandomState(1234)\n         A = rng.randn(10, 20, 5) + 0.5\n \n",
            "comment_added_diff": {
                "303": "        # Setting the seed to make the test reproducible",
                "338": "        # Setting the seed to make the test reproducible",
                "473": "        # Setting the seed to make the test reproducible",
                "499": "        # Setting the seed to make the test reproducible"
            },
            "comment_deleted_diff": {
                "303": "        # Setting the seed to make the test reproducable",
                "338": "        # Setting the seed to make the test reproducable",
                "473": "        # Setting the seed to make the test reproducable",
                "499": "        # Setting the seed to make the test reproducable"
            },
            "comment_modified_diff": {
                "303": "        # Setting the seed to make the test reproducable",
                "338": "        # Setting the seed to make the test reproducable",
                "473": "        # Setting the seed to make the test reproducable",
                "499": "        # Setting the seed to make the test reproducable"
            }
        }
    ],
    "test_records.py": [
        {
            "commit": "eef1cbd192bf3030e11e8d793ecc518ee9fd428d",
            "timestamp": "2023-07-18T11:50:24+02:00",
            "author": "Sebastian Berg",
            "commit_message": "TST: Some more test fixups for repr changes\n\nstill quite a few to do, and there are some things that we need to\nfigure out...",
            "additions": 2,
            "deletions": 1,
            "change_type": "MODIFY",
            "diff": "@@ -149,7 +149,8 @@ def test_recarray_from_repr(self):\n \n         recordarr_r = eval(\"np.\" + repr(recordarr), {'np': np})\n         recarr_r = eval(\"np.\" + repr(recarr), {'np': np})\n-        recordview_r = eval(\"np.\" + repr(recordview), {'np': np})\n+        # Prints the type `numpy.record` as part of the dtype:\n+        recordview_r = eval(\"np.\" + repr(recordview), {'np': np, 'numpy': np})\n \n         assert_equal(type(recordarr_r), np.recarray)\n         assert_equal(recordarr_r.dtype.type, np.record)\n",
            "comment_added_diff": {
                "152": "        # Prints the type `numpy.record` as part of the dtype:"
            },
            "comment_deleted_diff": {},
            "comment_modified_diff": {
                "152": "        recordview_r = eval(\"np.\" + repr(recordview), {'np': np})"
            }
        },
        {
            "commit": "2391866e421a97b1f28d0221100aa5f037344857",
            "timestamp": "2023-09-17T19:35:07+02:00",
            "author": "Mateusz Sok\u00f3\u0142",
            "commit_message": "API: Remove chararray, compare_chararrays, recarray and format_parser from main namespace",
            "additions": 28,
            "deletions": 26,
            "change_type": "MODIFY",
            "diff": "@@ -146,7 +146,7 @@ def test_recarray_from_repr(self):\n         a = np.array([(1,'ABC'), (2, \"DEF\")],\n                      dtype=[('foo', int), ('bar', 'S4')])\n         recordarr = np.rec.array(a)\n-        recarr = a.view(np.recarray)\n+        recarr = a.view(np.rec.recarray)\n         recordview = a.view(np.dtype((np.record, a.dtype)))\n \n         recordarr_r = eval(\"np.\" + repr(recordarr), {'np': np})\n@@ -154,11 +154,11 @@ def test_recarray_from_repr(self):\n         # Prints the type `numpy.record` as part of the dtype:\n         recordview_r = eval(\"np.\" + repr(recordview), {'np': np, 'numpy': np})\n \n-        assert_equal(type(recordarr_r), np.recarray)\n+        assert_equal(type(recordarr_r), np.rec.recarray)\n         assert_equal(recordarr_r.dtype.type, np.record)\n         assert_equal(recordarr, recordarr_r)\n \n-        assert_equal(type(recarr_r), np.recarray)\n+        assert_equal(type(recarr_r), np.rec.recarray)\n         assert_equal(recarr_r.dtype.type, np.record)\n         assert_equal(recarr, recarr_r)\n \n@@ -173,30 +173,30 @@ def test_recarray_views(self):\n \n         #check that np.rec.array gives right dtypes\n         assert_equal(np.rec.array(a).dtype.type, np.record)\n-        assert_equal(type(np.rec.array(a)), np.recarray)\n+        assert_equal(type(np.rec.array(a)), np.rec.recarray)\n         assert_equal(np.rec.array(b).dtype.type, np.int64)\n-        assert_equal(type(np.rec.array(b)), np.recarray)\n+        assert_equal(type(np.rec.array(b)), np.rec.recarray)\n \n         #check that viewing as recarray does the same\n-        assert_equal(a.view(np.recarray).dtype.type, np.record)\n-        assert_equal(type(a.view(np.recarray)), np.recarray)\n-        assert_equal(b.view(np.recarray).dtype.type, np.int64)\n-        assert_equal(type(b.view(np.recarray)), np.recarray)\n+        assert_equal(a.view(np.rec.recarray).dtype.type, np.record)\n+        assert_equal(type(a.view(np.rec.recarray)), np.rec.recarray)\n+        assert_equal(b.view(np.rec.recarray).dtype.type, np.int64)\n+        assert_equal(type(b.view(np.rec.recarray)), np.rec.recarray)\n \n-        #check that view to non-structured dtype preserves type=np.recarray\n+        #check that view to non-structured dtype preserves type=np.rec.recarray\n         r = np.rec.array(np.ones(4, dtype=\"f4,i4\"))\n         rv = r.view('f8').view('f4,i4')\n-        assert_equal(type(rv), np.recarray)\n+        assert_equal(type(rv), np.rec.recarray)\n         assert_equal(rv.dtype.type, np.record)\n \n-        #check that getitem also preserves np.recarray and np.record\n+        #check that getitem also preserves np.rec.recarray and np.record\n         r = np.rec.array(np.ones(4, dtype=[('a', 'i4'), ('b', 'i4'),\n                                            ('c', 'i4,i4')]))\n         assert_equal(r['c'].dtype.type, np.record)\n-        assert_equal(type(r['c']), np.recarray)\n+        assert_equal(type(r['c']), np.rec.recarray)\n \n         #and that it preserves subclasses (gh-6949)\n-        class C(np.recarray):\n+        class C(np.rec.recarray):\n             pass\n \n         c = r.view(C)\n@@ -278,7 +278,7 @@ def test_recarray_stringtypes(self):\n         # Issue #3993\n         a = np.array([('abc ', 1), ('abc', 2)],\n                      dtype=[('foo', 'S4'), ('bar', int)])\n-        a = a.view(np.recarray)\n+        a = a.view(np.rec.recarray)\n         assert_equal(a.foo[0] == a.foo[1], False)\n \n     def test_recarray_returntypes(self):\n@@ -290,10 +290,10 @@ def test_recarray_returntypes(self):\n                                 ('baz', int), ('qux', qux_fields)])\n         assert_equal(type(a.foo), np.ndarray)\n         assert_equal(type(a['foo']), np.ndarray)\n-        assert_equal(type(a.bar), np.recarray)\n-        assert_equal(type(a['bar']), np.recarray)\n+        assert_equal(type(a.bar), np.rec.recarray)\n+        assert_equal(type(a['bar']), np.rec.recarray)\n         assert_equal(a.bar.dtype.type, np.record)\n-        assert_equal(type(a['qux']), np.recarray)\n+        assert_equal(type(a['qux']), np.rec.recarray)\n         assert_equal(a.qux.dtype.type, np.record)\n         assert_equal(dict(a.qux.dtype.fields), qux_fields)\n         assert_equal(type(a.baz), np.ndarray)\n@@ -442,16 +442,18 @@ def test_pickle_void(self):\n     def test_objview_record(self):\n         # https://github.com/numpy/numpy/issues/2599\n         dt = np.dtype([('foo', 'i8'), ('bar', 'O')])\n-        r = np.zeros((1,3), dtype=dt).view(np.recarray)\n+        r = np.zeros((1, 3), dtype=dt).view(np.rec.recarray)\n         r.foo = np.array([1, 2, 3])  # TypeError?\n \n         # https://github.com/numpy/numpy/issues/3256\n-        ra = np.recarray((2,), dtype=[('x', object), ('y', float), ('z', int)])\n+        ra = np.rec.recarray(\n+            (2,), dtype=[('x', object), ('y', float), ('z', int)]\n+        )\n         ra[['x','y']]  # TypeError?\n \n     def test_record_scalar_setitem(self):\n         # https://github.com/numpy/numpy/issues/3561\n-        rec = np.recarray(1, dtype=[('x', float, 5)])\n+        rec = np.rec.recarray(1, dtype=[('x', float, 5)])\n         rec[0].x = 1\n         assert_equal(rec[0].x, np.ones(5))\n \n@@ -470,7 +472,7 @@ def test_fromarrays_nested_structured_arrays(self):\n     @pytest.mark.parametrize('nfields', [0, 1, 2])\n     def test_assign_dtype_attribute(self, nfields):\n         dt = np.dtype([('a', np.uint8), ('b', np.uint8), ('c', np.uint8)][:nfields])\n-        data = np.zeros(3, dt).view(np.recarray)\n+        data = np.zeros(3, dt).view(np.rec.recarray)\n \n         # the original and resulting dtypes differ on whether they are records\n         assert data.dtype.type == np.record\n@@ -486,9 +488,9 @@ def test_nested_fields_are_records(self, nfields):\n         dt = np.dtype([('a', np.uint8), ('b', np.uint8), ('c', np.uint8)][:nfields])\n         dt_outer = np.dtype([('inner', dt)])\n \n-        data = np.zeros(3, dt_outer).view(np.recarray)\n-        assert isinstance(data, np.recarray)\n-        assert isinstance(data['inner'], np.recarray)\n+        data = np.zeros(3, dt_outer).view(np.rec.recarray)\n+        assert isinstance(data, np.rec.recarray)\n+        assert isinstance(data['inner'], np.rec.recarray)\n \n         data0 = data[0]\n         assert isinstance(data0, np.record)\n@@ -503,7 +505,7 @@ def test_nested_dtype_padding(self):\n \n         dt_outer = np.dtype([('inner', dt_padded_end)])\n \n-        data = np.zeros(3, dt_outer).view(np.recarray)\n+        data = np.zeros(3, dt_outer).view(np.rec.recarray)\n         assert_equal(data['inner'].dtype, dt_padded_end)\n \n         data0 = data[0]\n",
            "comment_added_diff": {
                "186": "        #check that view to non-structured dtype preserves type=np.rec.recarray",
                "192": "        #check that getitem also preserves np.rec.recarray and np.record"
            },
            "comment_deleted_diff": {
                "186": "        #check that view to non-structured dtype preserves type=np.recarray",
                "192": "        #check that getitem also preserves np.recarray and np.record"
            },
            "comment_modified_diff": {
                "186": "        #check that view to non-structured dtype preserves type=np.recarray",
                "192": "        #check that getitem also preserves np.recarray and np.record"
            }
        },
        {
            "commit": "cee509746b55458b5e879a607b3e2501ff71ea2c",
            "timestamp": "2023-09-17T19:36:55+02:00",
            "author": "Mateusz Sok\u00f3\u0142",
            "commit_message": "Restore np.recarray and deprecate np.chararray",
            "additions": 26,
            "deletions": 26,
            "change_type": "MODIFY",
            "diff": "@@ -146,7 +146,7 @@ def test_recarray_from_repr(self):\n         a = np.array([(1,'ABC'), (2, \"DEF\")],\n                      dtype=[('foo', int), ('bar', 'S4')])\n         recordarr = np.rec.array(a)\n-        recarr = a.view(np.rec.recarray)\n+        recarr = a.view(np.recarray)\n         recordview = a.view(np.dtype((np.record, a.dtype)))\n \n         recordarr_r = eval(\"np.\" + repr(recordarr), {'np': np})\n@@ -154,11 +154,11 @@ def test_recarray_from_repr(self):\n         # Prints the type `numpy.record` as part of the dtype:\n         recordview_r = eval(\"np.\" + repr(recordview), {'np': np, 'numpy': np})\n \n-        assert_equal(type(recordarr_r), np.rec.recarray)\n+        assert_equal(type(recordarr_r), np.recarray)\n         assert_equal(recordarr_r.dtype.type, np.record)\n         assert_equal(recordarr, recordarr_r)\n \n-        assert_equal(type(recarr_r), np.rec.recarray)\n+        assert_equal(type(recarr_r), np.recarray)\n         assert_equal(recarr_r.dtype.type, np.record)\n         assert_equal(recarr, recarr_r)\n \n@@ -173,30 +173,30 @@ def test_recarray_views(self):\n \n         #check that np.rec.array gives right dtypes\n         assert_equal(np.rec.array(a).dtype.type, np.record)\n-        assert_equal(type(np.rec.array(a)), np.rec.recarray)\n+        assert_equal(type(np.rec.array(a)), np.recarray)\n         assert_equal(np.rec.array(b).dtype.type, np.int64)\n-        assert_equal(type(np.rec.array(b)), np.rec.recarray)\n+        assert_equal(type(np.rec.array(b)), np.recarray)\n \n         #check that viewing as recarray does the same\n-        assert_equal(a.view(np.rec.recarray).dtype.type, np.record)\n-        assert_equal(type(a.view(np.rec.recarray)), np.rec.recarray)\n-        assert_equal(b.view(np.rec.recarray).dtype.type, np.int64)\n-        assert_equal(type(b.view(np.rec.recarray)), np.rec.recarray)\n+        assert_equal(a.view(np.recarray).dtype.type, np.record)\n+        assert_equal(type(a.view(np.recarray)), np.recarray)\n+        assert_equal(b.view(np.recarray).dtype.type, np.int64)\n+        assert_equal(type(b.view(np.recarray)), np.recarray)\n \n-        #check that view to non-structured dtype preserves type=np.rec.recarray\n+        #check that view to non-structured dtype preserves type=np.recarray\n         r = np.rec.array(np.ones(4, dtype=\"f4,i4\"))\n         rv = r.view('f8').view('f4,i4')\n-        assert_equal(type(rv), np.rec.recarray)\n+        assert_equal(type(rv), np.recarray)\n         assert_equal(rv.dtype.type, np.record)\n \n-        #check that getitem also preserves np.rec.recarray and np.record\n+        #check that getitem also preserves np.recarray and np.record\n         r = np.rec.array(np.ones(4, dtype=[('a', 'i4'), ('b', 'i4'),\n                                            ('c', 'i4,i4')]))\n         assert_equal(r['c'].dtype.type, np.record)\n-        assert_equal(type(r['c']), np.rec.recarray)\n+        assert_equal(type(r['c']), np.recarray)\n \n         #and that it preserves subclasses (gh-6949)\n-        class C(np.rec.recarray):\n+        class C(np.recarray):\n             pass\n \n         c = r.view(C)\n@@ -278,7 +278,7 @@ def test_recarray_stringtypes(self):\n         # Issue #3993\n         a = np.array([('abc ', 1), ('abc', 2)],\n                      dtype=[('foo', 'S4'), ('bar', int)])\n-        a = a.view(np.rec.recarray)\n+        a = a.view(np.recarray)\n         assert_equal(a.foo[0] == a.foo[1], False)\n \n     def test_recarray_returntypes(self):\n@@ -290,10 +290,10 @@ def test_recarray_returntypes(self):\n                                 ('baz', int), ('qux', qux_fields)])\n         assert_equal(type(a.foo), np.ndarray)\n         assert_equal(type(a['foo']), np.ndarray)\n-        assert_equal(type(a.bar), np.rec.recarray)\n-        assert_equal(type(a['bar']), np.rec.recarray)\n+        assert_equal(type(a.bar), np.recarray)\n+        assert_equal(type(a['bar']), np.recarray)\n         assert_equal(a.bar.dtype.type, np.record)\n-        assert_equal(type(a['qux']), np.rec.recarray)\n+        assert_equal(type(a['qux']), np.recarray)\n         assert_equal(a.qux.dtype.type, np.record)\n         assert_equal(dict(a.qux.dtype.fields), qux_fields)\n         assert_equal(type(a.baz), np.ndarray)\n@@ -442,18 +442,18 @@ def test_pickle_void(self):\n     def test_objview_record(self):\n         # https://github.com/numpy/numpy/issues/2599\n         dt = np.dtype([('foo', 'i8'), ('bar', 'O')])\n-        r = np.zeros((1, 3), dtype=dt).view(np.rec.recarray)\n+        r = np.zeros((1, 3), dtype=dt).view(np.recarray)\n         r.foo = np.array([1, 2, 3])  # TypeError?\n \n         # https://github.com/numpy/numpy/issues/3256\n-        ra = np.rec.recarray(\n+        ra = np.recarray(\n             (2,), dtype=[('x', object), ('y', float), ('z', int)]\n         )\n         ra[['x','y']]  # TypeError?\n \n     def test_record_scalar_setitem(self):\n         # https://github.com/numpy/numpy/issues/3561\n-        rec = np.rec.recarray(1, dtype=[('x', float, 5)])\n+        rec = np.recarray(1, dtype=[('x', float, 5)])\n         rec[0].x = 1\n         assert_equal(rec[0].x, np.ones(5))\n \n@@ -472,7 +472,7 @@ def test_fromarrays_nested_structured_arrays(self):\n     @pytest.mark.parametrize('nfields', [0, 1, 2])\n     def test_assign_dtype_attribute(self, nfields):\n         dt = np.dtype([('a', np.uint8), ('b', np.uint8), ('c', np.uint8)][:nfields])\n-        data = np.zeros(3, dt).view(np.rec.recarray)\n+        data = np.zeros(3, dt).view(np.recarray)\n \n         # the original and resulting dtypes differ on whether they are records\n         assert data.dtype.type == np.record\n@@ -488,9 +488,9 @@ def test_nested_fields_are_records(self, nfields):\n         dt = np.dtype([('a', np.uint8), ('b', np.uint8), ('c', np.uint8)][:nfields])\n         dt_outer = np.dtype([('inner', dt)])\n \n-        data = np.zeros(3, dt_outer).view(np.rec.recarray)\n-        assert isinstance(data, np.rec.recarray)\n-        assert isinstance(data['inner'], np.rec.recarray)\n+        data = np.zeros(3, dt_outer).view(np.recarray)\n+        assert isinstance(data, np.recarray)\n+        assert isinstance(data['inner'], np.recarray)\n \n         data0 = data[0]\n         assert isinstance(data0, np.record)\n@@ -505,7 +505,7 @@ def test_nested_dtype_padding(self):\n \n         dt_outer = np.dtype([('inner', dt_padded_end)])\n \n-        data = np.zeros(3, dt_outer).view(np.rec.recarray)\n+        data = np.zeros(3, dt_outer).view(np.recarray)\n         assert_equal(data['inner'].dtype, dt_padded_end)\n \n         data0 = data[0]\n",
            "comment_added_diff": {
                "186": "        #check that view to non-structured dtype preserves type=np.recarray",
                "192": "        #check that getitem also preserves np.recarray and np.record"
            },
            "comment_deleted_diff": {
                "186": "        #check that view to non-structured dtype preserves type=np.rec.recarray",
                "192": "        #check that getitem also preserves np.rec.recarray and np.record"
            },
            "comment_modified_diff": {
                "186": "        #check that view to non-structured dtype preserves type=np.rec.recarray",
                "192": "        #check that getitem also preserves np.rec.recarray and np.record"
            }
        }
    ],
    "test_umath.py": [
        {
            "commit": "10908c5fd147572c9ea02cef0c0d5eae6892e4cb",
            "timestamp": "2022-11-07T14:42:32-08:00",
            "author": "Sebastian Berg",
            "commit_message": "DEP: Expire deprecation to ignore bad dtype= in logical ufuncs (#22541)\n\n* DEP: Expire deprecation to ignore bad dtype= in logical ufuncs\r\n\r\nBasically only `bool` and `object dtype make sense, but they did\r\nnot work correctly.  The dtype argument was rather ignored often.\r\n\r\nThe offending behavior was deprecated in 1.20 and is now removed.\r\n\r\nCo-authored-by: Sebastian Berg <sebastianb@nvidia.com>\r\nCo-authored-by: Charles Harris <charlesr.harris@gmail.com>",
            "additions": 17,
            "deletions": 2,
            "change_type": "MODIFY",
            "diff": "@@ -338,6 +338,21 @@ def test_error_in_equal_reduce(self):\n         assert_equal(np.equal.reduce(a, dtype=bool), True)\n         assert_raises(TypeError, np.equal.reduce, a)\n \n+    def test_object_dtype(self):\n+        assert np.equal(1, [1], dtype=object).dtype == object\n+        assert np.equal(1, [1], signature=(None, None, \"O\")).dtype == object\n+\n+    def test_object_nonbool_dtype_error(self):\n+        # bool output dtype is fine of course:\n+        assert np.equal(1, [1], dtype=bool).dtype == bool\n+\n+        # but the following are examples do not have a loop:\n+        with pytest.raises(TypeError, match=\"No loop matching\"):\n+            np.equal(1, 1, dtype=np.int64)\n+\n+        with pytest.raises(TypeError, match=\"No loop matching\"):\n+            np.equal(1, 1, sig=(None, None, \"l\"))\n+\n \n class TestAdd:\n     def test_reduce_alignment(self):\n@@ -1095,7 +1110,7 @@ def test_integer_to_negative_power(self):\n             assert_raises(ValueError, np.power, a, minusone)\n             assert_raises(ValueError, np.power, one, b)\n             assert_raises(ValueError, np.power, one, minusone)\n-    \n+\n     def test_float_to_inf_power(self):\n         for dt in [np.float32, np.float64]:\n             a = np.array([1, 1, 2, 2, -2, -2, np.inf, -np.inf], dt)\n@@ -1348,7 +1363,7 @@ def test_sincos_values(self):\n                 yf = np.array(y, dtype=dt)\n                 assert_equal(np.sin(yf), xf)\n                 assert_equal(np.cos(yf), xf)\n-        \n+\n \n         with np.errstate(invalid='raise'):\n             for callable in [np.sin, np.cos]:\n",
            "comment_added_diff": {
                "346": "        # bool output dtype is fine of course:",
                "349": "        # but the following are examples do not have a loop:"
            },
            "comment_deleted_diff": {},
            "comment_modified_diff": {}
        },
        {
            "commit": "38479a2e0bebcdec4e190d64268c1f6e7f8febc6",
            "timestamp": "2022-11-28T14:26:57-08:00",
            "author": "Developer-Ecosystem-Engineering",
            "commit_message": "add tests",
            "additions": 23,
            "deletions": 0,
            "change_type": "MODIFY",
            "diff": "@@ -2608,6 +2608,29 @@ def test_lower_align(self):\n         np.abs(d, out=d)\n         np.abs(np.ones_like(d), out=d)\n \n+    @pytest.mark.parametrize(\"dtype\", ['d', 'f', 'int32', 'int64'])\n+    def test_noncontiguous(self, dtype):\n+        data = np.array([-1.0, 1.0, -0.0, 0.0, 2.2251e-308, -2.5, 2.5, -6, 6,\n+                            -2.2251e-308, -8, 10], dtype=dtype)\n+        expect = np.array([1.0, -1.0, 0.0, -0.0, -2.2251e-308, 2.5, -2.5, 6, -6,\n+                             2.2251e-308, 8, -10], dtype=dtype)\n+        out = np.ndarray(data.shape, dtype=dtype)\n+        ncontig_in = data[1::2]\n+        ncontig_out = out[1::2]\n+        contig_in = np.array(ncontig_in)\n+        # contig in, contig out\n+        assert_array_equal(np.negative(contig_in), expect[1::2])\n+        # contig in, ncontig out\n+        assert_array_equal(np.negative(contig_in, out=ncontig_out), expect[1::2])\n+        # ncontig in, contig out\n+        assert_array_equal(np.negative(ncontig_in), expect[1::2])\n+        # ncontig in, ncontig out\n+        assert_array_equal(np.negative(ncontig_in, out=ncontig_out), expect[1::2])\n+        # contig in, contig out, nd stride\n+        data_split = np.array(np.array_split(data, 2))\n+        expect_split = np.array(np.array_split(expect, 2))\n+        assert_equal(np.negative(data_split), expect_split)\n+\n \n class TestPositive:\n     def test_valid(self):\n",
            "comment_added_diff": {
                "2621": "        # contig in, contig out",
                "2623": "        # contig in, ncontig out",
                "2625": "        # ncontig in, contig out",
                "2627": "        # ncontig in, ncontig out",
                "2629": "        # contig in, contig out, nd stride"
            },
            "comment_deleted_diff": {},
            "comment_modified_diff": {}
        },
        {
            "commit": "e33a161a0d749368f84f30f1c9b13b8df1060520",
            "timestamp": "2022-12-05T15:40:15-05:00",
            "author": "Matti Picus",
            "commit_message": "TST: skip floating-point error test on wasm",
            "additions": 2,
            "deletions": 1,
            "change_type": "MODIFY",
            "diff": "@@ -1069,8 +1069,9 @@ def assert_complex_equal(x, y):\n                 assert_complex_equal(np.power(zero, -p), cnan)\n             assert_complex_equal(np.power(zero, -1+0.2j), cnan)\n \n-    # Testing 0^{Non-zero} issue 18378\n+    @pytest.mark.skipif(IS_WASM, reason=\"fp errors don't work in wasm\")\n     def test_zero_power_nonzero(self):\n+        # Testing 0^{Non-zero} issue 18378\n         zero = np.array([0.0+0.0j])\n         cnan = np.array([complex(np.nan, np.nan)])\n \n",
            "comment_added_diff": {
                "1074": "        # Testing 0^{Non-zero} issue 18378"
            },
            "comment_deleted_diff": {
                "1072": "    # Testing 0^{Non-zero} issue 18378"
            },
            "comment_modified_diff": {}
        },
        {
            "commit": "14cd06f38852799603bf1a36460cc5bbab37ce2a",
            "timestamp": "2022-12-14T20:23:22+02:00",
            "author": "Sayed Adel",
            "commit_message": "ENH, TST: Test all FP unary ufunc against any unexpected fp exceptions",
            "additions": 78,
            "deletions": 10,
            "change_type": "MODIFY",
            "diff": "@@ -21,6 +21,15 @@\n     )\n from numpy.testing._private.utils import _glibc_older_than\n \n+UFUNCS = [obj for obj in np.core.umath.__dict__.values()\n+         if isinstance(obj, np.ufunc)]\n+\n+UFUNCS_UNARY = [\n+    uf for uf in UFUNCS if uf.nin == 1\n+]\n+UFUNCS_UNARY_FP = [\n+    uf for uf in UFUNCS_UNARY if 'f->f' in uf.types\n+]\n \n def interesting_binop_operands(val1, val2, dtype):\n     \"\"\"\n@@ -1086,7 +1095,7 @@ def assert_complex_equal(x, y):\n         assert_complex_equal(np.power(zero, 1+1j), zero)\n         assert_complex_equal(np.power(zero, 1+0j), zero)\n         assert_complex_equal(np.power(zero, 1-1j), zero)\n-        #Complex powers will negative real part or 0 (provided imaginary \n+        #Complex powers will negative real part or 0 (provided imaginary\n         # part is not zero) will generate a NAN and hence a RUNTIME warning\n         with pytest.warns(expected_warning=RuntimeWarning) as r:\n             assert_complex_equal(np.power(zero, -1+1j), cnan)\n@@ -1631,15 +1640,74 @@ def test_expm1(self):\n                                   np.array(value, dtype=dt))\n \n     # test to ensure no spurious FP exceptions are raised due to SIMD\n-    def test_spurious_fpexception(self):\n-        for dt in ['e', 'f', 'd']:\n-            arr = np.array([1.0, 2.0], dtype=dt)\n-            with assert_no_warnings():\n-                np.log(arr)\n-                np.log2(arr)\n-                np.log10(arr)\n-                np.arccosh(arr)\n-\n+    INF_INVALID_ERR = [\n+        np.cos, np.sin, np.tan, np.arccos, np.arcsin, np.spacing, np.arctanh\n+    ]\n+    NEG_INVALID_ERR = [\n+        np.log, np.log2, np.log10, np.log1p, np.sqrt, np.arccosh,\n+        np.arctanh\n+    ]\n+    ONE_INVALID_ERR = [\n+        np.arctanh,\n+    ]\n+    LTONE_INVALID_ERR = [\n+        np.arccosh,\n+    ]\n+    BYZERO_ERR = [\n+        np.log, np.log2, np.log10, np.reciprocal, np.arccosh\n+    ]\n+\n+    @pytest.mark.parametrize(\"ufunc\", UFUNCS_UNARY_FP)\n+    @pytest.mark.parametrize(\"dtype\", ('e', 'f', 'd'))\n+    @pytest.mark.parametrize(\"data, escape\", (\n+        ([0.03], LTONE_INVALID_ERR),\n+        ([0.03]*32, LTONE_INVALID_ERR),\n+        # neg\n+        ([-1.0], NEG_INVALID_ERR),\n+        ([-1.0]*32, NEG_INVALID_ERR),\n+        # flat\n+        ([1.0], ONE_INVALID_ERR),\n+        ([1.0]*32, ONE_INVALID_ERR),\n+        # zero\n+        ([0.0], BYZERO_ERR),\n+        ([0.0]*32, BYZERO_ERR),\n+        ([-0.0], BYZERO_ERR),\n+        ([-0.0]*32, BYZERO_ERR),\n+        # nan\n+        ([0.5, 0.5, 0.5, np.nan], LTONE_INVALID_ERR),\n+        ([0.5, 0.5, 0.5, np.nan]*32, LTONE_INVALID_ERR),\n+        ([np.nan, 1.0, 1.0, 1.0], ONE_INVALID_ERR),\n+        ([np.nan, 1.0, 1.0, 1.0]*32, ONE_INVALID_ERR),\n+        ([np.nan], []),\n+        ([np.nan]*32, []),\n+        # inf\n+        ([0.5, 0.5, 0.5, np.inf], INF_INVALID_ERR + LTONE_INVALID_ERR),\n+        ([0.5, 0.5, 0.5, np.inf]*32, INF_INVALID_ERR + LTONE_INVALID_ERR),\n+        ([np.inf, 1.0, 1.0, 1.0], INF_INVALID_ERR),\n+        ([np.inf, 1.0, 1.0, 1.0]*32, INF_INVALID_ERR),\n+        ([np.inf], INF_INVALID_ERR),\n+        ([np.inf]*32, INF_INVALID_ERR),\n+        # ninf\n+        ([0.5, 0.5, 0.5, -np.inf],\n+         NEG_INVALID_ERR + INF_INVALID_ERR + LTONE_INVALID_ERR),\n+        ([0.5, 0.5, 0.5, -np.inf]*32,\n+         NEG_INVALID_ERR + INF_INVALID_ERR + LTONE_INVALID_ERR),\n+        ([-np.inf, 1.0, 1.0, 1.0], NEG_INVALID_ERR + INF_INVALID_ERR),\n+        ([-np.inf, 1.0, 1.0, 1.0]*32, NEG_INVALID_ERR + INF_INVALID_ERR),\n+        ([-np.inf], NEG_INVALID_ERR + INF_INVALID_ERR),\n+        ([-np.inf]*32, NEG_INVALID_ERR + INF_INVALID_ERR),\n+    ))\n+    def test_unary_spurious_fpexception(self, ufunc, dtype, data, escape):\n+        if escape and ufunc in escape:\n+            return\n+        # FIXME: NAN raises FP invalid exception:\n+        #  - ceil/float16 on MSVC:32-bit\n+        #  - spacing/float16 on almost all platforms\n+        if ufunc in (np.spacing, np.ceil) and dtype == 'e':\n+            return\n+        array = np.array(data, dtype=dtype)\n+        with assert_no_warnings():\n+            ufunc(array)\n \n class TestFPClass:\n     @pytest.mark.parametrize(\"stride\", [-4,-2,-1,1,2,4])\n",
            "comment_added_diff": {
                "1098": "        #Complex powers will negative real part or 0 (provided imaginary",
                "1665": "        # neg",
                "1668": "        # flat",
                "1671": "        # zero",
                "1676": "        # nan",
                "1683": "        # inf",
                "1690": "        # ninf",
                "1703": "        # FIXME: NAN raises FP invalid exception:",
                "1704": "        #  - ceil/float16 on MSVC:32-bit",
                "1705": "        #  - spacing/float16 on almost all platforms"
            },
            "comment_deleted_diff": {
                "1089": "        #Complex powers will negative real part or 0 (provided imaginary"
            },
            "comment_modified_diff": {}
        },
        {
            "commit": "e2fe83155812440dbe482746777ea5df75bc4d83",
            "timestamp": "2023-01-04T02:19:18-08:00",
            "author": "Developer-Ecosystem-Engineering",
            "commit_message": "add some tests with different shape arrays",
            "additions": 16,
            "deletions": 1,
            "change_type": "MODIFY",
            "diff": "@@ -1717,7 +1717,7 @@ def test_unary_spurious_fpexception(self, ufunc, dtype, data, escape):\n             ufunc(array)\n \n class TestFPClass:\n-    @pytest.mark.parametrize(\"stride\", [-4,-2,-1,1,2,4])\n+    @pytest.mark.parametrize(\"stride\", [-5,-4,-3,-2,-1,1,2,4,5,6,7,8,9,10])\n     def test_fpclass(self, stride):\n         arr_f64 = np.array([np.nan, -np.nan, np.inf, -np.inf, -1.0, 1.0, -0.0, 0.0, 2.2251e-308, -2.2251e-308], dtype='d')\n         arr_f32 = np.array([np.nan, -np.nan, np.inf, -np.inf, -1.0, 1.0, -0.0, 0.0, 1.4013e-045, -1.4013e-045], dtype='f')\n@@ -1733,6 +1733,21 @@ def test_fpclass(self, stride):\n         assert_equal(np.signbit(arr_f64[::stride]), sign[::stride])\n         assert_equal(np.isfinite(arr_f32[::stride]), finite[::stride])\n         assert_equal(np.isfinite(arr_f64[::stride]), finite[::stride])\n+        # Try with split\n+        arr_f64_split = np.array(np.array_split(arr_f64, 2))\n+        arr_f32_split = np.array(np.array_split(arr_f32, 2))\n+        nan_split = np.array(np.array_split(nan, 2))\n+        inf_split = np.array(np.array_split(inf, 2))\n+        sign_split = np.array(np.array_split(sign, 2))\n+        finite_split = np.array(np.array_split(finite, 2))\n+        assert_equal(np.isnan(arr_f64_split), nan_split)\n+        assert_equal(np.isinf(arr_f64_split), inf_split)\n+        assert_equal(np.signbit(arr_f64_split), sign_split)\n+        assert_equal(np.isfinite(arr_f64_split), finite_split)\n+        assert_equal(np.isnan(arr_f32_split), nan_split)\n+        assert_equal(np.isinf(arr_f32_split), inf_split)\n+        assert_equal(np.signbit(arr_f32_split), sign_split)\n+        assert_equal(np.isfinite(arr_f32_split), finite_split)\n \n class TestLDExp:\n     @pytest.mark.parametrize(\"stride\", [-4,-2,-1,1,2,4])\n",
            "comment_added_diff": {
                "1736": "        # Try with split"
            },
            "comment_deleted_diff": {},
            "comment_modified_diff": {}
        },
        {
            "commit": "d5703e1be4cf5db2be5968702b46d2be156173e7",
            "timestamp": "2023-01-04T02:19:18-08:00",
            "author": "Developer-Ecosystem-Engineering",
            "commit_message": "see if stride_tricks can give us more coverage",
            "additions": 37,
            "deletions": 1,
            "change_type": "MODIFY",
            "diff": "@@ -1717,7 +1717,7 @@ def test_unary_spurious_fpexception(self, ufunc, dtype, data, escape):\n             ufunc(array)\n \n class TestFPClass:\n-    @pytest.mark.parametrize(\"stride\", [-5,-4,-3,-2,-1,1,2,4,5,6,7,8,9,10])\n+    @pytest.mark.parametrize(\"stride\", [-5, -4 ,-3, -2, -1, 1, 2, 4, 5, 6, 7, 8, 9, 10])\n     def test_fpclass(self, stride):\n         arr_f64 = np.array([np.nan, -np.nan, np.inf, -np.inf, -1.0, 1.0, -0.0, 0.0, 2.2251e-308, -2.2251e-308], dtype='d')\n         arr_f32 = np.array([np.nan, -np.nan, np.inf, -np.inf, -1.0, 1.0, -0.0, 0.0, 1.4013e-045, -1.4013e-045], dtype='f')\n@@ -1748,6 +1748,42 @@ def test_fpclass(self, stride):\n         assert_equal(np.isinf(arr_f32_split), inf_split)\n         assert_equal(np.signbit(arr_f32_split), sign_split)\n         assert_equal(np.isfinite(arr_f32_split), finite_split)\n+        # Try with as_strided\n+        arr_f64_strided = np.lib.stride_tricks.as_strided(arr_f64, strides=(stride, ))\n+        nan_strided =    [np.isnan(val) for val in arr_f64_strided]\n+        inf_strided =    [np.isinf(val) for val in arr_f64_strided]\n+        sign_strided =   [np.signbit(val) for val in arr_f64_strided]\n+        finite_strided = [np.isfinite(val) for val in arr_f64_strided]\n+        assert_equal(np.isnan(arr_f64_strided), nan_strided)\n+        assert_equal(np.isinf(arr_f64_strided), inf_strided)\n+        assert_equal(np.signbit(arr_f64_strided), sign_strided)\n+        assert_equal(np.isfinite(arr_f64_strided), finite_strided)\n+        out_strided = np.ndarray(arr_f64_strided.shape, dtype='bool')\n+        np.isnan(arr_f64_strided, out=out_strided)\n+        assert_equal(out_strided, nan_strided)\n+        np.isinf(arr_f64_strided, out=out_strided)\n+        assert_equal(out_strided, inf_strided)\n+        np.signbit(arr_f64_strided, out=out_strided)\n+        assert_equal(out_strided, sign_strided)\n+        np.isfinite(arr_f64_strided, out=out_strided)\n+        assert_equal(out_strided, finite_strided)\n+        arr_f32_strided = np.lib.stride_tricks.as_strided(arr_f32, strides=(stride, ))\n+        nan_strided =    [np.isnan(val) for val in arr_f32_strided]\n+        inf_strided =    [np.isinf(val) for val in arr_f32_strided]\n+        sign_strided =   [np.signbit(val) for val in arr_f32_strided]\n+        finite_strided = [np.isfinite(val) for val in arr_f32_strided]\n+        assert_equal(np.isnan(arr_f32_strided), nan_strided)\n+        assert_equal(np.isinf(arr_f32_strided), inf_strided)\n+        assert_equal(np.signbit(arr_f32_strided), sign_strided)\n+        assert_equal(np.isfinite(arr_f32_strided), finite_strided)\n+        np.isnan(arr_f32_strided, out=out_strided)\n+        assert_equal(out_strided, nan_strided)\n+        np.isinf(arr_f32_strided, out=out_strided)\n+        assert_equal(out_strided, inf_strided)\n+        np.signbit(arr_f32_strided, out=out_strided)\n+        assert_equal(out_strided, sign_strided)\n+        np.isfinite(arr_f32_strided, out=out_strided)\n+        assert_equal(out_strided, finite_strided)\n \n class TestLDExp:\n     @pytest.mark.parametrize(\"stride\", [-4,-2,-1,1,2,4])\n",
            "comment_added_diff": {
                "1751": "        # Try with as_strided"
            },
            "comment_deleted_diff": {},
            "comment_modified_diff": {}
        },
        {
            "commit": "6a6c6356647f7670e96a0a9b9d3f1ddd52e18f90",
            "timestamp": "2023-01-04T02:19:18-08:00",
            "author": "Developer-Ecosystem-Engineering",
            "commit_message": "fix lint & add test_fp_noncontiguous",
            "additions": 48,
            "deletions": 9,
            "change_type": "MODIFY",
            "diff": "@@ -1717,7 +1717,8 @@ def test_unary_spurious_fpexception(self, ufunc, dtype, data, escape):\n             ufunc(array)\n \n class TestFPClass:\n-    @pytest.mark.parametrize(\"stride\", [-5, -4 ,-3, -2, -1, 1, 2, 4, 5, 6, 7, 8, 9, 10])\n+    @pytest.mark.parametrize(\"stride\", [-5, -4, -3, -2, -1, 1,\n+                                2, 4, 5, 6, 7, 8, 9, 10])\n     def test_fpclass(self, stride):\n         arr_f64 = np.array([np.nan, -np.nan, np.inf, -np.inf, -1.0, 1.0, -0.0, 0.0, 2.2251e-308, -2.2251e-308], dtype='d')\n         arr_f32 = np.array([np.nan, -np.nan, np.inf, -np.inf, -1.0, 1.0, -0.0, 0.0, 1.4013e-045, -1.4013e-045], dtype='f')\n@@ -1749,10 +1750,11 @@ def test_fpclass(self, stride):\n         assert_equal(np.signbit(arr_f32_split), sign_split)\n         assert_equal(np.isfinite(arr_f32_split), finite_split)\n         # Try with as_strided\n-        arr_f64_strided = np.lib.stride_tricks.as_strided(arr_f64, strides=(stride, ))\n-        nan_strided =    [np.isnan(val) for val in arr_f64_strided]\n-        inf_strided =    [np.isinf(val) for val in arr_f64_strided]\n-        sign_strided =   [np.signbit(val) for val in arr_f64_strided]\n+        arr_f64_strided = np.lib.stride_tricks.as_strided(arr_f64,\n+            strides=(stride, ))\n+        nan_strided = [np.isnan(val) for val in arr_f64_strided]\n+        inf_strided = [np.isinf(val) for val in arr_f64_strided]\n+        sign_strided = [np.signbit(val) for val in arr_f64_strided]\n         finite_strided = [np.isfinite(val) for val in arr_f64_strided]\n         assert_equal(np.isnan(arr_f64_strided), nan_strided)\n         assert_equal(np.isinf(arr_f64_strided), inf_strided)\n@@ -1767,10 +1769,11 @@ def test_fpclass(self, stride):\n         assert_equal(out_strided, sign_strided)\n         np.isfinite(arr_f64_strided, out=out_strided)\n         assert_equal(out_strided, finite_strided)\n-        arr_f32_strided = np.lib.stride_tricks.as_strided(arr_f32, strides=(stride, ))\n-        nan_strided =    [np.isnan(val) for val in arr_f32_strided]\n-        inf_strided =    [np.isinf(val) for val in arr_f32_strided]\n-        sign_strided =   [np.signbit(val) for val in arr_f32_strided]\n+        arr_f32_strided = np.lib.stride_tricks.as_strided(arr_f32,\n+            strides=(stride, ))\n+        nan_strided = [np.isnan(val) for val in arr_f32_strided]\n+        inf_strided = [np.isinf(val) for val in arr_f32_strided]\n+        sign_strided = [np.signbit(val) for val in arr_f32_strided]\n         finite_strided = [np.isfinite(val) for val in arr_f32_strided]\n         assert_equal(np.isnan(arr_f32_strided), nan_strided)\n         assert_equal(np.isinf(arr_f32_strided), inf_strided)\n@@ -1785,6 +1788,42 @@ def test_fpclass(self, stride):\n         np.isfinite(arr_f32_strided, out=out_strided)\n         assert_equal(out_strided, finite_strided)\n \n+    @pytest.mark.parametrize(\"dtype\", ['d', 'f'])\n+    def test_fp_noncontiguous(self, dtype):\n+        data = np.array([np.nan, -np.nan, np.inf, -np.inf, -1.0,\n+                            1.0, -0.0, 0.0, 2.2251e-308,\n+                            -2.2251e-308], dtype=dtype)\n+        nan = np.array([True, True, False, False, False, False,\n+                            False, False, False, False])\n+        inf = np.array([False, False, True, True, False, False,\n+                            False, False, False, False])\n+        sign = np.array([False, True, False, True, True, False,\n+                            True, False, False, True])\n+        finite = np.array([False, False, False, False, True, True,\n+                            True, True, True, True])\n+        out = np.ndarray(data.shape, dtype='bool')\n+        ncontig_in = data[1::3]\n+        ncontig_out = out[1::3]\n+        contig_in = np.array(ncontig_in)\n+        assert_equal(ncontig_in.flags.c_contiguous, False)\n+        assert_equal(ncontig_out.flags.c_contiguous, False)\n+        assert_equal(contig_in.flags.c_contiguous, True)\n+        # ncontig in, ncontig out\n+        assert_equal(np.isnan(ncontig_in, out=ncontig_out), nan[1::3])\n+        assert_equal(np.isinf(ncontig_in, out=ncontig_out), inf[1::3])\n+        assert_equal(np.signbit(ncontig_in, out=ncontig_out), sign[1::3])\n+        assert_equal(np.isfinite(ncontig_in, out=ncontig_out), finite[1::3])\n+        # contig in, ncontig out\n+        assert_equal(np.isnan(contig_in, out=ncontig_out), nan[1::3])\n+        assert_equal(np.isinf(contig_in, out=ncontig_out), inf[1::3])\n+        assert_equal(np.signbit(contig_in, out=ncontig_out), sign[1::3])\n+        assert_equal(np.isfinite(contig_in, out=ncontig_out), finite[1::3])\n+        # ncontig in, contig out\n+        assert_equal(np.isnan(ncontig_in), nan[1::3])\n+        assert_equal(np.isinf(ncontig_in), inf[1::3])\n+        assert_equal(np.signbit(ncontig_in), sign[1::3])\n+        assert_equal(np.isfinite(ncontig_in), finite[1::3])\n+\n class TestLDExp:\n     @pytest.mark.parametrize(\"stride\", [-4,-2,-1,1,2,4])\n     @pytest.mark.parametrize(\"dtype\", ['f', 'd'])\n",
            "comment_added_diff": {
                "1811": "        # ncontig in, ncontig out",
                "1816": "        # contig in, ncontig out",
                "1821": "        # ncontig in, contig out"
            },
            "comment_deleted_diff": {},
            "comment_modified_diff": {}
        },
        {
            "commit": "61c80022c20f78cf2283d8d2336470ae488ea828",
            "timestamp": "2023-01-04T02:19:18-08:00",
            "author": "Developer-Ecosystem-Engineering",
            "commit_message": "move split test to test_fp_noncontiguous, remove use of as_strided as it was reading random data and not reliable also didn't give additional coverage",
            "additions": 10,
            "deletions": 53,
            "change_type": "MODIFY",
            "diff": "@@ -1734,59 +1734,6 @@ def test_fpclass(self, stride):\n         assert_equal(np.signbit(arr_f64[::stride]), sign[::stride])\n         assert_equal(np.isfinite(arr_f32[::stride]), finite[::stride])\n         assert_equal(np.isfinite(arr_f64[::stride]), finite[::stride])\n-        # Try with split\n-        arr_f64_split = np.array(np.array_split(arr_f64, 2))\n-        arr_f32_split = np.array(np.array_split(arr_f32, 2))\n-        nan_split = np.array(np.array_split(nan, 2))\n-        inf_split = np.array(np.array_split(inf, 2))\n-        sign_split = np.array(np.array_split(sign, 2))\n-        finite_split = np.array(np.array_split(finite, 2))\n-        assert_equal(np.isnan(arr_f64_split), nan_split)\n-        assert_equal(np.isinf(arr_f64_split), inf_split)\n-        assert_equal(np.signbit(arr_f64_split), sign_split)\n-        assert_equal(np.isfinite(arr_f64_split), finite_split)\n-        assert_equal(np.isnan(arr_f32_split), nan_split)\n-        assert_equal(np.isinf(arr_f32_split), inf_split)\n-        assert_equal(np.signbit(arr_f32_split), sign_split)\n-        assert_equal(np.isfinite(arr_f32_split), finite_split)\n-        # Try with as_strided\n-        arr_f64_strided = np.lib.stride_tricks.as_strided(arr_f64,\n-            strides=(stride, ))\n-        nan_strided = [np.isnan(val) for val in arr_f64_strided]\n-        inf_strided = [np.isinf(val) for val in arr_f64_strided]\n-        sign_strided = [np.signbit(val) for val in arr_f64_strided]\n-        finite_strided = [np.isfinite(val) for val in arr_f64_strided]\n-        assert_equal(np.isnan(arr_f64_strided), nan_strided)\n-        assert_equal(np.isinf(arr_f64_strided), inf_strided)\n-        assert_equal(np.signbit(arr_f64_strided), sign_strided)\n-        assert_equal(np.isfinite(arr_f64_strided), finite_strided)\n-        out_strided = np.ndarray(arr_f64_strided.shape, dtype='bool')\n-        np.isnan(arr_f64_strided, out=out_strided)\n-        assert_equal(out_strided, nan_strided)\n-        np.isinf(arr_f64_strided, out=out_strided)\n-        assert_equal(out_strided, inf_strided)\n-        np.signbit(arr_f64_strided, out=out_strided)\n-        assert_equal(out_strided, sign_strided)\n-        np.isfinite(arr_f64_strided, out=out_strided)\n-        assert_equal(out_strided, finite_strided)\n-        arr_f32_strided = np.lib.stride_tricks.as_strided(arr_f32,\n-            strides=(stride, ))\n-        nan_strided = [np.isnan(val) for val in arr_f32_strided]\n-        inf_strided = [np.isinf(val) for val in arr_f32_strided]\n-        sign_strided = [np.signbit(val) for val in arr_f32_strided]\n-        finite_strided = [np.isfinite(val) for val in arr_f32_strided]\n-        assert_equal(np.isnan(arr_f32_strided), nan_strided)\n-        assert_equal(np.isinf(arr_f32_strided), inf_strided)\n-        assert_equal(np.signbit(arr_f32_strided), sign_strided)\n-        assert_equal(np.isfinite(arr_f32_strided), finite_strided)\n-        np.isnan(arr_f32_strided, out=out_strided)\n-        assert_equal(out_strided, nan_strided)\n-        np.isinf(arr_f32_strided, out=out_strided)\n-        assert_equal(out_strided, inf_strided)\n-        np.signbit(arr_f32_strided, out=out_strided)\n-        assert_equal(out_strided, sign_strided)\n-        np.isfinite(arr_f32_strided, out=out_strided)\n-        assert_equal(out_strided, finite_strided)\n \n     @pytest.mark.parametrize(\"dtype\", ['d', 'f'])\n     def test_fp_noncontiguous(self, dtype):\n@@ -1823,6 +1770,16 @@ def test_fp_noncontiguous(self, dtype):\n         assert_equal(np.isinf(ncontig_in), inf[1::3])\n         assert_equal(np.signbit(ncontig_in), sign[1::3])\n         assert_equal(np.isfinite(ncontig_in), finite[1::3])\n+        # Try with split\n+        data_split = np.array(np.array_split(data, 2))\n+        nan_split = np.array(np.array_split(nan, 2))\n+        inf_split = np.array(np.array_split(inf, 2))\n+        sign_split = np.array(np.array_split(sign, 2))\n+        finite_split = np.array(np.array_split(finite, 2))\n+        assert_equal(np.isnan(data_split), nan_split)\n+        assert_equal(np.isinf(data_split), inf_split)\n+        assert_equal(np.signbit(data_split), sign_split)\n+        assert_equal(np.isfinite(data_split), finite_split)\n \n class TestLDExp:\n     @pytest.mark.parametrize(\"stride\", [-4,-2,-1,1,2,4])\n",
            "comment_added_diff": {
                "1773": "        # Try with split"
            },
            "comment_deleted_diff": {
                "1737": "        # Try with split",
                "1752": "        # Try with as_strided"
            },
            "comment_modified_diff": {
                "1773": "            strides=(stride, ))"
            }
        },
        {
            "commit": "4e38cee3ba690511778da3d925934db6737955ee",
            "timestamp": "2023-01-04T02:19:18-08:00",
            "author": "Developer-Ecosystem-Engineering",
            "commit_message": "fix comment",
            "additions": 1,
            "deletions": 1,
            "change_type": "MODIFY",
            "diff": "@@ -1770,7 +1770,7 @@ def test_fp_noncontiguous(self, dtype):\n         assert_equal(np.isinf(ncontig_in), inf[1::3])\n         assert_equal(np.signbit(ncontig_in), sign[1::3])\n         assert_equal(np.isfinite(ncontig_in), finite[1::3])\n-        # Try with split\n+        # contig in, contig out, nd stride\n         data_split = np.array(np.array_split(data, 2))\n         nan_split = np.array(np.array_split(nan, 2))\n         inf_split = np.array(np.array_split(inf, 2))\n",
            "comment_added_diff": {
                "1773": "        # contig in, contig out, nd stride"
            },
            "comment_deleted_diff": {
                "1773": "        # Try with split"
            },
            "comment_modified_diff": {
                "1773": "        # Try with split"
            }
        },
        {
            "commit": "f809fdd8e9b20b993a9a694f4c27f8c6daa07708",
            "timestamp": "2023-01-10T16:19:58+01:00",
            "author": "Sebastian Berg",
            "commit_message": "BUG: Ensure correct loop order in sin, cos, and arctan2\n\nThese were incorrect afer being vectorized.  The commit additional\ntests these (not arctan2 admittedly) and adds a check to generate_umath\nto make it a bit less likely that future additions add this type of thing.\n\nNote that the check allows duplicated loops so long they are correctly\nordered the *first* time.  This makes results correct, but duplicated\nloops are not nice anyways and it would be nice to remove them.\n\nWe could drop them manually in hindsight even?  In any case, that should\nnot be backported, so it is not includedhere.\n\nCloses gh-22984",
            "additions": 8,
            "deletions": 0,
            "change_type": "MODIFY",
            "diff": "@@ -4027,6 +4027,14 @@ def check(func, z0, d=1):\n             check(func, pts, 1j)\n             check(func, pts, 1+1j)\n \n+    @np.errstate(all=\"ignore\")\n+    def test_promotion_corner_cases(self):\n+        for func in self.funcs:\n+            assert func(np.float16(1)).dtype == np.float16\n+            # Integer to low precision float promotion is a dubious choice:\n+            assert func(np.uint8(1)).dtype == np.float16\n+            assert func(np.int16(1)).dtype == np.float32\n+\n \n class TestAttributes:\n     def test_attributes(self):\n",
            "comment_added_diff": {
                "4034": "            # Integer to low precision float promotion is a dubious choice:"
            },
            "comment_deleted_diff": {},
            "comment_modified_diff": {}
        },
        {
            "commit": "20d397400d6325cff3decbba3d6195418e873237",
            "timestamp": "2023-02-01T18:43:05+11:00",
            "author": "Andrew Nelson",
            "commit_message": "WHL: musllinux wheels [wheel build]",
            "additions": 15,
            "deletions": 4,
            "change_type": "MODIFY",
            "diff": "@@ -1292,9 +1292,20 @@ def test_log_values(self):\n \n         # test log() of max for dtype does not raise\n         for dt in ['f', 'd', 'g']:\n-            with np.errstate(all='raise'):\n-                x = np.finfo(dt).max\n-                np.log(x)\n+            try:\n+                with np.errstate(all='raise'):\n+                    x = np.finfo(dt).max\n+                    np.log(x)\n+            except FloatingPointError as exc:\n+                if dt == 'g' and IS_MUSL:\n+                    # FloatingPointError is known to occur on longdouble\n+                    # for musllinux_x86_64 x is very large\n+                    pytest.skip(\n+                        \"Overflow has occurred for\"\n+                        \" np.log(np.finfo(np.longdouble).max)\"\n+                    )\n+                else:\n+                    raise exc\n \n     def test_log_strides(self):\n         np.random.seed(42)\n@@ -4213,7 +4224,7 @@ def _test_spacing(t):\n     nan = t(np.nan)\n     inf = t(np.inf)\n     with np.errstate(invalid='ignore'):\n-        assert_(np.spacing(one) == eps)\n+        assert_equal(np.spacing(one), eps)\n         assert_(np.isnan(np.spacing(nan)))\n         assert_(np.isnan(np.spacing(inf)))\n         assert_(np.isnan(np.spacing(-inf)))\n",
            "comment_added_diff": {
                "1301": "                    # FloatingPointError is known to occur on longdouble",
                "1302": "                    # for musllinux_x86_64 x is very large"
            },
            "comment_deleted_diff": {},
            "comment_modified_diff": {}
        },
        {
            "commit": "e1e487acf1d820cbab8a6f97986bf2fb451dfa8e",
            "timestamp": "2023-02-11T22:46:28+01:00",
            "author": "Dimitri Papadopoulos",
            "commit_message": "Fix typos found by copdespell",
            "additions": 1,
            "deletions": 1,
            "change_type": "MODIFY",
            "diff": "@@ -424,7 +424,7 @@ def test_division_int(self):\n     def test_division_int_boundary(self, dtype, ex_val):\n         fo = np.iinfo(dtype)\n         neg = -1 if fo.min < 0 else 1\n-        # Large enough to test SIMD loops and remaind elements\n+        # Large enough to test SIMD loops and remainder elements\n         lsize = 512 + 7\n         a, b, divisors = eval(ex_val)\n         a_lst, b_lst = a.tolist(), b.tolist()\n",
            "comment_added_diff": {
                "427": "        # Large enough to test SIMD loops and remainder elements"
            },
            "comment_deleted_diff": {
                "427": "        # Large enough to test SIMD loops and remaind elements"
            },
            "comment_modified_diff": {
                "427": "        # Large enough to test SIMD loops and remaind elements"
            }
        },
        {
            "commit": "ec8d5db302c0e8597feb058f58863d5e9a6554c1",
            "timestamp": "2023-05-04T16:33:27+02:00",
            "author": "Sebastian Berg",
            "commit_message": "ENH: Make signed/unsigned integer comparisons exact\n\nThis makes comparisons between signed and unsigned integers exact\nby special-casing promotion in comparison to never promote integers\nto floats, but rather promote them to uint64 or int64 and use a\nspecific loop for that purpose.\n\nThis is a bit lazy, it doesn't make the scalar paths fast (they never were\nthough) nor does it try to vectorize the loop.\nThus, for cases that are not int64/uint64 already and require a cast in\neither case, it should be a bit slower.  OTOH, it was never really fast\nand the int64/uint64 mix is probably faster since it avoids casting.\n\n---\n\nNow... the reason I was looking into this was, that I had hoped\nit would help with NEP 50/weak scalar typing to allow:\n\n    uint64(1) < -1  # annoying that it fails with NEP 50\n\nbut, it doesn't actually, because if I use int64 for the -1 then very\nlarge numbers would be a problem...\nI could probably(?) add a *specific* \"Python integer\" ArrayMethod for comparisons\nand that could pick `object` dtype and thus get the original Python object\n(the loop could then in practice assume a scalar value).\n\n---\n\nIn either case, this works, and unless we worry about keeping the behavior\nwe probably might as well do this.\n(Potentially with follow-ups to speed it up.)",
            "additions": 58,
            "deletions": 0,
            "change_type": "MODIFY",
            "diff": "@@ -369,6 +369,64 @@ def test_object_nonbool_dtype_error(self):\n         with pytest.raises(TypeError, match=\"No loop matching\"):\n             np.equal(1, 1, sig=(None, None, \"l\"))\n \n+    @pytest.mark.parametrize(\"dtypes\", [\"qQ\", \"Qq\"])\n+    @pytest.mark.parametrize('py_comp, np_comp', [\n+        (operator.lt, np.less),\n+        (operator.le, np.less_equal),\n+        (operator.gt, np.greater),\n+        (operator.ge, np.greater_equal),\n+        (operator.eq, np.equal),\n+        (operator.ne, np.not_equal)\n+    ])\n+    @pytest.mark.parametrize(\"vals\", [(2**60, 2**60+1), (2**60+1, 2**60)])\n+    def test_large_integer_direct_comparison(\n+            self, dtypes, py_comp, np_comp, vals):\n+        # Note that float(2**60) + 1 == float(2**60).\n+        a1 = np.array([2**60], dtype=dtypes[0])\n+        a2 = np.array([2**60 + 1], dtype=dtypes[1])\n+        expected = py_comp(2**60, 2**60+1)\n+\n+        assert py_comp(a1, a2) == expected\n+        assert np_comp(a1, a2) == expected\n+        # Also check the scalars:\n+        s1 = a1[0]\n+        s2 = a2[0]\n+        assert isinstance(s1, np.integer)\n+        assert isinstance(s2, np.integer)\n+        # The Python operator here is mainly interesting:\n+        assert py_comp(s1, s2) == expected\n+        assert np_comp(s1, s2) == expected\n+\n+    @pytest.mark.parametrize(\"dtype\", np.typecodes['UnsignedInteger'])\n+    @pytest.mark.parametrize('py_comp_func, np_comp_func', [\n+        (operator.lt, np.less),\n+        (operator.le, np.less_equal),\n+        (operator.gt, np.greater),\n+        (operator.ge, np.greater_equal),\n+        (operator.eq, np.equal),\n+        (operator.ne, np.not_equal)\n+    ])\n+    @pytest.mark.parametrize(\"flip\", [True, False])\n+    def test_unsigned_signed_direct_comparison(\n+            self, dtype, py_comp_func, np_comp_func, flip):\n+        if flip:\n+            py_comp = lambda x, y: py_comp_func(y, x)\n+            np_comp = lambda x, y: np_comp_func(y, x)\n+        else:\n+            py_comp = py_comp_func\n+            np_comp = np_comp_func\n+\n+        arr = np.array([np.iinfo(dtype).max], dtype=dtype)\n+        expected = py_comp(int(arr[0]), -1)\n+\n+        assert py_comp(arr, -1) == expected\n+        assert np_comp(arr, -1) == expected\n+        scalar = arr[0]\n+        assert isinstance(scalar, np.integer)\n+        # The Python operator here is mainly interesting:\n+        assert py_comp(scalar, -1) == expected\n+        assert np_comp(scalar, -1) == expected\n+\n \n class TestAdd:\n     def test_reduce_alignment(self):\n",
            "comment_added_diff": {
                "384": "        # Note that float(2**60) + 1 == float(2**60).",
                "391": "        # Also check the scalars:",
                "396": "        # The Python operator here is mainly interesting:",
                "426": "        # The Python operator here is mainly interesting:"
            },
            "comment_deleted_diff": {},
            "comment_modified_diff": {}
        },
        {
            "commit": "aabacc722d469f7152812ae94a87810da012c80a",
            "timestamp": "2023-06-18T11:46:58+02:00",
            "author": "mattip",
            "commit_message": "fix PKG_CONFIG_PATH, force vsenv on windows builds, fix glibc version",
            "additions": 2,
            "deletions": 1,
            "change_type": "MODIFY",
            "diff": "@@ -4161,7 +4161,8 @@ def test_against_cmath(self):\n                 assert_(abs(a - b) < atol, \"%s %s: %s; cmath: %s\" % (fname, p, a, b))\n \n     @pytest.mark.xfail(\n-        _glibc_older_than(\"2.17\"),\n+        # manylinux2014 uses glibc2.17\n+        _glibc_older_than(\"2.18\"),\n         reason=\"Older glibc versions are imprecise (maybe passes with SIMD?)\"\n     )\n     @pytest.mark.xfail(IS_MUSL, reason=\"gh23049\")\n",
            "comment_added_diff": {
                "4164": "        # manylinux2014 uses glibc2.17"
            },
            "comment_deleted_diff": {},
            "comment_modified_diff": {
                "4164": "        _glibc_older_than(\"2.17\"),"
            }
        },
        {
            "commit": "0538f9ec6c1ad38f26a2f51b9c332982f4e34005",
            "timestamp": "2023-06-23T19:17:30+02:00",
            "author": "Sebastian Berg",
            "commit_message": "BUG: Ensure __array_ufunc__ works without any kwargs passed\n\nThis doesn't happen in practice unless the function is manually\ncalled, and even then probably never because users should be\npassing `**kwargs` ensuring there is a dict passed...",
            "additions": 13,
            "deletions": 0,
            "change_type": "MODIFY",
            "diff": "@@ -3916,6 +3916,19 @@ def __array_ufunc__(self, ufunc, method, *inputs, **kwargs):\n         assert_equal(a, check)\n         assert_(a.info, {'inputs': [0, 2]})\n \n+    def test_array_ufunc_direct_call(self):\n+        # This is mainly a regression test for gh-24023 (shouldn't segfault)\n+        a = np.array(1)\n+        with pytest.raises(TypeError):\n+            a.__array_ufunc__()\n+\n+        # No kwargs means kwargs may be NULL on the C-level\n+        with pytest.raises(TypeError):\n+            a.__array_ufunc__(1, 2)\n+\n+        # And the same with a valid call:\n+        res = a.__array_ufunc__(np.add, \"__call__\", a, a)\n+        assert_array_equal(res, a + a)\n \n class TestChoose:\n     def test_mixed(self):\n",
            "comment_added_diff": {
                "3920": "        # This is mainly a regression test for gh-24023 (shouldn't segfault)",
                "3925": "        # No kwargs means kwargs may be NULL on the C-level",
                "3929": "        # And the same with a valid call:"
            },
            "comment_deleted_diff": {},
            "comment_modified_diff": {}
        },
        {
            "commit": "7b1083fa0d786d8b7b54741bdb8136c2a45c910d",
            "timestamp": "2023-07-28T16:04:31+02:00",
            "author": "Ralf Gommers",
            "commit_message": "TST: skip FP exceptions test on 32-bit Windows",
            "additions": 4,
            "deletions": 0,
            "change_type": "MODIFY",
            "diff": "@@ -1764,6 +1764,8 @@ def test_expm1(self):\n         np.log, np.log2, np.log10, np.reciprocal, np.arccosh\n     ]\n \n+    @pytest.mark.skipif(sys.platform == \"win32\" and sys.maxsize < 2**31 + 1,\n+                        reason='failures on 32-bit Python, see FIXME below')\n     @pytest.mark.parametrize(\"ufunc\", UFUNCS_UNARY_FP)\n     @pytest.mark.parametrize(\"dtype\", ('e', 'f', 'd'))\n     @pytest.mark.parametrize(\"data, escape\", (\n@@ -1810,6 +1812,8 @@ def test_unary_spurious_fpexception(self, ufunc, dtype, data, escape):\n         # FIXME: NAN raises FP invalid exception:\n         #  - ceil/float16 on MSVC:32-bit\n         #  - spacing/float16 on almost all platforms\n+        # FIXME: skipped on MSVC:32-bit during switch to Meson, 10 cases fail\n+        #        when SIMD support not present / disabled\n         if ufunc in (np.spacing, np.ceil) and dtype == 'e':\n             return\n         array = np.array(data, dtype=dtype)\n",
            "comment_added_diff": {
                "1815": "        # FIXME: skipped on MSVC:32-bit during switch to Meson, 10 cases fail",
                "1816": "        #        when SIMD support not present / disabled"
            },
            "comment_deleted_diff": {},
            "comment_modified_diff": {}
        },
        {
            "commit": "451099ea77242552b62a568ba9a18511803174cd",
            "timestamp": "2023-08-11T22:03:46+02:00",
            "author": "Ralf Gommers",
            "commit_message": "TST: revert xfail in `test_umath.py`\n\nThis xfail was added in gh-24279 for 32-bit Python + MSVC when switching\nto Meson and having temporarily no SIMD support. That is back now, so\nthis test passes again.\n\n[skip circle] [skip cirrus] [skip travis]",
            "additions": 0,
            "deletions": 4,
            "change_type": "MODIFY",
            "diff": "@@ -1764,8 +1764,6 @@ def test_expm1(self):\n         np.log, np.log2, np.log10, np.reciprocal, np.arccosh\n     ]\n \n-    @pytest.mark.skipif(sys.platform == \"win32\" and sys.maxsize < 2**31 + 1,\n-                        reason='failures on 32-bit Python, see FIXME below')\n     @pytest.mark.parametrize(\"ufunc\", UFUNCS_UNARY_FP)\n     @pytest.mark.parametrize(\"dtype\", ('e', 'f', 'd'))\n     @pytest.mark.parametrize(\"data, escape\", (\n@@ -1812,8 +1810,6 @@ def test_unary_spurious_fpexception(self, ufunc, dtype, data, escape):\n         # FIXME: NAN raises FP invalid exception:\n         #  - ceil/float16 on MSVC:32-bit\n         #  - spacing/float16 on almost all platforms\n-        # FIXME: skipped on MSVC:32-bit during switch to Meson, 10 cases fail\n-        #        when SIMD support not present / disabled\n         if ufunc in (np.spacing, np.ceil) and dtype == 'e':\n             return\n         array = np.array(data, dtype=dtype)\n",
            "comment_added_diff": {},
            "comment_deleted_diff": {
                "1815": "        # FIXME: skipped on MSVC:32-bit during switch to Meson, 10 cases fail",
                "1816": "        #        when SIMD support not present / disabled"
            },
            "comment_modified_diff": {}
        },
        {
            "commit": "250e1479ce342d9d7ab8a592508f6ce892d4c98b",
            "timestamp": "2023-08-12T22:01:16+02:00",
            "author": "Mateusz Sok\u00f3\u0142",
            "commit_message": "API: Cleaning `numpy/__init__.py` and main namespace - Part 2 [NEP 52] (#24357)\n\n[skip ci]",
            "additions": 57,
            "deletions": 18,
            "change_type": "MODIFY",
            "diff": "@@ -17,7 +17,8 @@\n     assert_, assert_equal, assert_raises, assert_raises_regex,\n     assert_array_equal, assert_almost_equal, assert_array_almost_equal,\n     assert_array_max_ulp, assert_allclose, assert_no_warnings, suppress_warnings,\n-    _gen_alignment_data, assert_array_almost_equal_nulp, IS_WASM, IS_MUSL\n+    _gen_alignment_data, assert_array_almost_equal_nulp, IS_WASM, IS_MUSL, \n+    IS_PYPY\n     )\n from numpy.testing._private.utils import _glibc_older_than\n \n@@ -2152,38 +2153,38 @@ def test_one_one(self):\n \n     def test_zero_nzero(self):\n         # atan2(+-0, -0) returns +-pi.\n-        assert_almost_equal(ncu.arctan2(np.PZERO, np.NZERO), np.pi)\n-        assert_almost_equal(ncu.arctan2(np.NZERO, np.NZERO), -np.pi)\n+        assert_almost_equal(ncu.arctan2(ncu.PZERO, ncu.NZERO), np.pi)\n+        assert_almost_equal(ncu.arctan2(ncu.NZERO, ncu.NZERO), -np.pi)\n \n     def test_zero_pzero(self):\n         # atan2(+-0, +0) returns +-0.\n-        assert_arctan2_ispzero(np.PZERO, np.PZERO)\n-        assert_arctan2_isnzero(np.NZERO, np.PZERO)\n+        assert_arctan2_ispzero(ncu.PZERO, ncu.PZERO)\n+        assert_arctan2_isnzero(ncu.NZERO, ncu.PZERO)\n \n     def test_zero_negative(self):\n         # atan2(+-0, x) returns +-pi for x < 0.\n-        assert_almost_equal(ncu.arctan2(np.PZERO, -1), np.pi)\n-        assert_almost_equal(ncu.arctan2(np.NZERO, -1), -np.pi)\n+        assert_almost_equal(ncu.arctan2(ncu.PZERO, -1), np.pi)\n+        assert_almost_equal(ncu.arctan2(ncu.NZERO, -1), -np.pi)\n \n     def test_zero_positive(self):\n         # atan2(+-0, x) returns +-0 for x > 0.\n-        assert_arctan2_ispzero(np.PZERO, 1)\n-        assert_arctan2_isnzero(np.NZERO, 1)\n+        assert_arctan2_ispzero(ncu.PZERO, 1)\n+        assert_arctan2_isnzero(ncu.NZERO, 1)\n \n     def test_positive_zero(self):\n         # atan2(y, +-0) returns +pi/2 for y > 0.\n-        assert_almost_equal(ncu.arctan2(1, np.PZERO), 0.5 * np.pi)\n-        assert_almost_equal(ncu.arctan2(1, np.NZERO), 0.5 * np.pi)\n+        assert_almost_equal(ncu.arctan2(1, ncu.PZERO), 0.5 * np.pi)\n+        assert_almost_equal(ncu.arctan2(1, ncu.NZERO), 0.5 * np.pi)\n \n     def test_negative_zero(self):\n         # atan2(y, +-0) returns -pi/2 for y < 0.\n-        assert_almost_equal(ncu.arctan2(-1, np.PZERO), -0.5 * np.pi)\n-        assert_almost_equal(ncu.arctan2(-1, np.NZERO), -0.5 * np.pi)\n+        assert_almost_equal(ncu.arctan2(-1, ncu.PZERO), -0.5 * np.pi)\n+        assert_almost_equal(ncu.arctan2(-1, ncu.NZERO), -0.5 * np.pi)\n \n     def test_any_ninf(self):\n         # atan2(+-y, -infinity) returns +-pi for finite y > 0.\n-        assert_almost_equal(ncu.arctan2(1, np.NINF),  np.pi)\n-        assert_almost_equal(ncu.arctan2(-1, np.NINF), -np.pi)\n+        assert_almost_equal(ncu.arctan2(1, -np.inf),  np.pi)\n+        assert_almost_equal(ncu.arctan2(-1, -np.inf), -np.pi)\n \n     def test_any_pinf(self):\n         # atan2(+-y, +infinity) returns +-0 for finite y > 0.\n@@ -4390,14 +4391,14 @@ def _check_branch_cut(f, x0, dx, re_sign=1, im_sign=-1, sig_zero_ok=False,\n         ji = (x0.imag == 0) & (dx.imag != 0)\n         if np.any(jr):\n             x = x0[jr]\n-            x.real = np.NZERO\n+            x.real = ncu.NZERO\n             ym = f(x)\n             assert_(np.all(np.absolute(y0[jr].real - ym.real*re_sign) < atol), (y0[jr], ym))\n             assert_(np.all(np.absolute(y0[jr].imag - ym.imag*im_sign) < atol), (y0[jr], ym))\n \n         if np.any(ji):\n             x = x0[ji]\n-            x.imag = np.NZERO\n+            x.imag = ncu.NZERO\n             ym = f(x)\n             assert_(np.all(np.absolute(y0[ji].real - ym.real*re_sign) < atol), (y0[ji], ym))\n             assert_(np.all(np.absolute(y0[ji].imag - ym.imag*im_sign) < atol), (y0[ji], ym))\n@@ -4542,7 +4543,7 @@ def test_reduceat():\n     # test no buffer\n     np.setbufsize(32)\n     h1 = np.add.reduceat(a['value'], indx)\n-    np.setbufsize(np.UFUNC_BUFSIZE_DEFAULT)\n+    np.setbufsize(ncu.UFUNC_BUFSIZE_DEFAULT)\n     assert_array_almost_equal(h1, h2)\n \n def test_reduceat_empty():\n@@ -4720,3 +4721,41 @@ def test_bad_legacy_gufunc_silent_errors(x1):\n     # The signature of always_error_gufunc is '(i),()->()'.\n     with pytest.raises(RuntimeError, match=r\"How unexpected :\\)!\"):\n         ncu_tests.always_error_gufunc(x1, 0.0)\n+\n+\n+class TestAddDocstring:\n+    @pytest.mark.skipif(sys.flags.optimize == 2, reason=\"Python running -OO\")\n+    @pytest.mark.skipif(IS_PYPY, reason=\"PyPy does not modify tp_doc\")\n+    def test_add_same_docstring(self):\n+        # test for attributes (which are C-level defined)\n+        ncu.add_docstring(np.ndarray.flat, np.ndarray.flat.__doc__)\n+\n+        # And typical functions:\n+        def func():\n+            \"\"\"docstring\"\"\"\n+            return\n+\n+        ncu.add_docstring(func, func.__doc__)\n+\n+    @pytest.mark.skipif(sys.flags.optimize == 2, reason=\"Python running -OO\")\n+    def test_different_docstring_fails(self):\n+        # test for attributes (which are C-level defined)\n+        with assert_raises(RuntimeError):\n+            ncu.add_docstring(np.ndarray.flat, \"different docstring\")\n+            \n+        # And typical functions:\n+        def func():\n+            \"\"\"docstring\"\"\"\n+            return\n+\n+        with assert_raises(RuntimeError):\n+            ncu.add_docstring(func, \"different docstring\")\n+\n+\n+class TestAdd_newdoc_ufunc:\n+    def test_ufunc_arg(self):\n+        assert_raises(TypeError, ncu._add_newdoc_ufunc, 2, \"blah\")\n+        assert_raises(ValueError, ncu._add_newdoc_ufunc, np.add, \"blah\")\n+\n+    def test_string_arg(self):\n+        assert_raises(TypeError, ncu._add_newdoc_ufunc, np.add, 3)\n",
            "comment_added_diff": {
                "4730": "        # test for attributes (which are C-level defined)",
                "4733": "        # And typical functions:",
                "4742": "        # test for attributes (which are C-level defined)",
                "4746": "        # And typical functions:"
            },
            "comment_deleted_diff": {},
            "comment_modified_diff": {}
        },
        {
            "commit": "350b27c35de9a91e0f497adcc39e2d2774a23142",
            "timestamp": "2023-09-27T23:28:53+00:00",
            "author": "Ganesh Kathiresan",
            "commit_message": "ENH: Added countbits (popcount)\n\nENH, DOC: Added countbits (popcount)\n\nENH: Popcount implementation\n\nENH: Add popcount to umath\n\nENH: Added countbits (popcount) to umath `__all__`\n\nENH: Refined popcount logic\n\nDOC: Added `bit_count`\n\nCo-authored-by: Eric Wieser <wieser.eric@gmail.com>\n\nMAINT: Renamed `countbits` to `bit_count`\n\nMAINT: Fixed 4 1s magic number\n\nDOC: Added `popcount` to docstring\n\nENH: Added bit_count annotations\n\nENH: Added GNU/CLANG popcount\n\nDOC: Added `popcount` language example\n\nENH, BUG: Moved `bitcount` to npy_math.h as `popcount` | Fixed final right shift\n\nENH: Enable `popcount` for signed\n\nTST: Tests for `bit_count`\n\nBUG, DOC: (BUG) Added missing typecast causing an unwanted upcast\n          (DOC) Added more details on `popcount` implementation\n\nMAINT, BUG: (MAINT) Refined `popcount` TC to use typecode\n            (BUG) Fixed ufunc.ntypes to include signed ints\n\nENH: Added windows builtin support\n\nENH: Added `popcount` implementation for big python ints natively\n[1/2] `popcount` object loop changes\n\nENH: Object loop for `bit_count`\n[2/2] `popcount` object loop changes\n\nTST: Refined `bit_count` tests and added object type\n\nENH: Added `bit_count` to `np.int*`\n\nDOC: Added `np.bit_count` (#19355)\n\nMAINT: Various linting and minor fixes:\n1. Fixed passing all args to _internals umath bitcount.\n   Note: We use kwargs here that might hinder performance\n2. Fixed linting errors.\n3. Improved verbosity of logs\n4. Made a generic TO_BITS_LEN macro to accomdate more length based\n   functions in future\n\nBENCH: Added bit_count (popcount)\n\nMAINT: Style nits | Added signed case\n\nDOC, MAINT: Improved example\n\nENH: Added annotations for bit_count\n\nTST: Added annotations tests for bit_count\n\nMAINT: Fixed linting errors\n\nMAINT: Moved Magic constants to npy_math_internal\n\nMAINT: Remove python implementation | Added 3.10 check to tests\n\nDOC: Added abs value usage to doc\n\nMAINT: Resolved merge conflicts",
            "additions": 33,
            "deletions": 1,
            "change_type": "MODIFY",
            "diff": "@@ -2610,7 +2610,15 @@ def test_reduce(self):\n \n class TestBitwiseUFuncs:\n \n-    bitwise_types = [np.dtype(c) for c in '?' + 'bBhHiIlLqQ' + 'O']\n+    _all_ints_bits = [\n+        np.dtype(c).itemsize * 8 for c in np.typecodes[\"AllInteger\"]]\n+    bitwise_types = [\n+        np.dtype(c) for c in '?' + np.typecodes[\"AllInteger\"] + 'O']\n+    bitwise_bits = [\n+        2,  # boolean type\n+        *_all_ints_bits,  # All integers\n+        max(_all_ints_bits) + 1,  # Object_ type\n+    ]\n \n     def test_values(self):\n         for dt in self.bitwise_types:\n@@ -2691,6 +2699,30 @@ def test_reduction(self):\n             btype = np.array([True], dtype=object)\n             assert_(type(f.reduce(btype)) is bool, msg)\n \n+    @pytest.mark.parametrize(\"input_dtype_obj, bitsize\",\n+            zip(bitwise_types, bitwise_bits))\n+    def test_popcount(self, input_dtype_obj, bitsize):\n+        input_dtype = input_dtype_obj.type\n+\n+        # bit_count is only in-built in 3.10+\n+        if sys.version_info < (3, 10) and input_dtype == np.object_:\n+            pytest.skip()\n+\n+        for i in range(1, bitsize):\n+            num = 2**i - 1\n+            msg = f\"bit_count for {num}\"\n+            assert i == np.bit_count(input_dtype(num)), msg\n+            if np.issubdtype(\n+                input_dtype, np.signedinteger) or input_dtype == np.object_:\n+                assert i == np.bit_count(input_dtype(-num)), msg\n+\n+        a = np.array([2**i-1 for i in range(1, bitsize)], dtype=input_dtype)\n+        bit_count_a = np.bit_count(a)\n+        expected = np.arange(1, bitsize, dtype=input_dtype)\n+\n+        msg = f\"array bit_count for {input_dtype}\"\n+        assert all(bit_count_a == expected), msg\n+\n \n class TestInt:\n     def test_logical_not(self):\n",
            "comment_added_diff": {
                "2618": "        2,  # boolean type",
                "2619": "        *_all_ints_bits,  # All integers",
                "2620": "        max(_all_ints_bits) + 1,  # Object_ type",
                "2707": "        # bit_count is only in-built in 3.10+"
            },
            "comment_deleted_diff": {},
            "comment_modified_diff": {}
        },
        {
            "commit": "5d299c4df803dff236237cee6b291ac09e2731be",
            "timestamp": "2023-09-27T23:30:15+00:00",
            "author": "ganesh-k13",
            "commit_message": "MAINT: Change `bit_count` UFunc to `bitwise_count`",
            "additions": 7,
            "deletions": 7,
            "change_type": "MODIFY",
            "diff": "@@ -2704,24 +2704,24 @@ def test_reduction(self):\n     def test_popcount(self, input_dtype_obj, bitsize):\n         input_dtype = input_dtype_obj.type\n \n-        # bit_count is only in-built in 3.10+\n+        # bitwise_count is only in-built in 3.10+\n         if sys.version_info < (3, 10) and input_dtype == np.object_:\n             pytest.skip()\n \n         for i in range(1, bitsize):\n             num = 2**i - 1\n-            msg = f\"bit_count for {num}\"\n-            assert i == np.bit_count(input_dtype(num)), msg\n+            msg = f\"bitwise_count for {num}\"\n+            assert i == np.bitwise_count(input_dtype(num)), msg\n             if np.issubdtype(\n                 input_dtype, np.signedinteger) or input_dtype == np.object_:\n-                assert i == np.bit_count(input_dtype(-num)), msg\n+                assert i == np.bitwise_count(input_dtype(-num)), msg\n \n         a = np.array([2**i-1 for i in range(1, bitsize)], dtype=input_dtype)\n-        bit_count_a = np.bit_count(a)\n+        bitwise_count_a = np.bitwise_count(a)\n         expected = np.arange(1, bitsize, dtype=input_dtype)\n \n-        msg = f\"array bit_count for {input_dtype}\"\n-        assert all(bit_count_a == expected), msg\n+        msg = f\"array bitwise_count for {input_dtype}\"\n+        assert all(bitwise_count_a == expected), msg\n \n \n class TestInt:\n",
            "comment_added_diff": {
                "2707": "        # bitwise_count is only in-built in 3.10+"
            },
            "comment_deleted_diff": {
                "2707": "        # bit_count is only in-built in 3.10+"
            },
            "comment_modified_diff": {
                "2707": "        # bit_count is only in-built in 3.10+"
            }
        }
    ],
    "test_umath_complex.py": [
        {
            "commit": "d41572ca41c1c8d247ef0eb1bb6e47ee0e4f8314",
            "timestamp": "2023-09-22T23:14:36+02:00",
            "author": "Dimitri Papadopoulos",
            "commit_message": "MAINT, DOC: fix typos found by codespell",
            "additions": 1,
            "deletions": 1,
            "change_type": "MODIFY",
            "diff": "@@ -16,7 +16,7 @@\n # At least on Windows the results of many complex functions are not conforming\n # to the C99 standard. See ticket 1574.\n # Ditto for Solaris (ticket 1642) and OS X on PowerPC.\n-#FIXME: this will probably change when we require full C99 campatibility\n+#FIXME: this will probably change when we require full C99 compatibility\n with np.errstate(all='ignore'):\n     functions_seem_flaky = ((np.exp(complex(np.inf, 0)).imag != 0)\n                             or (np.log(complex(ncu.NZERO, 0)).imag != np.pi))\n",
            "comment_added_diff": {
                "19": "#FIXME: this will probably change when we require full C99 compatibility"
            },
            "comment_deleted_diff": {
                "19": "#FIXME: this will probably change when we require full C99 campatibility"
            },
            "comment_modified_diff": {
                "19": "#FIXME: this will probably change when we require full C99 campatibility"
            }
        }
    ],
    "test_ccompiler_opt_conf.py": [],
    "test_exec_command.py": [],
    "test_system_info.py": [
        {
            "commit": "aca242927f7222d3fa39d4910d86fb189bf76b12",
            "timestamp": "2023-08-07T12:13:50+02:00",
            "author": "Ralf Gommers",
            "commit_message": "TST: fix distutils tests for deprecations in recent setuptools versions\n\nCloses gh-24350\n\n[skip circle] [skip travis]",
            "additions": 11,
            "deletions": 0,
            "change_type": "MODIFY",
            "diff": "@@ -3,6 +3,7 @@\n import pytest\n from tempfile import mkstemp, mkdtemp\n from subprocess import Popen, PIPE\n+import importlib.metadata\n from distutils.errors import DistutilsError\n \n from numpy.testing import assert_, assert_equal, assert_raises\n@@ -13,6 +14,16 @@\n from numpy.distutils import _shell_utils\n \n \n+try:\n+    if importlib.metadata.version('setuptools') >= '60':\n+        # pkg-resources gives deprecation warnings, and there may be more issues.\n+        # we only support setuptools <60\n+        pytest.skip(\"setuptools is too new\", allow_module_level=True)\n+except importlib.metadata.PackageNotFoundError:\n+    # we don't require `setuptools`; if it is not found, continue\n+    pass\n+\n+\n def get_class(name, notfound_action=1):\n     \"\"\"\n     notfound_action:\n",
            "comment_added_diff": {
                "19": "        # pkg-resources gives deprecation warnings, and there may be more issues.",
                "20": "        # we only support setuptools <60",
                "23": "    # we don't require `setuptools`; if it is not found, continue"
            },
            "comment_deleted_diff": {},
            "comment_modified_diff": {}
        },
        {
            "commit": "240f389a4f5799e31914b206e570ec70f052a706",
            "timestamp": "2023-08-07T08:32:33-06:00",
            "author": "Charles Harris",
            "commit_message": "MAINT: Fix long line",
            "additions": 2,
            "deletions": 2,
            "change_type": "MODIFY",
            "diff": "@@ -16,8 +16,8 @@\n \n try:\n     if importlib.metadata.version('setuptools') >= '60':\n-        # pkg-resources gives deprecation warnings, and there may be more issues.\n-        # we only support setuptools <60\n+        # pkg-resources gives deprecation warnings, and there may be more\n+        # issues. We only support setuptools <60\n         pytest.skip(\"setuptools is too new\", allow_module_level=True)\n except importlib.metadata.PackageNotFoundError:\n     # we don't require `setuptools`; if it is not found, continue\n",
            "comment_added_diff": {
                "19": "        # pkg-resources gives deprecation warnings, and there may be more",
                "20": "        # issues. We only support setuptools <60"
            },
            "comment_deleted_diff": {
                "19": "        # pkg-resources gives deprecation warnings, and there may be more issues.",
                "20": "        # we only support setuptools <60"
            },
            "comment_modified_diff": {
                "19": "        # pkg-resources gives deprecation warnings, and there may be more issues.",
                "20": "        # we only support setuptools <60"
            }
        }
    ],
    "test_assumed_shape.py": [],
    "test__datasource.py": [],
    "test_histograms.py": [],
    "test_extras.py": [
        {
            "commit": "91432a36a3611c2374ea9e2d45592f0ac5e71adb",
            "timestamp": "2022-12-05T15:53:32-07:00",
            "author": "Roy Smart",
            "commit_message": "BUG: `keepdims=True` is ignored if `out` is not `None` in `numpy.median()`, `numpy.percentile()`, and `numpy.quantile()`.\n\nCloses #22714, #22544.",
            "additions": 29,
            "deletions": 0,
            "change_type": "MODIFY",
            "diff": "@@ -12,6 +12,7 @@\n import pytest\n \n import numpy as np\n+from numpy.core.numeric import normalize_axis_tuple\n from numpy.testing import (\n     assert_warns, suppress_warnings\n     )\n@@ -989,6 +990,34 @@ def test_out(self):\n             assert_(r is out)\n             assert_(type(r) is MaskedArray)\n \n+    @pytest.mark.parametrize(\n+        argnames='axis',\n+        argvalues=[\n+            None,\n+            1,\n+            (1, ),\n+            (0, 1),\n+            (-3, -1),\n+        ]\n+    )\n+    def test_keepdims_out(self, axis):\n+        mask = np.zeros((3, 5, 7, 11), dtype=bool)\n+        # Randomly set some elements to True:\n+        w = np.random.random((4, 200)) * np.array(mask.shape)[:, None]\n+        w = w.astype(np.intp)\n+        mask[tuple(w)] = np.nan\n+        d = masked_array(np.ones(mask.shape), mask=mask)\n+        if axis is None:\n+            shape_out = (1,) * d.ndim\n+        else:\n+            axis_norm = normalize_axis_tuple(axis, d.ndim)\n+            shape_out = tuple(\n+                1 if i in axis_norm else d.shape[i] for i in range(d.ndim))\n+        out = masked_array(np.empty(shape_out))\n+        result = median(d, axis=axis, keepdims=True, out=out)\n+        assert result is out\n+        assert_equal(result.shape, shape_out)\n+\n     def test_single_non_masked_value_on_axis(self):\n         data = [[1., 0.],\n                 [0., 3.],\n",
            "comment_added_diff": {
                "1005": "        # Randomly set some elements to True:"
            },
            "comment_deleted_diff": {},
            "comment_modified_diff": {}
        },
        {
            "commit": "03edb7b8dddb12198509d2bf602ea8b382d27199",
            "timestamp": "2023-03-03T03:07:17+00:00",
            "author": "yuki",
            "commit_message": "add support for non-2d arrays",
            "additions": 29,
            "deletions": 0,
            "change_type": "MODIFY",
            "diff": "@@ -730,6 +730,35 @@ def test_dot(self):\n         assert_equal(c.mask, [[0, 0, 1], [1, 1, 1], [0, 0, 1]])\n         c = dot(b, a, strict=False)\n         assert_equal(c, np.dot(b.filled(0), a.filled(0)))\n+        #\n+        a = masked_array(np.arange(8), mask=[1, 0, 0, 0, 0, 0, 0, 0]).reshape(2, 2, 2)\n+        b = masked_array(np.arange(8), mask=[0, 0, 0, 0, 0, 0, 0, 1]).reshape(2, 2, 2)\n+        c = dot(a, b, strict=True)\n+        assert_equal(c.mask, [[[[1, 1], [1, 1]],[[0, 0], [0, 1]]],[[[0, 0], [0, 1]],[[0, 0], [0, 1]]]])\n+        c = dot(a, b, strict=False)\n+        assert_equal(c.mask, [[[[0, 0], [0, 1]],[[0, 0], [0, 0]]],[[[0, 0], [0, 0]],[[0, 0], [0, 0]]]])\n+        c = dot(b, a, strict=True)\n+        assert_equal(c.mask, [[[[1, 0], [0, 0]],[[1, 0], [0, 0]]],[[[1, 0], [0, 0]],[[1, 1], [1, 1]]]])\n+        c = dot(b, a, strict=False)\n+        assert_equal(c.mask, [[[[0, 0], [0, 0]],[[0, 0], [0, 0]]],[[[0, 0], [0, 0]],[[1, 0], [0, 0]]]])\n+        #\n+        a = masked_array(np.arange(8), mask=[1, 0, 0, 0, 0, 0, 0, 0]).reshape(2, 2, 2)\n+        b = 5.\n+        c = dot(a, b, strict=True)\n+        assert_equal(c.mask, [[[1, 0], [0, 0]],[[0, 0], [0, 0]]])\n+        c = dot(a, b, strict=False)\n+        assert_equal(c.mask, [[[1, 0], [0, 0]],[[0, 0], [0, 0]]])\n+        c = dot(b, a, strict=True)\n+        assert_equal(c.mask, [[[1, 0], [0, 0]],[[0, 0], [0, 0]]])\n+        c = dot(b, a, strict=False)\n+        assert_equal(c.mask, [[[1, 0], [0, 0]],[[0, 0], [0, 0]]])\n+        #\n+        a = masked_array(np.arange(8), mask=[1, 0, 0, 0, 0, 0, 0, 0]).reshape(2, 2, 2)\n+        b = masked_array(np.arange(2), mask=[0, 1])\n+        c = dot(a, b, strict=True)\n+        assert_equal(c.mask, [[1, 1], [1, 1]])\n+        c = dot(a, b, strict=False)\n+        assert_equal(c.mask, [[1, 0], [0, 0]])\n \n     def test_dot_returns_maskedarray(self):\n         # See gh-6611\n",
            "comment_added_diff": {
                "733": "        #",
                "744": "        #",
                "755": "        #"
            },
            "comment_deleted_diff": {},
            "comment_modified_diff": {}
        }
    ],
    "test_mrecords.py": [],
    "test_old_ma.py": [
        {
            "commit": "65bb8ddd03763b8a931de2cd472d8ce8bd2f9fbd",
            "timestamp": "2023-09-11T18:51:42+02:00",
            "author": "Mateusz Sok\u00f3\u0142",
            "commit_message": "API: Remove ptp, setitem and newbyteorder from np.ndarray class",
            "additions": 5,
            "deletions": 3,
            "change_type": "MODIFY",
            "diff": "@@ -821,13 +821,15 @@ def test_clip(self):\n     def test_ptp(self):\n         (x, X, XX, m, mx, mX, mXX,) = self.d\n         (n, m) = X.shape\n-        assert_equal(mx.ptp(), mx.compressed().ptp())\n+        # print(type(mx), mx.compressed())\n+        # raise Exception()\n+        assert_equal(mx.ptp(), np.ptp(mx.compressed()))\n         rows = np.zeros(n, np.float64)\n         cols = np.zeros(m, np.float64)\n         for k in range(m):\n-            cols[k] = mX[:, k].compressed().ptp()\n+            cols[k] = np.ptp(mX[:, k].compressed())\n         for k in range(n):\n-            rows[k] = mX[k].compressed().ptp()\n+            rows[k] = np.ptp(mX[k].compressed())\n         assert_(eq(mX.ptp(0), cols))\n         assert_(eq(mX.ptp(1), rows))\n \n",
            "comment_added_diff": {
                "824": "        # print(type(mx), mx.compressed())",
                "825": "        # raise Exception()"
            },
            "comment_deleted_diff": {},
            "comment_modified_diff": {
                "824": "        assert_equal(mx.ptp(), mx.compressed().ptp())"
            }
        }
    ],
    "test_subclassing.py": [
        {
            "commit": "a38eb6d9bb5990b70e042c19f2d6daa51d784d6c",
            "timestamp": "2023-01-27T12:01:36-05:00",
            "author": "Mark Harfouche",
            "commit_message": "TST: Add a test for slots and NDArrayOperatorsMixin subclassing",
            "additions": 10,
            "deletions": 0,
            "change_type": "MODIFY",
            "diff": "@@ -154,6 +154,7 @@ class WrappedArray(NDArrayOperatorsMixin):\n     ufunc deferrals are commutative.\n     See: https://github.com/numpy/numpy/issues/15200)\n     \"\"\"\n+    __slots__ = ('_array', 'attrs')\n     __array_priority__ = 20\n \n     def __init__(self, array, **attrs):\n@@ -448,3 +449,12 @@ def test_masked_binary_operations(self):\n         assert_(isinstance(np.divide(wm, m2), WrappedArray))\n         assert_(isinstance(np.divide(m2, wm), WrappedArray))\n         assert_equal(np.divide(m2, wm), np.divide(wm, m2))\n+\n+    def test_mixins_have_slots(self):\n+        mixin = NDArrayOperatorsMixin()\n+        # Should raise an error\n+        assert_raises(AttributeError, mixin.__setattr__, \"not_a_real_attr\", 1)\n+\n+        m = np.ma.masked_array([1, 3, 5], mask=[False, True, False])\n+        wm = WrappedArray(m)\n+        assert_raises(AttributeError, wm.__setattr__, \"not_an_attr\", 2)\n",
            "comment_added_diff": {
                "455": "        # Should raise an error"
            },
            "comment_deleted_diff": {},
            "comment_modified_diff": {}
        }
    ],
    "test_masked_matrix.py": [],
    "test_random.py": [],
    "test_utils.py": [
        {
            "commit": "f75bb0edb0e6eec2564de4bf798242984860a19b",
            "timestamp": "2023-01-19T14:35:44-07:00",
            "author": "Charles Harris",
            "commit_message": "MAINT: Remove all nose testing support.\n\nNumPy switched to using pytest in 2018 and nose has been unmaintained\nfor many years. We have kept NumPy's nose support to avoid breaking\ndownstream projects who might have been using it and not yet switched to\npytest or some other testing framework. With the arrival of Python 3.12,\nunpatched nose will raise an error. It it time to move on.\n\nDecorators removed\n\n- raises\n- slow\n- setastest\n- skipif\n- knownfailif\n- deprecated\n- parametrize\n- _needs_refcount\n\nThese are not to be confused with pytest versions with similar names,\ne.g., pytest.mark.slow, pytest.mark.skipif, pytest.mark.parametrize.\n\nFunctions removed\n\n- Tester\n- import_nose\n- run_module_suite",
            "additions": 1,
            "deletions": 36,
            "change_type": "MODIFY",
            "diff": "@@ -8,7 +8,7 @@\n import numpy as np\n from numpy.testing import (\n     assert_equal, assert_array_equal, assert_almost_equal,\n-    assert_array_almost_equal, assert_array_less, build_err_msg, raises,\n+    assert_array_almost_equal, assert_array_less, build_err_msg,\n     assert_raises, assert_warns, assert_no_warnings, assert_allclose,\n     assert_approx_equal, assert_array_almost_equal_nulp, assert_array_max_ulp,\n     clear_and_catch_warnings, suppress_warnings, assert_string_equal, assert_,\n@@ -793,41 +793,6 @@ def test_inf_compare_array(self):\n         self._assert_func(-ainf, x)\n \n \n-@pytest.mark.skip(reason=\"The raises decorator depends on Nose\")\n-class TestRaises:\n-\n-    def setup_method(self):\n-        class MyException(Exception):\n-            pass\n-\n-        self.e = MyException\n-\n-    def raises_exception(self, e):\n-        raise e\n-\n-    def does_not_raise_exception(self):\n-        pass\n-\n-    def test_correct_catch(self):\n-        raises(self.e)(self.raises_exception)(self.e)  # raises?\n-\n-    def test_wrong_exception(self):\n-        try:\n-            raises(self.e)(self.raises_exception)(RuntimeError)  # raises?\n-        except RuntimeError:\n-            return\n-        else:\n-            raise AssertionError(\"should have caught RuntimeError\")\n-\n-    def test_catch_no_raise(self):\n-        try:\n-            raises(self.e)(self.does_not_raise_exception)()  # raises?\n-        except AssertionError:\n-            return\n-        else:\n-            raise AssertionError(\"should have raised an AssertionError\")\n-\n-\n class TestWarns:\n \n     def test_warn(self):\n",
            "comment_added_diff": {},
            "comment_deleted_diff": {
                "812": "        raises(self.e)(self.raises_exception)(self.e)  # raises?",
                "816": "            raises(self.e)(self.raises_exception)(RuntimeError)  # raises?",
                "824": "            raises(self.e)(self.does_not_raise_exception)()  # raises?"
            },
            "comment_modified_diff": {}
        },
        {
            "commit": "7698f9dbce84aca177787cb88979226c799e5c6c",
            "timestamp": "2023-04-27T13:25:11+02:00",
            "author": "ninousf",
            "commit_message": "ENH: rename dtype_without_metadata to drop_metadata, and do not use descr attribute",
            "additions": 38,
            "deletions": 0,
            "change_type": "MODIFY",
            "diff": "@@ -2,6 +2,7 @@\n import sys\n import pytest\n \n+import numpy as np\n from numpy.core import arange\n from numpy.testing import assert_, assert_equal, assert_raises_regex\n from numpy.lib import deprecate, deprecate_with_doc\n@@ -176,3 +177,40 @@ def _has_method_heading(cls):\n \n     assert _has_method_heading(WithPublicMethods)\n     assert not _has_method_heading(NoPublicMethods)\n+\n+def test__drop_metadata():\n+    def _compare_dtypes(dt1, dt2):\n+        return np.can_cast(dt1, dt2, casting='no')\n+\n+    # structured dtype\n+    dt = np.dtype([('l1', [('l2', np.dtype('S8', metadata={'msg': 'toto'}))])],\n+                  metadata={'msg': 'titi'})\n+    dt_m = utils.drop_metadata(dt)\n+    assert _compare_dtypes(dt, dt_m) is True\n+    assert dt_m.metadata is None\n+    assert dt_m['l1'].metadata is None\n+    assert dt_m['l1']['l2'].metadata is None\n+    \n+    # alignement\n+    dt = np.dtype([('x', '<f8'), ('y', '<i4')],\n+                  align=True,\n+                  metadata={'msg': 'toto'})\n+    dt_m = utils.drop_metadata(dt)\n+    assert _compare_dtypes(dt, dt_m) is True\n+    assert dt_m.metadata is None\n+\n+    # subdtype\n+    dt = np.dtype('8f',\n+                  metadata={'msg': 'toto'})\n+    dt_m = utils.drop_metadata(dt)\n+    assert _compare_dtypes(dt, dt_m) is True\n+    assert dt_m.metadata is None\n+\n+    # scalar\n+    dt = np.dtype('uint32',\n+                  metadata={'msg': 'toto'})\n+    dt_m = utils.drop_metadata(dt)\n+    assert _compare_dtypes(dt, dt_m) is True\n+    assert dt_m.metadata is None\n+\n+    \n",
            "comment_added_diff": {
                "185": "    # structured dtype",
                "194": "    # alignement",
                "202": "    # subdtype",
                "209": "    # scalar"
            },
            "comment_deleted_diff": {},
            "comment_modified_diff": {}
        },
        {
            "commit": "e2cbe5347523586d90845e8e907c7c80f388ec7e",
            "timestamp": "2023-04-27T17:56:26+02:00",
            "author": "Sebastian Berg",
            "commit_message": "TST: Improve tests a bit (and a bit of docs)",
            "additions": 15,
            "deletions": 1,
            "change_type": "MODIFY",
            "diff": "@@ -178,7 +178,8 @@ def _has_method_heading(cls):\n     assert _has_method_heading(WithPublicMethods)\n     assert not _has_method_heading(NoPublicMethods)\n \n-def test__drop_metadata():\n+\n+def test_drop_metadata():\n     def _compare_dtypes(dt1, dt2):\n         return np.can_cast(dt1, dt2, casting='no')\n \n@@ -212,3 +213,16 @@ def _compare_dtypes(dt1, dt2):\n     dt_m = utils.drop_metadata(dt)\n     assert _compare_dtypes(dt, dt_m) is True\n     assert dt_m.metadata is None\n+\n+\n+@pytest.mark.parametrize(\"dtype\",\n+        [np.dtype(\"i,i,i,i\")[[\"f1\", \"f3\"]],\n+        np.dtype(\"f8\"),\n+        np.dtype(\"10i\")])\n+def test_drop_metadata_identity_and_copy(dtype):\n+    # If there is no metadata, the identity is preserved:\n+    assert utils.drop_metadata(dtype) is dtype\n+\n+    # If there is any, it is dropped (subforms are checked above)\n+    dtype = np.dtype(dtype, metadata={1: 2})\n+    assert utils.drop_metadata(dtype).metadata is None\n",
            "comment_added_diff": {
                "223": "    # If there is no metadata, the identity is preserved:",
                "226": "    # If there is any, it is dropped (subforms are checked above)"
            },
            "comment_deleted_diff": {},
            "comment_modified_diff": {}
        },
        {
            "commit": "d68d7b62659a9e8a4c5c51a6d9705f9496733caa",
            "timestamp": "2023-05-28T12:47:52+02:00",
            "author": "Dimitri Papadopoulos",
            "commit_message": "MAINT: fix typos found by codespell",
            "additions": 1,
            "deletions": 1,
            "change_type": "MODIFY",
            "diff": "@@ -192,7 +192,7 @@ def _compare_dtypes(dt1, dt2):\n     assert dt_m['l1'].metadata is None\n     assert dt_m['l1']['l2'].metadata is None\n     \n-    # alignement\n+    # alignment\n     dt = np.dtype([('x', '<f8'), ('y', '<i4')],\n                   align=True,\n                   metadata={'msg': 'toto'})\n",
            "comment_added_diff": {
                "195": "    # alignment"
            },
            "comment_deleted_diff": {
                "195": "    # alignement"
            },
            "comment_modified_diff": {
                "195": "    # alignement"
            }
        },
        {
            "commit": "017cef318ad37ac05738830ebb3173c65c128759",
            "timestamp": "2023-07-11T22:25:10+02:00",
            "author": "Mateusz Sok\u00f3\u0142",
            "commit_message": "API: apply review comments",
            "additions": 0,
            "deletions": 99,
            "change_type": "MODIFY",
            "diff": "@@ -1,4 +1,3 @@\n-import inspect\n import sys\n import pytest\n \n@@ -6,7 +5,6 @@\n from numpy.core import arange\n from numpy.testing import assert_, assert_equal, assert_raises_regex\n import numpy.lib.utils as utils\n-from numpy._utils import deprecate, deprecate_with_doc\n \n from io import StringIO\n \n@@ -24,109 +22,12 @@ def test_lookfor():\n     assert_('numpy.linalg.eig' in out)\n \n \n-@deprecate\n-def old_func(self, x):\n-    return x\n-\n-\n-@deprecate(message=\"Rather use new_func2\")\n-def old_func2(self, x):\n-    return x\n-\n-\n-def old_func3(self, x):\n-    return x\n-new_func3 = deprecate(old_func3, old_name=\"old_func3\", new_name=\"new_func3\")\n-\n-\n-def old_func4(self, x):\n-    \"\"\"Summary.\n-\n-    Further info.\n-    \"\"\"\n-    return x\n-new_func4 = deprecate(old_func4)\n-\n-\n-def old_func5(self, x):\n-    \"\"\"Summary.\n-\n-        Bizarre indentation.\n-    \"\"\"\n-    return x\n-new_func5 = deprecate(old_func5, message=\"This function is\\ndeprecated.\")\n-\n-\n-def old_func6(self, x):\n-    \"\"\"\n-    Also in PEP-257.\n-    \"\"\"\n-    return x\n-new_func6 = deprecate(old_func6)\n-\n-\n-@deprecate_with_doc(msg=\"Rather use new_func7\")\n-def old_func7(self,x):\n-    return x\n-\n-\n-def test_deprecate_decorator():\n-    assert_('deprecated' in old_func.__doc__)\n-\n-\n-def test_deprecate_decorator_message():\n-    assert_('Rather use new_func2' in old_func2.__doc__)\n-\n-\n-def test_deprecate_fn():\n-    assert_('old_func3' in new_func3.__doc__)\n-    assert_('new_func3' in new_func3.__doc__)\n-\n-\n-def test_deprecate_with_doc_decorator_message():\n-    assert_('Rather use new_func7' in old_func7.__doc__)\n-\n-\n-@pytest.mark.skipif(sys.flags.optimize == 2, reason=\"-OO discards docstrings\")\n-@pytest.mark.parametrize('old_func, new_func', [\n-    (old_func4, new_func4),\n-    (old_func5, new_func5),\n-    (old_func6, new_func6),\n-])\n-def test_deprecate_help_indentation(old_func, new_func):\n-    _compare_docs(old_func, new_func)\n-    # Ensure we don't mess up the indentation\n-    for knd, func in (('old', old_func), ('new', new_func)):\n-        for li, line in enumerate(func.__doc__.split('\\n')):\n-            if li == 0:\n-                assert line.startswith('    ') or not line.startswith(' '), knd\n-            elif line:\n-                assert line.startswith('    '), knd\n-\n-\n-def _compare_docs(old_func, new_func):\n-    old_doc = inspect.getdoc(old_func)\n-    new_doc = inspect.getdoc(new_func)\n-    index = new_doc.index('\\n\\n') + 2\n-    assert_equal(new_doc[index:], old_doc)\n-\n-\n-@pytest.mark.skipif(sys.flags.optimize == 2, reason=\"-OO discards docstrings\")\n-def test_deprecate_preserve_whitespace():\n-    assert_('\\n        Bizarre' in new_func5.__doc__)\n-\n-\n-def test_deprecate_module():\n-    assert_(old_func.__module__ == __name__)\n-\n-\n @pytest.mark.filterwarnings(\"ignore:.*safe_eval.*:DeprecationWarning\")\n def test_safe_eval_nameconstant():\n     # Test if safe_eval supports Python 3.4 _ast.NameConstant\n     utils.safe_eval('None')\n \n \n-@pytest.mark.filterwarnings(\"ignore:.*byte_bounds.*:DeprecationWarning\")\n class TestByteBounds:\n \n     def test_byte_bounds(self):\n",
            "comment_added_diff": {},
            "comment_deleted_diff": {
                "98": "    # Ensure we don't mess up the indentation"
            },
            "comment_modified_diff": {}
        },
        {
            "commit": "bd3ee30602ae9ca96cb69b7f2f5494622e1f142f",
            "timestamp": "2023-08-09T18:42:15+08:00",
            "author": "Kei",
            "commit_message": "Fix implementation of array_compare to consider only violating pairs",
            "additions": 313,
            "deletions": 37,
            "change_type": "MODIFY",
            "diff": "@@ -89,16 +89,27 @@ def foo(t):\n     def test_0_ndim_array(self):\n         x = np.array(473963742225900817127911193656584771)\n         y = np.array(18535119325151578301457182298393896)\n-        assert_raises(AssertionError, self._assert_func, x, y)\n+        \n+        with pytest.raises(AssertionError) as exc_info:\n+            self._assert_func(x, y)\n+        msg = str(exc_info.value)\n+        assert_('Mismatched elements: 1 / 1 (100%)\\n'\n+        in msg)\n \n         y = x\n         self._assert_func(x, y)\n \n-        x = np.array(43)\n-        y = np.array(10)\n-        assert_raises(AssertionError, self._assert_func, x, y)\n+        x = np.array(4395065348745.5643764887869876)\n+        y = np.array(0)\n+        with pytest.raises(AssertionError) as exc_info:\n+            self._assert_func(x, y)\n+        msg = str(exc_info.value)\n+        assert_('Mismatched elements: 1 / 1 (100%)\\n'\n+                'Max absolute difference among violations: 4.39506535e+12\\n'\n+                'Max relative difference among violations: inf'\n+                in msg)\n \n-        y = x\n+        x = y\n         self._assert_func(x, y)\n \n     def test_generic_rank3(self):\n@@ -190,6 +201,23 @@ def __ne__(self, other):\n         self._test_not_equal(a, b)\n         self._test_not_equal(b, a)\n \n+        with pytest.raises(AssertionError) as exc_info:\n+            self._test_equal(a, b)\n+        msg = str(exc_info.value)\n+        assert_('Mismatched elements: 1 / 2 (50%)\\n'\n+                'Max absolute difference among violations: 1.\\n'\n+                'Max relative difference among violations: 0.5'\n+                in msg)\n+        \n+        c = np.array([0., 2.9]).view(MyArray)\n+        with pytest.raises(AssertionError) as exc_info:\n+            self._test_equal(b, c)\n+        msg = str(exc_info.value)\n+        # assert_('Mismatched elements: 2 / 2 (100%)\\n'\n+        #         'Max absolute difference among violations: 2.\\n'\n+        #         'Max relative difference among violations: inf'\n+        #         in msg)\n+\n     def test_subclass_that_does_not_implement_npall(self):\n         class MyArray(np.ndarray):\n             def __array_function__(self, *args, **kwargs):\n@@ -218,12 +246,27 @@ def test_array_vs_scalar_is_equal(self):\n \n         self._test_equal(a, b)\n \n-    def test_array_vs_scalar_not_equal(self):\n+    def test_array_vs_array_not_equal(self):\n         \"\"\"Test comparing an array with a scalar when not all values equal.\"\"\"\n-        a = np.array([1., 2., 3.])\n-        b = 1.\n+        a = np.array([34986, 545676, 439655, 563766])\n+        b = np.array([34986, 545676, 439655, 0])\n \n-        self._test_not_equal(a, b)\n+        with pytest.raises(AssertionError) as exc_info:\n+            self._assert_func(a, b)\n+        msg = str(exc_info.value)\n+        assert_('Mismatched elements: 1 / 4 (25%)\\n'\n+                'Max absolute difference among violations: 563766\\n'\n+                'Max relative difference among violations: inf'\n+                in msg)\n+        \n+        a = np.array([34986, 545676, 439655.2, 563766])\n+        with pytest.raises(AssertionError) as exc_info:\n+            self._assert_func(a, b)\n+        msg = str(exc_info.value)\n+        assert_('Mismatched elements: 2 / 4 (50%)\\n'\n+                'Max absolute difference among violations: 563766.\\n'\n+                'Max relative difference among violations: 4.54902139e-07'\n+                in msg)\n \n     def test_array_vs_scalar_strict(self):\n         \"\"\"Test comparing an array with a scalar with strict option.\"\"\"\n@@ -262,6 +305,8 @@ def test_build_err_msg_defaults(self):\n              '2.00003, 3.00004])')\n         assert_equal(a, b)\n \n+    \n+\n     def test_build_err_msg_no_verbose(self):\n         x = np.array([1.00001, 2.00002, 3.00003])\n         y = np.array([1.00002, 2.00003, 3.00004])\n@@ -402,14 +447,41 @@ def test_closeness(self):\n         # so this check serves to preserve the wrongness.\n \n         # test scalars\n-        self._assert_func(1.499999, 0.0, decimal=0)\n-        assert_raises(AssertionError,\n-                          lambda: self._assert_func(1.5, 0.0, decimal=0))\n+        with pytest.raises(AssertionError) as exc_info:\n+            self._assert_func(1.5, 0.0, decimal=0)\n+        msg = str(exc_info.value)\n+        assert_('Mismatched elements: 1 / 1 (100%)\\n'\n+            'Max absolute difference among violations: 1.5\\n'\n+            'Max relative difference among violations: inf'\n+            in msg)\n \n         # test arrays\n         self._assert_func([1.499999], [0.0], decimal=0)\n-        assert_raises(AssertionError,\n-                          lambda: self._assert_func([1.5], [0.0], decimal=0))\n+        with pytest.raises(AssertionError) as exc_info:\n+           self._assert_func([1.5], [0.0], decimal=0)\n+        msg = str(exc_info.value)\n+        assert_('Mismatched elements: 1 / 1 (100%)\\n'\n+            'Max absolute difference among violations: 1.5\\n'\n+            'Max relative difference among violations: inf'\n+            in msg)\n+        \n+        a = [1.4999999, 0.00003]\n+        b = [1.49999991, 0]\n+        with pytest.raises(AssertionError) as exc_info:\n+                self._assert_func(a, b, decimal=7)\n+        msg = str(exc_info.value)\n+        assert_('Mismatched elements: 1 / 2 (50%)\\n'\n+            'Max absolute difference among violations: 3e-5\\n'\n+            'Max relative difference among violations: inf'\n+            in msg)\n+        \n+        with pytest.raises(AssertionError) as exc_info:\n+                self._assert_func(b, a, decimal=7)\n+        msg = str(exc_info.value)\n+        assert_('Mismatched elements: 1 / 2 (50%)\\n'\n+            'Max absolute difference among violations: 3e-5\\n'\n+            'Max relative difference among violations: 0.'\n+            in msg)\n \n     def test_simple(self):\n         x = np.array([1234.2222])\n@@ -417,8 +489,52 @@ def test_simple(self):\n \n         self._assert_func(x, y, decimal=3)\n         self._assert_func(x, y, decimal=4)\n-        assert_raises(AssertionError,\n-                lambda: self._assert_func(x, y, decimal=5))\n+        with pytest.raises(AssertionError) as exc_info:\n+           self._assert_func(x, y, decimal=5)\n+        msg = str(exc_info.value)\n+        assert_('Mismatched elements: 1 / 1 (100%)\\n'\n+            'Max absolute difference among violations: 1.e-04\\n'\n+            'Max relative difference among violations: 8.10226812e-08'\n+            in msg)\n+        \n+    def test_array_vs_scalar(self):\n+        a = [5498.42354, 849.54345, 0.00]\n+        b = 5498.42354\n+        with pytest.raises(AssertionError) as exc_info:\n+                self._assert_func(a, b, decimal=9)\n+        msg = str(exc_info.value)\n+        assert_('Mismatched elements: 2 / 3 (66.7%)\\n'\n+            'Max absolute difference among violations: 5498.42354\\n'\n+            'Max relative difference among violations: 1.'\n+            in msg)\n+        \n+\n+        with pytest.raises(AssertionError) as exc_info:\n+                self._assert_func(b, a, decimal=9)\n+        msg = str(exc_info.value)\n+        assert_('Mismatched elements: 2 / 3 (66.7%)\\n'\n+            'Max absolute difference among violations: 5498.42354\\n'\n+            'Max relative difference among violations: 5.4722099'\n+            in msg)\n+        \n+        a = [5498.42354, 0.00]\n+        with pytest.raises(AssertionError) as exc_info:\n+                self._assert_func(b, a, decimal=7)\n+        msg = str(exc_info.value)\n+        assert_('Mismatched elements: 1 / 2 (50%)\\n'\n+            'Max absolute difference among violations: 5498.42354\\n'\n+            'Max relative difference among violations: inf'\n+            in msg)\n+        \n+        b = 0\n+        with pytest.raises(AssertionError) as exc_info:\n+            self._assert_func(a, b, decimal=7)\n+        msg = str(exc_info.value)\n+        assert_('Mismatched elements: 1 / 2 (50%)\\n'\n+            'Max absolute difference among violations: 5498.42354\\n'\n+            'Max relative difference among violations: inf'\n+            in msg)\n+\n \n     def test_nan(self):\n         anan = np.array([np.nan])\n@@ -468,6 +584,34 @@ def test_subclass(self):\n         self._test_equal(a, b)\n         self._test_equal(b, a)\n \n+    def test_subclass_2(self):\n+        # While we cannot guarantee testing functions will always work for\n+        # subclasses, the tests should ideally rely only on subclasses having\n+        # comparison operators, not on them being able to store booleans\n+        # (which, e.g., astropy Quantity cannot usefully do). See gh-8452.\n+        class MyArray(np.ndarray):\n+            def __eq__(self, other):\n+                return super().__eq__(other).view(np.ndarray)\n+\n+            def __lt__(self, other):\n+                return super().__lt__(other).view(np.ndarray)\n+\n+            def all(self, *args, **kwargs):\n+                raise super().all(args, kwargs)\n+\n+        a = np.array([1., 2.]).view(MyArray)\n+        self._assert_func(a, a)\n+\n+        b = np.array([1., 202]).view(MyArray)\n+        with pytest.raises(AssertionError) as exc_info:\n+            self._assert_func(a, b)\n+        msg = str(exc_info.value)\n+        assert_('Mismatched elements: 1 / 2 (50%)\\n'\n+                'Max absolute difference among violations: 200\\n'\n+                'Max relative difference among violations: 0.99009'\n+                in msg)\n+        \n+\n     def test_subclass_that_cannot_be_bool(self):\n         # While we cannot guarantee testing functions will always work for\n         # subclasses, the tests should ideally rely only on subclasses having\n@@ -485,7 +629,7 @@ def all(self, *args, **kwargs):\n \n         a = np.array([1., 2.]).view(MyArray)\n         self._assert_func(a, a)\n-\n+        \n \n class TestAlmostEqual(_GenericTest):\n \n@@ -556,8 +700,8 @@ def test_error_message(self):\n             self._assert_func(x, y, decimal=12)\n         msgs = str(exc_info.value).split('\\n')\n         assert_equal(msgs[3], 'Mismatched elements: 3 / 3 (100%)')\n-        assert_equal(msgs[4], 'Max absolute difference: 1.e-05')\n-        assert_equal(msgs[5], 'Max relative difference: 3.33328889e-06')\n+        assert_equal(msgs[4], 'Max absolute difference among violations: 1.e-05')\n+        assert_equal(msgs[5], 'Max relative difference among violations: 3.33328889e-06')\n         assert_equal(\n             msgs[6],\n             ' x: array([1.00000000001, 2.00000000002, 3.00003      ])')\n@@ -572,8 +716,8 @@ def test_error_message(self):\n             self._assert_func(x, y)\n         msgs = str(exc_info.value).split('\\n')\n         assert_equal(msgs[3], 'Mismatched elements: 1 / 3 (33.3%)')\n-        assert_equal(msgs[4], 'Max absolute difference: 1.e-05')\n-        assert_equal(msgs[5], 'Max relative difference: 3.33328889e-06')\n+        assert_equal(msgs[4], 'Max absolute difference among violations: 1.e-05')\n+        assert_equal(msgs[5], 'Max relative difference among violations: 3.33328889e-06')\n         assert_equal(msgs[6], ' x: array([1.     , 2.     , 3.00003])')\n         assert_equal(msgs[7], ' y: array([1.     , 2.     , 3.00004])')\n \n@@ -584,8 +728,8 @@ def test_error_message(self):\n             self._assert_func(x, y)\n         msgs = str(exc_info.value).split('\\n')\n         assert_equal(msgs[3], 'Mismatched elements: 1 / 2 (50%)')\n-        assert_equal(msgs[4], 'Max absolute difference: 1.')\n-        assert_equal(msgs[5], 'Max relative difference: 1.')\n+        assert_equal(msgs[4], 'Max absolute difference among violations: 1.')\n+        assert_equal(msgs[5], 'Max relative difference among violations: 1.')\n         assert_equal(msgs[6], ' x: array([inf,  0.])')\n         assert_equal(msgs[7], ' y: array([inf,  1.])')\n \n@@ -596,8 +740,8 @@ def test_error_message(self):\n             self._assert_func(x, y)\n         msgs = str(exc_info.value).split('\\n')\n         assert_equal(msgs[3], 'Mismatched elements: 2 / 2 (100%)')\n-        assert_equal(msgs[4], 'Max absolute difference: 2')\n-        assert_equal(msgs[5], 'Max relative difference: inf')\n+        assert_equal(msgs[4], 'Max absolute difference among violations: 2')\n+        assert_equal(msgs[5], 'Max relative difference among violations: inf')\n \n     def test_error_message_2(self):\n         \"\"\"Check the message is formatted correctly when either x or y is a scalar.\"\"\"\n@@ -607,8 +751,8 @@ def test_error_message_2(self):\n             self._assert_func(x, y)\n         msgs = str(exc_info.value).split('\\n')\n         assert_equal(msgs[3], 'Mismatched elements: 20 / 20 (100%)')\n-        assert_equal(msgs[4], 'Max absolute difference: 1.')\n-        assert_equal(msgs[5], 'Max relative difference: 1.')\n+        assert_equal(msgs[4], 'Max absolute difference among violations: 1.')\n+        assert_equal(msgs[5], 'Max relative difference among violations: 1.')\n \n         y = 2\n         x = np.ones(20)\n@@ -616,8 +760,8 @@ def test_error_message_2(self):\n             self._assert_func(x, y)\n         msgs = str(exc_info.value).split('\\n')\n         assert_equal(msgs[3], 'Mismatched elements: 20 / 20 (100%)')\n-        assert_equal(msgs[4], 'Max absolute difference: 1.')\n-        assert_equal(msgs[5], 'Max relative difference: 0.5')\n+        assert_equal(msgs[4], 'Max absolute difference among violations: 1.')\n+        assert_equal(msgs[5], 'Max relative difference among violations: 0.5')\n \n     def test_subclass_that_cannot_be_bool(self):\n         # While we cannot guarantee testing functions will always work for\n@@ -698,12 +842,28 @@ def test_simple_arrays(self):\n         assert_raises(AssertionError, lambda: self._assert_func(x, y))\n         assert_raises(AssertionError, lambda: self._assert_func(y, x))\n \n+        a = np.array([1, 3, 6, 20])\n+        b = np.array([2, 4, 6, 8])\n+\n+        with pytest.raises(AssertionError) as exc_info:\n+            self._assert_func(a, b)\n+        msg = str(exc_info.value)\n+        assert_('Mismatched elements: 2 / 4 (50%)\\n'\n+                'Max absolute difference among violations: 12\\n'\n+                'Max relative difference among violations: 1.5' in msg)\n+\n     def test_rank2(self):\n         x = np.array([[1.1, 2.2], [3.3, 4.4]])\n         y = np.array([[1.2, 2.3], [3.4, 4.5]])\n \n         self._assert_func(x, y)\n-        assert_raises(AssertionError, lambda: self._assert_func(y, x))\n+        with pytest.raises(AssertionError) as exc_info:\n+            self._assert_func(y, x)\n+        msg = str(exc_info.value)\n+        assert_('Mismatched elements: 4 / 4 (100%)\\n'\n+            'Max absolute difference among violations: 0.1\\n'\n+            'Max relative difference among violations: 0.09090909'\n+            in msg)\n \n         y = np.array([[1.0, 2.3], [3.4, 4.5]])\n \n@@ -718,8 +878,13 @@ def test_rank3(self):\n         assert_raises(AssertionError, lambda: self._assert_func(y, x))\n \n         y[0, 0, 0] = 0\n-\n-        assert_raises(AssertionError, lambda: self._assert_func(x, y))\n+        with pytest.raises(AssertionError) as exc_info:\n+            self._assert_func(x, y)\n+        msg = str(exc_info.value)\n+        assert_('Mismatched elements: 1 / 8 (12.5%)\\n'\n+            'Max absolute difference among violations: 1.\\n'\n+            'Max relative difference among violations: inf'in msg)\n+        \n         assert_raises(AssertionError, lambda: self._assert_func(y, x))\n \n     def test_simple_items(self):\n@@ -727,7 +892,12 @@ def test_simple_items(self):\n         y = 2.2\n \n         self._assert_func(x, y)\n-        assert_raises(AssertionError, lambda: self._assert_func(y, x))\n+        with pytest.raises(AssertionError) as exc_info:\n+            self._assert_func(y, x)\n+        msg = str(exc_info.value)\n+        assert_('Mismatched elements: 1 / 1 (100%)\\n'\n+            'Max absolute difference among violations: 1.1\\n'\n+            'Max relative difference among violations: 1.'in msg)\n \n         y = np.array([2.2, 3.3])\n \n@@ -738,6 +908,75 @@ def test_simple_items(self):\n \n         assert_raises(AssertionError, lambda: self._assert_func(x, y))\n \n+    def test_simple_items_and_array(self):\n+        x = np.array([[621.345454, 390.5436, 43.54657, 626.4535],\n+                      [54.54, 627.3399, 13., 405.5435],\n+                      [543.545, 8.34, 91.543, 333.3]])\n+        y = 627.34\n+        self._assert_func(x, y)\n+\n+        y = 8.339999\n+        self._assert_func(y, x)\n+\n+        x = np.array([[3.4536, 2390.5436, 435.54657, 324525.4535],\n+                      [5449.54, 999090.54, 130303.54, 405.5435],\n+                      [543.545, 8.34, 91.543, 999090.53999]])\n+        y = 999090.54\n+\n+        with pytest.raises(AssertionError) as exc_info:\n+            self._assert_func(x, y)\n+        msg = str(exc_info.value)\n+        assert_('Mismatched elements: 1 / 12 (8.33%)\\n'\n+            'Max absolute difference among violations: 0.\\n'\n+            'Max relative difference among violations: 0.'in msg)\n+\n+        with pytest.raises(AssertionError) as exc_info:\n+            self._assert_func(y, x)\n+        msg = str(exc_info.value)\n+        assert_('Mismatched elements: 12 / 12 (100%)\\n'\n+            'Max absolute difference among violations: 999087.0864\\n'\n+            'Max relative difference among violations: 289288.5934676'\n+            in msg)\n+        \n+\n+    def test_zeroes(self):\n+        x = np.array([546456., 0, 15.455])\n+        y = np.array(87654.)\n+\n+        with pytest.raises(AssertionError) as exc_info:\n+            self._assert_func(x, y)\n+        msg = str(exc_info.value)\n+        assert_('Mismatched elements: 1 / 3 (33.3%)\\n'\n+            'Max absolute difference among violations: 458802.\\n'\n+            'Max relative difference among violations: 5.23423917'\n+            in msg)\n+        \n+        with pytest.raises(AssertionError) as exc_info:\n+            self._assert_func(y, x)\n+        msg = str(exc_info.value)\n+        assert_('Mismatched elements: 2 / 3 (66.7%)\\n'\n+            'Max absolute difference among violations: 87654.\\n'\n+            'Max relative difference among violations: 5670.5626011'\n+            in msg)\n+        \n+        y = 0\n+\n+        with pytest.raises(AssertionError) as exc_info:\n+            self._assert_func(x, y)\n+        msg = str(exc_info.value)\n+        assert_('Mismatched elements: 3 / 3 (100%)\\n'\n+            'Max absolute difference among violations: 546456.\\n'\n+            'Max relative difference among violations: inf'\n+            in msg)\n+        \n+        with pytest.raises(AssertionError) as exc_info:\n+            self._assert_func(y, x)\n+        msg = str(exc_info.value)\n+        assert_('Mismatched elements: 1 / 3 (33.3%)\\n'\n+            'Max absolute difference among violations: 0.\\n'\n+            'Max relative difference among violations: inf'\n+            in msg)\n+    \n     def test_nan_noncompare(self):\n         anan = np.array(np.nan)\n         aone = np.array(1)\n@@ -849,6 +1088,27 @@ def test_simple(self):\n \n         assert_allclose(x, y, atol=1)\n         assert_raises(AssertionError, assert_allclose, x, y)\n+        with pytest.raises(AssertionError) as exc_info:\n+            assert_allclose(x, y)\n+        msg = str(exc_info.value)\n+        assert_('Mismatched elements: 1 / 1 (100%)\\n'\n+            'Max absolute difference among violations: 0.001\\n'\n+            'Max relative difference among violations: 999999.'in msg)\n+        \n+        z = 0\n+        with pytest.raises(AssertionError) as exc_info:\n+            assert_allclose(y, z)\n+        msg = str(exc_info.value)\n+        assert_('Mismatched elements: 1 / 1 (100%)\\n'\n+            'Max absolute difference among violations: 1.e-09\\n'\n+            'Max relative difference among violations: inf'in msg)\n+        \n+        with pytest.raises(AssertionError) as exc_info:\n+            assert_allclose(z, y)\n+        msg = str(exc_info.value)\n+        assert_('Mismatched elements: 1 / 1 (100%)\\n'\n+            'Max absolute difference among violations: 1.e-09\\n'\n+            'Max relative difference among violations: 1.'in msg)\n \n         a = np.array([x, y, x, y])\n         b = np.array([x, y, x, x])\n@@ -863,6 +1123,22 @@ def test_simple(self):\n         assert_allclose(6, 10, rtol=0.5)\n         assert_raises(AssertionError, assert_allclose, 10, 6, rtol=0.5)\n \n+        b = np.array([x, y, x, x])\n+        c = np.array([x, y, x, z])\n+        with pytest.raises(AssertionError) as exc_info:\n+            assert_allclose(b, c)\n+        msg = str(exc_info.value)\n+        assert_('Mismatched elements: 1 / 4 (25%)\\n'\n+            'Max absolute difference among violations: 0.001\\n'\n+            'Max relative difference among violations: inf'in msg)\n+        \n+        with pytest.raises(AssertionError) as exc_info:\n+            assert_allclose(c, b)\n+        msg = str(exc_info.value)\n+        assert_('Mismatched elements: 1 / 4 (25%)\\n'\n+            'Max absolute difference among violations: 0.001\\n'\n+            'Max relative difference among violations: 1.'in msg)\n+\n     def test_min_int(self):\n         a = np.array([np.iinfo(np.int_).min], dtype=np.int_)\n         # Should not raise:\n@@ -876,8 +1152,8 @@ def test_report_fail_percentage(self):\n             assert_allclose(a, b)\n         msg = str(exc_info.value)\n         assert_('Mismatched elements: 1 / 4 (25%)\\n'\n-                'Max absolute difference: 1\\n'\n-                'Max relative difference: 0.5' in msg)\n+                'Max absolute difference among violations: 1\\n'\n+                'Max relative difference among violations: 0.5' in msg)\n \n     def test_equal_nan(self):\n         a = np.array([np.nan])\n@@ -908,7 +1184,7 @@ def test_report_max_relative_error(self):\n         with pytest.raises(AssertionError) as exc_info:\n             assert_allclose(a, b)\n         msg = str(exc_info.value)\n-        assert_('Max relative difference: 0.5' in msg)\n+        assert_('Max relative difference among violations: 0.5' in msg)\n \n     def test_timedelta(self):\n         # see gh-18286\n@@ -927,7 +1203,7 @@ def test_error_message_unsigned(self):\n         with pytest.raises(AssertionError) as exc_info:\n             assert_allclose(x, y, atol=3)\n         msgs = str(exc_info.value).split('\\n')\n-        assert_equal(msgs[4], 'Max absolute difference: 4')\n+        assert_equal(msgs[4], 'Max absolute difference among violations: 4')\n \n \n class TestArrayAlmostEqualNulp:\n",
            "comment_added_diff": {
                "216": "        # assert_('Mismatched elements: 2 / 2 (100%)\\n'",
                "217": "        #         'Max absolute difference among violations: 2.\\n'",
                "218": "        #         'Max relative difference among violations: inf'",
                "219": "        #         in msg)",
                "588": "        # While we cannot guarantee testing functions will always work for",
                "589": "        # subclasses, the tests should ideally rely only on subclasses having",
                "590": "        # comparison operators, not on them being able to store booleans",
                "591": "        # (which, e.g., astropy Quantity cannot usefully do). See gh-8452."
            },
            "comment_deleted_diff": {},
            "comment_modified_diff": {
                "588": "        assert_equal(msgs[5], 'Max relative difference: 1.')"
            }
        },
        {
            "commit": "dd9a33e702a9e1ddf17358a59058f11203887a18",
            "timestamp": "2023-08-20T21:12:04+08:00",
            "author": "ellaella12",
            "commit_message": "Update tests to check of error message in pytest.raises(match=...)",
            "additions": 193,
            "deletions": 257,
            "change_type": "MODIFY",
            "diff": "@@ -4,6 +4,7 @@\n import itertools\n import pytest\n import weakref\n+import re\n \n import numpy as np\n from numpy.testing import (\n@@ -13,7 +14,7 @@\n     assert_approx_equal, assert_array_almost_equal_nulp, assert_array_max_ulp,\n     clear_and_catch_warnings, suppress_warnings, assert_string_equal, assert_,\n     tempdir, temppath, assert_no_gc_cycles, HAS_REFCOUNT\n-    )\n+)\n \n \n class _GenericTest:\n@@ -89,25 +90,23 @@ def foo(t):\n     def test_0_ndim_array(self):\n         x = np.array(473963742225900817127911193656584771)\n         y = np.array(18535119325151578301457182298393896)\n-        \n+\n         with pytest.raises(AssertionError) as exc_info:\n             self._assert_func(x, y)\n         msg = str(exc_info.value)\n         assert_('Mismatched elements: 1 / 1 (100%)\\n'\n-        in msg)\n+                in msg)\n \n         y = x\n         self._assert_func(x, y)\n \n         x = np.array(4395065348745.5643764887869876)\n         y = np.array(0)\n-        with pytest.raises(AssertionError) as exc_info:\n+        expected_msg = ('Mismatched elements: 1 / 1 (100%)\\n'\n+                        'Max absolute difference among violations: 4.39506535e+12\\n'\n+                        'Max relative difference among violations: inf\\n')\n+        with pytest.raises(AssertionError, match=re.escape(expected_msg)):\n             self._assert_func(x, y)\n-        msg = str(exc_info.value)\n-        assert_('Mismatched elements: 1 / 1 (100%)\\n'\n-                'Max absolute difference among violations: 4.39506535e+12\\n'\n-                'Max relative difference among violations: inf'\n-                in msg)\n \n         x = y\n         self._assert_func(x, y)\n@@ -201,14 +200,12 @@ def __ne__(self, other):\n         self._test_not_equal(a, b)\n         self._test_not_equal(b, a)\n \n-        with pytest.raises(AssertionError) as exc_info:\n+        expected_msg = ('Mismatched elements: 1 / 2 (50%)\\n'\n+                        'Max absolute difference among violations: 1.\\n'\n+                        'Max relative difference among violations: 0.5')\n+        with pytest.raises(AssertionError, match=re.escape(expected_msg)):\n             self._test_equal(a, b)\n-        msg = str(exc_info.value)\n-        assert_('Mismatched elements: 1 / 2 (50%)\\n'\n-                'Max absolute difference among violations: 1.\\n'\n-                'Max relative difference among violations: 0.5'\n-                in msg)\n-        \n+\n         c = np.array([0., 2.9]).view(MyArray)\n         with pytest.raises(AssertionError) as exc_info:\n             self._test_equal(b, c)\n@@ -251,22 +248,18 @@ def test_array_vs_array_not_equal(self):\n         a = np.array([34986, 545676, 439655, 563766])\n         b = np.array([34986, 545676, 439655, 0])\n \n-        with pytest.raises(AssertionError) as exc_info:\n+        expected_msg = ('Mismatched elements: 1 / 4 (25%)\\n'\n+                        'Max absolute difference among violations: 563766\\n'\n+                        'Max relative difference among violations: inf')\n+        with pytest.raises(AssertionError, match=re.escape(expected_msg)):\n             self._assert_func(a, b)\n-        msg = str(exc_info.value)\n-        assert_('Mismatched elements: 1 / 4 (25%)\\n'\n-                'Max absolute difference among violations: 563766\\n'\n-                'Max relative difference among violations: inf'\n-                in msg)\n-        \n+\n         a = np.array([34986, 545676, 439655.2, 563766])\n-        with pytest.raises(AssertionError) as exc_info:\n+        expected_msg = ('Mismatched elements: 2 / 4 (50%)\\n'\n+                        'Max absolute difference among violations: 563766.\\n'\n+                        'Max relative difference among violations: 4.54902139e-07')\n+        with pytest.raises(AssertionError, match=re.escape(expected_msg)):\n             self._assert_func(a, b)\n-        msg = str(exc_info.value)\n-        assert_('Mismatched elements: 2 / 4 (50%)\\n'\n-                'Max absolute difference among violations: 563766.\\n'\n-                'Max relative difference among violations: 4.54902139e-07'\n-                in msg)\n \n     def test_array_vs_scalar_strict(self):\n         \"\"\"Test comparing an array with a scalar with strict option.\"\"\"\n@@ -305,8 +298,6 @@ def test_build_err_msg_defaults(self):\n              '2.00003, 3.00004])')\n         assert_equal(a, b)\n \n-    \n-\n     def test_build_err_msg_no_verbose(self):\n         x = np.array([1.00001, 2.00002, 3.00003])\n         y = np.array([1.00002, 2.00003, 3.00004])\n@@ -427,7 +418,7 @@ def test_complex(self):\n         self._test_not_equal(x, y)\n \n     def test_object(self):\n-        #gh-12942\n+        # gh-12942\n         import datetime\n         a = np.array([datetime.datetime(2000, 1, 1),\n                       datetime.datetime(2000, 1, 2)])\n@@ -447,41 +438,34 @@ def test_closeness(self):\n         # so this check serves to preserve the wrongness.\n \n         # test scalars\n-        with pytest.raises(AssertionError) as exc_info:\n+        expected_msg = ('Mismatched elements: 1 / 1 (100%)\\n'\n+                        'Max absolute difference among violations: 1.5\\n'\n+                        'Max relative difference among violations: inf')\n+        with pytest.raises(AssertionError, match=re.escape(expected_msg)):\n             self._assert_func(1.5, 0.0, decimal=0)\n-        msg = str(exc_info.value)\n-        assert_('Mismatched elements: 1 / 1 (100%)\\n'\n-            'Max absolute difference among violations: 1.5\\n'\n-            'Max relative difference among violations: inf'\n-            in msg)\n \n         # test arrays\n         self._assert_func([1.499999], [0.0], decimal=0)\n-        with pytest.raises(AssertionError) as exc_info:\n+\n+        expected_msg = ('Mismatched elements: 1 / 1 (100%)\\n'\n+                        'Max absolute difference among violations: 1.5\\n'\n+                        'Max relative difference among violations: inf')\n+        with pytest.raises(AssertionError, match=re.escape(expected_msg)):\n             self._assert_func([1.5], [0.0], decimal=0)\n-        msg = str(exc_info.value)\n-        assert_('Mismatched elements: 1 / 1 (100%)\\n'\n-            'Max absolute difference among violations: 1.5\\n'\n-            'Max relative difference among violations: inf'\n-            in msg)\n-        \n+\n         a = [1.4999999, 0.00003]\n         b = [1.49999991, 0]\n-        with pytest.raises(AssertionError) as exc_info:\n+        expected_msg = ('Mismatched elements: 1 / 2 (50%)\\n'\n+                        'Max absolute difference among violations: 3.e-05\\n'\n+                        'Max relative difference among violations: inf')\n+        with pytest.raises(AssertionError, match=re.escape(expected_msg)):\n             self._assert_func(a, b, decimal=7)\n-        msg = str(exc_info.value)\n-        assert_('Mismatched elements: 1 / 2 (50%)\\n'\n-                'Max absolute difference among violations: 3.e-05\\n'\n-                'Max relative difference among violations: inf'\n-                in msg)\n-        \n-        with pytest.raises(AssertionError) as exc_info:\n+\n+        expected_msg = ('Mismatched elements: 1 / 2 (50%)\\n'\n+                        'Max absolute difference among violations: 3.e-05\\n'\n+                        'Max relative difference among violations: 1.')\n+        with pytest.raises(AssertionError, match=re.escape(expected_msg)):\n             self._assert_func(b, a, decimal=7)\n-        msg = str(exc_info.value)\n-        assert_('Mismatched elements: 1 / 2 (50%)\\n'\n-            'Max absolute difference among violations: 3.e-05\\n'\n-            'Max relative difference among violations: 1.'\n-            in msg)\n \n     def test_simple(self):\n         x = np.array([1234.2222])\n@@ -489,51 +473,41 @@ def test_simple(self):\n \n         self._assert_func(x, y, decimal=3)\n         self._assert_func(x, y, decimal=4)\n-        with pytest.raises(AssertionError) as exc_info:\n+\n+        expected_msg = ('Mismatched elements: 1 / 1 (100%)\\n'\n+                        'Max absolute difference among violations: 1.e-04\\n'\n+                        'Max relative difference among violations: 8.10226812e-08')\n+        with pytest.raises(AssertionError, match=re.escape(expected_msg)):\n             self._assert_func(x, y, decimal=5)\n-        msg = str(exc_info.value)\n-        assert_('Mismatched elements: 1 / 1 (100%)\\n'\n-            'Max absolute difference among violations: 1.e-04\\n'\n-            'Max relative difference among violations: 8.10226812e-08'\n-            in msg)\n-        \n+\n     def test_array_vs_scalar(self):\n         a = [5498.42354, 849.54345, 0.00]\n         b = 5498.42354\n-        with pytest.raises(AssertionError) as exc_info:\n+        expected_msg = ('Mismatched elements: 2 / 3 (66.7%)\\n'\n+                        'Max absolute difference among violations: 5498.42354\\n'\n+                        'Max relative difference among violations: 1.')\n+        with pytest.raises(AssertionError, match=re.escape(expected_msg)):\n             self._assert_func(a, b, decimal=9)\n-        msg = str(exc_info.value)\n-        assert_('Mismatched elements: 2 / 3 (66.7%)\\n'\n-            'Max absolute difference among violations: 5498.42354\\n'\n-            'Max relative difference among violations: 1.'\n-            in msg)\n \n-        with pytest.raises(AssertionError) as exc_info:\n+        expected_msg = ('Mismatched elements: 2 / 3 (66.7%)\\n'\n+                        'Max absolute difference among violations: 5498.42354\\n'\n+                        'Max relative difference among violations: 5.4722099')\n+        with pytest.raises(AssertionError, match=re.escape(expected_msg)):\n             self._assert_func(b, a, decimal=9)\n-        msg = str(exc_info.value)\n-        assert_('Mismatched elements: 2 / 3 (66.7%)\\n'\n-                'Max absolute difference among violations: 5498.42354\\n'\n-                'Max relative difference among violations: 5.4722099'\n-                in msg)\n-        \n+\n         a = [5498.42354, 0.00]\n-        with pytest.raises(AssertionError) as exc_info:\n+        expected_msg = ('Mismatched elements: 1 / 2 (50%)\\n'\n+                        'Max absolute difference among violations: 5498.42354\\n'\n+                        'Max relative difference among violations: inf')\n+        with pytest.raises(AssertionError, match=re.escape(expected_msg)):\n             self._assert_func(b, a, decimal=7)\n-        msg = str(exc_info.value)\n-        assert_('Mismatched elements: 1 / 2 (50%)\\n'\n-            'Max absolute difference among violations: 5498.42354\\n'\n-            'Max relative difference among violations: inf'\n-            in msg)\n-        \n+\n         b = 0\n-        with pytest.raises(AssertionError) as exc_info:\n+        expected_msg = ('Mismatched elements: 1 / 2 (50%)\\n'\n+                        'Max absolute difference among violations: 5498.42354\\n'\n+                        'Max relative difference among violations: inf')\n+        with pytest.raises(AssertionError, match=re.escape(expected_msg)):\n             self._assert_func(a, b, decimal=7)\n-        msg = str(exc_info.value)\n-        assert_('Mismatched elements: 1 / 2 (50%)\\n'\n-            'Max absolute difference among violations: 5498.42354\\n'\n-            'Max relative difference among violations: inf'\n-            in msg)\n-\n \n     def test_nan(self):\n         anan = np.array([np.nan])\n@@ -541,21 +515,21 @@ def test_nan(self):\n         ainf = np.array([np.inf])\n         self._assert_func(anan, anan)\n         assert_raises(AssertionError,\n-                lambda: self._assert_func(anan, aone))\n+                      lambda: self._assert_func(anan, aone))\n         assert_raises(AssertionError,\n-                lambda: self._assert_func(anan, ainf))\n+                      lambda: self._assert_func(anan, ainf))\n         assert_raises(AssertionError,\n-                lambda: self._assert_func(ainf, anan))\n+                      lambda: self._assert_func(ainf, anan))\n \n     def test_inf(self):\n         a = np.array([[1., 2.], [3., 4.]])\n         b = a.copy()\n         a[0, 0] = np.inf\n         assert_raises(AssertionError,\n-                lambda: self._assert_func(a, b))\n+                      lambda: self._assert_func(a, b))\n         b[0, 0] = -np.inf\n         assert_raises(AssertionError,\n-                lambda: self._assert_func(a, b))\n+                      lambda: self._assert_func(a, b))\n \n     def test_subclass(self):\n         a = np.array([[1., 2.], [3., 4.]])\n@@ -604,14 +578,12 @@ def all(self, *args, **kwargs):\n         z = np.array([True, True]).view(MyArray)\n         all(z)\n         b = np.array([1., 202]).view(MyArray)\n-        with pytest.raises(AssertionError) as exc_info:\n+        expected_msg = ('Mismatched elements: 1 / 2 (50%)\\n'\n+                        'Max absolute difference among violations: 200.\\n'\n+                        'Max relative difference among violations: 0.99009')\n+        with pytest.raises(AssertionError, match=re.escape(expected_msg)):\n             self._assert_func(a, b)\n-        msg = str(exc_info.value)\n-        assert_('Mismatched elements: 1 / 2 (50%)\\n'\n-                'Max absolute difference among violations: 200.\\n'\n-                'Max relative difference among violations: 0.99009'\n-                in msg)\n-        \n+\n     def test_subclass_that_cannot_be_bool(self):\n         # While we cannot guarantee testing functions will always work for\n         # subclasses, the tests should ideally rely only on subclasses having\n@@ -630,6 +602,7 @@ def all(self, *args, **kwargs):\n         a = np.array([1., 2.]).view(MyArray)\n         self._assert_func(a, a)\n \n+\n class TestAlmostEqual(_GenericTest):\n \n     def setup_method(self):\n@@ -695,76 +668,62 @@ def test_error_message(self):\n         y = np.array([1.00000000002, 2.00000000003, 3.00004])\n \n         # Test with a different amount of decimal digits\n-        with pytest.raises(AssertionError) as exc_info:\n+        expected_msg = ('Mismatched elements: 3 / 3 (100%)\\n'\n+                        'Max absolute difference among violations: 1.e-05\\n'\n+                        'Max relative difference among violations: 3.33328889e-06\\n'\n+                        ' x: array([1.00000000001, 2.00000000002, 3.00003      ])\\n'\n+                        ' y: array([1.00000000002, 2.00000000003, 3.00004      ])')\n+        with pytest.raises(AssertionError, match=re.escape(expected_msg)):\n             self._assert_func(x, y, decimal=12)\n-        msgs = str(exc_info.value).split('\\n')\n-        assert_equal(msgs[3], 'Mismatched elements: 3 / 3 (100%)')\n-        assert_equal(msgs[4], 'Max absolute difference among violations: '\n-                     '1.e-05')\n-        assert_equal(msgs[5], 'Max relative difference among violations: '\n-                     '3.33328889e-06')\n-        assert_equal(\n-            msgs[6],\n-            ' x: array([1.00000000001, 2.00000000002, 3.00003      ])')\n-        assert_equal(\n-            msgs[7],\n-            ' y: array([1.00000000002, 2.00000000003, 3.00004      ])')\n \n         # With the default value of decimal digits, only the 3rd element\n         # differs. Note that we only check for the formatting of the arrays\n         # themselves.\n-        with pytest.raises(AssertionError) as exc_info:\n+        expected_msg = ('Mismatched elements: 1 / 3 (33.3%)\\n'\n+                        'Max absolute difference among violations: 1.e-05\\n'\n+                        'Max relative difference among violations: 3.33328889e-06\\n'\n+                        ' x: array([1.     , 2.     , 3.00003])\\n'\n+                        ' y: array([1.     , 2.     , 3.00004])')\n+        with pytest.raises(AssertionError, match=re.escape(expected_msg)):\n             self._assert_func(x, y)\n-        msgs = str(exc_info.value).split('\\n')\n-        assert_equal(msgs[3], 'Mismatched elements: 1 / 3 (33.3%)')\n-        assert_equal(msgs[4], 'Max absolute difference among violations: '\n-                     '1.e-05')\n-        assert_equal(msgs[5], 'Max relative difference among violations: '\n-                     '3.33328889e-06')\n-        assert_equal(msgs[6], ' x: array([1.     , 2.     , 3.00003])')\n-        assert_equal(msgs[7], ' y: array([1.     , 2.     , 3.00004])')\n \n         # Check the error message when input includes inf\n         x = np.array([np.inf, 0])\n         y = np.array([np.inf, 1])\n-        with pytest.raises(AssertionError) as exc_info:\n+        expected_msg = ('Mismatched elements: 1 / 2 (50%)\\n'\n+                        'Max absolute difference among violations: 1.\\n'\n+                        'Max relative difference among violations: 1.\\n'\n+                        ' x: array([inf,  0.])\\n'\n+                        ' y: array([inf,  1.])')\n+        with pytest.raises(AssertionError, match=re.escape(expected_msg)):\n             self._assert_func(x, y)\n-        msgs = str(exc_info.value).split('\\n')\n-        assert_equal(msgs[3], 'Mismatched elements: 1 / 2 (50%)')\n-        assert_equal(msgs[4], 'Max absolute difference among violations: 1.')\n-        assert_equal(msgs[5], 'Max relative difference among violations: 1.')\n-        assert_equal(msgs[6], ' x: array([inf,  0.])')\n-        assert_equal(msgs[7], ' y: array([inf,  1.])')\n \n         # Check the error message when dividing by zero\n         x = np.array([1, 2])\n         y = np.array([0, 0])\n-        with pytest.raises(AssertionError) as exc_info:\n+        expected_msg = ('Mismatched elements: 2 / 2 (100%)\\n'\n+                        'Max absolute difference among violations: 2\\n'\n+                        'Max relative difference among violations: inf')\n+        with pytest.raises(AssertionError, match=re.escape(expected_msg)):\n             self._assert_func(x, y)\n-        msgs = str(exc_info.value).split('\\n')\n-        assert_equal(msgs[3], 'Mismatched elements: 2 / 2 (100%)')\n-        assert_equal(msgs[4], 'Max absolute difference among violations: 2')\n-        assert_equal(msgs[5], 'Max relative difference among violations: inf')\n \n     def test_error_message_2(self):\n         \"\"\"Check the message is formatted correctly when either x or y is a scalar.\"\"\"\n         x = 2\n         y = np.ones(20)\n-        with pytest.raises(AssertionError) as exc_info:\n+        expected_msg = ('Mismatched elements: 20 / 20 (100%)\\n'\n+                        'Max absolute difference among violations: 1.\\n'\n+                        'Max relative difference among violations: 1.')\n+        with pytest.raises(AssertionError, match=re.escape(expected_msg)):\n             self._assert_func(x, y)\n-        msgs = str(exc_info.value).split('\\n')\n-        assert_equal(msgs[3], 'Mismatched elements: 20 / 20 (100%)')\n-        assert_equal(msgs[4], 'Max absolute difference among violations: 1.')\n-        assert_equal(msgs[5], 'Max relative difference among violations: 1.')\n \n         y = 2\n         x = np.ones(20)\n-        with pytest.raises(AssertionError) as exc_info:\n+        expected_msg = ('Mismatched elements: 20 / 20 (100%)\\n'\n+                        'Max absolute difference among violations: 1.\\n'\n+                        'Max relative difference among violations: 0.5')\n+        with pytest.raises(AssertionError, match=re.escape(expected_msg)):\n             self._assert_func(x, y)\n-        msgs = str(exc_info.value).split('\\n')\n-        assert_equal(msgs[3], 'Mismatched elements: 20 / 20 (100%)')\n-        assert_equal(msgs[4], 'Max absolute difference among violations: 1.')\n-        assert_equal(msgs[5], 'Max relative difference among violations: 0.5')\n \n     def test_subclass_that_cannot_be_bool(self):\n         # While we cannot guarantee testing functions will always work for\n@@ -848,28 +807,24 @@ def test_simple_arrays(self):\n         a = np.array([1, 3, 6, 20])\n         b = np.array([2, 4, 6, 8])\n \n-        with pytest.raises(AssertionError) as exc_info:\n+        expected_msg = ('Mismatched elements: 2 / 4 (50%)\\n'\n+                        'Max absolute difference among violations: 12\\n'\n+                        'Max relative difference among violations: 1.5')\n+        with pytest.raises(AssertionError, match=re.escape(expected_msg)):\n             self._assert_func(a, b)\n-        msg = str(exc_info.value)\n-        assert_('Mismatched elements: 2 / 4 (50%)\\n'\n-                'Max absolute difference among violations: 12\\n'\n-                'Max relative difference among violations: 1.5' in msg)\n \n     def test_rank2(self):\n         x = np.array([[1.1, 2.2], [3.3, 4.4]])\n         y = np.array([[1.2, 2.3], [3.4, 4.5]])\n \n         self._assert_func(x, y)\n-        with pytest.raises(AssertionError) as exc_info:\n+        expected_msg = ('Mismatched elements: 4 / 4 (100%)\\n'\n+                        'Max absolute difference among violations: 0.1\\n'\n+                        'Max relative difference among violations: 0.09090909')\n+        with pytest.raises(AssertionError, match=re.escape(expected_msg)):\n             self._assert_func(y, x)\n-        msg = str(exc_info.value)\n-        assert_('Mismatched elements: 4 / 4 (100%)\\n'\n-            'Max absolute difference among violations: 0.1\\n'\n-            'Max relative difference among violations: 0.09090909'\n-            in msg)\n \n         y = np.array([[1.0, 2.3], [3.4, 4.5]])\n-\n         assert_raises(AssertionError, lambda: self._assert_func(x, y))\n         assert_raises(AssertionError, lambda: self._assert_func(y, x))\n \n@@ -881,13 +836,12 @@ def test_rank3(self):\n         assert_raises(AssertionError, lambda: self._assert_func(y, x))\n \n         y[0, 0, 0] = 0\n-        with pytest.raises(AssertionError) as exc_info:\n+        expected_msg = ('Mismatched elements: 1 / 8 (12.5%)\\n'\n+                        'Max absolute difference among violations: 1.\\n'\n+                        'Max relative difference among violations: inf')\n+        with pytest.raises(AssertionError, match=re.escape(expected_msg)):\n             self._assert_func(x, y)\n-        msg = str(exc_info.value)\n-        assert_('Mismatched elements: 1 / 8 (12.5%)\\n'\n-            'Max absolute difference among violations: 1.\\n'\n-            'Max relative difference among violations: inf' in msg)\n-        \n+\n         assert_raises(AssertionError, lambda: self._assert_func(y, x))\n \n     def test_simple_items(self):\n@@ -895,12 +849,11 @@ def test_simple_items(self):\n         y = 2.2\n \n         self._assert_func(x, y)\n-        with pytest.raises(AssertionError) as exc_info:\n+        expected_msg = ('Mismatched elements: 1 / 1 (100%)\\n'\n+                        'Max absolute difference among violations: 1.1\\n'\n+                        'Max relative difference among violations: 1.')\n+        with pytest.raises(AssertionError, match=re.escape(expected_msg)):\n             self._assert_func(y, x)\n-        msg = str(exc_info.value)\n-        assert_('Mismatched elements: 1 / 1 (100%)\\n'\n-            'Max absolute difference among violations: 1.1\\n'\n-            'Max relative difference among violations: 1.' in msg)\n \n         y = np.array([2.2, 3.3])\n \n@@ -926,59 +879,48 @@ def test_simple_items_and_array(self):\n                       [543.545, 8.34, 91.543, 999090.53999]])\n         y = 999090.54\n \n-        with pytest.raises(AssertionError) as exc_info:\n+        expected_msg = ('Mismatched elements: 1 / 12 (8.33%)\\n'\n+                        'Max absolute difference among violations: 0.\\n'\n+                        'Max relative difference among violations: 0.')\n+        with pytest.raises(AssertionError, match=re.escape(expected_msg)):\n             self._assert_func(x, y)\n-        msg = str(exc_info.value)\n-        assert_('Mismatched elements: 1 / 12 (8.33%)\\n'\n-            'Max absolute difference among violations: 0.\\n'\n-            'Max relative difference among violations: 0.' in msg)\n \n-        with pytest.raises(AssertionError) as exc_info:\n+        expected_msg = ('Mismatched elements: 12 / 12 (100%)\\n'\n+                        'Max absolute difference among violations: 999087.0864\\n'\n+                        'Max relative difference among violations: 289288.5934676')\n+        with pytest.raises(AssertionError, match=re.escape(expected_msg)):\n             self._assert_func(y, x)\n-        msg = str(exc_info.value)\n-        assert_('Mismatched elements: 12 / 12 (100%)\\n'\n-            'Max absolute difference among violations: 999087.0864\\n'\n-            'Max relative difference among violations: 289288.5934676'\n-            in msg)  \n \n     def test_zeroes(self):\n         x = np.array([546456., 0, 15.455])\n         y = np.array(87654.)\n \n-        with pytest.raises(AssertionError) as exc_info:\n+        expected_msg = ('Mismatched elements: 1 / 3 (33.3%)\\n'\n+                        'Max absolute difference among violations: 458802.\\n'\n+                        'Max relative difference among violations: 5.23423917')\n+        with pytest.raises(AssertionError, match=re.escape(expected_msg)):\n             self._assert_func(x, y)\n-        msg = str(exc_info.value)\n-        assert_('Mismatched elements: 1 / 3 (33.3%)\\n'\n-            'Max absolute difference among violations: 458802.\\n'\n-            'Max relative difference among violations: 5.23423917'\n-            in msg)\n-        \n-        with pytest.raises(AssertionError) as exc_info:\n+\n+        expected_msg = ('Mismatched elements: 2 / 3 (66.7%)\\n'\n+                        'Max absolute difference among violations: 87654.\\n'\n+                        'Max relative difference among violations: 5670.5626011')\n+        with pytest.raises(AssertionError, match=re.escape(expected_msg)):\n             self._assert_func(y, x)\n-        msg = str(exc_info.value)\n-        assert_('Mismatched elements: 2 / 3 (66.7%)\\n'\n-            'Max absolute difference among violations: 87654.\\n'\n-            'Max relative difference among violations: 5670.5626011'\n-            in msg)\n-        \n+\n         y = 0\n \n-        with pytest.raises(AssertionError) as exc_info:\n+        expected_msg = ('Mismatched elements: 3 / 3 (100%)\\n'\n+                        'Max absolute difference among violations: 546456.\\n'\n+                        'Max relative difference among violations: inf')\n+        with pytest.raises(AssertionError, match=re.escape(expected_msg)):\n             self._assert_func(x, y)\n-        msg = str(exc_info.value)\n-        assert_('Mismatched elements: 3 / 3 (100%)\\n'\n-            'Max absolute difference among violations: 546456.\\n'\n-            'Max relative difference among violations: inf'\n-            in msg)\n-        \n-        with pytest.raises(AssertionError) as exc_info:\n+\n+        expected_msg = ('Mismatched elements: 1 / 3 (33.3%)\\n'\n+                        'Max absolute difference among violations: 0.\\n'\n+                        'Max relative difference among violations: inf')\n+        with pytest.raises(AssertionError, match=re.escape(expected_msg)):\n             self._assert_func(y, x)\n-        msg = str(exc_info.value)\n-        assert_('Mismatched elements: 1 / 3 (33.3%)\\n'\n-            'Max absolute difference among violations: 0.\\n'\n-            'Max relative difference among violations: inf'\n-            in msg)\n-    \n+\n     def test_nan_noncompare(self):\n         anan = np.array(np.nan)\n         aone = np.array(1)\n@@ -1090,27 +1032,25 @@ def test_simple(self):\n \n         assert_allclose(x, y, atol=1)\n         assert_raises(AssertionError, assert_allclose, x, y)\n-        with pytest.raises(AssertionError) as exc_info:\n+\n+        expected_msg = ('Mismatched elements: 1 / 1 (100%)\\n'\n+                        'Max absolute difference among violations: 0.001\\n'\n+                        'Max relative difference among violations: 999999.')\n+        with pytest.raises(AssertionError, match=re.escape(expected_msg)):\n             assert_allclose(x, y)\n-        msg = str(exc_info.value)\n-        assert_('Mismatched elements: 1 / 1 (100%)\\n'\n-            'Max absolute difference among violations: 0.001\\n'\n-            'Max relative difference among violations: 999999.' in msg)\n-        \n+\n         z = 0\n-        with pytest.raises(AssertionError) as exc_info:\n+        expected_msg = ('Mismatched elements: 1 / 1 (100%)\\n'\n+                        'Max absolute difference among violations: 1.e-09\\n'\n+                        'Max relative difference among violations: inf')\n+        with pytest.raises(AssertionError, match=re.escape(expected_msg)):\n             assert_allclose(y, z)\n-        msg = str(exc_info.value)\n-        assert_('Mismatched elements: 1 / 1 (100%)\\n'\n-            'Max absolute difference among violations: 1.e-09\\n'\n-            'Max relative difference among violations: inf' in msg)\n-        \n-        with pytest.raises(AssertionError) as exc_info:\n+\n+        expected_msg = ('Mismatched elements: 1 / 1 (100%)\\n'\n+                        'Max absolute difference among violations: 1.e-09\\n'\n+                        'Max relative difference among violations: 1.')\n+        with pytest.raises(AssertionError, match=re.escape(expected_msg)):\n             assert_allclose(z, y)\n-        msg = str(exc_info.value)\n-        assert_('Mismatched elements: 1 / 1 (100%)\\n'\n-            'Max absolute difference among violations: 1.e-09\\n'\n-            'Max relative difference among violations: 1.' in msg)\n \n         a = np.array([x, y, x, y])\n         b = np.array([x, y, x, x])\n@@ -1127,19 +1067,17 @@ def test_simple(self):\n \n         b = np.array([x, y, x, x])\n         c = np.array([x, y, x, z])\n-        with pytest.raises(AssertionError) as exc_info:\n+        expected_msg = ('Mismatched elements: 1 / 4 (25%)\\n'\n+                        'Max absolute difference among violations: 0.001\\n'\n+                        'Max relative difference among violations: inf')\n+        with pytest.raises(AssertionError, match=re.escape(expected_msg)):\n             assert_allclose(b, c)\n-        msg = str(exc_info.value)\n-        assert_('Mismatched elements: 1 / 4 (25%)\\n'\n-            'Max absolute difference among violations: 0.001\\n'\n-            'Max relative difference among violations: inf' in msg)\n-        \n-        with pytest.raises(AssertionError) as exc_info:\n+\n+        expected_msg = ('Mismatched elements: 1 / 4 (25%)\\n'\n+                        'Max absolute difference among violations: 0.001\\n'\n+                        'Max relative difference among violations: 1.')\n+        with pytest.raises(AssertionError, match=re.escape(expected_msg)):\n             assert_allclose(c, b)\n-        msg = str(exc_info.value)\n-        assert_('Mismatched elements: 1 / 4 (25%)\\n'\n-            'Max absolute difference among violations: 0.001\\n'\n-            'Max relative difference among violations: 1.' in msg)\n \n     def test_min_int(self):\n         a = np.array([np.iinfo(np.int_).min], dtype=np.int_)\n@@ -1150,12 +1088,11 @@ def test_report_fail_percentage(self):\n         a = np.array([1, 1, 1, 1])\n         b = np.array([1, 1, 1, 2])\n \n-        with pytest.raises(AssertionError) as exc_info:\n+        expected_msg = ('Mismatched elements: 1 / 4 (25%)\\n'\n+                        'Max absolute difference among violations: 1\\n'\n+                        'Max relative difference among violations: 0.5')\n+        with pytest.raises(AssertionError, match=re.escape(expected_msg)):\n             assert_allclose(a, b)\n-        msg = str(exc_info.value)\n-        assert_('Mismatched elements: 1 / 4 (25%)\\n'\n-                'Max absolute difference among violations: 1\\n'\n-                'Max relative difference among violations: 0.5' in msg)\n \n     def test_equal_nan(self):\n         a = np.array([np.nan])\n@@ -1183,10 +1120,9 @@ def test_report_max_relative_error(self):\n         a = np.array([0, 1])\n         b = np.array([0, 2])\n \n-        with pytest.raises(AssertionError) as exc_info:\n+        expected_msg = 'Max relative difference among violations: 0.5'\n+        with pytest.raises(AssertionError, match=re.escape(expected_msg)):\n             assert_allclose(a, b)\n-        msg = str(exc_info.value)\n-        assert_('Max relative difference among violations: 0.5' in msg)\n \n     def test_timedelta(self):\n         # see gh-18286\n@@ -1202,10 +1138,9 @@ def test_error_message_unsigned(self):\n         #        y - x\n         x = np.asarray([0, 1, 8], dtype='uint8')\n         y = np.asarray([4, 4, 4], dtype='uint8')\n-        with pytest.raises(AssertionError) as exc_info:\n+        expected_msg = 'Max absolute difference among violations: 4'\n+        with pytest.raises(AssertionError, match=re.escape(expected_msg)):\n             assert_allclose(x, y, atol=3)\n-        msgs = str(exc_info.value).split('\\n')\n-        assert_equal(msgs[4], 'Max absolute difference among violations: 4')\n \n \n class TestArrayAlmostEqualNulp:\n@@ -1479,19 +1414,19 @@ def test_nan(self):\n             nzero = np.array([np.NZERO]).astype(dt)\n             assert_raises(AssertionError,\n                           lambda: assert_array_max_ulp(nan, inf,\n-                          maxulp=maxulp))\n+                                                       maxulp=maxulp))\n             assert_raises(AssertionError,\n                           lambda: assert_array_max_ulp(nan, big,\n-                          maxulp=maxulp))\n+                                                       maxulp=maxulp))\n             assert_raises(AssertionError,\n                           lambda: assert_array_max_ulp(nan, tiny,\n-                          maxulp=maxulp))\n+                                                       maxulp=maxulp))\n             assert_raises(AssertionError,\n                           lambda: assert_array_max_ulp(nan, zero,\n-                          maxulp=maxulp))\n+                                                       maxulp=maxulp))\n             assert_raises(AssertionError,\n                           lambda: assert_array_max_ulp(nan, nzero,\n-                          maxulp=maxulp))\n+                                                       maxulp=maxulp))\n \n \n class TestStringEqual:\n@@ -1557,8 +1492,8 @@ class mod:\n     # attribute should be present\n     class mod:\n         def __init__(self):\n-            self.__warningregistry__ = {'warning1':1,\n-                                        'warning2':2}\n+            self.__warningregistry__ = {'warning1': 1,\n+                                        'warning2': 2}\n \n     mod_inst = mod()\n     assert_warn_len_equal(mod=mod_inst,\n@@ -1711,7 +1646,7 @@ def test_suppress_warnings_record():\n \n         assert_equal(len(sup.log), 2)\n         assert_equal(len(log1), 1)\n-        assert_equal(len(log2),1)\n+        assert_equal(len(log2), 1)\n         assert_equal(log2[0].message.args[0], 'Some other warning 2')\n \n     # Do it again, with the same context to see if some warnings survived:\n@@ -1835,6 +1770,7 @@ def test_clear_and_catch_warnings_inherit():\n @pytest.mark.skipif(not HAS_REFCOUNT, reason=\"Python lacks refcounts\")\n class TestAssertNoGcCycles:\n     \"\"\" Test assert_no_gc_cycles \"\"\"\n+\n     def test_passes(self):\n         def no_cycle():\n             b = []\n",
            "comment_added_diff": {
                "421": "        # gh-12942"
            },
            "comment_deleted_diff": {
                "430": "        #gh-12942"
            },
            "comment_modified_diff": {}
        },
        {
            "commit": "85850071115ea36ecafd43c85ca48807406c2fdd",
            "timestamp": "2023-08-21T20:42:18+08:00",
            "author": "ellaella12",
            "commit_message": "Uncomment tests",
            "additions": 4,
            "deletions": 4,
            "change_type": "MODIFY",
            "diff": "@@ -211,10 +211,10 @@ def __ne__(self, other):\n         with pytest.raises(AssertionError) as exc_info:\n             self._test_equal(b, c)\n         msg = str(exc_info.value)\n-        # assert_('Mismatched elements: 2 / 2 (100%)\\n'\n-        #         'Max absolute difference among violations: 2.\\n'\n-        #         'Max relative difference among violations: inf'\n-        #         in msg)\n+        assert_('Mismatched elements: 2 / 2 (100%)\\n'\n+                'Max absolute difference among violations: 2.\\n'\n+                'Max relative difference among violations: inf'\n+                in msg)\n \n     def test_subclass_that_does_not_implement_npall(self):\n         class MyArray(np.ndarray):\n",
            "comment_added_diff": {},
            "comment_deleted_diff": {
                "214": "        # assert_('Mismatched elements: 2 / 2 (100%)\\n'",
                "215": "        #         'Max absolute difference among violations: 2.\\n'",
                "216": "        #         'Max relative difference among violations: inf'",
                "217": "        #         in msg)"
            },
            "comment_modified_diff": {}
        },
        {
            "commit": "ace444596424846058d8bfaacace29cd7e0c849a",
            "timestamp": "2023-08-29T21:03:43+02:00",
            "author": "Mateusz Sok\u00f3\u0142",
            "commit_message": "API: Update lib.arraypad lib.arraysetops lib.ufunclike lib.utils namespaces",
            "additions": 17,
            "deletions": 25,
            "change_type": "MODIFY",
            "diff": "@@ -1,45 +1,37 @@\n-import sys\n import pytest\n \n import numpy as np\n-from numpy.core import arange\n-from numpy.testing import assert_, assert_equal, assert_raises_regex\n-import numpy.lib.utils as utils\n+from numpy.testing import assert_equal, assert_raises_regex\n+import numpy.lib._utils_impl as _utils_impl\n \n from io import StringIO\n \n \n-@pytest.mark.filterwarnings(\"ignore:.*safe_eval.*:DeprecationWarning\")\n-def test_safe_eval_nameconstant():\n-    # Test if safe_eval supports Python 3.4 _ast.NameConstant\n-    utils.safe_eval('None')\n-\n-\n class TestByteBounds:\n \n     def test_byte_bounds(self):\n         # pointer difference matches size * itemsize\n         # due to contiguity\n-        a = arange(12).reshape(3, 4)\n-        low, high = utils.byte_bounds(a)\n+        a = np.arange(12).reshape(3, 4)\n+        low, high = np.byte_bounds(a)\n         assert_equal(high - low, a.size * a.itemsize)\n \n     def test_unusual_order_positive_stride(self):\n-        a = arange(12).reshape(3, 4)\n+        a = np.arange(12).reshape(3, 4)\n         b = a.T\n-        low, high = utils.byte_bounds(b)\n+        low, high = np.byte_bounds(b)\n         assert_equal(high - low, b.size * b.itemsize)\n \n     def test_unusual_order_negative_stride(self):\n-        a = arange(12).reshape(3, 4)\n+        a = np.arange(12).reshape(3, 4)\n         b = a.T[::-1]\n-        low, high = utils.byte_bounds(b)\n+        low, high = np.byte_bounds(b)\n         assert_equal(high - low, b.size * b.itemsize)\n \n     def test_strided(self):\n-        a = arange(12)\n+        a = np.arange(12)\n         b = a[::2]\n-        low, high = utils.byte_bounds(b)\n+        low, high = np.byte_bounds(b)\n         # the largest pointer address is lost (even numbers only in the\n         # stride), and compensate addresses for striding by 2\n         assert_equal(high - low, b.size * 2 * b.itemsize - b.itemsize)\n@@ -62,7 +54,7 @@ def first_method():\n \n     def _has_method_heading(cls):\n         out = StringIO()\n-        utils.info(cls, output=out)\n+        np.info(cls, output=out)\n         return 'Methods:' in out.getvalue()\n \n     assert _has_method_heading(WithPublicMethods)\n@@ -76,7 +68,7 @@ def _compare_dtypes(dt1, dt2):\n     # structured dtype\n     dt = np.dtype([('l1', [('l2', np.dtype('S8', metadata={'msg': 'toto'}))])],\n                   metadata={'msg': 'titi'})\n-    dt_m = utils.drop_metadata(dt)\n+    dt_m = _utils_impl.drop_metadata(dt)\n     assert _compare_dtypes(dt, dt_m) is True\n     assert dt_m.metadata is None\n     assert dt_m['l1'].metadata is None\n@@ -86,21 +78,21 @@ def _compare_dtypes(dt1, dt2):\n     dt = np.dtype([('x', '<f8'), ('y', '<i4')],\n                   align=True,\n                   metadata={'msg': 'toto'})\n-    dt_m = utils.drop_metadata(dt)\n+    dt_m = _utils_impl.drop_metadata(dt)\n     assert _compare_dtypes(dt, dt_m) is True\n     assert dt_m.metadata is None\n \n     # subdtype\n     dt = np.dtype('8f',\n                   metadata={'msg': 'toto'})\n-    dt_m = utils.drop_metadata(dt)\n+    dt_m = _utils_impl.drop_metadata(dt)\n     assert _compare_dtypes(dt, dt_m) is True\n     assert dt_m.metadata is None\n \n     # scalar\n     dt = np.dtype('uint32',\n                   metadata={'msg': 'toto'})\n-    dt_m = utils.drop_metadata(dt)\n+    dt_m = _utils_impl.drop_metadata(dt)\n     assert _compare_dtypes(dt, dt_m) is True\n     assert dt_m.metadata is None\n \n@@ -111,8 +103,8 @@ def _compare_dtypes(dt1, dt2):\n         np.dtype(\"10i\")])\n def test_drop_metadata_identity_and_copy(dtype):\n     # If there is no metadata, the identity is preserved:\n-    assert utils.drop_metadata(dtype) is dtype\n+    assert _utils_impl.drop_metadata(dtype) is dtype\n \n     # If there is any, it is dropped (subforms are checked above)\n     dtype = np.dtype(dtype, metadata={1: 2})\n-    assert utils.drop_metadata(dtype).metadata is None\n+    assert _utils_impl.drop_metadata(dtype).metadata is None\n",
            "comment_added_diff": {},
            "comment_deleted_diff": {
                "14": "    # Test if safe_eval supports Python 3.4 _ast.NameConstant"
            },
            "comment_modified_diff": {}
        },
        {
            "commit": "52db499e7130239da0a66812b4970c4a92af3e35",
            "timestamp": "2023-09-06T10:20:42+02:00",
            "author": "Mateusz Sok\u00f3\u0142",
            "commit_message": "API: Add lib.array_utils namespace",
            "additions": 5,
            "deletions": 29,
            "change_type": "MODIFY",
            "diff": "@@ -1,40 +1,16 @@\n import pytest\n \n import numpy as np\n-from numpy.testing import assert_equal, assert_raises_regex\n+from numpy.testing import assert_raises_regex\n import numpy.lib._utils_impl as _utils_impl\n \n from io import StringIO\n \n \n-class TestByteBounds:\n-\n-    def test_byte_bounds(self):\n-        # pointer difference matches size * itemsize\n-        # due to contiguity\n-        a = np.arange(12).reshape(3, 4)\n-        low, high = np.byte_bounds(a)\n-        assert_equal(high - low, a.size * a.itemsize)\n-\n-    def test_unusual_order_positive_stride(self):\n-        a = np.arange(12).reshape(3, 4)\n-        b = a.T\n-        low, high = np.byte_bounds(b)\n-        assert_equal(high - low, b.size * b.itemsize)\n-\n-    def test_unusual_order_negative_stride(self):\n-        a = np.arange(12).reshape(3, 4)\n-        b = a.T[::-1]\n-        low, high = np.byte_bounds(b)\n-        assert_equal(high - low, b.size * b.itemsize)\n-\n-    def test_strided(self):\n-        a = np.arange(12)\n-        b = a[::2]\n-        low, high = np.byte_bounds(b)\n-        # the largest pointer address is lost (even numbers only in the\n-        # stride), and compensate addresses for striding by 2\n-        assert_equal(high - low, b.size * 2 * b.itemsize - b.itemsize)\n+@pytest.mark.filterwarnings(\"ignore:.*safe_eval.*:DeprecationWarning\")\n+def test_safe_eval_nameconstant():\n+    # Test if safe_eval supports Python 3.4 _ast.NameConstant\n+    utils.safe_eval('None')\n \n \n def test_assert_raises_regex_context_manager():\n",
            "comment_added_diff": {
                "12": "    # Test if safe_eval supports Python 3.4 _ast.NameConstant"
            },
            "comment_deleted_diff": {
                "13": "        # pointer difference matches size * itemsize",
                "14": "        # due to contiguity",
                "35": "        # the largest pointer address is lost (even numbers only in the",
                "36": "        # stride), and compensate addresses for striding by 2"
            },
            "comment_modified_diff": {
                "12": "    def test_byte_bounds(self):"
            }
        },
        {
            "commit": "0ff4ac72af38c55015e024e45026bbf36a7efefe",
            "timestamp": "2023-09-06T10:22:38+02:00",
            "author": "Mateusz Sok\u00f3\u0142",
            "commit_message": "Fix after rebase",
            "additions": 0,
            "deletions": 6,
            "change_type": "MODIFY",
            "diff": "@@ -7,12 +7,6 @@\n from io import StringIO\n \n \n-@pytest.mark.filterwarnings(\"ignore:.*safe_eval.*:DeprecationWarning\")\n-def test_safe_eval_nameconstant():\n-    # Test if safe_eval supports Python 3.4 _ast.NameConstant\n-    utils.safe_eval('None')\n-\n-\n def test_assert_raises_regex_context_manager():\n     with assert_raises_regex(ValueError, 'no deprecation warning'):\n         raise ValueError('no deprecation warning')\n",
            "comment_added_diff": {},
            "comment_deleted_diff": {
                "12": "    # Test if safe_eval supports Python 3.4 _ast.NameConstant"
            },
            "comment_modified_diff": {}
        }
    ],
    "test_ccompiler_opt.py": [],
    "util.py": [
        {
            "commit": "08c6e9c142e619ac5175b6a13342ba2f2c571ddd",
            "timestamp": "2022-11-11T02:52:06-08:00",
            "author": "Hood Chatham",
            "commit_message": "TST: Skip tests that are not currently supported in wasm",
            "additions": 4,
            "deletions": 1,
            "change_type": "MODIFY",
            "diff": "@@ -20,7 +20,7 @@\n \n from pathlib import Path\n from numpy.compat import asbytes, asstr\n-from numpy.testing import temppath\n+from numpy.testing import temppath, IS_WASM\n from importlib import import_module\n \n #\n@@ -187,6 +187,9 @@ def _get_compiler_status():\n         return _compiler_status\n \n     _compiler_status = (False, False, False)\n+    if IS_WASM:\n+        # Can't run compiler from inside WASM.\n+        return _compiler_status\n \n     # XXX: this is really ugly. But I don't know how to invoke Distutils\n     #      in a safer way...\n",
            "comment_added_diff": {
                "191": "        # Can't run compiler from inside WASM."
            },
            "comment_deleted_diff": {},
            "comment_modified_diff": {}
        },
        {
            "commit": "608864613b801b9c85573186a9d07eeac5e7e465",
            "timestamp": "2023-01-26T14:29:13-05:00",
            "author": "DWesl",
            "commit_message": "TST: Rebase F2Py test modules on Cygwin.\n\nLet's see if this fixes the 8-50 fork failures.",
            "additions": 16,
            "deletions": 0,
            "change_type": "MODIFY",
            "diff": "@@ -30,6 +30,9 @@\n _module_dir = None\n _module_num = 5403\n \n+if sys.platform == \"cygwin\":\n+    _module_list = []\n+\n \n def _cleanup():\n     global _module_dir\n@@ -147,6 +150,19 @@ def build_module(source_files, options=[], skip=[], only=[], module_name=None):\n         for fn in dst_sources:\n             os.unlink(fn)\n \n+    # Rebase\n+    if sys.platform == \"cygwin\":\n+        # If someone starts deleting modules after import, this will\n+        # need to change to record how big each module is, rather than\n+        # relying on rebase being able to find that from the files.\n+        _module_list.extend(\n+            glob.glob(os.path.join(d, \"{:s}*\".format(module_name)))\n+        )\n+        subprocess.check_call(\n+            [\"/usr/bin/rebase\", \"--database\", \"--oblivious\", \"--verbose\"]\n+            + _module_list\n+        )\n+\n     # Import\n     return import_module(module_name)\n \n",
            "comment_added_diff": {
                "153": "    # Rebase",
                "155": "        # If someone starts deleting modules after import, this will",
                "156": "        # need to change to record how big each module is, rather than",
                "157": "        # relying on rebase being able to find that from the files."
            },
            "comment_deleted_diff": {},
            "comment_modified_diff": {}
        },
        {
            "commit": "015ecf60a0402074c37821f8019f10fce78143ba",
            "timestamp": "2023-01-27T21:26:16-05:00",
            "author": "DWesl",
            "commit_message": "Revert \"TST: Rebase F2Py test modules on Cygwin.\"\n\nThis reverts commit 608864613b801b9c85573186a9d07eeac5e7e465.",
            "additions": 0,
            "deletions": 16,
            "change_type": "MODIFY",
            "diff": "@@ -31,9 +31,6 @@\n _module_dir = None\n _module_num = 5403\n \n-if sys.platform == \"cygwin\":\n-    _module_list = []\n-\n \n def _cleanup():\n     global _module_dir\n@@ -151,19 +148,6 @@ def build_module(source_files, options=[], skip=[], only=[], module_name=None):\n         for fn in dst_sources:\n             os.unlink(fn)\n \n-    # Rebase\n-    if sys.platform == \"cygwin\":\n-        # If someone starts deleting modules after import, this will\n-        # need to change to record how big each module is, rather than\n-        # relying on rebase being able to find that from the files.\n-        _module_list.extend(\n-            glob.glob(os.path.join(d, \"{:s}*\".format(module_name)))\n-        )\n-        subprocess.check_call(\n-            [\"/usr/bin/rebase\", \"--database\", \"--oblivious\", \"--verbose\"]\n-            + _module_list\n-        )\n-\n     # Import\n     return import_module(module_name)\n \n",
            "comment_added_diff": {},
            "comment_deleted_diff": {
                "154": "    # Rebase",
                "156": "        # If someone starts deleting modules after import, this will",
                "157": "        # need to change to record how big each module is, rather than",
                "158": "        # relying on rebase being able to find that from the files."
            },
            "comment_modified_diff": {}
        },
        {
            "commit": "2fa4441529747b2ff9c712f5c102267888da1f80",
            "timestamp": "2023-01-29T11:50:51-05:00",
            "author": "DWesl",
            "commit_message": "TST: Rebase F2Py-built extension modules.\n\nAlso adjust CI so they don't immediately collide with NumPy.\nI forgot to do that last time, which caused problems.",
            "additions": 19,
            "deletions": 0,
            "change_type": "MODIFY",
            "diff": "@@ -6,6 +6,7 @@\n - determining paths to tests\n \n \"\"\"\n+import glob\n import os\n import sys\n import subprocess\n@@ -30,6 +31,9 @@\n _module_dir = None\n _module_num = 5403\n \n+if sys.platform == \"cygwin\":\n+    _module_list = []\n+\n \n def _cleanup():\n     global _module_dir\n@@ -147,6 +151,21 @@ def build_module(source_files, options=[], skip=[], only=[], module_name=None):\n         for fn in dst_sources:\n             os.unlink(fn)\n \n+    # Rebase (Cygwin-only)\n+    if sys.platform == \"cygwin\":\n+        # If someone starts deleting modules after import, this will\n+        # need to change to record how big each module is, rather than\n+        # relying on rebase being able to find that from the files.\n+        _module_list.extend(\n+            glob.glob(os.path.join(d, \"{:s}*\".format(module_name)))\n+        )\n+        subprocess.check_call(\n+            [\"/usr/bin/rebase\", \"--database\", \"--oblivious\", \"--verbose\"]\n+            + _module_list\n+        )\n+\n+\n+\n     # Import\n     return import_module(module_name)\n \n",
            "comment_added_diff": {
                "154": "    # Rebase (Cygwin-only)",
                "156": "        # If someone starts deleting modules after import, this will",
                "157": "        # need to change to record how big each module is, rather than",
                "158": "        # relying on rebase being able to find that from the files."
            },
            "comment_deleted_diff": {},
            "comment_modified_diff": {}
        }
    ],
    "postprocess.py": [],
    "preprocess.py": [],
    "gen_features.py": [],
    "genapi.py": [
        {
            "commit": "4002a7d421ff10780c28a3643683af7a9754f87f",
            "timestamp": "2022-11-25T12:37:46+01:00",
            "author": "Ralf Gommers",
            "commit_message": "BLD: enable building NumPy with Meson\n\nThis enables building with NumPy on Linux and macOS. Windows support\nshould be complete to, but is untested as of now and may need a few\ntweaks. This contains:\n- A set of `meson.build` files and related code generation script\n  tweaks, header templates, etc.\n- One CI job on Linux\n- Basic docs on using Meson to build NumPy (not yet integrated in the\n  html docs, it's too early for that - this is for early adopters right\n  now).\n\nThe build should be complete, with the major exception of SIMD support.\nThe full test suite passes. See gh-22546 for the tracking issue with\ndetailed notes on the plan for switching NumPy to Meson as its build\nsystem.\n\nCo-authored-by: Stefan van der Walt <stefanv@berkeley.edu>",
            "additions": 20,
            "deletions": 2,
            "change_type": "MODIFY",
            "diff": "@@ -6,17 +6,35 @@\n specified.\n \n \"\"\"\n-from numpy.distutils.conv_template import process_file as process_c_file\n-\n import hashlib\n import io\n import os\n import re\n import sys\n import textwrap\n+import importlib.util\n \n from os.path import join\n \n+\n+def get_processor():\n+    # Convoluted because we can't import from numpy.distutils\n+    # (numpy is not yet built)\n+    conv_template_path = os.path.join(\n+        os.path.dirname(__file__),\n+        '..', '..', 'distutils', 'conv_template.py'\n+    )\n+    spec = importlib.util.spec_from_file_location(\n+        'conv_template', conv_template_path\n+    )\n+    mod = importlib.util.module_from_spec(spec)\n+    spec.loader.exec_module(mod)\n+    return mod.process_file\n+\n+\n+process_c_file = get_processor()\n+\n+\n __docformat__ = 'restructuredtext'\n \n # The files under src/ that are scanned for API functions\n",
            "comment_added_diff": {
                "21": "    # Convoluted because we can't import from numpy.distutils",
                "22": "    # (numpy is not yet built)"
            },
            "comment_deleted_diff": {},
            "comment_modified_diff": {}
        },
        {
            "commit": "3a811358830c324b7b6819b88dec4e4bcd91444a",
            "timestamp": "2023-04-04T17:17:17+02:00",
            "author": "Sebastian Berg",
            "commit_message": "ENH: Allow compiling compatibly to old NumPy versions\n\nThe default compiles compatibly with 1.17.x, we allow going back to\n1.15 (mainly because it is easy).\n\nThere were few additions in this time, a few structs grew and very\nfew API functions were added.  Added a way to mark API functions\nas requiring a specific target version.\n\nIf a user wishes to use the *new* API, they have to add the definition:\n\n    #define NPY_TARGET_VERSION NPY_1_22_API_VERSION\n\nBefore importing NumPy.  (Our version numbering is a bit funny\nI first thought to use a hex version of the main NumPy version,\nbut since we already have the `NPY_1_22_API_VERSION` defines...)",
            "additions": 50,
            "deletions": 8,
            "change_type": "MODIFY",
            "diff": "@@ -12,6 +12,7 @@\n import re\n import sys\n import importlib.util\n+import textwrap\n \n from os.path import join\n \n@@ -98,6 +99,33 @@ def _repl(str):\n     return str.replace('Bool', 'npy_bool')\n \n \n+class MinVersion:\n+    def __init__(self, version):\n+        \"\"\" Version should be the normal NumPy version, e.g. \"1.25\" \"\"\"\n+        major, minor = version.split(\".\")\n+        self.version = f\"NPY_{major}_{minor}_API_VERSION\"\n+\n+    def __str__(self):\n+        # Used by version hashing:\n+        return self.version\n+\n+    def add_guard(self, name, normal_define):\n+        \"\"\"Wrap a definition behind a version guard\"\"\"\n+        numpy_target_help_pointer = (\n+            \"(Old_NumPy_target see also: \"\n+            \"https://numpy.org/devdocs/dev/depending_on_numpy.html)\")\n+\n+        wrap = textwrap.dedent(f\"\"\"\n+            #if NPY_FEATURE_VERSION < {self.version}\n+                #define {name} {numpy_target_help_pointer}\n+            #else\n+            {{define}}\n+            #endif\"\"\")\n+\n+        # we only insert `define` later to avoid confusing dedent:\n+        return wrap.format(define=normal_define)\n+\n+\n class StealRef:\n     def __init__(self, arg):\n         self.arg = arg # counting from 1\n@@ -391,7 +419,20 @@ class FunctionApi:\n     def __init__(self, name, index, annotations, return_type, args, api_name):\n         self.name = name\n         self.index = index\n-        self.annotations = annotations\n+\n+        self.min_version = None\n+        self.annotations = []\n+        for annotation in annotations:\n+            # String checks, because manual import breaks isinstance\n+            if type(annotation).__name__ == \"StealRef\":\n+                self.annotations.append(annotation)\n+            elif type(annotation).__name__ == \"MinVersion\":\n+                if self.min_version is not None:\n+                    raise ValueError(\"Two minimum versions specified!\")\n+                self.min_version = annotation\n+            else:\n+                raise ValueError(f\"unknown annotation {annotation}\")\n+\n         self.return_type = return_type\n         self.args = args\n         self.api_name = api_name\n@@ -403,13 +444,14 @@ def _argtypes_string(self):\n         return argstr\n \n     def define_from_array_api_string(self):\n-        define = \"\"\"\\\n-#define %s \\\\\\n        (*(%s (*)(%s)) \\\\\n-         %s[%d])\"\"\" % (self.name,\n-                                self.return_type,\n-                                self._argtypes_string(),\n-                                self.api_name,\n-                                self.index)\n+        arguments = self._argtypes_string()\n+        define = textwrap.dedent(f\"\"\"\\\n+            #define {self.name} \\\\\n+                    (*({self.return_type} (*)({arguments})) \\\\\n+                {self.api_name}[{self.index}])\"\"\")\n+\n+        if self.min_version is not None:\n+            define = self.min_version.add_guard(self.name, define)\n         return define\n \n     def array_api_define(self):\n",
            "comment_added_diff": {
                "109": "        # Used by version hashing:",
                "119": "            #if NPY_FEATURE_VERSION < {self.version}",
                "120": "                #define {name} {numpy_target_help_pointer}",
                "121": "            #else",
                "123": "            #endif\"\"\")",
                "125": "        # we only insert `define` later to avoid confusing dedent:",
                "426": "            # String checks, because manual import breaks isinstance",
                "449": "            #define {self.name} \\\\"
            },
            "comment_deleted_diff": {
                "407": "#define %s \\\\\\n        (*(%s (*)(%s)) \\\\"
            },
            "comment_modified_diff": {}
        },
        {
            "commit": "1a2a1142198db34bd48532f3553c2db97a0f3985",
            "timestamp": "2023-04-20T11:29:59+02:00",
            "author": "Sebastian Berg",
            "commit_message": "MAINT,DOC: Update based on Ralf's review",
            "additions": 1,
            "deletions": 7,
            "change_type": "MODIFY",
            "diff": "@@ -111,14 +111,8 @@ def __str__(self):\n \n     def add_guard(self, name, normal_define):\n         \"\"\"Wrap a definition behind a version guard\"\"\"\n-        numpy_target_help_pointer = (\n-            \"(Old_NumPy_target_see_depending_on_numpy__\"\n-            \"https://numpy.org/devdocs/dev/depending_on_numpy.html)\")\n-\n         wrap = textwrap.dedent(f\"\"\"\n-            #if NPY_FEATURE_VERSION < {self.version}\n-                #define {name} {numpy_target_help_pointer}\n-            #else\n+            #if NPY_FEATURE_VERSION >= {self.version}\n             {{define}}\n             #endif\"\"\")\n \n",
            "comment_added_diff": {
                "115": "            #if NPY_FEATURE_VERSION >= {self.version}"
            },
            "comment_deleted_diff": {
                "119": "            #if NPY_FEATURE_VERSION < {self.version}",
                "120": "                #define {name} {numpy_target_help_pointer}",
                "121": "            #else"
            },
            "comment_modified_diff": {
                "115": "            \"(Old_NumPy_target_see_depending_on_numpy__\""
            }
        },
        {
            "commit": "bf66ff05751faabd95f97af4975961c05d0280a2",
            "timestamp": "2023-06-11T20:31:38+02:00",
            "author": "Sebastian Berg",
            "commit_message": "MAINT: Allow the API table to have `__unused_indices__`\n\nSince we force users to compile against NumPy 2.0 (if they wish to\nsupport NumPy 2.0+), it is OK to remove things from the API Table.\n\nAllow doing so!\n\n(note that NEP 53 suggests introducing `numpy2_compat`.  This would\nallow fully replacing the table, for now, lets see how far we can\nget without it though!",
            "additions": 13,
            "deletions": 4,
            "change_type": "MODIFY",
            "diff": "@@ -476,7 +476,11 @@ def merge_api_dicts(dicts):\n     return ret\n \n def check_api_dict(d):\n-    \"\"\"Check that an api dict is valid (does not use the same index twice).\"\"\"\n+    \"\"\"Check that an api dict is valid (does not use the same index twice)\n+    and removed `__unused_indices__` from it (which is important only here)\n+    \"\"\"\n+    # Pop the `__unused_indices__` field:  These are known holes:\n+    removed = set(d.pop(\"__unused_indices__\", []))\n     # remove the extra value fields that aren't the index\n     index_d = {k: v[0] for k, v in d.items()}\n \n@@ -501,9 +505,12 @@ def check_api_dict(d):\n \n     # No 'hole' in the indexes may be allowed, and it must starts at 0\n     indexes = set(index_d.values())\n-    expected = set(range(len(indexes)))\n-    if indexes != expected:\n-        diff = expected.symmetric_difference(indexes)\n+    expected = set(range(len(indexes) + len(removed)))\n+    if not indexes.isdisjoint(removed):\n+        raise ValueError(\"API index used but marked unused: \"\n+                         f\"{indexes.intersection(removed)}\")\n+    if indexes.union(removed) != expected:\n+        diff = expected.symmetric_difference(indexes.union(removed))\n         msg = \"There are some holes in the API indexing: \" \\\n               \"(symmetric diff is %s)\" % diff\n         raise ValueError(msg)\n@@ -522,6 +529,8 @@ def fullapi_hash(api_dicts):\n     of the list of items in the API (as a string).\"\"\"\n     a = []\n     for d in api_dicts:\n+        d = d.copy()\n+        d.pop(\"__unused_indices__\", None)\n         for name, data in order_dict(d):\n             a.extend(name)\n             a.extend(','.join(map(str, data)))\n",
            "comment_added_diff": {
                "482": "    # Pop the `__unused_indices__` field:  These are known holes:"
            },
            "comment_deleted_diff": {},
            "comment_modified_diff": {}
        }
    ],
    "test_cpu_features.py": [
        {
            "commit": "76b31b760e8b4c23fa8229987360962f83cca2e3",
            "timestamp": "2023-04-08T21:27:25+03:00",
            "author": "Meekail Zain",
            "commit_message": "ENH: add NPY_ENABLE_CPU_FEATURES to allow limiting set of enabled features",
            "additions": 216,
            "deletions": 2,
            "change_type": "MODIFY",
            "diff": "@@ -1,5 +1,14 @@\n import sys, platform, re, pytest\n-from numpy.core._multiarray_umath import __cpu_features__\n+from numpy.core._multiarray_umath import (\n+    __cpu_features__,\n+    __cpu_baseline__,\n+    __cpu_dispatch__,\n+)\n+import numpy as np\n+import subprocess\n+import pathlib\n+import os\n+import re\n \n def assert_features_equal(actual, desired, fname):\n     __tracebackhide__ = True  # Hide traceback for py.test\n@@ -48,6 +57,10 @@ def assert_features_equal(actual, desired, fname):\n         \"%s\"\n     ) % (fname, actual, desired, error_report))\n \n+def _text_to_list(txt):\n+    out = txt.strip(\"][\\n\").replace(\"'\", \"\").split(', ')\n+    return None if out[0] == \"\" else out\n+\n class AbstractTest:\n     features = []\n     features_groups = {}\n@@ -92,7 +105,6 @@ def get_cpuinfo_item(self, magic_key):\n         return values\n \n     def load_flags_auxv(self):\n-        import subprocess\n         auxv = subprocess.check_output(['/bin/true'], env=dict(LD_SHOW_AUXV=\"1\"))\n         for at in auxv.split(b'\\n'):\n             if not at.startswith(b\"AT_HWCAP\"):\n@@ -103,6 +115,208 @@ def load_flags_auxv(self):\n                     hwcap_value[1].upper().decode().split()\n                 )\n \n+@pytest.mark.skipif(\n+    sys.platform == 'emscripten',\n+    reason= (\n+        \"The subprocess module is not available on WASM platforms and\"\n+        \" therefore this test class cannot be properly executed.\"\n+    ),\n+)\n+class TestEnvPrivation:\n+    cwd = pathlib.Path(__file__).parent.resolve()\n+    env = os.environ.copy()\n+    _enable = os.environ.pop('NPY_ENABLE_CPU_FEATURES', None)\n+    _disable = os.environ.pop('NPY_DISABLE_CPU_FEATURES', None)\n+    SUBPROCESS_ARGS = dict(cwd=cwd, capture_output=True, text=True, check=True)\n+    unavailable_feats = [\n+        feat for feat in __cpu_dispatch__ if not __cpu_features__[feat]\n+    ]\n+    UNAVAILABLE_FEAT = (\n+        None if len(unavailable_feats) == 0\n+        else unavailable_feats[0]\n+    )\n+    BASELINE_FEAT = None if len(__cpu_baseline__) == 0 else __cpu_baseline__[0]\n+    SCRIPT = \"\"\"\n+def main():\n+    from numpy.core._multiarray_umath import __cpu_features__, __cpu_dispatch__\n+\n+    detected = [feat for feat in __cpu_dispatch__ if __cpu_features__[feat]]\n+    print(detected)\n+\n+if __name__ == \"__main__\":\n+    main()\n+    \"\"\"\n+\n+    @pytest.fixture(autouse=True)\n+    def setup_class(self, tmp_path_factory):\n+        file = tmp_path_factory.mktemp(\"runtime_test_script\")\n+        file /= \"_runtime_detect.py\"\n+        file.write_text(self.SCRIPT)\n+        self.file = file\n+        return\n+\n+    def _run(self):\n+        return subprocess.run(\n+            [sys.executable, self.file],\n+            env=self.env,\n+            **self.SUBPROCESS_ARGS,\n+            )\n+\n+    # Helper function mimicing pytest.raises for subprocess call\n+    def _expect_error(\n+        self,\n+        msg,\n+        err_type,\n+        no_error_msg=\"Failed to generate error\"\n+    ):\n+        try:\n+            self._run()\n+        except subprocess.CalledProcessError as e:\n+            assertion_message = f\"Expected: {msg}\\nGot: {e.stderr}\"\n+            assert re.search(msg, e.stderr), assertion_message\n+\n+            assertion_message = (\n+                f\"Expected error of type: {err_type}; see full \"\n+                f\"error:\\n{e.stderr}\"\n+            )\n+            assert re.search(err_type, e.stderr), assertion_message\n+        else:\n+            assert False, no_error_msg\n+\n+    def setup_method(self):\n+        \"\"\"Ensure that the environment is reset\"\"\"\n+        self.env = os.environ.copy()\n+        return\n+\n+    def test_runtime_feature_selection(self):\n+        \"\"\"\n+        Ensure that when selecting `NPY_ENABLE_CPU_FEATURES`, only the\n+        features exactly specified are dispatched.\n+        \"\"\"\n+\n+        # Capture runtime-enabled features\n+        out = self._run()\n+        non_baseline_features = _text_to_list(out.stdout)\n+\n+        if non_baseline_features is None:\n+            pytest.skip(\n+                \"No dispatchable features outside of baseline detected.\"\n+            )\n+        feature = non_baseline_features[0]\n+\n+        # Capture runtime-enabled features when `NPY_ENABLE_CPU_FEATURES` is\n+        # specified\n+        self.env['NPY_ENABLE_CPU_FEATURES'] = feature\n+        out = self._run()\n+        enabled_features = _text_to_list(out.stdout)\n+\n+        # Ensure that only one feature is enabled, and it is exactly the one\n+        # specified by `NPY_ENABLE_CPU_FEATURES`\n+        assert set(enabled_features) == {feature}\n+\n+        if len(non_baseline_features) < 2:\n+            pytest.skip(\"Only one non-baseline feature detected.\")\n+        # Capture runtime-enabled features when `NPY_ENABLE_CPU_FEATURES` is\n+        # specified\n+        self.env['NPY_ENABLE_CPU_FEATURES'] = \",\".join(non_baseline_features)\n+        out = self._run()\n+        enabled_features = _text_to_list(out.stdout)\n+\n+        # Ensure that both features are enabled, and they are exactly the ones\n+        # specified by `NPY_ENABLE_CPU_FEATURES`\n+        assert set(enabled_features) == set(non_baseline_features)\n+        return\n+\n+    @pytest.mark.parametrize(\"enabled, disabled\",\n+    [\n+        (\"feature\", \"feature\"),\n+        (\"feature\", \"same\"),\n+    ])\n+    def test_both_enable_disable_set(self, enabled, disabled):\n+        \"\"\"\n+        Ensure that when both environment variables are set then an\n+        ImportError is thrown\n+        \"\"\"\n+        self.env['NPY_ENABLE_CPU_FEATURES'] = enabled\n+        self.env['NPY_DISABLE_CPU_FEATURES'] = disabled\n+        msg = \"Both NPY_DISABLE_CPU_FEATURES and NPY_ENABLE_CPU_FEATURES\"\n+        err_type = \"ImportError\"\n+        self._expect_error(msg, err_type)\n+\n+    @pytest.mark.skipif(\n+        not __cpu_dispatch__,\n+        reason=(\n+            \"NPY_*_CPU_FEATURES only parsed if \"\n+            \"`__cpu_dispatch__` is non-empty\"\n+        )\n+    )\n+    @pytest.mark.parametrize(\"action\", [\"ENABLE\", \"DISABLE\"])\n+    def test_variable_too_long(self, action):\n+        \"\"\"\n+        Test that an error is thrown if the environment variables are too long\n+        to be processed. Current limit is 1024, but this may change later.\n+        \"\"\"\n+        MAX_VAR_LENGTH = 1024\n+        # Actual length is MAX_VAR_LENGTH + 1 due to null-termination\n+        self.env[f'NPY_{action}_CPU_FEATURES'] = \"t\" * MAX_VAR_LENGTH\n+        msg = (\n+            f\"Length of environment variable 'NPY_{action}_CPU_FEATURES' is \"\n+            f\"{MAX_VAR_LENGTH + 1}, only {MAX_VAR_LENGTH} accepted\"\n+        )\n+        err_type = \"RuntimeError\"\n+        self._expect_error(msg, err_type)\n+\n+    @pytest.mark.skipif(\n+        not __cpu_dispatch__,\n+        reason=(\n+            \"NPY_*_CPU_FEATURES only parsed if \"\n+            \"`__cpu_dispatch__` is non-empty\"\n+        )\n+    )\n+    def test_impossible_feature_disable(self):\n+        \"\"\"\n+        Test that a RuntimeError is thrown if an impossible feature-disabling\n+        request is made. This includes disabling a baseline feature.\n+        \"\"\"\n+\n+        if self.BASELINE_FEAT is None:\n+            pytest.skip(\"There are no unavailable features to test with\")\n+        bad_feature = self.BASELINE_FEAT\n+        self.env['NPY_DISABLE_CPU_FEATURES'] = bad_feature\n+        msg = (\n+            f\"You cannot disable CPU feature '{bad_feature}', since it is \"\n+            \"part of the baseline optimizations\"\n+        )\n+        err_type = \"RuntimeError\"\n+        self._expect_error(msg, err_type)\n+\n+    def test_impossible_feature_enable(self):\n+        \"\"\"\n+        Test that a RuntimeError is thrown if an impossible feature-enabling\n+        request is made. This includes enabling a feature not supported by the\n+        machine, or disabling a baseline optimization.\n+        \"\"\"\n+\n+        if self.UNAVAILABLE_FEAT is None:\n+            pytest.skip(\"There are no unavailable features to test with\")\n+        bad_feature = self.UNAVAILABLE_FEAT\n+        self.env['NPY_ENABLE_CPU_FEATURES'] = bad_feature\n+        msg = (\n+            f\"You cannot enable CPU features \\\\({bad_feature}\\\\), since \"\n+            \"they are not supported by your machine.\"\n+        )\n+        err_type = \"RuntimeError\"\n+        self._expect_error(msg, err_type)\n+\n+        # Ensure that only the bad feature gets reported\n+        feats = f\"{bad_feature}, {self.BASELINE_FEAT}\"\n+        self.env['NPY_ENABLE_CPU_FEATURES'] = feats\n+        msg = (\n+            f\"You cannot enable CPU features \\\\({bad_feature}\\\\), since they \"\n+            \"are not supported by your machine.\"\n+        )\n+        self._expect_error(msg, err_type)\n+\n is_linux = sys.platform.startswith('linux')\n is_cygwin = sys.platform.startswith('cygwin')\n machine  = platform.machine()\n",
            "comment_added_diff": {
                "165": "    # Helper function mimicing pytest.raises for subprocess call",
                "197": "        # Capture runtime-enabled features",
                "207": "        # Capture runtime-enabled features when `NPY_ENABLE_CPU_FEATURES` is",
                "208": "        # specified",
                "213": "        # Ensure that only one feature is enabled, and it is exactly the one",
                "214": "        # specified by `NPY_ENABLE_CPU_FEATURES`",
                "219": "        # Capture runtime-enabled features when `NPY_ENABLE_CPU_FEATURES` is",
                "220": "        # specified",
                "225": "        # Ensure that both features are enabled, and they are exactly the ones",
                "226": "        # specified by `NPY_ENABLE_CPU_FEATURES`",
                "260": "        # Actual length is MAX_VAR_LENGTH + 1 due to null-termination",
                "311": "        # Ensure that only the bad feature gets reported"
            },
            "comment_deleted_diff": {},
            "comment_modified_diff": {}
        },
        {
            "commit": "f9ff49e9daf20c2f1acf716b07d9c8d340240317",
            "timestamp": "2023-06-18T18:30:24+02:00",
            "author": "Dimitri Papadopoulos",
            "commit_message": "MAINT: Fix typos found by codespell",
            "additions": 1,
            "deletions": 1,
            "change_type": "MODIFY",
            "diff": "@@ -162,7 +162,7 @@ def _run(self):\n             **self.SUBPROCESS_ARGS,\n             )\n \n-    # Helper function mimicing pytest.raises for subprocess call\n+    # Helper function mimicking pytest.raises for subprocess call\n     def _expect_error(\n         self,\n         msg,\n",
            "comment_added_diff": {
                "165": "    # Helper function mimicking pytest.raises for subprocess call"
            },
            "comment_deleted_diff": {
                "165": "    # Helper function mimicing pytest.raises for subprocess call"
            },
            "comment_modified_diff": {
                "165": "    # Helper function mimicing pytest.raises for subprocess call"
            }
        }
    ],
    "test_longdouble.py": [
        {
            "commit": "20d397400d6325cff3decbba3d6195418e873237",
            "timestamp": "2023-02-01T18:43:05+11:00",
            "author": "Andrew Nelson",
            "commit_message": "WHL: musllinux wheels [wheel build]",
            "additions": 26,
            "deletions": 1,
            "change_type": "MODIFY",
            "diff": "@@ -1,10 +1,11 @@\n import warnings\n+import platform\n import pytest\n \n import numpy as np\n from numpy.testing import (\n     assert_, assert_equal, assert_raises, assert_warns, assert_array_equal,\n-    temppath,\n+    temppath, IS_MUSL\n     )\n from numpy.core.tests._locales import CommaDecimalPointLocale\n \n@@ -30,6 +31,10 @@ def test_scalar_extraction():\n # 0.1 not exactly representable in base 2 floating point.\n repr_precision = len(repr(np.longdouble(0.1)))\n # +2 from macro block starting around line 842 in scalartypes.c.src.\n+\n+\n+@pytest.mark.skipif(IS_MUSL,\n+                    reason=\"test flaky on musllinux\")\n @pytest.mark.skipif(LD_INFO.precision + 2 >= repr_precision,\n                     reason=\"repr precision not enough to show eps\")\n def test_repr_roundtrip():\n@@ -368,3 +373,23 @@ def test_longdouble_from_int(int_val):\n     True, False])\n def test_longdouble_from_bool(bool_val):\n     assert np.longdouble(bool_val) == np.longdouble(int(bool_val))\n+\n+\n+@pytest.mark.skipif(\n+    not (IS_MUSL and platform.machine() == \"x86_64\"),\n+    reason=\"only need to run on musllinux_x86_64\"\n+)\n+def test_musllinux_x86_64_signature():\n+    # this test may fail if you're emulating musllinux_x86_64 on a different\n+    # architecture, but should pass natively.\n+    known_sigs = [b'\\xcd\\xcc\\xcc\\xcc\\xcc\\xcc\\xcc\\xcc\\xfb\\xbf']\n+    sig = (np.longdouble(-1.0) / np.longdouble(10.0)\n+           ).newbyteorder('<').tobytes()[:10]\n+    assert sig in known_sigs\n+\n+\n+def test_eps_positive():\n+    # np.finfo('g').eps should be positive on all platforms. If this isn't true\n+    # then something may have gone wrong with the MachArLike, e.g. if\n+    # np.core.getlimits._discovered_machar didn't work properly\n+    assert np.finfo(np.longdouble).eps > 0.\n",
            "comment_added_diff": {
                "383": "    # this test may fail if you're emulating musllinux_x86_64 on a different",
                "384": "    # architecture, but should pass natively.",
                "392": "    # np.finfo('g').eps should be positive on all platforms. If this isn't true",
                "393": "    # then something may have gone wrong with the MachArLike, e.g. if",
                "394": "    # np.core.getlimits._discovered_machar didn't work properly"
            },
            "comment_deleted_diff": {},
            "comment_modified_diff": {}
        }
    ],
    "ccompiler.py": [
        {
            "commit": "8a32e35327acf2c2ca377658b6528a474565b2f3",
            "timestamp": "2023-04-08T11:13:23+10:00",
            "author": "Andrew Nelson",
            "commit_message": "BLD: remove -ftrapping-math on macosx_arm64",
            "additions": 8,
            "deletions": 1,
            "change_type": "MODIFY",
            "diff": "@@ -1,6 +1,7 @@\n import os\n import re\n import sys\n+import platform\n import shlex\n import time\n import subprocess\n@@ -394,8 +395,14 @@ def CCompiler_customize_cmd(self, cmd, ignore=()):\n     log.info('customize %s using %s' % (self.__class__.__name__,\n                                         cmd.__class__.__name__))\n \n-    if hasattr(self, 'compiler') and 'clang' in self.compiler[0]:\n+    if (\n+        hasattr(self, 'compiler') and\n+        'clang' in self.compiler[0] and\n+        not (platform.machine() == 'arm64' and sys.platform == 'darwin')\n+    ):\n         # clang defaults to a non-strict floating error point model.\n+        # However, '-ftrapping-math' is not currently supported (2022-04-08)\n+        # for macosx_arm64.\n         # Since NumPy and most Python libs give warnings for these, override:\n         self.compiler.append('-ftrapping-math')\n         self.compiler_so.append('-ftrapping-math')\n",
            "comment_added_diff": {
                "404": "        # However, '-ftrapping-math' is not currently supported (2022-04-08)",
                "405": "        # for macosx_arm64."
            },
            "comment_deleted_diff": {},
            "comment_modified_diff": {}
        },
        {
            "commit": "340149c538aa8234fcc778315bee149335b50cb7",
            "timestamp": "2023-04-10T08:38:17-06:00",
            "author": "Charles Harris",
            "commit_message": "MAINT: Fix wrong date in comment.",
            "additions": 1,
            "deletions": 1,
            "change_type": "MODIFY",
            "diff": "@@ -401,7 +401,7 @@ def CCompiler_customize_cmd(self, cmd, ignore=()):\n         not (platform.machine() == 'arm64' and sys.platform == 'darwin')\n     ):\n         # clang defaults to a non-strict floating error point model.\n-        # However, '-ftrapping-math' is not currently supported (2022-04-08)\n+        # However, '-ftrapping-math' is not currently supported (2023-04-08)\n         # for macosx_arm64.\n         # Since NumPy and most Python libs give warnings for these, override:\n         self.compiler.append('-ftrapping-math')\n",
            "comment_added_diff": {
                "404": "        # However, '-ftrapping-math' is not currently supported (2023-04-08)"
            },
            "comment_deleted_diff": {
                "404": "        # However, '-ftrapping-math' is not currently supported (2022-04-08)"
            },
            "comment_modified_diff": {
                "404": "        # However, '-ftrapping-math' is not currently supported (2022-04-08)"
            }
        }
    ],
    "build_ext.py": [
        {
            "commit": "9e144f7c1598221510d49d8c6b79c66dc000edf6",
            "timestamp": "2022-11-17T09:17:24-07:00",
            "author": "Matti Picus",
            "commit_message": "BLD: update OpenBLAS to 0.3.21 and clean up openblas download test (#22525)\n\n* BUILD: update OpenBLAS to 0.3.21 and clean up openblas download test\r\n\r\n* set LDFLAGS on windows64 like the openblaslib build does\r\n\r\n* use rtools compilers on windows when building wheels\r\n\r\n* fix typos\r\n\r\n* add rtools gfortran to PATH\r\n\r\n* use the openblas dll from the zip archive without rewrapping\r\n\r\n* typos\r\n\r\n* copy dll import library for 64-bit interfaces\r\n\r\n* revert many of the changes to azure-steps-windows.yaml, copy openblas better in wheels\r\n\r\n* fix wildcard copy\r\n\r\n* test OpenBLAS build worked with threadpoolctl\r\n\r\n* typos\r\n\r\n* install threadpoolctl where needed, use for loop to recursively copy\r\n\r\n* update macos OpenBLAS suffixes for newer gfortran hashes\r\n\r\n* use libgfortran5.dylib on macos\r\n\r\n* fix scripts\r\n\r\n* re-use gfortran install from MacPython/gfortran-install on macos\r\n\r\n* use pre-release version of delocate\r\n\r\n* fixes for wheel builds/tests\r\n\r\n* add debugging cruft for pypy+win, macos wheels\r\n\r\n* add DYLD_LIBRARY_PATH on macosx-x86_64\r\n\r\n* use 32-bit openblas interfaces for ppc64le tests\r\n\r\n* skip large_archive test that sometimes segfaults on PyPy+windows",
            "additions": 7,
            "deletions": 0,
            "change_type": "MODIFY",
            "diff": "@@ -586,6 +586,13 @@ def build_extension(self, ext):\n             # not using fcompiler linker\n             self._libs_with_msvc_and_fortran(\n                 fcompiler, libraries, library_dirs)\n+            if ext.runtime_library_dirs:\n+                # gcc adds RPATH to the link. On windows, copy the dll into\n+                # self.extra_dll_dir instead.\n+                for d in ext.runtime_library_dirs:\n+                    for f in glob(d + '/*.dll'):\n+                        copy_file(f, self.extra_dll_dir)\n+                ext.runtime_library_dirs = []\n \n         elif ext.language in ['f77', 'f90'] and fcompiler is not None:\n             linker = fcompiler.link_shared_object\n",
            "comment_added_diff": {
                "590": "                # gcc adds RPATH to the link. On windows, copy the dll into",
                "591": "                # self.extra_dll_dir instead."
            },
            "comment_deleted_diff": {},
            "comment_modified_diff": {}
        },
        {
            "commit": "16f73741f425fca39cb2a33f72e251432391f30b",
            "timestamp": "2023-08-21T01:42:41+04:00",
            "author": "Sayed Adel",
            "commit_message": "BUG: Fix meson build failure due to uncleaned inplace auto-generated dispatch config headers\n\n  Ensure that the distutils generated config files and wrapped sources,\n  derived from dispatch-able sources are consistently generated within the build directory\n  when the inplace build option is enabled.\n\n  This change is crucial to prevent conflicts with meson-generated config headers.\n  Given that `spin build --clean` does not remove these headers, which\n  requires cleaning up the numpy root via `git clean` otherwise\n  the build will fails.",
            "additions": 12,
            "deletions": 1,
            "change_type": "MODIFY",
            "diff": "@@ -458,7 +458,18 @@ def build_extension(self, ext):\n             dispatch_hpath = os.path.join(bsrc_dir, dispatch_hpath)\n             include_dirs.append(dispatch_hpath)\n \n-            copt_build_src = None if self.inplace else bsrc_dir\n+            # copt_build_src = None if self.inplace else bsrc_dir\n+            # Always generate the generated config files and\n+            # dispatch-able sources inside the build directory,\n+            # even if the build option `inplace` is enabled.\n+            # This approach prevents conflicts with Meson-generated\n+            # config headers. Since `spin build --clean` will not remove\n+            # these headers, they might overwrite the generated Meson headers,\n+            # causing compatibility issues. Maintaining separate directories\n+            # ensures compatibility between distutils dispatch config headers\n+            # and Meson headers, avoiding build disruptions.\n+            # See gh-24450 for more details.\n+            copt_build_src = bsrc_dir\n             for _srcs, _dst, _ext in (\n                 ((c_sources,), copt_c_sources, ('.dispatch.c',)),\n                 ((c_sources, cxx_sources), copt_cxx_sources,\n",
            "comment_added_diff": {
                "461": "            # copt_build_src = None if self.inplace else bsrc_dir",
                "462": "            # Always generate the generated config files and",
                "463": "            # dispatch-able sources inside the build directory,",
                "464": "            # even if the build option `inplace` is enabled.",
                "465": "            # This approach prevents conflicts with Meson-generated",
                "466": "            # config headers. Since `spin build --clean` will not remove",
                "467": "            # these headers, they might overwrite the generated Meson headers,",
                "468": "            # causing compatibility issues. Maintaining separate directories",
                "469": "            # ensures compatibility between distutils dispatch config headers",
                "470": "            # and Meson headers, avoiding build disruptions.",
                "471": "            # See gh-24450 for more details."
            },
            "comment_deleted_diff": {},
            "comment_modified_diff": {
                "461": "            copt_build_src = None if self.inplace else bsrc_dir"
            }
        }
    ],
    "build_src.py": [],
    "install.py": [],
    "ibm.py": [],
    "system_info.py": [
        {
            "commit": "7881b1ed6a7da47a08c84b57b6740fc73fa1736b",
            "timestamp": "2023-03-09T14:20:02+09:00",
            "author": "Kentaro Kawakami",
            "commit_message": "BLD: add SSL2 default path for Fujitsu C/C++ compiler",
            "additions": 30,
            "deletions": 1,
            "change_type": "MODIFY",
            "diff": "@@ -1334,12 +1334,41 @@ class blas_mkl_info(mkl_info):\n class ssl2_info(system_info):\n     section = 'ssl2'\n     dir_env_var = 'SSL2_DIR'\n-    _lib_ssl2 = ['fjlapacksve']\n+    # Multi-threaded version. Python itself must be built by Fujitsu compiler.\n+    _lib_ssl2 = ['fjlapackexsve']\n+    # Single-threaded version\n+    #_lib_ssl2 = ['fjlapacksve']\n+\n+    def get_tcsds_rootdir(self):\n+        tcsdsroot = os.environ.get('TCSDS_PATH', None)\n+        if tcsdsroot is not None:\n+            return tcsdsroot\n+        return None\n+\n+    def __init__(self):\n+        tcsdsroot = self.get_tcsds_rootdir()\n+        if tcsdsroot is None:\n+            system_info.__init__(self)\n+        else:\n+            system_info.__init__(\n+                self,\n+                default_lib_dirs=[os.path.join(tcsdsroot, 'lib64')],\n+                default_include_dirs=[os.path.join(tcsdsroot,\n+                    'clang-comp/include')])\n \n     def calc_info(self):\n+        tcsdsroot = self.get_tcsds_rootdir()\n+\n         lib_dirs = self.get_lib_dirs()\n+        if lib_dirs is None:\n+            lib_dirs = os.path.join(tcsdsroot, 'lib64')\n+\n         incl_dirs = self.get_include_dirs()\n+        if incl_dirs is None:\n+            incl_dirs = os.path.join(tcsdsroot, 'clang-comp/include')\n+\n         ssl2_libs = self.get_libs('ssl2_libs', self._lib_ssl2)\n+\n         info = self.check_libs2(lib_dirs, ssl2_libs)\n         if info is None:\n             return\n",
            "comment_added_diff": {
                "1337": "    # Multi-threaded version. Python itself must be built by Fujitsu compiler.",
                "1339": "    # Single-threaded version",
                "1340": "    #_lib_ssl2 = ['fjlapacksve']"
            },
            "comment_deleted_diff": {},
            "comment_modified_diff": {
                "1337": "    _lib_ssl2 = ['fjlapacksve']"
            }
        }
    ],
    "capi_maps.py": [
        {
            "commit": "44482a63c23bb8f6991533e3d6ecdefd6a2f9bc0",
            "timestamp": "2023-06-21T13:59:19+02:00",
            "author": "Sebastian Berg",
            "commit_message": "MAINT: Remove hardcoded f2py numeric/numarray compatibility switch\n\nMaybe users were supposed to monkey-patch?  In either case, numeric/numarray\nis far enough in the past that we can just delete this.",
            "additions": 32,
            "deletions": 76,
            "change_type": "MODIFY",
            "diff": "@@ -32,9 +32,6 @@\n ]\n \n \n-# Numarray and Numeric users should set this False\n-using_newcore = True\n-\n depargs = []\n lcb_map = {}\n lcb2_map = {}\n@@ -58,89 +55,48 @@\n             'string': 'string',\n             'character': 'bytes',\n             }\n+\n c2capi_map = {'double': 'NPY_DOUBLE',\n-              'float': 'NPY_FLOAT',\n-              'long_double': 'NPY_DOUBLE',           # forced casting\n-              'char': 'NPY_STRING',\n-              'unsigned_char': 'NPY_UBYTE',\n-              'signed_char': 'NPY_BYTE',\n-              'short': 'NPY_SHORT',\n-              'unsigned_short': 'NPY_USHORT',\n-              'int': 'NPY_INT',\n-              'unsigned': 'NPY_UINT',\n-              'long': 'NPY_LONG',\n-              'long_long': 'NPY_LONG',                # forced casting\n-              'complex_float': 'NPY_CFLOAT',\n-              'complex_double': 'NPY_CDOUBLE',\n-              'complex_long_double': 'NPY_CDOUBLE',   # forced casting\n-              'string': 'NPY_STRING',\n-              'character': 'NPY_CHAR'}\n-\n-# These new maps aren't used anywhere yet, but should be by default\n-#  unless building numeric or numarray extensions.\n-if using_newcore:\n-    c2capi_map = {'double': 'NPY_DOUBLE',\n-                  'float': 'NPY_FLOAT',\n-                  'long_double': 'NPY_LONGDOUBLE',\n-                  'char': 'NPY_BYTE',\n-                  'unsigned_char': 'NPY_UBYTE',\n-                  'signed_char': 'NPY_BYTE',\n-                  'short': 'NPY_SHORT',\n-                  'unsigned_short': 'NPY_USHORT',\n-                  'int': 'NPY_INT',\n-                  'unsigned': 'NPY_UINT',\n-                  'long': 'NPY_LONG',\n-                  'unsigned_long': 'NPY_ULONG',\n-                  'long_long': 'NPY_LONGLONG',\n-                  'unsigned_long_long': 'NPY_ULONGLONG',\n-                  'complex_float': 'NPY_CFLOAT',\n-                  'complex_double': 'NPY_CDOUBLE',\n-                  'complex_long_double': 'NPY_CDOUBLE',\n-                  'string': 'NPY_STRING',\n-                  'character': 'NPY_STRING'}\n+                'float': 'NPY_FLOAT',\n+                'long_double': 'NPY_LONGDOUBLE',\n+                'char': 'NPY_BYTE',\n+                'unsigned_char': 'NPY_UBYTE',\n+                'signed_char': 'NPY_BYTE',\n+                'short': 'NPY_SHORT',\n+                'unsigned_short': 'NPY_USHORT',\n+                'int': 'NPY_INT',\n+                'unsigned': 'NPY_UINT',\n+                'long': 'NPY_LONG',\n+                'unsigned_long': 'NPY_ULONG',\n+                'long_long': 'NPY_LONGLONG',\n+                'unsigned_long_long': 'NPY_ULONGLONG',\n+                'complex_float': 'NPY_CFLOAT',\n+                'complex_double': 'NPY_CDOUBLE',\n+                'complex_long_double': 'NPY_CDOUBLE',\n+                'string': 'NPY_STRING',\n+                'character': 'NPY_STRING'}\n \n c2pycode_map = {'double': 'd',\n                 'float': 'f',\n-                'long_double': 'd',                       # forced casting\n-                'char': '1',\n-                'signed_char': '1',\n-                'unsigned_char': 'b',\n-                'short': 's',\n-                'unsigned_short': 'w',\n+                'long_double': 'g',\n+                'char': 'b',\n+                'unsigned_char': 'B',\n+                'signed_char': 'b',\n+                'short': 'h',\n+                'unsigned_short': 'H',\n                 'int': 'i',\n-                'unsigned': 'u',\n+                'unsigned': 'I',\n                 'long': 'l',\n-                'long_long': 'L',\n+                'unsigned_long': 'L',\n+                'long_long': 'q',\n+                'unsigned_long_long': 'Q',\n                 'complex_float': 'F',\n                 'complex_double': 'D',\n-                'complex_long_double': 'D',               # forced casting\n-                'string': 'c',\n-                'character': 'c'\n-                }\n-\n-if using_newcore:\n-    c2pycode_map = {'double': 'd',\n-                    'float': 'f',\n-                    'long_double': 'g',\n-                    'char': 'b',\n-                    'unsigned_char': 'B',\n-                    'signed_char': 'b',\n-                    'short': 'h',\n-                    'unsigned_short': 'H',\n-                    'int': 'i',\n-                    'unsigned': 'I',\n-                    'long': 'l',\n-                    'unsigned_long': 'L',\n-                    'long_long': 'q',\n-                    'unsigned_long_long': 'Q',\n-                    'complex_float': 'F',\n-                    'complex_double': 'D',\n-                    'complex_long_double': 'G',\n-                    'string': 'S',\n-                    'character': 'c'}\n+                'complex_long_double': 'G',\n+                'string': 'S',\n+                'character': 'c'}\n \n # https://docs.python.org/3/c-api/arg.html#building-values\n-# c2buildvalue_map is NumPy agnostic, so no need to bother with using_newcore\n c2buildvalue_map = {'double': 'd',\n                     'float': 'f',\n                     'char': 'b',\n",
            "comment_added_diff": {},
            "comment_deleted_diff": {
                "35": "# Numarray and Numeric users should set this False",
                "63": "              'long_double': 'NPY_DOUBLE',           # forced casting",
                "72": "              'long_long': 'NPY_LONG',                # forced casting",
                "75": "              'complex_long_double': 'NPY_CDOUBLE',   # forced casting",
                "79": "# These new maps aren't used anywhere yet, but should be by default",
                "80": "#  unless building numeric or numarray extensions.",
                "104": "                'long_double': 'd',                       # forced casting",
                "116": "                'complex_long_double': 'D',               # forced casting",
                "143": "# c2buildvalue_map is NumPy agnostic, so no need to bother with using_newcore"
            },
            "comment_modified_diff": {}
        }
    ],
    "clapack_scrub.py": [
        {
            "commit": "a4c582d4b3c530176be393ac596dd00130367ddb",
            "timestamp": "2023-07-18T08:53:25+02:00",
            "author": "Dimitri Papadopoulos",
            "commit_message": "MAINT: Fix new or residual typos found by codespell",
            "additions": 1,
            "deletions": 1,
            "change_type": "MODIFY",
            "diff": "@@ -237,7 +237,7 @@ def removeSubroutinePrototypes(source):\n     # While we could \"fix\" this function to do what the name implies\n     # it should do, we have no hint of what it should really do.\n     #\n-    # Therefore we keep the existing (non-)functionaity, documenting\n+    # Therefore we keep the existing (non-)functionality, documenting\n     # this function as doing nothing at all.\n     return source\n \n",
            "comment_added_diff": {
                "240": "    # Therefore we keep the existing (non-)functionality, documenting"
            },
            "comment_deleted_diff": {
                "240": "    # Therefore we keep the existing (non-)functionaity, documenting"
            },
            "comment_modified_diff": {
                "240": "    # Therefore we keep the existing (non-)functionaity, documenting"
            }
        }
    ],
    "make_lite.py": [],
    "utils.py": [
        {
            "commit": "df3751b03d789ee04cac3a9a8a7f7f3aba58735c",
            "timestamp": "2023-01-20T16:03:28+01:00",
            "author": "Andrew Nelson",
            "commit_message": "CI: musllinux_x86_64 (#22864)\n\n[ci skip]",
            "additions": 15,
            "deletions": 1,
            "change_type": "MODIFY",
            "diff": "@@ -16,6 +16,7 @@\n from unittest.case import SkipTest\n from warnings import WarningMessage\n import pprint\n+import sysconfig\n \n import numpy as np\n from numpy.core import(\n@@ -36,7 +37,7 @@\n         'SkipTest', 'KnownFailureException', 'temppath', 'tempdir', 'IS_PYPY',\n         'HAS_REFCOUNT', \"IS_WASM\", 'suppress_warnings', 'assert_array_compare',\n         'assert_no_gc_cycles', 'break_cycles', 'HAS_LAPACK64', 'IS_PYSTON',\n-        '_OLD_PROMOTION'\n+        '_OLD_PROMOTION', 'IS_MUSL'\n         ]\n \n \n@@ -56,6 +57,19 @@ class KnownFailureException(Exception):\n \n _OLD_PROMOTION = lambda: np._get_promotion_state() == 'legacy'\n \n+IS_MUSL = False\n+try:\n+    from packaging.tags import sys_tags\n+    _tags = list(sys_tags())\n+    if 'musllinux' in _tags[0].platform:\n+        IS_MUSL = True\n+except ImportError:\n+    # fallback to sysconfig (might be flaky)\n+    # value could be None.\n+    v = sysconfig.get_config_var('HOST_GNU_TYPE') or ''\n+    if 'musl' in v:\n+        IS_MUSL = True\n+\n \n def assert_(val, msg=''):\n     \"\"\"\n",
            "comment_added_diff": {
                "67": "    # fallback to sysconfig (might be flaky)",
                "68": "    # value could be None."
            },
            "comment_deleted_diff": {},
            "comment_modified_diff": {}
        },
        {
            "commit": "d62280de7b4d7e0da26cbe3342ad85ed7562ef5b",
            "timestamp": "2023-01-21T17:06:44-07:00",
            "author": "Charles Harris",
            "commit_message": "DEP: Remove the deprecated utils.py shim.\n\nThe shim has been deprecated since 2019, the proper place to import\nutils funtions is directly from numpy.testing.",
            "additions": 0,
            "deletions": 29,
            "change_type": "DELETE",
            "diff": "@@ -1,29 +0,0 @@\n-\"\"\"\n-Back compatibility utils module. It will import the appropriate\n-set of tools\n-\n-\"\"\"\n-import warnings\n-\n-# 2018-04-04, numpy 1.15.0 ImportWarning\n-# 2019-09-18, numpy 1.18.0 DeprecatonWarning (changed)\n-warnings.warn(\"Importing from numpy.testing.utils is deprecated \"\n-              \"since 1.15.0, import from numpy.testing instead.\",\n-              DeprecationWarning, stacklevel=2)\n-\n-from ._private.utils import *\n-from ._private.utils import _assert_valid_refcount, _gen_alignment_data\n-\n-__all__ = [\n-        'assert_equal', 'assert_almost_equal', 'assert_approx_equal',\n-        'assert_array_equal', 'assert_array_less', 'assert_string_equal',\n-        'assert_array_almost_equal', 'assert_raises', 'build_err_msg',\n-        'decorate_methods', 'jiffies', 'memusage', 'print_assert_equal',\n-        'raises', 'rundocs', 'runstring', 'verbose', 'measure',\n-        'assert_', 'assert_array_almost_equal_nulp', 'assert_raises_regex',\n-        'assert_array_max_ulp', 'assert_warns', 'assert_no_warnings',\n-        'assert_allclose', 'IgnoreException', 'clear_and_catch_warnings',\n-        'SkipTest', 'KnownFailureException', 'temppath', 'tempdir', 'IS_PYPY',\n-        'HAS_REFCOUNT', 'suppress_warnings', 'assert_array_compare',\n-        'assert_no_gc_cycles'\n-        ]\n",
            "comment_added_diff": {},
            "comment_deleted_diff": {
                "8": "# 2018-04-04, numpy 1.15.0 ImportWarning",
                "9": "# 2019-09-18, numpy 1.18.0 DeprecatonWarning (changed)"
            },
            "comment_modified_diff": {}
        },
        {
            "commit": "700c0342a8dbfd5649e72b3ddfdd98f7c9cb58dc",
            "timestamp": "2023-03-07T18:04:10-05:00",
            "author": "warren",
            "commit_message": "DOC: Add 'may vary' markup in info() docstring.\n\n[skip actions] [skip travis] [skip cirrus]",
            "additions": 1,
            "deletions": 1,
            "change_type": "MODIFY",
            "diff": "@@ -587,7 +587,7 @@ def info(object=None, maxwidth=76, output=None, toplevel='numpy'):\n     aligned:  True\n     contiguous:  True\n     fortran:  False\n-    data pointer: 0x562b6e0d2860\n+    data pointer: 0x562b6e0d2860  # may vary\n     byteorder:  little\n     byteswap:  False\n     type: complex64\n",
            "comment_added_diff": {
                "590": "    data pointer: 0x562b6e0d2860  # may vary"
            },
            "comment_deleted_diff": {},
            "comment_modified_diff": {
                "590": "    data pointer: 0x562b6e0d2860"
            }
        },
        {
            "commit": "7698f9dbce84aca177787cb88979226c799e5c6c",
            "timestamp": "2023-04-27T13:25:11+02:00",
            "author": "ninousf",
            "commit_message": "ENH: rename dtype_without_metadata to drop_metadata, and do not use descr attribute",
            "additions": 27,
            "deletions": 0,
            "change_type": "MODIFY",
            "diff": "@@ -1140,4 +1140,31 @@ def _opt_info():\n             enabled_features += f\" {feature}?\"\n \n     return enabled_features\n+\n+def drop_metadata(dt):\n+    align = dt.isalignedstruct\n+    if dt.names:\n+        # structured dtype\n+        l = list()\n+        for n in dt.names:\n+            t = dt.fields[n]\n+            l.append((\n+                n,  # name\n+                drop_metadata(t[0]),  # format\n+                t[1],  # offset\n+                t[2] if len(t) == 3 else None,  # title\n+            ))\n+        d = {k: [e[i] for e in l] for i, k in enumerate(\n+            ['names', 'formats', 'offsets', 'titles']\n+        )}\n+        d['itemsize'] = dt.itemsize\n+    elif dt.subdtype:\n+        # subdtype\n+        (item_dtype, shape) = dt.subdtype\n+        d = (drop_metadata(item_dtype), shape)\n+    else:\n+        # scalar dtype\n+        d = dt.str\n+    new_dt = np.dtype(dtype=d, align=dt.isalignedstruct) \n+    return new_dt\n #-----------------------------------------------------------------------------\n",
            "comment_added_diff": {
                "1147": "        # structured dtype",
                "1152": "                n,  # name",
                "1153": "                drop_metadata(t[0]),  # format",
                "1154": "                t[1],  # offset",
                "1155": "                t[2] if len(t) == 3 else None,  # title",
                "1162": "        # subdtype",
                "1166": "        # scalar dtype"
            },
            "comment_deleted_diff": {},
            "comment_modified_diff": {}
        },
        {
            "commit": "d03741ff1646d6c80200c9e5a90448f45beb7599",
            "timestamp": "2023-04-27T13:25:11+02:00",
            "author": "ninousf",
            "commit_message": "STY: drop_metadata more readable",
            "additions": 5,
            "deletions": 7,
            "change_type": "MODIFY",
            "diff": "@@ -1150,13 +1150,11 @@ def drop_metadata(dt):\n         l = list()\n         for n in dt.names:\n             t = dt.fields[n]\n-            l.append((\n-                n,  # name\n-                drop_metadata(t[0]),  # format\n-                t[1],  # offset\n-                t[2] if len(t) == 3 else None,  # title\n-            ))\n-        d = {k: [e[i] for e in l] for i, k in enumerate(\n+            if len(t) == 3:\n+                l.append((n, drop_metadata(t[0]), t[1], t[2]))\n+            else:\n+                l.append((n, drop_metadata(t[0]), t[1], None))\n+            d = {k: [e[i] for e in l] for i, k in enumerate(\n             ['names', 'formats', 'offsets', 'titles']\n         )}\n         d['itemsize'] = dt.itemsize\n",
            "comment_added_diff": {},
            "comment_deleted_diff": {
                "1154": "                n,  # name",
                "1155": "                drop_metadata(t[0]),  # format",
                "1156": "                t[1],  # offset",
                "1157": "                t[2] if len(t) == 3 else None,  # title"
            },
            "comment_modified_diff": {}
        },
        {
            "commit": "b294b9f9b2ce52ef96708ac08dc21b2ef8d034b4",
            "timestamp": "2023-04-27T17:48:15+02:00",
            "author": "Sebastian Berg",
            "commit_message": "ENH: Refactor drop_metadata and use it for npy/npz saving",
            "additions": 56,
            "deletions": 24,
            "change_type": "MODIFY",
            "diff": "@@ -1141,30 +1141,62 @@ def _opt_info():\n \n     return enabled_features\n \n-def drop_metadata(dt):\n+\n+def drop_metadata(dtype, /):\n     \"\"\"\n-    Returns the copy of a dtype where all metadata has been removed\n+    Returns the dtype unchanged if it contained no metadata or a copy of the\n+    dtype it (or any of its structure dtypes) contained metadata.\n+\n+    .. note::\n+\n+        Due to its limitation this function may move to a more appropriate\n+        home or change in the future and is considered semi-public API only.\n+\n+    .. warning::\n+\n+        This function does not preserve more strange things like record dtypes\n+        and user dtypes may simply return the wrong thing.  If you need to be\n+        sure about the latter, check the result with:\n+        ``np.can_cast(new_dtype, dtype, casting=\"no\")``.\n+\n     \"\"\"\n-    if dt.names:\n-        # structured dtype\n-        l = list()\n-        for n in dt.names:\n-            t = dt.fields[n]\n-            if len(t) == 3:\n-                l.append((n, drop_metadata(t[0]), t[1], t[2]))\n-            else:\n-                l.append((n, drop_metadata(t[0]), t[1], None))\n-            d = {k: [e[i] for e in l] for i, k in enumerate(\n-            ['names', 'formats', 'offsets', 'titles']\n-        )}\n-        d['itemsize'] = dt.itemsize\n-    elif dt.subdtype:\n-        # subdtype\n-        (item_dtype, shape) = dt.subdtype\n-        d = (drop_metadata(item_dtype), shape)\n+    if dtype.fields is not None:\n+        found_metadata = dtype.metadata is not None\n+\n+        names = []\n+        formats = []\n+        offsets = []\n+        titles = []\n+        for name, field in dtype.fields.items():\n+            field_dt = drop_metadata(field[0])\n+            if field_dt is not field[0]:\n+                found_metadata = True\n+\n+            names.append(name)\n+            formats.append(field_dt)\n+            offsets.append(field[1])\n+            titles.append(None if len(field) < 3 else field[2])\n+\n+        if not found_metadata:\n+            return dtype\n+\n+        structure = dict(\n+            names=names, formats=formats, offsets=offsets, titles=titles,\n+            itemsize=dtype.itemsize)\n+\n+        # NOTE: Could pass (dtype.type, structure) to preserve record dtypes...\n+        return np.dtype(structure, align=dtype.isalignedstruct)\n+    elif dtype.subdtype is not None:\n+        # subarray dtype\n+        subdtype, shape = dtype.subdtype\n+        new_subdtype = drop_metadata(subdtype)\n+        if dtype.metadata is None and new_subdtype is subdtype:\n+            return dtype\n+\n+        return np.dtype((new_subdtype, shape))\n     else:\n-        # scalar dtype\n-        d = dt.str\n-    new_dt = np.dtype(dtype=d, align=dt.isalignedstruct) \n-    return new_dt\n-#-----------------------------------------------------------------------------\n+        # Normal unstructured dtype\n+        if dtype.metadata is None:\n+            return dtype\n+        # Note that `dt.str` doesn't round-trip e.g. for user-dtypes.\n+        return np.dtype(dtype.str)\n",
            "comment_added_diff": {
                "1187": "        # NOTE: Could pass (dtype.type, structure) to preserve record dtypes...",
                "1190": "        # subarray dtype",
                "1198": "        # Normal unstructured dtype",
                "1201": "        # Note that `dt.str` doesn't round-trip e.g. for user-dtypes."
            },
            "comment_deleted_diff": {
                "1149": "        # structured dtype",
                "1162": "        # subdtype",
                "1166": "        # scalar dtype",
                "1170": "#-----------------------------------------------------------------------------"
            },
            "comment_modified_diff": {}
        },
        {
            "commit": "57f5cf0501dac874dc2863e598272d7bf61f296e",
            "timestamp": "2023-05-03T13:11:01+02:00",
            "author": "Sebastian Berg",
            "commit_message": "MAINT: Remove gisnan, gisinf, and gisfinite from testing code\n\nThese were introduced many years ago when ufuncs were buggy and\ncould return NotImplemented sometimes.  This has been fixed for\nmany years, though.\n\nI suspect the errstate for `isfinite` is not required so removed it.\nIt was a 12+ year old work-around for warnings that really shouldn't\nhappen to begin with.  (The commit mentions `np.isinf(np.inf)` giving\na warning, which doesn't make sense, I think.)",
            "additions": 13,
            "deletions": 68,
            "change_type": "MODIFY",
            "diff": "@@ -21,6 +21,7 @@\n import numpy as np\n from numpy.core import (\n      intp, float32, empty, arange, array_repr, ndarray, isnat, array)\n+from numpy import isfinite, isnan, isinf\n import numpy.linalg.lapack_lite\n \n from io import StringIO\n@@ -91,62 +92,6 @@ def assert_(val, msg=''):\n         raise AssertionError(smsg)\n \n \n-def gisnan(x):\n-    \"\"\"like isnan, but always raise an error if type not supported instead of\n-    returning a TypeError object.\n-\n-    Notes\n-    -----\n-    isnan and other ufunc sometimes return a NotImplementedType object instead\n-    of raising any exception. This function is a wrapper to make sure an\n-    exception is always raised.\n-\n-    This should be removed once this problem is solved at the Ufunc level.\"\"\"\n-    from numpy.core import isnan\n-    st = isnan(x)\n-    if isinstance(st, type(NotImplemented)):\n-        raise TypeError(\"isnan not supported for this type\")\n-    return st\n-\n-\n-def gisfinite(x):\n-    \"\"\"like isfinite, but always raise an error if type not supported instead\n-    of returning a TypeError object.\n-\n-    Notes\n-    -----\n-    isfinite and other ufunc sometimes return a NotImplementedType object\n-    instead of raising any exception. This function is a wrapper to make sure\n-    an exception is always raised.\n-\n-    This should be removed once this problem is solved at the Ufunc level.\"\"\"\n-    from numpy.core import isfinite, errstate\n-    with errstate(invalid='ignore'):\n-        st = isfinite(x)\n-        if isinstance(st, type(NotImplemented)):\n-            raise TypeError(\"isfinite not supported for this type\")\n-    return st\n-\n-\n-def gisinf(x):\n-    \"\"\"like isinf, but always raise an error if type not supported instead of\n-    returning a TypeError object.\n-\n-    Notes\n-    -----\n-    isinf and other ufunc sometimes return a NotImplementedType object instead\n-    of raising any exception. This function is a wrapper to make sure an\n-    exception is always raised.\n-\n-    This should be removed once this problem is solved at the Ufunc level.\"\"\"\n-    from numpy.core import isinf, errstate\n-    with errstate(invalid='ignore'):\n-        st = isinf(x)\n-        if isinstance(st, type(NotImplemented)):\n-            raise TypeError(\"isinf not supported for this type\")\n-    return st\n-\n-\n if os.name == 'nt':\n     # Code \"stolen\" from enthought/debug/memusage.py\n     def GetPerformanceAttributes(object, counter, instance=None,\n@@ -390,8 +335,8 @@ def assert_equal(actual, desired, err_msg='', verbose=True):\n \n     # Inf/nan/negative zero handling\n     try:\n-        isdesnan = gisnan(desired)\n-        isactnan = gisnan(actual)\n+        isdesnan = isnan(desired)\n+        isactnan = isnan(actual)\n         if isdesnan and isactnan:\n             return  # both nan, so equal\n \n@@ -401,7 +346,7 @@ def assert_equal(actual, desired, err_msg='', verbose=True):\n         if (array_actual.dtype.char in 'Mm' or\n                 array_desired.dtype.char in 'Mm'):\n             # version 1.18\n-            # until this version, gisnan failed for datetime64 and timedelta64.\n+            # until this version, isnan failed for datetime64 and timedelta64.\n             # Now it succeeds but comparison to scalar with a different type\n             # emits a DeprecationWarning.\n             # Avoid that by skipping the next check\n@@ -582,9 +527,9 @@ def _build_err_msg():\n         # If one of desired/actual is not finite, handle it specially here:\n         # check that both are nan if any is a nan, and test for equality\n         # otherwise\n-        if not (gisfinite(desired) and gisfinite(actual)):\n-            if gisnan(desired) or gisnan(actual):\n-                if not (gisnan(desired) and gisnan(actual)):\n+        if not (isfinite(desired) and isfinite(actual)):\n+            if isnan(desired) or isnan(actual):\n+                if not (isnan(desired) and isnan(actual)):\n                     raise AssertionError(_build_err_msg())\n             else:\n                 if not desired == actual:\n@@ -683,9 +628,9 @@ def assert_approx_equal(actual, desired, significant=7, err_msg='',\n         # If one of desired/actual is not finite, handle it specially here:\n         # check that both are nan if any is a nan, and test for equality\n         # otherwise\n-        if not (gisfinite(desired) and gisfinite(actual)):\n-            if gisnan(desired) or gisnan(actual):\n-                if not (gisnan(desired) and gisnan(actual)):\n+        if not (isfinite(desired) and isfinite(actual)):\n+            if isnan(desired) or isnan(actual):\n+                if not (isnan(desired) and isnan(actual)):\n                     raise AssertionError(msg)\n             else:\n                 if not desired == actual:\n@@ -1066,9 +1011,9 @@ def assert_array_almost_equal(x, y, decimal=6, err_msg='', verbose=True):\n \n     def compare(x, y):\n         try:\n-            if npany(gisinf(x)) or npany(gisinf(y)):\n-                xinfid = gisinf(x)\n-                yinfid = gisinf(y)\n+            if npany(isinf(x)) or npany(isinf(y)):\n+                xinfid = isinf(x)\n+                yinfid = isinf(y)\n                 if not (xinfid == yinfid).all():\n                     return False\n                 # if one item, x and y is +- inf\n",
            "comment_added_diff": {
                "349": "            # until this version, isnan failed for datetime64 and timedelta64."
            },
            "comment_deleted_diff": {
                "404": "            # until this version, gisnan failed for datetime64 and timedelta64."
            },
            "comment_modified_diff": {}
        },
        {
            "commit": "6ac4d6ded2dded576a5af12820fa49a961130468",
            "timestamp": "2023-05-17T14:03:39+02:00",
            "author": "Sebastian Berg",
            "commit_message": "BUG: Fix median and quantile NaT handling\n\nNote that this doesn't mean that rounding is correct at least for\nquantiles, so there is some dubious about it being a good idea to\nuse this.\n\nBut, it does fix the issue, and I the `copyto` solution seems rather\ngood to me, the only thing that isn't ideal is the `supports_nan`\ndefinition itself.\n\nCloses gh-20376",
            "additions": 14,
            "deletions": 8,
            "change_type": "MODIFY",
            "diff": "@@ -1101,17 +1101,23 @@ def _median_nancheck(data, result, axis):\n     \"\"\"\n     if data.size == 0:\n         return result\n-    n = np.isnan(data.take(-1, axis=axis))\n-    # masked NaN values are ok\n+    potential_nans = data.take(-1, axis=axis)\n+    n = np.isnan(potential_nans)\n+    # masked NaN values are ok, although for masked the copyto may fail for\n+    # unmasked ones (this was always broken) when the result is a scalar.\n     if np.ma.isMaskedArray(n):\n         n = n.filled(False)\n-    if np.count_nonzero(n.ravel()) > 0:\n-        # Without given output, it is possible that the current result is a\n-        # numpy scalar, which is not writeable.  If so, just return nan.\n-        if isinstance(result, np.generic):\n-            return data.dtype.type(np.nan)\n \n-        result[n] = np.nan\n+    if not n.any():\n+        return result\n+\n+    # Without given output, it is possible that the current result is a\n+    # numpy scalar, which is not writeable.  If so, just return nan.\n+    if isinstance(result, np.generic):\n+        return potential_nans\n+\n+    # Otherwise copy NaNs (if there are any)\n+    np.copyto(result, potential_nans, where=n)\n     return result\n \n def _opt_info():\n",
            "comment_added_diff": {
                "1106": "    # masked NaN values are ok, although for masked the copyto may fail for",
                "1107": "    # unmasked ones (this was always broken) when the result is a scalar.",
                "1114": "    # Without given output, it is possible that the current result is a",
                "1115": "    # numpy scalar, which is not writeable.  If so, just return nan.",
                "1119": "    # Otherwise copy NaNs (if there are any)"
            },
            "comment_deleted_diff": {
                "1105": "    # masked NaN values are ok",
                "1109": "        # Without given output, it is possible that the current result is a",
                "1110": "        # numpy scalar, which is not writeable.  If so, just return nan."
            },
            "comment_modified_diff": {
                "1114": "        result[n] = np.nan"
            }
        },
        {
            "commit": "5e84001b3d4b0af5cbbeec98af26476c27504439",
            "timestamp": "2023-06-30T11:59:59+10:00",
            "author": "Andrew Nelson",
            "commit_message": "MAINT: testing for IS_MUSL",
            "additions": 7,
            "deletions": 11,
            "change_type": "MODIFY",
            "diff": "@@ -59,17 +59,13 @@ class KnownFailureException(Exception):\n _OLD_PROMOTION = lambda: np._get_promotion_state() == 'legacy'\n \n IS_MUSL = False\n-try:\n-    from packaging.tags import sys_tags\n-    _tags = list(sys_tags())\n-    if 'musllinux' in _tags[0].platform:\n-        IS_MUSL = True\n-except ImportError:\n-    # fallback to sysconfig (might be flaky)\n-    # value could be None.\n-    v = sysconfig.get_config_var('HOST_GNU_TYPE') or ''\n-    if 'musl' in v:\n-        IS_MUSL = True\n+# alternate way is\n+# from packaging.tags import sys_tags\n+#     _tags = list(sys_tags())\n+#     if 'musllinux' in _tags[0].platform:\n+_v = sysconfig.get_config_var('HOST_GNU_TYPE') or ''\n+if 'musl' in _v:\n+    IS_MUSL = True\n \n \n def assert_(val, msg=''):\n",
            "comment_added_diff": {
                "62": "# alternate way is",
                "63": "# from packaging.tags import sys_tags",
                "64": "#     _tags = list(sys_tags())",
                "65": "#     if 'musllinux' in _tags[0].platform:"
            },
            "comment_deleted_diff": {
                "68": "    # fallback to sysconfig (might be flaky)",
                "69": "    # value could be None."
            },
            "comment_modified_diff": {
                "62": "try:",
                "63": "    from packaging.tags import sys_tags",
                "64": "    _tags = list(sys_tags())",
                "65": "    if 'musllinux' in _tags[0].platform:"
            }
        },
        {
            "commit": "f71fd87d6a965bc071060fca4f057414bb218ed5",
            "timestamp": "2023-07-08T16:00:34+03:00",
            "author": "Evgeni Burovski",
            "commit_message": "DEP: remove np.lookfor",
            "additions": 2,
            "deletions": 285,
            "change_type": "MODIFY",
            "diff": "@@ -14,8 +14,8 @@\n \n __all__ = [\n     'issubclass_', 'issubsctype', 'issubdtype', 'deprecate',\n-    'deprecate_with_doc', 'get_include', 'info', 'source', 'who',\n-    'lookfor', 'byte_bounds', 'safe_eval', 'show_runtime'\n+    'deprecate_with_doc', 'get_include', 'source', 'who',\n+    'byte_bounds', 'safe_eval', 'show_runtime'\n     ]\n \n \n@@ -743,289 +743,6 @@ def interp(x, xp, fp, left=None, right=None):\n         print(\"Not available for this object.\", file=output)\n \n \n-# Cache for lookfor: {id(module): {name: (docstring, kind, index), ...}...}\n-# where kind: \"func\", \"class\", \"module\", \"object\"\n-# and index: index in breadth-first namespace traversal\n-_lookfor_caches = {}\n-\n-# regexp whose match indicates that the string may contain a function\n-# signature\n-_function_signature_re = re.compile(r\"[a-z0-9_]+\\(.*[,=].*\\)\", re.I)\n-\n-\n-@set_module('numpy')\n-def lookfor(what, module=None, import_modules=True, regenerate=False,\n-            output=None):\n-    \"\"\"\n-    Do a keyword search on docstrings.\n-\n-    A list of objects that matched the search is displayed,\n-    sorted by relevance. All given keywords need to be found in the\n-    docstring for it to be returned as a result, but the order does\n-    not matter.\n-\n-    Parameters\n-    ----------\n-    what : str\n-        String containing words to look for.\n-    module : str or list, optional\n-        Name of module(s) whose docstrings to go through.\n-    import_modules : bool, optional\n-        Whether to import sub-modules in packages. Default is True.\n-    regenerate : bool, optional\n-        Whether to re-generate the docstring cache. Default is False.\n-    output : file-like, optional\n-        File-like object to write the output to. If omitted, use a pager.\n-\n-    See Also\n-    --------\n-    source, info\n-\n-    Notes\n-    -----\n-    Relevance is determined only roughly, by checking if the keywords occur\n-    in the function name, at the start of a docstring, etc.\n-\n-    Examples\n-    --------\n-    >>> np.lookfor('binary representation') # doctest: +SKIP\n-    Search results for 'binary representation'\n-    ------------------------------------------\n-    numpy.binary_repr\n-        Return the binary representation of the input number as a string.\n-    numpy.core.setup_common.long_double_representation\n-        Given a binary dump as given by GNU od -b, look for long double\n-    numpy.base_repr\n-        Return a string representation of a number in the given base system.\n-    ...\n-\n-    \"\"\"\n-    import pydoc\n-\n-    # Cache\n-    cache = _lookfor_generate_cache(module, import_modules, regenerate)\n-\n-    # Search\n-    # XXX: maybe using a real stemming search engine would be better?\n-    found = []\n-    whats = str(what).lower().split()\n-    if not whats:\n-        return\n-\n-    for name, (docstring, kind, index) in cache.items():\n-        if kind in ('module', 'object'):\n-            # don't show modules or objects\n-            continue\n-        doc = docstring.lower()\n-        if all(w in doc for w in whats):\n-            found.append(name)\n-\n-    # Relevance sort\n-    # XXX: this is full Harrison-Stetson heuristics now,\n-    # XXX: it probably could be improved\n-\n-    kind_relevance = {'func': 1000, 'class': 1000,\n-                      'module': -1000, 'object': -1000}\n-\n-    def relevance(name, docstr, kind, index):\n-        r = 0\n-        # do the keywords occur within the start of the docstring?\n-        first_doc = \"\\n\".join(docstr.lower().strip().split(\"\\n\")[:3])\n-        r += sum([200 for w in whats if w in first_doc])\n-        # do the keywords occur in the function name?\n-        r += sum([30 for w in whats if w in name])\n-        # is the full name long?\n-        r += -len(name) * 5\n-        # is the object of bad type?\n-        r += kind_relevance.get(kind, -1000)\n-        # is the object deep in namespace hierarchy?\n-        r += -name.count('.') * 10\n-        r += max(-index / 100, -100)\n-        return r\n-\n-    def relevance_value(a):\n-        return relevance(a, *cache[a])\n-    found.sort(key=relevance_value)\n-\n-    # Pretty-print\n-    s = \"Search results for '%s'\" % (' '.join(whats))\n-    help_text = [s, \"-\"*len(s)]\n-    for name in found[::-1]:\n-        doc, kind, ix = cache[name]\n-\n-        doclines = [line.strip() for line in doc.strip().split(\"\\n\")\n-                    if line.strip()]\n-\n-        # find a suitable short description\n-        try:\n-            first_doc = doclines[0].strip()\n-            if _function_signature_re.search(first_doc):\n-                first_doc = doclines[1].strip()\n-        except IndexError:\n-            first_doc = \"\"\n-        help_text.append(\"%s\\n    %s\" % (name, first_doc))\n-\n-    if not found:\n-        help_text.append(\"Nothing found.\")\n-\n-    # Output\n-    if output is not None:\n-        output.write(\"\\n\".join(help_text))\n-    elif len(help_text) > 10:\n-        pager = pydoc.getpager()\n-        pager(\"\\n\".join(help_text))\n-    else:\n-        print(\"\\n\".join(help_text))\n-\n-def _lookfor_generate_cache(module, import_modules, regenerate):\n-    \"\"\"\n-    Generate docstring cache for given module.\n-\n-    Parameters\n-    ----------\n-    module : str, None, module\n-        Module for which to generate docstring cache\n-    import_modules : bool\n-        Whether to import sub-modules in packages.\n-    regenerate : bool\n-        Re-generate the docstring cache\n-\n-    Returns\n-    -------\n-    cache : dict {obj_full_name: (docstring, kind, index), ...}\n-        Docstring cache for the module, either cached one (regenerate=False)\n-        or newly generated.\n-\n-    \"\"\"\n-    # Local import to speed up numpy's import time.\n-    import inspect\n-\n-    from io import StringIO\n-\n-    if module is None:\n-        module = \"numpy\"\n-\n-    if isinstance(module, str):\n-        try:\n-            __import__(module)\n-        except ImportError:\n-            return {}\n-        module = sys.modules[module]\n-    elif isinstance(module, list) or isinstance(module, tuple):\n-        cache = {}\n-        for mod in module:\n-            cache.update(_lookfor_generate_cache(mod, import_modules,\n-                                                 regenerate))\n-        return cache\n-\n-    if id(module) in _lookfor_caches and not regenerate:\n-        return _lookfor_caches[id(module)]\n-\n-    # walk items and collect docstrings\n-    cache = {}\n-    _lookfor_caches[id(module)] = cache\n-    seen = {}\n-    index = 0\n-    stack = [(module.__name__, module)]\n-    while stack:\n-        name, item = stack.pop(0)\n-        if id(item) in seen:\n-            continue\n-        seen[id(item)] = True\n-\n-        index += 1\n-        kind = \"object\"\n-\n-        if inspect.ismodule(item):\n-            kind = \"module\"\n-            try:\n-                _all = item.__all__\n-            except AttributeError:\n-                _all = None\n-\n-            # import sub-packages\n-            if import_modules and hasattr(item, '__path__'):\n-                for pth in item.__path__:\n-                    for mod_path in os.listdir(pth):\n-                        this_py = os.path.join(pth, mod_path)\n-                        init_py = os.path.join(pth, mod_path, '__init__.py')\n-                        if (os.path.isfile(this_py) and\n-                                mod_path.endswith('.py')):\n-                            to_import = mod_path[:-3]\n-                        elif os.path.isfile(init_py):\n-                            to_import = mod_path\n-                        else:\n-                            continue\n-                        if to_import == '__init__':\n-                            continue\n-\n-                        try:\n-                            old_stdout = sys.stdout\n-                            old_stderr = sys.stderr\n-                            try:\n-                                sys.stdout = StringIO()\n-                                sys.stderr = StringIO()\n-                                __import__(\"%s.%s\" % (name, to_import))\n-                            finally:\n-                                sys.stdout = old_stdout\n-                                sys.stderr = old_stderr\n-                        except KeyboardInterrupt:\n-                            # Assume keyboard interrupt came from a user\n-                            raise\n-                        except BaseException:\n-                            # Ignore also SystemExit and pytests.importorskip\n-                            # `Skipped` (these are BaseExceptions; gh-22345)\n-                            continue\n-\n-            for n, v in _getmembers(item):\n-                try:\n-                    item_name = getattr(v, '__name__', \"%s.%s\" % (name, n))\n-                    mod_name = getattr(v, '__module__', None)\n-                except NameError:\n-                    # ref. SWIG's global cvars\n-                    #    NameError: Unknown C global variable\n-                    item_name = \"%s.%s\" % (name, n)\n-                    mod_name = None\n-                if '.' not in item_name and mod_name:\n-                    item_name = \"%s.%s\" % (mod_name, item_name)\n-\n-                if not item_name.startswith(name + '.'):\n-                    # don't crawl \"foreign\" objects\n-                    if isinstance(v, ufunc):\n-                        # ... unless they are ufuncs\n-                        pass\n-                    else:\n-                        continue\n-                elif not (inspect.ismodule(v) or _all is None or n in _all):\n-                    continue\n-                stack.append((\"%s.%s\" % (name, n), v))\n-        elif inspect.isclass(item):\n-            kind = \"class\"\n-            for n, v in _getmembers(item):\n-                stack.append((\"%s.%s\" % (name, n), v))\n-        elif hasattr(item, \"__call__\"):\n-            kind = \"func\"\n-\n-        try:\n-            doc = inspect.getdoc(item)\n-        except NameError:\n-            # ref SWIG's NameError: Unknown C global variable\n-            doc = None\n-        if doc is not None:\n-            cache[name] = (doc, kind, index)\n-\n-    return cache\n-\n-def _getmembers(item):\n-    import inspect\n-    try:\n-        members = inspect.getmembers(item)\n-    except Exception:\n-        members = [(x, getattr(item, x)) for x in dir(item)\n-                   if hasattr(item, x)]\n-    return members\n-\n-\n @deprecate\n def safe_eval(source):\n     \"\"\"\n",
            "comment_added_diff": {},
            "comment_deleted_diff": {
                "746": "# Cache for lookfor: {id(module): {name: (docstring, kind, index), ...}...}",
                "747": "# where kind: \"func\", \"class\", \"module\", \"object\"",
                "748": "# and index: index in breadth-first namespace traversal",
                "751": "# regexp whose match indicates that the string may contain a function",
                "752": "# signature",
                "791": "    >>> np.lookfor('binary representation') # doctest: +SKIP",
                "805": "    # Cache",
                "808": "    # Search",
                "809": "    # XXX: maybe using a real stemming search engine would be better?",
                "817": "            # don't show modules or objects",
                "823": "    # Relevance sort",
                "824": "    # XXX: this is full Harrison-Stetson heuristics now,",
                "825": "    # XXX: it probably could be improved",
                "832": "        # do the keywords occur within the start of the docstring?",
                "835": "        # do the keywords occur in the function name?",
                "837": "        # is the full name long?",
                "839": "        # is the object of bad type?",
                "841": "        # is the object deep in namespace hierarchy?",
                "850": "    # Pretty-print",
                "859": "        # find a suitable short description",
                "871": "    # Output",
                "900": "    # Local import to speed up numpy's import time.",
                "924": "    # walk items and collect docstrings",
                "946": "            # import sub-packages",
                "973": "                            # Assume keyboard interrupt came from a user",
                "976": "                            # Ignore also SystemExit and pytests.importorskip",
                "977": "                            # `Skipped` (these are BaseExceptions; gh-22345)",
                "985": "                    # ref. SWIG's global cvars",
                "986": "                    #    NameError: Unknown C global variable",
                "993": "                    # don't crawl \"foreign\" objects",
                "995": "                        # ... unless they are ufuncs",
                "1012": "            # ref SWIG's NameError: Unknown C global variable"
            },
            "comment_modified_diff": {}
        },
        {
            "commit": "5579f0b64296d611621171087a0c161f2b4c80dc",
            "timestamp": "2023-07-08T16:05:18+03:00",
            "author": "Evgeni Burovski",
            "commit_message": "DEP: remove np.source",
            "additions": 1,
            "deletions": 50,
            "change_type": "MODIFY",
            "diff": "@@ -14,7 +14,7 @@\n \n __all__ = [\n     'issubclass_', 'issubsctype', 'issubdtype', 'deprecate',\n-    'deprecate_with_doc', 'get_include', 'source', 'who',\n+    'deprecate_with_doc', 'get_include', 'info', 'who',\n     'byte_bounds', 'safe_eval', 'show_runtime'\n     ]\n \n@@ -694,55 +694,6 @@ def info(object=None, maxwidth=76, output=None, toplevel='numpy'):\n         print(inspect.getdoc(object), file=output)\n \n \n-@set_module('numpy')\n-def source(object, output=sys.stdout):\n-    \"\"\"\n-    Print or write to a file the source code for a NumPy object.\n-\n-    The source code is only returned for objects written in Python. Many\n-    functions and classes are defined in C and will therefore not return\n-    useful information.\n-\n-    Parameters\n-    ----------\n-    object : numpy object\n-        Input object. This can be any object (function, class, module,\n-        ...).\n-    output : file object, optional\n-        If `output` not supplied then source code is printed to screen\n-        (sys.stdout).  File object must be created with either write 'w' or\n-        append 'a' modes.\n-\n-    See Also\n-    --------\n-    lookfor, info\n-\n-    Examples\n-    --------\n-    >>> np.source(np.interp)                        #doctest: +SKIP\n-    In file: /usr/lib/python2.6/dist-packages/numpy/lib/function_base.py\n-    def interp(x, xp, fp, left=None, right=None):\n-        \\\"\\\"\\\".... (full docstring printed)\\\"\\\"\\\"\n-        if isinstance(x, (float, int, number)):\n-            return compiled_interp([x], xp, fp, left, right).item()\n-        else:\n-            return compiled_interp(x, xp, fp, left, right)\n-\n-    The source code is only returned for objects written in Python.\n-\n-    >>> np.source(np.array)                         #doctest: +SKIP\n-    Not available for this object.\n-\n-    \"\"\"\n-    # Local import to speed up numpy's import time.\n-    import inspect\n-    try:\n-        print(\"In file: %s\\n\" % inspect.getsourcefile(object), file=output)\n-        print(inspect.getsource(object), file=output)\n-    except Exception:\n-        print(\"Not available for this object.\", file=output)\n-\n-\n @deprecate\n def safe_eval(source):\n     \"\"\"\n",
            "comment_added_diff": {},
            "comment_deleted_diff": {
                "722": "    >>> np.source(np.interp)                        #doctest: +SKIP",
                "733": "    >>> np.source(np.array)                         #doctest: +SKIP",
                "737": "    # Local import to speed up numpy's import time."
            },
            "comment_modified_diff": {}
        },
        {
            "commit": "ab17394de8f55534eadf741dffba6408fc608514",
            "timestamp": "2023-07-10T15:27:32+02:00",
            "author": "Mateusz Sok\u00f3\u0142",
            "commit_message": "API: deprecate undocumented functions",
            "additions": 4,
            "deletions": 169,
            "change_type": "MODIFY",
            "diff": "@@ -1,21 +1,17 @@\n import os\n import sys\n-import textwrap\n import types\n import re\n-import warnings\n-import functools\n import platform\n \n-from .._utils import set_module\n+from numpy._utils import set_module, deprecate\n from numpy.core.numerictypes import issubclass_, issubsctype, issubdtype\n from numpy.core import ndarray, ufunc, asarray\n import numpy as np\n \n __all__ = [\n-    'issubclass_', 'issubsctype', 'issubdtype', 'deprecate',\n-    'deprecate_with_doc', 'get_include', 'info', 'source', 'who',\n-    'lookfor', 'byte_bounds', 'safe_eval', 'show_runtime'\n+    'issubclass_', 'issubsctype', 'issubdtype', 'get_include', 'info', \n+    'source', 'who', 'lookfor', 'byte_bounds', 'safe_eval', 'show_runtime'\n     ]\n \n \n@@ -101,172 +97,11 @@ def get_include():\n     return d\n \n \n-class _Deprecate:\n-    \"\"\"\n-    Decorator class to deprecate old functions.\n-\n-    Refer to `deprecate` for details.\n-\n-    See Also\n-    --------\n-    deprecate\n-\n-    \"\"\"\n-\n-    def __init__(self, old_name=None, new_name=None, message=None):\n-        self.old_name = old_name\n-        self.new_name = new_name\n-        self.message = message\n-\n-    def __call__(self, func, *args, **kwargs):\n-        \"\"\"\n-        Decorator call.  Refer to ``decorate``.\n-\n-        \"\"\"\n-        old_name = self.old_name\n-        new_name = self.new_name\n-        message = self.message\n-\n-        if old_name is None:\n-            old_name = func.__name__\n-        if new_name is None:\n-            depdoc = \"`%s` is deprecated!\" % old_name\n-        else:\n-            depdoc = \"`%s` is deprecated, use `%s` instead!\" % \\\n-                     (old_name, new_name)\n-\n-        if message is not None:\n-            depdoc += \"\\n\" + message\n-\n-        @functools.wraps(func)\n-        def newfunc(*args, **kwds):\n-            warnings.warn(depdoc, DeprecationWarning, stacklevel=2)\n-            return func(*args, **kwds)\n-\n-        newfunc.__name__ = old_name\n-        doc = func.__doc__\n-        if doc is None:\n-            doc = depdoc\n-        else:\n-            lines = doc.expandtabs().split('\\n')\n-            indent = _get_indent(lines[1:])\n-            if lines[0].lstrip():\n-                # Indent the original first line to let inspect.cleandoc()\n-                # dedent the docstring despite the deprecation notice.\n-                doc = indent * ' ' + doc\n-            else:\n-                # Remove the same leading blank lines as cleandoc() would.\n-                skip = len(lines[0]) + 1\n-                for line in lines[1:]:\n-                    if len(line) > indent:\n-                        break\n-                    skip += len(line) + 1\n-                doc = doc[skip:]\n-            depdoc = textwrap.indent(depdoc, ' ' * indent)\n-            doc = '\\n\\n'.join([depdoc, doc])\n-        newfunc.__doc__ = doc\n-\n-        return newfunc\n-\n-\n-def _get_indent(lines):\n-    \"\"\"\n-    Determines the leading whitespace that could be removed from all the lines.\n-    \"\"\"\n-    indent = sys.maxsize\n-    for line in lines:\n-        content = len(line.lstrip())\n-        if content:\n-            indent = min(indent, len(line) - content)\n-    if indent == sys.maxsize:\n-        indent = 0\n-    return indent\n-\n-\n-def deprecate(*args, **kwargs):\n-    \"\"\"\n-    Issues a DeprecationWarning, adds warning to `old_name`'s\n-    docstring, rebinds ``old_name.__name__`` and returns the new\n-    function object.\n-\n-    This function may also be used as a decorator.\n-\n-    Parameters\n-    ----------\n-    func : function\n-        The function to be deprecated.\n-    old_name : str, optional\n-        The name of the function to be deprecated. Default is None, in\n-        which case the name of `func` is used.\n-    new_name : str, optional\n-        The new name for the function. Default is None, in which case the\n-        deprecation message is that `old_name` is deprecated. If given, the\n-        deprecation message is that `old_name` is deprecated and `new_name`\n-        should be used instead.\n-    message : str, optional\n-        Additional explanation of the deprecation.  Displayed in the\n-        docstring after the warning.\n-\n-    Returns\n-    -------\n-    old_func : function\n-        The deprecated function.\n-\n-    Examples\n-    --------\n-    Note that ``olduint`` returns a value after printing Deprecation\n-    Warning:\n-\n-    >>> olduint = np.deprecate(np.uint)\n-    DeprecationWarning: `uint64` is deprecated! # may vary\n-    >>> olduint(6)\n-    6\n-\n-    \"\"\"\n-    # Deprecate may be run as a function or as a decorator\n-    # If run as a function, we initialise the decorator class\n-    # and execute its __call__ method.\n-\n-    if args:\n-        fn = args[0]\n-        args = args[1:]\n-\n-        return _Deprecate(*args, **kwargs)(fn)\n-    else:\n-        return _Deprecate(*args, **kwargs)\n-\n-\n-def deprecate_with_doc(msg):\n-    \"\"\"\n-    Deprecates a function and includes the deprecation in its docstring.\n-\n-    This function is used as a decorator. It returns an object that can be\n-    used to issue a DeprecationWarning, by passing the to-be decorated\n-    function as argument, this adds warning to the to-be decorated function's\n-    docstring and returns the new function object.\n-\n-    See Also\n-    --------\n-    deprecate : Decorate a function such that it issues a `DeprecationWarning`\n-\n-    Parameters\n-    ----------\n-    msg : str\n-        Additional explanation of the deprecation. Displayed in the\n-        docstring after the warning.\n-\n-    Returns\n-    -------\n-    obj : object\n-\n-    \"\"\"\n-    return _Deprecate(message=msg)\n-\n-\n #--------------------------------------------\n # Determine if two arrays can share memory\n #--------------------------------------------\n \n+\n @deprecate\n def byte_bounds(a):\n     \"\"\"\n",
            "comment_added_diff": {},
            "comment_deleted_diff": {
                "154": "                # Indent the original first line to let inspect.cleandoc()",
                "155": "                # dedent the docstring despite the deprecation notice.",
                "158": "                # Remove the same leading blank lines as cleandoc() would.",
                "221": "    DeprecationWarning: `uint64` is deprecated! # may vary",
                "226": "    # Deprecate may be run as a function or as a decorator",
                "227": "    # If run as a function, we initialise the decorator class",
                "228": "    # and execute its __call__ method."
            },
            "comment_modified_diff": {}
        },
        {
            "commit": "017cef318ad37ac05738830ebb3173c65c128759",
            "timestamp": "2023-07-11T22:25:10+02:00",
            "author": "Mateusz Sok\u00f3\u0142",
            "commit_message": "API: apply review comments",
            "additions": 206,
            "deletions": 6,
            "change_type": "MODIFY",
            "diff": "@@ -1,17 +1,21 @@\n import os\n import sys\n+import textwrap\n import types\n import re\n+import warnings\n+import functools\n import platform\n \n-from numpy._utils import set_module, deprecate\n from numpy.core.numerictypes import issubclass_, issubsctype, issubdtype\n from numpy.core import ndarray, ufunc, asarray\n+from numpy._utils import set_module\n import numpy as np\n \n __all__ = [\n-    'issubclass_', 'issubsctype', 'issubdtype', 'get_include', 'info', \n-    'source', 'who', 'lookfor', 'byte_bounds', 'safe_eval', 'show_runtime'\n+    'issubclass_', 'issubsctype', 'issubdtype', 'deprecate',\n+    'deprecate_with_doc', 'get_include', 'info', 'source', 'who',\n+    'lookfor', 'byte_bounds', 'safe_eval', 'show_runtime'\n     ]\n \n \n@@ -97,12 +101,192 @@ def get_include():\n     return d\n \n \n+class _Deprecate:\n+    \"\"\"\n+    Decorator class to deprecate old functions.\n+\n+    Refer to `deprecate` for details.\n+\n+    See Also\n+    --------\n+    deprecate\n+\n+    \"\"\"\n+\n+    def __init__(self, old_name=None, new_name=None, message=None):\n+        self.old_name = old_name\n+        self.new_name = new_name\n+        self.message = message\n+\n+    def __call__(self, func, *args, **kwargs):\n+        \"\"\"\n+        Decorator call.  Refer to ``decorate``.\n+\n+        \"\"\"\n+        old_name = self.old_name\n+        new_name = self.new_name\n+        message = self.message\n+\n+        if old_name is None:\n+            old_name = func.__name__\n+        if new_name is None:\n+            depdoc = \"`%s` is deprecated!\" % old_name\n+        else:\n+            depdoc = \"`%s` is deprecated, use `%s` instead!\" % \\\n+                     (old_name, new_name)\n+\n+        if message is not None:\n+            depdoc += \"\\n\" + message\n+\n+        @functools.wraps(func)\n+        def newfunc(*args, **kwds):\n+            warnings.warn(depdoc, DeprecationWarning, stacklevel=2)\n+            return func(*args, **kwds)\n+\n+        newfunc.__name__ = old_name\n+        doc = func.__doc__\n+        if doc is None:\n+            doc = depdoc\n+        else:\n+            lines = doc.expandtabs().split('\\n')\n+            indent = _get_indent(lines[1:])\n+            if lines[0].lstrip():\n+                # Indent the original first line to let inspect.cleandoc()\n+                # dedent the docstring despite the deprecation notice.\n+                doc = indent * ' ' + doc\n+            else:\n+                # Remove the same leading blank lines as cleandoc() would.\n+                skip = len(lines[0]) + 1\n+                for line in lines[1:]:\n+                    if len(line) > indent:\n+                        break\n+                    skip += len(line) + 1\n+                doc = doc[skip:]\n+            depdoc = textwrap.indent(depdoc, ' ' * indent)\n+            doc = '\\n\\n'.join([depdoc, doc])\n+        newfunc.__doc__ = doc\n+\n+        return newfunc\n+\n+\n+def _get_indent(lines):\n+    \"\"\"\n+    Determines the leading whitespace that could be removed from all the lines.\n+    \"\"\"\n+    indent = sys.maxsize\n+    for line in lines:\n+        content = len(line.lstrip())\n+        if content:\n+            indent = min(indent, len(line) - content)\n+    if indent == sys.maxsize:\n+        indent = 0\n+    return indent\n+\n+\n+def deprecate(*args, **kwargs):\n+    \"\"\"\n+    Issues a DeprecationWarning, adds warning to `old_name`'s\n+    docstring, rebinds ``old_name.__name__`` and returns the new\n+    function object.\n+\n+    This function may also be used as a decorator.\n+\n+    Parameters\n+    ----------\n+    func : function\n+        The function to be deprecated.\n+    old_name : str, optional\n+        The name of the function to be deprecated. Default is None, in\n+        which case the name of `func` is used.\n+    new_name : str, optional\n+        The new name for the function. Default is None, in which case the\n+        deprecation message is that `old_name` is deprecated. If given, the\n+        deprecation message is that `old_name` is deprecated and `new_name`\n+        should be used instead.\n+    message : str, optional\n+        Additional explanation of the deprecation.  Displayed in the\n+        docstring after the warning.\n+\n+    Returns\n+    -------\n+    old_func : function\n+        The deprecated function.\n+\n+    Examples\n+    --------\n+    Note that ``olduint`` returns a value after printing Deprecation\n+    Warning:\n+\n+    >>> olduint = np.deprecate(np.uint)\n+    DeprecationWarning: `uint64` is deprecated! # may vary\n+    >>> olduint(6)\n+    6\n+\n+    \"\"\"\n+    # Deprecate may be run as a function or as a decorator\n+    # If run as a function, we initialise the decorator class\n+    # and execute its __call__ method.\n+\n+    # Deprecated in NumPy 2.0, 2023-07-11\n+    warnings.warn(\n+        \"`deprecate` is deprecated, \"\n+        \"use `warn` with `DeprecationWarning` instead. \"\n+        \"(deprecated in NumPy 2.0)\",\n+        DeprecationWarning, \n+        stacklevel=2\n+    )\n+\n+    if args:\n+        fn = args[0]\n+        args = args[1:]\n+\n+        return _Deprecate(*args, **kwargs)(fn)\n+    else:\n+        return _Deprecate(*args, **kwargs)\n+\n+\n+def deprecate_with_doc(msg):\n+    \"\"\"\n+    Deprecates a function and includes the deprecation in its docstring.\n+\n+    This function is used as a decorator. It returns an object that can be\n+    used to issue a DeprecationWarning, by passing the to-be decorated\n+    function as argument, this adds warning to the to-be decorated function's\n+    docstring and returns the new function object.\n+\n+    See Also\n+    --------\n+    deprecate : Decorate a function such that it issues a `DeprecationWarning`\n+\n+    Parameters\n+    ----------\n+    msg : str\n+        Additional explanation of the deprecation. Displayed in the\n+        docstring after the warning.\n+\n+    Returns\n+    -------\n+    obj : object\n+\n+    \"\"\"\n+\n+    # Deprecated in NumPy 2.0, 2023-07-11\n+    warnings.warn(\n+        \"`deprecate` is deprecated, \"\n+        \"use `warn` with `DeprecationWarning` instead. \"\n+        \"(deprecated in NumPy 2.0)\",\n+        DeprecationWarning, \n+        stacklevel=2\n+    )\n+\n+    return _Deprecate(message=msg)\n+\n+\n #--------------------------------------------\n # Determine if two arrays can share memory\n #--------------------------------------------\n \n \n-@deprecate\n def byte_bounds(a):\n     \"\"\"\n     Returns pointers to the end-points of an array.\n@@ -160,7 +344,6 @@ def byte_bounds(a):\n #-----------------------------------------------------------------------------\n \n \n-@deprecate\n def who(vardict=None):\n     \"\"\"\n     Print the NumPy arrays in the given dictionary.\n@@ -205,6 +388,15 @@ def who(vardict=None):\n     Upper bound on total bytes  =       40\n \n     \"\"\"\n+\n+    # Deprecated in NumPy 2.0, 2023-07-11\n+    warnings.warn(\n+        \"`who` is deprecated. \"\n+        \"(deprecated in NumPy 2.0)\",\n+        DeprecationWarning,\n+        stacklevel=2\n+    )\n+\n     if vardict is None:\n         frame = sys._getframe().f_back\n         vardict = frame.f_globals\n@@ -861,7 +1053,6 @@ def _getmembers(item):\n     return members\n \n \n-@deprecate\n def safe_eval(source):\n     \"\"\"\n     Protected string evaluation.\n@@ -911,6 +1102,15 @@ def safe_eval(source):\n     ValueError: malformed node or string: <_ast.Call object at 0x...>\n \n     \"\"\"\n+\n+    # Deprecated in NumPy 2.0, 2023-07-11\n+    warnings.warn(\n+        \"`safe_eval` is deprecated. \"\n+        \"(deprecated in NumPy 2.0)\",\n+        DeprecationWarning,\n+        stacklevel=2\n+    )\n+\n     # Local import to speed up numpy's import time.\n     import ast\n     return ast.literal_eval(source)\n",
            "comment_added_diff": {
                "154": "                # Indent the original first line to let inspect.cleandoc()",
                "155": "                # dedent the docstring despite the deprecation notice.",
                "158": "                # Remove the same leading blank lines as cleandoc() would.",
                "221": "    DeprecationWarning: `uint64` is deprecated! # may vary",
                "226": "    # Deprecate may be run as a function or as a decorator",
                "227": "    # If run as a function, we initialise the decorator class",
                "228": "    # and execute its __call__ method.",
                "230": "    # Deprecated in NumPy 2.0, 2023-07-11",
                "273": "    # Deprecated in NumPy 2.0, 2023-07-11",
                "392": "    # Deprecated in NumPy 2.0, 2023-07-11",
                "1106": "    # Deprecated in NumPy 2.0, 2023-07-11"
            },
            "comment_deleted_diff": {},
            "comment_modified_diff": {}
        },
        {
            "commit": "703f1437284ef8c1bb5b014aeb372c67c02ddb4f",
            "timestamp": "2023-07-30T10:34:25+02:00",
            "author": "Mateusz Sok\u00f3\u0142",
            "commit_message": "API: apply review comments",
            "additions": 3,
            "deletions": 275,
            "change_type": "MODIFY",
            "diff": "@@ -780,279 +780,6 @@ def interp(x, xp, fp, left=None, right=None):\n _function_signature_re = re.compile(r\"[a-z0-9_]+\\(.*[,=].*\\)\", re.I)\n \n \n-@set_module('numpy')\n-def lookfor(what, module=None, import_modules=True, regenerate=False,\n-            output=None):\n-    \"\"\"\n-    Do a keyword search on docstrings.\n-\n-    A list of objects that matched the search is displayed,\n-    sorted by relevance. All given keywords need to be found in the\n-    docstring for it to be returned as a result, but the order does\n-    not matter.\n-\n-    Parameters\n-    ----------\n-    what : str\n-        String containing words to look for.\n-    module : str or list, optional\n-        Name of module(s) whose docstrings to go through.\n-    import_modules : bool, optional\n-        Whether to import sub-modules in packages. Default is True.\n-    regenerate : bool, optional\n-        Whether to re-generate the docstring cache. Default is False.\n-    output : file-like, optional\n-        File-like object to write the output to. If omitted, use a pager.\n-\n-    See Also\n-    --------\n-    source, info\n-\n-    Notes\n-    -----\n-    Relevance is determined only roughly, by checking if the keywords occur\n-    in the function name, at the start of a docstring, etc.\n-\n-    Examples\n-    --------\n-    >>> np.lookfor('binary representation') # doctest: +SKIP\n-    Search results for 'binary representation'\n-    ------------------------------------------\n-    numpy.binary_repr\n-        Return the binary representation of the input number as a string.\n-    numpy.core.setup_common.long_double_representation\n-        Given a binary dump as given by GNU od -b, look for long double\n-    numpy.base_repr\n-        Return a string representation of a number in the given base system.\n-    ...\n-\n-    \"\"\"\n-    import pydoc\n-\n-    # Cache\n-    cache = _lookfor_generate_cache(module, import_modules, regenerate)\n-\n-    # Search\n-    # XXX: maybe using a real stemming search engine would be better?\n-    found = []\n-    whats = str(what).lower().split()\n-    if not whats:\n-        return\n-\n-    for name, (docstring, kind, index) in cache.items():\n-        if kind in ('module', 'object'):\n-            # don't show modules or objects\n-            continue\n-        doc = docstring.lower()\n-        if all(w in doc for w in whats):\n-            found.append(name)\n-\n-    # Relevance sort\n-    # XXX: this is full Harrison-Stetson heuristics now,\n-    # XXX: it probably could be improved\n-\n-    kind_relevance = {'func': 1000, 'class': 1000,\n-                      'module': -1000, 'object': -1000}\n-\n-    def relevance(name, docstr, kind, index):\n-        r = 0\n-        # do the keywords occur within the start of the docstring?\n-        first_doc = \"\\n\".join(docstr.lower().strip().split(\"\\n\")[:3])\n-        r += sum([200 for w in whats if w in first_doc])\n-        # do the keywords occur in the function name?\n-        r += sum([30 for w in whats if w in name])\n-        # is the full name long?\n-        r += -len(name) * 5\n-        # is the object of bad type?\n-        r += kind_relevance.get(kind, -1000)\n-        # is the object deep in namespace hierarchy?\n-        r += -name.count('.') * 10\n-        r += max(-index / 100, -100)\n-        return r\n-\n-    def relevance_value(a):\n-        return relevance(a, *cache[a])\n-    found.sort(key=relevance_value)\n-\n-    # Pretty-print\n-    s = \"Search results for '%s'\" % (' '.join(whats))\n-    help_text = [s, \"-\"*len(s)]\n-    for name in found[::-1]:\n-        doc, kind, ix = cache[name]\n-\n-        doclines = [line.strip() for line in doc.strip().split(\"\\n\")\n-                    if line.strip()]\n-\n-        # find a suitable short description\n-        try:\n-            first_doc = doclines[0].strip()\n-            if _function_signature_re.search(first_doc):\n-                first_doc = doclines[1].strip()\n-        except IndexError:\n-            first_doc = \"\"\n-        help_text.append(\"%s\\n    %s\" % (name, first_doc))\n-\n-    if not found:\n-        help_text.append(\"Nothing found.\")\n-\n-    # Output\n-    if output is not None:\n-        output.write(\"\\n\".join(help_text))\n-    elif len(help_text) > 10:\n-        pager = pydoc.getpager()\n-        pager(\"\\n\".join(help_text))\n-    else:\n-        print(\"\\n\".join(help_text))\n-\n-def _lookfor_generate_cache(module, import_modules, regenerate):\n-    \"\"\"\n-    Generate docstring cache for given module.\n-\n-    Parameters\n-    ----------\n-    module : str, None, module\n-        Module for which to generate docstring cache\n-    import_modules : bool\n-        Whether to import sub-modules in packages.\n-    regenerate : bool\n-        Re-generate the docstring cache\n-\n-    Returns\n-    -------\n-    cache : dict {obj_full_name: (docstring, kind, index), ...}\n-        Docstring cache for the module, either cached one (regenerate=False)\n-        or newly generated.\n-\n-    \"\"\"\n-    # Local import to speed up numpy's import time.\n-    import inspect\n-\n-    from io import StringIO\n-\n-    if module is None:\n-        module = \"numpy\"\n-\n-    if isinstance(module, str):\n-        try:\n-            __import__(module)\n-        except ImportError:\n-            return {}\n-        module = sys.modules[module]\n-    elif isinstance(module, list) or isinstance(module, tuple):\n-        cache = {}\n-        for mod in module:\n-            cache.update(_lookfor_generate_cache(mod, import_modules,\n-                                                 regenerate))\n-        return cache\n-\n-    if id(module) in _lookfor_caches and not regenerate:\n-        return _lookfor_caches[id(module)]\n-\n-    # walk items and collect docstrings\n-    cache = {}\n-    _lookfor_caches[id(module)] = cache\n-    seen = {}\n-    index = 0\n-    stack = [(module.__name__, module)]\n-    while stack:\n-        name, item = stack.pop(0)\n-        if id(item) in seen:\n-            continue\n-        seen[id(item)] = True\n-\n-        index += 1\n-        kind = \"object\"\n-\n-        if inspect.ismodule(item):\n-            kind = \"module\"\n-            try:\n-                _all = item.__all__\n-            except AttributeError:\n-                _all = None\n-\n-            # import sub-packages\n-            if import_modules and hasattr(item, '__path__'):\n-                for pth in item.__path__:\n-                    for mod_path in os.listdir(pth):\n-                        this_py = os.path.join(pth, mod_path)\n-                        init_py = os.path.join(pth, mod_path, '__init__.py')\n-                        if (os.path.isfile(this_py) and\n-                                mod_path.endswith('.py')):\n-                            to_import = mod_path[:-3]\n-                        elif os.path.isfile(init_py):\n-                            to_import = mod_path\n-                        else:\n-                            continue\n-                        if to_import == '__init__':\n-                            continue\n-\n-                        try:\n-                            old_stdout = sys.stdout\n-                            old_stderr = sys.stderr\n-                            try:\n-                                sys.stdout = StringIO()\n-                                sys.stderr = StringIO()\n-                                __import__(\"%s.%s\" % (name, to_import))\n-                            finally:\n-                                sys.stdout = old_stdout\n-                                sys.stderr = old_stderr\n-                        except KeyboardInterrupt:\n-                            # Assume keyboard interrupt came from a user\n-                            raise\n-                        except BaseException:\n-                            # Ignore also SystemExit and pytests.importorskip\n-                            # `Skipped` (these are BaseExceptions; gh-22345)\n-                            continue\n-\n-            for n, v in _getmembers(item):\n-                try:\n-                    item_name = getattr(v, '__name__', \"%s.%s\" % (name, n))\n-                    mod_name = getattr(v, '__module__', None)\n-                except NameError:\n-                    # ref. SWIG's global cvars\n-                    #    NameError: Unknown C global variable\n-                    item_name = \"%s.%s\" % (name, n)\n-                    mod_name = None\n-                if '.' not in item_name and mod_name:\n-                    item_name = \"%s.%s\" % (mod_name, item_name)\n-\n-                if not item_name.startswith(name + '.'):\n-                    # don't crawl \"foreign\" objects\n-                    if isinstance(v, ufunc):\n-                        # ... unless they are ufuncs\n-                        pass\n-                    else:\n-                        continue\n-                elif not (inspect.ismodule(v) or _all is None or n in _all):\n-                    continue\n-                stack.append((\"%s.%s\" % (name, n), v))\n-        elif inspect.isclass(item):\n-            kind = \"class\"\n-            for n, v in _getmembers(item):\n-                stack.append((\"%s.%s\" % (name, n), v))\n-        elif hasattr(item, \"__call__\"):\n-            kind = \"func\"\n-\n-        try:\n-            doc = inspect.getdoc(item)\n-        except NameError:\n-            # ref SWIG's NameError: Unknown C global variable\n-            doc = None\n-        if doc is not None:\n-            cache[name] = (doc, kind, index)\n-\n-    return cache\n-\n-def _getmembers(item):\n-    import inspect\n-    try:\n-        members = inspect.getmembers(item)\n-    except Exception:\n-        members = [(x, getattr(item, x)) for x in dir(item)\n-                   if hasattr(item, x)]\n-    return members\n-\n-\n def safe_eval(source):\n     \"\"\"\n     Protected string evaluation.\n@@ -1105,8 +832,9 @@ def safe_eval(source):\n \n     # Deprecated in NumPy 2.0, 2023-07-11\n     warnings.warn(\n-        \"`safe_eval` is deprecated. \"\n-        \"(deprecated in NumPy 2.0)\",\n+        \"`safe_eval` is deprecated. Use `ast.literal_eval` instead. \"\n+        \"Be aware of security implications, such as memory exhaustion \"\n+        \"based attacks (deprecated in NumPy 2.0)\",\n         DeprecationWarning,\n         stacklevel=2\n     )\n",
            "comment_added_diff": {},
            "comment_deleted_diff": {
                "818": "    >>> np.lookfor('binary representation') # doctest: +SKIP",
                "832": "    # Cache",
                "835": "    # Search",
                "836": "    # XXX: maybe using a real stemming search engine would be better?",
                "844": "            # don't show modules or objects",
                "850": "    # Relevance sort",
                "851": "    # XXX: this is full Harrison-Stetson heuristics now,",
                "852": "    # XXX: it probably could be improved",
                "859": "        # do the keywords occur within the start of the docstring?",
                "862": "        # do the keywords occur in the function name?",
                "864": "        # is the full name long?",
                "866": "        # is the object of bad type?",
                "868": "        # is the object deep in namespace hierarchy?",
                "877": "    # Pretty-print",
                "886": "        # find a suitable short description",
                "898": "    # Output",
                "927": "    # Local import to speed up numpy's import time.",
                "951": "    # walk items and collect docstrings",
                "973": "            # import sub-packages",
                "1000": "                            # Assume keyboard interrupt came from a user",
                "1003": "                            # Ignore also SystemExit and pytests.importorskip",
                "1004": "                            # `Skipped` (these are BaseExceptions; gh-22345)",
                "1012": "                    # ref. SWIG's global cvars",
                "1013": "                    #    NameError: Unknown C global variable",
                "1020": "                    # don't crawl \"foreign\" objects",
                "1022": "                        # ... unless they are ufuncs",
                "1039": "            # ref SWIG's NameError: Unknown C global variable"
            },
            "comment_modified_diff": {}
        },
        {
            "commit": "ceb78f2220def0c62419615cd1a544bd5d7285d3",
            "timestamp": "2023-08-16T11:02:42+02:00",
            "author": "Pieter Eendebak",
            "commit_message": "DEP: Remove deprecated numpy.who (#24321)\n\n[skip ci]",
            "additions": 1,
            "deletions": 111,
            "change_type": "MODIFY",
            "diff": "@@ -14,7 +14,7 @@\n \n __all__ = [\n     'issubclass_', 'issubsctype', 'deprecate',\n-    'deprecate_with_doc', 'get_include', 'info', 'who',\n+    'deprecate_with_doc', 'get_include', 'info',\n     'byte_bounds', 'safe_eval', 'show_runtime'\n     ]\n \n@@ -345,116 +345,6 @@ def byte_bounds(a):\n     return a_low, a_high\n \n \n-#-----------------------------------------------------------------------------\n-# Function for output and information on the variables used.\n-#-----------------------------------------------------------------------------\n-\n-\n-def who(vardict=None):\n-    \"\"\"\n-    Print the NumPy arrays in the given dictionary.\n-\n-    .. deprecated:: 2.0\n-\n-    If there is no dictionary passed in or `vardict` is None then returns\n-    NumPy arrays in the globals() dictionary (all NumPy arrays in the\n-    namespace).\n-\n-    Parameters\n-    ----------\n-    vardict : dict, optional\n-        A dictionary possibly containing ndarrays.  Default is globals().\n-\n-    Returns\n-    -------\n-    out : None\n-        Returns 'None'.\n-\n-    Notes\n-    -----\n-    Prints out the name, shape, bytes and type of all of the ndarrays\n-    present in `vardict`.\n-\n-    Examples\n-    --------\n-    >>> a = np.arange(10)\n-    >>> b = np.ones(20)\n-    >>> np.who()\n-    Name            Shape            Bytes            Type\n-    ===========================================================\n-    a               10               80               int64\n-    b               20               160              float64\n-    Upper bound on total bytes  =       240\n-\n-    >>> d = {'x': np.arange(2.0), 'y': np.arange(3.0), 'txt': 'Some str',\n-    ... 'idx':5}\n-    >>> np.who(d)\n-    Name            Shape            Bytes            Type\n-    ===========================================================\n-    x               2                16               float64\n-    y               3                24               float64\n-    Upper bound on total bytes  =       40\n-\n-    \"\"\"\n-\n-    # Deprecated in NumPy 2.0, 2023-07-11\n-    warnings.warn(\n-        \"`who` is deprecated. \"\n-        \"(deprecated in NumPy 2.0)\",\n-        DeprecationWarning,\n-        stacklevel=2\n-    )\n-\n-    if vardict is None:\n-        frame = sys._getframe().f_back\n-        vardict = frame.f_globals\n-    sta = []\n-    cache = {}\n-    for name in vardict.keys():\n-        if isinstance(vardict[name], ndarray):\n-            var = vardict[name]\n-            idv = id(var)\n-            if idv in cache.keys():\n-                namestr = name + \" (%s)\" % cache[idv]\n-                original = 0\n-            else:\n-                cache[idv] = name\n-                namestr = name\n-                original = 1\n-            shapestr = \" x \".join(map(str, var.shape))\n-            bytestr = str(var.nbytes)\n-            sta.append([namestr, shapestr, bytestr, var.dtype.name,\n-                        original])\n-\n-    maxname = 0\n-    maxshape = 0\n-    maxbyte = 0\n-    totalbytes = 0\n-    for val in sta:\n-        if maxname < len(val[0]):\n-            maxname = len(val[0])\n-        if maxshape < len(val[1]):\n-            maxshape = len(val[1])\n-        if maxbyte < len(val[2]):\n-            maxbyte = len(val[2])\n-        if val[4]:\n-            totalbytes += int(val[2])\n-\n-    if len(sta) > 0:\n-        sp1 = max(10, maxname)\n-        sp2 = max(10, maxshape)\n-        sp3 = max(10, maxbyte)\n-        prval = \"Name %s Shape %s Bytes %s Type\" % (sp1*' ', sp2*' ', sp3*' ')\n-        print(prval + \"\\n\" + \"=\"*(len(prval)+5) + \"\\n\")\n-\n-    for val in sta:\n-        print(\"%s %s %s %s %s %s %s\" % (val[0], ' '*(sp1-len(val[0])+4),\n-                                        val[1], ' '*(sp2-len(val[1])+5),\n-                                        val[2], ' '*(sp3-len(val[2])+5),\n-                                        val[3]))\n-    print(\"\\nUpper bound on total bytes  =       %d\" % totalbytes)\n-    return\n-\n #-----------------------------------------------------------------------------\n \n \n",
            "comment_added_diff": {},
            "comment_deleted_diff": {
                "348": "#-----------------------------------------------------------------------------",
                "349": "# Function for output and information on the variables used.",
                "350": "#-----------------------------------------------------------------------------",
                "400": "    # Deprecated in NumPy 2.0, 2023-07-11"
            },
            "comment_modified_diff": {}
        },
        {
            "commit": "6cb136ee909b2b3909a202ac94b09c89c8dae4dd",
            "timestamp": "2023-08-24T22:48:39+02:00",
            "author": "Mateusz Sok\u00f3\u0142",
            "commit_message": "API: Remove other scalar aliases [skip ci]",
            "additions": 2,
            "deletions": 2,
            "change_type": "MODIFY",
            "diff": "@@ -1001,7 +1001,7 @@ def assert_array_almost_equal(x, y, decimal=6, err_msg='', verbose=True):\n \n     \"\"\"\n     __tracebackhide__ = True  # Hide traceback for py.test\n-    from numpy.core import number, float_, result_type\n+    from numpy.core import number, double, result_type\n     from numpy.core.numerictypes import issubdtype\n     from numpy.core.fromnumeric import any as npany\n \n@@ -1027,7 +1027,7 @@ def compare(x, y):\n         z = abs(x - y)\n \n         if not issubdtype(z.dtype, number):\n-            z = z.astype(float_)  # handle object arrays\n+            z = z.astype(np.double)  # handle object arrays\n \n         return z < 1.5 * 10.0**(-decimal)\n \n",
            "comment_added_diff": {
                "1030": "            z = z.astype(np.double)  # handle object arrays"
            },
            "comment_deleted_diff": {
                "1030": "            z = z.astype(float_)  # handle object arrays"
            },
            "comment_modified_diff": {
                "1030": "            z = z.astype(float_)  # handle object arrays"
            }
        },
        {
            "commit": "d047b4296b1f158c70d287ecd1c4a49fbd784c7e",
            "timestamp": "2023-08-24T22:50:13+02:00",
            "author": "Mateusz Sok\u00f3\u0142",
            "commit_message": "API: Finalize third batch of changes",
            "additions": 111,
            "deletions": 1,
            "change_type": "MODIFY",
            "diff": "@@ -217,7 +217,7 @@ def deprecate(*args, **kwargs):\n     Note that ``olduint`` returns a value after printing Deprecation\n     Warning:\n \n-    >>> olduint = np.lib.deprecate(np.uint)\n+    >>> olduint = np.lib.utils.deprecate(np.uint)\n     DeprecationWarning: `uint64` is deprecated! # may vary\n     >>> olduint(6)\n     6\n@@ -342,6 +342,116 @@ def byte_bounds(a):\n     return a_low, a_high\n \n \n+#-----------------------------------------------------------------------------\n+# Function for output and information on the variables used.\n+#-----------------------------------------------------------------------------\n+\n+\n+def who(vardict=None):\n+    \"\"\"\n+    Print the NumPy arrays in the given dictionary.\n+\n+    .. deprecated:: 2.0\n+\n+    If there is no dictionary passed in or `vardict` is None then returns\n+    NumPy arrays in the globals() dictionary (all NumPy arrays in the\n+    namespace).\n+\n+    Parameters\n+    ----------\n+    vardict : dict, optional\n+        A dictionary possibly containing ndarrays.  Default is globals().\n+\n+    Returns\n+    -------\n+    out : None\n+        Returns 'None'.\n+\n+    Notes\n+    -----\n+    Prints out the name, shape, bytes and type of all of the ndarrays\n+    present in `vardict`.\n+\n+    Examples\n+    --------\n+    >>> a = np.arange(10)\n+    >>> b = np.ones(20)\n+    >>> np.lib.utils.who()\n+    Name            Shape            Bytes            Type\n+    ===========================================================\n+    a               10               80               int64\n+    b               20               160              float64\n+    Upper bound on total bytes  =       240\n+\n+    >>> d = {'x': np.arange(2.0), 'y': np.arange(3.0), 'txt': 'Some str',\n+    ... 'idx':5}\n+    >>> np.who(d)\n+    Name            Shape            Bytes            Type\n+    ===========================================================\n+    x               2                16               float64\n+    y               3                24               float64\n+    Upper bound on total bytes  =       40\n+\n+    \"\"\"\n+\n+    # Deprecated in NumPy 2.0, 2023-07-11\n+    warnings.warn(\n+        \"`who` is deprecated. \"\n+        \"(deprecated in NumPy 2.0)\",\n+        DeprecationWarning,\n+        stacklevel=2\n+    )\n+\n+    if vardict is None:\n+        frame = sys._getframe().f_back\n+        vardict = frame.f_globals\n+    sta = []\n+    cache = {}\n+    for name in vardict.keys():\n+        if isinstance(vardict[name], ndarray):\n+            var = vardict[name]\n+            idv = id(var)\n+            if idv in cache.keys():\n+                namestr = name + \" (%s)\" % cache[idv]\n+                original = 0\n+            else:\n+                cache[idv] = name\n+                namestr = name\n+                original = 1\n+            shapestr = \" x \".join(map(str, var.shape))\n+            bytestr = str(var.nbytes)\n+            sta.append([namestr, shapestr, bytestr, var.dtype.name,\n+                        original])\n+\n+    maxname = 0\n+    maxshape = 0\n+    maxbyte = 0\n+    totalbytes = 0\n+    for val in sta:\n+        if maxname < len(val[0]):\n+            maxname = len(val[0])\n+        if maxshape < len(val[1]):\n+            maxshape = len(val[1])\n+        if maxbyte < len(val[2]):\n+            maxbyte = len(val[2])\n+        if val[4]:\n+            totalbytes += int(val[2])\n+\n+    if len(sta) > 0:\n+        sp1 = max(10, maxname)\n+        sp2 = max(10, maxshape)\n+        sp3 = max(10, maxbyte)\n+        prval = \"Name %s Shape %s Bytes %s Type\" % (sp1*' ', sp2*' ', sp3*' ')\n+        print(prval + \"\\n\" + \"=\"*(len(prval)+5) + \"\\n\")\n+\n+    for val in sta:\n+        print(\"%s %s %s %s %s %s %s\" % (val[0], ' '*(sp1-len(val[0])+4),\n+                                        val[1], ' '*(sp2-len(val[1])+5),\n+                                        val[2], ' '*(sp3-len(val[2])+5),\n+                                        val[3]))\n+    print(\"\\nUpper bound on total bytes  =       %d\" % totalbytes)\n+    return\n+\n #-----------------------------------------------------------------------------\n \n \n",
            "comment_added_diff": {
                "345": "#-----------------------------------------------------------------------------",
                "346": "# Function for output and information on the variables used.",
                "347": "#-----------------------------------------------------------------------------",
                "397": "    # Deprecated in NumPy 2.0, 2023-07-11"
            },
            "comment_deleted_diff": {},
            "comment_modified_diff": {}
        },
        {
            "commit": "d047b4296b1f158c70d287ecd1c4a49fbd784c7e",
            "timestamp": "2023-08-24T22:50:13+02:00",
            "author": "Mateusz Sok\u00f3\u0142",
            "commit_message": "API: Finalize third batch of changes",
            "additions": 1,
            "deletions": 1,
            "change_type": "MODIFY",
            "diff": "@@ -1027,7 +1027,7 @@ def compare(x, y):\n         z = abs(x - y)\n \n         if not issubdtype(z.dtype, number):\n-            z = z.astype(np.double)  # handle object arrays\n+            z = z.astype(double)  # handle object arrays\n \n         return z < 1.5 * 10.0**(-decimal)\n \n",
            "comment_added_diff": {
                "1030": "            z = z.astype(double)  # handle object arrays"
            },
            "comment_deleted_diff": {
                "1030": "            z = z.astype(np.double)  # handle object arrays"
            },
            "comment_modified_diff": {
                "1030": "            z = z.astype(np.double)  # handle object arrays"
            }
        },
        {
            "commit": "84e4210e12d6289492cdee0d2b8de90cabad751b",
            "timestamp": "2023-08-24T22:50:13+02:00",
            "author": "Mateusz Sok\u00f3\u0142",
            "commit_message": "API: Apply review comments",
            "additions": 2,
            "deletions": 2,
            "change_type": "MODIFY",
            "diff": "@@ -1001,7 +1001,7 @@ def assert_array_almost_equal(x, y, decimal=6, err_msg='', verbose=True):\n \n     \"\"\"\n     __tracebackhide__ = True  # Hide traceback for py.test\n-    from numpy.core import number, double, result_type\n+    from numpy.core import number, result_type\n     from numpy.core.numerictypes import issubdtype\n     from numpy.core.fromnumeric import any as npany\n \n@@ -1027,7 +1027,7 @@ def compare(x, y):\n         z = abs(x - y)\n \n         if not issubdtype(z.dtype, number):\n-            z = z.astype(double)  # handle object arrays\n+            z = z.astype(np.float64)  # handle object arrays\n \n         return z < 1.5 * 10.0**(-decimal)\n \n",
            "comment_added_diff": {
                "1030": "            z = z.astype(np.float64)  # handle object arrays"
            },
            "comment_deleted_diff": {
                "1030": "            z = z.astype(double)  # handle object arrays"
            },
            "comment_modified_diff": {
                "1030": "            z = z.astype(double)  # handle object arrays"
            }
        },
        {
            "commit": "67d8d2c38ac7e59b28495b7fff8389691909a0b4",
            "timestamp": "2023-08-24T22:52:53+02:00",
            "author": "Mateusz Sok\u00f3\u0142",
            "commit_message": "API: Remove cfloat usage",
            "additions": 0,
            "deletions": 110,
            "change_type": "MODIFY",
            "diff": "@@ -342,116 +342,6 @@ def byte_bounds(a):\n     return a_low, a_high\n \n \n-#-----------------------------------------------------------------------------\n-# Function for output and information on the variables used.\n-#-----------------------------------------------------------------------------\n-\n-\n-def who(vardict=None):\n-    \"\"\"\n-    Print the NumPy arrays in the given dictionary.\n-\n-    .. deprecated:: 2.0\n-\n-    If there is no dictionary passed in or `vardict` is None then returns\n-    NumPy arrays in the globals() dictionary (all NumPy arrays in the\n-    namespace).\n-\n-    Parameters\n-    ----------\n-    vardict : dict, optional\n-        A dictionary possibly containing ndarrays.  Default is globals().\n-\n-    Returns\n-    -------\n-    out : None\n-        Returns 'None'.\n-\n-    Notes\n-    -----\n-    Prints out the name, shape, bytes and type of all of the ndarrays\n-    present in `vardict`.\n-\n-    Examples\n-    --------\n-    >>> a = np.arange(10)\n-    >>> b = np.ones(20)\n-    >>> np.lib.utils.who()\n-    Name            Shape            Bytes            Type\n-    ===========================================================\n-    a               10               80               int64\n-    b               20               160              float64\n-    Upper bound on total bytes  =       240\n-\n-    >>> d = {'x': np.arange(2.0), 'y': np.arange(3.0), 'txt': 'Some str',\n-    ... 'idx':5}\n-    >>> np.who(d)\n-    Name            Shape            Bytes            Type\n-    ===========================================================\n-    x               2                16               float64\n-    y               3                24               float64\n-    Upper bound on total bytes  =       40\n-\n-    \"\"\"\n-\n-    # Deprecated in NumPy 2.0, 2023-07-11\n-    warnings.warn(\n-        \"`who` is deprecated. \"\n-        \"(deprecated in NumPy 2.0)\",\n-        DeprecationWarning,\n-        stacklevel=2\n-    )\n-\n-    if vardict is None:\n-        frame = sys._getframe().f_back\n-        vardict = frame.f_globals\n-    sta = []\n-    cache = {}\n-    for name in vardict.keys():\n-        if isinstance(vardict[name], ndarray):\n-            var = vardict[name]\n-            idv = id(var)\n-            if idv in cache.keys():\n-                namestr = name + \" (%s)\" % cache[idv]\n-                original = 0\n-            else:\n-                cache[idv] = name\n-                namestr = name\n-                original = 1\n-            shapestr = \" x \".join(map(str, var.shape))\n-            bytestr = str(var.nbytes)\n-            sta.append([namestr, shapestr, bytestr, var.dtype.name,\n-                        original])\n-\n-    maxname = 0\n-    maxshape = 0\n-    maxbyte = 0\n-    totalbytes = 0\n-    for val in sta:\n-        if maxname < len(val[0]):\n-            maxname = len(val[0])\n-        if maxshape < len(val[1]):\n-            maxshape = len(val[1])\n-        if maxbyte < len(val[2]):\n-            maxbyte = len(val[2])\n-        if val[4]:\n-            totalbytes += int(val[2])\n-\n-    if len(sta) > 0:\n-        sp1 = max(10, maxname)\n-        sp2 = max(10, maxshape)\n-        sp3 = max(10, maxbyte)\n-        prval = \"Name %s Shape %s Bytes %s Type\" % (sp1*' ', sp2*' ', sp3*' ')\n-        print(prval + \"\\n\" + \"=\"*(len(prval)+5) + \"\\n\")\n-\n-    for val in sta:\n-        print(\"%s %s %s %s %s %s %s\" % (val[0], ' '*(sp1-len(val[0])+4),\n-                                        val[1], ' '*(sp2-len(val[1])+5),\n-                                        val[2], ' '*(sp3-len(val[2])+5),\n-                                        val[3]))\n-    print(\"\\nUpper bound on total bytes  =       %d\" % totalbytes)\n-    return\n-\n #-----------------------------------------------------------------------------\n \n \n",
            "comment_added_diff": {},
            "comment_deleted_diff": {
                "345": "#-----------------------------------------------------------------------------",
                "346": "# Function for output and information on the variables used.",
                "347": "#-----------------------------------------------------------------------------",
                "397": "    # Deprecated in NumPy 2.0, 2023-07-11"
            },
            "comment_modified_diff": {}
        }
    ],
    "test_typing.py": [
        {
            "commit": "7de0dbb01d8ab523b99bae0f2503cdd02b89c814",
            "timestamp": "2023-08-15T09:28:50+02:00",
            "author": "Ralf Gommers",
            "commit_message": "TST: disable mypy tests in test suite unless an environment variable is set\n\nThese tests are super slow, and they're effectively always passing in CI.\nRunning them on all \"full\" test suite runs is too expensive. Note that\nSciPy has an XSLOW mark, NumPy does not. So use an env var for now.\n\n[skip circle] [skip travis] [skip azp]",
            "additions": 15,
            "deletions": 0,
            "change_type": "MODIFY",
            "diff": "@@ -18,6 +18,21 @@\n     _C_INTP,\n )\n \n+\n+# Only trigger a full `mypy` run if this environment variable is set\n+# Note that these tests tend to take over a minute even on a macOS M1 CPU,\n+# and more than that in CI.\n+RUN_MYPY = \"NPY_RUN_MYPY_IN_TESTSUITE\" in os.environ\n+if RUN_MYPY and RUN_MYPY not in ('0', '', 'false'):\n+    RUN_MYPY = True\n+\n+# Skips all functions in this file\n+pytestmark = pytest.mark.skipif(\n+    not RUN_MYPY,\n+    reason=\"`NPY_RUN_MYPY_IN_TESTSUITE` not set\"\n+)\n+\n+\n try:\n     from mypy import api\n except ImportError:\n",
            "comment_added_diff": {
                "22": "# Only trigger a full `mypy` run if this environment variable is set",
                "23": "# Note that these tests tend to take over a minute even on a macOS M1 CPU,",
                "24": "# and more than that in CI.",
                "29": "# Skips all functions in this file"
            },
            "comment_deleted_diff": {},
            "comment_modified_diff": {}
        },
        {
            "commit": "fb65dc5198de809b376c3921d1817414cd6b45ff",
            "timestamp": "2023-09-04T17:30:02+02:00",
            "author": "Bas van Beek",
            "commit_message": "TYP: Overhaul the typing test suite",
            "additions": 51,
            "deletions": 233,
            "change_type": "MODIFY",
            "diff": "@@ -1,22 +1,15 @@\n from __future__ import annotations\n \n import importlib.util\n-import itertools\n import os\n import re\n import shutil\n from collections import defaultdict\n from collections.abc import Iterator\n-from typing import IO, TYPE_CHECKING\n+from typing import TYPE_CHECKING\n \n import pytest\n-import numpy as np\n-import numpy.typing as npt\n-from numpy.typing.mypy_plugin import (\n-    _PRECISION_DICT,\n-    _EXTENDED_PRECISION_LIST,\n-    _C_INTP,\n-)\n+from numpy.typing.mypy_plugin import _EXTENDED_PRECISION_LIST\n \n \n # Only trigger a full `mypy` run if this environment variable is set\n@@ -55,7 +48,7 @@\n \n #: A dictionary with file names as keys and lists of the mypy stdout as values.\n #: To-be populated by `run_mypy`.\n-OUTPUT_MYPY: dict[str, list[str]] = {}\n+OUTPUT_MYPY: defaultdict[str, list[str]] = defaultdict(list)\n \n \n def _key_func(key: str) -> str:\n@@ -67,10 +60,11 @@ def _key_func(key: str) -> str:\n     return os.path.join(drive, tail.split(\":\", 1)[0])\n \n \n-def _strip_filename(msg: str) -> str:\n-    \"\"\"Strip the filename from a mypy message.\"\"\"\n+def _strip_filename(msg: str) -> tuple[int, str]:\n+    \"\"\"Strip the filename and line number from a mypy message.\"\"\"\n     _, tail = os.path.splitdrive(msg)\n-    return tail.split(\":\", 1)[-1]\n+    _, lineno, msg = tail.split(\":\", 2)\n+    return int(lineno), msg.strip()\n \n \n def strip_func(match: re.Match[str]) -> str:\n@@ -96,6 +90,7 @@ def run_mypy() -> None:\n     ):\n         shutil.rmtree(CACHE_DIR)\n \n+    split_pattern = re.compile(r\"(\\s+)?\\^(\\~+)?\")\n     for directory in (PASS_DIR, REVEAL_DIR, FAIL_DIR, MISC_DIR):\n         # Run mypy\n         stdout, stderr, exit_code = api.run([\n@@ -109,11 +104,20 @@ def run_mypy() -> None:\n             pytest.fail(f\"Unexpected mypy standard error\\n\\n{stderr}\")\n         elif exit_code not in {0, 1}:\n             pytest.fail(f\"Unexpected mypy exit code: {exit_code}\\n\\n{stdout}\")\n-        stdout = stdout.replace('*', '')\n \n-        # Parse the output\n-        iterator = itertools.groupby(stdout.split(\"\\n\"), key=_key_func)\n-        OUTPUT_MYPY.update((k, list(v)) for k, v in iterator if k)\n+        str_concat = \"\"\n+        filename: str | None = None\n+        for i in stdout.split(\"\\n\"):\n+            if \"note:\" in i:\n+                continue\n+            if filename is None:\n+                filename = _key_func(i)\n+\n+            str_concat += f\"{i}\\n\"\n+            if split_pattern.match(i) is not None:\n+                OUTPUT_MYPY[filename].append(str_concat)\n+                str_concat = \"\"\n+                filename = None\n \n \n def get_test_cases(directory: str) -> Iterator[ParameterSet]:\n@@ -133,7 +137,7 @@ def test_success(path) -> None:\n     output_mypy = OUTPUT_MYPY\n     if path in output_mypy:\n         msg = \"Unexpected mypy output\\n\\n\"\n-        msg += \"\\n\".join(_strip_filename(v) for v in output_mypy[path])\n+        msg += \"\\n\".join(_strip_filename(v)[1] for v in output_mypy[path])\n         raise AssertionError(msg)\n \n \n@@ -150,15 +154,9 @@ def test_fail(path: str) -> None:\n \n     output_mypy = OUTPUT_MYPY\n     assert path in output_mypy\n+\n     for error_line in output_mypy[path]:\n-        error_line = _strip_filename(error_line).split(\"\\n\", 1)[0]\n-        match = re.match(\n-            r\"(?P<lineno>\\d+): (error|note): .+$\",\n-            error_line,\n-        )\n-        if match is None:\n-            raise ValueError(f\"Unexpected error line format: {error_line}\")\n-        lineno = int(match.group('lineno'))\n+        lineno, error_line = _strip_filename(error_line)\n         errors[lineno] += f'{error_line}\\n'\n \n     for i, line in enumerate(lines):\n@@ -190,7 +188,7 @@ def test_fail(path: str) -> None:\n _FAIL_MSG2 = \"\"\"Error mismatch at line {}\n \n Expression: {}\n-Expected error: {!r}\n+Expected error: {}\n Observed error: {!r}\n \"\"\"\n \n@@ -210,141 +208,10 @@ def _test_fail(\n         ))\n \n \n-def _construct_ctypes_dict() -> dict[str, str]:\n-    dct = {\n-        \"ubyte\": \"c_ubyte\",\n-        \"ushort\": \"c_ushort\",\n-        \"uintc\": \"c_uint\",\n-        \"uint\": \"c_ulong\",\n-        \"ulonglong\": \"c_ulonglong\",\n-        \"byte\": \"c_byte\",\n-        \"short\": \"c_short\",\n-        \"intc\": \"c_int\",\n-        \"int_\": \"c_long\",\n-        \"longlong\": \"c_longlong\",\n-        \"single\": \"c_float\",\n-        \"double\": \"c_double\",\n-        \"longdouble\": \"c_longdouble\",\n-    }\n-\n-    # Match `ctypes` names to the first ctypes type with a given kind and\n-    # precision, e.g. {\"c_double\": \"c_double\", \"c_longdouble\": \"c_double\"}\n-    # if both types represent 64-bit floats.\n-    # In this context \"first\" is defined by the order of `dct`\n-    ret = {}\n-    visited: dict[tuple[str, int], str] = {}\n-    for np_name, ct_name in dct.items():\n-        np_scalar = getattr(np, np_name)()\n-\n-        # Find the first `ctypes` type for a given `kind`/`itemsize` combo\n-        key = (np_scalar.dtype.kind, np_scalar.dtype.itemsize)\n-        ret[ct_name] = visited.setdefault(key, f\"ctypes.{ct_name}\")\n-    return ret\n-\n-\n-def _construct_format_dict() -> dict[str, str]:\n-    dct = {k.split(\".\")[-1]: v.replace(\"numpy\", \"numpy._typing\") for\n-           k, v in _PRECISION_DICT.items()}\n-\n-    return {\n-        \"uint8\": \"numpy.unsignedinteger[numpy._typing._8Bit]\",\n-        \"uint16\": \"numpy.unsignedinteger[numpy._typing._16Bit]\",\n-        \"uint32\": \"numpy.unsignedinteger[numpy._typing._32Bit]\",\n-        \"uint64\": \"numpy.unsignedinteger[numpy._typing._64Bit]\",\n-        \"uint128\": \"numpy.unsignedinteger[numpy._typing._128Bit]\",\n-        \"uint256\": \"numpy.unsignedinteger[numpy._typing._256Bit]\",\n-        \"int8\": \"numpy.signedinteger[numpy._typing._8Bit]\",\n-        \"int16\": \"numpy.signedinteger[numpy._typing._16Bit]\",\n-        \"int32\": \"numpy.signedinteger[numpy._typing._32Bit]\",\n-        \"int64\": \"numpy.signedinteger[numpy._typing._64Bit]\",\n-        \"int128\": \"numpy.signedinteger[numpy._typing._128Bit]\",\n-        \"int256\": \"numpy.signedinteger[numpy._typing._256Bit]\",\n-        \"float16\": \"numpy.floating[numpy._typing._16Bit]\",\n-        \"float32\": \"numpy.floating[numpy._typing._32Bit]\",\n-        \"float64\": \"numpy.floating[numpy._typing._64Bit]\",\n-        \"float80\": \"numpy.floating[numpy._typing._80Bit]\",\n-        \"float96\": \"numpy.floating[numpy._typing._96Bit]\",\n-        \"float128\": \"numpy.floating[numpy._typing._128Bit]\",\n-        \"float256\": \"numpy.floating[numpy._typing._256Bit]\",\n-        \"complex64\": (\"numpy.complexfloating\"\n-                      \"[numpy._typing._32Bit, numpy._typing._32Bit]\"),\n-        \"complex128\": (\"numpy.complexfloating\"\n-                       \"[numpy._typing._64Bit, numpy._typing._64Bit]\"),\n-        \"complex160\": (\"numpy.complexfloating\"\n-                       \"[numpy._typing._80Bit, numpy._typing._80Bit]\"),\n-        \"complex192\": (\"numpy.complexfloating\"\n-                       \"[numpy._typing._96Bit, numpy._typing._96Bit]\"),\n-        \"complex256\": (\"numpy.complexfloating\"\n-                       \"[numpy._typing._128Bit, numpy._typing._128Bit]\"),\n-        \"complex512\": (\"numpy.complexfloating\"\n-                       \"[numpy._typing._256Bit, numpy._typing._256Bit]\"),\n-\n-        \"ubyte\": f\"numpy.unsignedinteger[{dct['_NBitByte']}]\",\n-        \"ushort\": f\"numpy.unsignedinteger[{dct['_NBitShort']}]\",\n-        \"uintc\": f\"numpy.unsignedinteger[{dct['_NBitIntC']}]\",\n-        \"uintp\": f\"numpy.unsignedinteger[{dct['_NBitIntP']}]\",\n-        \"uint\": f\"numpy.unsignedinteger[{dct['_NBitInt']}]\",\n-        \"ulonglong\": f\"numpy.unsignedinteger[{dct['_NBitLongLong']}]\",\n-        \"byte\": f\"numpy.signedinteger[{dct['_NBitByte']}]\",\n-        \"short\": f\"numpy.signedinteger[{dct['_NBitShort']}]\",\n-        \"intc\": f\"numpy.signedinteger[{dct['_NBitIntC']}]\",\n-        \"intp\": f\"numpy.signedinteger[{dct['_NBitIntP']}]\",\n-        \"int_\": f\"numpy.signedinteger[{dct['_NBitInt']}]\",\n-        \"longlong\": f\"numpy.signedinteger[{dct['_NBitLongLong']}]\",\n-\n-        \"half\": f\"numpy.floating[{dct['_NBitHalf']}]\",\n-        \"single\": f\"numpy.floating[{dct['_NBitSingle']}]\",\n-        \"double\": f\"numpy.floating[{dct['_NBitDouble']}]\",\n-        \"longdouble\": f\"numpy.floating[{dct['_NBitLongDouble']}]\",\n-        \"csingle\": (\"numpy.complexfloating\"\n-                    f\"[{dct['_NBitSingle']}, {dct['_NBitSingle']}]\"),\n-        \"cdouble\": (\"numpy.complexfloating\"\n-                    f\"[{dct['_NBitDouble']}, {dct['_NBitDouble']}]\"),\n-        \"clongdouble\": (\n-            \"numpy.complexfloating\"\n-            f\"[{dct['_NBitLongDouble']}, {dct['_NBitLongDouble']}]\"\n-        ),\n-\n-        # numpy.typing\n-        \"_NBitInt\": dct['_NBitInt'],\n-\n-        # numpy.ctypeslib\n-        \"c_intp\": f\"ctypes.{_C_INTP}\"\n-    }\n-\n-\n-#: A dictionary with all supported format keys (as keys)\n-#: and matching values\n-FORMAT_DICT: dict[str, str] = _construct_format_dict()\n-FORMAT_DICT.update(_construct_ctypes_dict())\n-\n-\n-def _parse_reveals(file: IO[str]) -> tuple[npt.NDArray[np.str_], list[str]]:\n-    \"\"\"Extract and parse all ``\"  # E: \"`` comments from the passed\n-    file-like object.\n-\n-    All format keys will be substituted for their respective value\n-    from `FORMAT_DICT`, *e.g.* ``\"{float64}\"`` becomes\n-    ``\"numpy.floating[numpy._typing._64Bit]\"``.\n-    \"\"\"\n-    string = file.read().replace(\"*\", \"\")\n-\n-    # Grab all `# E:`-based comments and matching expressions\n-    expression_array, _, comments_array = np.char.partition(\n-        string.split(\"\\n\"), sep=\"  # E: \"\n-    ).T\n-    comments = \"/n\".join(comments_array)\n-\n-    # Only search for the `{*}` pattern within comments, otherwise\n-    # there is the risk of accidentally grabbing dictionaries and sets\n-    key_set = set(re.findall(r\"\\{(.*?)\\}\", comments))\n-    kwargs = {\n-        k: FORMAT_DICT.get(k, f\"<UNRECOGNIZED FORMAT KEY {k!r}>\") for\n-        k in key_set\n-    }\n-    fmt_str = comments.format(**kwargs)\n+_REVEAL_MSG = \"\"\"Reveal mismatch at line {}\n \n-    return expression_array, fmt_str.split(\"/n\")\n+{}\n+\"\"\"\n \n \n @pytest.mark.slow\n@@ -356,53 +223,13 @@ def test_reveal(path: str) -> None:\n     \"\"\"\n     __tracebackhide__ = True\n \n-    with open(path) as fin:\n-        expression_array, reveal_list = _parse_reveals(fin)\n-\n     output_mypy = OUTPUT_MYPY\n-    assert path in output_mypy\n-    for error_line in output_mypy[path]:\n-        error_line = _strip_filename(error_line)\n-        match = re.match(\n-            r\"(?P<lineno>\\d+): note: .+$\",\n-            error_line,\n-        )\n-        if match is None:\n-            raise ValueError(f\"Unexpected reveal line format: {error_line}\")\n-        lineno = int(match.group('lineno')) - 1\n-        assert \"Revealed type is\" in error_line\n-\n-        marker = reveal_list[lineno]\n-        expression = expression_array[lineno]\n-        _test_reveal(path, expression, marker, error_line, 1 + lineno)\n-\n+    if path not in output_mypy:\n+        return\n \n-_REVEAL_MSG = \"\"\"Reveal mismatch at line {}\n-\n-Expression: {}\n-Expected reveal: {!r}\n-Observed reveal: {!r}\n-\"\"\"\n-_STRIP_PATTERN = re.compile(r\"(\\w+\\.)+(\\w+)\")\n-\n-\n-def _test_reveal(\n-    path: str,\n-    expression: str,\n-    reveal: str,\n-    expected_reveal: str,\n-    lineno: int,\n-) -> None:\n-    \"\"\"Error-reporting helper function for `test_reveal`.\"\"\"\n-    stripped_reveal = _STRIP_PATTERN.sub(strip_func, reveal)\n-    stripped_expected_reveal = _STRIP_PATTERN.sub(strip_func, expected_reveal)\n-    if stripped_reveal not in stripped_expected_reveal:\n-        raise AssertionError(\n-            _REVEAL_MSG.format(lineno,\n-                               expression,\n-                               stripped_expected_reveal,\n-                               stripped_reveal)\n-        )\n+    for error_line in output_mypy[path]:\n+        lineno, error_line = _strip_filename(error_line)\n+        raise AssertionError(_REVEAL_MSG.format(lineno, error_line))\n \n \n @pytest.mark.slow\n@@ -424,18 +251,18 @@ def test_code_runs(path: str) -> None:\n \n \n LINENO_MAPPING = {\n-    3: \"uint128\",\n-    4: \"uint256\",\n-    6: \"int128\",\n-    7: \"int256\",\n-    9: \"float80\",\n-    10: \"float96\",\n-    11: \"float128\",\n-    12: \"float256\",\n-    14: \"complex160\",\n-    15: \"complex192\",\n-    16: \"complex256\",\n-    17: \"complex512\",\n+    11: \"uint128\",\n+    12: \"uint256\",\n+    14: \"int128\",\n+    15: \"int256\",\n+    17: \"float80\",\n+    18: \"float96\",\n+    19: \"float128\",\n+    20: \"float256\",\n+    22: \"complex160\",\n+    23: \"complex192\",\n+    24: \"complex256\",\n+    25: \"complex512\",\n }\n \n \n@@ -450,21 +277,12 @@ def test_extended_precision() -> None:\n         expression_list = f.readlines()\n \n     for _msg in output_mypy[path]:\n-        *_, _lineno, msg_typ, msg = _msg.split(\":\")\n-\n-        msg = _strip_filename(msg)\n-        lineno = int(_lineno)\n+        lineno, msg = _strip_filename(_msg)\n         expression = expression_list[lineno - 1].rstrip(\"\\n\")\n-        msg_typ = msg_typ.strip()\n-        assert msg_typ in {\"error\", \"note\"}\n \n         if LINENO_MAPPING[lineno] in _EXTENDED_PRECISION_LIST:\n-            if msg_typ == \"error\":\n-                raise ValueError(f\"Unexpected reveal line format: {lineno}\")\n-            else:\n-                marker = FORMAT_DICT[LINENO_MAPPING[lineno]]\n-                _test_reveal(path, expression, marker, msg, lineno)\n-        else:\n-            if msg_typ == \"error\":\n-                marker = \"Module has no attribute\"\n-                _test_fail(path, expression, marker, msg, lineno)\n+            raise AssertionError(_REVEAL_MSG.format(lineno, msg))\n+        elif \"error\" not in msg:\n+            _test_fail(\n+                path, expression, msg, 'Expression is of type \"Any\"', lineno\n+            )\n",
            "comment_added_diff": {},
            "comment_deleted_diff": {
                "114": "        # Parse the output",
                "230": "    # Match `ctypes` names to the first ctypes type with a given kind and",
                "231": "    # precision, e.g. {\"c_double\": \"c_double\", \"c_longdouble\": \"c_double\"}",
                "232": "    # if both types represent 64-bit floats.",
                "233": "    # In this context \"first\" is defined by the order of `dct`",
                "239": "        # Find the first `ctypes` type for a given `kind`/`itemsize` combo",
                "308": "        # numpy.typing",
                "311": "        # numpy.ctypeslib",
                "316": "#: A dictionary with all supported format keys (as keys)",
                "317": "#: and matching values",
                "323": "    \"\"\"Extract and parse all ``\"  # E: \"`` comments from the passed",
                "332": "    # Grab all `# E:`-based comments and matching expressions",
                "334": "        string.split(\"\\n\"), sep=\"  # E: \"",
                "338": "    # Only search for the `{*}` pattern within comments, otherwise",
                "339": "    # there is the risk of accidentally grabbing dictionaries and sets"
            },
            "comment_modified_diff": {}
        }
    ],
    "runtests.py": [
        {
            "commit": "2293a623b9deebdd284336e223ac175a7baa77b0",
            "timestamp": "2023-01-30T08:24:02-05:00",
            "author": "DWesl",
            "commit_message": "CI: Rebase numpy DLLs in runtests.py.\n\nThis assumes NumPy is rebased before tests run,\nbut does not assume the locations are in the database.",
            "additions": 11,
            "deletions": 0,
            "change_type": "MODIFY",
            "diff": "@@ -545,6 +545,17 @@ def build_project(args):\n             print(\"Build failed!\")\n         sys.exit(1)\n \n+    # Rebase\n+    if sys.platform == \"cygwin\":\n+        from pathlib import path\n+        testenv_root = Path(config_vars[\"platbase\"])\n+        dll_list = testenv_root.glob(\"**/*.dll\")\n+        rebase_cmd = [\"/usr/bin/rebase\", \"--database\", \"--oblivious\"]\n+        rebase_cmd.extend(dll_list)\n+        if subprocess.run(rebase_cmd):\n+            print(\"Rebase failed\")\n+            sys.exit(1)\n+\n     return site_dir, site_dir_noarch\n \n def asv_compare_config(bench_path, args, h_commits):\n",
            "comment_added_diff": {
                "548": "    # Rebase"
            },
            "comment_deleted_diff": {},
            "comment_modified_diff": {}
        },
        {
            "commit": "63a1fee8d1c1e442aa8ca08ac7cefb195f99b2a9",
            "timestamp": "2023-09-13T22:52:14+02:00",
            "author": "Ralf Gommers",
            "commit_message": "MAINT: remove all `setup.py`'s and related files for distutils builds",
            "additions": 0,
            "deletions": 714,
            "change_type": "DELETE",
            "diff": "@@ -1,714 +0,0 @@\n-#!/usr/bin/env python3\n-\"\"\"\n-runtests.py [OPTIONS] [-- ARGS]\n-\n-Run tests, building the project first.\n-\n-Examples::\n-\n-    $ python runtests.py\n-    $ python runtests.py -s {SAMPLE_SUBMODULE}\n-    $ # Run a standalone test function:\n-    $ python runtests.py -t {SAMPLE_TEST}\n-    $ # Run a test defined as a method of a TestXXX class:\n-    $ python runtests.py -t {SAMPLE_TEST2}\n-    $ python runtests.py --ipython\n-    $ python runtests.py --python somescript.py\n-    $ python runtests.py --bench\n-    $ python runtests.py --durations 20\n-\n-Run a debugger:\n-\n-    $ gdb --args python runtests.py [...other args...]\n-\n-Disable pytest capturing of output by using its '-s' option:\n-\n-    $ python runtests.py -- -s\n-\n-Generate C code coverage listing under build/lcov/:\n-(requires https://github.com/linux-test-project/lcov)\n-\n-    $ python runtests.py --gcov [...other args...]\n-    $ python runtests.py --lcov-html\n-\n-Run lint checks.\n-Provide target branch name or `uncommitted` to check before committing:\n-\n-    $ python runtests.py --lint main\n-    $ python runtests.py --lint uncommitted\n-\n-\"\"\"\n-#\n-# This is a generic test runner script for projects using NumPy's test\n-# framework. Change the following values to adapt to your project:\n-#\n-\n-PROJECT_MODULE = \"numpy\"\n-PROJECT_ROOT_FILES = ['numpy', 'LICENSE.txt', 'setup.py']\n-SAMPLE_TEST = \"numpy/linalg/tests/test_linalg.py::test_byteorder_check\"\n-SAMPLE_TEST2 = \"numpy/core/tests/test_memmap.py::TestMemmap::test_open_with_filename\"\n-SAMPLE_SUBMODULE = \"linalg\"\n-\n-EXTRA_PATH = ['/usr/lib/ccache', '/usr/lib/f90cache',\n-              '/usr/local/lib/ccache', '/usr/local/lib/f90cache']\n-\n-# ---------------------------------------------------------------------\n-\n-\n-if __doc__ is None:\n-    __doc__ = \"Run without -OO if you want usage info\"\n-else:\n-    __doc__ = __doc__.format(**globals())\n-\n-\n-import sys\n-import os, glob\n-\n-# In case we are run from the source directory, we don't want to import the\n-# project from there:\n-sys.path.pop(0)\n-\n-import shutil\n-import subprocess\n-import time\n-from argparse import ArgumentParser, REMAINDER\n-\n-ROOT_DIR = os.path.abspath(os.path.join(os.path.dirname(__file__)))\n-\n-def main(argv):\n-    parser = ArgumentParser(usage=__doc__.lstrip())\n-    parser.add_argument(\"--verbose\", \"-v\", action=\"count\", default=1,\n-                        help=\"Add one verbosity level to pytest. Default is 0\")\n-    parser.add_argument(\"--debug-info\", action=\"store_true\",\n-                        help=(\"Add --verbose-cfg to build_src to show \"\n-                              \"compiler configuration output while creating \"\n-                              \"_numpyconfig.h and config.h\"))\n-    parser.add_argument(\"--no-build\", \"-n\", action=\"store_true\", default=False,\n-                        help=\"Do not build the project (use system installed \"\n-                             \"version)\")\n-    parser.add_argument(\"--build-only\", \"-b\", action=\"store_true\",\n-                        default=False, help=\"Just build, do not run any tests\")\n-    parser.add_argument(\"--doctests\", action=\"store_true\", default=False,\n-                        help=\"Run doctests in module\")\n-    parser.add_argument(\"--refguide-check\", action=\"store_true\", default=False,\n-                        help=\"Run refguide (doctest) check (do not run \"\n-                             \"regular tests.)\")\n-    parser.add_argument(\"--coverage\", action=\"store_true\", default=False,\n-                        help=(\"Report coverage of project code. HTML output \"\n-                              \"goes under build/coverage\"))\n-    parser.add_argument(\"--lint\", default=None,\n-                        help=\"'<Target Branch>' or 'uncommitted', passed to \"\n-                             \"tools/linter.py [--branch BRANCH] \"\n-                             \"[--uncommitted]\")\n-    parser.add_argument(\"--durations\", action=\"store\", default=-1, type=int,\n-                        help=(\"Time N slowest tests, time all if 0, time none \"\n-                              \"if < 0\"))\n-    parser.add_argument(\"--gcov\", action=\"store_true\", default=False,\n-                        help=(\"Enable C code coverage via gcov (requires \"\n-                              \"GCC). gcov output goes to build/**/*.gc*\"))\n-    parser.add_argument(\"--lcov-html\", action=\"store_true\", default=False,\n-                        help=(\"Produce HTML for C code coverage information \"\n-                              \"from a previous run with --gcov. \"\n-                              \"HTML output goes to build/lcov/\"))\n-    parser.add_argument(\"--mode\", \"-m\", default=\"fast\",\n-                        help=\"'fast', 'full', or something that could be \"\n-                             \"passed to nosetests -A [default: fast]\")\n-    parser.add_argument(\"--submodule\", \"-s\", default=None,\n-                        help=\"Submodule whose tests to run (cluster, \"\n-                             \"constants, ...)\")\n-    parser.add_argument(\"--pythonpath\", \"-p\", default=None,\n-                        help=\"Paths to prepend to PYTHONPATH\")\n-    parser.add_argument(\"--tests\", \"-t\", action='append',\n-                        help=\"Specify tests to run\")\n-    parser.add_argument(\"--python\", action=\"store_true\",\n-                        help=\"Start a Python shell with PYTHONPATH set\")\n-    parser.add_argument(\"--ipython\", \"-i\", action=\"store_true\",\n-                        help=\"Start IPython shell with PYTHONPATH set\")\n-    parser.add_argument(\"--shell\", action=\"store_true\",\n-                        help=\"Start Unix shell with PYTHONPATH set\")\n-    parser.add_argument(\"--mypy\", action=\"store_true\",\n-                        help=\"Run mypy on files with NumPy on the MYPYPATH\")\n-    parser.add_argument(\"--debug\", \"-g\", action=\"store_true\",\n-                        help=\"Debug build\")\n-    parser.add_argument(\"--parallel\", \"-j\", type=int, default=0,\n-                        help=\"Number of parallel jobs during build\")\n-    parser.add_argument(\"--warn-error\", action=\"store_true\",\n-                        help=\"Set -Werror to convert all compiler warnings to \"\n-                             \"errors\")\n-    parser.add_argument(\"--cpu-baseline\", default=None,\n-                        help=\"Specify a list of enabled baseline CPU \"\n-                             \"optimizations\"),\n-    parser.add_argument(\"--cpu-dispatch\", default=None,\n-                        help=\"Specify a list of dispatched CPU optimizations\"),\n-    parser.add_argument(\"--disable-optimization\", action=\"store_true\",\n-                        help=\"Disable CPU optimized code (dispatch, simd, \"\n-                             \"fast, ...)\"),\n-    parser.add_argument(\"--simd-test\", default=None,\n-                        help=\"Specify a list of CPU optimizations to be \"\n-                             \"tested against NumPy SIMD interface\"),\n-    parser.add_argument(\"--show-build-log\", action=\"store_true\",\n-                        help=\"Show build output rather than using a log file\")\n-    parser.add_argument(\"--bench\", action=\"store_true\",\n-                        help=\"Run benchmark suite instead of test suite\")\n-    parser.add_argument(\"--bench-compare\", action=\"store\", metavar=\"COMMIT\",\n-                        help=(\"Compare benchmark results of current HEAD to \"\n-                              \"BEFORE. Use an additional \"\n-                              \"--bench-compare=COMMIT to override HEAD with \"\n-                              \"COMMIT. Note that you need to commit your \"\n-                              \"changes first!\"))\n-    parser.add_argument(\"args\", metavar=\"ARGS\", default=[], nargs=REMAINDER,\n-                        help=\"Arguments to pass to pytest, asv, mypy, Python \"\n-                             \"or shell\")\n-    args = parser.parse_args(argv)\n-\n-    if args.durations < 0:\n-        args.durations = -1\n-\n-    if args.bench_compare:\n-        args.bench = True\n-        args.no_build = True # ASV does the building\n-\n-    if args.lcov_html:\n-        # generate C code coverage output\n-        lcov_generate()\n-        sys.exit(0)\n-\n-    if args.pythonpath:\n-        for p in reversed(args.pythonpath.split(os.pathsep)):\n-            sys.path.insert(0, p)\n-\n-    if args.gcov:\n-        gcov_reset_counters()\n-\n-    if args.debug and args.bench:\n-        print(\"*** Benchmarks should not be run against debug \"\n-              \"version; remove -g flag ***\")\n-\n-    if args.lint:\n-        check_lint(args.lint)\n-\n-    if not args.no_build:\n-        # we need the noarch path in case the package is pure python.\n-        site_dir, site_dir_noarch = build_project(args)\n-        sys.path.insert(0, site_dir)\n-        sys.path.insert(0, site_dir_noarch)\n-        os.environ['PYTHONPATH'] = \\\n-            os.pathsep.join((\n-                site_dir,\n-                site_dir_noarch,\n-                os.environ.get('PYTHONPATH', '')\n-            ))\n-    else:\n-        if not args.bench_compare:\n-            _temp = __import__(PROJECT_MODULE)\n-            site_dir = os.path.sep.join(_temp.__file__.split(os.path.sep)[:-2])\n-\n-    extra_argv = args.args[:]\n-    if not args.bench:\n-        # extra_argv may also lists selected benchmarks\n-        if extra_argv and extra_argv[0] == '--':\n-            extra_argv = extra_argv[1:]\n-\n-    if args.python:\n-        # Debugging issues with warnings is much easier if you can see them\n-        print(\"Enabling display of all warnings\")\n-        import warnings\n-        import types\n-\n-        warnings.filterwarnings(\"always\")\n-        if extra_argv:\n-            # Don't use subprocess, since we don't want to include the\n-            # current path in PYTHONPATH.\n-            sys.argv = extra_argv\n-            with open(extra_argv[0]) as f:\n-                script = f.read()\n-            sys.modules['__main__'] = types.ModuleType('__main__')\n-            ns = dict(__name__='__main__',\n-                      __file__=extra_argv[0])\n-            exec(script, ns)\n-            sys.exit(0)\n-        else:\n-            import code\n-            code.interact()\n-            sys.exit(0)\n-\n-    if args.ipython:\n-        # Debugging issues with warnings is much easier if you can see them\n-        print(\"Enabling display of all warnings and pre-importing numpy as np\")\n-        import warnings; warnings.filterwarnings(\"always\")\n-        import IPython\n-        import numpy as np\n-        IPython.embed(colors='neutral', user_ns={\"np\": np})\n-        sys.exit(0)\n-\n-    if args.shell:\n-        shell = os.environ.get('SHELL', 'cmd' if os.name == 'nt' else 'sh')\n-        print(\"Spawning a shell ({})...\".format(shell))\n-        subprocess.call([shell] + extra_argv)\n-        sys.exit(0)\n-\n-    if args.mypy:\n-        try:\n-            import mypy.api\n-        except ImportError:\n-            raise RuntimeError(\n-                \"Mypy not found. Please install it by running \"\n-                \"pip install -r test_requirements.txt from the repo root\"\n-            )\n-\n-        # By default mypy won't color the output since it isn't being\n-        # invoked from a tty.\n-        os.environ['MYPY_FORCE_COLOR'] = '1'\n-\n-        config = os.path.join(\n-            site_dir,\n-            \"numpy\",\n-            \"typing\",\n-            \"tests\",\n-            \"data\",\n-            \"mypy.ini\",\n-        )\n-\n-        report, errors, status = mypy.api.run(\n-            ['--config-file', config] + args.args\n-        )\n-        print(report, end='')\n-        print(errors, end='', file=sys.stderr)\n-        sys.exit(status)\n-\n-    if args.coverage:\n-        dst_dir = os.path.join(ROOT_DIR, 'build', 'coverage')\n-        fn = os.path.join(dst_dir, 'coverage_html.js')\n-        if os.path.isdir(dst_dir) and os.path.isfile(fn):\n-            shutil.rmtree(dst_dir)\n-        extra_argv += ['--cov-report=html:' + dst_dir]\n-\n-    if args.refguide_check:\n-        cmd = [os.path.join(ROOT_DIR, 'tools', 'refguide_check.py'),\n-               '--doctests']\n-        if args.verbose:\n-            cmd += ['-' + 'v'*args.verbose]\n-        if args.submodule:\n-            cmd += [args.submodule]\n-        os.execv(sys.executable, [sys.executable] + cmd)\n-        sys.exit(0)\n-\n-    if args.bench:\n-        # Run ASV\n-        for i, v in enumerate(extra_argv):\n-            if v.startswith(\"--\"):\n-                items = extra_argv[:i]\n-                if v == \"--\":\n-                    i += 1  # skip '--' indicating further are passed on.\n-                bench_args = extra_argv[i:]\n-                break\n-        else:\n-            items = extra_argv\n-            bench_args = []\n-\n-        if args.tests:\n-            items += args.tests\n-        if args.submodule:\n-            items += [args.submodule]\n-        for a in items:\n-            bench_args.extend(['--bench', a])\n-\n-        if not args.bench_compare:\n-            cmd = ['asv', 'run', '-n', '-e', '--python=same'] + bench_args\n-            ret = subprocess.call(cmd, cwd=os.path.join(ROOT_DIR, 'benchmarks'))\n-            sys.exit(ret)\n-        else:\n-            commits = [x.strip() for x in args.bench_compare.split(',')]\n-            if len(commits) == 1:\n-                commit_a = commits[0]\n-                commit_b = 'HEAD'\n-            elif len(commits) == 2:\n-                commit_a, commit_b = commits\n-            else:\n-                p.error(\"Too many commits to compare benchmarks for\")\n-\n-            # Check for uncommitted files\n-            if commit_b == 'HEAD':\n-                r1 = subprocess.call(['git', 'diff-index', '--quiet',\n-                                      '--cached', 'HEAD'])\n-                r2 = subprocess.call(['git', 'diff-files', '--quiet'])\n-                if r1 != 0 or r2 != 0:\n-                    print(\"*\"*80)\n-                    print(\"WARNING: you have uncommitted changes --- \"\n-                          \"these will NOT be benchmarked!\")\n-                    print(\"*\"*80)\n-\n-            # Fix commit ids (HEAD is local to current repo)\n-            out = subprocess.check_output(['git', 'rev-parse', commit_b])\n-            commit_b = out.strip().decode('ascii')\n-\n-            out = subprocess.check_output(['git', 'rev-parse', commit_a])\n-            commit_a = out.strip().decode('ascii')\n-\n-            # generate config file with the required build options\n-            asv_cfpath = [\n-                '--config', asv_compare_config(\n-                    os.path.join(ROOT_DIR, 'benchmarks'), args,\n-                    # to clear the cache if the user changed build options\n-                    (commit_a, commit_b)\n-                )\n-            ]\n-            cmd = ['asv', 'continuous', '-e', '-f', '1.05',\n-                   commit_a, commit_b] + asv_cfpath + bench_args\n-            ret = subprocess.call(cmd, cwd=os.path.join(ROOT_DIR, 'benchmarks'))\n-            sys.exit(ret)\n-\n-    if args.build_only:\n-        sys.exit(0)\n-    else:\n-        __import__(PROJECT_MODULE)\n-        test = sys.modules[PROJECT_MODULE].test\n-\n-    if args.submodule:\n-        tests = [PROJECT_MODULE + \".\" + args.submodule]\n-    elif args.tests:\n-        tests = args.tests\n-    else:\n-        tests = None\n-\n-\n-    # Run the tests under build/test\n-\n-    if not args.no_build:\n-        test_dir = site_dir\n-    else:\n-        test_dir = os.path.join(ROOT_DIR, 'build', 'test')\n-        if not os.path.isdir(test_dir):\n-            os.makedirs(test_dir)\n-\n-    shutil.copyfile(os.path.join(ROOT_DIR, '.coveragerc'),\n-                    os.path.join(test_dir, '.coveragerc'))\n-\n-    cwd = os.getcwd()\n-    try:\n-        os.chdir(test_dir)\n-        result = test(args.mode,\n-                      verbose=args.verbose,\n-                      extra_argv=extra_argv,\n-                      doctests=args.doctests,\n-                      coverage=args.coverage,\n-                      durations=args.durations,\n-                      tests=tests)\n-    finally:\n-        os.chdir(cwd)\n-\n-    if isinstance(result, bool):\n-        sys.exit(0 if result else 1)\n-    elif result.wasSuccessful():\n-        sys.exit(0)\n-    else:\n-        sys.exit(1)\n-\n-def build_project(args):\n-    \"\"\"\n-    Build a dev version of the project.\n-\n-    Returns\n-    -------\n-    site_dir\n-        site-packages directory where it was installed\n-\n-    \"\"\"\n-\n-    import sysconfig\n-\n-    root_ok = [os.path.exists(os.path.join(ROOT_DIR, fn))\n-               for fn in PROJECT_ROOT_FILES]\n-    if not all(root_ok):\n-        print(\"To build the project, run runtests.py in \"\n-              \"git checkout or unpacked source\")\n-        sys.exit(1)\n-\n-    dst_dir = os.path.join(ROOT_DIR, 'build', 'testenv')\n-\n-    env = dict(os.environ)\n-    cmd = [sys.executable, 'setup.py']\n-\n-    # Always use ccache, if installed\n-    env['PATH'] = os.pathsep.join(EXTRA_PATH + env.get('PATH', '').split(os.pathsep))\n-    cvars = sysconfig.get_config_vars()\n-    compiler = env.get('CC') or cvars.get('CC', '')\n-    if 'gcc' in compiler:\n-        # Check that this isn't clang masquerading as gcc.\n-        if sys.platform != 'darwin' or 'gnu-gcc' in compiler:\n-            # add flags used as werrors\n-            warnings_as_errors = ' '.join([\n-                # from tools/travis-test.sh\n-                '-Werror=vla',\n-                '-Werror=nonnull',\n-                '-Werror=pointer-arith',\n-                '-Wlogical-op',\n-                # from sysconfig\n-                '-Werror=unused-function',\n-            ])\n-            env['CFLAGS'] = warnings_as_errors + ' ' + env.get('CFLAGS', '')\n-    if args.debug or args.gcov:\n-        # assume everyone uses gcc/gfortran\n-        env['OPT'] = '-O0 -ggdb'\n-        env['FOPT'] = '-O0 -ggdb'\n-        if args.gcov:\n-            env['OPT'] = '-O0 -ggdb'\n-            env['FOPT'] = '-O0 -ggdb'\n-            env['CC'] = cvars['CC'] + ' --coverage'\n-            env['CXX'] = cvars['CXX'] + ' --coverage'\n-            env['F77'] = 'gfortran --coverage '\n-            env['F90'] = 'gfortran --coverage '\n-            env['LDSHARED'] = cvars['LDSHARED'] + ' --coverage'\n-            env['LDFLAGS'] = \" \".join(cvars['LDSHARED'].split()[1:]) + ' --coverage'\n-\n-    cmd += [\"build\"]\n-    if args.parallel > 1:\n-        cmd += [\"-j\", str(args.parallel)]\n-    if args.warn_error:\n-        cmd += [\"--warn-error\"]\n-    if args.cpu_baseline:\n-        cmd += [\"--cpu-baseline\", args.cpu_baseline]\n-    if args.cpu_dispatch:\n-        cmd += [\"--cpu-dispatch\", args.cpu_dispatch]\n-    if args.disable_optimization:\n-        cmd += [\"--disable-optimization\"]\n-    if args.simd_test is not None:\n-        cmd += [\"--simd-test\", args.simd_test]\n-    if args.debug_info:\n-        cmd += [\"build_src\", \"--verbose-cfg\"]\n-    # Install; avoid producing eggs so numpy can be imported from dst_dir.\n-    cmd += ['install', '--prefix=' + dst_dir,\n-            '--single-version-externally-managed',\n-            '--record=' + dst_dir + 'tmp_install_log.txt']\n-\n-    config_vars = dict(sysconfig.get_config_vars())\n-    config_vars[\"platbase\"] = dst_dir\n-    config_vars[\"base\"] = dst_dir\n-\n-    site_dir_template = os.path.normpath(sysconfig.get_path(\n-        'platlib', expand=False\n-    ))\n-    site_dir = site_dir_template.format(**config_vars)\n-    noarch_template = os.path.normpath(sysconfig.get_path(\n-        'purelib', expand=False\n-    ))\n-    site_dir_noarch = noarch_template.format(**config_vars)\n-\n-    # easy_install won't install to a path that Python by default cannot see\n-    # and isn't on the PYTHONPATH.  Plus, it has to exist.\n-    if not os.path.exists(site_dir):\n-        os.makedirs(site_dir)\n-    if not os.path.exists(site_dir_noarch):\n-        os.makedirs(site_dir_noarch)\n-    env['PYTHONPATH'] = \\\n-        os.pathsep.join((site_dir, site_dir_noarch, env.get('PYTHONPATH', '')))\n-\n-    log_filename = os.path.join(ROOT_DIR, 'build.log')\n-\n-    if args.show_build_log:\n-        ret = subprocess.call(cmd, env=env, cwd=ROOT_DIR)\n-    else:\n-        log_filename = os.path.join(ROOT_DIR, 'build.log')\n-        print(\"Building, see build.log...\")\n-        with open(log_filename, 'w') as log:\n-            p = subprocess.Popen(cmd, env=env, stdout=log, stderr=log,\n-                                 cwd=ROOT_DIR)\n-        try:\n-            # Wait for it to finish, and print something to indicate the\n-            # process is alive, but only if the log file has grown (to\n-            # allow continuous integration environments kill a hanging\n-            # process accurately if it produces no output)\n-            last_blip = time.time()\n-            last_log_size = os.stat(log_filename).st_size\n-            while p.poll() is None:\n-                time.sleep(0.5)\n-                if time.time() - last_blip > 60:\n-                    log_size = os.stat(log_filename).st_size\n-                    if log_size > last_log_size:\n-                        print(\"    ... build in progress\")\n-                        last_blip = time.time()\n-                        last_log_size = log_size\n-\n-            ret = p.wait()\n-        except:\n-            p.kill()\n-            p.wait()\n-            raise\n-\n-    if ret == 0:\n-        print(\"Build OK\")\n-    else:\n-        if not args.show_build_log:\n-            with open(log_filename) as f:\n-                print(f.read())\n-            print(\"Build failed!\")\n-        sys.exit(1)\n-\n-    # Rebase\n-    if sys.platform == \"cygwin\":\n-        from pathlib import path\n-        testenv_root = Path(config_vars[\"platbase\"])\n-        dll_list = testenv_root.glob(\"**/*.dll\")\n-        rebase_cmd = [\"/usr/bin/rebase\", \"--database\", \"--oblivious\"]\n-        rebase_cmd.extend(dll_list)\n-        if subprocess.run(rebase_cmd):\n-            print(\"Rebase failed\")\n-            sys.exit(1)\n-\n-    return site_dir, site_dir_noarch\n-\n-def asv_compare_config(bench_path, args, h_commits):\n-    \"\"\"\n-    Fill the required build options through custom variable\n-    'numpy_build_options' and return the generated config path.\n-    \"\"\"\n-    conf_path = os.path.join(bench_path, \"asv_compare.conf.json.tpl\")\n-    nconf_path = os.path.join(bench_path, \"_asv_compare.conf.json\")\n-\n-    # add custom build\n-    build = []\n-    if args.parallel > 1:\n-        build += [\"-j\", str(args.parallel)]\n-    if args.cpu_baseline:\n-        build += [\"--cpu-baseline\", args.cpu_baseline]\n-    if args.cpu_dispatch:\n-        build += [\"--cpu-dispatch\", args.cpu_dispatch]\n-    if args.disable_optimization:\n-        build += [\"--disable-optimization\"]\n-\n-    is_cached = asv_substitute_config(conf_path, nconf_path,\n-        numpy_build_options = ' '.join([f'\\\\\"{v}\\\\\"' for v in build]),\n-        numpy_global_options= ' '.join([f'--global-option=\\\\\"{v}\\\\\"' for v in [\"build\"] + build])\n-    )\n-    if not is_cached:\n-        asv_clear_cache(bench_path, h_commits)\n-    return nconf_path\n-\n-def asv_clear_cache(bench_path, h_commits, env_dir=\"env\"):\n-    \"\"\"\n-    Force ASV to clear the cache according to specified commit hashes.\n-    \"\"\"\n-    # FIXME: only clear the cache from the current environment dir\n-    asv_build_pattern = os.path.join(bench_path, env_dir, \"*\", \"asv-build-cache\")\n-    for asv_build_cache in glob.glob(asv_build_pattern, recursive=True):\n-        for c in h_commits:\n-            try: shutil.rmtree(os.path.join(asv_build_cache, c))\n-            except OSError: pass\n-\n-def asv_substitute_config(in_config, out_config, **custom_vars):\n-    \"\"\"\n-    A workaround to allow substituting custom tokens within\n-    ASV configuration file since there's no official way to add custom\n-    variables(e.g. env vars).\n-\n-    Parameters\n-    ----------\n-    in_config : str\n-        The path of ASV configuration file, e.g. '/path/to/asv.conf.json'\n-    out_config : str\n-        The path of generated configuration file,\n-        e.g. '/path/to/asv_substituted.conf.json'.\n-\n-    The other keyword arguments represent the custom variables.\n-\n-    Returns\n-    -------\n-    True(is cached) if 'out_config' is already generated with\n-    the same '**custom_vars' and updated with latest 'in_config',\n-    False otherwise.\n-\n-    Examples\n-    --------\n-    See asv_compare_config().\n-    \"\"\"\n-    assert in_config != out_config\n-    assert len(custom_vars) > 0\n-\n-    def sdbm_hash(*factors):\n-        chash = 0\n-        for f in factors:\n-            for char in str(f):\n-                chash  = ord(char) + (chash << 6) + (chash << 16) - chash\n-                chash &= 0xFFFFFFFF\n-        return chash\n-\n-    vars_hash = sdbm_hash(custom_vars, os.path.getmtime(in_config))\n-    try:\n-        with open(out_config) as wfd:\n-            hash_line = wfd.readline().split('hash:')\n-            if len(hash_line) > 1 and int(hash_line[1]) == vars_hash:\n-                return True\n-    except OSError:\n-        pass\n-\n-    custom_vars = {f'{{{k}}}':v for k, v in custom_vars.items()}\n-    with open(in_config, \"r\") as rfd, open(out_config, \"w\") as wfd:\n-        wfd.write(f\"// hash:{vars_hash}\\n\")\n-        wfd.write(\"// This file is automatically generated by runtests.py\\n\")\n-        for line in rfd:\n-            for key, val in custom_vars.items():\n-                line = line.replace(key, val)\n-            wfd.write(line)\n-    return False\n-\n-#\n-# GCOV support\n-#\n-def gcov_reset_counters():\n-    print(\"Removing previous GCOV .gcda files...\")\n-    build_dir = os.path.join(ROOT_DIR, 'build')\n-    for dirpath, dirnames, filenames in os.walk(build_dir):\n-        for fn in filenames:\n-            if fn.endswith('.gcda') or fn.endswith('.da'):\n-                pth = os.path.join(dirpath, fn)\n-                os.unlink(pth)\n-\n-#\n-# LCOV support\n-#\n-\n-LCOV_OUTPUT_FILE = os.path.join(ROOT_DIR, 'build', 'lcov.out')\n-LCOV_HTML_DIR = os.path.join(ROOT_DIR, 'build', 'lcov')\n-\n-def lcov_generate():\n-    try: os.unlink(LCOV_OUTPUT_FILE)\n-    except OSError: pass\n-    try: shutil.rmtree(LCOV_HTML_DIR)\n-    except OSError: pass\n-\n-    print(\"Capturing lcov info...\")\n-    subprocess.call(['lcov', '-q', '-c',\n-                     '-d', os.path.join(ROOT_DIR, 'build'),\n-                     '-b', ROOT_DIR,\n-                     '--output-file', LCOV_OUTPUT_FILE])\n-\n-    print(\"Generating lcov HTML output...\")\n-    ret = subprocess.call(['genhtml', '-q', LCOV_OUTPUT_FILE,\n-                           '--output-directory', LCOV_HTML_DIR,\n-                           '--legend', '--highlight'])\n-    if ret != 0:\n-        print(\"genhtml failed!\")\n-    else:\n-        print(\"HTML output generated under build/lcov/\")\n-\n-def check_lint(lint_args):\n-    \"\"\"\n-    Adds ROOT_DIR to path and performs lint checks.\n-    This functions exits the program with status code of lint check.\n-    \"\"\"\n-    sys.path.append(ROOT_DIR)\n-    try:\n-        from tools.linter import DiffLinter\n-    except ModuleNotFoundError as e:\n-        print(f\"Error: {e.msg}. \"\n-              \"Install using linter_requirements.txt.\")\n-        sys.exit(1)\n-\n-    uncommitted = lint_args == \"uncommitted\"\n-    branch = \"main\" if uncommitted else lint_args\n-\n-    DiffLinter(branch).run_lint(uncommitted)\n-\n-\n-if __name__ == \"__main__\":\n-    main(argv=sys.argv[1:])\n",
            "comment_added_diff": {},
            "comment_deleted_diff": {
                "1": "#!/usr/bin/env python3",
                "11": "    $ # Run a standalone test function:",
                "13": "    $ # Run a test defined as a method of a TestXXX class:",
                "41": "#",
                "42": "# This is a generic test runner script for projects using NumPy's test",
                "43": "# framework. Change the following values to adapt to your project:",
                "44": "#",
                "55": "# ---------------------------------------------------------------------",
                "67": "# In case we are run from the source directory, we don't want to import the",
                "68": "# project from there:",
                "169": "        args.no_build = True # ASV does the building",
                "172": "        # generate C code coverage output",
                "191": "        # we need the noarch path in case the package is pure python.",
                "208": "        # extra_argv may also lists selected benchmarks",
                "213": "        # Debugging issues with warnings is much easier if you can see them",
                "220": "            # Don't use subprocess, since we don't want to include the",
                "221": "            # current path in PYTHONPATH.",
                "236": "        # Debugging issues with warnings is much easier if you can see them",
                "259": "        # By default mypy won't color the output since it isn't being",
                "260": "        # invoked from a tty.",
                "297": "        # Run ASV",
                "302": "                    i += 1  # skip '--' indicating further are passed on.",
                "330": "            # Check for uncommitted files",
                "341": "            # Fix commit ids (HEAD is local to current repo)",
                "348": "            # generate config file with the required build options",
                "352": "                    # to clear the cache if the user changed build options",
                "375": "    # Run the tests under build/test",
                "432": "    # Always use ccache, if installed",
                "437": "        # Check that this isn't clang masquerading as gcc.",
                "439": "            # add flags used as werrors",
                "441": "                # from tools/travis-test.sh",
                "446": "                # from sysconfig",
                "451": "        # assume everyone uses gcc/gfortran",
                "479": "    # Install; avoid producing eggs so numpy can be imported from dst_dir.",
                "497": "    # easy_install won't install to a path that Python by default cannot see",
                "498": "    # and isn't on the PYTHONPATH.  Plus, it has to exist.",
                "517": "            # Wait for it to finish, and print something to indicate the",
                "518": "            # process is alive, but only if the log file has grown (to",
                "519": "            # allow continuous integration environments kill a hanging",
                "520": "            # process accurately if it produces no output)",
                "547": "    # Rebase",
                "568": "    # add custom build",
                "591": "    # FIXME: only clear the cache from the current environment dir",
                "654": "#",
                "655": "# GCOV support",
                "656": "#",
                "666": "#",
                "667": "# LCOV support",
                "668": "#"
            },
            "comment_modified_diff": {}
        }
    ],
    "cythonize.py": [
        {
            "commit": "63a1fee8d1c1e442aa8ca08ac7cefb195f99b2a9",
            "timestamp": "2023-09-13T22:52:14+02:00",
            "author": "Ralf Gommers",
            "commit_message": "MAINT: remove all `setup.py`'s and related files for distutils builds",
            "additions": 0,
            "deletions": 205,
            "change_type": "DELETE",
            "diff": "@@ -1,205 +0,0 @@\n-#!/usr/bin/env python3\n-\"\"\" cythonize\n-\n-Cythonize pyx files into C files as needed.\n-\n-Usage: cythonize [root_dir]\n-\n-Default [root_dir] is 'numpy'.\n-\n-Checks pyx files to see if they have been changed relative to their\n-corresponding C files.  If they have, then runs cython on these files to\n-recreate the C files.\n-\n-The script thinks that the pyx files have changed relative to the C files\n-by comparing hashes stored in a database file.\n-\n-Simple script to invoke Cython (and Tempita) on all .pyx (.pyx.in)\n-files; while waiting for a proper build system. Uses file hashes to\n-figure out if rebuild is needed.\n-\n-Originally written by Dag Sverre Seljebotn, and copied here from:\n-\n-https://raw.github.com/dagss/private-scipy-refactor/cythonize/cythonize.py\n-\n-Note: this script does not check any of the dependent C libraries; it only\n-operates on the Cython .pyx files.\n-\"\"\"\n-\n-import os\n-import re\n-import sys\n-import hashlib\n-import subprocess\n-\n-HASH_FILE = 'cythonize.dat'\n-DEFAULT_ROOT = 'numpy'\n-VENDOR = 'NumPy'\n-\n-#\n-# Rules\n-#\n-def process_pyx(fromfile, tofile):\n-    flags = ['-3', '--fast-fail']\n-    if tofile.endswith('.cxx'):\n-        flags.append('--cplus')\n-\n-    subprocess.check_call(\n-        [sys.executable, '-m', 'cython'] + flags + [\"-o\", tofile, fromfile])\n-\n-\n-def process_tempita_pyx(fromfile, tofile):\n-    import npy_tempita as tempita\n-\n-    assert fromfile.endswith('.pyx.in')\n-    with open(fromfile) as f:\n-        tmpl = f.read()\n-    pyxcontent = tempita.sub(tmpl)\n-    pyxfile = fromfile[:-len('.pyx.in')] + '.pyx'\n-    with open(pyxfile, \"w\") as f:\n-        f.write(pyxcontent)\n-    process_pyx(pyxfile, tofile)\n-\n-\n-def process_tempita_pyd(fromfile, tofile):\n-    import npy_tempita as tempita\n-\n-    assert fromfile.endswith('.pxd.in')\n-    assert tofile.endswith('.pxd')\n-    with open(fromfile) as f:\n-        tmpl = f.read()\n-    pyxcontent = tempita.sub(tmpl)\n-    with open(tofile, \"w\") as f:\n-        f.write(pyxcontent)\n-\n-def process_tempita_pxi(fromfile, tofile):\n-    import npy_tempita as tempita\n-\n-    assert fromfile.endswith('.pxi.in')\n-    assert tofile.endswith('.pxi')\n-    with open(fromfile) as f:\n-        tmpl = f.read()\n-    pyxcontent = tempita.sub(tmpl)\n-    with open(tofile, \"w\") as f:\n-        f.write(pyxcontent)\n-\n-def process_tempita_pxd(fromfile, tofile):\n-    import npy_tempita as tempita\n-\n-    assert fromfile.endswith('.pxd.in')\n-    assert tofile.endswith('.pxd')\n-    with open(fromfile) as f:\n-        tmpl = f.read()\n-    pyxcontent = tempita.sub(tmpl)\n-    with open(tofile, \"w\") as f:\n-        f.write(pyxcontent)\n-\n-rules = {\n-    # fromext : function, toext\n-    '.pyx' : (process_pyx, '.c'),\n-    '.pyx.in' : (process_tempita_pyx, '.c'),\n-    '.pxi.in' : (process_tempita_pxi, '.pxi'),\n-    '.pxd.in' : (process_tempita_pxd, '.pxd'),\n-    '.pyd.in' : (process_tempita_pyd, '.pyd'),\n-    }\n-#\n-# Hash db\n-#\n-def load_hashes(filename):\n-    # Return { filename : (sha256 of input, sha256 of output) }\n-    if os.path.isfile(filename):\n-        hashes = {}\n-        with open(filename) as f:\n-            for line in f:\n-                filename, inhash, outhash = line.split()\n-                hashes[filename] = (inhash, outhash)\n-    else:\n-        hashes = {}\n-    return hashes\n-\n-def save_hashes(hash_db, filename):\n-    with open(filename, 'w') as f:\n-        for key, value in sorted(hash_db.items()):\n-            f.write(\"%s %s %s\\n\" % (key, value[0], value[1]))\n-\n-def sha256_of_file(filename):\n-    h = hashlib.sha256()\n-    with open(filename, \"rb\") as f:\n-        h.update(f.read())\n-    return h.hexdigest()\n-\n-#\n-# Main program\n-#\n-\n-def normpath(path):\n-    path = path.replace(os.sep, '/')\n-    if path.startswith('./'):\n-        path = path[2:]\n-    return path\n-\n-def get_hash(frompath, topath):\n-    from_hash = sha256_of_file(frompath)\n-    to_hash = sha256_of_file(topath) if os.path.exists(topath) else None\n-    return (from_hash, to_hash)\n-\n-def process(path, fromfile, tofile, processor_function, hash_db):\n-    fullfrompath = os.path.join(path, fromfile)\n-    fulltopath = os.path.join(path, tofile)\n-    current_hash = get_hash(fullfrompath, fulltopath)\n-    if current_hash == hash_db.get(normpath(fullfrompath), None):\n-        print(f'{fullfrompath} has not changed')\n-        return\n-\n-    orig_cwd = os.getcwd()\n-    try:\n-        os.chdir(path)\n-        print(f'Processing {fullfrompath}')\n-        processor_function(fromfile, tofile)\n-    finally:\n-        os.chdir(orig_cwd)\n-    # changed target file, recompute hash\n-    current_hash = get_hash(fullfrompath, fulltopath)\n-    # store hash in db\n-    hash_db[normpath(fullfrompath)] = current_hash\n-\n-\n-def find_process_files(root_dir):\n-    hash_db = load_hashes(HASH_FILE)\n-    files  = [x for x in os.listdir(root_dir) if not os.path.isdir(x)]\n-    # .pxi or .pxi.in files are most likely dependencies for\n-    # .pyx files, so we need to process them first\n-    files.sort(key=lambda name: (name.endswith('.pxi') or\n-                                 name.endswith('.pxi.in') or\n-                                 name.endswith('.pxd.in')),\n-               reverse=True)\n-\n-    for filename in files:\n-        in_file = os.path.join(root_dir, filename + \".in\")\n-        for fromext, value in rules.items():\n-            if filename.endswith(fromext):\n-                if not value:\n-                    break\n-                function, toext = value\n-                if toext == '.c':\n-                    with open(os.path.join(root_dir, filename), 'rb') as f:\n-                        data = f.read()\n-                        m = re.search(br\"^\\s*#\\s*distutils:\\s*language\\s*=\\s*c\\+\\+\\s*$\", data, re.I|re.M)\n-                        if m:\n-                            toext = \".cxx\"\n-                fromfile = filename\n-                tofile = filename[:-len(fromext)] + toext\n-                process(root_dir, fromfile, tofile, function, hash_db)\n-                save_hashes(hash_db, HASH_FILE)\n-                break\n-\n-def main():\n-    try:\n-        root_dir = sys.argv[1]\n-    except IndexError:\n-        root_dir = DEFAULT_ROOT\n-    find_process_files(root_dir)\n-\n-\n-if __name__ == '__main__':\n-    main()\n",
            "comment_added_diff": {},
            "comment_deleted_diff": {
                "1": "#!/usr/bin/env python3",
                "39": "#",
                "40": "# Rules",
                "41": "#",
                "98": "    # fromext : function, toext",
                "105": "#",
                "106": "# Hash db",
                "107": "#",
                "109": "    # Return { filename : (sha256 of input, sha256 of output) }",
                "131": "#",
                "132": "# Main program",
                "133": "#",
                "161": "    # changed target file, recompute hash",
                "163": "    # store hash in db",
                "170": "    # .pxi or .pxi.in files are most likely dependencies for",
                "171": "    # .pyx files, so we need to process them first",
                "187": "                        m = re.search(br\"^\\s*#\\s*distutils:\\s*language\\s*=\\s*c\\+\\+\\s*$\", data, re.I|re.M)"
            },
            "comment_modified_diff": {}
        }
    ],
    "openblas_support.py": [
        {
            "commit": "9e144f7c1598221510d49d8c6b79c66dc000edf6",
            "timestamp": "2022-11-17T09:17:24-07:00",
            "author": "Matti Picus",
            "commit_message": "BLD: update OpenBLAS to 0.3.21 and clean up openblas download test (#22525)\n\n* BUILD: update OpenBLAS to 0.3.21 and clean up openblas download test\r\n\r\n* set LDFLAGS on windows64 like the openblaslib build does\r\n\r\n* use rtools compilers on windows when building wheels\r\n\r\n* fix typos\r\n\r\n* add rtools gfortran to PATH\r\n\r\n* use the openblas dll from the zip archive without rewrapping\r\n\r\n* typos\r\n\r\n* copy dll import library for 64-bit interfaces\r\n\r\n* revert many of the changes to azure-steps-windows.yaml, copy openblas better in wheels\r\n\r\n* fix wildcard copy\r\n\r\n* test OpenBLAS build worked with threadpoolctl\r\n\r\n* typos\r\n\r\n* install threadpoolctl where needed, use for loop to recursively copy\r\n\r\n* update macos OpenBLAS suffixes for newer gfortran hashes\r\n\r\n* use libgfortran5.dylib on macos\r\n\r\n* fix scripts\r\n\r\n* re-use gfortran install from MacPython/gfortran-install on macos\r\n\r\n* use pre-release version of delocate\r\n\r\n* fixes for wheel builds/tests\r\n\r\n* add debugging cruft for pypy+win, macos wheels\r\n\r\n* add DYLD_LIBRARY_PATH on macosx-x86_64\r\n\r\n* use 32-bit openblas interfaces for ppc64le tests\r\n\r\n* skip large_archive test that sometimes segfaults on PyPy+windows",
            "additions": 46,
            "deletions": 61,
            "change_type": "MODIFY",
            "diff": "@@ -13,8 +13,8 @@\n from urllib.request import urlopen, Request\n from urllib.error import HTTPError\n \n-OPENBLAS_V = '0.3.20'\n-OPENBLAS_LONG = 'v0.3.20'\n+OPENBLAS_V = '0.3.21'\n+OPENBLAS_LONG = 'v0.3.21'\n BASE_LOC = 'https://anaconda.org/multibuild-wheels-staging/openblas-libs'\n BASEURL = f'{BASE_LOC}/{OPENBLAS_LONG}/download'\n SUPPORTED_PLATFORMS = [\n@@ -54,13 +54,10 @@ def get_ilp64():\n \n \n def get_manylinux(arch):\n-    if arch in ('x86_64', 'i686'):\n-        default = '2010'\n-    else:\n-        default = '2014'\n+    default = '2014'\n     ret = os.environ.get(\"MB_ML_VER\", default)\n     # XXX For PEP 600 this can be a glibc version\n-    assert ret in ('1', '2010', '2014', '_2_24'), f'invalid MB_ML_VER {ret}'\n+    assert ret in ('2010', '2014', '_2_24'), f'invalid MB_ML_VER {ret}'\n     return ret\n \n \n@@ -77,16 +74,16 @@ def download_openblas(target, plat, ilp64):\n         suffix = f'manylinux{ml_ver}_{arch}.tar.gz'\n         typ = 'tar.gz'\n     elif plat == 'macosx-x86_64':\n-        suffix = 'macosx_10_9_x86_64-gf_1becaaa.tar.gz'\n+        suffix = 'macosx_10_9_x86_64-gf_c469a42.tar.gz'\n         typ = 'tar.gz'\n     elif plat == 'macosx-arm64':\n-        suffix = 'macosx_11_0_arm64-gf_f26990f.tar.gz'\n+        suffix = 'macosx_11_0_arm64-gf_5272328.tar.gz'\n         typ = 'tar.gz'\n     elif osname == 'win':\n         if plat == \"win-32\":\n-            suffix = 'win32-gcc_8_1_0.zip'\n+            suffix = 'win32-gcc_8_3_0.zip'\n         else:\n-            suffix = 'win_amd64-gcc_8_1_0.zip'\n+            suffix = 'win_amd64-gcc_10_3_0.zip'\n         typ = 'zip'\n \n     if not suffix:\n@@ -102,11 +99,11 @@ def download_openblas(target, plat, ilp64):\n     if response.status != 200:\n         print(f'Could not download \"{filename}\"', file=sys.stderr)\n         return None\n-    print(f\"Downloading {length} from {filename}\", file=sys.stderr)\n+    # print(f\"Downloading {length} from {filename}\", file=sys.stderr)\n     data = response.read()\n     # Verify hash\n     key = os.path.basename(filename)\n-    print(\"Saving to file\", file=sys.stderr)\n+    # print(\"Saving to file\", file=sys.stderr)\n     with open(target, 'wb') as fid:\n         fid.write(data)\n     return typ\n@@ -133,28 +130,34 @@ def setup_openblas(plat=get_plat(), ilp64=get_ilp64()):\n     if osname == 'win':\n         if not typ == 'zip':\n             return f'expecting to download zipfile on windows, not {typ}'\n-        return unpack_windows_zip(tmp)\n+        return unpack_windows_zip(tmp, plat)\n     else:\n         if not typ == 'tar.gz':\n             return 'expecting to download tar.gz, not %s' % str(typ)\n         return unpack_targz(tmp)\n \n \n-def unpack_windows_zip(fname):\n+def unpack_windows_zip(fname, plat):\n+    unzip_base = os.path.join(gettempdir(), 'openblas')\n+    if not os.path.exists(unzip_base):\n+        os.mkdir(unzip_base)\n     with zipfile.ZipFile(fname, 'r') as zf:\n-        # Get the openblas.a file, but not openblas.dll.a nor openblas.dev.a\n-        lib = [x for x in zf.namelist() if OPENBLAS_LONG in x and\n-               x.endswith('a') and not x.endswith('dll.a') and\n-               not x.endswith('dev.a')]\n-        if not lib:\n-            return 'could not find libopenblas_%s*.a ' \\\n-                    'in downloaded zipfile' % OPENBLAS_LONG\n-        if get_ilp64() is None:\n-            target = os.path.join(gettempdir(), 'openblas.a')\n-        else:\n-            target = os.path.join(gettempdir(), 'openblas64_.a')\n-        with open(target, 'wb') as fid:\n-            fid.write(zf.read(lib[0]))\n+        zf.extractall(unzip_base)\n+    if plat == \"win-32\":\n+        target = os.path.join(unzip_base, \"32\")\n+    else:\n+        target = os.path.join(unzip_base, \"64\")\n+    # Copy the lib to openblas.lib. Once we can properly use pkg-config\n+    # this will not be needed\n+    lib = glob.glob(os.path.join(target, 'lib', '*.lib'))\n+    assert len(lib) == 1\n+    for f in lib:\n+        shutil.copy(f, os.path.join(target, 'lib', 'openblas.lib'))\n+        shutil.copy(f, os.path.join(target, 'lib', 'openblas64_.lib'))\n+    # Copy the dll from bin to lib so system_info can pick it up\n+    dll = glob.glob(os.path.join(target, 'bin', '*.dll'))\n+    for f in dll:\n+        shutil.copy(f, os.path.join(target, 'lib'))\n     return target\n \n \n@@ -235,7 +238,8 @@ def make_init(dirname):\n \n def test_setup(plats):\n     '''\n-    Make sure all the downloadable files exist and can be opened\n+    Make sure all the downloadable files needed for wheel building\n+    exist and can be opened\n     '''\n     def items():\n         \"\"\" yields all combinations of arch, ilp64\n@@ -243,19 +247,8 @@ def items():\n         for plat in plats:\n             yield plat, None\n             osname, arch = plat.split(\"-\")\n-            if arch not in ('i686', 'arm64', '32'):\n+            if arch not in ('i686', '32'):\n                 yield plat, '64_'\n-            if osname == \"linux\" and arch in ('i686', 'x86_64'):\n-                oldval = os.environ.get('MB_ML_VER', None)\n-                os.environ['MB_ML_VER'] = '1'\n-                yield plat, None\n-                # Once we create x86_64 and i686 manylinux2014 wheels...\n-                # os.environ['MB_ML_VER'] = '2014'\n-                # yield arch, None, False\n-                if oldval:\n-                    os.environ['MB_ML_VER'] = oldval\n-                else:\n-                    os.environ.pop('MB_ML_VER')\n \n     errs = []\n     for plat, ilp64 in items():\n@@ -273,7 +266,7 @@ def items():\n                 continue\n             if not target:\n                 raise RuntimeError(f'Could not setup {plat}')\n-            print(target)\n+            print('success with', plat, ilp64)\n             if osname == 'win':\n                 if not target.endswith('.a'):\n                     raise RuntimeError(\"Not .a extracted!\")\n@@ -291,32 +284,24 @@ def items():\n         raise errs[0]\n \n \n-def test_version(expected_version, ilp64=get_ilp64()):\n+def test_version(expected_version=None):\n     \"\"\"\n     Assert that expected OpenBLAS version is\n-    actually available via NumPy\n+    actually available via NumPy. Requires threadpoolctl\n     \"\"\"\n     import numpy\n-    import ctypes\n+    import threadpoolctl\n \n-    dll = ctypes.CDLL(numpy.core._multiarray_umath.__file__)\n-    if ilp64 == \"64_\":\n-        get_config = dll.openblas_get_config64_\n-    else:\n-        get_config = dll.openblas_get_config\n-    get_config.restype = ctypes.c_char_p\n-    res = get_config()\n-    print('OpenBLAS get_config returned', str(res))\n+    data = threadpoolctl.threadpool_info()\n+    if len(data) != 1:\n+        raise ValueError(f\"expected single threadpool_info result, got {data}\")\n     if not expected_version:\n         expected_version = OPENBLAS_V\n-    check_str = b'OpenBLAS %s' % expected_version.encode()\n-    print(check_str)\n-    assert check_str in res, f'{expected_version} not found in {res}'\n-    if ilp64:\n-        assert b\"USE64BITINT\" in res\n-    else:\n-        assert b\"USE64BITINT\" not in res\n-\n+    if data[0]['version'] != expected_version:\n+        raise ValueError(\n+            f\"expected OpenBLAS version {expected_version}, got {data}\"\n+        )\n+    print(\"OK\")\n \n if __name__ == '__main__':\n     import argparse\n",
            "comment_added_diff": {
                "102": "    # print(f\"Downloading {length} from {filename}\", file=sys.stderr)",
                "106": "    # print(\"Saving to file\", file=sys.stderr)",
                "150": "    # Copy the lib to openblas.lib. Once we can properly use pkg-config",
                "151": "    # this will not be needed",
                "157": "    # Copy the dll from bin to lib so system_info can pick it up"
            },
            "comment_deleted_diff": {
                "145": "        # Get the openblas.a file, but not openblas.dll.a nor openblas.dev.a",
                "252": "                # Once we create x86_64 and i686 manylinux2014 wheels...",
                "253": "                # os.environ['MB_ML_VER'] = '2014'",
                "254": "                # yield arch, None, False"
            },
            "comment_modified_diff": {
                "150": "            return 'could not find libopenblas_%s*.a ' \\",
                "151": "                    'in downloaded zipfile' % OPENBLAS_LONG",
                "157": "            fid.write(zf.read(lib[0]))"
            }
        },
        {
            "commit": "df3751b03d789ee04cac3a9a8a7f7f3aba58735c",
            "timestamp": "2023-01-20T16:03:28+01:00",
            "author": "Andrew Nelson",
            "commit_message": "CI: musllinux_x86_64 (#22864)\n\n[ci skip]",
            "additions": 34,
            "deletions": 5,
            "change_type": "MODIFY",
            "diff": "@@ -20,6 +20,7 @@\n SUPPORTED_PLATFORMS = [\n     'linux-aarch64',\n     'linux-x86_64',\n+    'musllinux-x86_64',\n     'linux-i686',\n     'linux-ppc64le',\n     'linux-s390x',\n@@ -55,10 +56,39 @@ def get_ilp64():\n \n def get_manylinux(arch):\n     default = '2014'\n-    ret = os.environ.get(\"MB_ML_VER\", default)\n+    ml_ver = os.environ.get(\"MB_ML_VER\", default)\n     # XXX For PEP 600 this can be a glibc version\n-    assert ret in ('2010', '2014', '_2_24'), f'invalid MB_ML_VER {ret}'\n-    return ret\n+    assert ml_ver in ('2010', '2014', '_2_24'), f'invalid MB_ML_VER {ml_ver}'\n+    suffix = f'manylinux{ml_ver}_{arch}.tar.gz'\n+    return suffix\n+\n+\n+def get_musllinux(arch):\n+    musl_ver = \"1_1\"\n+    suffix = f'musllinux_{musl_ver}_{arch}.tar.gz'\n+    return suffix\n+\n+\n+def get_linux(arch):\n+    # best way of figuring out whether manylinux or musllinux is to look\n+    # at the packaging tags. If packaging isn't installed (it's not by default)\n+    # fallback to sysconfig (which may be flakier)\n+    try:\n+        from packaging.tags import sys_tags\n+        tags = list(sys_tags())\n+        plat = tags[0].platform\n+    except ImportError:\n+        # fallback to sysconfig for figuring out if you're using musl\n+        plat = 'manylinux'\n+        # value could be None\n+        v = sysconfig.get_config_var('HOST_GNU_TYPE') or ''\n+        if 'musl' in v:\n+            plat = 'musllinux'\n+\n+    if 'manylinux' in plat:\n+        return get_manylinux(arch)\n+    elif 'musllinux' in plat:\n+        return get_musllinux(arch)\n \n \n def download_openblas(target, plat, ilp64):\n@@ -70,8 +100,7 @@ def download_openblas(target, plat, ilp64):\n                 '(KHTML, like Gecko) Chrome/41.0.2228.0 Safari/537.3')}\n     suffix = None\n     if osname == \"linux\":\n-        ml_ver = get_manylinux(arch)\n-        suffix = f'manylinux{ml_ver}_{arch}.tar.gz'\n+        suffix = get_linux(arch)\n         typ = 'tar.gz'\n     elif plat == 'macosx-x86_64':\n         suffix = 'macosx_10_9_x86_64-gf_c469a42.tar.gz'\n",
            "comment_added_diff": {
                "73": "    # best way of figuring out whether manylinux or musllinux is to look",
                "74": "    # at the packaging tags. If packaging isn't installed (it's not by default)",
                "75": "    # fallback to sysconfig (which may be flakier)",
                "81": "        # fallback to sysconfig for figuring out if you're using musl",
                "83": "        # value could be None"
            },
            "comment_deleted_diff": {},
            "comment_modified_diff": {
                "73": "        ml_ver = get_manylinux(arch)",
                "74": "        suffix = f'manylinux{ml_ver}_{arch}.tar.gz'"
            }
        },
        {
            "commit": "587907098e69f833c4cd39aca49127c2f8abecbb",
            "timestamp": "2023-02-20T21:07:30+00:00",
            "author": "mattip",
            "commit_message": "BUILD: add a windows meson CI job, tweak meson build parameters, fix typo",
            "additions": 10,
            "deletions": 28,
            "change_type": "MODIFY",
            "diff": "@@ -232,36 +232,13 @@ def make_init(dirname):\n     with open(os.path.join(dirname, '_distributor_init.py'), 'wt') as fid:\n         fid.write(textwrap.dedent(\"\"\"\n             '''\n-            Helper to preload windows dlls to prevent dll not found errors.\n-            Once a DLL is preloaded, its namespace is made available to any\n-            subsequent DLL. This file originated in the numpy-wheels repo,\n-            and is created as part of the scripts that build the wheel.\n+            Helper to add the path to the openblas dll to the dll search space\n             '''\n             import os\n-            import glob\n-            if os.name == 'nt':\n-                # convention for storing / loading the DLL from\n-                # numpy/.libs/, if present\n-                try:\n-                    from ctypes import WinDLL\n-                    basedir = os.path.dirname(__file__)\n-                except:\n-                    pass\n-                else:\n-                    libs_dir = os.path.abspath(os.path.join(basedir, '.libs'))\n-                    DLL_filenames = []\n-                    if os.path.isdir(libs_dir):\n-                        for filename in glob.glob(os.path.join(libs_dir,\n-                                                               '*openblas*dll')):\n-                            # NOTE: would it change behavior to load ALL\n-                            # DLLs at this path vs. the name restriction?\n-                            WinDLL(os.path.abspath(filename))\n-                            DLL_filenames.append(filename)\n-                    if len(DLL_filenames) > 1:\n-                        import warnings\n-                        warnings.warn(\"loaded more than 1 DLL from .libs:\"\n-                                      \"\\\\n%s\" % \"\\\\n\".join(DLL_filenames),\n-                                      stacklevel=1)\n+            import sys\n+            extra_dll_dir = os.path.join(os.path.dirname(__file__), '.libs')\n+            if sys.platform == 'win32' and os.path.isdir(extra_dll_dir):\n+                os.add_dll_directory(extra_dll_dir)\n     \"\"\"))\n \n \n@@ -343,9 +320,14 @@ def test_version(expected_version=None):\n     parser.add_argument('--check_version', nargs='?', default='',\n                         help='Check provided OpenBLAS version string '\n                              'against available OpenBLAS')\n+    parser.add_argument('--write-init', nargs=1,\n+                        metavar='OUT_SCIPY_DIR',\n+                        help='Write distribution init to named dir')\n     args = parser.parse_args()\n     if args.check_version != '':\n         test_version(args.check_version)\n+    elif args.write_init:\n+        make_init(args.write_init[0])\n     elif args.test is None:\n         print(setup_openblas())\n     else:\n",
            "comment_added_diff": {},
            "comment_deleted_diff": {
                "243": "                # convention for storing / loading the DLL from",
                "244": "                # numpy/.libs/, if present",
                "256": "                            # NOTE: would it change behavior to load ALL",
                "257": "                            # DLLs at this path vs. the name restriction?"
            },
            "comment_modified_diff": {}
        },
        {
            "commit": "dacd6dcb3c3a9832ae9b3189439bd2df840994e5",
            "timestamp": "2023-02-21T11:08:36+02:00",
            "author": "mattip",
            "commit_message": "BLD: fix review comments",
            "additions": 28,
            "deletions": 10,
            "change_type": "MODIFY",
            "diff": "@@ -232,13 +232,36 @@ def make_init(dirname):\n     with open(os.path.join(dirname, '_distributor_init.py'), 'wt') as fid:\n         fid.write(textwrap.dedent(\"\"\"\n             '''\n-            Helper to add the path to the openblas dll to the dll search space\n+            Helper to preload windows dlls to prevent dll not found errors.\n+            Once a DLL is preloaded, its namespace is made available to any\n+            subsequent DLL. This file originated in the numpy-wheels repo,\n+            and is created as part of the scripts that build the wheel.\n             '''\n             import os\n-            import sys\n-            extra_dll_dir = os.path.join(os.path.dirname(__file__), '.libs')\n-            if sys.platform == 'win32' and os.path.isdir(extra_dll_dir):\n-                os.add_dll_directory(extra_dll_dir)\n+            import glob\n+            if os.name == 'nt':\n+                # convention for storing / loading the DLL from\n+                # numpy/.libs/, if present\n+                try:\n+                    from ctypes import WinDLL\n+                    basedir = os.path.dirname(__file__)\n+                except:\n+                    pass\n+                else:\n+                    libs_dir = os.path.abspath(os.path.join(basedir, '.libs'))\n+                    DLL_filenames = []\n+                    if os.path.isdir(libs_dir):\n+                        for filename in glob.glob(os.path.join(libs_dir,\n+                                                               '*openblas*dll')):\n+                            # NOTE: would it change behavior to load ALL\n+                            # DLLs at this path vs. the name restriction?\n+                            WinDLL(os.path.abspath(filename))\n+                            DLL_filenames.append(filename)\n+                    if len(DLL_filenames) > 1:\n+                        import warnings\n+                        warnings.warn(\"loaded more than 1 DLL from .libs:\"\n+                                      \"\\\\n%s\" % \"\\\\n\".join(DLL_filenames),\n+                                      stacklevel=1)\n     \"\"\"))\n \n \n@@ -320,14 +343,9 @@ def test_version(expected_version=None):\n     parser.add_argument('--check_version', nargs='?', default='',\n                         help='Check provided OpenBLAS version string '\n                              'against available OpenBLAS')\n-    parser.add_argument('--write-init', nargs=1,\n-                        metavar='OUT_SCIPY_DIR',\n-                        help='Write distribution init to named dir')\n     args = parser.parse_args()\n     if args.check_version != '':\n         test_version(args.check_version)\n-    elif args.write_init:\n-        make_init(args.write_init[0])\n     elif args.test is None:\n         print(setup_openblas())\n     else:\n",
            "comment_added_diff": {
                "243": "                # convention for storing / loading the DLL from",
                "244": "                # numpy/.libs/, if present",
                "256": "                            # NOTE: would it change behavior to load ALL",
                "257": "                            # DLLs at this path vs. the name restriction?"
            },
            "comment_deleted_diff": {},
            "comment_modified_diff": {}
        },
        {
            "commit": "2e84e219632f807d699aeef0a1aa531d71f07a2d",
            "timestamp": "2023-06-18T11:46:58+02:00",
            "author": "Matti Picus",
            "commit_message": "reuse parts of scipy's scripts for windows includding delvewheel; fix non-wheel build",
            "additions": 5,
            "deletions": 4,
            "change_type": "MODIFY",
            "diff": "@@ -182,10 +182,11 @@ def unpack_windows_zip(fname, plat):\n     # Copy the lib to openblas.lib. Once we can properly use pkg-config\n     # this will not be needed\n     lib = glob.glob(os.path.join(target, 'lib', '*.lib'))\n-    assert len(lib) == 1\n-    for f in lib:\n-        shutil.copy(f, os.path.join(target, 'lib', 'openblas.lib'))\n-        shutil.copy(f, os.path.join(target, 'lib', 'openblas64_.lib'))\n+    if len(lib) == 1:\n+        # The 64-bit tarball already has these copied, no need to do it\n+        for f in lib:\n+            shutil.copy(f, os.path.join(target, 'lib', 'openblas.lib'))\n+            shutil.copy(f, os.path.join(target, 'lib', 'openblas64_.lib'))\n     # Copy the dll from bin to lib so system_info can pick it up\n     dll = glob.glob(os.path.join(target, 'bin', '*.dll'))\n     for f in dll:\n",
            "comment_added_diff": {
                "186": "        # The 64-bit tarball already has these copied, no need to do it"
            },
            "comment_deleted_diff": {},
            "comment_modified_diff": {
                "186": "    for f in lib:"
            }
        },
        {
            "commit": "085665c91e37be663f8135854f4b89da7b2ada67",
            "timestamp": "2023-09-21T13:07:17+02:00",
            "author": "Matti Picus",
            "commit_message": "BLD: add libquadmath to licences and other tweaks (#24753)\n\nOther improvements:\r\n\r\n- use SPDX short specifiers, fixes from review\r\n- only use `openblas_support.make_init()` in CI, fix paths\r\n- fix license concatenation for wheels we distribute\r\n\r\nCloses gh-24764",
            "additions": 8,
            "deletions": 4,
            "change_type": "MODIFY",
            "diff": "@@ -238,6 +238,9 @@ def get_members():\n def make_init(dirname):\n     '''\n     Create a _distributor_init.py file for OpenBlas\n+\n+    Obsoleted by the use of delvewheel in wheel building, which\n+    adds an equivalent snippet to numpy/__init__.py, but still useful in CI\n     '''\n     with open(os.path.join(dirname, '_distributor_init.py'), 'w') as fid:\n         fid.write(textwrap.dedent(\"\"\"\n@@ -246,19 +249,20 @@ def make_init(dirname):\n             Once a DLL is preloaded, its namespace is made available to any\n             subsequent DLL. This file originated in the numpy-wheels repo,\n             and is created as part of the scripts that build the wheel.\n+\n             '''\n             import os\n             import glob\n             if os.name == 'nt':\n-                # convention for storing / loading the DLL from\n-                # numpy/.libs/, if present\n+                # load any DLL from numpy/../numpy.libs/, if present\n                 try:\n                     from ctypes import WinDLL\n-                    basedir = os.path.dirname(__file__)\n                 except:\n                     pass\n                 else:\n-                    libs_dir = os.path.abspath(os.path.join(basedir, '.libs'))\n+                    basedir = os.path.dirname(__file__)\n+                    libs_dir = os.path.join(basedir, os.pardir, 'numpy.libs')\n+                    libs_dir = os.path.abspath(libs_dir)\n                     DLL_filenames = []\n                     if os.path.isdir(libs_dir):\n                         for filename in glob.glob(os.path.join(libs_dir,\n",
            "comment_added_diff": {
                "257": "                # load any DLL from numpy/../numpy.libs/, if present"
            },
            "comment_deleted_diff": {
                "253": "                # convention for storing / loading the DLL from",
                "254": "                # numpy/.libs/, if present"
            },
            "comment_modified_diff": {
                "257": "                    basedir = os.path.dirname(__file__)"
            }
        }
    ],
    "check_license.py": [
        {
            "commit": "aef314b7cee6389f1003cbd9ddbd11d7d8e39875",
            "timestamp": "2023-06-18T11:46:58+02:00",
            "author": "Ralf Gommers",
            "commit_message": "BLD: handle license files inclusion in wheels correctly",
            "additions": 6,
            "deletions": 2,
            "change_type": "MODIFY",
            "diff": "@@ -7,10 +7,10 @@\n distribution.\n \n \"\"\"\n-import os\n import sys\n import re\n import argparse\n+import pathlib\n \n \n def check_text(text):\n@@ -33,8 +33,12 @@ def main():\n     __import__(args.module)\n     mod = sys.modules[args.module]\n \n+    # LICENSE.txt is installed in the .dist-info directory, so find ot tjere\"\n+    sitepkgs = pathlib.Path(mod.__file__).parent.parent\n+    distinfo_path = [s for s in sitepkgs.glob(\"numpy-*.dist-info\")][0]\n+\n     # Check license text\n-    license_txt = os.path.join(os.path.dirname(mod.__file__), \"LICENSE.txt\")\n+    license_txt = distinfo_path / \"LICENSE.txt\"\n     with open(license_txt, encoding=\"utf-8\") as f:\n         text = f.read()\n \n",
            "comment_added_diff": {
                "36": "    # LICENSE.txt is installed in the .dist-info directory, so find ot tjere\""
            },
            "comment_deleted_diff": {},
            "comment_modified_diff": {}
        },
        {
            "commit": "34933ac970d4f0ed57562c8759910ed575e6d0d3",
            "timestamp": "2023-06-18T11:46:58+02:00",
            "author": "mattip",
            "commit_message": "windows fixes, xfail test on manylinux2014",
            "additions": 1,
            "deletions": 1,
            "change_type": "MODIFY",
            "diff": "@@ -33,7 +33,7 @@ def main():\n     __import__(args.module)\n     mod = sys.modules[args.module]\n \n-    # LICENSE.txt is installed in the .dist-info directory, so find ot tjere\"\n+    # LICENSE.txt is installed in the .dist-info directory, so find it there\n     sitepkgs = pathlib.Path(mod.__file__).parent.parent\n     distinfo_path = [s for s in sitepkgs.glob(\"numpy-*.dist-info\")][0]\n \n",
            "comment_added_diff": {
                "36": "    # LICENSE.txt is installed in the .dist-info directory, so find it there"
            },
            "comment_deleted_diff": {
                "36": "    # LICENSE.txt is installed in the .dist-info directory, so find ot tjere\""
            },
            "comment_modified_diff": {
                "36": "    # LICENSE.txt is installed in the .dist-info directory, so find ot tjere\""
            }
        }
    ],
    "wheels.yml": [],
    "numerictypes.py": [
        {
            "commit": "49b18ff32ed25f16b7af8282237b5668ddfb5087",
            "timestamp": "2022-11-07T12:14:34+01:00",
            "author": "Sebastian Berg",
            "commit_message": "DEP: Deprecate `np.find_common_type`\n\nThe function uses the numeric scalar common dtype/promotion rules.\nThese are subtly different from the typical NumPy rules defined by\n`np.result_type`.\nMainly, there is no good reason to have two subtly different rules\nexposed and `find_common_type` is less reliable, slower, and not really\nmaintainable when it comes to NEP 50.",
            "additions": 19,
            "deletions": 0,
            "change_type": "MODIFY",
            "diff": "@@ -80,6 +80,7 @@\n \n \"\"\"\n import numbers\n+import warnings\n \n from numpy.core.multiarray import (\n         ndarray, array, dtype, datetime_data, datetime_as_string,\n@@ -599,6 +600,16 @@ def find_common_type(array_types, scalar_types):\n     \"\"\"\n     Determine common type following standard coercion rules.\n \n+    .. deprecated:: NumPy 1.24\n+\n+        This function is deprecated, use `numpy.promote_types` or\n+        `numpy.result_type` instead.  To achieve semantics for the\n+        `scalar_types` argument, use `numpy.result_type` and pass the Python\n+        values `0`, `0.0`, or `0j`.\n+        This will give the same results in almost all cases.\n+        More information and rare exception can be found in the\n+        `NumPy 1.24 release notes <https://numpy.org/devdocs/release/1.24.0-notes.html>`_.\n+\n     Parameters\n     ----------\n     array_types : sequence\n@@ -646,6 +657,14 @@ def find_common_type(array_types, scalar_types):\n     dtype('complex128')\n \n     \"\"\"\n+    # Deprecated 2022-11-07, NumPy 1.24\n+    warnings.warn(\n+            \"np.find_common_type is deprecated.  Please use `np.result_type` \"\n+            \"or `np.promote_types`.\\n\"\n+            \"See https://numpy.org/devdocs/release/1.24.0-notes.html and the \"\n+            \"docs for more information.  (Deprecated NumPy 1.24)\",\n+            DeprecationWarning, stacklevel=2)\n+\n     array_types = [dtype(x) for x in array_types]\n     scalar_types = [dtype(x) for x in scalar_types]\n \n",
            "comment_added_diff": {
                "660": "    # Deprecated 2022-11-07, NumPy 1.24"
            },
            "comment_deleted_diff": {},
            "comment_modified_diff": {}
        },
        {
            "commit": "d8a0814c0913ecd7eae1421021ce23b4e7eba72d",
            "timestamp": "2023-02-19T12:08:54-07:00",
            "author": "Charles Harris",
            "commit_message": "MAINT: Update release from 1.24 to 1.25",
            "additions": 5,
            "deletions": 5,
            "change_type": "MODIFY",
            "diff": "@@ -600,7 +600,7 @@ def find_common_type(array_types, scalar_types):\n     \"\"\"\n     Determine common type following standard coercion rules.\n \n-    .. deprecated:: NumPy 1.24\n+    .. deprecated:: NumPy 1.25\n \n         This function is deprecated, use `numpy.promote_types` or\n         `numpy.result_type` instead.  To achieve semantics for the\n@@ -608,7 +608,7 @@ def find_common_type(array_types, scalar_types):\n         values `0`, `0.0`, or `0j`.\n         This will give the same results in almost all cases.\n         More information and rare exception can be found in the\n-        `NumPy 1.24 release notes <https://numpy.org/devdocs/release/1.24.0-notes.html>`_.\n+        `NumPy 1.25 release notes <https://numpy.org/devdocs/release/1.25.0-notes.html>`_.\n \n     Parameters\n     ----------\n@@ -657,12 +657,12 @@ def find_common_type(array_types, scalar_types):\n     dtype('complex128')\n \n     \"\"\"\n-    # Deprecated 2022-11-07, NumPy 1.24\n+    # Deprecated 2022-11-07, NumPy 1.25\n     warnings.warn(\n             \"np.find_common_type is deprecated.  Please use `np.result_type` \"\n             \"or `np.promote_types`.\\n\"\n-            \"See https://numpy.org/devdocs/release/1.24.0-notes.html and the \"\n-            \"docs for more information.  (Deprecated NumPy 1.24)\",\n+            \"See https://numpy.org/devdocs/release/1.25.0-notes.html and the \"\n+            \"docs for more information.  (Deprecated NumPy 1.25)\",\n             DeprecationWarning, stacklevel=2)\n \n     array_types = [dtype(x) for x in array_types]\n",
            "comment_added_diff": {
                "660": "    # Deprecated 2022-11-07, NumPy 1.25"
            },
            "comment_deleted_diff": {
                "660": "    # Deprecated 2022-11-07, NumPy 1.24"
            },
            "comment_modified_diff": {
                "660": "    # Deprecated 2022-11-07, NumPy 1.24"
            }
        },
        {
            "commit": "f2ac4f967fb8bbf9da30bc19f16805f3f3260c7b",
            "timestamp": "2023-07-08T13:05:42+03:00",
            "author": "Evgeni Burovski",
            "commit_message": "DEP: remove np.cast",
            "additions": 1,
            "deletions": 7,
            "change_type": "MODIFY",
            "diff": "@@ -90,7 +90,7 @@\n \n # we add more at the bottom\n __all__ = ['sctypeDict', 'sctypes',\n-           'ScalarType', 'obj2sctype', 'cast', 'nbytes', 'sctype2char',\n+           'ScalarType', 'obj2sctype', 'nbytes', 'sctype2char',\n            'maximum_sctype', 'issctype', 'typecodes', 'find_common_type',\n            'issubdtype', 'datetime_data', 'datetime_as_string',\n            'busday_offset', 'busday_count', 'is_busday', 'busdaycalendar',\n@@ -503,12 +503,6 @@ def sctype2char(sctype):\n         raise KeyError(sctype)\n     return dtype(sctype).char\n \n-# Create dictionary of casting functions that wrap sequences\n-# indexed by type or type character\n-cast = _typedict()\n-for key in _concrete_types:\n-    cast[key] = lambda x, k=key: array(x, copy=False).astype(k)\n-\n \n def _scalar_type_key(typ):\n     \"\"\"A ``key`` function for `sorted`.\"\"\"\n",
            "comment_added_diff": {},
            "comment_deleted_diff": {
                "506": "# Create dictionary of casting functions that wrap sequences",
                "507": "# indexed by type or type character"
            },
            "comment_modified_diff": {}
        },
        {
            "commit": "017cef318ad37ac05738830ebb3173c65c128759",
            "timestamp": "2023-07-11T22:25:10+02:00",
            "author": "Mateusz Sok\u00f3\u0142",
            "commit_message": "API: apply review comments",
            "additions": 10,
            "deletions": 2,
            "change_type": "MODIFY",
            "diff": "@@ -86,7 +86,7 @@\n         ndarray, array, dtype, datetime_data, datetime_as_string,\n         busday_offset, busday_count, is_busday, busdaycalendar\n         )\n-from .._utils import deprecate, set_module\n+from .._utils import set_module\n \n # we add more at the bottom\n __all__ = ['sctypeDict', 'sctypes',\n@@ -129,7 +129,6 @@\n                    'complex32', 'complex64', 'complex128', 'complex160',\n                    'complex192', 'complex256', 'complex512', 'object']\n \n-@deprecate\n @set_module('numpy')\n def maximum_sctype(t):\n     \"\"\"\n@@ -169,6 +168,15 @@ def maximum_sctype(t):\n     <class 'numpy.float128'> # may vary\n \n     \"\"\"\n+\n+    # Deprecated in NumPy 2.0, 2023-07-11\n+    warnings.warn(\n+        \"`maximum_sctype` is deprecated. \"\n+        \"(deprecated in NumPy 2.0)\",\n+        DeprecationWarning,\n+        stacklevel=2\n+    )\n+\n     g = obj2sctype(t)\n     if g is None:\n         return t\n",
            "comment_added_diff": {
                "172": "    # Deprecated in NumPy 2.0, 2023-07-11"
            },
            "comment_deleted_diff": {},
            "comment_modified_diff": {}
        },
        {
            "commit": "b43384e8f9f7242c59985c4a3d687c95a2a9dbf4",
            "timestamp": "2023-08-30T09:34:12+02:00",
            "author": "Mateusz Sok\u00f3\u0142",
            "commit_message": "MAINT: Remove deprecated functions [NEP 52] (#24477)",
            "additions": 6,
            "deletions": 144,
            "change_type": "MODIFY",
            "diff": "@@ -86,10 +86,11 @@\n from .._utils import set_module\n \n # we add more at the bottom\n-__all__ = ['ScalarType', 'nbytes', 'typecodes', 'find_common_type',\n-           'issubdtype', 'datetime_data', 'datetime_as_string',\n-           'busday_offset', 'busday_count', 'is_busday', 'busdaycalendar',\n-           ]\n+__all__ = [\n+    'ScalarType', 'typecodes', 'issubdtype', 'datetime_data', \n+    'datetime_as_string', 'busday_offset', 'busday_count', \n+    'is_busday', 'busdaycalendar'\n+]\n \n # we don't need all these imports, but we need to keep them for compatibility\n # for users using np.core.numerictypes.UPPER_TABLE\n@@ -443,13 +444,11 @@ class _typedict(dict):\n     def __getitem__(self, obj):\n         return dict.__getitem__(self, obj2sctype(obj))\n \n-nbytes = _typedict()\n _maxvals = _typedict()\n _minvals = _typedict()\n def _construct_lookups():\n-    for name, info in _concrete_typeinfo.items():\n+    for info in _concrete_typeinfo.values():\n         obj = info.type\n-        nbytes[obj] = info.bits // 8\n         if len(info) > 5:\n             _maxvals[obj] = info.max\n             _minvals[obj] = info.min\n@@ -545,49 +544,6 @@ def _scalar_type_key(typ):\n # Formal deprecation: Numpy 1.20.0, 2020-10-19 (see numpy/__init__.py)\n typeDict = sctypeDict\n \n-# b -> boolean\n-# u -> unsigned integer\n-# i -> signed integer\n-# f -> floating point\n-# c -> complex\n-# M -> datetime\n-# m -> timedelta\n-# S -> string\n-# U -> Unicode string\n-# V -> record\n-# O -> Python object\n-_kind_list = ['b', 'u', 'i', 'f', 'c', 'S', 'U', 'V', 'O', 'M', 'm']\n-\n-__test_types = '?'+typecodes['AllInteger'][:-2]+typecodes['AllFloat']+'O'\n-__len_test_types = len(__test_types)\n-\n-# Keep incrementing until a common type both can be coerced to\n-#  is found.  Otherwise, return None\n-def _find_common_coerce(a, b):\n-    if a > b:\n-        return a\n-    try:\n-        thisind = __test_types.index(a.char)\n-    except ValueError:\n-        return None\n-    return _can_coerce_all([a, b], start=thisind)\n-\n-# Find a data-type that all data-types in a list can be coerced to\n-def _can_coerce_all(dtypelist, start=0):\n-    N = len(dtypelist)\n-    if N == 0:\n-        return None\n-    if N == 1:\n-        return dtypelist[0]\n-    thisind = start\n-    while thisind < __len_test_types:\n-        newdtype = dtype(__test_types[thisind])\n-        numcoerce = len([x for x in dtypelist if newdtype >= x])\n-        if numcoerce == N:\n-            return newdtype\n-        thisind += 1\n-    return None\n-\n def _register_types():\n     numbers.Integral.register(integer)\n     numbers.Complex.register(inexact)\n@@ -595,97 +551,3 @@ def _register_types():\n     numbers.Number.register(number)\n \n _register_types()\n-\n-\n-@set_module('numpy')\n-def find_common_type(array_types, scalar_types):\n-    \"\"\"\n-    Determine common type following standard coercion rules.\n-\n-    .. deprecated:: NumPy 1.25\n-\n-        This function is deprecated, use `numpy.promote_types` or\n-        `numpy.result_type` instead.  To achieve semantics for the\n-        `scalar_types` argument, use `numpy.result_type` and pass the Python\n-        values `0`, `0.0`, or `0j`.\n-        This will give the same results in almost all cases.\n-        More information and rare exception can be found in the\n-        `NumPy 1.25 release notes <https://numpy.org/devdocs/release/1.25.0-notes.html>`_.\n-\n-    Parameters\n-    ----------\n-    array_types : sequence\n-        A list of dtypes or dtype convertible objects representing arrays.\n-    scalar_types : sequence\n-        A list of dtypes or dtype convertible objects representing scalars.\n-\n-    Returns\n-    -------\n-    datatype : dtype\n-        The common data type, which is the maximum of `array_types` ignoring\n-        `scalar_types`, unless the maximum of `scalar_types` is of a\n-        different kind (`dtype.kind`). If the kind is not understood, then\n-        None is returned.\n-\n-    See Also\n-    --------\n-    dtype, common_type, can_cast, mintypecode\n-\n-    Examples\n-    --------\n-    >>> np.find_common_type([], [np.int64, np.float32, complex])\n-    dtype('complex128')\n-    >>> np.find_common_type([np.int64, np.float32], [])\n-    dtype('float64')\n-\n-    The standard casting rules ensure that a scalar cannot up-cast an\n-    array unless the scalar is of a fundamentally different kind of data\n-    (i.e. under a different hierarchy in the data type hierarchy) then\n-    the array:\n-\n-    >>> np.find_common_type([np.float32], [np.int64, np.float64])\n-    dtype('float32')\n-\n-    Complex is of a different type, so it up-casts the float in the\n-    `array_types` argument:\n-\n-    >>> np.find_common_type([np.float32], [complex])\n-    dtype('complex128')\n-\n-    Type specifier strings are convertible to dtypes and can therefore\n-    be used instead of dtypes:\n-\n-    >>> np.find_common_type(['f4', 'f4', 'i4'], ['c8'])\n-    dtype('complex128')\n-\n-    \"\"\"\n-    # Deprecated 2022-11-07, NumPy 1.25\n-    warnings.warn(\n-            \"np.find_common_type is deprecated.  Please use `np.result_type` \"\n-            \"or `np.promote_types`.\\n\"\n-            \"See https://numpy.org/devdocs/release/1.25.0-notes.html and the \"\n-            \"docs for more information.  (Deprecated NumPy 1.25)\",\n-            DeprecationWarning, stacklevel=2)\n-\n-    array_types = [dtype(x) for x in array_types]\n-    scalar_types = [dtype(x) for x in scalar_types]\n-\n-    maxa = _can_coerce_all(array_types)\n-    maxsc = _can_coerce_all(scalar_types)\n-\n-    if maxa is None:\n-        return maxsc\n-\n-    if maxsc is None:\n-        return maxa\n-\n-    try:\n-        index_a = _kind_list.index(maxa.kind)\n-        index_sc = _kind_list.index(maxsc.kind)\n-    except ValueError:\n-        return None\n-\n-    if index_sc > index_a:\n-        return _find_common_coerce(maxsc, maxa)\n-    else:\n-        return maxa\n",
            "comment_added_diff": {},
            "comment_deleted_diff": {
                "548": "# b -> boolean",
                "549": "# u -> unsigned integer",
                "550": "# i -> signed integer",
                "551": "# f -> floating point",
                "552": "# c -> complex",
                "553": "# M -> datetime",
                "554": "# m -> timedelta",
                "555": "# S -> string",
                "556": "# U -> Unicode string",
                "557": "# V -> record",
                "558": "# O -> Python object",
                "564": "# Keep incrementing until a common type both can be coerced to",
                "565": "#  is found.  Otherwise, return None",
                "575": "# Find a data-type that all data-types in a list can be coerced to",
                "662": "    # Deprecated 2022-11-07, NumPy 1.25"
            },
            "comment_modified_diff": {}
        },
        {
            "commit": "acfb63cb6661fd9776db2d208ccd49500f6bcf2e",
            "timestamp": "2023-10-05T11:45:12+02:00",
            "author": "Mateusz Sok\u00f3\u0142",
            "commit_message": "MAINT: Refactor allTypes, sctypeDict and sctypes build process",
            "additions": 3,
            "deletions": 37,
            "change_type": "MODIFY",
            "diff": "@@ -99,13 +99,7 @@\n )\n \n from ._type_aliases import (\n-    sctypeDict,\n-    allTypes,\n-    bitname,\n-    sctypes,\n-    _concrete_types,\n-    _concrete_typeinfo,\n-    _bits_of,\n+    sctypeDict, allTypes, sctypes\n )\n from ._dtype import _kind_name\n \n@@ -431,34 +425,6 @@ def issubdtype(arg1, arg2):\n     return issubclass(arg1, arg2)\n \n \n-# This dictionary allows look up based on any alias for an array data-type\n-class _typedict(dict):\n-    \"\"\"\n-    Base object for a dictionary for look-up with any alias for an array dtype.\n-\n-    Instances of `_typedict` can not be used as dictionaries directly,\n-    first they have to be populated.\n-\n-    \"\"\"\n-\n-    def __getitem__(self, obj):\n-        return dict.__getitem__(self, obj2sctype(obj))\n-\n-_maxvals = _typedict()\n-_minvals = _typedict()\n-def _construct_lookups():\n-    for info in _concrete_typeinfo.values():\n-        obj = info.type\n-        if len(info) > 5:\n-            _maxvals[obj] = info.max\n-            _minvals[obj] = info.min\n-        else:\n-            _maxvals[obj] = None\n-            _minvals[obj] = None\n-\n-_construct_lookups()\n-\n-\n @set_module('numpy')\n def sctype2char(sctype):\n     \"\"\"\n@@ -506,7 +472,7 @@ def sctype2char(sctype):\n     sctype = obj2sctype(sctype)\n     if sctype is None:\n         raise ValueError(\"unrecognized type\")\n-    if sctype not in _concrete_types:\n+    if sctype not in sctypeDict.values():\n         # for compatibility\n         raise KeyError(sctype)\n     return dtype(sctype).char\n@@ -519,7 +485,7 @@ def _scalar_type_key(typ):\n \n \n ScalarType = [int, float, complex, bool, bytes, str, memoryview]\n-ScalarType += sorted(_concrete_types, key=_scalar_type_key)\n+ScalarType += sorted(set(sctypeDict.values()), key=_scalar_type_key)\n ScalarType = tuple(ScalarType)\n \n \n",
            "comment_added_diff": {},
            "comment_deleted_diff": {
                "434": "# This dictionary allows look up based on any alias for an array data-type"
            },
            "comment_modified_diff": {}
        }
    ],
    "numerictypes.pyi": [],
    "22542.compatibility.rst": [],
    "dlpack.c": [],
    "test_dlpack.py": [],
    "22541.expired.rst": [],
    "dispatching.c": [],
    "ufunc_type_resolution.c": [],
    "22539.change.rst": [],
    "_multiarray_tests.c.src": [],
    "test_casting_floatingpoint_errors.py": [],
    "test_cython.py": [
        {
            "commit": "843cf595b69cf895e7c09cba65d06a3ea71ba7f9",
            "timestamp": "2023-02-11T15:26:14-08:00",
            "author": "Brock",
            "commit_message": "tests",
            "additions": 19,
            "deletions": 0,
            "change_type": "MODIFY",
            "diff": "@@ -2,6 +2,7 @@\n import shutil\n import subprocess\n import sys\n+import time\n import pytest\n \n import numpy as np\n@@ -135,3 +136,21 @@ def test_abstract_scalars(install_temp):\n     assert checks.is_integer(1)\n     assert checks.is_integer(np.int8(1))\n     assert checks.is_integer(np.uint64(1))\n+\n+\n+def test_get_datetime_iso_8601_strlen(install_temp):\n+    # GH#21199\n+    import checks\n+    res = checks.get_datetime_iso_8601_strlen()\n+    assert len(res) == 9\n+\n+\n+def test_convert_datetime_to_datetimestruct(install_temp):\n+    # GH#21199\n+    import checks\n+\n+    res = checks.convert_datetime_to_datetimestruct()\n+\n+    exp = {\"year\": 2022, \"month\": 3, \"day\": 15, \"hour\": 20, \"minute\": 2}\n+\n+    assert res == exp  # FIXME: need to get seconds/microseconds into \"exp\"\n",
            "comment_added_diff": {
                "142": "    # GH#21199",
                "149": "    # GH#21199",
                "156": "    assert res == exp  # FIXME: need to get seconds/microseconds into \"exp\""
            },
            "comment_deleted_diff": {},
            "comment_modified_diff": {}
        },
        {
            "commit": "0e9ef3b18f509ffc765af387ee5293348e2b6f9d",
            "timestamp": "2023-02-11T15:26:14-08:00",
            "author": "Brock",
            "commit_message": "salvage bis of #16364",
            "additions": 17,
            "deletions": 7,
            "change_type": "MODIFY",
            "diff": "@@ -1,3 +1,4 @@\n+from datetime import datetime\n import os\n import shutil\n import subprocess\n@@ -138,13 +139,6 @@ def test_abstract_scalars(install_temp):\n     assert checks.is_integer(np.uint64(1))\n \n \n-def test_get_datetime_iso_8601_strlen(install_temp):\n-    # GH#21199\n-    import checks\n-    res = checks.get_datetime_iso_8601_strlen()\n-    assert len(res) == 9\n-\n-\n def test_convert_datetime_to_datetimestruct(install_temp):\n     # GH#21199\n     import checks\n@@ -154,3 +148,19 @@ def test_convert_datetime_to_datetimestruct(install_temp):\n     exp = {\"year\": 2022, \"month\": 3, \"day\": 15, \"hour\": 20, \"minute\": 2}\n \n     assert res == exp  # FIXME: need to get seconds/microseconds into \"exp\"\n+\n+\n+class TestDatetimeStrings:\n+    def test_make_iso_8601_datetime(self, install_temp):\n+        # GH#21199\n+        dt = datetime(2016, 6, 2, 10, 45, 19)\n+        # uses NPY_FR_s\n+        result = checks.make_iso_8601_datetime(dt)\n+        assert result == \"2016-05-02 10:45:19\"\n+\n+    def test_get_datetime_iso_8601_strlen(install_temp):\n+        # GH#21199\n+        import checks\n+        # uses NPY_FR_ns\n+        res = checks.get_datetime_iso_8601_strlen()\n+        assert len(res) == 9\n",
            "comment_added_diff": {
                "155": "        # GH#21199",
                "157": "        # uses NPY_FR_s",
                "162": "        # GH#21199",
                "164": "        # uses NPY_FR_ns"
            },
            "comment_deleted_diff": {
                "142": "    # GH#21199"
            },
            "comment_modified_diff": {}
        },
        {
            "commit": "d5f2376abf46ba3beb99f41f894a29a629a6aeee",
            "timestamp": "2023-02-27T16:17:26-08:00",
            "author": "Brock",
            "commit_message": "fix tests",
            "additions": 5,
            "deletions": 4,
            "change_type": "MODIFY",
            "diff": "@@ -139,15 +139,16 @@ def test_abstract_scalars(install_temp):\n     assert checks.is_integer(np.uint64(1))\n \n \n+\n def test_convert_datetime_to_datetimestruct(install_temp):\n     # GH#21199\n     import checks\n \n     res = checks.convert_datetime_to_datetimestruct()\n \n-    exp = {\"year\": 2022, \"month\": 3, \"day\": 15, \"hour\": 20, \"minute\": 2}\n+    exp = {\"year\": 2022, \"month\": 3, \"day\": 15, \"hour\": 20, \"min\": 1, \"sec\": 55, \"us\": 260292, \"ps\": 0, \"as\": 0}\n \n-    assert res == exp  # FIXME: need to get seconds/microseconds into \"exp\"\n+    assert res == exp\n \n \n class TestDatetimeStrings:\n@@ -157,11 +158,11 @@ def test_make_iso_8601_datetime(self, install_temp):\n         dt = datetime(2016, 6, 2, 10, 45, 19)\n         # uses NPY_FR_s\n         result = checks.make_iso_8601_datetime(dt)\n-        assert result == \"2016-05-02 10:45:19\"\n+        assert result == b\"2016-06-02T10:45:19\"\n \n     def test_get_datetime_iso_8601_strlen(self, install_temp):\n         # GH#21199\n         import checks\n         # uses NPY_FR_ns\n         res = checks.get_datetime_iso_8601_strlen()\n-        assert len(res) == 9\n+        assert res == 48\n",
            "comment_added_diff": {},
            "comment_deleted_diff": {
                "150": "    assert res == exp  # FIXME: need to get seconds/microseconds into \"exp\""
            },
            "comment_modified_diff": {}
        },
        {
            "commit": "1195ff552d9014196b1db502547e24bc0fc499a2",
            "timestamp": "2023-08-31T11:12:50+02:00",
            "author": "Matti Picus",
            "commit_message": "TST: convert cython test from setup.py to meson (#24206)\n\nThe limited-api test has to wait for a new Meson version (see gh-24206).\r\nThis converts the regular Cython test for `numpy.core`.\r\n\r\n[skip ci]",
            "additions": 21,
            "deletions": 34,
            "change_type": "MODIFY",
            "diff": "@@ -31,44 +31,31 @@\n \n \n @pytest.fixture\n-def install_temp(request, tmp_path):\n+def install_temp(tmp_path):\n     # Based in part on test_cython from random.tests.test_extending\n     if IS_WASM:\n         pytest.skip(\"No subprocess\")\n \n-    here = os.path.dirname(__file__)\n-    ext_dir = os.path.join(here, \"examples\", \"cython\")\n-\n-    cytest = str(tmp_path / \"cytest\")\n-\n-    shutil.copytree(ext_dir, cytest)\n-    # build the examples and \"install\" them into a temporary directory\n-\n-    install_log = str(tmp_path / \"tmp_install_log.txt\")\n-    subprocess.check_output(\n-        [\n-            sys.executable,\n-            \"setup.py\",\n-            \"build\",\n-            \"install\",\n-            \"--prefix\", str(tmp_path / \"installdir\"),\n-            \"--single-version-externally-managed\",\n-            \"--record\",\n-            install_log,\n-        ],\n-        cwd=cytest,\n-    )\n-\n-    # In order to import the built module, we need its path to sys.path\n-    # so parse that out of the record\n-    with open(install_log) as fid:\n-        for line in fid:\n-            if \"checks\" in line:\n-                sys.path.append(os.path.dirname(line))\n-                break\n-        else:\n-            raise RuntimeError(f'could not parse \"{install_log}\"')\n-\n+    srcdir = os.path.join(os.path.dirname(__file__), 'examples', 'cython')\n+    build_dir = tmp_path / \"build\"\n+    os.makedirs(build_dir, exist_ok=True)\n+    try:\n+        subprocess.check_call([\"meson\", \"--version\"])\n+    except FileNotFoundError:\n+        pytest.skip(\"No usable 'meson' found\")\n+    if sys.platform == \"win32\":\n+        subprocess.check_call([\"meson\", \"setup\",\n+                               \"--buildtype=release\",\n+                               \"--vsenv\", str(srcdir)],\n+                              cwd=build_dir,\n+                              )\n+    else:\n+        subprocess.check_call([\"meson\", \"setup\", str(srcdir)],\n+                              cwd=build_dir\n+                              )\n+    subprocess.check_call([\"meson\", \"compile\", \"-vv\"], cwd=build_dir)\n+\n+    sys.path.append(str(build_dir))\n \n def test_is_timedelta64_object(install_temp):\n     import checks\n",
            "comment_added_diff": {},
            "comment_deleted_diff": {
                "45": "    # build the examples and \"install\" them into a temporary directory",
                "62": "    # In order to import the built module, we need its path to sys.path",
                "63": "    # so parse that out of the record"
            },
            "comment_modified_diff": {}
        }
    ],
    "test_datetime.py": [
        {
            "commit": "3139a881346ff7ad4326ecd296e6eeddf6c268a0",
            "timestamp": "2022-12-13T11:30:22-08:00",
            "author": "Sebastian Berg",
            "commit_message": "MAINT: Remove two TODO notes that got outdated (#22788)\n\nThe first one should have been removed in gh-22735, the second an even more\r\nrandom find.",
            "additions": 3,
            "deletions": 4,
            "change_type": "MODIFY",
            "diff": "@@ -1003,12 +1003,11 @@ def test_different_unit_comparison(self):\n                      casting='unsafe'))\n \n         # Shouldn't be able to compare datetime and timedelta\n-        # TODO: Changing to 'same_kind' or 'safe' casting in the ufuncs by\n-        #       default is needed to properly catch this kind of thing...\n         a = np.array('2012-12-21', dtype='M8[D]')\n         b = np.array(3, dtype='m8[D]')\n-        #assert_raises(TypeError, np.less, a, b)\n-        assert_raises(TypeError, np.less, a, b, casting='same_kind')\n+        assert_raises(TypeError, np.less, a, b)\n+        # not even if \"unsafe\"\n+        assert_raises(TypeError, np.less, a, b, casting='unsafe')\n \n     def test_datetime_like(self):\n         a = np.array([3], dtype='m8[4D]')\n",
            "comment_added_diff": {
                "1009": "        # not even if \"unsafe\""
            },
            "comment_deleted_diff": {
                "1006": "        # TODO: Changing to 'same_kind' or 'safe' casting in the ufuncs by",
                "1007": "        #       default is needed to properly catch this kind of thing...",
                "1010": "        #assert_raises(TypeError, np.less, a, b)"
            },
            "comment_modified_diff": {}
        },
        {
            "commit": "80d5aeb986a885b8cc43b27839477a15677bcac8",
            "timestamp": "2023-02-13T19:57:28+01:00",
            "author": "jbrockmendel",
            "commit_message": "BUG: datetime64/timedelta64 comparisons return NotImplemented (#23201)\n\n* BUG: datetime64/timedelta64 comparisons return NotImplemented\r\n\r\n* typo fixup\r\n\r\n* Update numpy/core/src/multiarray/scalartypes.c.src\r\n\r\nCo-authored-by: Sebastian Berg <sebastian@sipsolutions.net>\r\n\r\n* Update numpy/core/src/multiarray/scalartypes.c.src\r\n\r\n---------\r\n\r\nCo-authored-by: Sebastian Berg <sebastian@sipsolutions.net>",
            "additions": 20,
            "deletions": 0,
            "change_type": "MODIFY",
            "diff": "@@ -2529,3 +2529,23 @@ def test_non_ascii(self):\n \n         dt = np.datetime64('2000', '5\u03bcs')\n         assert np.datetime_data(dt.dtype) == ('us', 5)\n+\n+\n+def test_comparisons_return_not_implemented():\n+    # GH#17017\n+\n+    class custom:\n+        __array_priority__ = 10000\n+\n+    obj = custom()\n+\n+    dt = np.datetime64('2000', 'ns')\n+    td = dt - dt\n+\n+    for item in [dt, td]:\n+        assert item.__eq__(obj) is NotImplemented\n+        assert item.__ne__(obj) is NotImplemented\n+        assert item.__le__(obj) is NotImplemented\n+        assert item.__lt__(obj) is NotImplemented\n+        assert item.__ge__(obj) is NotImplemented\n+        assert item.__gt__(obj) is NotImplemented\n",
            "comment_added_diff": {
                "2535": "    # GH#17017"
            },
            "comment_deleted_diff": {},
            "comment_modified_diff": {}
        },
        {
            "commit": "1cf882248961a5d411c55d75bff190d7f7eb30e2",
            "timestamp": "2023-03-20T08:59:59+01:00",
            "author": "Pieter Eendebak",
            "commit_message": "BUG: Fix busday_count for reversed dates (#23229)\n\nFixes #23197",
            "additions": 21,
            "deletions": 3,
            "change_type": "MODIFY",
            "diff": "@@ -2330,16 +2330,23 @@ def test_datetime_busday_holidays_count(self):\n         assert_equal(np.busday_count('2011-01-01', dates, busdaycal=bdd),\n                      np.arange(366))\n         # Returns negative value when reversed\n+        # -1 since the '2011-01-01' is not a busday\n         assert_equal(np.busday_count(dates, '2011-01-01', busdaycal=bdd),\n-                     -np.arange(366))\n+                     -np.arange(366) - 1)\n \n+        # 2011-12-31 is a saturday\n         dates = np.busday_offset('2011-12-31', -np.arange(366),\n                         roll='forward', busdaycal=bdd)\n+        # only the first generated date is in the future of 2011-12-31\n+        expected = np.arange(366)\n+        expected[0] = -1\n         assert_equal(np.busday_count(dates, '2011-12-31', busdaycal=bdd),\n-                     np.arange(366))\n+                     expected)\n         # Returns negative value when reversed\n+        expected = -np.arange(366)+1\n+        expected[0] = 0\n         assert_equal(np.busday_count('2011-12-31', dates, busdaycal=bdd),\n-                     -np.arange(366))\n+                     expected)\n \n         # Can't supply both a weekmask/holidays and busdaycal\n         assert_raises(ValueError, np.busday_offset, '2012-01-03', '2012-02-03',\n@@ -2352,6 +2359,17 @@ def test_datetime_busday_holidays_count(self):\n         # Returns negative value when reversed\n         assert_equal(np.busday_count('2011-04', '2011-03', weekmask='Mon'), -4)\n \n+        sunday = np.datetime64('2023-03-05')\n+        monday = sunday + 1\n+        friday = sunday + 5\n+        saturday = sunday + 6\n+        assert_equal(np.busday_count(sunday, monday), 0)\n+        assert_equal(np.busday_count(monday, sunday), -1)\n+\n+        assert_equal(np.busday_count(friday, saturday), 1)\n+        assert_equal(np.busday_count(saturday, friday), 0)\n+\n+\n     def test_datetime_is_busday(self):\n         holidays = ['2011-01-01', '2011-10-10', '2011-11-11', '2011-11-24',\n                     '2011-12-25', '2011-05-30', '2011-02-21', '2011-01-17',\n",
            "comment_added_diff": {
                "2333": "        # -1 since the '2011-01-01' is not a busday",
                "2337": "        # 2011-12-31 is a saturday",
                "2340": "        # only the first generated date is in the future of 2011-12-31"
            },
            "comment_deleted_diff": {},
            "comment_modified_diff": {}
        }
    ],
    "test_errstate.py": [
        {
            "commit": "cd4ef0778cb0b53498ba787ab12ebe0d45bccd87",
            "timestamp": "2023-06-21T21:29:40+02:00",
            "author": "Sebastian Berg",
            "commit_message": "BUG: Make errstate decorator compatible with threading\n\nWe need to store the token per-thread, which doesn't work if one\ninstance of errstate is shared between threads (and we store it on\nthe instance).  `ContextDecorator` has a mechanism for this, but\nit is not public.\n\nSo, this just implements it the full manual style.\n\nI additionally add a new note on async-safety and the missing test\n(yes, fails on 1.25).\n\nCloses gh-24013",
            "additions": 42,
            "deletions": 0,
            "change_type": "MODIFY",
            "diff": "@@ -70,3 +70,45 @@ def foo():\n             a // 0\n             \n         foo()\n+\n+    def test_asyncio_safe(self):\n+        # May not be available e.g. on web assembly\n+        asyncio = pytest.importorskip(\"asyncio\")\n+\n+        @np.errstate(invalid=\"ignore\")\n+        def decorated():\n+            # Decorated non-async function (it is not safe to decorate an\n+            # async one)\n+            assert np.geterr()[\"invalid\"] == \"ignore\"\n+\n+        async def func1():\n+            decorated()\n+            await asyncio.sleep(0.1)\n+            decorated()\n+\n+        async def func2():\n+            with np.errstate(invalid=\"raise\"):\n+                assert np.geterr()[\"invalid\"] == \"raise\"\n+                await asyncio.sleep(0.125)\n+                assert np.geterr()[\"invalid\"] == \"raise\"\n+\n+        # for good sport, a third one with yet another state:\n+        async def func3():\n+            with np.errstate(invalid=\"print\"):\n+                assert np.geterr()[\"invalid\"] == \"print\"\n+                await asyncio.sleep(0.11)\n+                assert np.geterr()[\"invalid\"] == \"print\"\n+\n+        async def main():\n+            # simply run all three function multiple times:\n+            await asyncio.gather(\n+                    func1(), func2(), func3(), func1(), func2(), func3(),\n+                    func1(), func2(), func3(), func1(), func2(), func3())\n+\n+        loop = asyncio.new_event_loop()\n+        with np.errstate(invalid=\"warn\"):\n+            asyncio.run(main())\n+            assert np.geterr()[\"invalid\"] == \"warn\"\n+\n+        assert np.geterr()[\"invalid\"] == \"warn\"  # the default\n+        loop.close()\n",
            "comment_added_diff": {
                "75": "        # May not be available e.g. on web assembly",
                "80": "            # Decorated non-async function (it is not safe to decorate an",
                "81": "            # async one)",
                "95": "        # for good sport, a third one with yet another state:",
                "103": "            # simply run all three function multiple times:",
                "113": "        assert np.geterr()[\"invalid\"] == \"warn\"  # the default"
            },
            "comment_deleted_diff": {},
            "comment_modified_diff": {}
        },
        {
            "commit": "616afcacac8274b23068da6f2cac9ec0ca38836c",
            "timestamp": "2023-06-21T23:12:54+02:00",
            "author": "Sebastian Berg",
            "commit_message": "TST: Skip new test on wasm which doesn't support asyncio",
            "additions": 4,
            "deletions": 1,
            "change_type": "MODIFY",
            "diff": "@@ -71,8 +71,11 @@ def foo():\n             \n         foo()\n \n+    @pytest.mark.skipif(IS_WASM, reason=\"wasm doesn't support asyncio\")\n     def test_asyncio_safe(self):\n-        # May not be available e.g. on web assembly\n+        # asyncio may not always work, lets assume its fine if missing\n+        # Pyiodine/wasm doesn't support it.  If this test makes problems,\n+        # it should just be skipped liberally (or run differently).\n         asyncio = pytest.importorskip(\"asyncio\")\n \n         @np.errstate(invalid=\"ignore\")\n",
            "comment_added_diff": {
                "76": "        # asyncio may not always work, lets assume its fine if missing",
                "77": "        # Pyiodine/wasm doesn't support it.  If this test makes problems,",
                "78": "        # it should just be skipped liberally (or run differently)."
            },
            "comment_deleted_diff": {
                "75": "        # May not be available e.g. on web assembly"
            },
            "comment_modified_diff": {}
        },
        {
            "commit": "3a4bbda9fa114dfd5839f2a4de72b75ab7615ab2",
            "timestamp": "2023-06-22T09:23:54+02:00",
            "author": "Sebastian Berg",
            "commit_message": "Update numpy/core/tests/test_errstate.py\r\n\r\nAs per Guillaume Lemaitre comment.",
            "additions": 1,
            "deletions": 1,
            "change_type": "MODIFY",
            "diff": "@@ -74,7 +74,7 @@ def foo():\n     @pytest.mark.skipif(IS_WASM, reason=\"wasm doesn't support asyncio\")\n     def test_asyncio_safe(self):\n         # asyncio may not always work, lets assume its fine if missing\n-        # Pyiodine/wasm doesn't support it.  If this test makes problems,\n+        # Pyodide/wasm doesn't support it.  If this test makes problems,\n         # it should just be skipped liberally (or run differently).\n         asyncio = pytest.importorskip(\"asyncio\")\n \n",
            "comment_added_diff": {
                "77": "        # Pyodide/wasm doesn't support it.  If this test makes problems,"
            },
            "comment_deleted_diff": {
                "77": "        # Pyiodine/wasm doesn't support it.  If this test makes problems,"
            },
            "comment_modified_diff": {
                "77": "        # Pyiodine/wasm doesn't support it.  If this test makes problems,"
            }
        },
        {
            "commit": "1cf60159178971e3faacdbb0e4610485c5d0c6c6",
            "timestamp": "2023-06-27T13:09:42+02:00",
            "author": "Sebastian Berg",
            "commit_message": "TST: Add test that errstate can only be entered once",
            "additions": 12,
            "deletions": 0,
            "change_type": "MODIFY",
            "diff": "@@ -71,6 +71,18 @@ def foo():\n             \n         foo()\n \n+    def test_errstate_enter_once(self):\n+        errstate = np.errstate(invalid=\"warn\")\n+        with errstate:\n+            pass\n+\n+        # The errstate context cannot be entered twice as that would not be\n+        # thread-safe\n+        with pytest.raises(TypeError,\n+                match=\"Cannot enter `np.errstate` twice\"):\n+            with errstate:\n+                pass\n+\n     @pytest.mark.skipif(IS_WASM, reason=\"wasm doesn't support asyncio\")\n     def test_asyncio_safe(self):\n         # asyncio may not always work, lets assume its fine if missing\n",
            "comment_added_diff": {
                "79": "        # The errstate context cannot be entered twice as that would not be",
                "80": "        # thread-safe"
            },
            "comment_deleted_diff": {},
            "comment_modified_diff": {}
        }
    ],
    "test_limited_api.py": [],
    "test_mem_policy.py": [
        {
            "commit": "eeb8153a251675f90636fa09b9ae17044b5dc7f2",
            "timestamp": "2023-04-04T18:09:15+02:00",
            "author": "Sebastian Berg",
            "commit_message": "TST,DOC: Avoid spaces to hopefully ensure more info on error and fix memtests",
            "additions": 1,
            "deletions": 0,
            "change_type": "MODIFY",
            "diff": "@@ -89,6 +89,7 @@ def get_module(tmp_path):\n          \"\"\"),\n     ]\n     prologue = '''\n+        #define NPY_TARGET_VERSION NPY_1_22_API_VERSION\n         #define NPY_NO_DEPRECATED_API NPY_1_7_API_VERSION\n         #include <numpy/arrayobject.h>\n         /*\n",
            "comment_added_diff": {
                "92": "        #define NPY_TARGET_VERSION NPY_1_22_API_VERSION"
            },
            "comment_deleted_diff": {},
            "comment_modified_diff": {}
        },
        {
            "commit": "89ffd5765b8f9d30f59bec68e706ca777c4e2478",
            "timestamp": "2023-06-18T11:46:58+02:00",
            "author": "mattip",
            "commit_message": "fixes from review, try a -O0 build",
            "additions": 3,
            "deletions": 2,
            "change_type": "MODIFY",
            "diff": "@@ -377,10 +377,11 @@ def test_new_policy(get_module):\n         #\n         # if needed, debug this by\n         # - running tests with -- -s (to not capture stdout/stderr\n+        # - setting verbose=2\n         # - setting extra_argv=['-vv'] here\n-        assert np.core.test('full', verbose=2, extra_argv=['-vv'])\n+        assert np.core.test('full', verbose=1, extra_argv=[])\n         # also try the ma tests, the pickling test is quite tricky\n-        assert np.ma.test('full', verbose=2, extra_argv=['-vv'])\n+        assert np.ma.test('full', verbose=1, extra_argv=[])\n \n     get_module.set_old_policy(orig_policy)\n \n",
            "comment_added_diff": {
                "380": "        # - setting verbose=2"
            },
            "comment_deleted_diff": {},
            "comment_modified_diff": {}
        },
        {
            "commit": "e42fc93b54a6d41dab72d86921f96e5ebc4c4198",
            "timestamp": "2023-06-19T19:51:36+02:00",
            "author": "Ralf Gommers",
            "commit_message": "TST: skip memory allocator and `array_interface` tests on py312\n\nThey require numpy.distutils, which isn't available on >=3.12\nThe `numpy.testing.extbuild` utility will need changing to make this\nwork again. Could either use plain `setuptools` or `meson`.",
            "additions": 15,
            "deletions": 0,
            "change_type": "MODIFY",
            "diff": "@@ -9,6 +9,11 @@\n import sys\n \n \n+# FIXME: numpy.testing.extbuild uses `numpy.distutils`, so this won't work on\n+# Python 3.12 and up. It's an internal test utility, so for now we just skip\n+# these tests.\n+\n+\n @pytest.fixture\n def get_module(tmp_path):\n     \"\"\" Add a memory policy that returns a false pointer 64 bytes into the\n@@ -213,6 +218,7 @@ def get_module(tmp_path):\n                                                more_init=more_init)\n \n \n+@pytest.mark.skipif(sys.version_info >= (3, 12), reason=\"no numpy.distutils\")\n def test_set_policy(get_module):\n \n     get_handler_name = np.core.multiarray.get_handler_name\n@@ -241,6 +247,7 @@ def test_set_policy(get_module):\n         assert get_handler_name() == orig_policy_name\n \n \n+@pytest.mark.skipif(sys.version_info >= (3, 12), reason=\"no numpy.distutils\")\n def test_default_policy_singleton(get_module):\n     get_handler_name = np.core.multiarray.get_handler_name\n \n@@ -262,6 +269,7 @@ def test_default_policy_singleton(get_module):\n     assert def_policy_1 is def_policy_2 is get_module.get_default_policy()\n \n \n+@pytest.mark.skipif(sys.version_info >= (3, 12), reason=\"no numpy.distutils\")\n def test_policy_propagation(get_module):\n     # The memory policy goes hand-in-hand with flags.owndata\n \n@@ -320,6 +328,7 @@ async def async_test_context_locality(get_module):\n     assert np.core.multiarray.get_handler_name() == orig_policy_name\n \n \n+@pytest.mark.skipif(sys.version_info >= (3, 12), reason=\"no numpy.distutils\")\n def test_context_locality(get_module):\n     if (sys.implementation.name == 'pypy'\n             and sys.pypy_version_info[:3] < (7, 3, 6)):\n@@ -341,6 +350,7 @@ def concurrent_thread2(get_module, event):\n     get_module.set_secret_data_policy()\n \n \n+@pytest.mark.skipif(sys.version_info >= (3, 12), reason=\"no numpy.distutils\")\n def test_thread_locality(get_module):\n     orig_policy_name = np.core.multiarray.get_handler_name()\n \n@@ -359,6 +369,7 @@ def test_thread_locality(get_module):\n     assert np.core.multiarray.get_handler_name() == orig_policy_name\n \n \n+@pytest.mark.skipif(sys.version_info >= (3, 12), reason=\"no numpy.distutils\")\n @pytest.mark.skip(reason=\"too slow, see gh-23975\")\n def test_new_policy(get_module):\n     a = np.arange(10)\n@@ -388,6 +399,8 @@ def test_new_policy(get_module):\n     c = np.arange(10)\n     assert np.core.multiarray.get_handler_name(c) == orig_policy_name\n \n+\n+@pytest.mark.skipif(sys.version_info >= (3, 12), reason=\"no numpy.distutils\")\n @pytest.mark.xfail(sys.implementation.name == \"pypy\",\n                    reason=(\"bad interaction between getenv and \"\n                            \"os.environ inside pytest\"))\n@@ -420,6 +433,8 @@ def test_switch_owner(get_module, policy):\n         else:\n             os.environ['NUMPY_WARN_IF_NO_MEM_POLICY'] = oldval\n \n+\n+@pytest.mark.skipif(sys.version_info >= (3, 12), reason=\"no numpy.distutils\")\n def test_owner_is_base(get_module):\n     a = get_module.get_array_with_base()\n     with pytest.warns(UserWarning, match='warn_on_free'):\n",
            "comment_added_diff": {
                "12": "# FIXME: numpy.testing.extbuild uses `numpy.distutils`, so this won't work on",
                "13": "# Python 3.12 and up. It's an internal test utility, so for now we just skip",
                "14": "# these tests."
            },
            "comment_deleted_diff": {},
            "comment_modified_diff": {}
        },
        {
            "commit": "1d0487b5053405775ef136e9acafdf4c51f5a1df",
            "timestamp": "2023-07-24T10:12:09+02:00",
            "author": "Sebastian Berg",
            "commit_message": "BUG: Factor out slow `getenv` call used for memory policy warning\n\nUsing `getenv` regularly is probably not great anyway, but it seems\nvery slow on windows which leads to a large overhead for every array\ndeallocation here.\n\nRefactor it out to only check on first import and add helper because\nthe tests are set up slightly differently.\n(Manually checked that the startup works, tests run with policy\nset to 1, not modifying it and passing.)",
            "additions": 10,
            "deletions": 10,
            "change_type": "MODIFY",
            "diff": "@@ -409,16 +409,19 @@ def test_switch_owner(get_module, policy):\n     a = get_module.get_array()\n     assert np.core.multiarray.get_handler_name(a) is None\n     get_module.set_own(a)\n-    oldval = os.environ.get('NUMPY_WARN_IF_NO_MEM_POLICY', None)\n+\n     if policy is None:\n-        if 'NUMPY_WARN_IF_NO_MEM_POLICY' in os.environ:\n-            os.environ.pop('NUMPY_WARN_IF_NO_MEM_POLICY')\n+        # See what we expect to be set based on the env variable\n+        policy = os.getenv(\"NUMPY_WARN_IF_NO_MEM_POLICY\", \"0\") == \"1\"\n+        oldval = None\n     else:\n-        os.environ['NUMPY_WARN_IF_NO_MEM_POLICY'] = policy\n+        policy = policy == \"1\"\n+        oldval = np.core._multiarray_umath._set_numpy_warn_if_no_mem_policy(\n+            policy)\n     try:\n         # The policy should be NULL, so we have to assume we can call\n         # \"free\".  A warning is given if the policy == \"1\"\n-        if policy == \"1\":\n+        if policy:\n             with assert_warns(RuntimeWarning) as w:\n                 del a\n                 gc.collect()\n@@ -427,11 +430,8 @@ def test_switch_owner(get_module, policy):\n             gc.collect()\n \n     finally:\n-        if oldval is None:\n-            if 'NUMPY_WARN_IF_NO_MEM_POLICY' in os.environ:\n-                os.environ.pop('NUMPY_WARN_IF_NO_MEM_POLICY')\n-        else:\n-            os.environ['NUMPY_WARN_IF_NO_MEM_POLICY'] = oldval\n+        if oldval is not None:\n+            np.core._multiarray_umath._set_numpy_warn_if_no_mem_policy(oldval)\n \n \n @pytest.mark.skipif(sys.version_info >= (3, 12), reason=\"no numpy.distutils\")\n",
            "comment_added_diff": {
                "414": "        # See what we expect to be set based on the env variable"
            },
            "comment_deleted_diff": {},
            "comment_modified_diff": {
                "414": "        if 'NUMPY_WARN_IF_NO_MEM_POLICY' in os.environ:"
            }
        }
    ],
    "test_build_ext.py": [],
    "test_shell_utils.py": [],
    "test_abstract_interface.py": [],
    "test_pocketfft.py": [],
    "test_linalg.py": [
        {
            "commit": "bcd33b0ce57aa6a882c9e642ea13fa08ffbd059d",
            "timestamp": "2023-06-26T15:05:51+02:00",
            "author": "Quentin Barth\u00e9lemy",
            "commit_message": "TST: improve test for Cholesky decomposition (#24025)\n\nImproves the test for Cholesky decomposition,\r\ntesting if diagonal elements are real and positive.",
            "additions": 6,
            "deletions": 2,
            "change_type": "MODIFY",
            "diff": "@@ -1785,7 +1785,6 @@ def test_stacked_inputs(self, outer_size, size, dt):\n \n \n class TestCholesky:\n-    # TODO: are there no other tests for cholesky?\n \n     @pytest.mark.parametrize(\n         'shape', [(1, 1), (2, 2), (3, 3), (50, 50), (3, 10, 10)]\n@@ -1794,7 +1793,6 @@ class TestCholesky:\n         'dtype', (np.float32, np.float64, np.complex64, np.complex128)\n     )\n     def test_basic_property(self, shape, dtype):\n-        # Check A = L L^H\n         np.random.seed(1)\n         a = np.random.randn(*shape)\n         if np.issubdtype(dtype, np.complexfloating):\n@@ -1808,11 +1806,17 @@ def test_basic_property(self, shape, dtype):\n \n         c = np.linalg.cholesky(a)\n \n+        # Check A = L L^H\n         b = np.matmul(c, c.transpose(t).conj())\n         with np._no_nep50_warning():\n             atol = 500 * a.shape[0] * np.finfo(dtype).eps\n         assert_allclose(b, a, atol=atol, err_msg=f'{shape} {dtype}\\n{a}\\n{c}')\n \n+        # Check diag(L) is real and positive\n+        d = np.diagonal(c, axis1=-2, axis2=-1)\n+        assert_(np.all(np.isreal(d)))\n+        assert_(np.all(d >= 0))\n+\n     def test_0_size(self):\n         class ArraySubclass(np.ndarray):\n             pass\n",
            "comment_added_diff": {
                "1809": "        # Check A = L L^H",
                "1815": "        # Check diag(L) is real and positive"
            },
            "comment_deleted_diff": {
                "1788": "    # TODO: are there no other tests for cholesky?",
                "1797": "        # Check A = L L^H"
            },
            "comment_modified_diff": {}
        },
        {
            "commit": "b59da2d3000c56cc00dbcbd7228e35e1c839297a",
            "timestamp": "2023-07-23T23:37:14+02:00",
            "author": "Ralf Gommers",
            "commit_message": "TST: fix running the test suite in builds without BLAS/LAPACK\n\nThis was broken with `undefined symbol: dlapy3_` because the test\nsuite imports `linalg.lapack_lite` directly. See gh-24200 for more\ndetails.",
            "additions": 7,
            "deletions": 0,
            "change_type": "MODIFY",
            "diff": "@@ -21,6 +21,13 @@\n     assert_almost_equal, assert_allclose, suppress_warnings,\n     assert_raises_regex, HAS_LAPACK64, IS_WASM\n     )\n+try:\n+    import numpy.linalg.lapack_lite\n+except ImportError:\n+    # May be broken when numpy was built without BLAS/LAPACK present\n+    # If so, ensure we don't break the whole test suite - the `lapack_lite`\n+    # submodule should be removed, it's only used in two tests in this file.\n+    pass\n \n \n def consistent_subclass(out, in_):\n",
            "comment_added_diff": {
                "27": "    # May be broken when numpy was built without BLAS/LAPACK present",
                "28": "    # If so, ensure we don't break the whole test suite - the `lapack_lite`",
                "29": "    # submodule should be removed, it's only used in two tests in this file."
            },
            "comment_deleted_diff": {},
            "comment_modified_diff": {}
        }
    ],
    "test_extending.py": [
        {
            "commit": "d56c48754ba07b36e0e8561887cbc0c732ff54d0",
            "timestamp": "2022-12-02T20:28:48+01:00",
            "author": "Sebastian Berg",
            "commit_message": "TST: Skip when numba/numpy compat issues cause SystemError\n\nnumba is a bit buggy when wrapping ufuncs, so what should be nothing\nis (on non dev versions) a SystemError and not even an ImportError.\n\nSo simply catch those too, since it can be a confusing error during\ndev otherwise.",
            "additions": 2,
            "deletions": 1,
            "change_type": "MODIFY",
            "diff": "@@ -23,7 +23,8 @@\n         # numba issue gh-4733\n         warnings.filterwarnings('always', '', DeprecationWarning)\n         import numba\n-except ImportError:\n+except (ImportError, SystemError):\n+    # Certain numpy/numba versions trigger a SystemError due to a numba bug\n     numba = None\n \n try:\n",
            "comment_added_diff": {
                "27": "    # Certain numpy/numba versions trigger a SystemError due to a numba bug"
            },
            "comment_deleted_diff": {},
            "comment_modified_diff": {}
        },
        {
            "commit": "2e0044142c9ab00394b9a58d3698c8477191915d",
            "timestamp": "2023-07-13T16:44:03+03:00",
            "author": "mattip",
            "commit_message": "refactor test_extending to use meson",
            "additions": 69,
            "deletions": 27,
            "change_type": "MODIFY",
            "diff": "@@ -1,8 +1,11 @@\n+import importlib\n import os\n+import pathlib\n import pytest\n import shutil\n import subprocess\n import sys\n+import textwrap\n import warnings\n \n import numpy as np\n@@ -39,33 +42,77 @@\n     # other fixes in the 0.29 series that are needed even for earlier\n     # Python versions.\n     # Note: keep in sync with the one in pyproject.toml\n-    required_version = '0.29.30'\n+    required_version = '0.29.35'\n     if _pep440.parse(cython_version) < _pep440.Version(required_version):\n         # too old or wrong cython, skip the test\n         cython = None\n \n \n-@pytest.mark.skipif(sys.version_info >= (3, 12),\n-                    reason=\"numpy.distutils not supported anymore\")\n @pytest.mark.skipif(IS_WASM, reason=\"Can't start subprocess\")\n @pytest.mark.skipif(cython is None, reason=\"requires cython\")\n @pytest.mark.slow\n def test_cython(tmp_path):\n-    from numpy.distutils.misc_util import exec_mod_from_location\n+    import glob\n+    # build the examples in a temporary directory\n     srcdir = os.path.join(os.path.dirname(__file__), '..')\n     shutil.copytree(srcdir, tmp_path / 'random')\n-    # build the examples and \"install\" them into a temporary directory\n     build_dir = tmp_path / 'random' / '_examples' / 'cython'\n-    subprocess.check_call([sys.executable, 'setup.py', 'build', 'install',\n-                           '--prefix', str(tmp_path / 'installdir'),\n-                           '--single-version-externally-managed',\n-                           '--record', str(tmp_path/ 'tmp_install_log.txt'),\n-                          ],\n-                          cwd=str(build_dir),\n-                      )\n+    # We don't want a wheel build, so do the steps in a controlled way\n+    # The meson.build file is not copied as part of the build, so generate it\n+    with open(build_dir / \"meson.build\", \"wt\", encoding=\"utf-8\") as fid:\n+        fid.write(textwrap.dedent(\"\"\"\\\n+            project('random-build-examples', 'c', 'cpp', 'cython')\n+\n+            # https://mesonbuild.com/Python-module.html\n+            py_mod = import('python')\n+            py3 = py_mod.find_installation(pure: false)\n+            py3_dep = py3.dependency()\n+\n+            py_mod = import('python')\n+            py = py_mod.find_installation(pure: false)\n+            cc = meson.get_compiler('c')\n+            cy = meson.get_compiler('cython')\n+\n+            if not cy.version().version_compare('>=0.29.35')\n+              error('tests requires Cython >= 0.29.35')\n+            endif\n+\n+            _numpy_abs = run_command(py3,\n+                ['-c', 'import os; os.chdir(\"..\"); import numpy; print(os.path.abspath(numpy.get_include() + \"../../..\"))'],\n+                check: true\n+              ).stdout().strip()\n+\n+            npymath_path = _numpy_abs / 'core' / 'lib'\n+            npy_include_path = _numpy_abs / 'core' / 'include'\n+            npyrandom_path = _numpy_abs / 'random' / 'lib'\n+            npymath_lib = cc.find_library('npymath', dirs: npymath_path)\n+            npyrandom_lib = cc.find_library('npyrandom', dirs: npyrandom_path)\n+\n+            py.extension_module(\n+                'extending_distributions',\n+                'extending_distributions.pyx',\n+                install: false,\n+                include_directories: [npy_include_path],\n+                dependencies: [npyrandom_lib, npymath_lib],\n+            )\n+            py.extension_module(\n+                'extending',\n+                'extending.pyx',\n+                install: false,\n+                include_directories: [npy_include_path],\n+                dependencies: [npyrandom_lib, npymath_lib],\n+            )\n+        \"\"\"))\n+    target_dir = build_dir / \"build\"\n+    os.makedirs(target_dir, exist_ok=True)\n+    subprocess.check_call([\"meson\", \"setup\", str(build_dir)], cwd=target_dir)\n+    subprocess.check_call([\"meson\", \"compile\"], cwd=target_dir)\n+\n     # gh-16162: make sure numpy's __init__.pxd was used for cython\n     # not really part of this test, but it is a convenient place to check\n-    with open(build_dir / 'extending.c') as fid:\n+\n+    g = glob.glob(str(target_dir / \"*\" / \"extending.pyx.c\"))\n+    with open(g[0]) as fid:\n         txt_to_find = 'NumPy API declarations from \"numpy/__init__'\n         for i, line in enumerate(fid):\n             if txt_to_find in line:\n@@ -73,20 +120,15 @@ def test_cython(tmp_path):\n         else:\n             assert False, (\"Could not find '{}' in C file, \"\n                            \"wrong pxd used\".format(txt_to_find))\n-    # get the path to the so's\n-    so1 = so2 = None\n-    with open(tmp_path /'tmp_install_log.txt') as fid:\n-        for line in fid:\n-            if 'extending.' in line:\n-                so1 = line.strip()\n-            if 'extending_distributions' in line:\n-                so2 = line.strip()\n-    assert so1 is not None\n-    assert so2 is not None\n-    # import the so's without adding the directory to sys.path\n-    exec_mod_from_location('extending', so1)\n-    extending_distributions = exec_mod_from_location(\n-                    'extending_distributions', so2)\n+    # import without adding the directory to sys.path\n+    so1 = sorted(glob.glob(str(target_dir / \"extending.*\")))[0]\n+    so2 = sorted(glob.glob(str(target_dir / \"extending_distributions.*\")))[0]\n+    spec1 = importlib.util.spec_from_file_location(\"extending\", so1)\n+    spec2 = importlib.util.spec_from_file_location(\"extending_distributions\", so2)\n+    extending = importlib.util.module_from_spec(spec1)\n+    spec1.loader.exec_module(extending)\n+    extending_distributions = importlib.util.module_from_spec(spec2)\n+    spec2.loader.exec_module(extending_distributions)\n     # actually test the cython c-extension\n     from numpy.random import PCG64\n     values = extending_distributions.uniforms_ex(PCG64(0), 10, 'd')\n",
            "comment_added_diff": {
                "56": "    # build the examples in a temporary directory",
                "60": "    # We don't want a wheel build, so do the steps in a controlled way",
                "61": "    # The meson.build file is not copied as part of the build, so generate it",
                "66": "            # https://mesonbuild.com/Python-module.html",
                "123": "    # import without adding the directory to sys.path"
            },
            "comment_deleted_diff": {
                "57": "    # build the examples and \"install\" them into a temporary directory",
                "76": "    # get the path to the so's",
                "86": "    # import the so's without adding the directory to sys.path"
            },
            "comment_modified_diff": {
                "60": "                           '--prefix', str(tmp_path / 'installdir'),",
                "61": "                           '--single-version-externally-managed',"
            }
        },
        {
            "commit": "bedeacfc1b5cd1795609ed71d3aed51e9ef1811a",
            "timestamp": "2023-07-13T16:48:41+03:00",
            "author": "mattip",
            "commit_message": "TST: refactor to add a cython build that uses cpp",
            "additions": 21,
            "deletions": 8,
            "change_type": "MODIFY",
            "diff": "@@ -103,6 +103,15 @@ def test_cython(tmp_path):\n                 include_directories: [npy_include_path],\n                 dependencies: [npyrandom_lib, npymath_lib],\n             )\n+            py.extension_module(\n+                'extending_cpp',\n+                'extending_distributions.pyx',\n+                install: false,\n+                override_options : ['cython_language=cpp'],\n+                cython_args: ['--module-name', 'extending_cpp'],\n+                include_directories: [npy_include_path],\n+                dependencies: [npyrandom_lib, npymath_lib],\n+            )\n         \"\"\"))\n     target_dir = build_dir / \"build\"\n     os.makedirs(target_dir, exist_ok=True)\n@@ -132,15 +141,19 @@ def test_cython(tmp_path):\n                            \"wrong pxd used\".format(txt_to_find))\n     # import without adding the directory to sys.path\n     suffix = sysconfig.get_config_var('EXT_SUFFIX')\n-    so1 = (target_dir / \"extending\").with_suffix(suffix)\n-    so2 = (target_dir / \"extending_distributions\").with_suffix(suffix)\n-    spec1 = spec_from_file_location(\"extending\", so1)\n-    spec2 = spec_from_file_location(\"extending_distributions\", so2)\n-    extending = module_from_spec(spec1)\n-    spec1.loader.exec_module(extending)\n-    extending_distributions = module_from_spec(spec2)\n-    spec2.loader.exec_module(extending_distributions)\n+\n+    def load(modname):\n+        so = (target_dir / modname).with_suffix(suffix)\n+        spec = spec_from_file_location(modname, so)\n+        mod = module_from_spec(spec)\n+        spec.loader.exec_module(mod)\n+        return mod\n+\n+    # test that the module can be imported\n+    load(\"extending\")\n+    load(\"extending_cpp\")\n     # actually test the cython c-extension\n+    extending_distributions = load(\"extending_distributions\")\n     from numpy.random import PCG64\n     values = extending_distributions.uniforms_ex(PCG64(0), 10, 'd')\n     assert values.shape == (10,)\n",
            "comment_added_diff": {
                "152": "    # test that the module can be imported"
            },
            "comment_deleted_diff": {},
            "comment_modified_diff": {}
        },
        {
            "commit": "43c32e65d9534815435d56ddea93e60af9f25204",
            "timestamp": "2023-07-14T17:39:25+03:00",
            "author": "mattip",
            "commit_message": "fixes from review",
            "additions": 0,
            "deletions": 55,
            "change_type": "MODIFY",
            "diff": "@@ -58,61 +58,6 @@ def test_cython(tmp_path):\n     srcdir = os.path.join(os.path.dirname(__file__), '..')\n     shutil.copytree(srcdir, tmp_path / 'random')\n     build_dir = tmp_path / 'random' / '_examples' / 'cython'\n-    # We don't want a wheel build, so do the steps in a controlled way\n-    # The meson.build file is not copied as part of the build, so generate it\n-    with open(build_dir / \"meson.build\", \"wt\", encoding=\"utf-8\") as fid:\n-        get_inc = ('import os; os.chdir(\"..\"); import numpy; '\n-                   'print(os.path.abspath(numpy.get_include() + \"../../..\"))')\n-        fid.write(textwrap.dedent(f\"\"\"\\\n-            project('random-build-examples', 'c', 'cpp', 'cython')\n-\n-            # https://mesonbuild.com/Python-module.html\n-            py_mod = import('python')\n-            py3 = py_mod.find_installation(pure: false)\n-            py3_dep = py3.dependency()\n-\n-            py_mod = import('python')\n-            py = py_mod.find_installation(pure: false)\n-            cc = meson.get_compiler('c')\n-            cy = meson.get_compiler('cython')\n-\n-            if not cy.version().version_compare('>=0.29.35')\n-              error('tests requires Cython >= 0.29.35')\n-            endif\n-\n-            _numpy_abs = run_command(py3, ['-c', '{get_inc}'],\n-                                     check: true).stdout().strip()\n-\n-            npymath_path = _numpy_abs / 'core' / 'lib'\n-            npy_include_path = _numpy_abs / 'core' / 'include'\n-            npyrandom_path = _numpy_abs / 'random' / 'lib'\n-            npymath_lib = cc.find_library('npymath', dirs: npymath_path)\n-            npyrandom_lib = cc.find_library('npyrandom', dirs: npyrandom_path)\n-\n-            py.extension_module(\n-                'extending_distributions',\n-                'extending_distributions.pyx',\n-                install: false,\n-                include_directories: [npy_include_path],\n-                dependencies: [npyrandom_lib, npymath_lib],\n-            )\n-            py.extension_module(\n-                'extending',\n-                'extending.pyx',\n-                install: false,\n-                include_directories: [npy_include_path],\n-                dependencies: [npyrandom_lib, npymath_lib],\n-            )\n-            py.extension_module(\n-                'extending_cpp',\n-                'extending_distributions.pyx',\n-                install: false,\n-                override_options : ['cython_language=cpp'],\n-                cython_args: ['--module-name', 'extending_cpp'],\n-                include_directories: [npy_include_path],\n-                dependencies: [npyrandom_lib, npymath_lib],\n-            )\n-        \"\"\"))\n     target_dir = build_dir / \"build\"\n     os.makedirs(target_dir, exist_ok=True)\n     if sys.platform == \"win32\":\n",
            "comment_added_diff": {},
            "comment_deleted_diff": {
                "61": "    # We don't want a wheel build, so do the steps in a controlled way",
                "62": "    # The meson.build file is not copied as part of the build, so generate it",
                "69": "            # https://mesonbuild.com/Python-module.html"
            },
            "comment_modified_diff": {}
        }
    ],
    "test_public_api.py": [
        {
            "commit": "928a7b40a40941c0c282cfad78c3a6021422a71c",
            "timestamp": "2022-12-06T12:00:21+01:00",
            "author": "Sebastian Berg",
            "commit_message": "API: Hide exceptions from the main namespace\n\nI wasn't sure if we should already start deprecating the exceptions\nso opted to follow up with only hiding them from `__dir__()` but\nstill having them in `__all__` and available.\n\nThis also changes their module to `numpy.exceptions`, which matters\nbecause that is how they will be pickled (it would not be possible\nto unpickle such an exception in an older NumPy version).\n\nDue to pickling, we could put off changing the module.",
            "additions": 14,
            "deletions": 0,
            "change_type": "MODIFY",
            "diff": "@@ -505,3 +505,17 @@ def test_array_api_entry_point():\n         \"does not point to our Array API implementation\"\n     )\n     assert xp is numpy.array_api, msg\n+\n+\n+@pytest.mark.parametrize(\"name\", [\n+        'ModuleDeprecationWarning', 'VisibleDeprecationWarning',\n+        'ComplexWarning', 'TooHardError', 'AxisError'])\n+def test_moved_exceptions(name):\n+    # These were moved to the exceptions namespace, but currently still\n+    # available\n+    assert name in np.__all__\n+    assert name not in np.__dir__()\n+    # Fetching works, but __module__ is set correctly:\n+    assert getattr(np, name).__module__ == \"numpy.exceptions\"\n+    assert name in np.exceptions.__all__\n+    getattr(np.exceptions, name)\n",
            "comment_added_diff": {
                "514": "    # These were moved to the exceptions namespace, but currently still",
                "515": "    # available",
                "518": "    # Fetching works, but __module__ is set correctly:"
            },
            "comment_deleted_diff": {},
            "comment_modified_diff": {}
        },
        {
            "commit": "12efa8eec31bd476e0110bb90de43d603d0e76d3",
            "timestamp": "2023-06-14T19:30:01+02:00",
            "author": "Mateusz Sok\u00f3\u0142",
            "commit_message": "DEP: deprecate compat and selected lib utils (#23830)\n\n[skip ci]",
            "additions": 2,
            "deletions": 0,
            "change_type": "MODIFY",
            "diff": "@@ -326,6 +326,8 @@ def is_unexpected(name):\n ]\n \n \n+# suppressing warnings from deprecated modules\n+@pytest.mark.filterwarnings(\"ignore:.*np.compat.*:DeprecationWarning\")\n def test_all_modules_are_expected():\n     \"\"\"\n     Test that we don't add anything that looks like a new public module by\n",
            "comment_added_diff": {
                "329": "# suppressing warnings from deprecated modules"
            },
            "comment_deleted_diff": {},
            "comment_modified_diff": {}
        },
        {
            "commit": "21fac695731927e90074aed48941a22f2a12953d",
            "timestamp": "2023-07-24T10:35:07+02:00",
            "author": "Ralf Gommers",
            "commit_message": "TST: enable test that checks for `numpy.array_api` entry point",
            "additions": 15,
            "deletions": 6,
            "change_type": "MODIFY",
            "diff": "@@ -488,10 +488,7 @@ def check_importable(module_name):\n \n \n @pytest.mark.xfail(\n-    reason = \"meson-python doesn't install this entrypoint correctly yet\",\n-)\n-@pytest.mark.xfail(\n-    sysconfig.get_config_var(\"Py_DEBUG\") is not None,\n+    sysconfig.get_config_var(\"Py_DEBUG\") not in (None, 0, \"0\"),\n     reason=(\n         \"NumPy possibly built with `USE_DEBUG=True ./tools/travis-test.sh`, \"\n         \"which does not expose the `array_api` entry point. \"\n@@ -503,6 +500,11 @@ def test_array_api_entry_point():\n     Entry point for Array API implementation can be found with importlib and\n     returns the numpy.array_api namespace.\n     \"\"\"\n+    # For a development install that did not go through meson-python,\n+    # the entrypoint will not have been installed. So ensure this test fails\n+    # only if numpy is inside site-packages.\n+    numpy_in_sitepackages = sysconfig.get_path('platlib') in np.__file__\n+\n     eps = importlib.metadata.entry_points()\n     try:\n         xp_eps = eps.select(group=\"array_api\")\n@@ -512,12 +514,19 @@ def test_array_api_entry_point():\n         # Array API entry points so that running this test in <=3.9 will\n         # still work - see https://github.com/numpy/numpy/pull/19800.\n         xp_eps = eps.get(\"array_api\", [])\n-    assert len(xp_eps) > 0, \"No entry points for 'array_api' found\"\n+    if len(xp_eps) == 0:\n+        if numpy_in_sitepackages:\n+            msg = \"No entry points for 'array_api' found\"\n+            raise AssertionError(msg) from None\n+        return\n \n     try:\n         ep = next(ep for ep in xp_eps if ep.name == \"numpy\")\n     except StopIteration:\n-        raise AssertionError(\"'numpy' not in array_api entry points\") from None\n+        if numpy_in_sitepackages:\n+            msg = \"'numpy' not in array_api entry points\"\n+            raise AssertionError(msg) from None\n+        return\n \n     xp = ep.load()\n     msg = (\n",
            "comment_added_diff": {
                "503": "    # For a development install that did not go through meson-python,",
                "504": "    # the entrypoint will not have been installed. So ensure this test fails",
                "505": "    # only if numpy is inside site-packages."
            },
            "comment_deleted_diff": {},
            "comment_modified_diff": {}
        },
        {
            "commit": "c8e2343d22bc886dd08775589bc2405016349f37",
            "timestamp": "2023-08-07T22:02:46+02:00",
            "author": "Mateusz Sok\u00f3\u0142",
            "commit_message": "API: Cleaning `numpy/__init__.py` and main namespace - Part 1 [NEP 52] (#24316)",
            "additions": 0,
            "deletions": 14,
            "change_type": "MODIFY",
            "diff": "@@ -533,17 +533,3 @@ def test_array_api_entry_point():\n         \"does not point to our Array API implementation\"\n     )\n     assert xp is numpy.array_api, msg\n-\n-\n-@pytest.mark.parametrize(\"name\", [\n-        'ModuleDeprecationWarning', 'VisibleDeprecationWarning',\n-        'ComplexWarning', 'TooHardError', 'AxisError'])\n-def test_moved_exceptions(name):\n-    # These were moved to the exceptions namespace, but currently still\n-    # available\n-    assert name in np.__all__\n-    assert name not in np.__dir__()\n-    # Fetching works, but __module__ is set correctly:\n-    assert getattr(np, name).__module__ == \"numpy.exceptions\"\n-    assert name in np.exceptions.__all__\n-    getattr(np.exceptions, name)\n",
            "comment_added_diff": {},
            "comment_deleted_diff": {
                "542": "    # These were moved to the exceptions namespace, but currently still",
                "543": "    # available",
                "546": "    # Fetching works, but __module__ is set correctly:"
            },
            "comment_modified_diff": {}
        },
        {
            "commit": "be896c9c652c010093d26bd0ff732dbfabcd6361",
            "timestamp": "2023-08-11T22:40:16+02:00",
            "author": "Stefan van der Walt",
            "commit_message": "MAINT: Remove versioneer (#24196)\n\nCo-authored-by: Ralf Gommers <ralf.gommers@gmail.com>",
            "additions": 1,
            "deletions": 1,
            "change_type": "MODIFY",
            "diff": "@@ -156,7 +156,7 @@ def test_NPY_NO_EXPORT():\n     \"testing.overrides\",\n     \"typing\",\n     \"typing.mypy_plugin\",\n-    \"version\",\n+    \"version\"  # Should be removed for NumPy 2.0\n ]]\n if sys.version_info < (3, 12):\n     PUBLIC_MODULES += [\n",
            "comment_added_diff": {
                "159": "    \"version\"  # Should be removed for NumPy 2.0"
            },
            "comment_deleted_diff": {},
            "comment_modified_diff": {
                "159": "    \"version\","
            }
        },
        {
            "commit": "5312b6ed486a503e1917065197d5c6401e501400",
            "timestamp": "2023-09-17T20:02:44+02:00",
            "author": "Mateusz Sok\u00f3\u0142",
            "commit_message": "Apply review comments",
            "additions": 3,
            "deletions": 6,
            "change_type": "MODIFY",
            "diff": "@@ -31,14 +31,11 @@ def check_dir(module, module_name=None):\n \n \n def test_numpy_namespace():\n-    # None of these objects are publicly documented to be part of the main\n-    # NumPy namespace (some are useful though, others need to be cleaned up)\n-    undocumented = {\n+    # We override dir to not show these members\n+    allowlist = {\n+        'recarray': 'numpy.rec.recarray',\n         'show_config': 'numpy.__config__.show',\n-        'row_stack': 'numpy.lib._shape_base_impl.row_stack'\n     }\n-    # We override dir to not show these members\n-    allowlist = undocumented\n     bad_results = check_dir(np)\n     # pytest gives better error messages with the builtin assert than with\n     # assert_equal\n",
            "comment_added_diff": {
                "34": "    # We override dir to not show these members"
            },
            "comment_deleted_diff": {
                "34": "    # None of these objects are publicly documented to be part of the main",
                "35": "    # NumPy namespace (some are useful though, others need to be cleaned up)",
                "40": "    # We override dir to not show these members"
            },
            "comment_modified_diff": {
                "34": "    # None of these objects are publicly documented to be part of the main"
            }
        }
    ],
    "test_reloading.py": [],
    "test_scripts.py": [],
    "arrayprint.py": [
        {
            "commit": "071388f957c13c1a4f03bc811e3d128335ac686e",
            "timestamp": "2023-03-14T13:57:27+01:00",
            "author": "molsonkiko",
            "commit_message": "ENH: show dtype in array repr when endianness is non-native (#23295)\n\nFx problem where, for example,\r\n\r\nnp.array([1], dtype='>u2') and np.array([1], dtype='<u2')\r\n\r\nboth got represented as np.array([1], dtype=uint16), or the dtype is not shown for the default ones (float64, default int).",
            "additions": 9,
            "deletions": 1,
            "change_type": "MODIFY",
            "diff": "@@ -1428,6 +1428,10 @@ def dtype_is_implied(dtype):\n     # not just void types can be structured, and names are not part of the repr\n     if dtype.names is not None:\n         return False\n+    \n+    # should care about endianness *unless size is 1* (e.g., int8, bool)\n+    if not dtype.isnative:\n+        return False\n \n     return dtype.type in _typelessdata\n \n@@ -1453,10 +1457,14 @@ def dtype_short_repr(dtype):\n         return \"'%s'\" % str(dtype)\n \n     typename = dtype.name\n+    if not dtype.isnative:\n+        # deal with cases like dtype('<u2') that are identical to an\n+        # established dtype (in this case uint16)\n+        # except that they have a different endianness.\n+        return \"'%s'\" % str(dtype)\n     # quote typenames which can't be represented as python variable names\n     if typename and not (typename[0].isalpha() and typename.isalnum()):\n         typename = repr(typename)\n-\n     return typename\n \n \n",
            "comment_added_diff": {
                "1432": "    # should care about endianness *unless size is 1* (e.g., int8, bool)",
                "1461": "        # deal with cases like dtype('<u2') that are identical to an",
                "1462": "        # established dtype (in this case uint16)",
                "1463": "        # except that they have a different endianness."
            },
            "comment_deleted_diff": {},
            "comment_modified_diff": {}
        },
        {
            "commit": "017cef318ad37ac05738830ebb3173c65c128759",
            "timestamp": "2023-07-11T22:25:10+02:00",
            "author": "Mateusz Sok\u00f3\u0142",
            "commit_message": "API: apply review comments",
            "additions": 9,
            "deletions": 2,
            "change_type": "MODIFY",
            "diff": "@@ -42,7 +42,6 @@\n from .numerictypes import (longlong, intc, int_, float_, complex_, bool_,\n                            flexible)\n from .overrides import array_function_dispatch, set_module\n-from .._utils import deprecate\n import operator\n import warnings\n import contextlib\n@@ -1666,7 +1665,6 @@ def array_str(a, max_line_width=None, precision=None, suppress_small=None):\n                                         array2string=_array2string_impl)\n \n \n-@deprecate\n def set_string_function(f, repr=True):\n     \"\"\"\n     Set a Python function to be used when pretty printing arrays.\n@@ -1718,6 +1716,15 @@ def set_string_function(f, repr=True):\n     'array([0, 1, 2, 3])'\n \n     \"\"\"\n+\n+    # Deprecated in NumPy 2.0, 2023-07-11\n+    warnings.warn(\n+        \"`set_string_function` is deprecated. \"\n+        \"(deprecated in NumPy 2.0)\",\n+        DeprecationWarning,\n+        stacklevel=2\n+    )\n+\n     if f is None:\n         if repr:\n             return multiarray.set_string_function(_default_array_repr, 1)\n",
            "comment_added_diff": {
                "1720": "    # Deprecated in NumPy 2.0, 2023-07-11"
            },
            "comment_deleted_diff": {},
            "comment_modified_diff": {}
        },
        {
            "commit": "a7b045bfc23240d189d2e8c60b04a2ff600ea6a1",
            "timestamp": "2023-07-18T11:50:24+02:00",
            "author": "Sebastian Berg",
            "commit_message": "WIP: Continue with refactoring more...",
            "additions": 27,
            "deletions": 15,
            "change_type": "MODIFY",
            "diff": "@@ -453,12 +453,19 @@ def indirect(x):\n \n     return formatdict\n \n-def _get_format_function(data, **options):\n+def get_formatter(data, *, options=None):\n     \"\"\"\n-    find the right formatting function for the dtype_\n+    Find the right element formatting function for the dtype.\n+\n+    Parameters\n+    ----------\n+        scalar : bool\n+            If True, a scalar formatter is requested.  In that case all\n+            formatting options are ignored and the default formatter is\n+            returned.\n     \"\"\"\n-    dtype_ = data.dtype\n-    dtypeobj = dtype_.type\n+    dtype = data.dtype\n+    dtypeobj = dtype.type\n     formatdict = _get_formatdict(data, **options)\n     if dtypeobj is None:\n         return formatdict[\"numpystr\"]()\n@@ -486,7 +493,7 @@ def _get_format_function(data, **options):\n     elif issubclass(dtypeobj, _nt.object_):\n         return formatdict['object']()\n     elif issubclass(dtypeobj, _nt.void):\n-        if dtype_.names is not None:\n+        if dtype.names is not None:\n             return StructuredVoidFormat.from_data(data, **options)\n         else:\n             return formatdict['void']()\n@@ -526,29 +533,34 @@ def wrapper(self, *args, **kwargs):\n # gracefully handle recursive calls, when object arrays contain themselves\n @_recursive_guard()\n def _array2string(a, options, separator=' ', prefix=\"\"):\n-    # The formatter __init__s in _get_format_function cannot deal with\n+    # The formatter __init__s in get_formatter cannot deal with\n     # subclasses yet, and we also need to avoid recursion issues in\n     # _formatArray with subclasses which return 0d arrays in place of scalars\n     data = asarray(a)\n     if a.shape == ():\n         a = data\n \n-    if a.size > options['threshold']:\n+    threshold = options.pop(\"threshold\")\n+    linewidth = options.pop(\"linewidth\")\n+    edgeitems = options.pop(\"edgeitems\")\n+\n+    if a.size > threshold:\n         summary_insert = \"...\"\n-        data = _leading_trailing(data, options['edgeitems'])\n+        data = _leading_trailing(data, edgeitems)\n     else:\n         summary_insert = \"\"\n \n+    print(data.shape)\n     # find the right formatting function for the array\n-    format_function = _get_format_function(data, **options)\n+    format_function = get_formatter(data, options=options)\n \n     # skip over \"[\"\n     next_line_prefix = \" \"\n     # skip over array(\n     next_line_prefix += \" \"*len(prefix)\n \n-    lst = _formatArray(a, format_function, options['linewidth'],\n-                       next_line_prefix, separator, options['edgeitems'],\n+    lst = _formatArray(a, format_function, linewidth,\n+                       next_line_prefix, separator, edgeitems,\n                        summary_insert, options['legacy'])\n     return lst\n \n@@ -1386,9 +1398,9 @@ def from_data(cls, data, **options):\n         as input. Added to avoid changing the signature of __init__.\n         \"\"\"\n         format_functions = []\n-        for field_name in data.dtype.names:\n-            format_function = _get_format_function(data[field_name], **options)\n-            if data.dtype[field_name].shape != ():\n+        for field_name in dtype.names:\n+            format_function = get_formatter(dtype[field_name], options=options)\n+            if dtype[field_name].shape != ():\n                 format_function = SubArrayFormat(format_function, **options)\n             format_functions.append(format_function)\n         return cls(format_functions)\n@@ -1410,7 +1422,7 @@ def _void_scalar_repr(x):\n     scalartypes.c.src code, and is placed here because it uses the elementwise\n     formatters defined above.\n     \"\"\"\n-    return StructuredVoidFormat.from_data(array(x), **_format_options)(x)\n+    return StructuredVoidFormat.from_dtype(array(x).dtype, **_format_options)(x)\n \n \n _typelessdata = [int_, float_, complex_, bool_]\n",
            "comment_added_diff": {
                "536": "    # The formatter __init__s in get_formatter cannot deal with"
            },
            "comment_deleted_diff": {
                "529": "    # The formatter __init__s in _get_format_function cannot deal with"
            },
            "comment_modified_diff": {
                "536": "    if a.size > options['threshold']:"
            }
        },
        {
            "commit": "145998a892e41deddb9412088538021d0e39634b",
            "timestamp": "2023-07-18T11:50:24+02:00",
            "author": "Sebastian Berg",
            "commit_message": "WIP: Refactor array formatter with a new `get_formatter`",
            "additions": 65,
            "deletions": 14,
            "change_type": "MODIFY",
            "diff": "@@ -46,7 +46,7 @@\n import warnings\n import contextlib\n \n-_format_options = {\n+_default_format_options = {\n     'edgeitems': 3,  # repr N leading and trailing items of each dimension\n     'threshold': 1000,  # total items > triggers array summarization\n     'floatmode': 'maxprec',\n@@ -61,6 +61,9 @@\n     # str/False on the way in/out.\n     'legacy': sys.maxsize}\n \n+_format_options = _default_format_options.copy()\n+\n+\n def _make_options_dict(precision=None, threshold=None, edgeitems=None,\n                        linewidth=None, suppress=None, nanstr=None, infstr=None,\n                        sign=None, formatter=None, floatmode=None, legacy=None):\n@@ -453,20 +456,54 @@ def indirect(x):\n \n     return formatdict\n \n-def get_formatter(data, *, options=None):\n+def get_formatter(*, dtype=None, data=None, fmt=None, options=None):\n     \"\"\"\n-    Find the right element formatting function for the dtype.\n+    Return a format function for a single element or scalar.\n \n     Parameters\n     ----------\n-        scalar : bool\n-            If True, a scalar formatter is requested.  In that case all\n-            formatting options are ignored and the default formatter is\n-            returned.\n+    dtype : dtype\n+        datatype of the values to be formatted, either `data` or `dtype`\n+        must be given.  If both are given, the ``data.dtype`` of data has\n+        to match.\n+    data : ndarray or None\n+        The data to be printed.  This can be an N-D array including all\n+        elements to be printed.  It may also be ``None`` when ``fmt`` is\n+        ``\"s\"`` or ``\"r\"``\n+    fmt : str or None\n+        A formatting string indicating the desired format style.\n+        Using `None` means that array elements are being pretty-printed\n+        and the printoptions should be used.\n+        In this case, data may be an ndarray to be printed.\n+    options: dict\n+        Options dictionary, if given, must be compatible with\n+        `np.get_printoptions()` and values not None replace the default\n+        ones.\n     \"\"\"\n-    dtype = data.dtype\n+    if data is not None and not isinstance(data, np.ndarray):\n+        raise TypeError(\"get_formatter: data must be None or a NumPy array.\")\n+\n+    if dtype is not None and data is not None and data.dtype != dtype:\n+        raise TypeError(\n+            \"get_formatter: data.dtype and dtype must be equivalent\")\n+    if dtype is None:\n+        dtype = data.dtype\n+\n+    if fmt is not None and options is not None:\n+        raise TypeError(\n+                \"get_formatter: either `fmt` or `options` can be given.\")\n+\n+    if fmt is not None:\n+        if fmt != \"r\":\n+            raise TypeError(\n+                    \"get_formatter: only r format is currently supported.\")\n+\n+        options = _default_format_options.copy()\n+        options.update(floatmode=\"unique\")\n+\n     dtypeobj = dtype.type\n     formatdict = _get_formatdict(data, **options)\n+\n     if dtypeobj is None:\n         return formatdict[\"numpystr\"]()\n     elif issubclass(dtypeobj, _nt.bool_):\n@@ -495,6 +532,12 @@ def get_formatter(data, *, options=None):\n     elif issubclass(dtypeobj, _nt.void):\n         if dtype.names is not None:\n             return StructuredVoidFormat.from_data(data, **options)\n+        elif dtype.shape != ():\n+            # This path can only be hit in nested calls when data is not\n+            # given and `arr.dtype` cannot be a subarray dtype:\n+            assert data is None\n+            return SubArrayFormat(\n+                get_formatter(dtype=dtype.base, fmt=fmt, options=options))\n         else:\n             return formatdict['void']()\n     else:\n@@ -550,9 +593,8 @@ def _array2string(a, options, separator=' ', prefix=\"\"):\n     else:\n         summary_insert = \"\"\n \n-    print(data.shape)\n     # find the right formatting function for the array\n-    format_function = get_formatter(data, options=options)\n+    format_function = get_formatter(data=data, options=options)\n \n     # skip over \"[\"\n     next_line_prefix = \" \"\n@@ -1398,13 +1440,22 @@ def from_data(cls, data, **options):\n         as input. Added to avoid changing the signature of __init__.\n         \"\"\"\n         format_functions = []\n-        for field_name in dtype.names:\n-            format_function = get_formatter(dtype[field_name], options=options)\n-            if dtype[field_name].shape != ():\n+        for field_name in data.dtype.names:\n+            format_function = get_formatter(\n+                    data=data[field_name], options=options)\n+            if data.dtype[field_name].shape != ():\n                 format_function = SubArrayFormat(format_function, **options)\n             format_functions.append(format_function)\n         return cls(format_functions)\n \n+    @classmethod\n+    def _from_fmt(cls, dtype, fmt):\n+        format_functions = []\n+        for field_name in dtype.names:\n+            format_function = get_formatter(dtype=dtype[field_name], fmt=fmt)\n+            format_functions.append(format_function)\n+        return cls(format_functions)\n+\n     def __call__(self, x):\n         str_fields = [\n             format_function(field)\n@@ -1422,7 +1473,7 @@ def _void_scalar_repr(x):\n     scalartypes.c.src code, and is placed here because it uses the elementwise\n     formatters defined above.\n     \"\"\"\n-    return StructuredVoidFormat.from_dtype(array(x).dtype, **_format_options)(x)\n+    return StructuredVoidFormat.from_data(array(x), **_format_options)(x)\n \n \n _typelessdata = [int_, float_, complex_, bool_]\n",
            "comment_added_diff": {
                "536": "            # This path can only be hit in nested calls when data is not",
                "537": "            # given and `arr.dtype` cannot be a subarray dtype:"
            },
            "comment_deleted_diff": {},
            "comment_modified_diff": {}
        },
        {
            "commit": "4e16d865c3e1b58a4cf746d56d3f7122d7b6d5ef",
            "timestamp": "2023-07-18T11:50:24+02:00",
            "author": "Sebastian Berg",
            "commit_message": "ENH: Fixup records and void scalar printing",
            "additions": 38,
            "deletions": 19,
            "change_type": "MODIFY",
            "diff": "@@ -408,7 +408,7 @@ def str_format(x):\n     return str(x)\n \n def _get_formatdict(data, *, precision, floatmode, suppress, sign, legacy,\n-                    formatter, **kwargs):\n+                    formatter, quote=False, **kwargs):\n     # note: extra arguments in kwargs are ignored\n \n     # wrapped in lambdas to avoid taking a code path with the wrong type of data\n@@ -418,11 +418,13 @@ def _get_formatdict(data, *, precision, floatmode, suppress, sign, legacy,\n         'float': lambda: FloatingFormat(\n             data, precision, floatmode, suppress, sign, legacy=legacy),\n         'longfloat': lambda: FloatingFormat(\n-            data, precision, floatmode, suppress, sign, legacy=legacy),\n+            data, precision, floatmode, suppress, sign,\n+            legacy=legacy, quote=quote),\n         'complexfloat': lambda: ComplexFloatingFormat(\n             data, precision, floatmode, suppress, sign, legacy=legacy),\n         'longcomplexfloat': lambda: ComplexFloatingFormat(\n-            data, precision, floatmode, suppress, sign, legacy=legacy),\n+            data, precision, floatmode, suppress, sign,\n+            quote=quote, legacy=legacy),\n         'datetime': lambda: DatetimeFormat(data, legacy=legacy),\n         'timedelta': lambda: TimedeltaFormat(data),\n         'object': lambda: _object_format,\n@@ -494,15 +496,20 @@ def get_formatter(*, dtype=None, data=None, fmt=None, options=None):\n                 \"get_formatter: either `fmt` or `options` can be given.\")\n \n     if fmt is not None:\n-        if fmt != \"r\":\n+        if fmt not in \"rs\":\n             raise TypeError(\n-                    \"get_formatter: only r format is currently supported.\")\n-\n-        options = _default_format_options.copy()\n-        options.update(floatmode=\"unique\")\n+                    \"get_formatter: only r and s format is currently \"\n+                    \"supported.\")\n+\n+        _options = _default_format_options.copy()\n+        _options.update(floatmode=\"unique\")\n+        # _get_formatdict currently expects data, so create it.\n+        _data = np.array([], dtype=dtype)\n+        formatdict = _get_formatdict(_data, quote=fmt == \"r\", **_options)\n+    else:\n+        formatdict = _get_formatdict(data, **options)\n \n     dtypeobj = dtype.type\n-    formatdict = _get_formatdict(data, **options)\n \n     if dtypeobj is None:\n         return formatdict[\"numpystr\"]()\n@@ -531,6 +538,8 @@ def get_formatter(*, dtype=None, data=None, fmt=None, options=None):\n         return formatdict['object']()\n     elif issubclass(dtypeobj, _nt.void):\n         if dtype.names is not None:\n+            if fmt is not None:\n+                return StructuredVoidFormat._from_fmt(dtype, fmt)\n             return StructuredVoidFormat.from_data(data, **options)\n         elif dtype.shape != ():\n             # This path can only be hit in nested calls when data is not\n@@ -963,7 +972,7 @@ def _none_or_positive_arg(x, name):\n class FloatingFormat:\n     \"\"\" Formatter for subtypes of np.floating \"\"\"\n     def __init__(self, data, precision, floatmode, suppress_small, sign=False,\n-                 *, legacy=None):\n+                 *, legacy=None, quote=False):\n         # for backcompatibility, accept bools\n         if isinstance(sign, bool):\n             sign = '+' if sign else '-'\n@@ -986,6 +995,7 @@ def __init__(self, data, precision, floatmode, suppress_small, sign=False,\n         self.sign = sign\n         self.exp_format = False\n         self.large_exponent = False\n+        self._quote = quote  # only used for longdouble \"r\" fmt code\n \n         self.fillFormat(data)\n \n@@ -1010,7 +1020,7 @@ def fillFormat(self, data):\n             self.trim = '.'\n             self.exp_size = -1\n             self.unique = True\n-            self.min_digits = None\n+            self.min_digits = 1\n         elif self.exp_format:\n             trim, unique = '.', True\n             if self.floatmode == 'fixed' or self._legacy <= 113:\n@@ -1085,7 +1095,7 @@ def __call__(self, x):\n                 return ' '*(self.pad_left + self.pad_right + 1 - len(ret)) + ret\n \n         if self.exp_format:\n-            return dragon4_scientific(x,\n+            res = dragon4_scientific(x,\n                                       precision=self.precision,\n                                       min_digits=self.min_digits,\n                                       unique=self.unique,\n@@ -1094,7 +1104,7 @@ def __call__(self, x):\n                                       pad_left=self.pad_left,\n                                       exp_digits=self.exp_size)\n         else:\n-            return dragon4_positional(x,\n+            res = dragon4_positional(x,\n                                       precision=self.precision,\n                                       min_digits=self.min_digits,\n                                       unique=self.unique,\n@@ -1103,7 +1113,9 @@ def __call__(self, x):\n                                       sign=self.sign == '+',\n                                       pad_left=self.pad_left,\n                                       pad_right=self.pad_right)\n-\n+        if not self._quote:\n+            return res\n+        return f\"'{res}'\"\n \n @set_module('numpy')\n def format_float_scientific(x, precision=None, unique=True, trim='k',\n@@ -1303,7 +1315,7 @@ def __call__(self, x):\n class ComplexFloatingFormat:\n     \"\"\" Formatter for subtypes of np.complexfloating \"\"\"\n     def __init__(self, x, precision, floatmode, suppress_small,\n-                 sign=False, *, legacy=None):\n+                 sign=False, *, legacy=None, quote=False):\n         # for backcompatibility, accept bools\n         if isinstance(sign, bool):\n             sign = '+' if sign else '-'\n@@ -1322,6 +1334,8 @@ def __init__(self, x, precision, floatmode, suppress_small,\n             sign='+', legacy=legacy\n         )\n \n+        self._quote = quote\n+\n     def __call__(self, x):\n         r = self.real_format(x.real)\n         i = self.imag_format(x.imag)\n@@ -1330,8 +1344,9 @@ def __call__(self, x):\n         sp = len(i.rstrip())\n         i = i[:sp] + 'j' + i[sp:]\n \n-        return r + i\n-\n+        if not self._quote:\n+            return r + i\n+        return f\"'{r}{i}'\"\n \n class _TimelikeFormat:\n     def __init__(self, data):\n@@ -1467,13 +1482,17 @@ def __call__(self, x):\n             return \"({})\".format(\", \".join(str_fields))\n \n \n-def _void_scalar_repr(x):\n+def _void_scalar_repr(x, is_repr=True):\n     \"\"\"\n     Implements the repr for structured-void scalars. It is called from the\n     scalartypes.c.src code, and is placed here because it uses the elementwise\n     formatters defined above.\n     \"\"\"\n-    return StructuredVoidFormat.from_data(array(x), **_format_options)(x)\n+    fmt = \"r\" if is_repr else \"s\"\n+    val_repr = StructuredVoidFormat._from_fmt(x.dtype, fmt)(x)\n+    if not is_repr:\n+        return val_repr\n+    return f\"np.void({val_repr}, dtype={x.dtype!s})\"\n \n \n _typelessdata = [int_, float_, complex_, bool_]\n",
            "comment_added_diff": {
                "506": "        # _get_formatdict currently expects data, so create it.",
                "998": "        self._quote = quote  # only used for longdouble \"r\" fmt code"
            },
            "comment_deleted_diff": {},
            "comment_modified_diff": {}
        },
        {
            "commit": "e7ce9eb387765f48f03849244d7c92c007f06213",
            "timestamp": "2023-07-18T11:50:24+02:00",
            "author": "Sebastian Berg",
            "commit_message": "ENH: Only print type information when helpful for MA fill value",
            "additions": 6,
            "deletions": 2,
            "change_type": "MODIFY",
            "diff": "@@ -1305,8 +1305,12 @@ def __call__(self, x):\n class BoolFormat:\n     def __init__(self, data, **kwargs):\n         # add an extra space so \" True\" and \"False\" have the same length and\n-        # array elements align nicely when printed, except in 0d arrays\n-        self.truestr = ' True' if data.shape != () else 'True'\n+        # array elements align nicely when printed, except in 0d arrays and\n+        # when there is no data (e.g. scalar formatting we pass nothing)\n+        if data.shape == () or data.size == 0:\n+            self.truestr = 'True'\n+        else:\n+            self.truestr = ' True'\n \n     def __call__(self, x):\n         return self.truestr if x else \"False\"\n",
            "comment_added_diff": {
                "1308": "        # array elements align nicely when printed, except in 0d arrays and",
                "1309": "        # when there is no data (e.g. scalar formatting we pass nothing)"
            },
            "comment_deleted_diff": {
                "1308": "        # array elements align nicely when printed, except in 0d arrays"
            },
            "comment_modified_diff": {
                "1308": "        # array elements align nicely when printed, except in 0d arrays",
                "1309": "        self.truestr = ' True' if data.shape != () else 'True'"
            }
        },
        {
            "commit": "6316dd42646a800b2038634f5ab34736ceeb23de",
            "timestamp": "2023-07-18T11:50:24+02:00",
            "author": "Sebastian Berg",
            "commit_message": "Adapt format to not block s and r format codes\n\nNot sure that is really necessary, but I think empty string for\n`s` should be OK in practice (in principle `!s` exists in formatting\nsyntax which calls `str` explicitly...)",
            "additions": 10,
            "deletions": 9,
            "change_type": "MODIFY",
            "diff": "@@ -472,11 +472,12 @@ def get_formatter(*, dtype=None, data=None, fmt=None, options=None):\n         The data to be printed.  This can be an N-D array including all\n         elements to be printed.  It may also be ``None`` when ``fmt`` is\n         ``\"s\"`` or ``\"r\"``\n-    fmt : str or None\n+    fmt : str, repr literal, or None\n         A formatting string indicating the desired format style.\n-        Using `None` means that array elements are being pretty-printed\n-        and the printoptions should be used.\n-        In this case, data may be an ndarray to be printed.\n+        Using `None` means that array elements are being printed as normally\n+        and the printoptions should be used.  Using ``repr`` (the Python\n+        function), asks for a value representation (similar to ``repr`` on\n+        the scalar, but without the need for type information).\n     options: dict\n         Options dictionary, if given, must be compatible with\n         `np.get_printoptions()` and values not None replace the default\n@@ -496,16 +497,16 @@ def get_formatter(*, dtype=None, data=None, fmt=None, options=None):\n                 \"get_formatter: either `fmt` or `options` can be given.\")\n \n     if fmt is not None:\n-        if fmt not in \"rs\":\n+        if fmt is not repr and fmt != \"\":\n             raise TypeError(\n-                    \"get_formatter: only r and s format is currently \"\n+                    \"get_formatter: only `repr` and \"\" format is currently \"\n                     \"supported.\")\n \n         _options = _default_format_options.copy()\n         _options.update(floatmode=\"unique\")\n         # _get_formatdict currently expects data, so create it.\n         _data = np.array([], dtype=dtype)\n-        formatdict = _get_formatdict(_data, quote=fmt == \"r\", **_options)\n+        formatdict = _get_formatdict(_data, quote=fmt == repr, **_options)\n     else:\n         formatdict = _get_formatdict(data, **options)\n \n@@ -995,7 +996,7 @@ def __init__(self, data, precision, floatmode, suppress_small, sign=False,\n         self.sign = sign\n         self.exp_format = False\n         self.large_exponent = False\n-        self._quote = quote  # only used for longdouble \"r\" fmt code\n+        self._quote = quote  # only used for longdouble `repr` fmt code\n \n         self.fillFormat(data)\n \n@@ -1492,7 +1493,7 @@ def _void_scalar_repr(x, is_repr=True):\n     scalartypes.c.src code, and is placed here because it uses the elementwise\n     formatters defined above.\n     \"\"\"\n-    fmt = \"r\" if is_repr else \"s\"\n+    fmt = repr if is_repr else \"\"\n     val_repr = StructuredVoidFormat._from_fmt(x.dtype, fmt)(x)\n     if not is_repr:\n         return val_repr\n",
            "comment_added_diff": {
                "999": "        self._quote = quote  # only used for longdouble `repr` fmt code"
            },
            "comment_deleted_diff": {
                "998": "        self._quote = quote  # only used for longdouble \"r\" fmt code"
            },
            "comment_modified_diff": {}
        },
        {
            "commit": "e8f04923ad166a2e1f1688b6066ac5df8af9b4b4",
            "timestamp": "2023-07-18T11:50:24+02:00",
            "author": "Sebastian Berg",
            "commit_message": "WIP: Fixup arrayprint for scalar values",
            "additions": 14,
            "deletions": 15,
            "change_type": "MODIFY",
            "diff": "@@ -404,11 +404,11 @@ def repr_format(x):\n \n def str_format(x):\n     if isinstance(x, (np.str_, np.bytes_)):\n-        return repr(x.item())\n+        return str(x.item())\n     return str(x)\n \n def _get_formatdict(data, *, precision, floatmode, suppress, sign, legacy,\n-                    formatter, quote=False, **kwargs):\n+                    formatter, repr=False, **kwargs):\n     # note: extra arguments in kwargs are ignored\n \n     # wrapped in lambdas to avoid taking a code path with the wrong type of data\n@@ -416,20 +416,21 @@ def _get_formatdict(data, *, precision, floatmode, suppress, sign, legacy,\n         'bool': lambda: BoolFormat(data),\n         'int': lambda: IntegerFormat(data),\n         'float': lambda: FloatingFormat(\n-            data, precision, floatmode, suppress, sign, legacy=legacy),\n+            data, precision, floatmode, suppress, sign,\n+            legacy=legacy, repr=repr),\n         'longfloat': lambda: FloatingFormat(\n             data, precision, floatmode, suppress, sign,\n-            legacy=legacy, quote=quote),\n+            legacy=legacy, repr=repr),\n         'complexfloat': lambda: ComplexFloatingFormat(\n             data, precision, floatmode, suppress, sign, legacy=legacy),\n         'longcomplexfloat': lambda: ComplexFloatingFormat(\n             data, precision, floatmode, suppress, sign,\n-            quote=quote, legacy=legacy),\n+            legacy=legacy, repr=repr),\n         'datetime': lambda: DatetimeFormat(data, legacy=legacy),\n         'timedelta': lambda: TimedeltaFormat(data),\n         'object': lambda: _object_format,\n         'void': lambda: str_format,\n-        'numpystr': lambda: repr_format}\n+        'numpystr': lambda: repr_format if repr else str_formt}\n \n     # we need to wrap values in `formatter` in a lambda, so that the interface\n     # is the same as the above values.\n@@ -472,12 +473,10 @@ def get_formatter(*, dtype=None, data=None, fmt=None, options=None):\n         The data to be printed.  This can be an N-D array including all\n         elements to be printed.  It may also be ``None`` when ``fmt`` is\n         ``\"s\"`` or ``\"r\"``\n-    fmt : str, repr literal, or None\n+    fmt : str, repr literal, str literal, or None\n         A formatting string indicating the desired format style.\n-        Using `None` means that array elements are being printed as normally\n-        and the printoptions should be used.  Using ``repr`` (the Python\n-        function), asks for a value representation (similar to ``repr`` on\n-        the scalar, but without the need for type information).\n+        Using `None` means that the options should be used for formatting\n+        and is identical to the empty string.\n     options: dict\n         Options dictionary, if given, must be compatible with\n         `np.get_printoptions()` and values not None replace the default\n@@ -506,7 +505,7 @@ def get_formatter(*, dtype=None, data=None, fmt=None, options=None):\n         _options.update(floatmode=\"unique\")\n         # _get_formatdict currently expects data, so create it.\n         _data = np.array([], dtype=dtype)\n-        formatdict = _get_formatdict(_data, quote=fmt == repr, **_options)\n+        formatdict = _get_formatdict(_data, repr=fmt == repr, **_options)\n     else:\n         formatdict = _get_formatdict(data, **options)\n \n@@ -973,7 +972,7 @@ def _none_or_positive_arg(x, name):\n class FloatingFormat:\n     \"\"\" Formatter for subtypes of np.floating \"\"\"\n     def __init__(self, data, precision, floatmode, suppress_small, sign=False,\n-                 *, legacy=None, quote=False):\n+                 *, legacy=None, fmt=None):\n         # for backcompatibility, accept bools\n         if isinstance(sign, bool):\n             sign = '+' if sign else '-'\n@@ -996,7 +995,7 @@ def __init__(self, data, precision, floatmode, suppress_small, sign=False,\n         self.sign = sign\n         self.exp_format = False\n         self.large_exponent = False\n-        self._quote = quote  # only used for longdouble `repr` fmt code\n+        self._fmt = fmt\n \n         self.fillFormat(data)\n \n@@ -1493,7 +1492,7 @@ def _void_scalar_repr(x, is_repr=True):\n     scalartypes.c.src code, and is placed here because it uses the elementwise\n     formatters defined above.\n     \"\"\"\n-    fmt = repr if is_repr else \"\"\n+    fmt = repr if is_repr else str\n     val_repr = StructuredVoidFormat._from_fmt(x.dtype, fmt)(x)\n     if not is_repr:\n         return val_repr\n",
            "comment_added_diff": {},
            "comment_deleted_diff": {
                "999": "        self._quote = quote  # only used for longdouble `repr` fmt code"
            },
            "comment_modified_diff": {}
        },
        {
            "commit": "413f1a7d8f2b69f4fe1d536a16f153d4c01c85e2",
            "timestamp": "2023-07-18T11:50:24+02:00",
            "author": "Sebastian Berg",
            "commit_message": "MAINT: Switch to also have str explicitly, fixup MA and forward str/repr to scalar\n\nMost importantly, use the scalar `str()` for formatting str/repr of\n\"array elements\" when a format code is given (for floats).\nThe reason is that floats to print differently currently and falling back is\nsimplest.",
            "additions": 56,
            "deletions": 29,
            "change_type": "MODIFY",
            "diff": "@@ -408,7 +408,7 @@ def str_format(x):\n     return str(x)\n \n def _get_formatdict(data, *, precision, floatmode, suppress, sign, legacy,\n-                    formatter, repr=False, **kwargs):\n+                    formatter, fmt=None, **kwargs):\n     # note: extra arguments in kwargs are ignored\n \n     # wrapped in lambdas to avoid taking a code path with the wrong type of data\n@@ -417,15 +417,15 @@ def _get_formatdict(data, *, precision, floatmode, suppress, sign, legacy,\n         'int': lambda: IntegerFormat(data),\n         'float': lambda: FloatingFormat(\n             data, precision, floatmode, suppress, sign,\n-            legacy=legacy, repr=repr),\n+            legacy=legacy, fmt=fmt),\n         'longfloat': lambda: FloatingFormat(\n             data, precision, floatmode, suppress, sign,\n-            legacy=legacy, repr=repr),\n+            legacy=legacy, fmt=fmt, longdouble_quoting=True),\n         'complexfloat': lambda: ComplexFloatingFormat(\n             data, precision, floatmode, suppress, sign, legacy=legacy),\n         'longcomplexfloat': lambda: ComplexFloatingFormat(\n             data, precision, floatmode, suppress, sign,\n-            legacy=legacy, repr=repr),\n+            legacy=legacy, fmt=fmt, longdouble_quoting=True),\n         'datetime': lambda: DatetimeFormat(data, legacy=legacy),\n         'timedelta': lambda: TimedeltaFormat(data),\n         'object': lambda: _object_format,\n@@ -495,19 +495,21 @@ def get_formatter(*, dtype=None, data=None, fmt=None, options=None):\n         raise TypeError(\n                 \"get_formatter: either `fmt` or `options` can be given.\")\n \n+    if fmt == \"\":\n+        fmt = None\n+\n     if fmt is not None:\n-        if fmt is not repr and fmt != \"\":\n+        if options is not None:\n+            raise TypeError(\"Use of options is only supported if `fmt=None`\")\n+\n+        options = _default_format_options\n+\n+        if fmt is not repr and fmt is not str:\n             raise TypeError(\n-                    \"get_formatter: only `repr` and \"\" format is currently \"\n-                    \"supported.\")\n-\n-        _options = _default_format_options.copy()\n-        _options.update(floatmode=\"unique\")\n-        # _get_formatdict currently expects data, so create it.\n-        _data = np.array([], dtype=dtype)\n-        formatdict = _get_formatdict(_data, repr=fmt == repr, **_options)\n-    else:\n-        formatdict = _get_formatdict(data, **options)\n+                    \"get_formatter(): only `repr`, `str`, and None (or '') \"\n+                    \"is currently supported for `fmt`.\")\n+\n+    formatdict = _get_formatdict(data, fmt=fmt, **options)\n \n     dtypeobj = dtype.type\n \n@@ -972,7 +974,7 @@ def _none_or_positive_arg(x, name):\n class FloatingFormat:\n     \"\"\" Formatter for subtypes of np.floating \"\"\"\n     def __init__(self, data, precision, floatmode, suppress_small, sign=False,\n-                 *, legacy=None, fmt=None):\n+                 *, legacy=None, fmt=None, longdouble_quoting=False):\n         # for backcompatibility, accept bools\n         if isinstance(sign, bool):\n             sign = '+' if sign else '-'\n@@ -995,11 +997,27 @@ def __init__(self, data, precision, floatmode, suppress_small, sign=False,\n         self.sign = sign\n         self.exp_format = False\n         self.large_exponent = False\n-        self._fmt = fmt\n \n-        self.fillFormat(data)\n \n-    def fillFormat(self, data):\n+        if fmt is repr and longdouble_quoting:\n+            self._fmt = repr  # use longdouble quoting\n+        elif fmt is str or fmt is repr:\n+            # string and repr are identical, simply use str:\n+            self._fmt = str\n+        elif fmt is None:\n+            self._fmt = None\n+        else:\n+            raise ValueError(\"Only `str` and `repr` format implemented.\")\n+\n+        if self._fmt is not None:\n+            # We should allow format, in which case we probably still want\n+            # to left pad when data is given, right now data must be empty.\n+            if data is not None:\n+                raise NotImplementedError(\"no data supported for float `fmt`.\")\n+        else:\n+            self.fillFormat(data)\n+\n+    def fillFormat(self, data):            \n         # only the finite values are used to compute the number of digits\n         finite_vals = data[isfinite(data)]\n \n@@ -1084,6 +1102,12 @@ def fillFormat(self, data):\n             self.pad_left = max(self.pad_left, nanlen - offset, inflen - offset)\n \n     def __call__(self, x):\n+        if self._fmt is repr:\n+            # use `str()` for the value, but add quotes with longdouble:\n+            return f\"'{x[()]!s}'\"\n+        elif self._fmt is str:\n+            return str(x[()])\n+\n         if not np.isfinite(x):\n             with errstate(invalid='ignore'):\n                 if np.isnan(x):\n@@ -1113,9 +1137,7 @@ def __call__(self, x):\n                                       sign=self.sign == '+',\n                                       pad_left=self.pad_left,\n                                       pad_right=self.pad_right)\n-        if not self._quote:\n-            return res\n-        return f\"'{res}'\"\n+        return res\n \n @set_module('numpy')\n def format_float_scientific(x, precision=None, unique=True, trim='k',\n@@ -1291,7 +1313,7 @@ def format_float_positional(x, precision=None, unique=True,\n \n class IntegerFormat:\n     def __init__(self, data):\n-        if data.size > 0:\n+        if data is not None and data.size > 0:\n             max_str_len = max(len(str(np.max(data))),\n                               len(str(np.min(data))))\n         else:\n@@ -1307,7 +1329,7 @@ def __init__(self, data, **kwargs):\n         # add an extra space so \" True\" and \"False\" have the same length and\n         # array elements align nicely when printed, except in 0d arrays and\n         # when there is no data (e.g. scalar formatting we pass nothing)\n-        if data.shape == () or data.size == 0:\n+        if data is None or data.shape == ():\n             self.truestr = 'True'\n         else:\n             self.truestr = ' True'\n@@ -1319,11 +1341,18 @@ def __call__(self, x):\n class ComplexFloatingFormat:\n     \"\"\" Formatter for subtypes of np.complexfloating \"\"\"\n     def __init__(self, x, precision, floatmode, suppress_small,\n-                 sign=False, *, legacy=None, quote=False):\n+                 sign=False, *, legacy=None,\n+                 fmt=None, longdouble_quoting=False):\n         # for backcompatibility, accept bools\n         if isinstance(sign, bool):\n             sign = '+' if sign else '-'\n \n+        if fmt is repr and longdouble_quoting:\n+            self._quote = True\n+            fmt = str  # always use string for the float parts (no quote)\n+        else:\n+            self._quote = False\n+\n         floatmode_real = floatmode_imag = floatmode\n         if legacy <= 113:\n             floatmode_real = 'maxprec_equal'\n@@ -1331,15 +1360,13 @@ def __init__(self, x, precision, floatmode, suppress_small,\n \n         self.real_format = FloatingFormat(\n             x.real, precision, floatmode_real, suppress_small,\n-            sign=sign, legacy=legacy\n+            sign=sign, legacy=legacy, fmt=fmt, longdouble_quoting=False\n         )\n         self.imag_format = FloatingFormat(\n             x.imag, precision, floatmode_imag, suppress_small,\n-            sign='+', legacy=legacy\n+            sign='+', legacy=legacy, fmt=fmt, longdouble_quoting=False\n         )\n \n-        self._quote = quote\n-\n     def __call__(self, x):\n         r = self.real_format(x.real)\n         i = self.imag_format(x.imag)\n",
            "comment_added_diff": {
                "1003": "            self._fmt = repr  # use longdouble quoting",
                "1005": "            # string and repr are identical, simply use str:",
                "1013": "            # We should allow format, in which case we probably still want",
                "1014": "            # to left pad when data is given, right now data must be empty.",
                "1106": "            # use `str()` for the value, but add quotes with longdouble:",
                "1352": "            fmt = str  # always use string for the float parts (no quote)"
            },
            "comment_deleted_diff": {
                "506": "        # _get_formatdict currently expects data, so create it."
            },
            "comment_modified_diff": {}
        },
        {
            "commit": "1a53d88d7884935a8f892f7b6e365b23fb0cbaaa",
            "timestamp": "2023-07-18T11:50:24+02:00",
            "author": "Sebastian Berg",
            "commit_message": "BUG: Fixups for last changes to make CI pass\n\nI had tested the refguide mainly (the functions part only), which missed\na lot of smaller errors in the last iteration...",
            "additions": 22,
            "deletions": 18,
            "change_type": "MODIFY",
            "diff": "@@ -408,7 +408,7 @@ def str_format(x):\n     return str(x)\n \n def _get_formatdict(data, *, precision, floatmode, suppress, sign, legacy,\n-                    formatter, fmt=None, **kwargs):\n+                    formatter, fmt=None, dtype=None, **kwargs):\n     # note: extra arguments in kwargs are ignored\n \n     # wrapped in lambdas to avoid taking a code path with the wrong type of data\n@@ -426,7 +426,7 @@ def _get_formatdict(data, *, precision, floatmode, suppress, sign, legacy,\n         'longcomplexfloat': lambda: ComplexFloatingFormat(\n             data, precision, floatmode, suppress, sign,\n             legacy=legacy, fmt=fmt, longdouble_quoting=True),\n-        'datetime': lambda: DatetimeFormat(data, legacy=legacy),\n+        'datetime': lambda: DatetimeFormat(data, legacy=legacy, dtype=dtype),\n         'timedelta': lambda: TimedeltaFormat(data),\n         'object': lambda: _object_format,\n         'void': lambda: str_format,\n@@ -499,9 +499,6 @@ def get_formatter(*, dtype=None, data=None, fmt=None, options=None):\n         fmt = None\n \n     if fmt is not None:\n-        if options is not None:\n-            raise TypeError(\"Use of options is only supported if `fmt=None`\")\n-\n         options = _default_format_options\n \n         if fmt is not repr and fmt is not str:\n@@ -509,7 +506,7 @@ def get_formatter(*, dtype=None, data=None, fmt=None, options=None):\n                     \"get_formatter(): only `repr`, `str`, and None (or '') \"\n                     \"is currently supported for `fmt`.\")\n \n-    formatdict = _get_formatdict(data, fmt=fmt, **options)\n+    formatdict = _get_formatdict(data, dtype=dtype, fmt=fmt, **options)\n \n     dtypeobj = dtype.type\n \n@@ -548,7 +545,8 @@ def get_formatter(*, dtype=None, data=None, fmt=None, options=None):\n             # given and `arr.dtype` cannot be a subarray dtype:\n             assert data is None\n             return SubArrayFormat(\n-                get_formatter(dtype=dtype.base, fmt=fmt, options=options))\n+                get_formatter(dtype=dtype.base, fmt=fmt,\n+                              options=None if fmt is not None else options))\n         else:\n             return formatdict['void']()\n     else:\n@@ -1380,16 +1378,20 @@ def __call__(self, x):\n \n class _TimelikeFormat:\n     def __init__(self, data):\n-        non_nat = data[~isnat(data)]\n-        if len(non_nat) > 0:\n-            # Max str length of non-NaT elements\n-            max_str_len = max(len(self._format_non_nat(np.max(non_nat))),\n-                              len(self._format_non_nat(np.min(non_nat))))\n+        if data is not None:\n+            non_nat = data[~isnat(data)]\n+            if len(non_nat) > 0:\n+                # Max str length of non-NaT elements\n+                max_str_len = max(len(self._format_non_nat(np.max(non_nat))),\n+                                  len(self._format_non_nat(np.min(non_nat))))\n+            else:\n+                max_str_len = 0\n+            if len(non_nat) < data.size:\n+                # data contains a NaT\n+                max_str_len = max(max_str_len, 5)\n         else:\n             max_str_len = 0\n-        if len(non_nat) < data.size:\n-            # data contains a NaT\n-            max_str_len = max(max_str_len, 5)\n+\n         self._format = '%{}s'.format(max_str_len)\n         self._nat = \"'NaT'\".rjust(max_str_len)\n \n@@ -1406,11 +1408,13 @@ def __call__(self, x):\n \n class DatetimeFormat(_TimelikeFormat):\n     def __init__(self, x, unit=None, timezone=None, casting='same_kind',\n-                 legacy=False):\n+                 legacy=False, *, dtype=None):\n         # Get the unit from the dtype\n         if unit is None:\n-            if x.dtype.kind == 'M':\n-                unit = datetime_data(x.dtype)[0]\n+            if dtype is None:\n+                dtype = x.dtype\n+            if dtype.kind == 'M':\n+                unit = datetime_data(dtype)[0]\n             else:\n                 unit = 's'\n \n",
            "comment_added_diff": {
                "1384": "                # Max str length of non-NaT elements",
                "1390": "                # data contains a NaT"
            },
            "comment_deleted_diff": {
                "1385": "            # Max str length of non-NaT elements",
                "1391": "            # data contains a NaT"
            },
            "comment_modified_diff": {
                "1384": "        if len(non_nat) > 0:",
                "1390": "        if len(non_nat) < data.size:"
            }
        },
        {
            "commit": "90a8b616b43b656247c81841753896825b47e6dc",
            "timestamp": "2023-07-18T11:50:24+02:00",
            "author": "Sebastian Berg",
            "commit_message": "MAINT: Allow all options again (because subarrays need them) and fix test\n\nIf we use a format string, just ignore all other options and grab\nthe global ones.  I am not 100% sure if there might be a weird path\nwhere things get lost on the way for structured arrays or so,\nthis change is new, I so I think its fine eithe  way.\n\n(and I suspect it is actually correct)",
            "additions": 11,
            "deletions": 3,
            "change_type": "MODIFY",
            "diff": "@@ -592,9 +592,11 @@ def _array2string(a, options, separator=' ', prefix=\"\"):\n     if a.shape == ():\n         a = data\n \n-    threshold = options.pop(\"threshold\")\n-    linewidth = options.pop(\"linewidth\")\n-    edgeitems = options.pop(\"edgeitems\")\n+    # These options should not really be used by the item formatter, but\n+    # the subarray one does use some of them:\n+    threshold = options[\"threshold\"]\n+    linewidth = options[\"linewidth\"]\n+    edgeitems = options[\"edgeitems\"]\n \n     if a.size > threshold:\n         summary_insert = \"...\"\n@@ -1448,6 +1450,12 @@ def _format_non_nat(self, x):\n class SubArrayFormat:\n     def __init__(self, format_function, **options):\n         self.format_function = format_function\n+\n+        # The subarray formatter is special and needs the threshold/edgeitems\n+        # always (even when a format is used).\n+        if not options:\n+            options = _format_options\n+\n         self.threshold = options['threshold']\n         self.edge_items = options['edgeitems']\n \n",
            "comment_added_diff": {
                "595": "    # These options should not really be used by the item formatter, but",
                "596": "    # the subarray one does use some of them:",
                "1454": "        # The subarray formatter is special and needs the threshold/edgeitems",
                "1455": "        # always (even when a format is used)."
            },
            "comment_deleted_diff": {},
            "comment_modified_diff": {
                "595": "    threshold = options.pop(\"threshold\")",
                "596": "    linewidth = options.pop(\"linewidth\")"
            }
        },
        {
            "commit": "01a1d29e156a111694659137a1ac05d54bf44774",
            "timestamp": "2023-07-18T11:50:24+02:00",
            "author": "Marten van Kerkwijk",
            "commit_message": "Simplify, removing new fmt option",
            "additions": 33,
            "deletions": 79,
            "change_type": "MODIFY",
            "diff": "@@ -412,7 +412,7 @@ def str_format(x):\n     return str(x)\n \n def _get_formatdict(data, *, precision, floatmode, suppress, sign, legacy,\n-                    formatter, fmt=None, dtype=None, **kwargs):\n+                    formatter, dtype=None, **kwargs):\n     # note: extra arguments in kwargs are ignored\n \n     # wrapped in lambdas to avoid taking a code path with the wrong type of data\n@@ -421,20 +421,20 @@ def _get_formatdict(data, *, precision, floatmode, suppress, sign, legacy,\n         'int': lambda: IntegerFormat(data),\n         'float': lambda: FloatingFormat(\n             data, precision, floatmode, suppress, sign,\n-            legacy=legacy, fmt=fmt),\n+            legacy=legacy),\n         'longfloat': lambda: FloatingFormat(\n             data, precision, floatmode, suppress, sign,\n-            legacy=legacy, fmt=fmt, longdouble_quoting=True),\n+            legacy=legacy, longdouble_quoting=True),\n         'complexfloat': lambda: ComplexFloatingFormat(\n             data, precision, floatmode, suppress, sign, legacy=legacy),\n         'longcomplexfloat': lambda: ComplexFloatingFormat(\n             data, precision, floatmode, suppress, sign,\n-            legacy=legacy, fmt=fmt, longdouble_quoting=True),\n+            legacy=legacy, longdouble_quoting=True),\n         'datetime': lambda: DatetimeFormat(data, legacy=legacy, dtype=dtype),\n         'timedelta': lambda: TimedeltaFormat(data),\n         'object': lambda: _object_format,\n         'void': lambda: str_format,\n-        'numpystr': lambda: repr_format if repr else str_formt}\n+        'numpystr': lambda: repr_format if repr else str_format}\n \n     # we need to wrap values in `formatter` in a lambda, so that the interface\n     # is the same as the above values.\n@@ -463,7 +463,7 @@ def indirect(x):\n \n     return formatdict\n \n-def get_formatter(*, dtype=None, data=None, fmt=None, options=None):\n+def get_formatter(*, dtype=None, data=None, options=None):\n     \"\"\"\n     Return a format function for a single element or scalar.\n \n@@ -475,12 +475,7 @@ def get_formatter(*, dtype=None, data=None, fmt=None, options=None):\n         to match.\n     data : ndarray or None\n         The data to be printed.  This can be an N-D array including all\n-        elements to be printed.  It may also be ``None`` when ``fmt`` is\n-        ``\"s\"`` or ``\"r\"``\n-    fmt : str, repr literal, str literal, or None\n-        A formatting string indicating the desired format style.\n-        Using `None` means that the options should be used for formatting\n-        and is identical to the empty string.\n+        elements to be printed.\n     options: dict\n         Options dictionary, if given, must be compatible with\n         `np.get_printoptions()` and values not None replace the default\n@@ -495,22 +490,7 @@ def get_formatter(*, dtype=None, data=None, fmt=None, options=None):\n     if dtype is None:\n         dtype = data.dtype\n \n-    if fmt is not None and options is not None:\n-        raise TypeError(\n-                \"get_formatter: either `fmt` or `options` can be given.\")\n-\n-    if fmt == \"\":\n-        fmt = None\n-\n-    if fmt is not None:\n-        options = _default_format_options\n-\n-        if fmt is not repr and fmt is not str:\n-            raise TypeError(\n-                    \"get_formatter(): only `repr`, `str`, and None (or '') \"\n-                    \"is currently supported for `fmt`.\")\n-\n-    formatdict = _get_formatdict(data, dtype=dtype, fmt=fmt, **options)\n+    formatdict = _get_formatdict(data, dtype=dtype, **options)\n \n     dtypeobj = dtype.type\n \n@@ -541,16 +521,15 @@ def get_formatter(*, dtype=None, data=None, fmt=None, options=None):\n         return formatdict['object']()\n     elif issubclass(dtypeobj, _nt.void):\n         if dtype.names is not None:\n-            if fmt is not None:\n-                return StructuredVoidFormat._from_fmt(dtype, fmt)\n+            if data is None:\n+                data = np.zeros((), dtype)\n             return StructuredVoidFormat.from_data(data, **options)\n         elif dtype.shape != ():\n             # This path can only be hit in nested calls when data is not\n             # given and `arr.dtype` cannot be a subarray dtype:\n             assert data is None\n             return SubArrayFormat(\n-                get_formatter(dtype=dtype.base, fmt=fmt,\n-                              options=None if fmt is not None else options))\n+                get_formatter(dtype=dtype.base, options=options))\n         else:\n             return formatdict['void']()\n     else:\n@@ -978,7 +957,7 @@ def _none_or_positive_arg(x, name):\n class FloatingFormat:\n     \"\"\" Formatter for subtypes of np.floating \"\"\"\n     def __init__(self, data, precision, floatmode, suppress_small, sign=False,\n-                 *, legacy=None, fmt=None, longdouble_quoting=False):\n+                 *, legacy=None, longdouble_quoting=False):\n         # for backcompatibility, accept bools\n         if isinstance(sign, bool):\n             sign = '+' if sign else '-'\n@@ -1001,24 +980,11 @@ def __init__(self, data, precision, floatmode, suppress_small, sign=False,\n         self.sign = sign\n         self.exp_format = False\n         self.large_exponent = False\n+        self.longdouble_quoting = (longdouble_quoting\n+                                   and legacy > 125\n+                                   and data.dtype == np.longdouble)\n \n-        if fmt is repr and longdouble_quoting:\n-            self._fmt = repr  # use longdouble quoting\n-        elif fmt is str or fmt is repr:\n-            # string and repr are identical, simply use str:\n-            self._fmt = str\n-        elif fmt is None:\n-            self._fmt = None\n-        else:\n-            raise ValueError(\"Only `str` and `repr` format implemented.\")\n-\n-        if self._fmt is not None:\n-            # We should allow format, in which case we probably still want\n-            # to left pad when data is given, right now data must be empty.\n-            if data is not None:\n-                raise NotImplementedError(\"no data supported for float `fmt`.\")\n-        else:\n-            self.fillFormat(data)\n+        self.fillFormat(data)\n \n     def fillFormat(self, data):            \n         # only the finite values are used to compute the number of digits\n@@ -1105,12 +1071,6 @@ def fillFormat(self, data):\n             self.pad_left = max(self.pad_left, nanlen - offset, inflen - offset)\n \n     def __call__(self, x):\n-        if self._fmt is repr:\n-            # use `str()` for the value, but add quotes with longdouble:\n-            return f\"'{x[()]!s}'\"\n-        elif self._fmt is str:\n-            return str(x[()])\n-\n         if not np.isfinite(x):\n             with errstate(invalid='ignore'):\n                 if np.isnan(x):\n@@ -1140,7 +1100,7 @@ def __call__(self, x):\n                                       sign=self.sign == '+',\n                                       pad_left=self.pad_left,\n                                       pad_right=self.pad_right)\n-        return res\n+        return f\"'{res}'\" if self.longdouble_quoting else res \n \n @set_module('numpy')\n def format_float_scientific(x, precision=None, unique=True, trim='k',\n@@ -1345,16 +1305,14 @@ class ComplexFloatingFormat:\n     \"\"\" Formatter for subtypes of np.complexfloating \"\"\"\n     def __init__(self, x, precision, floatmode, suppress_small,\n                  sign=False, *, legacy=None,\n-                 fmt=None, longdouble_quoting=False):\n+                 longdouble_quoting=False):\n         # for backcompatibility, accept bools\n         if isinstance(sign, bool):\n             sign = '+' if sign else '-'\n \n-        if fmt is repr and longdouble_quoting:\n-            self._quote = True\n-            fmt = str  # always use string for the float parts (no quote)\n-        else:\n-            self._quote = False\n+        self.longdouble_quoting = (longdouble_quoting\n+                                   and legacy > 125\n+                                   and x.dtype == np.longdouble)\n \n         floatmode_real = floatmode_imag = floatmode\n         if legacy <= 113:\n@@ -1363,11 +1321,11 @@ def __init__(self, x, precision, floatmode, suppress_small,\n \n         self.real_format = FloatingFormat(\n             x.real, precision, floatmode_real, suppress_small,\n-            sign=sign, legacy=legacy, fmt=fmt, longdouble_quoting=False\n+            sign=sign, legacy=legacy, longdouble_quoting=False\n         )\n         self.imag_format = FloatingFormat(\n             x.imag, precision, floatmode_imag, suppress_small,\n-            sign='+', legacy=legacy, fmt=fmt, longdouble_quoting=False\n+            sign='+', legacy=legacy, longdouble_quoting=False\n         )\n \n     def __call__(self, x):\n@@ -1378,9 +1336,7 @@ def __call__(self, x):\n         sp = len(i.rstrip())\n         i = i[:sp] + 'j' + i[sp:]\n \n-        if not self._quote:\n-            return r + i\n-        return f\"'{r}{i}'\"\n+        return f\"'{r}{i}'\" if self.longdouble_quoting else r + i\n \n class _TimelikeFormat:\n     def __init__(self, data):\n@@ -1509,14 +1465,6 @@ def from_data(cls, data, **options):\n             format_functions.append(format_function)\n         return cls(format_functions)\n \n-    @classmethod\n-    def _from_fmt(cls, dtype, fmt):\n-        format_functions = []\n-        for field_name in dtype.names:\n-            format_function = get_formatter(dtype=dtype[field_name], fmt=fmt)\n-            format_functions.append(format_function)\n-        return cls(format_functions)\n-\n     def __call__(self, x):\n         str_fields = [\n             format_function(field)\n@@ -1534,11 +1482,17 @@ def _void_scalar_repr(x, is_repr=True):\n     scalartypes.c.src code, and is placed here because it uses the elementwise\n     formatters defined above.\n     \"\"\"\n-    fmt = repr if is_repr else str\n-    val_repr = StructuredVoidFormat._from_fmt(x.dtype, fmt)(x)\n+    options = _format_options.copy()\n+    if options.get('formatter') is None:\n+        options['formatter'] = {}\n+    options['formatter'].setdefault('float_kind', str)\n+    val_repr = StructuredVoidFormat.from_data(array(x), **options)(x)\n     if not is_repr:\n         return val_repr\n-    return f\"np.void({val_repr}, dtype={x.dtype!s})\"\n+    cls = type(x)\n+    cls_fqn = cls.__module__.replace(\"numpy\", \"np\") + \".\" + cls.__name__\n+    void_dtype = np.dtype((np.void, x.dtype))\n+    return f\"{cls_fqn}({val_repr}, dtype={void_dtype!s})\"\n \n \n _typelessdata = [int_, float_, complex_, bool_]\n",
            "comment_added_diff": {},
            "comment_deleted_diff": {
                "1006": "            self._fmt = repr  # use longdouble quoting",
                "1008": "            # string and repr are identical, simply use str:",
                "1016": "            # We should allow format, in which case we probably still want",
                "1017": "            # to left pad when data is given, right now data must be empty.",
                "1109": "            # use `str()` for the value, but add quotes with longdouble:",
                "1355": "            fmt = str  # always use string for the float parts (no quote)"
            },
            "comment_modified_diff": {}
        },
        {
            "commit": "d40922d2e61054b885532ca503fb24c086ce4564",
            "timestamp": "2023-07-18T11:50:24+02:00",
            "author": "Marten van Kerkwijk",
            "commit_message": "Minimize changes relative to main",
            "additions": 57,
            "deletions": 116,
            "change_type": "MODIFY",
            "diff": "@@ -46,7 +46,7 @@\n import warnings\n import contextlib\n \n-_default_format_options = {\n+_format_options = {\n     'edgeitems': 3,  # repr N leading and trailing items of each dimension\n     'threshold': 1000,  # total items > triggers array summarization\n     'floatmode': 'maxprec',\n@@ -61,9 +61,6 @@\n     # str/False on the way in/out.\n     'legacy': sys.maxsize}\n \n-_format_options = _default_format_options.copy()\n-\n-\n def _make_options_dict(precision=None, threshold=None, edgeitems=None,\n                        linewidth=None, suppress=None, nanstr=None, infstr=None,\n                        sign=None, formatter=None, floatmode=None, legacy=None):\n@@ -402,17 +399,17 @@ def _object_format(o):\n     return fmt.format(o)\n \n def repr_format(x):\n-    if isinstance(x, (np.str_, np.bytes_)):\n+    if isinstance(x, (np.str_, np.bytes_)) and _format_options['legacy'] > 125:\n         return repr(x.item())\n     return repr(x)\n \n def str_format(x):\n-    if isinstance(x, (np.str_, np.bytes_)):\n+    if isinstance(x, (np.str_, np.bytes_)) and _format_options['legacy'] > 125:\n         return str(x.item())\n     return str(x)\n \n def _get_formatdict(data, *, precision, floatmode, suppress, sign, legacy,\n-                    formatter, dtype=None, **kwargs):\n+                    formatter, **kwargs):\n     # note: extra arguments in kwargs are ignored\n \n     # wrapped in lambdas to avoid taking a code path with the wrong type of data\n@@ -420,21 +417,20 @@ def _get_formatdict(data, *, precision, floatmode, suppress, sign, legacy,\n         'bool': lambda: BoolFormat(data),\n         'int': lambda: IntegerFormat(data),\n         'float': lambda: FloatingFormat(\n-            data, precision, floatmode, suppress, sign,\n-            legacy=legacy),\n+            data, precision, floatmode, suppress, sign, legacy=legacy),\n         'longfloat': lambda: FloatingFormat(\n-            data, precision, floatmode, suppress, sign,\n-            legacy=legacy, longdouble_quoting=True),\n+            data, precision, floatmode, suppress, sign, legacy=legacy,\n+            longdouble_quoting=True),\n         'complexfloat': lambda: ComplexFloatingFormat(\n             data, precision, floatmode, suppress, sign, legacy=legacy),\n         'longcomplexfloat': lambda: ComplexFloatingFormat(\n-            data, precision, floatmode, suppress, sign,\n-            legacy=legacy, longdouble_quoting=True),\n-        'datetime': lambda: DatetimeFormat(data, legacy=legacy, dtype=dtype),\n+            data, precision, floatmode, suppress, sign, legacy=legacy,\n+            longdouble_quoting=True),\n+        'datetime': lambda: DatetimeFormat(data, legacy=legacy),\n         'timedelta': lambda: TimedeltaFormat(data),\n         'object': lambda: _object_format,\n         'void': lambda: str_format,\n-        'numpystr': lambda: repr_format if repr else str_format}\n+        'numpystr': lambda: repr_format}\n \n     # we need to wrap values in `formatter` in a lambda, so that the interface\n     # is the same as the above values.\n@@ -463,37 +459,13 @@ def indirect(x):\n \n     return formatdict\n \n-def get_formatter(*, dtype=None, data=None, options=None):\n+def _get_format_function(data, **options):\n     \"\"\"\n-    Return a format function for a single element or scalar.\n-\n-    Parameters\n-    ----------\n-    dtype : dtype\n-        datatype of the values to be formatted, either `data` or `dtype`\n-        must be given.  If both are given, the ``data.dtype`` of data has\n-        to match.\n-    data : ndarray or None\n-        The data to be printed.  This can be an N-D array including all\n-        elements to be printed.\n-    options: dict\n-        Options dictionary, if given, must be compatible with\n-        `np.get_printoptions()` and values not None replace the default\n-        ones.\n+    find the right formatting function for the dtype_\n     \"\"\"\n-    if data is not None and not isinstance(data, np.ndarray):\n-        raise TypeError(\"get_formatter: data must be None or a NumPy array.\")\n-\n-    if dtype is not None and data is not None and data.dtype != dtype:\n-        raise TypeError(\n-            \"get_formatter: data.dtype and dtype must be equivalent\")\n-    if dtype is None:\n-        dtype = data.dtype\n-\n-    formatdict = _get_formatdict(data, dtype=dtype, **options)\n-\n-    dtypeobj = dtype.type\n-\n+    dtype_ = data.dtype\n+    dtypeobj = dtype_.type\n+    formatdict = _get_formatdict(data, **options)\n     if dtypeobj is None:\n         return formatdict[\"numpystr\"]()\n     elif issubclass(dtypeobj, _nt.bool_):\n@@ -520,16 +492,8 @@ def get_formatter(*, dtype=None, data=None, options=None):\n     elif issubclass(dtypeobj, _nt.object_):\n         return formatdict['object']()\n     elif issubclass(dtypeobj, _nt.void):\n-        if dtype.names is not None:\n-            if data is None:\n-                data = np.zeros((), dtype)\n+        if dtype_.names is not None:\n             return StructuredVoidFormat.from_data(data, **options)\n-        elif dtype.shape != ():\n-            # This path can only be hit in nested calls when data is not\n-            # given and `arr.dtype` cannot be a subarray dtype:\n-            assert data is None\n-            return SubArrayFormat(\n-                get_formatter(dtype=dtype.base, options=options))\n         else:\n             return formatdict['void']()\n     else:\n@@ -568,35 +532,29 @@ def wrapper(self, *args, **kwargs):\n # gracefully handle recursive calls, when object arrays contain themselves\n @_recursive_guard()\n def _array2string(a, options, separator=' ', prefix=\"\"):\n-    # The formatter __init__s in get_formatter cannot deal with\n+    # The formatter __init__s in _get_format_function cannot deal with\n     # subclasses yet, and we also need to avoid recursion issues in\n     # _formatArray with subclasses which return 0d arrays in place of scalars\n     data = asarray(a)\n     if a.shape == ():\n         a = data\n \n-    # These options should not really be used by the item formatter, but\n-    # the subarray one does use some of them:\n-    threshold = options[\"threshold\"]\n-    linewidth = options[\"linewidth\"]\n-    edgeitems = options[\"edgeitems\"]\n-\n-    if a.size > threshold:\n+    if a.size > options['threshold']:\n         summary_insert = \"...\"\n-        data = _leading_trailing(data, edgeitems)\n+        data = _leading_trailing(data, options['edgeitems'])\n     else:\n         summary_insert = \"\"\n \n     # find the right formatting function for the array\n-    format_function = get_formatter(data=data, options=options)\n+    format_function = _get_format_function(data=data, **options)\n \n     # skip over \"[\"\n     next_line_prefix = \" \"\n     # skip over array(\n     next_line_prefix += \" \"*len(prefix)\n \n-    lst = _formatArray(a, format_function, linewidth,\n-                       next_line_prefix, separator, edgeitems,\n+    lst = _formatArray(a, format_function, options['linewidth'],\n+                       next_line_prefix, separator, options['edgeitems'],\n                        summary_insert, options['legacy'])\n     return lst\n \n@@ -986,7 +944,7 @@ def __init__(self, data, precision, floatmode, suppress_small, sign=False,\n \n         self.fillFormat(data)\n \n-    def fillFormat(self, data):            \n+    def fillFormat(self, data):\n         # only the finite values are used to compute the number of digits\n         finite_vals = data[isfinite(data)]\n \n@@ -1007,7 +965,7 @@ def fillFormat(self, data):\n             self.trim = '.'\n             self.exp_size = -1\n             self.unique = True\n-            self.min_digits = 1\n+            self.min_digits = None\n         elif self.exp_format:\n             trim, unique = '.', True\n             if self.floatmode == 'fixed' or self._legacy <= 113:\n@@ -1083,23 +1041,23 @@ def __call__(self, x):\n \n         if self.exp_format:\n             res = dragon4_scientific(x,\n-                                      precision=self.precision,\n-                                      min_digits=self.min_digits,\n-                                      unique=self.unique,\n-                                      trim=self.trim,\n-                                      sign=self.sign == '+',\n-                                      pad_left=self.pad_left,\n-                                      exp_digits=self.exp_size)\n+                                     precision=self.precision,\n+                                     min_digits=self.min_digits,\n+                                     unique=self.unique,\n+                                     trim=self.trim,\n+                                     sign=self.sign == '+',\n+                                     pad_left=self.pad_left,\n+                                     exp_digits=self.exp_size)\n         else:\n             res = dragon4_positional(x,\n-                                      precision=self.precision,\n-                                      min_digits=self.min_digits,\n-                                      unique=self.unique,\n-                                      fractional=True,\n-                                      trim=self.trim,\n-                                      sign=self.sign == '+',\n-                                      pad_left=self.pad_left,\n-                                      pad_right=self.pad_right)\n+                                     precision=self.precision,\n+                                     min_digits=self.min_digits,\n+                                     unique=self.unique,\n+                                     fractional=True,\n+                                     trim=self.trim,\n+                                     sign=self.sign == '+',\n+                                     pad_left=self.pad_left,\n+                                     pad_right=self.pad_right)\n         return f\"'{res}'\" if self.longdouble_quoting else res \n \n @set_module('numpy')\n@@ -1276,7 +1234,7 @@ def format_float_positional(x, precision=None, unique=True,\n \n class IntegerFormat:\n     def __init__(self, data):\n-        if data is not None and data.size > 0:\n+        if data.size > 0:\n             max_str_len = max(len(str(np.max(data))),\n                               len(str(np.min(data))))\n         else:\n@@ -1290,12 +1248,8 @@ def __call__(self, x):\n class BoolFormat:\n     def __init__(self, data, **kwargs):\n         # add an extra space so \" True\" and \"False\" have the same length and\n-        # array elements align nicely when printed, except in 0d arrays and\n-        # when there is no data (e.g. scalar formatting we pass nothing)\n-        if data is None or data.shape == ():\n-            self.truestr = 'True'\n-        else:\n-            self.truestr = ' True'\n+        # array elements align nicely when printed, except in 0d arrays\n+        self.truestr = ' True' if data.shape != () else 'True'\n \n     def __call__(self, x):\n         return self.truestr if x else \"False\"\n@@ -1321,11 +1275,11 @@ def __init__(self, x, precision, floatmode, suppress_small,\n \n         self.real_format = FloatingFormat(\n             x.real, precision, floatmode_real, suppress_small,\n-            sign=sign, legacy=legacy, longdouble_quoting=False\n+            sign=sign, legacy=legacy\n         )\n         self.imag_format = FloatingFormat(\n             x.imag, precision, floatmode_imag, suppress_small,\n-            sign='+', legacy=legacy, longdouble_quoting=False\n+            sign='+', legacy=legacy\n         )\n \n     def __call__(self, x):\n@@ -1340,20 +1294,16 @@ def __call__(self, x):\n \n class _TimelikeFormat:\n     def __init__(self, data):\n-        if data is not None:\n-            non_nat = data[~isnat(data)]\n-            if len(non_nat) > 0:\n-                # Max str length of non-NaT elements\n-                max_str_len = max(len(self._format_non_nat(np.max(non_nat))),\n-                                  len(self._format_non_nat(np.min(non_nat))))\n-            else:\n-                max_str_len = 0\n-            if len(non_nat) < data.size:\n-                # data contains a NaT\n-                max_str_len = max(max_str_len, 5)\n+        non_nat = data[~isnat(data)]\n+        if len(non_nat) > 0:\n+            # Max str length of non-NaT elements\n+            max_str_len = max(len(self._format_non_nat(np.max(non_nat))),\n+                              len(self._format_non_nat(np.min(non_nat))))\n         else:\n             max_str_len = 0\n-\n+        if len(non_nat) < data.size:\n+            # data contains a NaT\n+            max_str_len = max(max_str_len, 5)\n         self._format = '%{}s'.format(max_str_len)\n         self._nat = \"'NaT'\".rjust(max_str_len)\n \n@@ -1370,13 +1320,11 @@ def __call__(self, x):\n \n class DatetimeFormat(_TimelikeFormat):\n     def __init__(self, x, unit=None, timezone=None, casting='same_kind',\n-                 legacy=False, *, dtype=None):\n+                 legacy=False):\n         # Get the unit from the dtype\n         if unit is None:\n-            if dtype is None:\n-                dtype = x.dtype\n-            if dtype.kind == 'M':\n-                unit = datetime_data(dtype)[0]\n+            if x.dtype.kind == 'M':\n+                unit = datetime_data(x.dtype)[0]\n             else:\n                 unit = 's'\n \n@@ -1410,12 +1358,6 @@ def _format_non_nat(self, x):\n class SubArrayFormat:\n     def __init__(self, format_function, **options):\n         self.format_function = format_function\n-\n-        # The subarray formatter is special and needs the threshold/edgeitems\n-        # always (even when a format is used).\n-        if not options:\n-            options = _format_options\n-\n         self.threshold = options['threshold']\n         self.edge_items = options['edgeitems']\n \n@@ -1458,8 +1400,7 @@ def from_data(cls, data, **options):\n         \"\"\"\n         format_functions = []\n         for field_name in data.dtype.names:\n-            format_function = get_formatter(\n-                    data=data[field_name], options=options)\n+            format_function = _get_format_function(data[field_name], **options)\n             if data.dtype[field_name].shape != ():\n                 format_function = SubArrayFormat(format_function, **options)\n             format_functions.append(format_function)\n",
            "comment_added_diff": {
                "535": "    # The formatter __init__s in _get_format_function cannot deal with",
                "1251": "        # array elements align nicely when printed, except in 0d arrays",
                "1299": "            # Max str length of non-NaT elements",
                "1305": "            # data contains a NaT"
            },
            "comment_deleted_diff": {
                "528": "            # This path can only be hit in nested calls when data is not",
                "529": "            # given and `arr.dtype` cannot be a subarray dtype:",
                "571": "    # The formatter __init__s in get_formatter cannot deal with",
                "578": "    # These options should not really be used by the item formatter, but",
                "579": "    # the subarray one does use some of them:",
                "1293": "        # array elements align nicely when printed, except in 0d arrays and",
                "1294": "        # when there is no data (e.g. scalar formatting we pass nothing)",
                "1346": "                # Max str length of non-NaT elements",
                "1352": "                # data contains a NaT",
                "1414": "        # The subarray formatter is special and needs the threshold/edgeitems",
                "1415": "        # always (even when a format is used)."
            },
            "comment_modified_diff": {}
        }
    ],
    "shape_base.py": [
        {
            "commit": "ff78e59f5a4ff5b4c5fbf58228a6be8dab9480a8",
            "timestamp": "2023-01-17T22:22:42+01:00",
            "author": "Sebastian Berg",
            "commit_message": "DEP: Finalize the non-sequence stacking deprecation\n\nThe `__array_function__` API currently will exhaust iterators so we\ncannot accept sequences reasonably.  Checking for `__getitem__` is presumably\nenough to reject that (and was what the deprecation used).\n\nFuture changes could allow this again, although it is not a useful API\nanyway, since we have to materialize the iterable in any case.",
            "additions": 14,
            "deletions": 18,
            "change_type": "MODIFY",
            "diff": "@@ -204,19 +204,15 @@ def atleast_3d(*arys):\n         return res\n \n \n-def _arrays_for_stack_dispatcher(arrays, stacklevel=4):\n-    if not hasattr(arrays, '__getitem__') and hasattr(arrays, '__iter__'):\n-        warnings.warn('arrays to stack must be passed as a \"sequence\" type '\n-                      'such as list or tuple. Support for non-sequence '\n-                      'iterables such as generators is deprecated as of '\n-                      'NumPy 1.16 and will raise an error in the future.',\n-                      FutureWarning, stacklevel=stacklevel)\n-        return ()\n-    return arrays\n+def _arrays_for_stack_dispatcher(arrays):\n+    if not hasattr(arrays, \"__getitem__\"):\n+        raise TypeError('arrays to stack must be passed as a \"sequence\" type '\n+                        'such as list or tuple.')\n+\n+    return tuple(arrays)\n \n \n-def _vhstack_dispatcher(tup, *, \n-                        dtype=None, casting=None):\n+def _vhstack_dispatcher(tup, *, dtype=None, casting=None):\n     return _arrays_for_stack_dispatcher(tup)\n \n \n@@ -288,8 +284,8 @@ def vstack(tup, *, dtype=None, casting=\"same_kind\"):\n \n     \"\"\"\n     if not overrides.ARRAY_FUNCTION_ENABLED:\n-        # raise warning if necessary\n-        _arrays_for_stack_dispatcher(tup, stacklevel=2)\n+        # reject non-sequences (and make tuple)\n+        tup = _arrays_for_stack_dispatcher(tup)\n     arrs = atleast_2d(*tup)\n     if not isinstance(arrs, list):\n         arrs = [arrs]\n@@ -357,8 +353,8 @@ def hstack(tup, *, dtype=None, casting=\"same_kind\"):\n \n     \"\"\"\n     if not overrides.ARRAY_FUNCTION_ENABLED:\n-        # raise warning if necessary\n-        _arrays_for_stack_dispatcher(tup, stacklevel=2)\n+        # reject non-sequences (and make tuple)\n+        tup = _arrays_for_stack_dispatcher(tup)\n \n     arrs = atleast_1d(*tup)\n     if not isinstance(arrs, list):\n@@ -372,7 +368,7 @@ def hstack(tup, *, dtype=None, casting=\"same_kind\"):\n \n def _stack_dispatcher(arrays, axis=None, out=None, *,\n                       dtype=None, casting=None):\n-    arrays = _arrays_for_stack_dispatcher(arrays, stacklevel=6)\n+    arrays = _arrays_for_stack_dispatcher(arrays)\n     if out is not None:\n         # optimize for the typical case where only arrays is provided\n         arrays = list(arrays)\n@@ -452,8 +448,8 @@ def stack(arrays, axis=0, out=None, *, dtype=None, casting=\"same_kind\"):\n \n     \"\"\"\n     if not overrides.ARRAY_FUNCTION_ENABLED:\n-        # raise warning if necessary\n-        _arrays_for_stack_dispatcher(arrays, stacklevel=2)\n+        # reject non-sequences (and make tuple)\n+        arrays = _arrays_for_stack_dispatcher(arrays)\n \n     arrays = [asanyarray(arr) for arr in arrays]\n     if not arrays:\n",
            "comment_added_diff": {
                "287": "        # reject non-sequences (and make tuple)",
                "356": "        # reject non-sequences (and make tuple)",
                "451": "        # reject non-sequences (and make tuple)"
            },
            "comment_deleted_diff": {
                "291": "        # raise warning if necessary",
                "360": "        # raise warning if necessary",
                "455": "        # raise warning if necessary"
            },
            "comment_modified_diff": {}
        },
        {
            "commit": "ff78e59f5a4ff5b4c5fbf58228a6be8dab9480a8",
            "timestamp": "2023-01-17T22:22:42+01:00",
            "author": "Sebastian Berg",
            "commit_message": "DEP: Finalize the non-sequence stacking deprecation\n\nThe `__array_function__` API currently will exhaust iterators so we\ncannot accept sequences reasonably.  Checking for `__getitem__` is presumably\nenough to reject that (and was what the deprecation used).\n\nFuture changes could allow this again, although it is not a useful API\nanyway, since we have to materialize the iterable in any case.",
            "additions": 4,
            "deletions": 4,
            "change_type": "MODIFY",
            "diff": "@@ -644,8 +644,8 @@ def column_stack(tup):\n \n     \"\"\"\n     if not overrides.ARRAY_FUNCTION_ENABLED:\n-        # raise warning if necessary\n-        _arrays_for_stack_dispatcher(tup, stacklevel=2)\n+        # reject non-sequences (and make tuple)\n+        tup = _arrays_for_stack_dispatcher(tup)\n \n     arrays = []\n     for v in tup:\n@@ -714,8 +714,8 @@ def dstack(tup):\n \n     \"\"\"\n     if not overrides.ARRAY_FUNCTION_ENABLED:\n-        # raise warning if necessary\n-        _arrays_for_stack_dispatcher(tup, stacklevel=2)\n+        # reject non-sequences (and make tuple)\n+        tup = _arrays_for_stack_dispatcher(tup)\n \n     arrs = atleast_3d(*tup)\n     if not isinstance(arrs, list):\n",
            "comment_added_diff": {
                "647": "        # reject non-sequences (and make tuple)",
                "717": "        # reject non-sequences (and make tuple)"
            },
            "comment_deleted_diff": {
                "647": "        # raise warning if necessary",
                "717": "        # raise warning if necessary"
            },
            "comment_modified_diff": {
                "647": "        # raise warning if necessary",
                "717": "        # raise warning if necessary"
            }
        },
        {
            "commit": "1da1663196c95b3811ca84d9e335f32cfeb05e32",
            "timestamp": "2023-03-12T22:31:28+00:00",
            "author": "Ralf Gommers",
            "commit_message": "MAINT: remove `NUMPY_EXPERIMENTAL_ARRAY_FUNCTION` env var\n\nAs discussed in\nhttps://mail.python.org/archives/list/numpy-discussion@python.org/thread/UKZJACAP5FUG7KP2AQDPE4P5ADNWLOHZ/\n\nThis flag was always meant to be temporary, and cleaning it up is\nlong overdue.",
            "additions": 0,
            "deletions": 11,
            "change_type": "MODIFY",
            "diff": "@@ -283,9 +283,6 @@ def vstack(tup, *, dtype=None, casting=\"same_kind\"):\n            [6]])\n \n     \"\"\"\n-    if not overrides.ARRAY_FUNCTION_ENABLED:\n-        # reject non-sequences (and make tuple)\n-        tup = _arrays_for_stack_dispatcher(tup)\n     arrs = atleast_2d(*tup)\n     if not isinstance(arrs, list):\n         arrs = [arrs]\n@@ -352,10 +349,6 @@ def hstack(tup, *, dtype=None, casting=\"same_kind\"):\n            [3, 6]])\n \n     \"\"\"\n-    if not overrides.ARRAY_FUNCTION_ENABLED:\n-        # reject non-sequences (and make tuple)\n-        tup = _arrays_for_stack_dispatcher(tup)\n-\n     arrs = atleast_1d(*tup)\n     if not isinstance(arrs, list):\n         arrs = [arrs]\n@@ -447,10 +440,6 @@ def stack(arrays, axis=0, out=None, *, dtype=None, casting=\"same_kind\"):\n            [3, 6]])\n \n     \"\"\"\n-    if not overrides.ARRAY_FUNCTION_ENABLED:\n-        # reject non-sequences (and make tuple)\n-        arrays = _arrays_for_stack_dispatcher(arrays)\n-\n     arrays = [asanyarray(arr) for arr in arrays]\n     if not arrays:\n         raise ValueError('need at least one array to stack')\n",
            "comment_added_diff": {},
            "comment_deleted_diff": {
                "287": "        # reject non-sequences (and make tuple)",
                "356": "        # reject non-sequences (and make tuple)",
                "451": "        # reject non-sequences (and make tuple)"
            },
            "comment_modified_diff": {}
        },
        {
            "commit": "1da1663196c95b3811ca84d9e335f32cfeb05e32",
            "timestamp": "2023-03-12T22:31:28+00:00",
            "author": "Ralf Gommers",
            "commit_message": "MAINT: remove `NUMPY_EXPERIMENTAL_ARRAY_FUNCTION` env var\n\nAs discussed in\nhttps://mail.python.org/archives/list/numpy-discussion@python.org/thread/UKZJACAP5FUG7KP2AQDPE4P5ADNWLOHZ/\n\nThis flag was always meant to be temporary, and cleaning it up is\nlong overdue.",
            "additions": 0,
            "deletions": 8,
            "change_type": "MODIFY",
            "diff": "@@ -643,10 +643,6 @@ def column_stack(tup):\n            [3, 4]])\n \n     \"\"\"\n-    if not overrides.ARRAY_FUNCTION_ENABLED:\n-        # reject non-sequences (and make tuple)\n-        tup = _arrays_for_stack_dispatcher(tup)\n-\n     arrays = []\n     for v in tup:\n         arr = asanyarray(v)\n@@ -713,10 +709,6 @@ def dstack(tup):\n            [[3, 4]]])\n \n     \"\"\"\n-    if not overrides.ARRAY_FUNCTION_ENABLED:\n-        # reject non-sequences (and make tuple)\n-        tup = _arrays_for_stack_dispatcher(tup)\n-\n     arrs = atleast_3d(*tup)\n     if not isinstance(arrs, list):\n         arrs = [arrs]\n",
            "comment_added_diff": {},
            "comment_deleted_diff": {
                "647": "        # reject non-sequences (and make tuple)",
                "717": "        # reject non-sequences (and make tuple)"
            },
            "comment_modified_diff": {}
        },
        {
            "commit": "017cef318ad37ac05738830ebb3173c65c128759",
            "timestamp": "2023-07-11T22:25:10+02:00",
            "author": "Mateusz Sok\u00f3\u0142",
            "commit_message": "API: apply review comments",
            "additions": 10,
            "deletions": 2,
            "change_type": "MODIFY",
            "diff": "@@ -1,4 +1,5 @@\n import functools\n+import warnings\n \n import numpy.core.numeric as _nx\n from numpy.core.numeric import asarray, zeros, array, asanyarray\n@@ -10,7 +11,6 @@\n from numpy.core.shape_base import _arrays_for_stack_dispatcher\n from numpy.lib.index_tricks import ndindex\n from numpy.matrixlib.defmatrix import matrix  # this raises all the right alarm bells\n-from numpy._utils import deprecate\n \n \n __all__ = [\n@@ -1049,12 +1049,20 @@ def get_array_prepare(*args):\n     return None\n \n \n-@deprecate\n def get_array_wrap(*args):\n     \"\"\"Find the wrapper for the array with the highest priority.\n \n     In case of ties, leftmost wins. If no wrapper is found, return None\n     \"\"\"\n+\n+    # Deprecated in NumPy 2.0, 2023-07-11\n+    warnings.warn(\n+        \"`get_array_wrap` is deprecated. \"\n+        \"(deprecated in NumPy 2.0)\",\n+        DeprecationWarning,\n+        stacklevel=2\n+    )\n+\n     wrappers = sorted((getattr(x, '__array_priority__', 0), -i,\n                  x.__array_wrap__) for i, x in enumerate(args)\n                                    if hasattr(x, '__array_wrap__'))\n",
            "comment_added_diff": {
                "1058": "    # Deprecated in NumPy 2.0, 2023-07-11"
            },
            "comment_deleted_diff": {},
            "comment_modified_diff": {}
        }
    ],
    "arraypad.py": [
        {
            "commit": "2aad439eec34a80aaac0fd61a9b8de809710efa5",
            "timestamp": "2022-12-07T15:29:31+01:00",
            "author": "LU",
            "commit_message": "BUG: fix unexpected return of np.pad with mode=wrap (#22575)\n\nnp.pad with mode=\"wrap\" returns unexpected result that original data is not strictly looped in padding. This may happen in some occassions when padding widths in the same dimension are unbalanced (see added testcase in test_arraypad.py and the related issue). The reason is the function pad makes iterative calls of _set_wrap_both() in the above situation, yet period for padding is not correctly computed in each iteration.\r\n\r\nThe bug is fixed by guaranteeing that period is always a multiple of original data size, and also be the possible maximum for computation efficiency.\r\n\r\nCloses #22464\r\n\r\nCo-authored-by: Lars Gr\u00fcter <lagru+github@mailbox.org>",
            "additions": 18,
            "deletions": 13,
            "change_type": "MODIFY",
            "diff": "@@ -378,7 +378,7 @@ def _set_reflect_both(padded, axis, width_pair, method, include_edge=False):\n     return left_pad, right_pad\n \n \n-def _set_wrap_both(padded, axis, width_pair):\n+def _set_wrap_both(padded, axis, width_pair, original_period):\n     \"\"\"\n     Pad `axis` of `arr` with wrapped values.\n \n@@ -391,6 +391,8 @@ def _set_wrap_both(padded, axis, width_pair):\n     width_pair : (int, int)\n         Pair of widths that mark the pad area on both sides in the given\n         dimension.\n+    original_period : int\n+        Original length of data on `axis` of `arr`.\n \n     Returns\n     -------\n@@ -400,6 +402,9 @@ def _set_wrap_both(padded, axis, width_pair):\n     \"\"\"\n     left_pad, right_pad = width_pair\n     period = padded.shape[axis] - right_pad - left_pad\n+    # Avoid wrapping with only a subset of the original area by ensuring period\n+    # can only be a multiple of the original area's length.\n+    period = period // original_period * original_period\n \n     # If the current dimension of `arr` doesn't contain enough valid values\n     # (not part of the undefined pad area) we need to pad multiple times.\n@@ -410,14 +415,12 @@ def _set_wrap_both(padded, axis, width_pair):\n \n     if left_pad > 0:\n         # Pad with wrapped values on left side\n-        # First slice chunk from right side of the non-pad area.\n+        # First slice chunk from left side of the non-pad area.\n         # Use min(period, left_pad) to ensure that chunk is not larger than\n-        # pad area\n-        right_slice = _slice_at_axis(\n-            slice(-right_pad - min(period, left_pad),\n-                  -right_pad if right_pad != 0 else None),\n-            axis\n-        )\n+        # pad area.\n+        slice_end = left_pad + period\n+        slice_start = slice_end - min(period, left_pad)\n+        right_slice = _slice_at_axis(slice(slice_start, slice_end), axis)\n         right_chunk = padded[right_slice]\n \n         if left_pad > period:\n@@ -431,11 +434,12 @@ def _set_wrap_both(padded, axis, width_pair):\n \n     if right_pad > 0:\n         # Pad with wrapped values on right side\n-        # First slice chunk from left side of the non-pad area.\n+        # First slice chunk from right side of the non-pad area.\n         # Use min(period, right_pad) to ensure that chunk is not larger than\n-        # pad area\n-        left_slice = _slice_at_axis(\n-            slice(left_pad, left_pad + min(period, right_pad),), axis)\n+        # pad area.\n+        slice_start = -right_pad - period\n+        slice_end = slice_start + min(period, right_pad)\n+        left_slice = _slice_at_axis(slice(slice_start, slice_end), axis)\n         left_chunk = padded[left_slice]\n \n         if right_pad > period:\n@@ -867,11 +871,12 @@ def pad(array, pad_width, mode='constant', **kwargs):\n     elif mode == \"wrap\":\n         for axis, (left_index, right_index) in zip(axes, pad_width):\n             roi = _view_roi(padded, original_area_slice, axis)\n+            original_period = padded.shape[axis] - right_index - left_index\n             while left_index > 0 or right_index > 0:\n                 # Iteratively pad until dimension is filled with wrapped\n                 # values. This is necessary if the pad area is larger than\n                 # the length of the original values in the current dimension.\n                 left_index, right_index = _set_wrap_both(\n-                    roi, axis, (left_index, right_index))\n+                    roi, axis, (left_index, right_index), original_period)\n \n     return padded\n",
            "comment_added_diff": {
                "405": "    # Avoid wrapping with only a subset of the original area by ensuring period",
                "406": "    # can only be a multiple of the original area's length.",
                "418": "        # First slice chunk from left side of the non-pad area.",
                "420": "        # pad area.",
                "437": "        # First slice chunk from right side of the non-pad area.",
                "439": "        # pad area."
            },
            "comment_deleted_diff": {
                "413": "        # First slice chunk from right side of the non-pad area.",
                "415": "        # pad area",
                "434": "        # First slice chunk from left side of the non-pad area.",
                "436": "        # pad area"
            },
            "comment_modified_diff": {
                "418": "                  -right_pad if right_pad != 0 else None),",
                "420": "        )",
                "437": "        left_slice = _slice_at_axis("
            }
        }
    ],
    "routines.rst": [],
    "routines.testing.overrides.rst": [],
    "basics.dispatch.rst": [],
    "overrides.py": [
        {
            "commit": "1a8d3ca45f0a7294784bc200ec436dc8563f654a",
            "timestamp": "2022-11-16T09:14:25-07:00",
            "author": "Nathan Goldbaum",
            "commit_message": "API: Add numpy.testing.overrides to aid testing of custom array containers\n\nCloses #15544",
            "additions": 82,
            "deletions": 0,
            "change_type": "ADD",
            "diff": "@@ -0,0 +1,82 @@\n+\"\"\"Tools for testing implementations of __array_function__ and ufunc overrides\n+\n+\n+\"\"\"\n+\n+from numpy.core.overrides import ARRAY_FUNCTIONS as _array_functions\n+from numpy import ufunc as _ufunc\n+import numpy.core.umath as _umath\n+\n+def get_overridable_numpy_ufuncs():\n+    \"\"\"List all numpy ufuncs overridable via `__array_ufunc__`\n+\n+    Parameters\n+    ----------\n+    None\n+\n+    Returns\n+    -------\n+    set\n+        A set containing all overridable ufuncs in the public numpy API.\n+    \"\"\"\n+    ufuncs = {obj for obj in _umath.__dict__.values()\n+              if isinstance(obj, _ufunc)}\n+    \n+\n+def allows_array_ufunc_override(func):\n+    \"\"\"Determine if a function can be overriden via `__array_ufunc__`\n+\n+    Parameters\n+    ----------\n+    func : callable\n+        Function that may be overridable via `__array_ufunc__`\n+\n+    Returns\n+    -------\n+    bool\n+        `True` if `func` is overridable via `__array_ufunc__` and\n+        `False` otherwise.\n+\n+    Note\n+    ----\n+    This function is equivalent to `isinstance(func, np.ufunc)` and\n+    will work correctly for ufuncs defined outside of Numpy.\n+\n+    \"\"\"\n+    return isinstance(func, np.ufunc)\n+\n+\n+def get_overridable_numpy_array_functions():\n+    \"\"\"List all numpy functions overridable via `__array_function__`\n+\n+    Parameters\n+    ----------\n+    None\n+\n+    Returns\n+    -------\n+    set\n+        A set containing all functions in the public numpy API that are\n+        overridable via `__array_function__`.\n+\n+    \"\"\"\n+    # 'import numpy' doesn't import recfunctions, so make sure it's imported\n+    # so ufuncs defined there show up in the ufunc listing\n+    from numpy.lib import recfunctions\n+    return _array_functions.copy()\n+\n+def allows_array_function_override(func):\n+    \"\"\"Determine if a Numpy function can be overriden via `__array_function__`\n+\n+    Parameters\n+    ----------\n+    func : callable\n+        Function that may be overridable via `__array_function__`\n+\n+    Returns\n+    -------\n+    bool\n+        `True` if `func` is a function in the Numpy API that is\n+        overridable via `__array_function__` and `False` otherwise.\n+    \"\"\"\n+    return func in _array_functions\n",
            "comment_added_diff": {
                "63": "    # 'import numpy' doesn't import recfunctions, so make sure it's imported",
                "64": "    # so ufuncs defined there show up in the ufunc listing"
            },
            "comment_deleted_diff": {},
            "comment_modified_diff": {}
        },
        {
            "commit": "60a858a372b14b73547baacf4a472eccfade1073",
            "timestamp": "2023-01-17T18:40:44+01:00",
            "author": "Sebastian Berg",
            "commit_message": "ENH: Improve array function overhead by using vectorcall\n\nThis moves dispatching for `__array_function__` into a C-wrapper.  This\nhelps speed for multiple reasons:\n* Avoids one additional dispatching function call to C\n* Avoids the use of `*args, **kwargs` which is slower.\n* For simple NumPy calls we can stay in the faster \"vectorcall\" world\n\nThis speeds up things generally a little, but can speed things up a lot\nwhen keyword arguments are used on lightweight functions, for example::\n\n    np.can_cast(arr, dtype, casting=\"same_kind\")\n\nis more than twice as fast with this.\n\nThere is one alternative in principle to get best speed:  We could inline\nthe \"relevant argument\"/dispatcher extraction.  That changes behavior in\nan acceptable but larger way (passes default arguments).\nUnless the C-entry point seems unwanted, this should be a decent step\nin the right direction even if we want to do that eventually, though.\n\nCloses gh-20790\nCloses gh-18547  (although not quite sure why)",
            "additions": 37,
            "deletions": 57,
            "change_type": "MODIFY",
            "diff": "@@ -6,7 +6,7 @@\n from .._utils import set_module\n from .._utils._inspect import getargspec\n from numpy.core._multiarray_umath import (\n-    add_docstring, implement_array_function, _get_implementing_args)\n+    add_docstring,  _get_implementing_args, _ArrayFunctionDispatcher)\n \n \n ARRAY_FUNCTIONS = set()\n@@ -33,40 +33,33 @@ def set_array_function_like_doc(public_api):\n \n \n add_docstring(\n-    implement_array_function,\n+    _ArrayFunctionDispatcher,\n     \"\"\"\n-    Implement a function with checks for __array_function__ overrides.\n+    Class to wrap functions with checks for __array_function__ overrides.\n \n     All arguments are required, and can only be passed by position.\n \n     Parameters\n     ----------\n+    dispatcher : function or None\n+        The dispatcher function that returns a single sequence-like object\n+        of all arguments relevant.  It must have the same signature (except\n+        the default values) as the actual implementation.\n+        If ``None``, this is a ``like=`` dispatcher and the\n+        ``_ArrayFunctionDispatcher`` must be called with ``like`` as the\n+        first (additional and positional) argument.\n     implementation : function\n         Function that implements the operation on NumPy array without\n-        overrides when called like ``implementation(*args, **kwargs)``.\n-    public_api : function\n-        Function exposed by NumPy's public API originally called like\n-        ``public_api(*args, **kwargs)`` on which arguments are now being\n-        checked.\n-    relevant_args : iterable\n-        Iterable of arguments to check for __array_function__ methods.\n-    args : tuple\n-        Arbitrary positional arguments originally passed into ``public_api``.\n-    kwargs : dict\n-        Arbitrary keyword arguments originally passed into ``public_api``.\n+        overrides when called like.\n \n-    Returns\n-    -------\n-    Result from calling ``implementation()`` or an ``__array_function__``\n-    method, as appropriate.\n-\n-    Raises\n-    ------\n-    TypeError : if no implementation is found.\n+    Attributes\n+    ----------\n+    _implementation : function\n+        The original implementation passed in.\n     \"\"\")\n \n \n-# exposed for testing purposes; used internally by implement_array_function\n+# exposed for testing purposes; used internally by _ArrayFunctionDispatcher\n add_docstring(\n     _get_implementing_args,\n     \"\"\"\n@@ -110,7 +103,7 @@ def verify_matching_signatures(implementation, dispatcher):\n                                'default argument values')\n \n \n-def array_function_dispatch(dispatcher, module=None, verify=True,\n+def array_function_dispatch(dispatcher=None, module=None, verify=True,\n                             docs_from_dispatcher=False):\n     \"\"\"Decorator for adding dispatch with the __array_function__ protocol.\n \n@@ -118,10 +111,14 @@ def array_function_dispatch(dispatcher, module=None, verify=True,\n \n     Parameters\n     ----------\n-    dispatcher : callable\n+    dispatcher : callable or None\n         Function that when called like ``dispatcher(*args, **kwargs)`` with\n         arguments from the NumPy function call returns an iterable of\n         array-like arguments to check for ``__array_function__``.\n+\n+        If `None`, the first argument is used as the single `like=` argument\n+        and not passed on.  A function implementing `like=` must call its\n+        dispatcher with `like` as the first non-keyword argument.\n     module : str, optional\n         __module__ attribute to set on new function, e.g., ``module='numpy'``.\n         By default, module is copied from the decorated function.\n@@ -154,45 +151,28 @@ def decorator(implementation):\n \n     def decorator(implementation):\n         if verify:\n-            verify_matching_signatures(implementation, dispatcher)\n+            if dispatcher is not None:\n+                verify_matching_signatures(implementation, dispatcher)\n+            else:\n+                # Using __code__ directly similar to verify_matching_signature\n+                co = implementation.__code__\n+                last_arg = co.co_argcount + co.co_kwonlyargcount - 1\n+                last_arg = co.co_varnames[last_arg]\n+                if last_arg != \"like\" or co.co_kwonlyargcount == 0:\n+                    raise RuntimeError(\n+                        \"__array_function__ expects `like=` to be the last \"\n+                        \"argument and a keyword-only argument. \"\n+                        f\"{implementation} does not seem to comply.\")\n \n         if docs_from_dispatcher:\n             add_docstring(implementation, dispatcher.__doc__)\n \n-        @functools.wraps(implementation)\n-        def public_api(*args, **kwargs):\n-            try:\n-                relevant_args = dispatcher(*args, **kwargs)\n-            except TypeError as exc:\n-                # Try to clean up a signature related TypeError.  Such an\n-                # error will be something like:\n-                #     dispatcher.__name__() got an unexpected keyword argument\n-                #\n-                # So replace the dispatcher name in this case.  In principle\n-                # TypeErrors may be raised from _within_ the dispatcher, so\n-                # we check that the traceback contains a string that starts\n-                # with the name.  (In principle we could also check the\n-                # traceback length, as it would be deeper.)\n-                msg = exc.args[0]\n-                disp_name = dispatcher.__name__\n-                if not isinstance(msg, str) or not msg.startswith(disp_name):\n-                    raise\n-\n-                # Replace with the correct name and re-raise:\n-                new_msg = msg.replace(disp_name, public_api.__name__)\n-                raise TypeError(new_msg) from None\n-\n-            return implement_array_function(\n-                implementation, public_api, relevant_args, args, kwargs)\n-\n-        public_api.__code__ = public_api.__code__.replace(\n-                co_name=implementation.__name__,\n-                co_filename='<__array_function__ internals>')\n+        public_api = _ArrayFunctionDispatcher(dispatcher, implementation)\n+        public_api = functools.wraps(implementation)(public_api)\n+\n         if module is not None:\n             public_api.__module__ = module\n \n-        public_api._implementation = implementation\n-\n         ARRAY_FUNCTIONS.add(public_api)\n \n         return public_api\n",
            "comment_added_diff": {
                "62": "# exposed for testing purposes; used internally by _ArrayFunctionDispatcher",
                "157": "                # Using __code__ directly similar to verify_matching_signature"
            },
            "comment_deleted_diff": {
                "69": "# exposed for testing purposes; used internally by implement_array_function",
                "167": "                # Try to clean up a signature related TypeError.  Such an",
                "168": "                # error will be something like:",
                "169": "                #     dispatcher.__name__() got an unexpected keyword argument",
                "170": "                #",
                "171": "                # So replace the dispatcher name in this case.  In principle",
                "172": "                # TypeErrors may be raised from _within_ the dispatcher, so",
                "173": "                # we check that the traceback contains a string that starts",
                "174": "                # with the name.  (In principle we could also check the",
                "175": "                # traceback length, as it would be deeper.)",
                "181": "                # Replace with the correct name and re-raise:"
            },
            "comment_modified_diff": {
                "62": "",
                "157": "            verify_matching_signatures(implementation, dispatcher)"
            }
        }
    ],
    "_type_aliases.py": [
        {
            "commit": "075859216fae0509c52a54cb5c96c217f23026ca",
            "timestamp": "2022-11-17T14:21:54+01:00",
            "author": "Sebastian Berg",
            "commit_message": "DEP: Next step in scalar type alias deprecations/futurewarnings\n\nFinalizes the scalar type alias deprecations making them an error.\nHowever, at the same time adds a `FutureWarning` that new aliases\nwill be introduced in the future.\n(They would eventually be preferred over the `str_`, etc. version.)\n\nIt may make sense, that this FutureWarning is already propelled soon\nsince it interacts with things such as changing the representation of\nstrings to `np.str_(\"\")` if the preferred alias becomes `np.str`.\n\nIt also introduces a new deprecation to remove the 0 sized bit-aliases\nand the bitsize `bool8` alias.  (Unfortunately, these are here still allowed\nas part of the `np.sctypeDict`).",
            "additions": 6,
            "deletions": 6,
            "change_type": "MODIFY",
            "diff": "@@ -107,14 +107,16 @@ def _add_aliases():\n         if name in ('longdouble', 'clongdouble') and myname in allTypes:\n             continue\n \n-        allTypes[myname] = info.type\n-\n-        # add mapping for both the bit name and the numarray name\n-        sctypeDict[myname] = info.type\n+        # Add to the main namespace if desired:\n+        if bit != 0 and base != \"bool\":\n+            allTypes[myname] = info.type\n \n         # add forward, reverse, and string mapping to numarray\n         sctypeDict[char] = info.type\n \n+        # add mapping for both the bit name\n+        sctypeDict[myname] = info.type\n+\n \n _add_aliases()\n \n@@ -148,8 +150,6 @@ def _add_integer_aliases():\n #\n def _set_up_aliases():\n     type_pairs = [('complex_', 'cdouble'),\n-                  ('int0', 'intp'),\n-                  ('uint0', 'uintp'),\n                   ('single', 'float'),\n                   ('csingle', 'cfloat'),\n                   ('singlecomplex', 'cfloat'),\n",
            "comment_added_diff": {
                "110": "        # Add to the main namespace if desired:",
                "117": "        # add mapping for both the bit name"
            },
            "comment_deleted_diff": {
                "112": "        # add mapping for both the bit name and the numarray name"
            },
            "comment_modified_diff": {
                "110": "        allTypes[myname] = info.type"
            }
        },
        {
            "commit": "241d67678189872a25ebc0e56a07ad4254bcef9b",
            "timestamp": "2023-09-29T15:47:11+02:00",
            "author": "Mateusz Sok\u00f3\u0142",
            "commit_message": "API: Remove zero names from dtype aliases (#24807)",
            "additions": 4,
            "deletions": 8,
            "change_type": "MODIFY",
            "diff": "@@ -106,17 +106,15 @@ def _add_aliases():\n         if name in ('longdouble', 'clongdouble') and myname in allTypes:\n             continue\n \n-        # Add to the main namespace if desired:\n         if bit != 0 and base != \"bool\":\n+            # add to the main namespace\n             allTypes[myname] = info.type\n+            # add mapping for both the bit name\n+            sctypeDict[myname] = info.type\n \n         # add forward, reverse, and string mapping to numarray\n         sctypeDict[char] = info.type\n \n-        # add mapping for both the bit name\n-        sctypeDict[myname] = info.type\n-\n-\n _add_aliases()\n \n def _add_integer_aliases():\n@@ -224,9 +222,7 @@ def _set_array_types():\n \n # Add additional strings to the sctypeDict\n _toadd = ['int', ('float', 'double'), ('complex', 'cdouble'), \n-          'bool', 'object',\n-          'str', 'bytes', ('a', 'bytes_'),\n-          ('int0', 'intp'), ('uint0', 'uintp')]\n+          'bool', 'object', 'str', 'bytes', ('a', 'bytes_')]\n \n for name in _toadd:\n     if isinstance(name, tuple):\n",
            "comment_added_diff": {
                "110": "            # add to the main namespace",
                "112": "            # add mapping for both the bit name"
            },
            "comment_deleted_diff": {
                "109": "        # Add to the main namespace if desired:",
                "116": "        # add mapping for both the bit name"
            },
            "comment_modified_diff": {}
        },
        {
            "commit": "acfb63cb6661fd9776db2d208ccd49500f6bcf2e",
            "timestamp": "2023-10-05T11:45:12+02:00",
            "author": "Mateusz Sok\u00f3\u0142",
            "commit_message": "MAINT: Refactor allTypes, sctypeDict and sctypes build process",
            "additions": 71,
            "deletions": 209,
            "change_type": "MODIFY",
            "diff": "@@ -17,219 +17,81 @@\n \n \"\"\"\n \n-from numpy.core._string_helpers import english_lower\n+import numpy.core.multiarray as ma\n from numpy.core.multiarray import typeinfo, dtype\n-from numpy.core._dtype import _kind_name\n \n+######################################\n+# Building `sctypeDict` and `allTypes`\n+######################################\n \n-print(list(typeinfo.keys()))\n+sctypeDict = {}\n+allTypes = {}\n \n-sctypeDict = {}      # Contains all leaf-node scalar types with aliases\n-allTypes = {}            # Collect the types we will add to the module\n+_abstract_type_names = {\n+    \"generic\", \"integer\", \"inexact\", \"floating\", \"number\",\n+    \"flexible\", \"character\", \"complexfloating\", \"unsignedinteger\",\n+    \"signedinteger\"\n+}\n \n+for _abstract_type_name in _abstract_type_names:\n+    allTypes[_abstract_type_name] = getattr(ma, _abstract_type_name)\n \n-# separate the actual type info from the abstract base classes\n-_abstract_types = {}\n-_concrete_typeinfo = {}\n for k, v in typeinfo.items():\n-    # make all the keys lowercase too\n-    k = english_lower(k)\n-    if isinstance(v, type):\n-        _abstract_types[k] = v\n-    else:\n-        _concrete_typeinfo[k] = v\n-\n-_concrete_types = {v.type for k, v in _concrete_typeinfo.items()}\n-\n-\n-def _bits_of(obj):\n-    try:\n-        info = next(v for v in _concrete_typeinfo.values() if v.type is obj)\n-    except StopIteration:\n-        if obj in _abstract_types.values():\n-            msg = \"Cannot count the bits of an abstract type\"\n-            raise ValueError(msg) from None\n-\n-        # some third-party type - make a best-guess\n-        return dtype(obj).itemsize * 8\n-    else:\n-        return info.bits\n-\n-\n-def bitname(obj):\n-    \"\"\"Return a bit-width name for a given type object\"\"\"\n-    bits = _bits_of(obj)\n-    dt = dtype(obj)\n-    char = dt.kind\n-    base = _kind_name(dt)\n-\n-    if base == 'object':\n-        bits = 0\n-\n-    if bits != 0:\n-        char = \"%s%d\" % (char, bits // 8)\n-\n-    return base, bits, char\n-\n-\n-def _add_types():\n-    for name, info in _concrete_typeinfo.items():\n-        # define C-name and insert typenum and typechar references also\n-        allTypes[name] = info.type\n-        sctypeDict[name] = info.type\n-        sctypeDict[info.char] = info.type\n-        sctypeDict[info.num] = info.type\n-\n-    for name, cls in _abstract_types.items():\n-        allTypes[name] = cls\n-_add_types()\n-\n-# This is the priority order used to assign the bit-sized NPY_INTxx names, which\n-# must match the order in npy_common.h in order for NPY_INTxx and np.intxx to be\n-# consistent.\n-# If two C types have the same size, then the earliest one in this list is used\n-# as the sized name.\n-_int_ctypes = ['long', 'longlong', 'int', 'short', 'byte']\n-_uint_ctypes = list('u' + t for t in _int_ctypes)\n-\n-def _add_aliases():\n-    for name, info in _concrete_typeinfo.items():\n-        # these are handled by _add_integer_aliases\n-        if name in _int_ctypes or name in _uint_ctypes:\n-            continue\n-\n-        # insert bit-width version for this class (if relevant)\n-        base, bit, char = bitname(info.type)\n-\n-        myname = \"%s%d\" % (base, bit)\n-\n-        # ensure that (c)longdouble does not overwrite the aliases assigned to\n-        # (c)double\n-        if name in ('longdouble', 'clongdouble') and myname in allTypes:\n-            continue\n-\n-        if bit != 0 and base != \"bool\":\n-            # add to the main namespace\n-            allTypes[myname] = info.type\n-            # add mapping for both the bit name\n-            sctypeDict[myname] = info.type\n-\n-        # add forward, reverse, and string mapping to numarray\n-        sctypeDict[char] = info.type\n-\n-_add_aliases()\n-\n-def _add_integer_aliases():\n-    seen_bits = set()\n-    for i_ctype, u_ctype in zip(_int_ctypes, _uint_ctypes):\n-        i_info = _concrete_typeinfo[i_ctype]\n-        u_info = _concrete_typeinfo[u_ctype]\n-        bits = i_info.bits  # same for both\n-\n-        for info, charname, intname in [\n-                (i_info,'i%d' % (bits//8,), 'int%d' % bits),\n-                (u_info,'u%d' % (bits//8,), 'uint%d' % bits)]:\n-            if bits not in seen_bits:\n-                # sometimes two different types have the same number of bits\n-                # if so, the one iterated over first takes precedence\n-                allTypes[intname] = info.type\n-                sctypeDict[intname] = info.type\n-                sctypeDict[charname] = info.type\n-\n-        seen_bits.add(bits)\n-\n-_add_integer_aliases()\n-\n-# We use these later\n-void = allTypes['void']\n-\n-#\n-# Rework the Python names (so that float and complex and int are consistent\n-#                            with Python usage)\n-#\n-def _set_up_aliases():\n-    type_pairs = [('single', 'float'),\n-                  ('csingle', 'cfloat'),\n-                  ('intc', 'int'),\n-                  ('uintc', 'uint'),\n-                  ('int_', 'long'),\n-                  ('uint', 'ulong'),\n-                  ('bool_', 'bool'),\n-                  ('bytes_', 'string'),\n-                  ('str_', 'unicode'),\n-                  ('object_', 'object'),\n-                  ('cfloat', 'cdouble')]\n-    for alias, t in type_pairs:\n-        allTypes[alias] = allTypes[t]\n-        sctypeDict[alias] = sctypeDict[t]\n-    # Remove aliases overriding python types and modules\n-    to_remove = ['object', 'int', 'float', 'complex', 'bool', \n-                 'string', 'datetime', 'timedelta', 'bytes', 'str']\n-\n-    for t in to_remove:\n-        try:\n-            del allTypes[t]\n-            del sctypeDict[t]\n-        except KeyError:\n-            pass\n-\n-    # Additional aliases in sctypeDict that should not be exposed as attributes\n-    attrs_to_remove = ['ulong', 'long', 'unicode', 'cfloat']\n-\n-    for t in attrs_to_remove:\n-        try:\n-            del allTypes[t]\n-        except KeyError:\n-            pass\n-_set_up_aliases()\n-\n-\n-sctypes = {'int': [],\n-           'uint': [],\n-           'float': [],\n-           'complex': [],\n-           'others': [bool, object, bytes, str, void]}\n-\n-\n-def _add_array_type(typename, bits):\n-    try:\n-        t = allTypes['%s%d' % (typename, bits)]\n-    except KeyError:\n-        pass\n-    else:\n-        sctypes[typename].append(t)\n-\n-def _set_array_types():\n-    ibytes = [1, 2, 4, 8, 16, 32, 64]\n-    fbytes = [2, 4, 8, 10, 12, 16, 32, 64]\n-    for bytes in ibytes:\n-        bits = 8*bytes\n-        _add_array_type('int', bits)\n-        _add_array_type('uint', bits)\n-    for bytes in fbytes:\n-        bits = 8*bytes\n-        _add_array_type('float', bits)\n-        _add_array_type('complex', 2*bits)\n-    _gi = dtype('p')\n-    if _gi.type not in sctypes['int']:\n-        indx = 0\n-        sz = _gi.itemsize\n-        _lst = sctypes['int']\n-        while (indx < len(_lst) and sz >= _lst[indx](0).itemsize):\n-            indx += 1\n-        sctypes['int'].insert(indx, _gi.type)\n-        sctypes['uint'].insert(indx, dtype('P').type)\n-_set_array_types()\n-\n-\n-# Add additional strings to the sctypeDict\n-_toadd = ['int', ('float', 'double'), ('complex', 'cdouble'), \n-          'bool', 'object', 'str', 'bytes']\n-\n-for name in _toadd:\n-    if isinstance(name, tuple):\n-        sctypeDict[name[0]] = allTypes[name[1]]\n-    else:\n-        sctypeDict[name] = allTypes['%s_' % name]\n-\n-del _toadd, name\n+    concrete_type = v.type\n+    allTypes[k] = concrete_type\n+    sctypeDict[k] = concrete_type\n+\n+_aliases = {\n+    \"double\": \"float64\",\n+    \"cdouble\": \"complex128\",\n+    \"single\": \"float32\",\n+    \"csingle\": \"complex64\",\n+    \"half\": \"float16\"\n+}\n+\n+for k, v in _aliases.items():\n+    sctypeDict[k] = allTypes[v]\n+    allTypes[k] = allTypes[v]\n+\n+# extra aliases are added only to `sctypeDict`\n+# to support dtype name access, such as`np.dtype(\"float\")`\n+_extra_aliases = {  \n+    \"bool\": \"bool_\",\n+    \"float\": \"float64\",\n+    \"complex\": \"complex128\",\n+    \"object\": \"object_\",\n+    \"bytes\": \"bytes_\",\n+    \"int\": \"int64\",\n+    \"long\": \"int64\",\n+    \"ulong\": \"uint\",\n+    \"str\": \"str_\",\n+}\n+\n+for k, v in _extra_aliases.items():\n+    sctypeDict[k] = allTypes[v]\n+\n+####################\n+# Building `sctypes`\n+####################\n+\n+sctypes = {\"int\": [], \"uint\": [], \"float\": [], \"complex\": [], \"others\": []}\n+\n+for type_info in set(typeinfo.values()):\n+    if type_info.kind in [\"M\", \"m\"]:  # exclude timedelta and datetime\n+        continue\n+\n+    concrete_type = type_info.type\n+\n+    # find proper group for each concrete type\n+    for type_group, abstract_type in [\n+        (\"int\", ma.signedinteger), (\"uint\", ma.unsignedinteger), \n+        (\"float\", ma.floating), (\"complex\", ma.complexfloating), \n+        (\"others\", ma.generic)\n+    ]:\n+        if issubclass(concrete_type, abstract_type):\n+            sctypes[type_group].append(concrete_type)\n+            break\n+\n+# sort sctype groups by bitsize\n+for sctype_list in sctypes.values():\n+    sctype_list.sort(key=lambda x: dtype(x).itemsize)\n",
            "comment_added_diff": {
                "23": "######################################",
                "24": "# Building `sctypeDict` and `allTypes`",
                "25": "######################################",
                "56": "# extra aliases are added only to `sctypeDict`",
                "57": "# to support dtype name access, such as`np.dtype(\"float\")`",
                "73": "####################",
                "74": "# Building `sctypes`",
                "75": "####################",
                "80": "    if type_info.kind in [\"M\", \"m\"]:  # exclude timedelta and datetime",
                "85": "    # find proper group for each concrete type",
                "95": "# sort sctype groups by bitsize"
            },
            "comment_deleted_diff": {
                "27": "sctypeDict = {}      # Contains all leaf-node scalar types with aliases",
                "28": "allTypes = {}            # Collect the types we will add to the module",
                "31": "# separate the actual type info from the abstract base classes",
                "35": "    # make all the keys lowercase too",
                "53": "        # some third-party type - make a best-guess",
                "77": "        # define C-name and insert typenum and typechar references also",
                "87": "# This is the priority order used to assign the bit-sized NPY_INTxx names, which",
                "88": "# must match the order in npy_common.h in order for NPY_INTxx and np.intxx to be",
                "89": "# consistent.",
                "90": "# If two C types have the same size, then the earliest one in this list is used",
                "91": "# as the sized name.",
                "97": "        # these are handled by _add_integer_aliases",
                "101": "        # insert bit-width version for this class (if relevant)",
                "106": "        # ensure that (c)longdouble does not overwrite the aliases assigned to",
                "107": "        # (c)double",
                "112": "            # add to the main namespace",
                "114": "            # add mapping for both the bit name",
                "117": "        # add forward, reverse, and string mapping to numarray",
                "127": "        bits = i_info.bits  # same for both",
                "133": "                # sometimes two different types have the same number of bits",
                "134": "                # if so, the one iterated over first takes precedence",
                "143": "# We use these later",
                "146": "#",
                "147": "# Rework the Python names (so that float and complex and int are consistent",
                "148": "#                            with Python usage)",
                "149": "#",
                "165": "    # Remove aliases overriding python types and modules",
                "176": "    # Additional aliases in sctypeDict that should not be exposed as attributes",
                "225": "# Add additional strings to the sctypeDict"
            },
            "comment_modified_diff": {
                "25": "print(list(typeinfo.keys()))",
                "56": "        return info.bits",
                "57": "",
                "73": "",
                "74": "",
                "75": "def _add_types():",
                "80": "        sctypeDict[info.char] = info.type",
                "85": "_add_types()",
                "95": "def _add_aliases():"
            }
        },
        {
            "commit": "3968009d8fa45cf6d13dc639007fde4af35fb593",
            "timestamp": "2023-10-05T13:34:09+02:00",
            "author": "Mateusz Sok\u00f3\u0142",
            "commit_message": "Include c-names mapping",
            "additions": 19,
            "deletions": 3,
            "change_type": "MODIFY",
            "diff": "@@ -26,6 +26,7 @@\n \n sctypeDict = {}\n allTypes = {}\n+c_names_dict = {}\n \n _abstract_type_names = {\n     \"generic\", \"integer\", \"inexact\", \"floating\", \"number\",\n@@ -37,9 +38,12 @@\n     allTypes[_abstract_type_name] = getattr(ma, _abstract_type_name)\n \n for k, v in typeinfo.items():\n-    concrete_type = v.type\n-    allTypes[k] = concrete_type\n-    sctypeDict[k] = concrete_type\n+    if k.startswith(\"NPY_\") and v not in c_names_dict:\n+        c_names_dict[k[4:]] = v\n+    else:\n+        concrete_type = v.type\n+        allTypes[k] = concrete_type\n+        sctypeDict[k] = concrete_type\n \n _aliases = {\n     \"double\": \"float64\",\n@@ -70,6 +74,18 @@\n for k, v in _extra_aliases.items():\n     sctypeDict[k] = allTypes[v]\n \n+# include extended precision sized aliases\n+for is_complex, full_name in [(False, \"longdouble\"), (True, \"clongdouble\")]:\n+    longdouble_type: type = allTypes[full_name]\n+\n+    bits: int = dtype(longdouble_type).itemsize * 8\n+    base_name: str = \"complex\" if is_complex else \"float\"\n+    extended_prec_name: str = f\"{base_name}{bits}\"\n+    if extended_prec_name not in allTypes:\n+        sctypeDict[extended_prec_name] = longdouble_type\n+        allTypes[extended_prec_name] = longdouble_type\n+\n+\n ####################\n # Building `sctypes`\n ####################\n",
            "comment_added_diff": {
                "77": "# include extended precision sized aliases"
            },
            "comment_deleted_diff": {},
            "comment_modified_diff": {}
        }
    ],
    "__init__.pyi": [],
    "scalars.py": [],
    "scalars.pyi": [],
    "routines.dtype.rst": [],
    "circleci.yml": [],
    ".travis.yml": [],
    "azure-pipelines.yml": [],
    "azure-steps-windows.yml": [],
    "travis-test.sh": [],
    "cibw_before_build.sh": [],
    "gfortran_utils.sh": [],
    "_locales.py": [],
    "histograms.py": [
        {
            "commit": "11beedfd9906a6359f34e7d4c1f79d85fa8833d3",
            "timestamp": "2023-05-25T17:54:40-07:00",
            "author": "Brigitta Sip\u0151cz",
            "commit_message": "DOC: switching to use the plot directive",
            "additions": 14,
            "deletions": 10,
            "change_type": "MODIFY",
            "diff": "@@ -763,16 +763,20 @@ def histogram(a, bins=10, range=None, density=None, weights=None):\n     .. versionadded:: 1.11.0\n \n     Automated Bin Selection Methods example, using 2 peak random data\n-    with 2000 points:\n-\n-    >>> import matplotlib.pyplot as plt\n-    >>> rng = np.random.RandomState(10)  # deterministic random data\n-    >>> a = np.hstack((rng.normal(size=1000),\n-    ...                rng.normal(loc=5, scale=2, size=1000)))\n-    >>> _ = plt.hist(a, bins='auto')  # arguments are passed to np.histogram\n-    >>> plt.title(\"Histogram with 'auto' bins\")\n-    Text(0.5, 1.0, \"Histogram with 'auto' bins\")\n-    >>> plt.show()\n+    with 2000 points.\n+\n+    .. plot::\n+        :include-source:\n+\n+        import matplotlib.pyplot as plt\n+        import numpy as np\n+\n+        rng = np.random.RandomState(10)  # deterministic random data\n+        a = np.hstack((rng.normal(size=1000),\n+                       rng.normal(loc=5, scale=2, size=1000)))\n+        plt.hist(a, bins='auto')  # arguments are passed to np.histogram\n+        plt.title(\"Histogram with 'auto' bins\")\n+        plt.show()\n \n     \"\"\"\n     a, weights = _ravel_and_check_weights(a, weights)\n",
            "comment_added_diff": {
                "774": "        rng = np.random.RandomState(10)  # deterministic random data",
                "777": "        plt.hist(a, bins='auto')  # arguments are passed to np.histogram"
            },
            "comment_deleted_diff": {
                "769": "    >>> rng = np.random.RandomState(10)  # deterministic random data",
                "772": "    >>> _ = plt.hist(a, bins='auto')  # arguments are passed to np.histogram"
            },
            "comment_modified_diff": {
                "774": "    Text(0.5, 1.0, \"Histogram with 'auto' bins\")"
            }
        }
    ],
    "1.4.0-notes.rst": [],
    "_add_newdocs_scalars.py": [
        {
            "commit": "4c6ff640f918c4c0d221ebbfc3fb2226eb8f1b54",
            "timestamp": "2023-07-18T11:50:24+02:00",
            "author": "Sebastian Berg",
            "commit_message": "DOC: Update reference to pass refguide\n\nThis makes things pass even though there should be a massive amount\nof types added in other places, so that it is not complete.\n\nBut I think this may be useful, since the complete change would be\nquite verbose...\n\nOne thing to note may be that `np.float64(nan)` currently does not\nround-trip correctly of course, this would require adding `np.`\nto the repr (or quotes).",
            "additions": 5,
            "deletions": 5,
            "change_type": "MODIFY",
            "diff": "@@ -267,13 +267,13 @@ def add_newdoc_for_scalar_type(obj, fixed_aliases, doc):\n     Examples\n     --------\n     >>> np.void(5)\n-    void(b'\\x00\\x00\\x00\\x00\\x00')\n+    np.void(b'\\x00\\x00\\x00\\x00\\x00')\n     >>> np.void(b'abcd')\n-    void(b'\\x61\\x62\\x63\\x64')\n-    >>> np.void((5, 3.2, \"eggs\"), dtype=\"i,d,S5\")\n-    (5, 3.2, b'eggs')  # looks like a tuple, but is `np.void`\n+    np.void(b'\\x61\\x62\\x63\\x64')\n+    >>> np.void((3.2, b'eggs'), dtype=\"d,S5\")\n+    np.void((3.2, b'eggs'), dtype=[('f0', '<f8'), ('f1', 'S5')])\n     >>> np.void(3, dtype=[('x', np.int8), ('y', np.int8)])\n-    (3, 3)  # looks like a tuple, but is `np.void`\n+    np.void((3, 3), dtype=[('x', 'i1'), ('y', 'i1')])\n \n     \"\"\")\n \n",
            "comment_added_diff": {},
            "comment_deleted_diff": {
                "274": "    (5, 3.2, b'eggs')  # looks like a tuple, but is `np.void`",
                "276": "    (3, 3)  # looks like a tuple, but is `np.void`"
            },
            "comment_modified_diff": {}
        }
    ],
    "22055.improvement.rst": [],
    "ctors.c": [],
    "12065.performance.rst": [],
    "16154.new_feature.rst": [],
    "19388.improvement.rst": [],
    "19388.new_feature.rst": [],
    "20913.improvement.rst": [],
    "20924.compatibility.rst": [],
    "21437.improvement.rst": [],
    "21468.new_feature.rst": [],
    "21483.performance.rst": [],
    "21506.change.rst": [],
    "21595.new_feature.rst": [],
    "21623.new_feature.rst": [],
    "21627.new_feature.rst": [],
    "21645.expired.rst": [],
    "21807.improvement.rst": [],
    "21925.compatibility.rst": [],
    "21976.new_feature.rst": [],
    "21995.compatibility.rst": [],
    "22004.expired.rst": [],
    "22014.improvement.rst": [],
    "22046.change.rst": [],
    "22139.expired.rst": [],
    "22159.expired.rst": [],
    "22228.expired.rst": [],
    "22313.deprecation.rst": [],
    "22316.new_feature.rst": [],
    "22357.improvement.rst": [],
    "22393.deprecation.rst": [],
    "22456.deprecation.rst": [],
    "22540.expired.rst": [],
    "22598.compatibility.rst": [],
    "22607.deprecation.rst": [],
    "22607.expired.rst": [],
    "release.rst": [],
    "1.25.0-notes.rst": [],
    "cversions.txt": [],
    "numpyconfig.h": [],
    "pavement.py": [],
    "22638.deprecation.rst": [],
    "22638.expired.rst": [],
    "_machar.py": [
        {
            "commit": "6cb136ee909b2b3909a202ac94b09c89c8dae4dd",
            "timestamp": "2023-08-24T22:48:39+02:00",
            "author": "Mateusz Sok\u00f3\u0142",
            "commit_message": "API: Remove other scalar aliases [skip ci]",
            "additions": 1,
            "deletions": 1,
            "change_type": "MODIFY",
            "diff": "@@ -11,7 +11,7 @@\n from ._ufunc_config import errstate\n from .._utils import set_module\n \n-# Need to speed this up...especially for longfloat\n+# Need to speed this up...especially for longdouble\n \n # Deprecated 2021-10-20, NumPy 1.22\n class MachAr:\n",
            "comment_added_diff": {
                "14": "# Need to speed this up...especially for longdouble"
            },
            "comment_deleted_diff": {
                "14": "# Need to speed this up...especially for longfloat"
            },
            "comment_modified_diff": {
                "14": "# Need to speed this up...especially for longfloat"
            }
        }
    ],
    "getlimits.py": [
        {
            "commit": "8b9b0efbc08a502627f455ec59656fce68eb10d7",
            "timestamp": "2022-11-22T17:38:33+01:00",
            "author": "Sebastian Berg",
            "commit_message": "DEP: Finalize MachAr and machar deprecations\n\nThis removes the attributes on finfo and the \"public\" module.  It also\ndeprecates `np.core.MachAr`.  We should be able to get away with just\ndeleting it, but there seems little reason to not just deprecate it for now.",
            "additions": 3,
            "deletions": 20,
            "change_type": "MODIFY",
            "diff": "@@ -352,6 +352,9 @@ def _get_machar(ftype):\n \n def _discovered_machar(ftype):\n     \"\"\" Create MachAr instance with found information on float types\n+\n+    TODO: MachAr should be retired completely ideally.  We currently only\n+          ever use it system with broken longdouble (valgrind, WSL).\n     \"\"\"\n     params = _MACHAR_PARAMS[ftype]\n     return MachAr(lambda v: array([v], ftype),\n@@ -387,11 +390,6 @@ class finfo:\n     iexp : int\n         The number of bits in the exponent portion of the floating point\n         representation.\n-    machar : MachAr\n-        The object which calculated these parameters and holds more\n-        detailed information.\n-\n-        .. deprecated:: 1.22\n     machep : int\n         The exponent that yields `eps`.\n     max : floating point number of the appropriate type\n@@ -432,7 +430,6 @@ class finfo:\n \n     See Also\n     --------\n-    MachAr : The implementation of the tests that produce this information.\n     iinfo : The equivalent for integer data types.\n     spacing : The distance between a value and the nearest adjacent number\n     nextafter : The next floating point value after x1 towards x2\n@@ -595,20 +592,6 @@ def tiny(self):\n         \"\"\"\n         return self.smallest_normal\n \n-    @property\n-    def machar(self):\n-        \"\"\"The object which calculated these parameters and holds more\n-        detailed information.\n-\n-        .. deprecated:: 1.22\n-        \"\"\"\n-        # Deprecated 2021-10-27, NumPy 1.22\n-        warnings.warn(\n-            \"`finfo.machar` is deprecated (NumPy 1.22)\",\n-            DeprecationWarning, stacklevel=2,\n-        )\n-        return self._machar\n-\n \n @set_module('numpy')\n class iinfo:\n",
            "comment_added_diff": {},
            "comment_deleted_diff": {
                "605": "        # Deprecated 2021-10-27, NumPy 1.22"
            },
            "comment_modified_diff": {}
        },
        {
            "commit": "0cfbd3c4646aa867b89da2aef16c3a0c693d0303",
            "timestamp": "2023-01-19T18:20:49+01:00",
            "author": "Daiki Shintani",
            "commit_message": "DEP: deprecate np.finfo(None) (#23011)\n\nDeprecate np.finfo(None), it may be that we should more generally deprecate `np.dtype(None)` but this is a start and particularly weird maybe.\r\n\r\nCloses gh-14684",
            "additions": 9,
            "deletions": 0,
            "change_type": "MODIFY",
            "diff": "@@ -471,6 +471,15 @@ class finfo:\n     _finfo_cache = {}\n \n     def __new__(cls, dtype):\n+        if dtype is None:\n+            # Deprecated in NumPy 1.25, 2023-01-16\n+            warnings.warn(\n+                \"finfo() dtype cannot be None. This behavior will \"\n+                \"raise an error in the future. (Deprecated in NumPy 1.25)\",\n+                DeprecationWarning,\n+                stacklevel=2\n+            )\n+\n         try:\n             dtype = numeric.dtype(dtype)\n         except TypeError:\n",
            "comment_added_diff": {
                "475": "            # Deprecated in NumPy 1.25, 2023-01-16"
            },
            "comment_deleted_diff": {},
            "comment_modified_diff": {}
        },
        {
            "commit": "20d397400d6325cff3decbba3d6195418e873237",
            "timestamp": "2023-02-01T18:43:05+11:00",
            "author": "Andrew Nelson",
            "commit_message": "WHL: musllinux wheels [wheel build]",
            "additions": 14,
            "deletions": 3,
            "change_type": "MODIFY",
            "diff": "@@ -147,8 +147,12 @@ def _float_to_str(self, value):\n \n # Key to identify the floating point type.  Key is result of\n # ftype('-0.1').newbyteorder('<').tobytes()\n+#\n+# 20230201 - use (ftype(-1.0) / ftype(10.0)).newbyteorder('<').tobytes()\n+#            instead because stold may have deficiencies on some platforms.\n # See:\n # https://perl5.git.perl.org/perl.git/blob/3118d7d684b56cbeb702af874f4326683c45f045:/Configure\n+\n _KNOWN_TYPES = {}\n def _register_type(machar, bytepat):\n     _KNOWN_TYPES[bytepat] = machar\n@@ -238,8 +242,6 @@ def _register_known_types():\n                              huge=huge_f128,\n                              tiny=tiny_f128)\n     # IEEE 754 128-bit binary float\n-    _register_type(float128_ma,\n-        b'\\x9a\\x99\\x99\\x99\\x99\\x99\\x99\\x99\\x99\\x99\\x99\\x99\\x99\\x99\\xfb\\xbf')\n     _register_type(float128_ma,\n         b'\\x9a\\x99\\x99\\x99\\x99\\x99\\x99\\x99\\x99\\x99\\x99\\x99\\x99\\x99\\xfb\\xbf')\n     _float_ma[128] = float128_ma\n@@ -329,7 +331,9 @@ def _get_machar(ftype):\n     if params is None:\n         raise ValueError(repr(ftype))\n     # Detect known / suspected types\n-    key = ftype('-0.1').newbyteorder('<').tobytes()\n+    # ftype(-1.0) / ftype(10.0) is better than ftype('-0.1') because stold\n+    # may be deficient\n+    key = (ftype(-1.0) / ftype(10.)).newbyteorder('<').tobytes()\n     ma_like = None\n     if ftype == ntypes.longdouble:\n         # Could be 80 bit == 10 byte extended precision, where last bytes can\n@@ -338,7 +342,14 @@ def _get_machar(ftype):\n         # random garbage.\n         ma_like = _KNOWN_TYPES.get(key[:10])\n     if ma_like is None:\n+        # see if the full key is known.\n         ma_like = _KNOWN_TYPES.get(key)\n+    if ma_like is None and len(key) == 16:\n+        # machine limits could be f80 masquerading as np.float128,\n+        # find all keys with length 16 and make new dict, but make the keys\n+        # only 10 bytes long, the last bytes can be random garbage\n+        _kt = {k[:10]: v for k, v in _KNOWN_TYPES.items() if len(k) == 16}\n+        ma_like = _kt.get(key[:10])\n     if ma_like is not None:\n         return ma_like\n     # Fall back to parameter discovery\n",
            "comment_added_diff": {
                "150": "#",
                "151": "# 20230201 - use (ftype(-1.0) / ftype(10.0)).newbyteorder('<').tobytes()",
                "152": "#            instead because stold may have deficiencies on some platforms.",
                "334": "    # ftype(-1.0) / ftype(10.0) is better than ftype('-0.1') because stold",
                "335": "    # may be deficient",
                "345": "        # see if the full key is known.",
                "348": "        # machine limits could be f80 masquerading as np.float128,",
                "349": "        # find all keys with length 16 and make new dict, but make the keys",
                "350": "        # only 10 bytes long, the last bytes can be random garbage"
            },
            "comment_deleted_diff": {},
            "comment_modified_diff": {}
        },
        {
            "commit": "4aca866c292eec899f75475ff14f7d5c1025e394",
            "timestamp": "2023-02-15T16:59:07+01:00",
            "author": "Pieter Eendebak",
            "commit_message": "ENH: Improve performance of finfo and _commonType (#23088)\n\nThe finfo contains a cache for dtypes, but the np.complex128 dtype does not end up in the cache. The reason is that the np.complex128 is converted to np.float64 which is in the cache.\r\n\r\nPerformance improvement for finfo(np.complex128):\r\n\r\nMain: 2.07 \u00b5s \u00b1 75 ns per loop (mean \u00b1 std. dev. of 7 runs, 1000000 loops each)\r\nPr: 324 ns \u00b1 28.9 ns per loop (mean \u00b1 std. dev. of 7 runs, 1000000 loops each)\r\n\r\nImprove performance of finfo by making the cache check the first action in the __new__\r\n\r\nImprove performance of _commonType by re-using the expression for a.dtype.type and eliminating variables\r\nThe finfo and _commonType was part of the computatation time in lstsq when using scikit-rf. Since these methods are used in various other methods performance can improve there slightly as well.",
            "additions": 16,
            "deletions": 5,
            "change_type": "MODIFY",
            "diff": "@@ -482,6 +482,10 @@ class finfo:\n     _finfo_cache = {}\n \n     def __new__(cls, dtype):\n+        obj = cls._finfo_cache.get(dtype)  # most common path\n+        if obj is not None:\n+            return obj\n+\n         if dtype is None:\n             # Deprecated in NumPy 1.25, 2023-01-16\n             warnings.warn(\n@@ -497,7 +501,7 @@ def __new__(cls, dtype):\n             # In case a float instance was given\n             dtype = numeric.dtype(type(dtype))\n \n-        obj = cls._finfo_cache.get(dtype, None)\n+        obj = cls._finfo_cache.get(dtype)\n         if obj is not None:\n             return obj\n         dtypes = [dtype]\n@@ -507,17 +511,24 @@ def __new__(cls, dtype):\n             dtype = newdtype\n         if not issubclass(dtype, numeric.inexact):\n             raise ValueError(\"data type %r not inexact\" % (dtype))\n-        obj = cls._finfo_cache.get(dtype, None)\n+        obj = cls._finfo_cache.get(dtype)\n         if obj is not None:\n             return obj\n         if not issubclass(dtype, numeric.floating):\n             newdtype = _convert_to_float[dtype]\n             if newdtype is not dtype:\n+                # dtype changed, for example from complex128 to float64\n                 dtypes.append(newdtype)\n                 dtype = newdtype\n-        obj = cls._finfo_cache.get(dtype, None)\n-        if obj is not None:\n-            return obj\n+\n+                obj = cls._finfo_cache.get(dtype, None)\n+                if obj is not None:\n+                    # the original dtype was not in the cache, but the new\n+                    # dtype is in the cache. we add the original dtypes to\n+                    # the cache and return the result\n+                    for dt in dtypes:\n+                        cls._finfo_cache[dt] = obj\n+                    return obj\n         obj = object.__new__(cls)._init(dtype)\n         for dt in dtypes:\n             cls._finfo_cache[dt] = obj\n",
            "comment_added_diff": {
                "485": "        obj = cls._finfo_cache.get(dtype)  # most common path",
                "520": "                # dtype changed, for example from complex128 to float64",
                "526": "                    # the original dtype was not in the cache, but the new",
                "527": "                    # dtype is in the cache. we add the original dtypes to",
                "528": "                    # the cache and return the result"
            },
            "comment_deleted_diff": {},
            "comment_modified_diff": {
                "520": "            return obj"
            }
        },
        {
            "commit": "d8047967b6545b8677727b1257965db9d0c95d30",
            "timestamp": "2023-06-09T22:36:34+02:00",
            "author": "Pieter Eendebak",
            "commit_message": "BUG: Allow np.info on non-hashable objects with a dtype (#23911)\n\nIn this PR we restore the functionality to call np.finfo on non-hashable objects with a dtype.\r\n\r\nFixes #23867.",
            "additions": 6,
            "deletions": 3,
            "change_type": "MODIFY",
            "diff": "@@ -482,9 +482,12 @@ class finfo:\n     _finfo_cache = {}\n \n     def __new__(cls, dtype):\n-        obj = cls._finfo_cache.get(dtype)  # most common path\n-        if obj is not None:\n-            return obj\n+        try:\n+            obj = cls._finfo_cache.get(dtype)  # most common path\n+            if obj is not None:\n+                return obj\n+        except TypeError:\n+            pass\n \n         if dtype is None:\n             # Deprecated in NumPy 1.25, 2023-01-16\n",
            "comment_added_diff": {
                "486": "            obj = cls._finfo_cache.get(dtype)  # most common path"
            },
            "comment_deleted_diff": {
                "485": "        obj = cls._finfo_cache.get(dtype)  # most common path"
            },
            "comment_modified_diff": {
                "486": "        if obj is not None:"
            }
        }
    ],
    "memmap.py": [],
    "_utils.py": [],
    "_exceptions.py": [
        {
            "commit": "b0f318b38e7dd305c3ca93e6c912e1391dda999e",
            "timestamp": "2022-11-30T12:07:51+01:00",
            "author": "Sebastian Berg",
            "commit_message": "API: Add new exceptions module and move exception exposed via numeric\n\nThis means moving ComplexWarning, TooHardError, and AxisError.",
            "additions": 0,
            "deletions": 107,
            "change_type": "MODIFY",
            "diff": "@@ -114,113 +114,6 @@ def __str__(self):\n         )\n \n \n-# Exception used in shares_memory()\n-@set_module('numpy')\n-class TooHardError(RuntimeError):\n-    \"\"\"max_work was exceeded.\n-\n-    This is raised whenever the maximum number of candidate solutions \n-    to consider specified by the ``max_work`` parameter is exceeded.\n-    Assigning a finite number to max_work may have caused the operation \n-    to fail.\n-\n-    \"\"\"\n-    \n-    pass\n-\n-\n-@set_module('numpy')\n-class AxisError(ValueError, IndexError):\n-    \"\"\"Axis supplied was invalid.\n-\n-    This is raised whenever an ``axis`` parameter is specified that is larger\n-    than the number of array dimensions.\n-    For compatibility with code written against older numpy versions, which\n-    raised a mixture of `ValueError` and `IndexError` for this situation, this\n-    exception subclasses both to ensure that ``except ValueError`` and\n-    ``except IndexError`` statements continue to catch `AxisError`.\n-\n-    .. versionadded:: 1.13\n-\n-    Parameters\n-    ----------\n-    axis : int or str\n-        The out of bounds axis or a custom exception message.\n-        If an axis is provided, then `ndim` should be specified as well.\n-    ndim : int, optional\n-        The number of array dimensions.\n-    msg_prefix : str, optional\n-        A prefix for the exception message.\n-\n-    Attributes\n-    ----------\n-    axis : int, optional\n-        The out of bounds axis or ``None`` if a custom exception\n-        message was provided. This should be the axis as passed by\n-        the user, before any normalization to resolve negative indices.\n-\n-        .. versionadded:: 1.22\n-    ndim : int, optional\n-        The number of array dimensions or ``None`` if a custom exception\n-        message was provided.\n-\n-        .. versionadded:: 1.22\n-\n-\n-    Examples\n-    --------\n-    >>> array_1d = np.arange(10)\n-    >>> np.cumsum(array_1d, axis=1)\n-    Traceback (most recent call last):\n-      ...\n-    numpy.AxisError: axis 1 is out of bounds for array of dimension 1\n-\n-    Negative axes are preserved:\n-\n-    >>> np.cumsum(array_1d, axis=-2)\n-    Traceback (most recent call last):\n-      ...\n-    numpy.AxisError: axis -2 is out of bounds for array of dimension 1\n-\n-    The class constructor generally takes the axis and arrays'\n-    dimensionality as arguments:\n-\n-    >>> print(np.AxisError(2, 1, msg_prefix='error'))\n-    error: axis 2 is out of bounds for array of dimension 1\n-\n-    Alternatively, a custom exception message can be passed:\n-\n-    >>> print(np.AxisError('Custom error message'))\n-    Custom error message\n-\n-    \"\"\"\n-\n-    __slots__ = (\"axis\", \"ndim\", \"_msg\")\n-\n-    def __init__(self, axis, ndim=None, msg_prefix=None):\n-        if ndim is msg_prefix is None:\n-            # single-argument form: directly set the error message\n-            self._msg = axis\n-            self.axis = None\n-            self.ndim = None\n-        else:\n-            self._msg = msg_prefix\n-            self.axis = axis\n-            self.ndim = ndim\n-\n-    def __str__(self):\n-        axis = self.axis\n-        ndim = self.ndim\n-\n-        if axis is ndim is None:\n-            return self._msg\n-        else:\n-            msg = f\"axis {axis} is out of bounds for array of dimension {ndim}\"\n-            if self._msg is not None:\n-                msg = f\"{self._msg}: {msg}\"\n-            return msg\n-\n-\n @_display_as_base\n class _ArrayMemoryError(MemoryError):\n     \"\"\" Thrown when an array cannot be allocated\"\"\"\n",
            "comment_added_diff": {},
            "comment_deleted_diff": {
                "117": "# Exception used in shares_memory()",
                "202": "            # single-argument form: directly set the error message"
            },
            "comment_modified_diff": {}
        }
    ],
    "_ufunc_config.py": [
        {
            "commit": "2e9479eb2f964566d25826b21445c59a88ccd319",
            "timestamp": "2023-05-26T00:51:14-07:00",
            "author": "Sebastian Berg",
            "commit_message": "DOC: Clean up errstate handling in our tests (#23813)\n\n* TST: Clean up errstate handling in our tests\r\n\r\nIt is plausible that this doesn't fix everything, but it fixes\r\nat least most.\r\n\r\nCo-authored-by: Ross Barnowski <rossbar@berkeley.edu>",
            "additions": 26,
            "deletions": 20,
            "change_type": "MODIFY",
            "diff": "@@ -85,14 +85,11 @@ def seterr(all=None, divide=None, over=None, under=None, invalid=None):\n \n     Examples\n     --------\n-    >>> old_settings = np.seterr(all='ignore')  #seterr to known value\n-    >>> np.seterr(over='raise')\n-    {'divide': 'ignore', 'over': 'ignore', 'under': 'ignore', 'invalid': 'ignore'}\n-    >>> np.seterr(**old_settings)  # reset to default\n-    {'divide': 'ignore', 'over': 'raise', 'under': 'ignore', 'invalid': 'ignore'}\n-\n+    >>> orig_settings = np.seterr(all='ignore')  # seterr to known value\n     >>> np.int16(32000) * np.int16(3)\n     30464\n+    >>> np.seterr(over='raise')\n+    {'divide': 'ignore', 'over': 'ignore', 'under': 'ignore', 'invalid': 'ignore'}\n     >>> old_settings = np.seterr(all='warn', over='raise')\n     >>> np.int16(32000) * np.int16(3)\n     Traceback (most recent call last):\n@@ -104,6 +101,8 @@ def seterr(all=None, divide=None, over=None, under=None, invalid=None):\n     {'divide': 'print', 'over': 'print', 'under': 'print', 'invalid': 'print'}\n     >>> np.int16(32000) * np.int16(3)\n     30464\n+    >>> np.seterr(**orig_settings)  # restore original\n+    {'divide': 'print', 'over': 'print', 'under': 'print', 'invalid': 'print'}\n \n     \"\"\"\n \n@@ -155,14 +154,18 @@ def geterr():\n     --------\n     >>> np.geterr()\n     {'divide': 'warn', 'over': 'warn', 'under': 'ignore', 'invalid': 'warn'}\n-    >>> np.arange(3.) / np.arange(3.)\n+    >>> np.arange(3.) / np.arange(3.)  # doctest: +SHOW_WARNINGS\n     array([nan,  1.,  1.])\n+    RuntimeWarning: invalid value encountered in divide\n \n-    >>> oldsettings = np.seterr(all='warn', over='raise')\n+    >>> oldsettings = np.seterr(all='warn', invalid='raise')\n     >>> np.geterr()\n-    {'divide': 'warn', 'over': 'raise', 'under': 'warn', 'invalid': 'warn'}\n+    {'divide': 'warn', 'over': 'warn', 'under': 'warn', 'invalid': 'raise'}\n     >>> np.arange(3.) / np.arange(3.)\n-    array([nan,  1.,  1.])\n+    Traceback (most recent call last):\n+      ...\n+    FloatingPointError: invalid value encountered in divide\n+    >>> oldsettings = np.seterr(**oldsettings)  # restore original\n \n     \"\"\"\n     maskvalue = umath.geterrobj()[1]\n@@ -267,16 +270,16 @@ def seterrcall(func):\n     ...     print(\"Floating point error (%s), with flag %s\" % (type, flag))\n     ...\n \n-    >>> saved_handler = np.seterrcall(err_handler)\n-    >>> save_err = np.seterr(all='call')\n+    >>> orig_handler = np.seterrcall(err_handler)\n+    >>> orig_err = np.seterr(all='call')\n \n     >>> np.array([1, 2, 3]) / 0.0\n     Floating point error (divide by zero), with flag 1\n     array([inf, inf, inf])\n \n-    >>> np.seterrcall(saved_handler)\n+    >>> np.seterrcall(orig_handler)\n     <function err_handler at 0x...>\n-    >>> np.seterr(**save_err)\n+    >>> np.seterr(**orig_err)\n     {'divide': 'call', 'over': 'call', 'under': 'call', 'invalid': 'call'}\n \n     Log error message:\n@@ -294,9 +297,9 @@ def seterrcall(func):\n     LOG: Warning: divide by zero encountered in divide\n     array([inf, inf, inf])\n \n-    >>> np.seterrcall(saved_handler)\n-    <numpy.core.numeric.Log object at 0x...>\n-    >>> np.seterr(**save_err)\n+    >>> np.seterrcall(orig_handler)\n+    <numpy.Log object at 0x...>\n+    >>> np.seterr(**orig_err)\n     {'divide': 'log', 'over': 'log', 'under': 'log', 'invalid': 'log'}\n \n     \"\"\"\n@@ -341,10 +344,10 @@ def geterrcall():\n     --------\n     >>> np.geterrcall()  # we did not yet set a handler, returns None\n \n-    >>> oldsettings = np.seterr(all='call')\n+    >>> orig_settings = np.seterr(all='call')\n     >>> def err_handler(type, flag):\n     ...     print(\"Floating point error (%s), with flag %s\" % (type, flag))\n-    >>> oldhandler = np.seterrcall(err_handler)\n+    >>> old_handler = np.seterrcall(err_handler)\n     >>> np.array([1, 2, 3]) / 0.0\n     Floating point error (divide by zero), with flag 1\n     array([inf, inf, inf])\n@@ -352,6 +355,8 @@ def geterrcall():\n     >>> cur_handler = np.geterrcall()\n     >>> cur_handler is err_handler\n     True\n+    >>> old_settings = np.seterr(**orig_settings)  # restore original\n+    >>> old_handler = np.seterrcall(None)  # restore original\n \n     \"\"\"\n     return umath.geterrobj()[2]\n@@ -404,7 +409,7 @@ class errstate(contextlib.ContextDecorator):\n \n     >>> np.arange(3) / 0.\n     array([nan, inf, inf])\n-    >>> with np.errstate(divide='warn'):\n+    >>> with np.errstate(divide='ignore'):\n     ...     np.arange(3) / 0.\n     array([nan, inf, inf])\n \n@@ -420,6 +425,7 @@ class errstate(contextlib.ContextDecorator):\n \n     >>> np.geterr()\n     {'divide': 'ignore', 'over': 'ignore', 'under': 'ignore', 'invalid': 'ignore'}\n+    >>> olderr = np.seterr(**olderr)  # restore original state\n \n     \"\"\"\n \n",
            "comment_added_diff": {
                "88": "    >>> orig_settings = np.seterr(all='ignore')  # seterr to known value",
                "104": "    >>> np.seterr(**orig_settings)  # restore original",
                "157": "    >>> np.arange(3.) / np.arange(3.)  # doctest: +SHOW_WARNINGS",
                "168": "    >>> oldsettings = np.seterr(**oldsettings)  # restore original",
                "358": "    >>> old_settings = np.seterr(**orig_settings)  # restore original",
                "359": "    >>> old_handler = np.seterrcall(None)  # restore original",
                "428": "    >>> olderr = np.seterr(**olderr)  # restore original state"
            },
            "comment_deleted_diff": {
                "88": "    >>> old_settings = np.seterr(all='ignore')  #seterr to known value",
                "91": "    >>> np.seterr(**old_settings)  # reset to default"
            },
            "comment_modified_diff": {
                "88": "    >>> old_settings = np.seterr(all='ignore')  #seterr to known value"
            }
        },
        {
            "commit": "37e14b73939b953fce3c01547678dc0d026e3491",
            "timestamp": "2023-05-26T19:02:42-06:00",
            "author": "Charles Harris",
            "commit_message": "BUG: Doctest doesn't have a SHOW_WARNINGS directive.",
            "additions": 1,
            "deletions": 1,
            "change_type": "MODIFY",
            "diff": "@@ -154,7 +154,7 @@ def geterr():\n     --------\n     >>> np.geterr()\n     {'divide': 'warn', 'over': 'warn', 'under': 'ignore', 'invalid': 'warn'}\n-    >>> np.arange(3.) / np.arange(3.)  # doctest: +SHOW_WARNINGS\n+    >>> np.arange(3.) / np.arange(3.)  # doctest: +SKIP\n     array([nan,  1.,  1.])\n     RuntimeWarning: invalid value encountered in divide\n \n",
            "comment_added_diff": {
                "157": "    >>> np.arange(3.) / np.arange(3.)  # doctest: +SKIP"
            },
            "comment_deleted_diff": {
                "157": "    >>> np.arange(3.) / np.arange(3.)  # doctest: +SHOW_WARNINGS"
            },
            "comment_modified_diff": {
                "157": "    >>> np.arange(3.) / np.arange(3.)  # doctest: +SHOW_WARNINGS"
            }
        },
        {
            "commit": "fb9eae06ce046ac4300615ce094a95442ea0a2e0",
            "timestamp": "2023-06-14T09:32:22+02:00",
            "author": "Sebastian Berg",
            "commit_message": "ENH: Working restructure of the errstate/extobj into a contextvar",
            "additions": 43,
            "deletions": 79,
            "change_type": "MODIFY",
            "diff": "@@ -20,15 +20,6 @@\n     \"errstate\", '_no_nep50_warning'\n ]\n \n-_errdict = {\"ignore\": ERR_IGNORE,\n-            \"warn\": ERR_WARN,\n-            \"raise\": ERR_RAISE,\n-            \"call\": ERR_CALL,\n-            \"print\": ERR_PRINT,\n-            \"log\": ERR_LOG}\n-\n-_errdict_rev = {value: key for key, value in _errdict.items()}\n-\n \n @set_module('numpy')\n def seterr(all=None, divide=None, over=None, under=None, invalid=None):\n@@ -106,25 +97,13 @@ def seterr(all=None, divide=None, over=None, under=None, invalid=None):\n \n     \"\"\"\n \n-    pyvals = umath._geterrobj()\n-    old = geterr()\n-\n-    if divide is None:\n-        divide = all or old['divide']\n-    if over is None:\n-        over = all or old['over']\n-    if under is None:\n-        under = all or old['under']\n-    if invalid is None:\n-        invalid = all or old['invalid']\n-\n-    maskvalue = ((_errdict[divide] << SHIFT_DIVIDEBYZERO) +\n-                 (_errdict[over] << SHIFT_OVERFLOW) +\n-                 (_errdict[under] << SHIFT_UNDERFLOW) +\n-                 (_errdict[invalid] << SHIFT_INVALID))\n-\n-    pyvals[1] = maskvalue\n-    umath._seterrobj(pyvals)\n+    old = umath._geterrobj()\n+    # The errstate doesn't include call and bufsize, so pop them:\n+    old.pop(\"call\", None)\n+    old.pop(\"bufsize\", None)\n+\n+    umath._seterrobj(\n+            all=all, divide=divide, over=over, under=under, invalid=invalid)\n     return old\n \n \n@@ -168,17 +147,10 @@ def geterr():\n     >>> oldsettings = np.seterr(**oldsettings)  # restore original\n \n     \"\"\"\n-    maskvalue = umath._geterrobj()[1]\n-    mask = 7\n-    res = {}\n-    val = (maskvalue >> SHIFT_DIVIDEBYZERO) & mask\n-    res['divide'] = _errdict_rev[val]\n-    val = (maskvalue >> SHIFT_OVERFLOW) & mask\n-    res['over'] = _errdict_rev[val]\n-    val = (maskvalue >> SHIFT_UNDERFLOW) & mask\n-    res['under'] = _errdict_rev[val]\n-    val = (maskvalue >> SHIFT_INVALID) & mask\n-    res['invalid'] = _errdict_rev[val]\n+    res = umath._geterrobj()\n+    # The \"geterr\" doesn't include call and bufsize,:\n+    res.pop(\"call\", None)\n+    res.pop(\"bufsize\", None)\n     return res\n \n \n@@ -193,17 +165,8 @@ def setbufsize(size):\n         Size of buffer.\n \n     \"\"\"\n-    if size > 10e6:\n-        raise ValueError(\"Buffer size, %s, is too big.\" % size)\n-    if size < 5:\n-        raise ValueError(\"Buffer size, %s, is too small.\" % size)\n-    if size % 16 != 0:\n-        raise ValueError(\"Buffer size, %s, is not a multiple of 16.\" % size)\n-\n-    pyvals = umath._geterrobj()\n-    old = getbufsize()\n-    pyvals[0] = size\n-    umath._seterrobj(pyvals)\n+    old = umath._geterrobj()[\"bufsize\"]\n+    umath._seterrobj(bufsize=size)\n     return old\n \n \n@@ -218,7 +181,7 @@ def getbufsize():\n         Size of ufunc buffer in bytes.\n \n     \"\"\"\n-    return umath._geterrobj()[0]\n+    return umath._geterrobj()[\"bufsize\"]\n \n \n @set_module('numpy')\n@@ -303,14 +266,8 @@ def seterrcall(func):\n     {'divide': 'log', 'over': 'log', 'under': 'log', 'invalid': 'log'}\n \n     \"\"\"\n-    if func is not None and not isinstance(func, collections.abc.Callable):\n-        if (not hasattr(func, 'write') or\n-                not isinstance(func.write, collections.abc.Callable)):\n-            raise ValueError(\"Only callable can be used as callback\")\n-    pyvals = umath._geterrobj()\n-    old = geterrcall()\n-    pyvals[2] = func\n-    umath._seterrobj(pyvals)\n+    old = umath._geterrobj()[\"call\"]\n+    umath._seterrobj(call=func)\n     return old\n \n \n@@ -359,7 +316,7 @@ def geterrcall():\n     >>> old_handler = np.seterrcall(None)  # restore original\n \n     \"\"\"\n-    return umath._geterrobj()[2]\n+    return umath._geterrobj()[\"call\"]\n \n \n class _unspecified:\n@@ -428,29 +385,36 @@ class errstate(contextlib.ContextDecorator):\n     >>> olderr = np.seterr(**olderr)  # restore original state\n \n     \"\"\"\n-\n-    def __init__(self, *, call=_Unspecified, **kwargs):\n-        self.call = call\n-        self.kwargs = kwargs\n+    __slots__ = [\n+        \"_call\", \"_all\", \"_divide\", \"_over\", \"_under\", \"_invalid\", \"_token\"]\n+\n+    def __init__(self, *, call=_Unspecified,\n+                 all=None, divide=None, over=None, under=None, invalid=None):\n+        self._token = None\n+        self._call = call\n+        self._all = all\n+        self._divide = divide\n+        self._over = over\n+        self._under = under\n+        self._invalid = invalid\n \n     def __enter__(self):\n-        self.oldstate = seterr(**self.kwargs)\n-        if self.call is not _Unspecified:\n-            self.oldcall = seterrcall(self.call)\n+        if self._token is not None:\n+            raise TypeError(\"Cannot enter `np.errstate` twice.\")\n+        if self._call is _Unspecified:\n+            self._token = umath._seterrobj(\n+                    all=self._all, divide=self._divide, over=self._over,\n+                    under=self._under, invalid=self._invalid)\n+        else:\n+            self._token = umath._seterrobj(\n+                    call=self._call,\n+                    all=self._all, divide=self._divide, over=self._over,\n+                    under=self._under, invalid=self._invalid)\n \n     def __exit__(self, *exc_info):\n-        seterr(**self.oldstate)\n-        if self.call is not _Unspecified:\n-            seterrcall(self.oldcall)\n-\n-\n-def _setdef():\n-    defval = [UFUNC_BUFSIZE_DEFAULT, ERR_DEFAULT, None]\n-    umath._seterrobj(defval)\n-\n-\n-# set the default values\n-_setdef()\n+        umath._seterrobj(self._token)\n+        # Allow entering twice, so long as it is sequential:\n+        self._token = None\n \n \n NO_NEP50_WARNING = contextvars.ContextVar(\"_no_nep50_warning\", default=False)\n",
            "comment_added_diff": {
                "101": "    # The errstate doesn't include call and bufsize, so pop them:",
                "151": "    # The \"geterr\" doesn't include call and bufsize,:",
                "416": "        # Allow entering twice, so long as it is sequential:"
            },
            "comment_deleted_diff": {
                "452": "# set the default values"
            },
            "comment_modified_diff": {}
        },
        {
            "commit": "cd4ef0778cb0b53498ba787ab12ebe0d45bccd87",
            "timestamp": "2023-06-21T21:29:40+02:00",
            "author": "Sebastian Berg",
            "commit_message": "BUG: Make errstate decorator compatible with threading\n\nWe need to store the token per-thread, which doesn't work if one\ninstance of errstate is shared between threads (and we store it on\nthe instance).  `ContextDecorator` has a mechanism for this, but\nit is not public.\n\nSo, this just implements it the full manual style.\n\nI additionally add a new note on async-safety and the missing test\n(yes, fails on 1.25).\n\nCloses gh-24013",
            "additions": 34,
            "deletions": 2,
            "change_type": "MODIFY",
            "diff": "@@ -7,6 +7,7 @@\n import collections.abc\n import contextlib\n import contextvars\n+import functools\n \n from .._utils import set_module\n from .umath import _make_extobj, _get_extobj_dict, _extobj_contextvar\n@@ -330,7 +331,7 @@ class _unspecified:\n \n \n @set_module('numpy')\n-class errstate(contextlib.ContextDecorator):\n+class errstate:\n     \"\"\"\n     errstate(**kwargs)\n \n@@ -344,7 +345,11 @@ class errstate(contextlib.ContextDecorator):\n     ..  versionchanged:: 1.17.0\n         `errstate` is also usable as a function decorator, saving\n         a level of indentation if an entire function is wrapped.\n-        See :py:class:`contextlib.ContextDecorator` for more information.\n+\n+    .. versionchanged:: 2.0\n+        `errstate` is now fully thread and asyncio safe, but may not be\n+        entered more than once (unless sequentially).\n+        It is not safe to decorate async functions using ``errstate``.\n \n     Parameters\n     ----------\n@@ -402,6 +407,7 @@ def __init__(self, *, call=_Unspecified,\n         self._invalid = invalid\n \n     def __enter__(self):\n+        # Note that __call__ duplicates much of this logic\n         if self._token is not None:\n             raise TypeError(\"Cannot enter `np.errstate` twice.\")\n         if self._call is _Unspecified:\n@@ -421,6 +427,32 @@ def __exit__(self, *exc_info):\n         # Allow entering twice, so long as it is sequential:\n         self._token = None\n \n+    def __call__(self, func):\n+        # We need to customize `__call__` compared to `ContextDecorator`\n+        # because we must store the token per-thread so cannot store it on\n+        # the instance (we could create a new instance for this).\n+        # This duplicates the code from `__enter__`.\n+        @functools.wraps(func)\n+        def inner(*args, **kwargs):\n+            if self._call is _Unspecified:\n+                extobj = _make_extobj(\n+                        all=self._all, divide=self._divide, over=self._over,\n+                        under=self._under, invalid=self._invalid)\n+            else:\n+                extobj = _make_extobj(\n+                        call=self._call,\n+                        all=self._all, divide=self._divide, over=self._over,\n+                        under=self._under, invalid=self._invalid)\n+\n+            _token = _extobj_contextvar.set(extobj)\n+            try:\n+                # Call the original, decorated, function:\n+                return func(*args, **kwargs)\n+            finally:\n+                _extobj_contextvar.reset(_token)\n+\n+        return inner\n+\n \n NO_NEP50_WARNING = contextvars.ContextVar(\"_no_nep50_warning\", default=False)\n \n",
            "comment_added_diff": {
                "410": "        # Note that __call__ duplicates much of this logic",
                "431": "        # We need to customize `__call__` compared to `ContextDecorator`",
                "432": "        # because we must store the token per-thread so cannot store it on",
                "433": "        # the instance (we could create a new instance for this).",
                "434": "        # This duplicates the code from `__enter__`.",
                "449": "                # Call the original, decorated, function:"
            },
            "comment_deleted_diff": {},
            "comment_modified_diff": {}
        },
        {
            "commit": "ffe8b64e5e0ede5e3d1b7293e6307dfd306e2f00",
            "timestamp": "2023-06-27T11:51:15+02:00",
            "author": "Sebastian Berg",
            "commit_message": "MAINT: Remove ability to enter errstate twice (sequentially)\n\nThat was a bit of a slip.  The issue why I allowed it was that\nit \"fixed\" the decorator version, but it was not the correct fix.\n\nConsidering that things like `warnings.catch_warnings()` do not allow\nthis either, this seems better.\nI also don't think it was technically thread-safe.",
            "additions": 1,
            "deletions": 3,
            "change_type": "MODIFY",
            "diff": "@@ -348,7 +348,7 @@ class errstate:\n \n     .. versionchanged:: 2.0\n         `errstate` is now fully thread and asyncio safe, but may not be\n-        entered more than once (unless sequentially).\n+        entered more than once.\n         It is not safe to decorate async functions using ``errstate``.\n \n     Parameters\n@@ -424,8 +424,6 @@ def __enter__(self):\n \n     def __exit__(self, *exc_info):\n         _extobj_contextvar.reset(self._token)\n-        # Allow entering twice, so long as it is sequential:\n-        self._token = None\n \n     def __call__(self, func):\n         # We need to customize `__call__` compared to `ContextDecorator`\n",
            "comment_added_diff": {},
            "comment_deleted_diff": {
                "427": "        # Allow entering twice, so long as it is sequential:"
            },
            "comment_modified_diff": {}
        }
    ],
    "defchararray.py": [
        {
            "commit": "737b0647e712853d2b7defd944dde7e4b10cd224",
            "timestamp": "2023-01-10T17:55:51+01:00",
            "author": "Nathan Goldbaum",
            "commit_message": "ENH: allow NEP 42 dtypes to work with np.char (#22863)\n\nThis makes it possible for new-style NEP 42 string dtypes like ASCIIDType to work with the functions in np.char, this has leads to some mild modification (stricter behavior in bad paths).\r\n\r\nIt will only work with dtypes with a scalar that subclasses str or bytes. I also assume that you can create instances of the user dtype from python like dtype_instance = CustomDType(size_in_bytes). This is a pretty big assumption about the API of the dtype, I'm not sure offhand how I can do this more portably or more safely.\r\n\r\nI also added a new macro, NPY_DT_is_user_defined, which checks dtype->type_num == -1, which is currently true for all custom dtypes using the experimental dtype API. This new macro is needed because NPY_DT_is_legacy will return false for np.void.\r\n\r\nThis is only tested via the user dtypes currently.",
            "additions": 45,
            "deletions": 32,
            "change_type": "MODIFY",
            "diff": "@@ -46,26 +46,29 @@\n     overrides.array_function_dispatch, module='numpy.char')\n \n \n-def _use_unicode(*args):\n-    \"\"\"\n-    Helper function for determining the output type of some string\n-    operations.\n+def _is_unicode(arr):\n+    \"\"\"Returns True if arr is a string or a string array with a dtype that\n+    represents a unicode string, otherwise returns False.\n \n-    For an operation on two ndarrays, if at least one is unicode, the\n-    result should be unicode.\n     \"\"\"\n-    for x in args:\n-        if (isinstance(x, str) or\n-                issubclass(numpy.asarray(x).dtype.type, unicode_)):\n-            return unicode_\n-    return string_\n+    if (isinstance(arr, str) or\n+            issubclass(numpy.asarray(arr).dtype.type, str)):\n+        return True\n+    return False\n+\n \n-def _to_string_or_unicode_array(result):\n+def _to_string_or_unicode_array(result, output_dtype_like=None):\n     \"\"\"\n-    Helper function to cast a result back into a string or unicode array\n-    if an object array must be used as an intermediary.\n+    Helper function to cast a result back into an array\n+    with the appropriate dtype if an object array must be used\n+    as an intermediary.\n     \"\"\"\n-    return numpy.asarray(result.tolist())\n+    ret = numpy.asarray(result.tolist())\n+    dtype = getattr(output_dtype_like, 'dtype', None)\n+    if dtype is not None:\n+        return ret.astype(type(dtype)(_get_num_chars(ret)), copy=False)\n+    return ret\n+\n \n def _clean_args(*args):\n     \"\"\"\n@@ -319,9 +322,19 @@ def add(x1, x2):\n     arr1 = numpy.asarray(x1)\n     arr2 = numpy.asarray(x2)\n     out_size = _get_num_chars(arr1) + _get_num_chars(arr2)\n-    dtype = _use_unicode(arr1, arr2)\n-    return _vec_string(arr1, (dtype, out_size), '__add__', (arr2,))\n \n+    if type(arr1.dtype) != type(arr2.dtype):\n+        # Enforce this for now.  The solution to it will be implement add\n+        # as a ufunc.  It never worked right on Python 3: bytes + unicode gave\n+        # nonsense unicode + bytes errored, and unicode + object used the\n+        # object dtype itemsize as num chars (worked on short strings).\n+        # bytes + void worked but promoting void->bytes is dubious also.\n+        raise TypeError(\n+            \"np.char.add() requires both arrays of the same dtype kind, but \"\n+            f\"got dtypes: '{arr1.dtype}' and '{arr2.dtype}' (the few cases \"\n+            \"where this used to work often lead to incorrect results).\")\n+\n+    return _vec_string(arr1, type(arr1.dtype)(out_size), '__add__', (arr2,))\n \n def _multiply_dispatcher(a, i):\n     return (a,)\n@@ -371,7 +384,7 @@ def multiply(a, i):\n         raise ValueError(\"Can only multiply by integers\")\n     out_size = _get_num_chars(a_arr) * max(int(i_arr.max()), 0)\n     return _vec_string(\n-        a_arr, (a_arr.dtype.type, out_size), '__mul__', (i_arr,))\n+        a_arr, type(a_arr.dtype)(out_size), '__mul__', (i_arr,))\n \n \n def _mod_dispatcher(a, values):\n@@ -403,7 +416,7 @@ def mod(a, values):\n \n     \"\"\"\n     return _to_string_or_unicode_array(\n-        _vec_string(a, object_, '__mod__', (values,)))\n+        _vec_string(a, object_, '__mod__', (values,)), a)\n \n \n @array_function_dispatch(_unary_op_dispatcher)\n@@ -499,7 +512,7 @@ def center(a, width, fillchar=' '):\n     if numpy.issubdtype(a_arr.dtype, numpy.string_):\n         fillchar = asbytes(fillchar)\n     return _vec_string(\n-        a_arr, (a_arr.dtype.type, size), 'center', (width_arr, fillchar))\n+        a_arr, type(a_arr.dtype)(size), 'center', (width_arr, fillchar))\n \n \n def _count_dispatcher(a, sub, start=None, end=None):\n@@ -723,7 +736,7 @@ def expandtabs(a, tabsize=8):\n \n     \"\"\"\n     return _to_string_or_unicode_array(\n-        _vec_string(a, object_, 'expandtabs', (tabsize,)))\n+        _vec_string(a, object_, 'expandtabs', (tabsize,)), a)\n \n \n @array_function_dispatch(_count_dispatcher)\n@@ -1043,7 +1056,7 @@ def join(sep, seq):\n \n     \"\"\"\n     return _to_string_or_unicode_array(\n-        _vec_string(sep, object_, 'join', (seq,)))\n+        _vec_string(sep, object_, 'join', (seq,)), seq)\n \n \n \n@@ -1084,7 +1097,7 @@ def ljust(a, width, fillchar=' '):\n     if numpy.issubdtype(a_arr.dtype, numpy.string_):\n         fillchar = asbytes(fillchar)\n     return _vec_string(\n-        a_arr, (a_arr.dtype.type, size), 'ljust', (width_arr, fillchar))\n+        a_arr, type(a_arr.dtype)(size), 'ljust', (width_arr, fillchar))\n \n \n @array_function_dispatch(_unary_op_dispatcher)\n@@ -1218,7 +1231,7 @@ def partition(a, sep):\n \n     \"\"\"\n     return _to_string_or_unicode_array(\n-        _vec_string(a, object_, 'partition', (sep,)))\n+        _vec_string(a, object_, 'partition', (sep,)), a)\n \n \n def _replace_dispatcher(a, old, new, count=None):\n@@ -1263,8 +1276,7 @@ def replace(a, old, new, count=None):\n     array(['The dwash was fresh', 'Thwas was it'], dtype='<U19')\n     \"\"\"\n     return _to_string_or_unicode_array(\n-        _vec_string(\n-            a, object_, 'replace', [old, new] + _clean_args(count)))\n+        _vec_string(a, object_, 'replace', [old, new] + _clean_args(count)), a)\n \n \n @array_function_dispatch(_count_dispatcher)\n@@ -1363,7 +1375,7 @@ def rjust(a, width, fillchar=' '):\n     if numpy.issubdtype(a_arr.dtype, numpy.string_):\n         fillchar = asbytes(fillchar)\n     return _vec_string(\n-        a_arr, (a_arr.dtype.type, size), 'rjust', (width_arr, fillchar))\n+        a_arr, type(a_arr.dtype)(size), 'rjust', (width_arr, fillchar))\n \n \n @array_function_dispatch(_partition_dispatcher)\n@@ -1399,7 +1411,7 @@ def rpartition(a, sep):\n \n     \"\"\"\n     return _to_string_or_unicode_array(\n-        _vec_string(a, object_, 'rpartition', (sep,)))\n+        _vec_string(a, object_, 'rpartition', (sep,)), a)\n \n \n def _split_dispatcher(a, sep=None, maxsplit=None):\n@@ -1829,7 +1841,7 @@ def zfill(a, width):\n     width_arr = numpy.asarray(width)\n     size = int(numpy.max(width_arr.flat))\n     return _vec_string(\n-        a_arr, (a_arr.dtype.type, size), 'zfill', (width_arr,))\n+        a_arr, type(a_arr.dtype)(size), 'zfill', (width_arr,))\n \n \n @array_function_dispatch(_unary_op_dispatcher)\n@@ -1864,7 +1876,7 @@ def isnumeric(a):\n     array([ True, False, False, False, False])\n \n     \"\"\"\n-    if _use_unicode(a) != unicode_:\n+    if not _is_unicode(a):\n         raise TypeError(\"isnumeric is only available for Unicode strings and arrays\")\n     return _vec_string(a, bool_, 'isnumeric')\n \n@@ -1901,8 +1913,9 @@ def isdecimal(a):\n     array([ True, False, False, False])\n \n     \"\"\" \n-    if _use_unicode(a) != unicode_:\n-        raise TypeError(\"isnumeric is only available for Unicode strings and arrays\")\n+    if not _is_unicode(a):\n+        raise TypeError(\n+            \"isdecimal is only available for Unicode strings and arrays\")\n     return _vec_string(a, bool_, 'isdecimal')\n \n \n",
            "comment_added_diff": {
                "327": "        # Enforce this for now.  The solution to it will be implement add",
                "328": "        # as a ufunc.  It never worked right on Python 3: bytes + unicode gave",
                "329": "        # nonsense unicode + bytes errored, and unicode + object used the",
                "330": "        # object dtype itemsize as num chars (worked on short strings).",
                "331": "        # bytes + void worked but promoting void->bytes is dubious also."
            },
            "comment_deleted_diff": {},
            "comment_modified_diff": {}
        },
        {
            "commit": "8200573ad027ffab6de1cfb04c9968d96a711ba1",
            "timestamp": "2023-10-04T15:13:53+02:00",
            "author": "Lysandros Nikolaou",
            "commit_message": "Address feedback - add comments and remove rstrip",
            "additions": 4,
            "deletions": 2,
            "change_type": "MODIFY",
            "diff": "@@ -2093,12 +2093,14 @@ def __new__(subtype, shape, itemsize=1, unicode=False, buffer=None,\n         return self\n \n     def __array_prepare__(self, arr, context=None):\n-        if arr.dtype.char in \"US\":\n+        # When calling a ufunc, we return a chararray when the output is a string-like array\n+        # or an ndarray otherwise\n+        if arr.dtype.char in \"SUbc\":\n             return arr.view(type(self))\n         return arr\n \n     def __array_wrap__(self, arr, context=None):\n-        if arr.dtype.char in \"US\":\n+        if arr.dtype.char in \"SUbc\":\n             return arr.view(type(self))\n         return arr\n \n",
            "comment_added_diff": {
                "2096": "        # When calling a ufunc, we return a chararray when the output is a string-like array",
                "2097": "        # or an ndarray otherwise"
            },
            "comment_deleted_diff": {},
            "comment_modified_diff": {
                "2096": "        if arr.dtype.char in \"US\":"
            }
        },
        {
            "commit": "a3bfa73eae47f24223a433aa10fda3a5efc4158f",
            "timestamp": "2023-10-04T15:29:15+02:00",
            "author": "Lysandros Nikolaou",
            "commit_message": "Fix lint errors",
            "additions": 2,
            "deletions": 2,
            "change_type": "MODIFY",
            "diff": "@@ -2093,8 +2093,8 @@ def __new__(subtype, shape, itemsize=1, unicode=False, buffer=None,\n         return self\n \n     def __array_prepare__(self, arr, context=None):\n-        # When calling a ufunc, we return a chararray when the output is a string-like array\n-        # or an ndarray otherwise\n+        # When calling a ufunc, we return a chararray if the ufunc output\n+        # is a string-like array, or an ndarray otherwise\n         if arr.dtype.char in \"SUbc\":\n             return arr.view(type(self))\n         return arr\n",
            "comment_added_diff": {
                "2096": "        # When calling a ufunc, we return a chararray if the ufunc output",
                "2097": "        # is a string-like array, or an ndarray otherwise"
            },
            "comment_deleted_diff": {
                "2096": "        # When calling a ufunc, we return a chararray when the output is a string-like array",
                "2097": "        # or an ndarray otherwise"
            },
            "comment_modified_diff": {
                "2096": "        # When calling a ufunc, we return a chararray when the output is a string-like array",
                "2097": "        # or an ndarray otherwise"
            }
        },
        {
            "commit": "3ab96bfa0b480be3ca9f95c293ef5f810644c91d",
            "timestamp": "2023-10-04T19:29:26+02:00",
            "author": "Lysandros Nikolaou",
            "commit_message": "ENH: Extend np.add ufunc to work with unicode and byte dtypes",
            "additions": 3,
            "deletions": 17,
            "change_type": "MODIFY",
            "diff": "@@ -319,22 +319,8 @@ def add(x1, x2):\n         of the same shape as `x1` and `x2`.\n \n     \"\"\"\n-    arr1 = numpy.asarray(x1)\n-    arr2 = numpy.asarray(x2)\n-    out_size = _get_num_chars(arr1) + _get_num_chars(arr2)\n-\n-    if type(arr1.dtype) != type(arr2.dtype):\n-        # Enforce this for now.  The solution to it will be implement add\n-        # as a ufunc.  It never worked right on Python 3: bytes + unicode gave\n-        # nonsense unicode + bytes errored, and unicode + object used the\n-        # object dtype itemsize as num chars (worked on short strings).\n-        # bytes + void worked but promoting void->bytes is dubious also.\n-        raise TypeError(\n-            \"np.char.add() requires both arrays of the same dtype kind, but \"\n-            f\"got dtypes: '{arr1.dtype}' and '{arr2.dtype}' (the few cases \"\n-            \"where this used to work often lead to incorrect results).\")\n+    return numpy.add(x1, x2)\n \n-    return _vec_string(arr1, type(arr1.dtype)(out_size), '__add__', (arr2,))\n \n def _multiply_dispatcher(a, i):\n     return (a,)\n@@ -2195,7 +2181,7 @@ def __add__(self, other):\n         --------\n         add\n         \"\"\"\n-        return asarray(add(self, other))\n+        return add(self, other)\n \n     def __radd__(self, other):\n         \"\"\"\n@@ -2206,7 +2192,7 @@ def __radd__(self, other):\n         --------\n         add\n         \"\"\"\n-        return asarray(add(numpy.asarray(other), self))\n+        return add(other, self)\n \n     def __mul__(self, i):\n         \"\"\"\n",
            "comment_added_diff": {},
            "comment_deleted_diff": {
                "327": "        # Enforce this for now.  The solution to it will be implement add",
                "328": "        # as a ufunc.  It never worked right on Python 3: bytes + unicode gave",
                "329": "        # nonsense unicode + bytes errored, and unicode + object used the",
                "330": "        # object dtype itemsize as num chars (worked on short strings).",
                "331": "        # bytes + void worked but promoting void->bytes is dubious also."
            },
            "comment_modified_diff": {}
        },
        {
            "commit": "d066e851be00f5598ce1fd78c28f6c68a2a76b4c",
            "timestamp": "2023-10-05T11:16:23+02:00",
            "author": "Lysandros Nikolaou",
            "commit_message": "Only allow same argument types in np.char.add for now",
            "additions": 12,
            "deletions": 0,
            "change_type": "MODIFY",
            "diff": "@@ -319,6 +319,18 @@ def add(x1, x2):\n         of the same shape as `x1` and `x2`.\n \n     \"\"\"\n+    arr1 = numpy.asarray(x1)\n+    arr2 = numpy.asarray(x2)\n+    if type(arr1.dtype) != type(arr2.dtype):\n+        # Enforce this for now.  The solution to it will be implement add\n+        # as a ufunc.  It never worked right on Python 3: bytes + unicode gave\n+        # nonsense unicode + bytes errored, and unicode + object used the\n+        # object dtype itemsize as num chars (worked on short strings).\n+        # bytes + void worked but promoting void->bytes is dubious also.\n+        raise TypeError(\n+            \"np.char.add() requires both arrays of the same dtype kind, but \"\n+            f\"got dtypes: '{arr1.dtype}' and '{arr2.dtype}' (the few cases \"\n+            \"where this used to work often lead to incorrect results).\")\n     return numpy.add(x1, x2)\n \n \n",
            "comment_added_diff": {
                "325": "        # Enforce this for now.  The solution to it will be implement add",
                "326": "        # as a ufunc.  It never worked right on Python 3: bytes + unicode gave",
                "327": "        # nonsense unicode + bytes errored, and unicode + object used the",
                "328": "        # object dtype itemsize as num chars (worked on short strings).",
                "329": "        # bytes + void worked but promoting void->bytes is dubious also."
            },
            "comment_deleted_diff": {},
            "comment_modified_diff": {}
        }
    ],
    "records.py": [
        {
            "commit": "4e16d865c3e1b58a4cf746d56d3f7122d7b6d5ef",
            "timestamp": "2023-07-18T11:50:24+02:00",
            "author": "Sebastian Berg",
            "commit_message": "ENH: Fixup records and void scalar printing",
            "additions": 7,
            "deletions": 2,
            "change_type": "MODIFY",
            "diff": "@@ -41,7 +41,8 @@\n from .._utils import set_module\n from . import numeric as sb\n from . import numerictypes as nt\n-from .arrayprint import _get_legacy_print_mode\n+from .arrayprint import _get_legacy_print_mode, get_formatter as _get_formatter\n+\n \n # All of the functions allow formats to be a dtype\n __all__ = [\n@@ -232,7 +233,11 @@ class record(nt.void):\n     def __repr__(self):\n         if _get_legacy_print_mode() <= 113:\n             return self.__str__()\n-        return super().__repr__()\n+        # \"Manually\" print, mainly to get the dtype \"right\"\n+        repr_dtype = sb.dtype((nt.void, self.dtype))\n+        formatter = _get_formatter(dtype=repr_dtype, fmt=\"r\")\n+        value_repr = formatter(self)\n+        return f\"np.record({value_repr}, dtype={repr_dtype!s})\"\n \n     def __str__(self):\n         if _get_legacy_print_mode() <= 113:\n",
            "comment_added_diff": {
                "236": "        # \"Manually\" print, mainly to get the dtype \"right\""
            },
            "comment_deleted_diff": {},
            "comment_modified_diff": {}
        },
        {
            "commit": "01a1d29e156a111694659137a1ac05d54bf44774",
            "timestamp": "2023-07-18T11:50:24+02:00",
            "author": "Marten van Kerkwijk",
            "commit_message": "Simplify, removing new fmt option",
            "additions": 2,
            "deletions": 7,
            "change_type": "MODIFY",
            "diff": "@@ -41,8 +41,7 @@\n from .._utils import set_module\n from . import numeric as sb\n from . import numerictypes as nt\n-from .arrayprint import _get_legacy_print_mode, get_formatter as _get_formatter\n-\n+from .arrayprint import _get_legacy_print_mode\n \n # All of the functions allow formats to be a dtype\n __all__ = [\n@@ -233,11 +232,7 @@ class record(nt.void):\n     def __repr__(self):\n         if _get_legacy_print_mode() <= 113:\n             return self.__str__()\n-        # \"Manually\" print, mainly to get the dtype \"right\"\n-        repr_dtype = sb.dtype((nt.void, self.dtype))\n-        formatter = _get_formatter(dtype=repr_dtype, fmt=repr)\n-        value_repr = formatter(self)\n-        return f\"np.record({value_repr}, dtype={repr_dtype!s})\"\n+        return super().__repr__()\n \n     def __str__(self):\n         if _get_legacy_print_mode() <= 113:\n",
            "comment_added_diff": {},
            "comment_deleted_diff": {
                "236": "        # \"Manually\" print, mainly to get the dtype \"right\""
            },
            "comment_modified_diff": {}
        },
        {
            "commit": "2391866e421a97b1f28d0221100aa5f037344857",
            "timestamp": "2023-09-17T19:35:07+02:00",
            "author": "Mateusz Sok\u00f3\u0142",
            "commit_message": "API: Remove chararray, compare_chararrays, recarray and format_parser from main namespace",
            "additions": 18,
            "deletions": 23,
            "change_type": "MODIFY",
            "diff": "@@ -45,8 +45,8 @@\n \n # All of the functions allow formats to be a dtype\n __all__ = [\n-    'record', 'recarray', 'format_parser',\n-    'fromarrays', 'fromrecords', 'fromstring', 'fromfile', 'array',\n+    'record', 'recarray', 'format_parser', 'fromarrays', 'fromrecords',\n+    'fromstring', 'fromfile', 'array',\n ]\n \n \n@@ -84,7 +84,7 @@ def find_duplicate(list):\n     ]\n \n \n-@set_module('numpy')\n+@set_module('numpy.rec')\n class format_parser:\n     \"\"\"\n     Class to convert formats, names, titles description to a dtype.\n@@ -128,18 +128,18 @@ class format_parser:\n \n     Examples\n     --------\n-    >>> np.format_parser(['<f8', '<i4', '<a5'], ['col1', 'col2', 'col3'],\n-    ...                  ['T1', 'T2', 'T3']).dtype\n+    >>> np.rec.format_parser(['<f8', '<i4', '<a5'], ['col1', 'col2', 'col3'],\n+    ...                      ['T1', 'T2', 'T3']).dtype\n     dtype([(('T1', 'col1'), '<f8'), (('T2', 'col2'), '<i4'), (('T3', 'col3'), 'S5')])\n \n     `names` and/or `titles` can be empty lists. If `titles` is an empty list,\n     titles will simply not appear. If `names` is empty, default field names\n     will be used.\n \n-    >>> np.format_parser(['f8', 'i4', 'a5'], ['col1', 'col2', 'col3'],\n-    ...                  []).dtype\n+    >>> np.rec.format_parser(['f8', 'i4', 'a5'], ['col1', 'col2', 'col3'],\n+    ...                      []).dtype\n     dtype([('col1', '<f8'), ('col2', '<i4'), ('col3', '<S5')])\n-    >>> np.format_parser(['<f8', '<i4', '<a5'], [], []).dtype\n+    >>> np.rec.format_parser(['<f8', '<i4', '<a5'], [], []).dtype\n     dtype([('f0', '<f8'), ('f1', '<i4'), ('f2', 'S5')])\n \n     \"\"\"\n@@ -375,7 +375,7 @@ class recarray(ndarray):\n     use one of the following methods:\n \n     1. Create a standard ndarray and convert it to a record array,\n-       using ``arr.view(np.recarray)``\n+       using ``arr.view(np.rec.recarray)``\n     2. Use the `buf` keyword.\n     3. Use `np.rec.fromrecords`.\n \n@@ -392,7 +392,7 @@ class recarray(ndarray):\n \n     View the array as a record array:\n \n-    >>> x = x.view(np.recarray)\n+    >>> x = x.view(np.rec.recarray)\n \n     >>> x.x\n     array([1., 3.])\n@@ -402,7 +402,7 @@ class recarray(ndarray):\n \n     Create a new, empty record array:\n \n-    >>> np.recarray((2,),\n+    >>> np.rec.recarray((2,),\n     ... dtype=[('x', int), ('y', float), ('z', int)]) #doctest: +SKIP\n     rec.array([(-1073741821, 1.2249118382103472e-301, 24547520),\n            (3471280, 1.2134086255804012e-316, 0)],\n@@ -410,11 +410,6 @@ class recarray(ndarray):\n \n     \"\"\"\n \n-    # manually set name and module so that this class's type shows\n-    # up as \"numpy.recarray\" when printed\n-    __name__ = 'recarray'\n-    __module__ = 'numpy'\n-\n     def __new__(subtype, shape, dtype=None, buf=None, offset=0, strides=None,\n                 formats=None, names=None, titles=None,\n                 byteorder=None, aligned=False, order='C'):\n@@ -607,7 +602,7 @@ def fromarrays(arrayList, dtype=None, shape=None, formats=None,\n \n     Returns\n     -------\n-    np.recarray\n+    np.rec.recarray\n         Record array consisting of given arrayList columns.\n \n     Examples\n@@ -705,7 +700,7 @@ def fromrecords(recList, dtype=None, shape=None, formats=None, names=None,\n \n     Returns\n     -------\n-    np.recarray\n+    np.rec.recarray\n         record array consisting of given recList rows.\n \n     Examples\n@@ -791,7 +786,7 @@ def fromstring(datastring, dtype=None, shape=None, offset=0, formats=None,\n \n     Returns\n     -------\n-    np.recarray\n+    np.rec.recarray\n         Record array view into the data in datastring. This will be readonly\n         if `datastring` is readonly.\n \n@@ -873,7 +868,7 @@ def fromfile(fd, dtype=None, shape=None, offset=0, formats=None,\n \n     Returns\n     -------\n-    np.recarray\n+    np.rec.recarray\n         record array consisting of data enclosed in file.\n \n     Examples\n@@ -985,16 +980,16 @@ def array(obj, dtype=None, shape=None, offset=0, strides=None, formats=None,\n \n     Returns\n     -------\n-    np.recarray\n+    np.rec.recarray\n         Record array created from the specified object.\n \n     Notes\n     -----\n-    If `obj` is ``None``, then call the `~numpy.recarray` constructor. If\n+    If `obj` is ``None``, then call the `~numpy.rec.recarray` constructor. If\n     `obj` is a string, then call the `fromstring` constructor. If `obj` is a\n     list or a tuple, then if the first object is an `~numpy.ndarray`, call\n     `fromarrays`, otherwise call `fromrecords`. If `obj` is a\n-    `~numpy.recarray`, then make a copy of the data in the recarray\n+    `~numpy.rec.recarray`, then make a copy of the data in the recarray\n     (if ``copy=True``) and use the new formats, names, and titles. If `obj`\n     is a file, then call `fromfile`. Finally, if obj is an `ndarray`, then\n     return ``obj.view(recarray)``, making a copy of the data if ``copy=True``.\n",
            "comment_added_diff": {},
            "comment_deleted_diff": {
                "413": "    # manually set name and module so that this class's type shows",
                "414": "    # up as \"numpy.recarray\" when printed"
            },
            "comment_modified_diff": {}
        }
    ],
    "_datasource.py": [
        {
            "commit": "63d9da80788d75a5c2abb8e79c34a6f6eb02ef7e",
            "timestamp": "2023-09-01T10:53:35+02:00",
            "author": "Mateusz Sok\u00f3\u0142",
            "commit_message": "API: Update `lib.polynomial` and `lib.npyio` namespaces (#24578)",
            "additions": 4,
            "deletions": 4,
            "change_type": "MODIFY",
            "diff": "@@ -192,7 +192,7 @@ def open(path, mode='r', destpath=os.curdir, encoding=None, newline=None):\n     return ds.open(path, mode, encoding=encoding, newline=newline)\n \n \n-@set_module('numpy')\n+@set_module('numpy.lib.npyio')\n class DataSource:\n     \"\"\"\n     DataSource(destpath='.')\n@@ -216,7 +216,7 @@ class DataSource:\n     URLs require a scheme string (``http://``) to be used, without it they\n     will fail::\n \n-        >>> repos = np.DataSource()\n+        >>> repos = np.lib.npyio.DataSource()\n         >>> repos.exists('www.google.com/index.html')\n         False\n         >>> repos.exists('http://www.google.com/index.html')\n@@ -228,13 +228,13 @@ class DataSource:\n     --------\n     ::\n \n-        >>> ds = np.DataSource('/home/guido')\n+        >>> ds = np.lib.npyio.DataSource('/home/guido')\n         >>> urlname = 'http://www.google.com/'\n         >>> gfile = ds.open('http://www.google.com/')\n         >>> ds.abspath(urlname)\n         '/home/guido/www.google.com/index.html'\n \n-        >>> ds = np.DataSource(None)  # use with temporary file\n+        >>> ds = np.lib.npyio.DataSource(None)  # use with temporary file\n         >>> ds.open('/home/guido/foobar.txt')\n         <open file '/home/guido.foobar.txt', mode 'r' at 0x91d4430>\n         >>> ds.abspath('/home/guido/foobar.txt')\n",
            "comment_added_diff": {
                "237": "        >>> ds = np.lib.npyio.DataSource(None)  # use with temporary file"
            },
            "comment_deleted_diff": {
                "237": "        >>> ds = np.DataSource(None)  # use with temporary file"
            },
            "comment_modified_diff": {
                "237": "        >>> ds = np.DataSource(None)  # use with temporary file"
            }
        }
    ],
    "polynomial.py": [],
    "type_check.py": [],
    "linalg.py": [
        {
            "commit": "7c361420b4f81713f593ebbb5c924121c1f2d19d",
            "timestamp": "2022-11-24T17:03:42+01:00",
            "author": "Sebastian Berg",
            "commit_message": "MAINT: Move set_module to numpy.core to use without C import",
            "additions": 2,
            "deletions": 2,
            "change_type": "MODIFY",
            "diff": "@@ -18,6 +18,7 @@\n import operator\n import warnings\n \n+from .._utils import set_module\n from numpy.core import (\n     array, asarray, zeros, empty, empty_like, intc, single, double,\n     csingle, cdouble, inexact, complexfloating, newaxis, all, Inf, dot,\n@@ -28,7 +29,6 @@\n     reciprocal\n )\n from numpy.core.multiarray import normalize_axis_index\n-from numpy.core.overrides import set_module\n from numpy.core import overrides\n from numpy.lib.twodim_base import triu, eye\n from numpy.linalg import _umath_linalg\n@@ -943,7 +943,7 @@ def qr(a, mode='reduced'):\n         return wrap(a)\n \n     # mc is the number of columns in the resulting q\n-    # matrix. If the mode is complete then it is \n+    # matrix. If the mode is complete then it is\n     # same as number of rows, and if the mode is reduced,\n     # then it is the minimum of number of rows and columns.\n     if mode == 'complete' and m > n:\n",
            "comment_added_diff": {
                "946": "    # matrix. If the mode is complete then it is"
            },
            "comment_deleted_diff": {
                "946": "    # matrix. If the mode is complete then it is"
            },
            "comment_modified_diff": {
                "946": "    # matrix. If the mode is complete then it is"
            }
        },
        {
            "commit": "18e88b4a89a4cbc59bbb00423c957537fa9cdb58",
            "timestamp": "2022-12-12T18:23:04-07:00",
            "author": "Aaron Meurer",
            "commit_message": "Add namedtuple return types to linalg functions that return tuples\n\nThat is, eig(), eigh(), qr(), slogdet(), and svd(). For those functions that\nreturn non-tuples with certain keyword arguments, the return type is\nunchanged. This change should be completely backwards compatible.\n\nThe namedtuple attribute names come from the array API specification (see,\ne.g.,\nhttps://data-apis.org/array-api/latest/extensions/generated/signatures.linalg.eigh.html),\nwith the exception of eig() which is just the same as eigh(). The name of the\nnamedtuple object itself is not part of the specification or the public API.\n\nI have not used a namedtuple for the tuple output for qr(mode='raw'), which\nreturns (h, tau).\n\nThis updates the docstrings to use the updated namedtuple return names, and\nalso the examples to use those names more consistently. This also updates the\ntests to check each function for the namedtuple attributes at least once.",
            "additions": 148,
            "deletions": 107,
            "change_type": "MODIFY",
            "diff": "@@ -17,6 +17,7 @@\n import functools\n import operator\n import warnings\n+from typing import NamedTuple\n \n from .._utils import set_module\n from numpy.core import (\n@@ -33,6 +34,28 @@\n from numpy.lib.twodim_base import triu, eye\n from numpy.linalg import _umath_linalg\n \n+from numpy._typing import NDArray\n+\n+class EigResult(NamedTuple):\n+    eigenvalues: NDArray\n+    eigenvectors: NDArray\n+\n+class EighResult(NamedTuple):\n+    eigenvalues: NDArray\n+    eigenvectors: NDArray\n+\n+class QRResult(NamedTuple):\n+    Q: NDArray\n+    R: NDArray\n+\n+class SlogdetResult(NamedTuple):\n+    sign: NDArray\n+    logabsdet: NDArray\n+\n+class SVDResult(NamedTuple):\n+    U: NDArray\n+    S: NDArray\n+    Vh: NDArray\n \n array_function_dispatch = functools.partial(\n     overrides.array_function_dispatch, module='numpy.linalg')\n@@ -778,10 +801,9 @@ def qr(a, mode='reduced'):\n     mode : {'reduced', 'complete', 'r', 'raw'}, optional\n         If K = min(M, N), then\n \n-        * 'reduced'  : returns q, r with dimensions\n-                       (..., M, K), (..., K, N) (default)\n-        * 'complete' : returns q, r with dimensions (..., M, M), (..., M, N)\n-        * 'r'        : returns r only with dimensions (..., K, N)\n+        * 'reduced'  : returns Q, R with dimensions (..., M, K), (..., K, N) (default)\n+        * 'complete' : returns Q, R with dimensions (..., M, M), (..., M, N)\n+        * 'r'        : returns R only with dimensions (..., K, N)\n         * 'raw'      : returns h, tau with dimensions (..., N, M), (..., K,)\n \n         The options 'reduced', 'complete, and 'raw' are new in numpy 1.8,\n@@ -797,14 +819,17 @@ def qr(a, mode='reduced'):\n \n     Returns\n     -------\n-    q : ndarray of float or complex, optional\n+    When mode is 'reduced' or 'complete', the result will be a namedtuple with\n+    the attributes `Q` and `R`.\n+\n+    Q : ndarray of float or complex, optional\n         A matrix with orthonormal columns. When mode = 'complete' the\n         result is an orthogonal/unitary matrix depending on whether or not\n         a is real/complex. The determinant may be either +/- 1 in that\n         case. In case the number of dimensions in the input array is\n         greater than 2 then a stack of the matrices with above properties\n         is returned.\n-    r : ndarray of float or complex, optional\n+    R : ndarray of float or complex, optional\n         The upper-triangular matrix or a stack of upper-triangular\n         matrices if the number of dimensions in the input array is greater\n         than 2.\n@@ -849,19 +874,19 @@ def qr(a, mode='reduced'):\n     Examples\n     --------\n     >>> a = np.random.randn(9, 6)\n-    >>> q, r = np.linalg.qr(a)\n-    >>> np.allclose(a, np.dot(q, r))  # a does equal qr\n+    >>> Q, R = np.linalg.qr(a)\n+    >>> np.allclose(a, np.dot(Q, R))  # a does equal QR\n     True\n-    >>> r2 = np.linalg.qr(a, mode='r')\n-    >>> np.allclose(r, r2)  # mode='r' returns the same r as mode='full'\n+    >>> R2 = np.linalg.qr(a, mode='r')\n+    >>> np.allclose(R, R2)  # mode='r' returns the same R as mode='full'\n     True\n     >>> a = np.random.normal(size=(3, 2, 2)) # Stack of 2 x 2 matrices as input\n-    >>> q, r = np.linalg.qr(a)\n-    >>> q.shape\n+    >>> Q, R = np.linalg.qr(a)\n+    >>> Q.shape\n     (3, 2, 2)\n-    >>> r.shape\n+    >>> R.shape\n     (3, 2, 2)\n-    >>> np.allclose(a, np.matmul(q, r))\n+    >>> np.allclose(a, np.matmul(Q, R))\n     True\n \n     Example illustrating a common use of `qr`: solving of least squares\n@@ -876,8 +901,8 @@ def qr(a, mode='reduced'):\n       x = array([[y0], [m]])\n       b = array([[1], [0], [2], [1]])\n \n-    If A = qr such that q is orthonormal (which is always possible via\n-    Gram-Schmidt), then ``x = inv(r) * (q.T) * b``.  (In numpy practice,\n+    If A = QR such that Q is orthonormal (which is always possible via\n+    Gram-Schmidt), then ``x = inv(R) * (Q.T) * b``.  (In numpy practice,\n     however, we simply use `lstsq`.)\n \n     >>> A = np.array([[0, 1], [1, 1], [1, 1], [2, 1]])\n@@ -887,9 +912,9 @@ def qr(a, mode='reduced'):\n            [1, 1],\n            [2, 1]])\n     >>> b = np.array([1, 2, 2, 3])\n-    >>> q, r = np.linalg.qr(A)\n-    >>> p = np.dot(q.T, b)\n-    >>> np.dot(np.linalg.inv(r), p)\n+    >>> Q, R = np.linalg.qr(A)\n+    >>> p = np.dot(Q.T, b)\n+    >>> np.dot(np.linalg.inv(R), p)\n     array([  1.,   1.])\n \n     \"\"\"\n@@ -961,7 +986,7 @@ def qr(a, mode='reduced'):\n     q = q.astype(result_t, copy=False)\n     r = r.astype(result_t, copy=False)\n \n-    return wrap(q), wrap(r)\n+    return QRResult(wrap(q), wrap(r))\n \n # Eigenvalues\n \n@@ -1178,7 +1203,9 @@ def eig(a):\n \n     Returns\n     -------\n-    w : (..., M) array\n+    A namedtuple with the following attributes:\n+\n+    eigenvalues : (..., M) array\n         The eigenvalues, each repeated according to its multiplicity.\n         The eigenvalues are not necessarily ordered. The resulting\n         array will be of complex type, unless the imaginary part is\n@@ -1186,10 +1213,10 @@ def eig(a):\n         is real the resulting eigenvalues will be real (0 imaginary\n         part) or occur in conjugate pairs\n \n-    v : (..., M, M) array\n+    eigenvectors : (..., M, M) array\n         The normalized (unit \"length\") eigenvectors, such that the\n-        column ``v[:,i]`` is the eigenvector corresponding to the\n-        eigenvalue ``w[i]``.\n+        column ``eigenvectors[:,i]`` is the eigenvector corresponding to the\n+        eigenvalue ``eigenvalues[i]``.\n \n     Raises\n     ------\n@@ -1219,30 +1246,30 @@ def eig(a):\n     This is implemented using the ``_geev`` LAPACK routines which compute\n     the eigenvalues and eigenvectors of general square arrays.\n \n-    The number `w` is an eigenvalue of `a` if there exists a vector\n-    `v` such that ``a @ v = w * v``. Thus, the arrays `a`, `w`, and\n-    `v` satisfy the equations ``a @ v[:,i] = w[i] * v[:,i]``\n-    for :math:`i \\\\in \\\\{0,...,M-1\\\\}`.\n+    The number `w` is an eigenvalue of `a` if there exists a vector `v` such\n+    that ``a @ v = w * v``. Thus, the arrays `a`, `eigenvalues`, and\n+    `eigenvectors` satisfy the equations ``a @ eigenvectors[:,i] =\n+    eigenvalues[i] * eigenvalues[:,i]`` for :math:`i \\\\in \\\\{0,...,M-1\\\\}`.\n \n-    The array `v` of eigenvectors may not be of maximum rank, that is, some\n-    of the columns may be linearly dependent, although round-off error may\n-    obscure that fact. If the eigenvalues are all different, then theoretically\n-    the eigenvectors are linearly independent and `a` can be diagonalized by\n-    a similarity transformation using `v`, i.e, ``inv(v) @ a @ v`` is diagonal.\n+    The array `eigenvectors` may not be of maximum rank, that is, some of the\n+    columns may be linearly dependent, although round-off error may obscure\n+    that fact. If the eigenvalues are all different, then theoretically the\n+    eigenvectors are linearly independent and `a` can be diagonalized by a\n+    similarity transformation using `eigenvectors`, i.e, ``inv(eigenvectors) @\n+    a @ eigenvectors`` is diagonal.\n \n     For non-Hermitian normal matrices the SciPy function `scipy.linalg.schur`\n-    is preferred because the matrix `v` is guaranteed to be unitary, which is\n-    not the case when using `eig`. The Schur factorization produces an\n-    upper triangular matrix rather than a diagonal matrix, but for normal\n-    matrices only the diagonal of the upper triangular matrix is needed, the\n-    rest is roundoff error.\n-\n-    Finally, it is emphasized that `v` consists of the *right* (as in\n-    right-hand side) eigenvectors of `a`.  A vector `y` satisfying\n-    ``y.T @ a = z * y.T`` for some number `z` is called a *left*\n-    eigenvector of `a`, and, in general, the left and right eigenvectors\n-    of a matrix are not necessarily the (perhaps conjugate) transposes\n-    of each other.\n+    is preferred because the matrix `eigenvectors` is guaranteed to be\n+    unitary, which is not the case when using `eig`. The Schur factorization\n+    produces an upper triangular matrix rather than a diagonal matrix, but for\n+    normal matrices only the diagonal of the upper triangular matrix is\n+    needed, the rest is roundoff error.\n+\n+    Finally, it is emphasized that `eigenvectors` consists of the *right* (as\n+    in right-hand side) eigenvectors of `a`. A vector `y` satisfying ``y.T @ a\n+    = z * y.T`` for some number `z` is called a *left* eigenvector of `a`,\n+    and, in general, the left and right eigenvectors of a matrix are not\n+    necessarily the (perhaps conjugate) transposes of each other.\n \n     References\n     ----------\n@@ -1253,41 +1280,45 @@ def eig(a):\n     --------\n     >>> from numpy import linalg as LA\n \n-    (Almost) trivial example with real e-values and e-vectors.\n+    (Almost) trivial example with real eigenvalues and eigenvectors.\n \n-    >>> w, v = LA.eig(np.diag((1, 2, 3)))\n-    >>> w; v\n+    >>> eigenvalues, eigenvectors = LA.eig(np.diag((1, 2, 3)))\n+    >>> eigenvalues\n     array([1., 2., 3.])\n+    >>> eigenvectors\n     array([[1., 0., 0.],\n            [0., 1., 0.],\n            [0., 0., 1.]])\n \n-    Real matrix possessing complex e-values and e-vectors; note that the\n-    e-values are complex conjugates of each other.\n+    Real matrix possessing complex eigenvalues and eigenvectors; note that the\n+    eigenvalues are complex conjugates of each other.\n \n-    >>> w, v = LA.eig(np.array([[1, -1], [1, 1]]))\n-    >>> w; v\n+    >>> eigenvalues, eigenvectors = LA.eig(np.array([[1, -1], [1, 1]]))\n+    >>> eigenvalues\n     array([1.+1.j, 1.-1.j])\n+    >>> eigenvectors\n     array([[0.70710678+0.j        , 0.70710678-0.j        ],\n            [0.        -0.70710678j, 0.        +0.70710678j]])\n \n-    Complex-valued matrix with real e-values (but complex-valued e-vectors);\n+    Complex-valued matrix with real eigenvalues (but complex-valued eigenvectors);\n     note that ``a.conj().T == a``, i.e., `a` is Hermitian.\n \n     >>> a = np.array([[1, 1j], [-1j, 1]])\n-    >>> w, v = LA.eig(a)\n-    >>> w; v\n+    >>> eigenvalues, eigenvectors = LA.eig(a)\n+    >>> eigenvalues\n     array([2.+0.j, 0.+0.j])\n+    >>> eigenvectors\n     array([[ 0.        +0.70710678j,  0.70710678+0.j        ], # may vary\n            [ 0.70710678+0.j        , -0.        +0.70710678j]])\n \n     Be careful about round-off error!\n \n     >>> a = np.array([[1 + 1e-9, 0], [0, 1 - 1e-9]])\n-    >>> # Theor. e-values are 1 +/- 1e-9\n-    >>> w, v = LA.eig(a)\n-    >>> w; v\n+    >>> # Theor. eigenvalues are 1 +/- 1e-9\n+    >>> eigenvalues, eigenvectors = LA.eig(a)\n+    >>> eigenvalues\n     array([1., 1.])\n+    >>> eigenvectors\n     array([[1., 0.],\n            [0., 1.]])\n \n@@ -1311,7 +1342,7 @@ def eig(a):\n         result_t = _complexType(result_t)\n \n     vt = vt.astype(result_t, copy=False)\n-    return w.astype(result_t, copy=False), wrap(vt)\n+    return EigResult(w.astype(result_t, copy=False), wrap(vt))\n \n \n @array_function_dispatch(_eigvalsh_dispatcher)\n@@ -1339,13 +1370,15 @@ def eigh(a, UPLO='L'):\n \n     Returns\n     -------\n-    w : (..., M) ndarray\n+    A namedtuple with the following attributes:\n+\n+    eigenvalues : (..., M) ndarray\n         The eigenvalues in ascending order, each repeated according to\n         its multiplicity.\n-    v : {(..., M, M) ndarray, (..., M, M) matrix}\n-        The column ``v[:, i]`` is the normalized eigenvector corresponding\n-        to the eigenvalue ``w[i]``.  Will return a matrix object if `a` is\n-        a matrix object.\n+    eigenvectors : {(..., M, M) ndarray, (..., M, M) matrix}\n+        The column ``eigenvectors[:, i]`` is the normalized eigenvector\n+        corresponding to the eigenvalue ``eigenvalues[i]``.  Will return a\n+        matrix object if `a` is a matrix object.\n \n     Raises\n     ------\n@@ -1372,10 +1405,10 @@ def eigh(a, UPLO='L'):\n     The eigenvalues/eigenvectors are computed using LAPACK routines ``_syevd``,\n     ``_heevd``.\n \n-    The eigenvalues of real symmetric or complex Hermitian matrices are\n-    always real. [1]_ The array `v` of (column) eigenvectors is unitary\n-    and `a`, `w`, and `v` satisfy the equations\n-    ``dot(a, v[:, i]) = w[i] * v[:, i]``.\n+    The eigenvalues of real symmetric or complex Hermitian matrices are always\n+    real. [1]_ The array `eigenvalues` of (column) eigenvectors is unitary and\n+    `a`, `eigenvalues`, and `eigenvectors` satisfy the equations ``dot(a,\n+    eigenvectors[:, i]) = eigenvalues[i] * eigenvectors[:, i]``.\n \n     References\n     ----------\n@@ -1389,24 +1422,26 @@ def eigh(a, UPLO='L'):\n     >>> a\n     array([[ 1.+0.j, -0.-2.j],\n            [ 0.+2.j,  5.+0.j]])\n-    >>> w, v = LA.eigh(a)\n-    >>> w; v\n+    >>> eigenvalues, eigenvectors = LA.eigh(a)\n+    >>> eigenvalues\n     array([0.17157288, 5.82842712])\n+    >>> eigenvectors\n     array([[-0.92387953+0.j        , -0.38268343+0.j        ], # may vary\n            [ 0.        +0.38268343j,  0.        -0.92387953j]])\n \n-    >>> np.dot(a, v[:, 0]) - w[0] * v[:, 0] # verify 1st e-val/vec pair\n+    >>> np.dot(a, eigenvectors[:, 0]) - eigenvalues[0] * eigenvectors[:, 0] # verify 1st eigenval/vec pair\n     array([5.55111512e-17+0.0000000e+00j, 0.00000000e+00+1.2490009e-16j])\n-    >>> np.dot(a, v[:, 1]) - w[1] * v[:, 1] # verify 2nd e-val/vec pair\n+    >>> np.dot(a, eigenvectors[:, 1]) - eigenvalues[1] * eigenvectors[:, 1] # verify 2nd eigenval/vec pair\n     array([0.+0.j, 0.+0.j])\n \n     >>> A = np.matrix(a) # what happens if input is a matrix object\n     >>> A\n     matrix([[ 1.+0.j, -0.-2.j],\n             [ 0.+2.j,  5.+0.j]])\n-    >>> w, v = LA.eigh(A)\n-    >>> w; v\n+    >>> eigenvalues, eigenvectors = LA.eigh(A)\n+    >>> eigenvalues\n     array([0.17157288, 5.82842712])\n+    >>> eigenvectors\n     matrix([[-0.92387953+0.j        , -0.38268343+0.j        ], # may vary\n             [ 0.        +0.38268343j,  0.        -0.92387953j]])\n \n@@ -1430,6 +1465,7 @@ def eigh(a, UPLO='L'):\n            [ 0.        +0.89442719j,  0.        -0.4472136j ]])\n     array([[ 0.89442719+0.j       , -0.        +0.4472136j],\n            [-0.        +0.4472136j,  0.89442719+0.j       ]])\n+\n     \"\"\"\n     UPLO = UPLO.upper()\n     if UPLO not in ('L', 'U'):\n@@ -1451,7 +1487,7 @@ def eigh(a, UPLO='L'):\n     w, vt = gufunc(a, signature=signature, extobj=extobj)\n     w = w.astype(_realType(result_t), copy=False)\n     vt = vt.astype(result_t, copy=False)\n-    return w, wrap(vt)\n+    return EighResult(w, wrap(vt))\n \n \n # Singular value decomposition\n@@ -1493,16 +1529,19 @@ def svd(a, full_matrices=True, compute_uv=True, hermitian=False):\n \n     Returns\n     -------\n-    u : { (..., M, M), (..., M, K) } array\n+    When `compute_uv` is True, the result is a namedtuple with the following\n+    attribute names:\n+\n+    U : { (..., M, M), (..., M, K) } array\n         Unitary array(s). The first ``a.ndim - 2`` dimensions have the same\n         size as those of the input `a`. The size of the last two dimensions\n         depends on the value of `full_matrices`. Only returned when\n         `compute_uv` is True.\n-    s : (..., K) array\n+    S : (..., K) array\n         Vector(s) with the singular values, within each vector sorted in\n         descending order. The first ``a.ndim - 2`` dimensions have the same\n         size as those of the input `a`.\n-    vh : { (..., N, N), (..., K, N) } array\n+    Vh : { (..., N, N), (..., K, N) } array\n         Unitary array(s). The first ``a.ndim - 2`` dimensions have the same\n         size as those of the input `a`. The size of the last two dimensions\n         depends on the value of `full_matrices`. Only returned when\n@@ -1555,45 +1594,45 @@ def svd(a, full_matrices=True, compute_uv=True, hermitian=False):\n \n     Reconstruction based on full SVD, 2D case:\n \n-    >>> u, s, vh = np.linalg.svd(a, full_matrices=True)\n-    >>> u.shape, s.shape, vh.shape\n+    >>> U, S, Vh = np.linalg.svd(a, full_matrices=True)\n+    >>> U.shape, S.shape, Vh.shape\n     ((9, 9), (6,), (6, 6))\n-    >>> np.allclose(a, np.dot(u[:, :6] * s, vh))\n+    >>> np.allclose(a, np.dot(U[:, :6] * S, Vh))\n     True\n     >>> smat = np.zeros((9, 6), dtype=complex)\n-    >>> smat[:6, :6] = np.diag(s)\n-    >>> np.allclose(a, np.dot(u, np.dot(smat, vh)))\n+    >>> smat[:6, :6] = np.diag(S)\n+    >>> np.allclose(a, np.dot(U, np.dot(smat, Vh)))\n     True\n \n     Reconstruction based on reduced SVD, 2D case:\n \n-    >>> u, s, vh = np.linalg.svd(a, full_matrices=False)\n-    >>> u.shape, s.shape, vh.shape\n+    >>> U, S, Vh = np.linalg.svd(a, full_matrices=False)\n+    >>> U.shape, S.shape, Vh.shape\n     ((9, 6), (6,), (6, 6))\n-    >>> np.allclose(a, np.dot(u * s, vh))\n+    >>> np.allclose(a, np.dot(U * S, Vh))\n     True\n-    >>> smat = np.diag(s)\n-    >>> np.allclose(a, np.dot(u, np.dot(smat, vh)))\n+    >>> smat = np.diag(S)\n+    >>> np.allclose(a, np.dot(U, np.dot(smat, Vh)))\n     True\n \n     Reconstruction based on full SVD, 4D case:\n \n-    >>> u, s, vh = np.linalg.svd(b, full_matrices=True)\n-    >>> u.shape, s.shape, vh.shape\n+    >>> U, S, Vh = np.linalg.svd(b, full_matrices=True)\n+    >>> U.shape, S.shape, Vh.shape\n     ((2, 7, 8, 8), (2, 7, 3), (2, 7, 3, 3))\n-    >>> np.allclose(b, np.matmul(u[..., :3] * s[..., None, :], vh))\n+    >>> np.allclose(b, np.matmul(U[..., :3] * S[..., None, :], Vh))\n     True\n-    >>> np.allclose(b, np.matmul(u[..., :3], s[..., None] * vh))\n+    >>> np.allclose(b, np.matmul(U[..., :3], S[..., None] * Vh))\n     True\n \n     Reconstruction based on reduced SVD, 4D case:\n \n-    >>> u, s, vh = np.linalg.svd(b, full_matrices=False)\n-    >>> u.shape, s.shape, vh.shape\n+    >>> U, S, Vh = np.linalg.svd(b, full_matrices=False)\n+    >>> U.shape, S.shape, Vh.shape\n     ((2, 7, 8, 3), (2, 7, 3), (2, 7, 3, 3))\n-    >>> np.allclose(b, np.matmul(u * s[..., None, :], vh))\n+    >>> np.allclose(b, np.matmul(U * S[..., None, :], Vh))\n     True\n-    >>> np.allclose(b, np.matmul(u, s[..., None] * vh))\n+    >>> np.allclose(b, np.matmul(U, S[..., None] * Vh))\n     True\n \n     \"\"\"\n@@ -1614,7 +1653,7 @@ def svd(a, full_matrices=True, compute_uv=True, hermitian=False):\n             u = _nx.take_along_axis(u, sidx[..., None, :], axis=-1)\n             # singular values are unsigned, move the sign into v\n             vt = transpose(u * sgn[..., None, :]).conjugate()\n-            return wrap(u), s, wrap(vt)\n+            return SVDResult(wrap(u), s, wrap(vt))\n         else:\n             s = eigvalsh(a)\n             s = abs(s)\n@@ -1643,7 +1682,7 @@ def svd(a, full_matrices=True, compute_uv=True, hermitian=False):\n         u = u.astype(result_t, copy=False)\n         s = s.astype(_realType(result_t), copy=False)\n         vh = vh.astype(result_t, copy=False)\n-        return wrap(u), s, wrap(vh)\n+        return SVDResult(wrap(u), s, wrap(vh))\n     else:\n         if m < n:\n             gufunc = _umath_linalg.svd_m\n@@ -2012,15 +2051,17 @@ def slogdet(a):\n \n     Returns\n     -------\n+    A namedtuple with the following attributes:\n+\n     sign : (...) array_like\n         A number representing the sign of the determinant. For a real matrix,\n         this is 1, 0, or -1. For a complex matrix, this is a complex number\n         with absolute value 1 (i.e., it is on the unit circle), or else 0.\n-    logdet : (...) array_like\n+    logabsdet : (...) array_like\n         The natural log of the absolute value of the determinant.\n \n-    If the determinant is zero, then `sign` will be 0 and `logdet` will be\n-    -Inf. In all cases, the determinant is equal to ``sign * np.exp(logdet)``.\n+    If the determinant is zero, then `sign` will be 0 and `logabsdet` will be\n+    -Inf. In all cases, the determinant is equal to ``sign * np.exp(logabsdet)``.\n \n     See Also\n     --------\n@@ -2045,10 +2086,10 @@ def slogdet(a):\n     The determinant of a 2-D array ``[[a, b], [c, d]]`` is ``ad - bc``:\n \n     >>> a = np.array([[1, 2], [3, 4]])\n-    >>> (sign, logdet) = np.linalg.slogdet(a)\n-    >>> (sign, logdet)\n+    >>> (sign, logabsdet) = np.linalg.slogdet(a)\n+    >>> (sign, logabsdet)\n     (-1, 0.69314718055994529) # may vary\n-    >>> sign * np.exp(logdet)\n+    >>> sign * np.exp(logabsdet)\n     -2.0\n \n     Computing log-determinants for a stack of matrices:\n@@ -2056,8 +2097,8 @@ def slogdet(a):\n     >>> a = np.array([ [[1, 2], [3, 4]], [[1, 2], [2, 1]], [[1, 3], [3, 1]] ])\n     >>> a.shape\n     (3, 2, 2)\n-    >>> sign, logdet = np.linalg.slogdet(a)\n-    >>> (sign, logdet)\n+    >>> sign, logabsdet = np.linalg.slogdet(a)\n+    >>> (sign, logabsdet)\n     (array([-1., -1., -1.]), array([ 0.69314718,  1.09861229,  2.07944154]))\n     >>> sign * np.exp(logdet)\n     array([-2., -3., -8.])\n@@ -2079,7 +2120,7 @@ def slogdet(a):\n     sign, logdet = _umath_linalg.slogdet(a, signature=signature)\n     sign = sign.astype(result_t, copy=False)\n     logdet = logdet.astype(real_t, copy=False)\n-    return sign, logdet\n+    return SlogdetResult(sign, logdet)\n \n \n @array_function_dispatch(_unary_dispatcher)\n",
            "comment_added_diff": {
                "878": "    >>> np.allclose(a, np.dot(Q, R))  # a does equal QR",
                "881": "    >>> np.allclose(R, R2)  # mode='r' returns the same R as mode='full'",
                "1317": "    >>> # Theor. eigenvalues are 1 +/- 1e-9",
                "1432": "    >>> np.dot(a, eigenvectors[:, 0]) - eigenvalues[0] * eigenvectors[:, 0] # verify 1st eigenval/vec pair",
                "1434": "    >>> np.dot(a, eigenvectors[:, 1]) - eigenvalues[1] * eigenvectors[:, 1] # verify 2nd eigenval/vec pair"
            },
            "comment_deleted_diff": {
                "853": "    >>> np.allclose(a, np.dot(q, r))  # a does equal qr",
                "856": "    >>> np.allclose(r, r2)  # mode='r' returns the same r as mode='full'",
                "1287": "    >>> # Theor. e-values are 1 +/- 1e-9",
                "1398": "    >>> np.dot(a, v[:, 0]) - w[0] * v[:, 0] # verify 1st e-val/vec pair",
                "1400": "    >>> np.dot(a, v[:, 1]) - w[1] * v[:, 1] # verify 2nd e-val/vec pair"
            },
            "comment_modified_diff": {}
        },
        {
            "commit": "4e2a03ab936ac5035640df75e71965074c7d84c6",
            "timestamp": "2023-06-05T18:55:54-06:00",
            "author": "Aaron Meurer",
            "commit_message": "Add the dtype argument to numpy.array_api.linalg.trace",
            "additions": 19,
            "deletions": 4,
            "change_type": "MODIFY",
            "diff": "@@ -1,6 +1,13 @@\n from __future__ import annotations\n \n-from ._dtypes import _floating_dtypes, _numeric_dtypes\n+from ._dtypes import (\n+    _floating_dtypes,\n+    _numeric_dtypes,\n+    float32,\n+    float64,\n+    complex64,\n+    complex128\n+)\n from ._manipulation_functions import reshape\n from ._array_object import Array\n \n@@ -8,7 +15,7 @@\n \n from typing import TYPE_CHECKING\n if TYPE_CHECKING:\n-    from ._typing import Literal, Optional, Sequence, Tuple, Union\n+    from ._typing import Literal, Optional, Sequence, Tuple, Union, Dtype\n \n from typing import NamedTuple\n \n@@ -363,7 +370,7 @@ def tensordot(x1: Array, x2: Array, /, *, axes: Union[int, Tuple[Sequence[int],\n     return Array._new(np.tensordot(x1._array, x2._array, axes=axes))\n \n # Note: trace is the numpy top-level namespace, not np.linalg\n-def trace(x: Array, /, *, offset: int = 0) -> Array:\n+def trace(x: Array, /, *, offset: int = 0, dtype: Optional[Dtype] = None) -> Array:\n     \"\"\"\n     Array API compatible wrapper for :py:func:`np.trace <numpy.trace>`.\n \n@@ -371,9 +378,17 @@ def trace(x: Array, /, *, offset: int = 0) -> Array:\n     \"\"\"\n     if x.dtype not in _numeric_dtypes:\n         raise TypeError('Only numeric dtypes are allowed in trace')\n+\n+    # Note: trace() works the same as sum() and prod() (see\n+    # _statistical_functions.py)\n+    if dtype is None:\n+        if x.dtype == float32:\n+            dtype = float64\n+        elif x.dtype == complex64:\n+            dtype = complex128\n     # Note: trace always operates on the last two axes, whereas np.trace\n     # operates on the first two axes by default\n-    return Array._new(np.asarray(np.trace(x._array, offset=offset, axis1=-2, axis2=-1)))\n+    return Array._new(np.asarray(np.trace(x._array, offset=offset, axis1=-2, axis2=-1, dtype=dtype)))\n \n # Note: vecdot is not in NumPy\n def vecdot(x1: Array, x2: Array, /, *, axis: int = -1) -> Array:\n",
            "comment_added_diff": {
                "382": "    # Note: trace() works the same as sum() and prod() (see",
                "383": "    # _statistical_functions.py)"
            },
            "comment_deleted_diff": {},
            "comment_modified_diff": {}
        },
        {
            "commit": "2c58ab2ba592155f0fdaeb38592d55656bc1637a",
            "timestamp": "2023-06-13T09:26:30+02:00",
            "author": "Sebastian Berg",
            "commit_message": "MAINT: Use `errstate` rather than `extobj` for linalg ufunc calls\n\nThis may slow things down very slightly, but we should be able to\nrecover this easily and the `extobj=` API is a bit strange.",
            "additions": 44,
            "deletions": 47,
            "change_type": "MODIFY",
            "diff": "@@ -24,7 +24,7 @@\n     array, asarray, zeros, empty, empty_like, intc, single, double,\n     csingle, cdouble, inexact, complexfloating, newaxis, all, Inf, dot,\n     add, multiply, sqrt, sum, isfinite,\n-    finfo, errstate, geterrobj, moveaxis, amin, amax, prod, abs,\n+    finfo, errstate, moveaxis, amin, amax, prod, abs,\n     atleast_2d, intp, asanyarray, object_, matmul,\n     swapaxes, divide, count_nonzero, isnan, sign, argsort, sort,\n     reciprocal\n@@ -94,20 +94,6 @@ class LinAlgError(ValueError):\n     \"\"\"\n \n \n-def _determine_error_states():\n-    errobj = geterrobj()\n-    bufsize = errobj[0]\n-\n-    with errstate(invalid='call', over='ignore',\n-                  divide='ignore', under='ignore'):\n-        invalid_call_errmask = geterrobj()[1]\n-\n-    return [bufsize, invalid_call_errmask, None]\n-\n-# Dealing with errors in _umath_linalg\n-_linalg_error_extobj = _determine_error_states()\n-del _determine_error_states\n-\n def _raise_linalgerror_singular(err, flag):\n     raise LinAlgError(\"Singular matrix\")\n \n@@ -127,10 +113,6 @@ def _raise_linalgerror_qr(err, flag):\n     raise LinAlgError(\"Incorrect argument found while performing \"\n                       \"QR factorization\")\n \n-def get_linalg_error_extobj(callback):\n-    extobj = list(_linalg_error_extobj)  # make a copy\n-    extobj[2] = callback\n-    return extobj\n \n def _makearray(a):\n     new = asarray(a)\n@@ -405,8 +387,9 @@ def solve(a, b):\n         gufunc = _umath_linalg.solve\n \n     signature = 'DD->D' if isComplexType(t) else 'dd->d'\n-    extobj = get_linalg_error_extobj(_raise_linalgerror_singular)\n-    r = gufunc(a, b, signature=signature, extobj=extobj)\n+    with errstate(call=_raise_linalgerror_singular, invalid='call',\n+                  over='ignore', divide='ignore', under='ignore'):\n+        r = gufunc(a, b, signature=signature)\n \n     return wrap(r.astype(result_t, copy=False))\n \n@@ -557,8 +540,9 @@ def inv(a):\n     t, result_t = _commonType(a)\n \n     signature = 'D->D' if isComplexType(t) else 'd->d'\n-    extobj = get_linalg_error_extobj(_raise_linalgerror_singular)\n-    ainv = _umath_linalg.inv(a, signature=signature, extobj=extobj)\n+    with errstate(call=_raise_linalgerror_singular, invalid='call',\n+                  over='ignore', divide='ignore', under='ignore'):\n+        ainv = _umath_linalg.inv(a, signature=signature)\n     return wrap(ainv.astype(result_t, copy=False))\n \n \n@@ -769,14 +753,15 @@ def cholesky(a):\n             [ 0.+2.j,  1.+0.j]])\n \n     \"\"\"\n-    extobj = get_linalg_error_extobj(_raise_linalgerror_nonposdef)\n     gufunc = _umath_linalg.cholesky_lo\n     a, wrap = _makearray(a)\n     _assert_stacked_2d(a)\n     _assert_stacked_square(a)\n     t, result_t = _commonType(a)\n     signature = 'D->D' if isComplexType(t) else 'd->d'\n-    r = gufunc(a, signature=signature, extobj=extobj)\n+    with errstate(call=_raise_linalgerror_nonposdef, invalid='call',\n+                  over='ignore', divide='ignore', under='ignore'):\n+        r = gufunc(a, signature=signature)\n     return wrap(r.astype(result_t, copy=False))\n \n \n@@ -948,8 +933,9 @@ def qr(a, mode='reduced'):\n         gufunc = _umath_linalg.qr_r_raw_n\n \n     signature = 'D->D' if isComplexType(t) else 'd->d'\n-    extobj = get_linalg_error_extobj(_raise_linalgerror_qr)\n-    tau = gufunc(a, signature=signature, extobj=extobj)\n+    with errstate(call=_raise_linalgerror_qr, invalid='call',\n+                  over='ignore', divide='ignore', under='ignore'):\n+        tau = gufunc(a, signature=signature)\n \n     # handle modes that don't return q\n     if mode == 'r':\n@@ -979,8 +965,9 @@ def qr(a, mode='reduced'):\n         gufunc = _umath_linalg.qr_reduced\n \n     signature = 'DD->D' if isComplexType(t) else 'dd->d'\n-    extobj = get_linalg_error_extobj(_raise_linalgerror_qr)\n-    q = gufunc(a, tau, signature=signature, extobj=extobj)\n+    with errstate(call=_raise_linalgerror_qr, invalid='call',\n+                  over='ignore', divide='ignore', under='ignore'):\n+        q = gufunc(a, tau, signature=signature)\n     r = triu(a[..., :mc, :])\n \n     q = q.astype(result_t, copy=False)\n@@ -1068,10 +1055,11 @@ def eigvals(a):\n     _assert_finite(a)\n     t, result_t = _commonType(a)\n \n-    extobj = get_linalg_error_extobj(\n-        _raise_linalgerror_eigenvalues_nonconvergence)\n     signature = 'D->D' if isComplexType(t) else 'd->D'\n-    w = _umath_linalg.eigvals(a, signature=signature, extobj=extobj)\n+    with errstate(call=_raise_linalgerror_eigenvalues_nonconvergence,\n+                  invalid='call', over='ignore', divide='ignore',\n+                  under='ignore'):\n+        w = _umath_linalg.eigvals(a, signature=signature)\n \n     if not isComplexType(t):\n         if all(w.imag == 0):\n@@ -1166,8 +1154,6 @@ def eigvalsh(a, UPLO='L'):\n     if UPLO not in ('L', 'U'):\n         raise ValueError(\"UPLO argument must be 'L' or 'U'\")\n \n-    extobj = get_linalg_error_extobj(\n-        _raise_linalgerror_eigenvalues_nonconvergence)\n     if UPLO == 'L':\n         gufunc = _umath_linalg.eigvalsh_lo\n     else:\n@@ -1178,7 +1164,10 @@ def eigvalsh(a, UPLO='L'):\n     _assert_stacked_square(a)\n     t, result_t = _commonType(a)\n     signature = 'D->d' if isComplexType(t) else 'd->d'\n-    w = gufunc(a, signature=signature, extobj=extobj)\n+    with errstate(call=_raise_linalgerror_eigenvalues_nonconvergence,\n+                  invalid='call', over='ignore', divide='ignore',\n+                  under='ignore'):\n+        w = gufunc(a, signature=signature)\n     return w.astype(_realType(result_t), copy=False)\n \n def _convertarray(a):\n@@ -1329,10 +1318,11 @@ def eig(a):\n     _assert_finite(a)\n     t, result_t = _commonType(a)\n \n-    extobj = get_linalg_error_extobj(\n-        _raise_linalgerror_eigenvalues_nonconvergence)\n     signature = 'D->DD' if isComplexType(t) else 'd->DD'\n-    w, vt = _umath_linalg.eig(a, signature=signature, extobj=extobj)\n+    with errstate(call=_raise_linalgerror_eigenvalues_nonconvergence,\n+                  invalid='call', over='ignore', divide='ignore',\n+                  under='ignore'):\n+        w, vt = _umath_linalg.eig(a, signature=signature)\n \n     if not isComplexType(t) and all(w.imag == 0.0):\n         w = w.real\n@@ -1476,15 +1466,16 @@ def eigh(a, UPLO='L'):\n     _assert_stacked_square(a)\n     t, result_t = _commonType(a)\n \n-    extobj = get_linalg_error_extobj(\n-        _raise_linalgerror_eigenvalues_nonconvergence)\n     if UPLO == 'L':\n         gufunc = _umath_linalg.eigh_lo\n     else:\n         gufunc = _umath_linalg.eigh_up\n \n     signature = 'D->dD' if isComplexType(t) else 'd->dd'\n-    w, vt = gufunc(a, signature=signature, extobj=extobj)\n+    with errstate(call=_raise_linalgerror_eigenvalues_nonconvergence,\n+                  invalid='call', over='ignore', divide='ignore',\n+                  under='ignore'):\n+        w, vt = gufunc(a, signature=signature)\n     w = w.astype(_realType(result_t), copy=False)\n     vt = vt.astype(result_t, copy=False)\n     return EighResult(w, wrap(vt))\n@@ -1662,8 +1653,6 @@ def svd(a, full_matrices=True, compute_uv=True, hermitian=False):\n     _assert_stacked_2d(a)\n     t, result_t = _commonType(a)\n \n-    extobj = get_linalg_error_extobj(_raise_linalgerror_svd_nonconvergence)\n-\n     m, n = a.shape[-2:]\n     if compute_uv:\n         if full_matrices:\n@@ -1678,7 +1667,10 @@ def svd(a, full_matrices=True, compute_uv=True, hermitian=False):\n                 gufunc = _umath_linalg.svd_n_s\n \n         signature = 'D->DdD' if isComplexType(t) else 'd->ddd'\n-        u, s, vh = gufunc(a, signature=signature, extobj=extobj)\n+        with errstate(call=_raise_linalgerror_svd_nonconvergence,\n+                      invalid='call', over='ignore', divide='ignore',\n+                      under='ignore'):\n+            u, s, vh = gufunc(a, signature=signature)\n         u = u.astype(result_t, copy=False)\n         s = s.astype(_realType(result_t), copy=False)\n         vh = vh.astype(result_t, copy=False)\n@@ -1690,7 +1682,10 @@ def svd(a, full_matrices=True, compute_uv=True, hermitian=False):\n             gufunc = _umath_linalg.svd_n\n \n         signature = 'D->d' if isComplexType(t) else 'd->d'\n-        s = gufunc(a, signature=signature, extobj=extobj)\n+        with errstate(call=_raise_linalgerror_svd_nonconvergence,\n+                      invalid='call', over='ignore', divide='ignore',\n+                      under='ignore'):\n+            s = gufunc(a, signature=signature)\n         s = s.astype(_realType(result_t), copy=False)\n         return s\n \n@@ -2319,11 +2314,13 @@ def lstsq(a, b, rcond=\"warn\"):\n         gufunc = _umath_linalg.lstsq_n\n \n     signature = 'DDd->Ddid' if isComplexType(t) else 'ddd->ddid'\n-    extobj = get_linalg_error_extobj(_raise_linalgerror_lstsq)\n     if n_rhs == 0:\n         # lapack can't handle n_rhs = 0 - so allocate the array one larger in that axis\n         b = zeros(b.shape[:-2] + (m, n_rhs + 1), dtype=b.dtype)\n-    x, resids, rank, s = gufunc(a, b, rcond, signature=signature, extobj=extobj)\n+\n+    with errstate(call=_raise_linalgerror_lstsq, invalid='call',\n+                  over='ignore', divide='ignore', under='ignore'):\n+        x, resids, rank, s = gufunc(a, b, rcond, signature=signature)\n     if m == 0:\n         x[...] = 0\n     if n_rhs == 0:\n",
            "comment_added_diff": {},
            "comment_deleted_diff": {
                "107": "# Dealing with errors in _umath_linalg",
                "131": "    extobj = list(_linalg_error_extobj)  # make a copy"
            },
            "comment_modified_diff": {}
        }
    ],
    "defmatrix.py": [],
    "hook-numpy.py": [
        {
            "commit": "99104bd2d0557078d7ea9a590129c87dd63df623",
            "timestamp": "2023-01-20T13:10:36+01:00",
            "author": "Sebastian Berg",
            "commit_message": "MAINT: Move unused import into hook for pyinstaller\n\npyinstaller should pack it to allow running the tests, but doesn't\npack the tests themselves and thus doesn't find the `import` statements\nthat use `numpy.core._multiarray_tests`.\n\nThis makes sure that pyinstaller will ship `numpy.core._multiarray_tests`\nin any case.",
            "additions": 3,
            "deletions": 3,
            "change_type": "MODIFY",
            "diff": "@@ -20,9 +20,9 @@\n     from PyInstaller.utils.hooks import conda_support\n     datas = conda_support.collect_dynamic_libs(\"numpy\", dependencies=True)\n \n-# Submodules PyInstaller cannot detect (probably because they are only imported\n-# by extension modules, which PyInstaller cannot read).\n-hiddenimports = ['numpy.core._dtype_ctypes']\n+# Submodules PyInstaller cannot detect.  `_dtype_ctypes` is only imported\n+# from C and `_multiarray_tests` is used in tests (which are not packed).\n+hiddenimports = ['numpy.core._dtype_ctypes', 'numpy.core._multiarray_tests']\n \n # Remove testing and building code and packages that are referenced throughout\n # NumPy but are not really dependencies.\n",
            "comment_added_diff": {
                "23": "# Submodules PyInstaller cannot detect.  `_dtype_ctypes` is only imported",
                "24": "# from C and `_multiarray_tests` is used in tests (which are not packed)."
            },
            "comment_deleted_diff": {
                "23": "# Submodules PyInstaller cannot detect (probably because they are only imported",
                "24": "# by extension modules, which PyInstaller cannot read)."
            },
            "comment_modified_diff": {
                "23": "# Submodules PyInstaller cannot detect (probably because they are only imported",
                "24": "# by extension modules, which PyInstaller cannot read)."
            }
        }
    ],
    "_polybase.py": [],
    "test_polynomial.py": [
        {
            "commit": "f5fdf65f01f0f435e6080f014ef33180a2556096",
            "timestamp": "2023-08-28T23:36:11+02:00",
            "author": "Pieter Eendebak",
            "commit_message": "add test",
            "additions": 16,
            "deletions": 10,
            "change_type": "MODIFY",
            "diff": "@@ -2,14 +2,14 @@\n \n \"\"\"\n from functools import reduce\n-import fractions\n+from fractions import Fraction\n import numpy as np\n import numpy.polynomial.polynomial as poly\n import pickle\n from copy import deepcopy\n from numpy.testing import (\n     assert_almost_equal, assert_raises, assert_equal, assert_,\n-    assert_warns, assert_array_equal, assert_raises_regex)\n+    assert_array_equal, assert_raises_regex)\n \n \n def trim(x):\n@@ -125,16 +125,22 @@ def test_polypow(self):\n class TestFraction:\n \n     def test_Fraction(self):\n-        f = fractions.Fraction(2,3)\n-        one = fractions.Fraction(1,1)\n-        zero = fractions.Fraction(0,1)\n+        # assert we can use Polynomials with coefficients of object dtype\n+        f = Fraction(2, 3)\n+        one = Fraction(1, 1)\n+        zero = Fraction(0, 1)\n         p = poly.Polynomial([f, f], domain=[zero, one], window=[zero, one])\n+        \n+        x = 2 * p + p ** 2\n+        assert_equal(x.coef, np.array([Fraction(16, 9), Fraction(20, 9),\n+                                       Fraction(4, 9)], dtype=object))\n         assert_equal(p.domain, [zero, one])\n-\n-        self.assertIsInstance(p(f), fractions.Fraction)\n-        assert_equal(p(f), fractions.Fraction(10, 9))\n-        p_deriv = poly.Polynomial([fractions.Fraction(2,3)], domain=[zero, one], window=[zero, one])\n-        assert_equal(p.deriv(), p_deriv )\n+        assert_equal(p.coef.dtype, np.dtypes.ObjectDType())\n+        self.assertIsInstance(p(f), Fraction)\n+        assert_equal(p(f), Fraction(10, 9))\n+        p_deriv = poly.Polynomial([Fraction(2, 3)], domain=[zero, one],\n+                                  window=[zero, one])\n+        assert_equal(p.deriv(), p_deriv)\n \n class TestEvaluation:\n     # coefficients of 1 + 2*x + 3*x**2\n",
            "comment_added_diff": {
                "128": "        # assert we can use Polynomials with coefficients of object dtype"
            },
            "comment_deleted_diff": {},
            "comment_modified_diff": {
                "128": "        f = fractions.Fraction(2,3)"
            }
        }
    ],
    "generate_ufunc_api.py": [
        {
            "commit": "bb89a3824fa698370dadb51576e06613ec6a49c5",
            "timestamp": "2022-12-15T05:51:36+02:00",
            "author": "mattip",
            "commit_message": "MAINT: remove unused API documentation generation",
            "additions": 1,
            "deletions": 14,
            "change_type": "MODIFY",
            "diff": "@@ -122,8 +122,7 @@ def generate_api(output_dir, force=False):\n \n     h_file = os.path.join(output_dir, '__%s.h' % basename)\n     c_file = os.path.join(output_dir, '__%s.c' % basename)\n-    d_file = os.path.join(output_dir, '%s.txt' % basename)\n-    targets = (h_file, c_file, d_file)\n+    targets = (h_file, c_file)\n \n     sources = ['ufunc_api_order.txt']\n \n@@ -137,7 +136,6 @@ def generate_api(output_dir, force=False):\n def do_generate_api(targets, sources):\n     header_file = targets[0]\n     c_file = targets[1]\n-    doc_file = targets[2]\n \n     ufunc_api_index = genapi.merge_api_dicts((\n             numpy_api.ufunc_funcs_api,\n@@ -179,17 +177,6 @@ def do_generate_api(targets, sources):\n     s = c_template % ',\\n'.join(init_list)\n     genapi.write_file(c_file, s)\n \n-    # Write to documentation\n-    s = '''\n-=================\n-NumPy Ufunc C-API\n-=================\n-'''\n-    for func in ufunc_api_list:\n-        s += func.to_ReST()\n-        s += '\\n\\n'\n-    genapi.write_file(doc_file, s)\n-\n     return targets\n \n \n",
            "comment_added_diff": {},
            "comment_deleted_diff": {
                "182": "    # Write to documentation"
            },
            "comment_modified_diff": {}
        },
        {
            "commit": "bf66ff05751faabd95f97af4975961c05d0280a2",
            "timestamp": "2023-06-11T20:31:38+02:00",
            "author": "Sebastian Berg",
            "commit_message": "MAINT: Allow the API table to have `__unused_indices__`\n\nSince we force users to compile against NumPy 2.0 (if they wish to\nsupport NumPy 2.0+), it is OK to remove things from the API Table.\n\nAllow doing so!\n\n(note that NEP 53 suggests introducing `numpy2_compat`.  This would\nallow fully replacing the table, for now, lets see how far we can\nget without it though!",
            "additions": 5,
            "deletions": 0,
            "change_type": "MODIFY",
            "diff": "@@ -165,6 +165,11 @@ def do_generate_api(targets, sources):\n \n     for name, index in genapi.order_dict(ufunc_api_index):\n         api_item = ufunc_api_dict[name]\n+        # In NumPy 2.0 the API may have holes (which may be filled again)\n+        # in that case, add `NULL` to fill it.\n+        while len(init_list) < api_item.index:\n+            init_list.append(\"        NULL\")\n+\n         extension_list.append(api_item.define_from_array_api_string())\n         init_list.append(api_item.array_api_define())\n         module_list.append(api_item.internal_define())\n",
            "comment_added_diff": {
                "168": "        # In NumPy 2.0 the API may have holes (which may be filled again)",
                "169": "        # in that case, add `NULL` to fill it."
            },
            "comment_deleted_diff": {},
            "comment_modified_diff": {}
        }
    ],
    "_neighborhood_iterator_imp.h": [],
    "experimental_dtype_api.h": [],
    "ndarrayobject.h": [],
    "ndarraytypes.h": [],
    "npy_3kcompat.h": [],
    "npy_common.h": [],
    "distributions.h": [],
    "get_attr_string.h": [],
    "lowlevel_strided_loops.h": [],
    "npy_cblas.h": [],
    "npy_ctypes.h": [],
    "npy_extint128.h": [],
    "npy_hashtable.c": [],
    "npy_import.h": [],
    "npy_pycompat.h": [],
    "npy_sort.h.src": [],
    "templ_common.h.src": [],
    "abstractdtypes.c": [],
    "abstractdtypes.h": [],
    "alloc.c": [],
    "alloc.h": [],
    "array_coercion.c": [],
    "array_method.c": [],
    "arrayobject.c": [],
    "buffer.c": [],
    "common.h": [],
    "compiled_base.c": [],
    "conversion_utils.c": [],
    "conversion_utils.h": [],
    "dtype_transfer.c": [],
    "dtype_transfer.h": [],
    "dtypemeta.c": [],
    "item_selection.c": [],
    "iterators.c": [],
    "mapping.c": [],
    "nditer_impl.h": [],
    "scalartypes.c.src": [],
    "shape.c": [],
    "conversions.c": [],
    "stream_pyobject.c": [],
    "npy_math_complex.c.src": [],
    "npysort_common.h": [],
    "npysort_heapsort.h": [],
    "fast_loop_macros.h": [],
    "loops_arithm_fp.dispatch.c.src": [],
    "loops_arithmetic.dispatch.c.src": [],
    "loops_comparison.dispatch.c.src": [],
    "loops_exponent_log.dispatch.c.src": [],
    "loops_modulo.dispatch.c.src": [],
    "loops_utils.h.src": [],
    "matmul.c.src": [],
    "simd.inc.src": [],
    "string_ufuncs.cpp": [],
    "parse.py": [
        {
            "commit": "3947b1a023a07a55522de65b4d302339bac2bad7",
            "timestamp": "2022-11-25T10:51:14+01:00",
            "author": "Ralf Gommers",
            "commit_message": "MAINT: replace `NPY_INLINE` with `inline`\n\nCloses gh-22100",
            "additions": 2,
            "deletions": 3,
            "change_type": "MODIFY",
            "diff": "@@ -36,9 +36,9 @@ def parse_distributions_h(ffi, inc_dir):\n                 continue\n     \n             # skip any inlined function definition\n-            # which starts with 'static NPY_INLINE xxx(...) {'\n+            # which starts with 'static inline xxx(...) {'\n             # and ends with a closing '}'\n-            if line.strip().startswith('static NPY_INLINE'):\n+            if line.strip().startswith('static inline'):\n                 in_skip += line.count('{')\n                 continue\n             elif in_skip > 0:\n@@ -48,7 +48,6 @@ def parse_distributions_h(ffi, inc_dir):\n     \n             # replace defines with their value or remove them\n             line = line.replace('DECLDIR', '')\n-            line = line.replace('NPY_INLINE', '')\n             line = line.replace('RAND_INT_TYPE', 'int64_t')\n             s.append(line)\n         ffi.cdef('\\n'.join(s))\n",
            "comment_added_diff": {
                "39": "            # which starts with 'static inline xxx(...) {'"
            },
            "comment_deleted_diff": {
                "39": "            # which starts with 'static NPY_INLINE xxx(...) {'"
            },
            "comment_modified_diff": {
                "39": "            # which starts with 'static NPY_INLINE xxx(...) {'"
            }
        }
    ],
    "aligned_malloc.h": [],
    "distributions.c": [],
    "legacy-distributions.c": [],
    "randomkit.c": [],
    "philox.c": [],
    "philox.h": [],
    "sfc64.h": [],
    "linux_meson.yml": [],
    ".gitignore": [],
    "build_requirements.txt": [],
    "building_with_meson.md": [],
    "dev.py": [
        {
            "commit": "4002a7d421ff10780c28a3643683af7a9754f87f",
            "timestamp": "2022-11-25T12:37:46+01:00",
            "author": "Ralf Gommers",
            "commit_message": "BLD: enable building NumPy with Meson\n\nThis enables building with NumPy on Linux and macOS. Windows support\nshould be complete to, but is untested as of now and may need a few\ntweaks. This contains:\n- A set of `meson.build` files and related code generation script\n  tweaks, header templates, etc.\n- One CI job on Linux\n- Basic docs on using Meson to build NumPy (not yet integrated in the\n  html docs, it's too early for that - this is for early adopters right\n  now).\n\nThe build should be complete, with the major exception of SIMD support.\nThe full test suite passes. See gh-22546 for the tracking issue with\ndetailed notes on the plan for switching NumPy to Meson as its build\nsystem.\n\nCo-authored-by: Stefan van der Walt <stefanv@berkeley.edu>",
            "additions": 19,
            "deletions": 0,
            "change_type": "ADD",
            "diff": "@@ -0,0 +1,19 @@\n+#!/usr/bin/env python\n+#\n+# Example stub for running `python -m dev.py`\n+#\n+# Copy this into your project root.\n+\n+import os\n+import sys\n+import runpy\n+\n+sys.path.remove(os.path.abspath(os.path.dirname(sys.argv[0])))\n+try:\n+    runpy.run_module(\"devpy\", run_name=\"__main__\")\n+except ImportError:\n+    print(\"Cannot import devpy; please install it using\")\n+    print()\n+    print(\"  pip install git+https://github.com/scientific-python/devpy\")\n+    print()\n+    sys.exit(1)\n",
            "comment_added_diff": {
                "1": "#!/usr/bin/env python",
                "2": "#",
                "3": "# Example stub for running `python -m dev.py`",
                "4": "#",
                "5": "# Copy this into your project root."
            },
            "comment_deleted_diff": {},
            "comment_modified_diff": {}
        },
        {
            "commit": "0e4af5f5e4392c5cf65ad18deae19610a6179057",
            "timestamp": "2023-03-17T15:49:14-07:00",
            "author": "Jarrod Millman",
            "commit_message": "Rename devpy to spin",
            "additions": 0,
            "deletions": 19,
            "change_type": "DELETE",
            "diff": "@@ -1,19 +0,0 @@\n-#!/usr/bin/env python\n-#\n-# Example stub for running `python -m dev.py`\n-#\n-# Copy this into your project root.\n-\n-import os\n-import sys\n-import runpy\n-\n-sys.path.remove(os.path.abspath(os.path.dirname(sys.argv[0])))\n-try:\n-    runpy.run_module(\"devpy\", run_name=\"__main__\")\n-except ImportError:\n-    print(\"Cannot import devpy; please install it using\")\n-    print()\n-    print(\"  pip install git+https://github.com/scientific-python/devpy@v0.1\")\n-    print()\n-    sys.exit(1)\n",
            "comment_added_diff": {},
            "comment_deleted_diff": {
                "1": "#!/usr/bin/env python",
                "2": "#",
                "3": "# Example stub for running `python -m dev.py`",
                "4": "#",
                "5": "# Copy this into your project root."
            },
            "comment_modified_diff": {}
        }
    ],
    "meson.rst": [],
    "meson.build": [],
    "meson_upd.build": [],
    "environment.yml": [],
    "generate_version.py": [
        {
            "commit": "4002a7d421ff10780c28a3643683af7a9754f87f",
            "timestamp": "2022-11-25T12:37:46+01:00",
            "author": "Ralf Gommers",
            "commit_message": "BLD: enable building NumPy with Meson\n\nThis enables building with NumPy on Linux and macOS. Windows support\nshould be complete to, but is untested as of now and may need a few\ntweaks. This contains:\n- A set of `meson.build` files and related code generation script\n  tweaks, header templates, etc.\n- One CI job on Linux\n- Basic docs on using Meson to build NumPy (not yet integrated in the\n  html docs, it's too early for that - this is for early adopters right\n  now).\n\nThe build should be complete, with the major exception of SIMD support.\nThe full test suite passes. See gh-22546 for the tracking issue with\ndetailed notes on the plan for switching NumPy to Meson as its build\nsystem.\n\nCo-authored-by: Stefan van der Walt <stefanv@berkeley.edu>",
            "additions": 40,
            "deletions": 0,
            "change_type": "ADD",
            "diff": "@@ -0,0 +1,40 @@\n+# Note: This file has to live next to versioneer.py or it will not work\n+import argparse\n+import os\n+\n+import versioneer\n+\n+\n+def write_version_info(path):\n+    vinfo = versioneer.get_versions()\n+    full_version = vinfo['version']\n+    git_revision = vinfo['full-revisionid']\n+\n+    if os.environ.get(\"MESON_DIST_ROOT\"):\n+        path = os.path.join(os.environ.get(\"MESON_DIST_ROOT\"), path)\n+\n+    with open(path, \"w\") as f:\n+        f.write(\"def get_versions():\\n\")\n+        f.write(\"    return {\\n\")\n+        f.write(f\"        'full-revisionid': '{git_revision}',\\n\")\n+        f.write(f\"        'version': '{full_version}'\\n\")\n+        f.write(\"}\")\n+\n+\n+def main():\n+    parser = argparse.ArgumentParser()\n+    parser.add_argument(\n+        \"-o\", \"--outfile\", type=str, help=\"Path to write version info to\"\n+    )\n+    args = parser.parse_args()\n+\n+    if not args.outfile.endswith(\".py\"):\n+        raise ValueError(\n+            f\"Output file must be a Python file. \"\n+            f\"Got: {args.outfile} as filename instead\"\n+        )\n+\n+    write_version_info(args.outfile)\n+\n+\n+main()\n",
            "comment_added_diff": {
                "1": "# Note: This file has to live next to versioneer.py or it will not work"
            },
            "comment_deleted_diff": {},
            "comment_modified_diff": {}
        },
        {
            "commit": "be896c9c652c010093d26bd0ff732dbfabcd6361",
            "timestamp": "2023-08-11T22:40:16+02:00",
            "author": "Stefan van der Walt",
            "commit_message": "MAINT: Remove versioneer (#24196)\n\nCo-authored-by: Ralf Gommers <ralf.gommers@gmail.com>",
            "additions": 0,
            "deletions": 40,
            "change_type": "DELETE",
            "diff": "@@ -1,40 +0,0 @@\n-# Note: This file has to live next to versioneer.py or it will not work\n-import argparse\n-import os\n-\n-import versioneer\n-\n-\n-def write_version_info(path):\n-    vinfo = versioneer.get_versions()\n-    full_version = vinfo['version']\n-    git_revision = vinfo['full-revisionid']\n-\n-    if os.environ.get(\"MESON_DIST_ROOT\"):\n-        path = os.path.join(os.environ.get(\"MESON_DIST_ROOT\"), path)\n-\n-    with open(path, \"w\") as f:\n-        f.write(\"def get_versions():\\n\")\n-        f.write(\"    return {\\n\")\n-        f.write(f\"        'full-revisionid': '{git_revision}',\\n\")\n-        f.write(f\"        'version': '{full_version}'\\n\")\n-        f.write(\"}\")\n-\n-\n-def main():\n-    parser = argparse.ArgumentParser()\n-    parser.add_argument(\n-        \"-o\", \"--outfile\", type=str, help=\"Path to write version info to\"\n-    )\n-    args = parser.parse_args()\n-\n-    if not args.outfile.endswith(\".py\"):\n-        raise ValueError(\n-            f\"Output file must be a Python file. \"\n-            f\"Got: {args.outfile} as filename instead\"\n-        )\n-\n-    write_version_info(args.outfile)\n-\n-\n-main()\n",
            "comment_added_diff": {},
            "comment_deleted_diff": {
                "1": "# Note: This file has to live next to versioneer.py or it will not work"
            },
            "comment_modified_diff": {}
        }
    ],
    "meson_options.txt": [],
    "__config__.py.in": [],
    "gcc_build_bitness.py": [
        {
            "commit": "4002a7d421ff10780c28a3643683af7a9754f87f",
            "timestamp": "2022-11-25T12:37:46+01:00",
            "author": "Ralf Gommers",
            "commit_message": "BLD: enable building NumPy with Meson\n\nThis enables building with NumPy on Linux and macOS. Windows support\nshould be complete to, but is untested as of now and may need a few\ntweaks. This contains:\n- A set of `meson.build` files and related code generation script\n  tweaks, header templates, etc.\n- One CI job on Linux\n- Basic docs on using Meson to build NumPy (not yet integrated in the\n  html docs, it's too early for that - this is for early adopters right\n  now).\n\nThe build should be complete, with the major exception of SIMD support.\nThe full test suite passes. See gh-22546 for the tracking issue with\ndetailed notes on the plan for switching NumPy to Meson as its build\nsystem.\n\nCo-authored-by: Stefan van der Walt <stefanv@berkeley.edu>",
            "additions": 21,
            "deletions": 0,
            "change_type": "ADD",
            "diff": "@@ -0,0 +1,21 @@\n+#!python\n+\"\"\" Detect bitness (32 or 64) of Mingw-w64 gcc build target on Windows.\n+\"\"\"\n+\n+import re\n+from subprocess import run, PIPE\n+\n+\n+def main():\n+    res = run(['gcc', '-v'], check=True, text=True, capture_output=True)\n+    target = re.search(r'^Target: (.*)$', res.stderr, flags=re.M).groups()[0]\n+    if target.startswith('i686'):\n+        print('32')\n+    elif target.startswith('x86_64'):\n+        print('64')\n+    else:\n+        raise RuntimeError('Could not detect Mingw-w64 bitness')\n+\n+\n+if __name__ == \"__main__\":\n+    main()\n",
            "comment_added_diff": {
                "1": "#!python"
            },
            "comment_deleted_diff": {},
            "comment_modified_diff": {}
        }
    ],
    "process_src_template.py": [
        {
            "commit": "4002a7d421ff10780c28a3643683af7a9754f87f",
            "timestamp": "2022-11-25T12:37:46+01:00",
            "author": "Ralf Gommers",
            "commit_message": "BLD: enable building NumPy with Meson\n\nThis enables building with NumPy on Linux and macOS. Windows support\nshould be complete to, but is untested as of now and may need a few\ntweaks. This contains:\n- A set of `meson.build` files and related code generation script\n  tweaks, header templates, etc.\n- One CI job on Linux\n- Basic docs on using Meson to build NumPy (not yet integrated in the\n  html docs, it's too early for that - this is for early adopters right\n  now).\n\nThe build should be complete, with the major exception of SIMD support.\nThe full test suite passes. See gh-22546 for the tracking issue with\ndetailed notes on the plan for switching NumPy to Meson as its build\nsystem.\n\nCo-authored-by: Stefan van der Walt <stefanv@berkeley.edu>",
            "additions": 67,
            "deletions": 0,
            "change_type": "ADD",
            "diff": "@@ -0,0 +1,67 @@\n+#!/usr/bin/env python3\n+import sys\n+import os\n+import argparse\n+import importlib.util\n+\n+\n+def get_processor():\n+    # Convoluted because we can't import from numpy.distutils\n+    # (numpy is not yet built)\n+    conv_template_path = os.path.join(\n+        os.path.dirname(__file__),\n+        '..', 'distutils', 'conv_template.py'\n+    )\n+    spec = importlib.util.spec_from_file_location(\n+        'conv_template', conv_template_path\n+    )\n+    mod = importlib.util.module_from_spec(spec)\n+    spec.loader.exec_module(mod)\n+    return mod.process_file\n+\n+\n+def process_and_write_file(fromfile, outfile):\n+    \"\"\"Process tempita templated file and write out the result.\n+\n+    The template file is expected to end in `.src`\n+    (e.g., `.c.src` or `.h.src`).\n+    Processing `npy_somefile.c.src` generates `npy_somefile.c`.\n+\n+    \"\"\"\n+    process_file = get_processor()\n+    content = process_file(fromfile)\n+    with open(outfile, 'w') as f:\n+        f.write(content)\n+\n+\n+def main():\n+    parser = argparse.ArgumentParser()\n+    parser.add_argument(\n+        \"infile\",\n+        type=str,\n+        help=\"Path to the input file\"\n+    )\n+    parser.add_argument(\n+        \"-o\",\n+        \"--outfile\",\n+        type=str,\n+        help=\"Path to the output file\"\n+    )\n+    parser.add_argument(\n+        \"-i\",\n+        \"--ignore\",\n+        type=str,\n+        help=\"An ignored input - may be useful to add a \"\n+        \"dependency between custom targets\",\n+    )\n+    args = parser.parse_args()\n+\n+    if not args.infile.endswith('.src'):\n+        raise ValueError(f\"Unexpected extension: {args.infile}\")\n+\n+    outfile_abs = os.path.join(os.getcwd(), args.outfile)\n+    process_and_write_file(args.infile, outfile_abs)\n+\n+\n+if __name__ == \"__main__\":\n+    main()\n",
            "comment_added_diff": {
                "1": "#!/usr/bin/env python3",
                "9": "    # Convoluted because we can't import from numpy.distutils",
                "10": "    # (numpy is not yet built)"
            },
            "comment_deleted_diff": {},
            "comment_modified_diff": {}
        }
    ],
    "tempita.py": [
        {
            "commit": "4002a7d421ff10780c28a3643683af7a9754f87f",
            "timestamp": "2022-11-25T12:37:46+01:00",
            "author": "Ralf Gommers",
            "commit_message": "BLD: enable building NumPy with Meson\n\nThis enables building with NumPy on Linux and macOS. Windows support\nshould be complete to, but is untested as of now and may need a few\ntweaks. This contains:\n- A set of `meson.build` files and related code generation script\n  tweaks, header templates, etc.\n- One CI job on Linux\n- Basic docs on using Meson to build NumPy (not yet integrated in the\n  html docs, it's too early for that - this is for early adopters right\n  now).\n\nThe build should be complete, with the major exception of SIMD support.\nThe full test suite passes. See gh-22546 for the tracking issue with\ndetailed notes on the plan for switching NumPy to Meson as its build\nsystem.\n\nCo-authored-by: Stefan van der Walt <stefanv@berkeley.edu>",
            "additions": 62,
            "deletions": 0,
            "change_type": "ADD",
            "diff": "@@ -0,0 +1,62 @@\n+#!/usr/bin/env python3\n+import sys\n+import os\n+import argparse\n+\n+from Cython import Tempita as tempita\n+\n+# XXX: If this import ever fails (does it really?), vendor either\n+# cython.tempita or numpy/npy_tempita.\n+\n+\n+def process_tempita(fromfile, outfile=None):\n+    \"\"\"Process tempita templated file and write out the result.\n+\n+    The template file is expected to end in `.c.in` or `.pyx.in`:\n+    E.g. processing `template.c.in` generates `template.c`.\n+\n+    \"\"\"\n+    if outfile is None:\n+        # We're dealing with a distutils build here, write in-place\n+        outfile = os.path.splitext(fromfile)[0]\n+\n+    from_filename = tempita.Template.from_filename\n+    template = from_filename(fromfile, encoding=sys.getdefaultencoding())\n+\n+    content = template.substitute()\n+\n+    with open(outfile, 'w') as f:\n+        f.write(content)\n+\n+\n+def main():\n+    parser = argparse.ArgumentParser()\n+    parser.add_argument(\n+        \"infile\",\n+        type=str,\n+        help=\"Path to the input file\"\n+    )\n+    parser.add_argument(\n+        \"-o\",\n+        \"--outfile\",\n+        type=str,\n+        help=\"Path to the output file\"\n+    )\n+    parser.add_argument(\n+        \"-i\",\n+        \"--ignore\",\n+        type=str,\n+        help=\"An ignored input - may be useful to add a \"\n+        \"dependency between custom targets\",\n+    )\n+    args = parser.parse_args()\n+\n+    if not args.infile.endswith('.in'):\n+        raise ValueError(f\"Unexpected extension: {args.infile}\")\n+\n+    outfile_abs = os.path.join(os.getcwd(), args.outfile)\n+    process_tempita(args.infile, outfile_abs)\n+\n+\n+if __name__ == \"__main__\":\n+    main()\n",
            "comment_added_diff": {
                "1": "#!/usr/bin/env python3",
                "8": "# XXX: If this import ever fails (does it really?), vendor either",
                "9": "# cython.tempita or numpy/npy_tempita.",
                "20": "        # We're dealing with a distutils build here, write in-place"
            },
            "comment_deleted_diff": {},
            "comment_modified_diff": {}
        },
        {
            "commit": "63a1fee8d1c1e442aa8ca08ac7cefb195f99b2a9",
            "timestamp": "2023-09-13T22:52:14+02:00",
            "author": "Ralf Gommers",
            "commit_message": "MAINT: remove all `setup.py`'s and related files for distutils builds",
            "additions": 1,
            "deletions": 2,
            "change_type": "MODIFY",
            "diff": "@@ -5,8 +5,7 @@\n \n from Cython import Tempita as tempita\n \n-# XXX: If this import ever fails (does it really?), vendor either\n-# cython.tempita or numpy/npy_tempita.\n+# XXX: If this import ever fails (does it really?), vendor cython.tempita\n \n \n def process_tempita(fromfile, outfile=None):\n",
            "comment_added_diff": {
                "8": "# XXX: If this import ever fails (does it really?), vendor cython.tempita"
            },
            "comment_deleted_diff": {
                "8": "# XXX: If this import ever fails (does it really?), vendor either",
                "9": "# cython.tempita or numpy/npy_tempita."
            },
            "comment_modified_diff": {
                "8": "# XXX: If this import ever fails (does it really?), vendor either"
            }
        }
    ],
    "check_longdouble.c": [],
    "generate_numpy_api.py": [
        {
            "commit": "4002a7d421ff10780c28a3643683af7a9754f87f",
            "timestamp": "2022-11-25T12:37:46+01:00",
            "author": "Ralf Gommers",
            "commit_message": "BLD: enable building NumPy with Meson\n\nThis enables building with NumPy on Linux and macOS. Windows support\nshould be complete to, but is untested as of now and may need a few\ntweaks. This contains:\n- A set of `meson.build` files and related code generation script\n  tweaks, header templates, etc.\n- One CI job on Linux\n- Basic docs on using Meson to build NumPy (not yet integrated in the\n  html docs, it's too early for that - this is for early adopters right\n  now).\n\nThe build should be complete, with the major exception of SIMD support.\nThe full test suite passes. See gh-22546 for the tracking issue with\ndetailed notes on the plan for switching NumPy to Meson as its build\nsystem.\n\nCo-authored-by: Stefan van der Walt <stefanv@berkeley.edu>",
            "additions": 29,
            "deletions": 1,
            "change_type": "MODIFY",
            "diff": "@@ -1,6 +1,8 @@\n+#!/usr/bin/env python3\n import os\n-import genapi\n+import argparse\n \n+import genapi\n from genapi import \\\n         TypeApi, GlobalVarApi, FunctionApi, BoolValuesApi\n \n@@ -242,3 +244,29 @@ def do_generate_api(targets, sources):\n     genapi.write_file(doc_file, s)\n \n     return targets\n+\n+\n+def main():\n+    parser = argparse.ArgumentParser()\n+    parser.add_argument(\n+        \"-o\",\n+        \"--outdir\",\n+        type=str,\n+        help=\"Path to the output directory\"\n+    )\n+    parser.add_argument(\n+        \"-i\",\n+        \"--ignore\",\n+        type=str,\n+        help=\"An ignored input - may be useful to add a \"\n+             \"dependency between custom targets\"\n+    )\n+    args = parser.parse_args()\n+\n+    outdir_abs = os.path.join(os.getcwd(), args.outdir)\n+\n+    generate_api(outdir_abs)\n+\n+\n+if __name__ == \"__main__\":\n+    main()\n",
            "comment_added_diff": {
                "1": "#!/usr/bin/env python3"
            },
            "comment_deleted_diff": {},
            "comment_modified_diff": {}
        },
        {
            "commit": "bb89a3824fa698370dadb51576e06613ec6a49c5",
            "timestamp": "2022-12-15T05:51:36+02:00",
            "author": "mattip",
            "commit_message": "MAINT: remove unused API documentation generation",
            "additions": 1,
            "deletions": 16,
            "change_type": "MODIFY",
            "diff": "@@ -141,19 +141,12 @@\n };\n \"\"\"\n \n-c_api_header = \"\"\"\n-===========\n-NumPy C-API\n-===========\n-\"\"\"\n-\n def generate_api(output_dir, force=False):\n     basename = 'multiarray_api'\n \n     h_file = os.path.join(output_dir, '__%s.h' % basename)\n     c_file = os.path.join(output_dir, '__%s.c' % basename)\n-    d_file = os.path.join(output_dir, '%s.txt' % basename)\n-    targets = (h_file, c_file, d_file)\n+    targets = (h_file, c_file)\n \n     sources = numpy_api.multiarray_api\n \n@@ -167,7 +160,6 @@ def generate_api(output_dir, force=False):\n def do_generate_api(targets, sources):\n     header_file = targets[0]\n     c_file = targets[1]\n-    doc_file = targets[2]\n \n     global_vars = sources[0]\n     scalar_bool_values = sources[1]\n@@ -236,13 +228,6 @@ def do_generate_api(targets, sources):\n     s = c_template % ',\\n'.join(init_list)\n     genapi.write_file(c_file, s)\n \n-    # write to documentation\n-    s = c_api_header\n-    for func in numpyapi_list:\n-        s += func.to_ReST()\n-        s += '\\n\\n'\n-    genapi.write_file(doc_file, s)\n-\n     return targets\n \n \n",
            "comment_added_diff": {},
            "comment_deleted_diff": {
                "239": "    # write to documentation"
            },
            "comment_modified_diff": {}
        },
        {
            "commit": "bf66ff05751faabd95f97af4975961c05d0280a2",
            "timestamp": "2023-06-11T20:31:38+02:00",
            "author": "Sebastian Berg",
            "commit_message": "MAINT: Allow the API table to have `__unused_indices__`\n\nSince we force users to compile against NumPy 2.0 (if they wish to\nsupport NumPy 2.0+), it is OK to remove things from the API Table.\n\nAllow doing so!\n\n(note that NEP 53 suggests introducing `numpy2_compat`.  This would\nallow fully replacing the table, for now, lets see how far we can\nget without it though!",
            "additions": 5,
            "deletions": 0,
            "change_type": "MODIFY",
            "diff": "@@ -220,6 +220,11 @@ def do_generate_api(targets, sources):\n     extension_list = []\n     for name, index in genapi.order_dict(multiarray_api_index):\n         api_item = multiarray_api_dict[name]\n+        # In NumPy 2.0 the API may have holes (which may be filled again)\n+        # in that case, add `NULL` to fill it.\n+        while len(init_list) < api_item.index:\n+            init_list.append(\"        NULL\")\n+\n         extension_list.append(api_item.define_from_array_api_string())\n         init_list.append(api_item.array_api_define())\n         module_list.append(api_item.internal_define())\n",
            "comment_added_diff": {
                "223": "        # In NumPy 2.0 the API may have holes (which may be filled again)",
                "224": "        # in that case, add `NULL` to fill it."
            },
            "comment_deleted_diff": {},
            "comment_modified_diff": {}
        }
    ],
    "generate_umath_doc.py": [],
    "numpy_api.py": [
        {
            "commit": "4002a7d421ff10780c28a3643683af7a9754f87f",
            "timestamp": "2022-11-25T12:37:46+01:00",
            "author": "Ralf Gommers",
            "commit_message": "BLD: enable building NumPy with Meson\n\nThis enables building with NumPy on Linux and macOS. Windows support\nshould be complete to, but is untested as of now and may need a few\ntweaks. This contains:\n- A set of `meson.build` files and related code generation script\n  tweaks, header templates, etc.\n- One CI job on Linux\n- Basic docs on using Meson to build NumPy (not yet integrated in the\n  html docs, it's too early for that - this is for early adopters right\n  now).\n\nThe build should be complete, with the major exception of SIMD support.\nThe full test suite passes. See gh-22546 for the tracking issue with\ndetailed notes on the plan for switching NumPy to Meson as its build\nsystem.\n\nCo-authored-by: Stefan van der Walt <stefanv@berkeley.edu>",
            "additions": 17,
            "deletions": 1,
            "change_type": "MODIFY",
            "diff": "@@ -13,7 +13,23 @@\n exception, so it should hopefully not get unnoticed).\n \n \"\"\"\n-from code_generators.genapi import StealRef\n+\n+import os\n+import importlib.util\n+\n+\n+def get_StealRef():\n+    # Convoluted because we can't import from numpy.distutils\n+    # (numpy is not yet built)\n+    genapi_py = os.path.join(os.path.dirname(__file__), 'genapi.py')\n+    spec = importlib.util.spec_from_file_location('conv_template', genapi_py)\n+    mod = importlib.util.module_from_spec(spec)\n+    spec.loader.exec_module(mod)\n+    return mod.StealRef\n+\n+\n+StealRef = get_StealRef()\n+#from code_generators.genapi import StealRef\n \n # index, type\n multiarray_global_vars = {\n",
            "comment_added_diff": {
                "22": "    # Convoluted because we can't import from numpy.distutils",
                "23": "    # (numpy is not yet built)",
                "32": "#from code_generators.genapi import StealRef"
            },
            "comment_deleted_diff": {},
            "comment_modified_diff": {}
        },
        {
            "commit": "9ccf321082e2c2e725f25161c93ba78969df21b7",
            "timestamp": "2023-02-11T15:26:55-08:00",
            "author": "Brock",
            "commit_message": "add NUMPY_API to comments, revert includes",
            "additions": 5,
            "deletions": 0,
            "change_type": "MODIFY",
            "diff": "@@ -370,6 +370,11 @@ def get_StealRef():\n     'PyDataMem_SetHandler':                 (304,),\n     'PyDataMem_GetHandler':                 (305,),\n     # End 1.22 API\n+    'convert_datetime_to_datetimestruct':   (307,),  # GH#21199\n+    'convert_datetimestruct_to_datetime':   (308,),\n+    'convert_pydatetime_to_datetimestruct': (309,),\n+    'get_datetime_iso_8601_strlen':         (310,),\n+    'make_iso_8601_datetime':               (311,),\n }\n \n ufunc_types_api = {\n",
            "comment_added_diff": {
                "373": "    'convert_datetime_to_datetimestruct':   (307,),  # GH#21199"
            },
            "comment_deleted_diff": {},
            "comment_modified_diff": {}
        },
        {
            "commit": "3f8104d414a7ca7a42266d1fd29ecca28bf77908",
            "timestamp": "2023-03-06T17:40:32-08:00",
            "author": "Brock",
            "commit_message": "add NpyDatetime_ to names",
            "additions": 5,
            "deletions": 5,
            "change_type": "MODIFY",
            "diff": "@@ -370,11 +370,11 @@ def get_StealRef():\n     'PyDataMem_SetHandler':                 (304,),\n     'PyDataMem_GetHandler':                 (305,),\n     # End 1.22 API\n-    'convert_datetime_to_datetimestruct':   (307,),  # GH#21199\n-    'convert_datetimestruct_to_datetime':   (308,),\n-    'convert_pydatetime_to_datetimestruct': (309,),\n-    'get_datetime_iso_8601_strlen':         (310,),\n-    'make_iso_8601_datetime':               (311,),\n+    'NpyDatetime_ConvertDatetime64ToDatetimeStruct':   (307,),  # GH#21199\n+    'NpyDatetime_ConvertDatetimeStructToDatetime64':   (308,),\n+    'NpyDatetime_ConvertPydatetimeToDatetimeStruct': (309,),\n+    'NpyDatetime_GetDatetimeISO8601StrLen':         (310,),\n+    'NpyDatetime_MakeISO8601Datetime':               (311,),\n }\n \n ufunc_types_api = {\n",
            "comment_added_diff": {
                "373": "    'NpyDatetime_ConvertDatetime64ToDatetimeStruct':   (307,),  # GH#21199"
            },
            "comment_deleted_diff": {
                "373": "    'convert_datetime_to_datetimestruct':   (307,),  # GH#21199"
            },
            "comment_modified_diff": {
                "373": "    'convert_datetime_to_datetimestruct':   (307,),  # GH#21199"
            }
        },
        {
            "commit": "c335e7131994f12ecd84f83bfe8ffa46b1db14fc",
            "timestamp": "2023-03-13T16:37:22-07:00",
            "author": "Brock",
            "commit_message": "whitespace fixup",
            "additions": 4,
            "deletions": 4,
            "change_type": "MODIFY",
            "diff": "@@ -370,11 +370,11 @@ def get_StealRef():\n     'PyDataMem_SetHandler':                 (304,),\n     'PyDataMem_GetHandler':                 (305,),\n     # End 1.22 API\n-    'NpyDatetime_ConvertDatetime64ToDatetimeStruct':   (307,),  # GH#21199\n+    'NpyDatetime_ConvertDatetime64ToDatetimeStruct':   (307,),\n     'NpyDatetime_ConvertDatetimeStructToDatetime64':   (308,),\n-    'NpyDatetime_ConvertPydatetimeToDatetimeStruct': (309,),\n-    'NpyDatetime_GetDatetimeISO8601StrLen':         (310,),\n-    'NpyDatetime_MakeISO8601Datetime':               (311,),\n+    'NpyDatetime_ConvertPydatetimeToDatetimeStruct':   (309,),\n+    'NpyDatetime_GetDatetimeISO8601StrLen':            (310,),\n+    'NpyDatetime_MakeISO8601Datetime':                 (311,),\n }\n \n ufunc_types_api = {\n",
            "comment_added_diff": {},
            "comment_deleted_diff": {
                "373": "    'NpyDatetime_ConvertDatetime64ToDatetimeStruct':   (307,),  # GH#21199"
            },
            "comment_modified_diff": {}
        },
        {
            "commit": "4dc3727cb88937bd9776d9179b2157b91b46be24",
            "timestamp": "2023-06-11T21:30:38+02:00",
            "author": "Sebastian Berg",
            "commit_message": "MAINT: Remove disabled C-API functions from the API tables",
            "additions": 9,
            "deletions": 7,
            "change_type": "MODIFY",
            "diff": "@@ -98,6 +98,7 @@ def get_annotations():\n # define _PyArrayScalar_BoolValues ((PyBoolScalarObject *)PyArray_API[8])\n \n multiarray_funcs_api = {\n+    '__unused_indices__': [67, 68, 163, 164, 278],\n     'PyArray_GetNDArrayCVersion':           (0,),\n     'PyArray_SetNumericOps':                (40,),\n     'PyArray_GetNumericOps':                (41,),\n@@ -126,8 +127,8 @@ def get_annotations():\n     'PyArray_CastScalarDirect':             (64,),\n     'PyArray_ScalarFromObject':             (65,),\n     'PyArray_GetCastFunc':                  (66,),\n-    'PyArray_FromDims':                     (67,),\n-    'PyArray_FromDimsAndDataAndDescr':      (68, StealRef(3)),\n+    # Unused slot 67, was `PyArray_FromDims`\n+    # Unused slot 68, was `PyArray_FromDimsAndDataAndDescr`\n     'PyArray_FromAny':                      (69, StealRef(2)),\n     'PyArray_EnsureArray':                  (70, StealRef(1)),\n     'PyArray_EnsureAnyArray':               (71, StealRef(1)),\n@@ -222,8 +223,8 @@ def get_annotations():\n     'PyArray_GetPtr':                       (160,),\n     'PyArray_CompareLists':                 (161,),\n     'PyArray_AsCArray':                     (162, StealRef(5)),\n-    'PyArray_As1D':                         (163,),\n-    'PyArray_As2D':                         (164,),\n+    # Unused slot 163, was `PyArray_As1D`\n+    # Unused slot 164, was `PyArray_As2D`\n     'PyArray_Free':                         (165,),\n     'PyArray_Converter':                    (166,),\n     'PyArray_IntpFromSequence':             (167,),\n@@ -335,7 +336,7 @@ def get_annotations():\n     'PyArray_CanCastTypeTo':                (275,),\n     'PyArray_EinsteinSum':                  (276,),\n     'PyArray_NewLikeArray':                 (277, StealRef(3)),\n-    'PyArray_GetArrayParamsFromObject':     (278,),\n+    # Unused slot 278, was `PyArray_GetArrayParamsFromObject`\n     'PyArray_ConvertClipmodeSequence':      (279,),\n     'PyArray_MatrixProduct2':               (280,),\n     # End 1.6 API\n@@ -377,9 +378,10 @@ def get_annotations():\n }\n \n ufunc_funcs_api = {\n+    '__unused_indices__': [3, 32],\n     'PyUFunc_FromFuncAndData':                  (1,),\n     'PyUFunc_RegisterLoopForType':              (2,),\n-    'PyUFunc_GenericFunction':                  (3,),\n+    # Unused slot 3, was `PyUFunc_GenericFunction`\n     'PyUFunc_f_f_As_d_d':                       (4,),\n     'PyUFunc_d_d':                              (5,),\n     'PyUFunc_f_f':                              (6,),\n@@ -408,7 +410,7 @@ def get_annotations():\n     'PyUFunc_handlefperr':                      (29,),\n     'PyUFunc_ReplaceLoopBySignature':           (30,),\n     'PyUFunc_FromFuncAndDataAndSignature':      (31,),\n-    'PyUFunc_SetUsesArraysAsData':              (32,),\n+    # Unused slot 32, was `PyUFunc_SetUsesArraysAsData`\n     # End 1.5 API\n     'PyUFunc_e_e':                              (33,),\n     'PyUFunc_e_e_As_f_f':                       (34,),\n",
            "comment_added_diff": {
                "130": "    # Unused slot 67, was `PyArray_FromDims`",
                "131": "    # Unused slot 68, was `PyArray_FromDimsAndDataAndDescr`",
                "226": "    # Unused slot 163, was `PyArray_As1D`",
                "227": "    # Unused slot 164, was `PyArray_As2D`",
                "339": "    # Unused slot 278, was `PyArray_GetArrayParamsFromObject`",
                "384": "    # Unused slot 3, was `PyUFunc_GenericFunction`",
                "413": "    # Unused slot 32, was `PyUFunc_SetUsesArraysAsData`"
            },
            "comment_deleted_diff": {},
            "comment_modified_diff": {
                "130": "    'PyArray_FromDimsAndDataAndDescr':      (68, StealRef(3)),",
                "226": "    'PyArray_As2D':                         (164,),"
            }
        },
        {
            "commit": "1f94b81162b1aad3e87dd8d3d83a740fa5b2a3bd",
            "timestamp": "2023-06-12T14:36:15+02:00",
            "author": "Sebastian Berg",
            "commit_message": "MAINT: Remove de-facto deprecated interrupt handling",
            "additions": 3,
            "deletions": 3,
            "change_type": "MODIFY",
            "diff": "@@ -98,7 +98,7 @@ def get_annotations():\n # define _PyArrayScalar_BoolValues ((PyBoolScalarObject *)PyArray_API[8])\n \n multiarray_funcs_api = {\n-    '__unused_indices__': [67, 68, 163, 164, 278],\n+    '__unused_indices__': [67, 68, 163, 164, 201, 202, 278],\n     'PyArray_GetNDArrayCVersion':           (0,),\n     'PyArray_SetNumericOps':                (40,),\n     'PyArray_GetNumericOps':                (41,),\n@@ -261,8 +261,8 @@ def get_annotations():\n     'PyArray_ClipmodeConverter':            (198,),\n     'PyArray_OutputConverter':              (199,),\n     'PyArray_BroadcastToShape':             (200,),\n-    '_PyArray_SigintHandler':               (201,),\n-    '_PyArray_GetSigintBuf':                (202,),\n+    # Unused slot 201, was `_PyArray_SigintHandler`\n+    # Unused slot 202, was `_PyArray_GetSigintBuf`\n     'PyArray_DescrAlignConverter':          (203,),\n     'PyArray_DescrAlignConverter2':         (204,),\n     'PyArray_SearchsideConverter':          (205,),\n",
            "comment_added_diff": {
                "264": "    # Unused slot 201, was `_PyArray_SigintHandler`",
                "265": "    # Unused slot 202, was `_PyArray_GetSigintBuf`"
            },
            "comment_deleted_diff": {},
            "comment_modified_diff": {
                "264": "    '_PyArray_SigintHandler':               (201,),",
                "265": "    '_PyArray_GetSigintBuf':                (202,),"
            }
        },
        {
            "commit": "87feaf72531979dd2eea965651b1c9d557ef37c2",
            "timestamp": "2023-06-12T17:57:36+02:00",
            "author": "Sebastian Berg",
            "commit_message": "DEP: Expire the `PyDataMem_SetEventHook` deprecation and remove it",
            "additions": 2,
            "deletions": 2,
            "change_type": "MODIFY",
            "diff": "@@ -98,7 +98,7 @@ def get_annotations():\n # define _PyArrayScalar_BoolValues ((PyBoolScalarObject *)PyArray_API[8])\n \n multiarray_funcs_api = {\n-    '__unused_indices__': [67, 68, 163, 164, 201, 202, 278],\n+    '__unused_indices__': [67, 68, 163, 164, 201, 202, 278, 291],\n     'PyArray_GetNDArrayCVersion':           (0,),\n     'PyArray_SetNumericOps':                (40,),\n     'PyArray_GetNumericOps':                (41,),\n@@ -350,7 +350,7 @@ def get_annotations():\n     'PyDataMem_NEW':                        (288,),\n     'PyDataMem_FREE':                       (289,),\n     'PyDataMem_RENEW':                      (290,),\n-    'PyDataMem_SetEventHook':               (291,),\n+    # Unused slot 291, was `PyDataMem_SetEventHook`\n     'PyArray_MapIterSwapAxes':              (293,),\n     'PyArray_MapIterArray':                 (294,),\n     'PyArray_MapIterNext':                  (295,),\n",
            "comment_added_diff": {
                "353": "    # Unused slot 291, was `PyDataMem_SetEventHook`"
            },
            "comment_deleted_diff": {},
            "comment_modified_diff": {
                "353": "    'PyDataMem_SetEventHook':               (291,),"
            }
        },
        {
            "commit": "3c47251327c152ce7abb72f7c10f307f84977108",
            "timestamp": "2023-06-13T09:21:02+02:00",
            "author": "Sebastian Berg",
            "commit_message": "API: Remove `PyUFunc_` APIs `GetPyValues`, `checkfperr`, and `handlefperr`\n\nWe could introduce a new to combine these (mainly the GetPyValues and\nhandlefperr) and we could do so in a way that is backwards compatible:\nCombine both functions into one small helper.\n\nThere is no known user, but if someone comes up even restoring them\nshould be possible.\n\nThese functions are threaded through our code to deal with error handling\nbut create an unnecessary `errobj` that may have made some sense when\nyou wish to raise errors from a worker/inner-loop.\nHowever, we never do that, and there shouldn't a be reason for downstream\nto do that either:  The current split in functions just makes it hard\nto improve the `errstate` handling (because this API will not remain useful\ninternally).",
            "additions": 4,
            "deletions": 4,
            "change_type": "MODIFY",
            "diff": "@@ -378,7 +378,7 @@ def get_annotations():\n }\n \n ufunc_funcs_api = {\n-    '__unused_indices__': [3, 32],\n+    '__unused_indices__': [3, 25, 26, 29, 32],\n     'PyUFunc_FromFuncAndData':                  (1,),\n     'PyUFunc_RegisterLoopForType':              (2,),\n     # Unused slot 3, was `PyUFunc_GenericFunction`\n@@ -403,11 +403,11 @@ def get_annotations():\n     'PyUFunc_O_O_method':                       (22,),\n     'PyUFunc_OO_O_method':                      (23,),\n     'PyUFunc_On_Om':                            (24,),\n-    'PyUFunc_GetPyValues':                      (25,),\n-    'PyUFunc_checkfperr':                       (26,),\n+    # Unused slot 25, was `PyUFunc_GetPyValues`\n+    # Unused slot 26, was `PyUFunc_checkfperr`\n     'PyUFunc_clearfperr':                       (27,),\n     'PyUFunc_getfperr':                         (28,),\n-    'PyUFunc_handlefperr':                      (29,),\n+    # Unused slot 29, was `PyUFunc_handlefperr`\n     'PyUFunc_ReplaceLoopBySignature':           (30,),\n     'PyUFunc_FromFuncAndDataAndSignature':      (31,),\n     # Unused slot 32, was `PyUFunc_SetUsesArraysAsData`\n",
            "comment_added_diff": {
                "406": "    # Unused slot 25, was `PyUFunc_GetPyValues`",
                "407": "    # Unused slot 26, was `PyUFunc_checkfperr`",
                "410": "    # Unused slot 29, was `PyUFunc_handlefperr`"
            },
            "comment_deleted_diff": {},
            "comment_modified_diff": {
                "406": "    'PyUFunc_GetPyValues':                      (25,),",
                "407": "    'PyUFunc_checkfperr':                       (26,),",
                "410": "    'PyUFunc_handlefperr':                      (29,),"
            }
        },
        {
            "commit": "b6b5d977ea340f96ac5f746eff5907f1b4a66615",
            "timestamp": "2023-06-19T22:12:51+02:00",
            "author": "Sebastian Berg",
            "commit_message": "DEP: Expire `set_numeric_ops` and the corresponding C functions deprecation\n\nThis simply expires the deprecations because I saw they were one cause\nof doctest failures in the other PR, but its good anyway.",
            "additions": 3,
            "deletions": 3,
            "change_type": "MODIFY",
            "diff": "@@ -98,10 +98,10 @@ def get_annotations():\n # define _PyArrayScalar_BoolValues ((PyBoolScalarObject *)PyArray_API[8])\n \n multiarray_funcs_api = {\n-    '__unused_indices__': [67, 68, 163, 164, 201, 202, 278, 291],\n+    '__unused_indices__': [40, 41, 67, 68, 163, 164, 201, 202, 278, 291],\n     'PyArray_GetNDArrayCVersion':           (0,),\n-    'PyArray_SetNumericOps':                (40,),\n-    'PyArray_GetNumericOps':                (41,),\n+    # Unused slot 40, was `PyArray_SetNumericOps`\n+    # Unused slot 41, was `PyArray_GetNumericOps`,\n     'PyArray_INCREF':                       (42,),\n     'PyArray_XDECREF':                      (43,),\n     'PyArray_SetStringFunction':            (44,),\n",
            "comment_added_diff": {
                "103": "    # Unused slot 40, was `PyArray_SetNumericOps`",
                "104": "    # Unused slot 41, was `PyArray_GetNumericOps`,"
            },
            "comment_deleted_diff": {},
            "comment_modified_diff": {
                "103": "    'PyArray_SetNumericOps':                (40,),",
                "104": "    'PyArray_GetNumericOps':                (41,),"
            }
        },
        {
            "commit": "b7dc58d89b04c46b5f4873fef35a277591a3bbf0",
            "timestamp": "2023-08-01T12:23:04+02:00",
            "author": "Sebastian Berg",
            "commit_message": "DEP: Expire copyandtranspose (C and Python) and ScalarFromObject",
            "additions": 4,
            "deletions": 3,
            "change_type": "MODIFY",
            "diff": "@@ -98,7 +98,8 @@ def get_annotations():\n # define _PyArrayScalar_BoolValues ((PyBoolScalarObject *)PyArray_API[8])\n \n multiarray_funcs_api = {\n-    '__unused_indices__': [40, 41, 67, 68, 163, 164, 201, 202, 278, 291],\n+    '__unused_indices__': [\n+        40, 41, 65, 67, 68, 163, 164, 171, 201, 202, 278, 291],\n     'PyArray_GetNDArrayCVersion':           (0,),\n     # Unused slot 40, was `PyArray_SetNumericOps`\n     # Unused slot 41, was `PyArray_GetNumericOps`,\n@@ -125,7 +126,7 @@ def get_annotations():\n     'PyArray_ScalarAsCtype':                (62,),\n     'PyArray_CastScalarToCtype':            (63,),\n     'PyArray_CastScalarDirect':             (64,),\n-    'PyArray_ScalarFromObject':             (65,),\n+    # Unused slot 65, was `PyArray_ScalarFromObject`\n     'PyArray_GetCastFunc':                  (66,),\n     # Unused slot 67, was `PyArray_FromDims`\n     # Unused slot 68, was `PyArray_FromDimsAndDataAndDescr`\n@@ -231,7 +232,7 @@ def get_annotations():\n     'PyArray_Concatenate':                  (168,),\n     'PyArray_InnerProduct':                 (169,),\n     'PyArray_MatrixProduct':                (170,),\n-    'PyArray_CopyAndTranspose':             (171,),\n+    # Unused slot 171, was `PyArray_CopyAndTranspose`\n     'PyArray_Correlate':                    (172,),\n     'PyArray_TypestrConvert':               (173,),\n     'PyArray_DescrConverter':               (174,),\n",
            "comment_added_diff": {
                "129": "    # Unused slot 65, was `PyArray_ScalarFromObject`",
                "235": "    # Unused slot 171, was `PyArray_CopyAndTranspose`"
            },
            "comment_deleted_diff": {},
            "comment_modified_diff": {}
        }
    ],
    "verify_c_api_version.py": [
        {
            "commit": "4002a7d421ff10780c28a3643683af7a9754f87f",
            "timestamp": "2022-11-25T12:37:46+01:00",
            "author": "Ralf Gommers",
            "commit_message": "BLD: enable building NumPy with Meson\n\nThis enables building with NumPy on Linux and macOS. Windows support\nshould be complete to, but is untested as of now and may need a few\ntweaks. This contains:\n- A set of `meson.build` files and related code generation script\n  tweaks, header templates, etc.\n- One CI job on Linux\n- Basic docs on using Meson to build NumPy (not yet integrated in the\n  html docs, it's too early for that - this is for early adopters right\n  now).\n\nThe build should be complete, with the major exception of SIMD support.\nThe full test suite passes. See gh-22546 for the tracking issue with\ndetailed notes on the plan for switching NumPy to Meson as its build\nsystem.\n\nCo-authored-by: Stefan van der Walt <stefanv@berkeley.edu>",
            "additions": 66,
            "deletions": 0,
            "change_type": "ADD",
            "diff": "@@ -0,0 +1,66 @@\n+#!/usr/bin/env python3\n+import os\n+import sys\n+import argparse\n+\n+\n+class MismatchCAPIError(ValueError):\n+    pass\n+\n+\n+def get_api_versions(apiversion):\n+    \"\"\"\n+    Return current C API checksum and the recorded checksum.\n+\n+    Return current C API checksum and the recorded checksum for the given\n+    version of the C API version.\n+\n+    \"\"\"\n+    # Compute the hash of the current API as defined in the .txt files in\n+    # code_generators\n+    sys.path.insert(0, os.path.abspath(os.path.dirname(__file__)))\n+    try:\n+        m = __import__('genapi')\n+        numpy_api = __import__('numpy_api')\n+        curapi_hash = m.fullapi_hash(numpy_api.full_api)\n+        apis_hash = m.get_versions_hash()\n+    finally:\n+        del sys.path[0]\n+\n+    return curapi_hash, apis_hash[apiversion]\n+\n+\n+def check_api_version(apiversion):\n+    \"\"\"Emits a MismatchCAPIWarning if the C API version needs updating.\"\"\"\n+    curapi_hash, api_hash = get_api_versions(apiversion)\n+\n+    # If different hash, it means that the api .txt files in\n+    # codegen_dir have been updated without the API version being\n+    # updated. Any modification in those .txt files should be reflected\n+    # in the api and eventually abi versions.\n+    # To compute the checksum of the current API, use numpy/core/cversions.py\n+    if not curapi_hash == api_hash:\n+        msg = (\"API mismatch detected, the C API version \"\n+               \"numbers have to be updated. Current C api version is \"\n+               f\"{apiversion}, with checksum {curapi_hash}, but recorded \"\n+               f\"checksum in core/codegen_dir/cversions.txt is {api_hash}. If \"\n+               \"functions were added in the C API, you have to update \"\n+               f\"C_API_VERSION in {__file__}.\"\n+               )\n+        raise MismatchCAPIError(msg)\n+\n+\n+def main():\n+    parser = argparse.ArgumentParser()\n+    parser.add_argument(\n+        \"--api-version\",\n+        type=str,\n+        help=\"C API version to verify (as a hex string)\"\n+    )\n+    args = parser.parse_args()\n+\n+    check_api_version(int(args.api_version, base=16))\n+\n+\n+if __name__ == \"__main__\":\n+    main()\n",
            "comment_added_diff": {
                "1": "#!/usr/bin/env python3",
                "19": "    # Compute the hash of the current API as defined in the .txt files in",
                "20": "    # code_generators",
                "37": "    # If different hash, it means that the api .txt files in",
                "38": "    # codegen_dir have been updated without the API version being",
                "39": "    # updated. Any modification in those .txt files should be reflected",
                "40": "    # in the api and eventually abi versions.",
                "41": "    # To compute the checksum of the current API, use numpy/core/cversions.py"
            },
            "comment_deleted_diff": {},
            "comment_modified_diff": {}
        }
    ],
    "config.h.in": [],
    "_numpyconfig.h.in": [],
    "version.py": [],
    "pyproject.toml": [],
    "check_installed_files.py": [
        {
            "commit": "4002a7d421ff10780c28a3643683af7a9754f87f",
            "timestamp": "2022-11-25T12:37:46+01:00",
            "author": "Ralf Gommers",
            "commit_message": "BLD: enable building NumPy with Meson\n\nThis enables building with NumPy on Linux and macOS. Windows support\nshould be complete to, but is untested as of now and may need a few\ntweaks. This contains:\n- A set of `meson.build` files and related code generation script\n  tweaks, header templates, etc.\n- One CI job on Linux\n- Basic docs on using Meson to build NumPy (not yet integrated in the\n  html docs, it's too early for that - this is for early adopters right\n  now).\n\nThe build should be complete, with the major exception of SIMD support.\nThe full test suite passes. See gh-22546 for the tracking issue with\ndetailed notes on the plan for switching NumPy to Meson as its build\nsystem.\n\nCo-authored-by: Stefan van der Walt <stefanv@berkeley.edu>",
            "additions": 86,
            "deletions": 0,
            "change_type": "ADD",
            "diff": "@@ -0,0 +1,86 @@\n+\"\"\"\n+Check if all the test and .pyi files are installed after building.\n+\n+Examples::\n+\n+    $ python check_installed_files.py install_dirname\n+\n+        install_dirname:\n+            the relative path to the directory where NumPy is installed after\n+            building and running `meson install`.\n+\n+Notes\n+=====\n+\n+The script will stop on encountering the first missing file in the install dir,\n+it will not give a full listing. This should be okay, because the script is\n+meant for use in CI so it's not like many files will be missing at once.\n+\n+\"\"\"\n+\n+import os\n+import glob\n+import sys\n+\n+\n+CUR_DIR = os.path.abspath(os.path.join(os.path.dirname(__file__)))\n+ROOT_DIR = os.path.dirname(CUR_DIR)\n+NUMPY_DIR = os.path.join(ROOT_DIR, 'numpy')\n+\n+\n+# Files whose installation path will be different from original one\n+changed_installed_path = {\n+    #'numpy/_build_utils/some_file.py': 'numpy/lib/some_file.py'\n+}\n+\n+\n+def main(install_dir):\n+    INSTALLED_DIR = os.path.join(ROOT_DIR, install_dir)\n+    if not os.path.exists(INSTALLED_DIR):\n+        raise ValueError(\n+            f\"Provided install dir {INSTALLED_DIR} does not exist\"\n+        )\n+\n+    numpy_test_files = get_files(NUMPY_DIR, kind='test')\n+    installed_test_files = get_files(INSTALLED_DIR, kind='test')\n+\n+    # Check test files detected in repo are installed\n+    for test_file in numpy_test_files.keys():\n+        if test_file not in installed_test_files.keys():\n+            raise Exception(\n+                \"%s is not installed\" % numpy_test_files[test_file]\n+            )\n+\n+    print(\"----------- All the test files were installed --------------\")\n+\n+    numpy_pyi_files = get_files(NUMPY_DIR, kind='stub')\n+    installed_pyi_files = get_files(INSTALLED_DIR, kind='stub')\n+\n+    # Check *.pyi files detected in repo are installed\n+    for pyi_file in numpy_pyi_files.keys():\n+        if pyi_file not in installed_pyi_files.keys():\n+            raise Exception(\"%s is not installed\" % numpy_pyi_files[pyi_file])\n+\n+    print(\"----------- All the .pyi files were installed --------------\")\n+\n+\n+def get_files(dir_to_check, kind='test'):\n+    files = dict()\n+    patterns = {\n+        'test': f'{dir_to_check}/**/test_*.py',\n+        'stub': f'{dir_to_check}/**/*.pyi',\n+    }\n+    for path in glob.glob(patterns[kind], recursive=True):\n+        relpath = os.path.relpath(path, dir_to_check)\n+        files[relpath] = path\n+\n+    return files\n+\n+\n+if __name__ == '__main__':\n+    if not len(sys.argv) == 2:\n+        raise ValueError(\"Incorrect number of input arguments, need \"\n+                         \"check_installation.py relpath/to/installed/numpy\")\n+\n+    install_dir = sys.argv[1]\n+    main(install_dir)\n",
            "comment_added_diff": {
                "31": "# Files whose installation path will be different from original one",
                "33": "    #'numpy/_build_utils/some_file.py': 'numpy/lib/some_file.py'",
                "47": "    # Check test files detected in repo are installed",
                "59": "    # Check *.pyi files detected in repo are installed"
            },
            "comment_deleted_diff": {},
            "comment_modified_diff": {}
        }
    ],
    "_inspect.py": [],
    "_pep440.py": [],
    "arm64_exports.c": [],
    "coremath.rst": [],
    "_signbit.c": [],
    "ieee754.c.src": [],
    "22675.compatibility.rst": [],
    "loops.h.src": [],
    "loops_unary.dispatch.c.src": [],
    "numeric.py": [],
    "convert_datatype.c": [],
    "exceptions.py": [
        {
            "commit": "b0f318b38e7dd305c3ca93e6c912e1391dda999e",
            "timestamp": "2022-11-30T12:07:51+01:00",
            "author": "Sebastian Berg",
            "commit_message": "API: Add new exceptions module and move exception exposed via numeric\n\nThis means moving ComplexWarning, TooHardError, and AxisError.",
            "additions": 123,
            "deletions": 0,
            "change_type": "ADD",
            "diff": "@@ -0,0 +1,123 @@\n+from ._utils import set_module as _set_module\n+\n+__all__ = [\"ComplexWarning\", \"TooHardError\", \"AxisError\"]\n+\n+\n+@_set_module('numpy')\n+class ComplexWarning(RuntimeWarning):\n+    \"\"\"\n+    The warning raised when casting a complex dtype to a real dtype.\n+\n+    As implemented, casting a complex number to a real discards its imaginary\n+    part, but this behavior may not be what the user actually wants.\n+\n+    \"\"\"\n+    pass\n+\n+\n+\n+# Exception used in shares_memory()\n+@_set_module('numpy')\n+class TooHardError(RuntimeError):\n+    \"\"\"max_work was exceeded.\n+\n+    This is raised whenever the maximum number of candidate solutions\n+    to consider specified by the ``max_work`` parameter is exceeded.\n+    Assigning a finite number to max_work may have caused the operation\n+    to fail.\n+\n+    \"\"\"\n+\n+    pass\n+\n+\n+@_set_module('numpy')\n+class AxisError(ValueError, IndexError):\n+    \"\"\"Axis supplied was invalid.\n+\n+    This is raised whenever an ``axis`` parameter is specified that is larger\n+    than the number of array dimensions.\n+    For compatibility with code written against older numpy versions, which\n+    raised a mixture of `ValueError` and `IndexError` for this situation, this\n+    exception subclasses both to ensure that ``except ValueError`` and\n+    ``except IndexError`` statements continue to catch `AxisError`.\n+\n+    .. versionadded:: 1.13\n+\n+    Parameters\n+    ----------\n+    axis : int or str\n+        The out of bounds axis or a custom exception message.\n+        If an axis is provided, then `ndim` should be specified as well.\n+    ndim : int, optional\n+        The number of array dimensions.\n+    msg_prefix : str, optional\n+        A prefix for the exception message.\n+\n+    Attributes\n+    ----------\n+    axis : int, optional\n+        The out of bounds axis or ``None`` if a custom exception\n+        message was provided. This should be the axis as passed by\n+        the user, before any normalization to resolve negative indices.\n+\n+        .. versionadded:: 1.22\n+    ndim : int, optional\n+        The number of array dimensions or ``None`` if a custom exception\n+        message was provided.\n+\n+        .. versionadded:: 1.22\n+\n+\n+    Examples\n+    --------\n+    >>> array_1d = np.arange(10)\n+    >>> np.cumsum(array_1d, axis=1)\n+    Traceback (most recent call last):\n+      ...\n+    numpy.AxisError: axis 1 is out of bounds for array of dimension 1\n+\n+    Negative axes are preserved:\n+\n+    >>> np.cumsum(array_1d, axis=-2)\n+    Traceback (most recent call last):\n+      ...\n+    numpy.AxisError: axis -2 is out of bounds for array of dimension 1\n+\n+    The class constructor generally takes the axis and arrays'\n+    dimensionality as arguments:\n+\n+    >>> print(np.AxisError(2, 1, msg_prefix='error'))\n+    error: axis 2 is out of bounds for array of dimension 1\n+\n+    Alternatively, a custom exception message can be passed:\n+\n+    >>> print(np.AxisError('Custom error message'))\n+    Custom error message\n+\n+    \"\"\"\n+\n+    __slots__ = (\"axis\", \"ndim\", \"_msg\")\n+\n+    def __init__(self, axis, ndim=None, msg_prefix=None):\n+        if ndim is msg_prefix is None:\n+            # single-argument form: directly set the error message\n+            self._msg = axis\n+            self.axis = None\n+            self.ndim = None\n+        else:\n+            self._msg = msg_prefix\n+            self.axis = axis\n+            self.ndim = ndim\n+\n+    def __str__(self):\n+        axis = self.axis\n+        ndim = self.ndim\n+\n+        if axis is ndim is None:\n+            return self._msg\n+        else:\n+            msg = f\"axis {axis} is out of bounds for array of dimension {ndim}\"\n+            if self._msg is not None:\n+                msg = f\"{self._msg}: {msg}\"\n+            return msg\n",
            "comment_added_diff": {
                "19": "# Exception used in shares_memory()",
                "104": "            # single-argument form: directly set the error message"
            },
            "comment_deleted_diff": {},
            "comment_modified_diff": {}
        },
        {
            "commit": "1d31511ce8081f03ac507854c94cf4793a5982e3",
            "timestamp": "2022-11-30T12:07:51+01:00",
            "author": "Sebastian Berg",
            "commit_message": "MAINT: Move ModuleDeprecationWarning and ModuleDeprecationWarning\n\nBoth should now live in the \"exceptions\" module.",
            "additions": 38,
            "deletions": 1,
            "change_type": "MODIFY",
            "diff": "@@ -1,6 +1,20 @@\n from ._utils import set_module as _set_module\n \n-__all__ = [\"ComplexWarning\", \"TooHardError\", \"AxisError\"]\n+__all__ = [\n+    \"ComplexWarning\", \"ModuleDeprecationWarning\", \"VisibleDeprecationWarning\",\n+    \"TooHardError\", \"AxisError\"]\n+\n+\n+# Disallow reloading this module so as to preserve the identities of the\n+# classes defined here.\n+if '_is_loaded' in globals():\n+    raise RuntimeError('Reloading numpy._globals is not allowed')\n+_is_loaded = True\n+\n+\n+# TODO: One day, we should remove the _set_module here before removing them\n+#       fully.  Not doing it now, just to allow unpickling to work on older\n+#       versions for a bit.  (Module exists since NumPy 1.25.)\n \n \n @_set_module('numpy')\n@@ -16,6 +30,29 @@ class ComplexWarning(RuntimeWarning):\n \n \n \n+@_set_module(\"numpy\")\n+class ModuleDeprecationWarning(DeprecationWarning):\n+    \"\"\"Module deprecation warning.\n+\n+    The nose tester turns ordinary Deprecation warnings into test failures.\n+    That makes it hard to deprecate whole modules, because they get\n+    imported by default. So this is a special Deprecation warning that the\n+    nose tester will let pass without making tests fail.\n+\n+    \"\"\"\n+\n+\n+@_set_module(\"numpy\")\n+class VisibleDeprecationWarning(UserWarning):\n+    \"\"\"Visible deprecation warning.\n+\n+    By default, python will not show deprecation warnings, so this class\n+    can be used when a very visible warning is helpful, for example because\n+    the usage is most likely a user bug.\n+\n+    \"\"\"\n+\n+\n # Exception used in shares_memory()\n @_set_module('numpy')\n class TooHardError(RuntimeError):\n",
            "comment_added_diff": {
                "8": "# Disallow reloading this module so as to preserve the identities of the",
                "9": "# classes defined here.",
                "15": "# TODO: One day, we should remove the _set_module here before removing them",
                "16": "#       fully.  Not doing it now, just to allow unpickling to work on older",
                "17": "#       versions for a bit.  (Module exists since NumPy 1.25.)"
            },
            "comment_deleted_diff": {},
            "comment_modified_diff": {}
        },
        {
            "commit": "86029d0188e371d70b90d224c3f0061997f86fc5",
            "timestamp": "2022-11-30T12:07:51+01:00",
            "author": "Sebastian Berg",
            "commit_message": "TYP: Modify typing stubs for new `np.exceptions` module",
            "additions": 1,
            "deletions": 0,
            "change_type": "MODIFY",
            "diff": "@@ -15,6 +15,7 @@\n # TODO: One day, we should remove the _set_module here before removing them\n #       fully.  Not doing it now, just to allow unpickling to work on older\n #       versions for a bit.  (Module exists since NumPy 1.25.)\n+#       This then also means that the typing stubs should be moved!\n \n \n @_set_module('numpy')\n",
            "comment_added_diff": {
                "18": "#       This then also means that the typing stubs should be moved!"
            },
            "comment_deleted_diff": {},
            "comment_modified_diff": {}
        },
        {
            "commit": "7dce375bcf99eb0019f6956241cce941c31ec8b4",
            "timestamp": "2022-11-30T12:07:51+01:00",
            "author": "Sebastian Berg",
            "commit_message": "DOC: Document exceptions and warnings in the refguide\n\nAxisError did exist, but e.g. ComplexWarning wasn't even properly\nincluded.",
            "additions": 40,
            "deletions": 2,
            "change_type": "MODIFY",
            "diff": "@@ -1,7 +1,40 @@\n+\"\"\"\n+Exceptions and Warnings (:mod:`numpy.exceptions`)\n+=================================================\n+\n+General exceptions used by NumPy.  Note that some exceptions may be module\n+specific, such as linear algebra errors.\n+\n+.. versionadded:: NumPy 1.24\n+\n+    The exceptions module is new in NumPy 1.24.  Older exceptions remain\n+    available through the main NumPy namespace for compatibility.\n+\n+.. currentmodule:: numpy.exceptions\n+\n+Warnings\n+--------\n+.. autosummary::\n+   :toctree: generated/\n+\n+   ComplexWarning             Given when converting complex to real.\n+   VisibleDeprecationWarning  Same as a DeprecationWarning, but more visible.\n+\n+Exceptions\n+----------\n+.. autosummary::\n+   :toctree: generated/\n+\n+    AxisError       Given when an axis was invalid.\n+    TooHardError    Error specific to `numpy.shares_memory`.\n+\n+\"\"\"\n+\n+\n from ._utils import set_module as _set_module\n \n __all__ = [\n-    \"ComplexWarning\", \"ModuleDeprecationWarning\", \"VisibleDeprecationWarning\",\n+    \"ComplexWarning\", \"VisibleDeprecationWarning\",\n     \"TooHardError\", \"AxisError\"]\n \n \n@@ -14,7 +47,7 @@\n \n # TODO: One day, we should remove the _set_module here before removing them\n #       fully.  Not doing it now, just to allow unpickling to work on older\n-#       versions for a bit.  (Module exists since NumPy 1.25.)\n+#       versions for a bit.  (Module exists since NumPy 1.24.)\n #       This then also means that the typing stubs should be moved!\n \n \n@@ -35,6 +68,11 @@ class ComplexWarning(RuntimeWarning):\n class ModuleDeprecationWarning(DeprecationWarning):\n     \"\"\"Module deprecation warning.\n \n+    .. warning::\n+\n+        This warning should not be used, since nose testing is not relvant\n+        anymore.\n+\n     The nose tester turns ordinary Deprecation warnings into test failures.\n     That makes it hard to deprecate whole modules, because they get\n     imported by default. So this is a special Deprecation warning that the\n",
            "comment_added_diff": {
                "50": "#       versions for a bit.  (Module exists since NumPy 1.24.)"
            },
            "comment_deleted_diff": {
                "17": "#       versions for a bit.  (Module exists since NumPy 1.25.)"
            },
            "comment_modified_diff": {}
        },
        {
            "commit": "7b1ee55a1c17748e150ce05ceb7b7df4415ff8c2",
            "timestamp": "2022-11-30T12:07:51+01:00",
            "author": "Sebastian Berg",
            "commit_message": "DOC: Update messages to NumPy 1.25 although I hope its 2.0 :)",
            "additions": 3,
            "deletions": 3,
            "change_type": "MODIFY",
            "diff": "@@ -5,9 +5,9 @@\n General exceptions used by NumPy.  Note that some exceptions may be module\n specific, such as linear algebra errors.\n \n-.. versionadded:: NumPy 1.24\n+.. versionadded:: NumPy 1.25\n \n-    The exceptions module is new in NumPy 1.24.  Older exceptions remain\n+    The exceptions module is new in NumPy 1.25.  Older exceptions remain\n     available through the main NumPy namespace for compatibility.\n \n .. currentmodule:: numpy.exceptions\n@@ -47,7 +47,7 @@\n \n # TODO: One day, we should remove the _set_module here before removing them\n #       fully.  Not doing it now, just to allow unpickling to work on older\n-#       versions for a bit.  (Module exists since NumPy 1.24.)\n+#       versions for a bit.  (Module exists since NumPy 1.25.)\n #       This then also means that the typing stubs should be moved!\n \n \n",
            "comment_added_diff": {
                "50": "#       versions for a bit.  (Module exists since NumPy 1.25.)"
            },
            "comment_deleted_diff": {
                "50": "#       versions for a bit.  (Module exists since NumPy 1.24.)"
            },
            "comment_modified_diff": {
                "50": "#       versions for a bit.  (Module exists since NumPy 1.24.)"
            }
        },
        {
            "commit": "3139a881346ff7ad4326ecd296e6eeddf6c268a0",
            "timestamp": "2022-12-13T11:30:22-08:00",
            "author": "Sebastian Berg",
            "commit_message": "MAINT: Remove two TODO notes that got outdated (#22788)\n\nThe first one should have been removed in gh-22735, the second an even more\r\nrandom find.",
            "additions": 0,
            "deletions": 8,
            "change_type": "MODIFY",
            "diff": "@@ -32,8 +32,6 @@\n \"\"\"\n \n \n-from ._utils import set_module as _set_module\n-\n __all__ = [\n     \"ComplexWarning\", \"VisibleDeprecationWarning\", \"ModuleDeprecationWarning\",\n     \"TooHardError\", \"AxisError\", \"DTypePromotionError\"]\n@@ -46,12 +44,6 @@\n _is_loaded = True\n \n \n-# TODO: One day, we should remove the _set_module here before removing them\n-#       fully.  Not doing it now, just to allow unpickling to work on older\n-#       versions for a bit.  (Module exists since NumPy 1.25.)\n-#       This then also means that the typing stubs should be moved!\n-\n-\n class ComplexWarning(RuntimeWarning):\n     \"\"\"\n     The warning raised when casting a complex dtype to a real dtype.\n",
            "comment_added_diff": {},
            "comment_deleted_diff": {
                "49": "# TODO: One day, we should remove the _set_module here before removing them",
                "50": "#       fully.  Not doing it now, just to allow unpickling to work on older",
                "51": "#       versions for a bit.  (Module exists since NumPy 1.25.)",
                "52": "#       This then also means that the typing stubs should be moved!"
            },
            "comment_modified_diff": {}
        }
    ],
    "exceptions.pyi": [],
    "routines.other.rst": [],
    "_internal.py": [],
    "common_dtype.c": [],
    "convert_datatype.h": [],
    "datetime.c": [],
    "_array_object.py": [
        {
            "commit": "de0521fc22e641be5e819a2fec785c6f89ebca8c",
            "timestamp": "2022-12-02T00:29:32+01:00",
            "author": "Bas van Beek",
            "commit_message": "MAINT: Let `ndarray.__imatmul__` handle inplace matrix multiplication in the array-api",
            "additions": 2,
            "deletions": 12,
            "change_type": "MODIFY",
            "diff": "@@ -850,23 +850,13 @@ def __imatmul__(self: Array, other: Array, /) -> Array:\n         \"\"\"\n         Performs the operation __imatmul__.\n         \"\"\"\n-        # Note: NumPy does not implement __imatmul__.\n-\n         # matmul is not defined for scalars, but without this, we may get\n         # the wrong error message from asarray.\n         other = self._check_allowed_dtypes(other, \"numeric\", \"__imatmul__\")\n         if other is NotImplemented:\n             return other\n-\n-        # __imatmul__ can only be allowed when it would not change the shape\n-        # of self.\n-        other_shape = other.shape\n-        if self.shape == () or other_shape == ():\n-            raise ValueError(\"@= requires at least one dimension\")\n-        if len(other_shape) == 1 or other_shape[-1] != other_shape[-2]:\n-            raise ValueError(\"@= cannot change the shape of the input array\")\n-        self._array[:] = self._array.__matmul__(other._array)\n-        return self\n+        res = self._array.__imatmul__(other._array)\n+        return self.__class__._new(res)\n \n     def __rmatmul__(self: Array, other: Array, /) -> Array:\n         \"\"\"\n",
            "comment_added_diff": {},
            "comment_deleted_diff": {
                "853": "        # Note: NumPy does not implement __imatmul__.",
                "861": "        # __imatmul__ can only be allowed when it would not change the shape",
                "862": "        # of self."
            },
            "comment_modified_diff": {}
        },
        {
            "commit": "8b63fc295bafea3efd3d115964200dbaac7be8a5",
            "timestamp": "2023-06-05T16:25:42-06:00",
            "author": "Aaron Meurer",
            "commit_message": "Update numpy.array_api magic methods for complex numbers\n\nUpdates from the v2022.12 version of the spec:\n\n- Add __complex__.\n- __float__, __int__, and __bool__ are now more lenient in what dtypes they\n  can operate on.\n- Support complex scalars and dtypes in all operators (except those that\n  should not operate on complex numbers).\n- Disallow integer scalars that are out of the bounds of the array dtype.\n- Update the tests accordingly.",
            "additions": 38,
            "deletions": 17,
            "change_type": "MODIFY",
            "diff": "@@ -24,6 +24,7 @@\n     _integer_dtypes,\n     _integer_or_boolean_dtypes,\n     _floating_dtypes,\n+    _complex_floating_dtypes,\n     _numeric_dtypes,\n     _result_type,\n     _dtype_categories,\n@@ -139,7 +140,7 @@ def _check_allowed_dtypes(self, other: bool | int | float | Array, dtype_categor\n \n         if self.dtype not in _dtype_categories[dtype_category]:\n             raise TypeError(f\"Only {dtype_category} dtypes are allowed in {op}\")\n-        if isinstance(other, (int, float, bool)):\n+        if isinstance(other, (int, complex, float, bool)):\n             other = self._promote_scalar(other)\n         elif isinstance(other, Array):\n             if other.dtype not in _dtype_categories[dtype_category]:\n@@ -189,11 +190,23 @@ def _promote_scalar(self, scalar):\n                 raise TypeError(\n                     \"Python int scalars cannot be promoted with bool arrays\"\n                 )\n+            if self.dtype in _integer_dtypes:\n+                info = np.iinfo(self.dtype)\n+                if not (info.min <= scalar <= info.max):\n+                    raise OverflowError(\n+                        \"Python int scalars must be within the bounds of the dtype for integer arrays\"\n+                    )\n+            # int + array(floating) is allowed\n         elif isinstance(scalar, float):\n             if self.dtype not in _floating_dtypes:\n                 raise TypeError(\n                     \"Python float scalars can only be promoted with floating-point arrays.\"\n                 )\n+        elif isinstance(scalar, complex):\n+            if self.dtype not in _complex_floating_dtypes:\n+                raise TypeError(\n+                    \"Python complex scalars can only be promoted with complex floating-point arrays.\"\n+                )\n         else:\n             raise TypeError(\"'scalar' must be a Python scalar\")\n \n@@ -454,11 +467,19 @@ def __bool__(self: Array, /) -> bool:\n         # Note: This is an error here.\n         if self._array.ndim != 0:\n             raise TypeError(\"bool is only allowed on arrays with 0 dimensions\")\n-        if self.dtype not in _boolean_dtypes:\n-            raise ValueError(\"bool is only allowed on boolean arrays\")\n         res = self._array.__bool__()\n         return res\n \n+    def __complex__(self: Array, /) -> float:\n+        \"\"\"\n+        Performs the operation __complex__.\n+        \"\"\"\n+        # Note: This is an error here.\n+        if self._array.ndim != 0:\n+            raise TypeError(\"complex is only allowed on arrays with 0 dimensions\")\n+        res = self._array.__complex__()\n+        return res\n+\n     def __dlpack__(self: Array, /, *, stream: None = None) -> PyCapsule:\n         \"\"\"\n         Performs the operation __dlpack__.\n@@ -492,8 +513,8 @@ def __float__(self: Array, /) -> float:\n         # Note: This is an error here.\n         if self._array.ndim != 0:\n             raise TypeError(\"float is only allowed on arrays with 0 dimensions\")\n-        if self.dtype not in _floating_dtypes:\n-            raise ValueError(\"float is only allowed on floating-point arrays\")\n+        if self.dtype in _complex_floating_dtypes:\n+            raise TypeError(\"float is not allowed on complex floating-point arrays\")\n         res = self._array.__float__()\n         return res\n \n@@ -501,7 +522,7 @@ def __floordiv__(self: Array, other: Union[int, float, Array], /) -> Array:\n         \"\"\"\n         Performs the operation __floordiv__.\n         \"\"\"\n-        other = self._check_allowed_dtypes(other, \"numeric\", \"__floordiv__\")\n+        other = self._check_allowed_dtypes(other, \"real numeric\", \"__floordiv__\")\n         if other is NotImplemented:\n             return other\n         self, other = self._normalize_two_args(self, other)\n@@ -512,7 +533,7 @@ def __ge__(self: Array, other: Union[int, float, Array], /) -> Array:\n         \"\"\"\n         Performs the operation __ge__.\n         \"\"\"\n-        other = self._check_allowed_dtypes(other, \"numeric\", \"__ge__\")\n+        other = self._check_allowed_dtypes(other, \"real numeric\", \"__ge__\")\n         if other is NotImplemented:\n             return other\n         self, other = self._normalize_two_args(self, other)\n@@ -542,7 +563,7 @@ def __gt__(self: Array, other: Union[int, float, Array], /) -> Array:\n         \"\"\"\n         Performs the operation __gt__.\n         \"\"\"\n-        other = self._check_allowed_dtypes(other, \"numeric\", \"__gt__\")\n+        other = self._check_allowed_dtypes(other, \"real numeric\", \"__gt__\")\n         if other is NotImplemented:\n             return other\n         self, other = self._normalize_two_args(self, other)\n@@ -556,8 +577,8 @@ def __int__(self: Array, /) -> int:\n         # Note: This is an error here.\n         if self._array.ndim != 0:\n             raise TypeError(\"int is only allowed on arrays with 0 dimensions\")\n-        if self.dtype not in _integer_dtypes:\n-            raise ValueError(\"int is only allowed on integer arrays\")\n+        if self.dtype in _complex_floating_dtypes:\n+            raise TypeError(\"int is not allowed on complex floating-point arrays\")\n         res = self._array.__int__()\n         return res\n \n@@ -581,7 +602,7 @@ def __le__(self: Array, other: Union[int, float, Array], /) -> Array:\n         \"\"\"\n         Performs the operation __le__.\n         \"\"\"\n-        other = self._check_allowed_dtypes(other, \"numeric\", \"__le__\")\n+        other = self._check_allowed_dtypes(other, \"real numeric\", \"__le__\")\n         if other is NotImplemented:\n             return other\n         self, other = self._normalize_two_args(self, other)\n@@ -603,7 +624,7 @@ def __lt__(self: Array, other: Union[int, float, Array], /) -> Array:\n         \"\"\"\n         Performs the operation __lt__.\n         \"\"\"\n-        other = self._check_allowed_dtypes(other, \"numeric\", \"__lt__\")\n+        other = self._check_allowed_dtypes(other, \"real numeric\", \"__lt__\")\n         if other is NotImplemented:\n             return other\n         self, other = self._normalize_two_args(self, other)\n@@ -626,7 +647,7 @@ def __mod__(self: Array, other: Union[int, float, Array], /) -> Array:\n         \"\"\"\n         Performs the operation __mod__.\n         \"\"\"\n-        other = self._check_allowed_dtypes(other, \"numeric\", \"__mod__\")\n+        other = self._check_allowed_dtypes(other, \"real numeric\", \"__mod__\")\n         if other is NotImplemented:\n             return other\n         self, other = self._normalize_two_args(self, other)\n@@ -808,7 +829,7 @@ def __ifloordiv__(self: Array, other: Union[int, float, Array], /) -> Array:\n         \"\"\"\n         Performs the operation __ifloordiv__.\n         \"\"\"\n-        other = self._check_allowed_dtypes(other, \"numeric\", \"__ifloordiv__\")\n+        other = self._check_allowed_dtypes(other, \"real numeric\", \"__ifloordiv__\")\n         if other is NotImplemented:\n             return other\n         self._array.__ifloordiv__(other._array)\n@@ -818,7 +839,7 @@ def __rfloordiv__(self: Array, other: Union[int, float, Array], /) -> Array:\n         \"\"\"\n         Performs the operation __rfloordiv__.\n         \"\"\"\n-        other = self._check_allowed_dtypes(other, \"numeric\", \"__rfloordiv__\")\n+        other = self._check_allowed_dtypes(other, \"real numeric\", \"__rfloordiv__\")\n         if other is NotImplemented:\n             return other\n         self, other = self._normalize_two_args(self, other)\n@@ -874,7 +895,7 @@ def __imod__(self: Array, other: Union[int, float, Array], /) -> Array:\n         \"\"\"\n         Performs the operation __imod__.\n         \"\"\"\n-        other = self._check_allowed_dtypes(other, \"numeric\", \"__imod__\")\n+        other = self._check_allowed_dtypes(other, \"real numeric\", \"__imod__\")\n         if other is NotImplemented:\n             return other\n         self._array.__imod__(other._array)\n@@ -884,7 +905,7 @@ def __rmod__(self: Array, other: Union[int, float, Array], /) -> Array:\n         \"\"\"\n         Performs the operation __rmod__.\n         \"\"\"\n-        other = self._check_allowed_dtypes(other, \"numeric\", \"__rmod__\")\n+        other = self._check_allowed_dtypes(other, \"real numeric\", \"__rmod__\")\n         if other is NotImplemented:\n             return other\n         self, other = self._normalize_two_args(self, other)\n",
            "comment_added_diff": {
                "199": "            # int + array(floating) is allowed",
                "477": "        # Note: This is an error here."
            },
            "comment_deleted_diff": {},
            "comment_modified_diff": {}
        }
    ],
    "number.c": [],
    "bench_ma.py": [
        {
            "commit": "ecf195df053b619de21871653431154033cbd462",
            "timestamp": "2022-12-05T02:46:03+00:00",
            "author": "Rohit Goswami",
            "commit_message": "BENCH: Port ma/bench.py to asv",
            "additions": 146,
            "deletions": 0,
            "change_type": "MODIFY",
            "diff": "@@ -111,3 +111,149 @@ def setup(self, mode, n):\n \n     def time_it(self, mode, n):\n         np.ma.concatenate(self.args)\n+\n+\n+class MAFunctions1v(Benchmark):\n+    param_names = ['mtype', 'func', 'msize']\n+    params = [['np', 'np.ma'],\n+              ['sin', 'log', 'sqrt'],\n+              ['small', 'big']]\n+\n+    def setup(self, mtype, func, msize):\n+        xs = np.random.uniform(-1, 1, 6).reshape(2, 3)\n+        m1 = [[True, False, False], [False, False, True]]\n+        xl = np.random.uniform(-1, 1, 100*100).reshape(100, 100)\n+        maskx = xl > 0.8\n+        self.nmxs = np.ma.array(xs, mask=m1)\n+        self.nmxl = np.ma.array(xl, mask=maskx)\n+\n+    def time_functions_1v(self, mtype, func, msize):\n+        # fun = {'np.ma.sin': np.ma.sin, 'np.sin': np.sin}[func]\n+        fun = eval(f\"{mtype}.{func}\")\n+        if msize == 'small':\n+            fun(self.nmxs)\n+        elif msize == 'big':\n+            fun(self.nmxl)\n+\n+\n+class MAMethod0v(Benchmark):\n+    param_names = ['method', 'msize']\n+    params = [['ravel', 'transpose', 'compressed', 'conjugate'],\n+              ['small', 'big']]\n+\n+    def setup(self, method, msize):\n+        xs = np.random.uniform(-1, 1, 6).reshape(2, 3)\n+        m1 = [[True, False, False], [False, False, True]]\n+        xl = np.random.uniform(-1, 1, 100*100).reshape(100, 100)\n+        maskx = xl > 0.8\n+        self.nmxs = np.ma.array(xs, mask=m1)\n+        self.nmxl = np.ma.array(xl, mask=maskx)\n+\n+    def time_methods_0v(self, method, msize):\n+        if msize == 'small':\n+            mdat = self.nmxs\n+        elif msize == 'big':\n+            mdat = self.nmxl\n+        getattr(mdat, method)()\n+\n+\n+class MAFunctions2v(Benchmark):\n+    param_names = ['mtype', 'func', 'msize']\n+    params = [['np', 'np.ma'],\n+              ['multiply', 'divide', 'power'],\n+              ['small', 'big']]\n+\n+    def setup(self, mtype, func, msize):\n+        # Small arrays\n+        xs = np.random.uniform(-1, 1, 6).reshape(2, 3)\n+        ys = np.random.uniform(-1, 1, 6).reshape(2, 3)\n+        m1 = [[True, False, False], [False, False, True]]\n+        m2 = [[True, False, True], [False, False, True]]\n+        self.nmxs = np.ma.array(xs, mask=m1)\n+        self.nmys = np.ma.array(ys, mask=m2)\n+        # Big arrays\n+        xl = np.random.uniform(-1, 1, 100*100).reshape(100, 100)\n+        yl = np.random.uniform(-1, 1, 100*100).reshape(100, 100)\n+        maskx = xl > 0.8\n+        masky = yl < -0.8\n+        self.nmxl = np.ma.array(xl, mask=maskx)\n+        self.nmyl = np.ma.array(yl, mask=masky)\n+\n+    def time_functions_2v(self, mtype, func, msize):\n+        fun = eval(f\"{mtype}.{func}\")\n+        if msize == 'small':\n+            fun(self.nmxs, self.nmys)\n+        elif msize == 'big':\n+            fun(self.nmxl, self.nmyl)\n+\n+\n+class MAMethodGetItem(Benchmark):\n+    param_names = ['margs', 'msize']\n+    params = [[0, (0, 0), [0, -1]],\n+              ['small', 'big']]\n+\n+    def setup(self, margs, msize):\n+        xs = np.random.uniform(-1, 1, 6).reshape(2, 3)\n+        m1 = [[True, False, False], [False, False, True]]\n+        xl = np.random.uniform(-1, 1, 100*100).reshape(100, 100)\n+        maskx = xl > 0.8\n+        self.nmxs = np.ma.array(xs, mask=m1)\n+        self.nmxl = np.ma.array(xl, mask=maskx)\n+\n+    def time_methods_getitem(self, margs, msize):\n+        if msize == 'small':\n+            mdat = self.nmxs\n+        elif msize == 'big':\n+            mdat = self.nmxl\n+        getattr(mdat, '__getitem__')(margs)\n+\n+\n+class MAMethodSetItem(Benchmark):\n+    param_names = ['margs', 'mset', 'msize']\n+    params = [[0, (0, 0), (-1, 0)],\n+              [17, np.ma.masked],\n+              ['small', 'big']]\n+\n+    def setup(self, margs, mset, msize):\n+        xs = np.random.uniform(-1, 1, 6).reshape(2, 3)\n+        m1 = [[True, False, False], [False, False, True]]\n+        xl = np.random.uniform(-1, 1, 100*100).reshape(100, 100)\n+        maskx = xl > 0.8\n+        self.nmxs = np.ma.array(xs, mask=m1)\n+        self.nmxl = np.ma.array(xl, mask=maskx)\n+\n+    def time_methods_setitem(self, margs, mset, msize):\n+        if msize == 'small':\n+            mdat = self.nmxs\n+        elif msize == 'big':\n+            mdat = self.nmxl\n+        getattr(mdat, '__setitem__')(margs, mset)\n+\n+\n+class Where(Benchmark):\n+    param_names = ['mtype', 'msize']\n+    params = [['np', 'np.ma'],\n+              ['small', 'big']]\n+\n+    def setup(self, mtype, msize):\n+        # Small arrays\n+        xs = np.random.uniform(-1, 1, 6).reshape(2, 3)\n+        ys = np.random.uniform(-1, 1, 6).reshape(2, 3)\n+        m1 = [[True, False, False], [False, False, True]]\n+        m2 = [[True, False, True], [False, False, True]]\n+        self.nmxs = np.ma.array(xs, mask=m1)\n+        self.nmys = np.ma.array(ys, mask=m2)\n+        # Big arrays\n+        xl = np.random.uniform(-1, 1, 100*100).reshape(100, 100)\n+        yl = np.random.uniform(-1, 1, 100*100).reshape(100, 100)\n+        maskx = xl > 0.8\n+        masky = yl < -0.8\n+        self.nmxl = np.ma.array(xl, mask=maskx)\n+        self.nmyl = np.ma.array(yl, mask=masky)\n+\n+    def time_where(self, mtype, msize):\n+        fun = eval(f\"{mtype}.where\")\n+        if msize == 'small':\n+            fun(self.nmxs > 2, self.nmxs, self.nmys)\n+        elif msize == 'big':\n+            fun(self.nmxl > 2, self.nmxl, self.nmyl)\n",
            "comment_added_diff": {
                "131": "        # fun = {'np.ma.sin': np.ma.sin, 'np.sin': np.sin}[func]",
                "167": "        # Small arrays",
                "174": "        # Big arrays",
                "239": "        # Small arrays",
                "246": "        # Big arrays"
            },
            "comment_deleted_diff": {},
            "comment_modified_diff": {}
        }
    ],
    "bench.py": [
        {
            "commit": "9cf2cef43f683196242aba6450267049d956b41a",
            "timestamp": "2022-12-05T02:46:22+00:00",
            "author": "Rohit Goswami",
            "commit_message": "MAINT: Kill old hand-written benchmarks",
            "additions": 0,
            "deletions": 130,
            "change_type": "DELETE",
            "diff": "@@ -1,130 +0,0 @@\n-#!/usr/bin/env python3\n-\n-import timeit\n-import numpy\n-\n-\n-###############################################################################\n-#                               Global variables                              #\n-###############################################################################\n-\n-\n-# Small arrays\n-xs = numpy.random.uniform(-1, 1, 6).reshape(2, 3)\n-ys = numpy.random.uniform(-1, 1, 6).reshape(2, 3)\n-zs = xs + 1j * ys\n-m1 = [[True, False, False], [False, False, True]]\n-m2 = [[True, False, True], [False, False, True]]\n-nmxs = numpy.ma.array(xs, mask=m1)\n-nmys = numpy.ma.array(ys, mask=m2)\n-nmzs = numpy.ma.array(zs, mask=m1)\n-\n-# Big arrays\n-xl = numpy.random.uniform(-1, 1, 100*100).reshape(100, 100)\n-yl = numpy.random.uniform(-1, 1, 100*100).reshape(100, 100)\n-zl = xl + 1j * yl\n-maskx = xl > 0.8\n-masky = yl < -0.8\n-nmxl = numpy.ma.array(xl, mask=maskx)\n-nmyl = numpy.ma.array(yl, mask=masky)\n-nmzl = numpy.ma.array(zl, mask=maskx)\n-\n-\n-###############################################################################\n-#                                 Functions                                   #\n-###############################################################################\n-\n-\n-def timer(s, v='', nloop=500, nrep=3):\n-    units = [\"s\", \"ms\", \"\u00b5s\", \"ns\"]\n-    scaling = [1, 1e3, 1e6, 1e9]\n-    print(\"%s : %-50s : \" % (v, s), end=' ')\n-    varnames = [\"%ss,nm%ss,%sl,nm%sl\" % tuple(x*4) for x in 'xyz']\n-    setup = 'from __main__ import numpy, ma, %s' % ','.join(varnames)\n-    Timer = timeit.Timer(stmt=s, setup=setup)\n-    best = min(Timer.repeat(nrep, nloop)) / nloop\n-    if best > 0.0:\n-        order = min(-int(numpy.floor(numpy.log10(best)) // 3), 3)\n-    else:\n-        order = 3\n-    print(\"%d loops, best of %d: %.*g %s per loop\" % (nloop, nrep,\n-                                                      3,\n-                                                      best * scaling[order],\n-                                                      units[order]))\n-\n-\n-def compare_functions_1v(func, nloop=500,\n-                       xs=xs, nmxs=nmxs, xl=xl, nmxl=nmxl):\n-    funcname = func.__name__\n-    print(\"-\"*50)\n-    print(f'{funcname} on small arrays')\n-    module, data = \"numpy.ma\", \"nmxs\"\n-    timer(\"%(module)s.%(funcname)s(%(data)s)\" % locals(), v=\"%11s\" % module, nloop=nloop)\n-\n-    print(\"%s on large arrays\" % funcname)\n-    module, data = \"numpy.ma\", \"nmxl\"\n-    timer(\"%(module)s.%(funcname)s(%(data)s)\" % locals(), v=\"%11s\" % module, nloop=nloop)\n-    return\n-\n-def compare_methods(methodname, args, vars='x', nloop=500, test=True,\n-                    xs=xs, nmxs=nmxs, xl=xl, nmxl=nmxl):\n-    print(\"-\"*50)\n-    print(f'{methodname} on small arrays')\n-    data, ver = f'nm{vars}l', 'numpy.ma'\n-    timer(\"%(data)s.%(methodname)s(%(args)s)\" % locals(), v=ver, nloop=nloop)\n-\n-    print(\"%s on large arrays\" % methodname)\n-    data, ver = \"nm%sl\" % vars, 'numpy.ma'\n-    timer(\"%(data)s.%(methodname)s(%(args)s)\" % locals(), v=ver, nloop=nloop)\n-    return\n-\n-def compare_functions_2v(func, nloop=500, test=True,\n-                       xs=xs, nmxs=nmxs,\n-                       ys=ys, nmys=nmys,\n-                       xl=xl, nmxl=nmxl,\n-                       yl=yl, nmyl=nmyl):\n-    funcname = func.__name__\n-    print(\"-\"*50)\n-    print(f'{funcname} on small arrays')\n-    module, data = \"numpy.ma\", \"nmxs,nmys\"\n-    timer(\"%(module)s.%(funcname)s(%(data)s)\" % locals(), v=\"%11s\" % module, nloop=nloop)\n-\n-    print(f'{funcname} on large arrays')\n-    module, data = \"numpy.ma\", \"nmxl,nmyl\"\n-    timer(\"%(module)s.%(funcname)s(%(data)s)\" % locals(), v=\"%11s\" % module, nloop=nloop)\n-    return\n-\n-\n-if __name__ == '__main__':\n-    compare_functions_1v(numpy.sin)\n-    compare_functions_1v(numpy.log)\n-    compare_functions_1v(numpy.sqrt)\n-\n-    compare_functions_2v(numpy.multiply)\n-    compare_functions_2v(numpy.divide)\n-    compare_functions_2v(numpy.power)\n-\n-    compare_methods('ravel', '', nloop=1000)\n-    compare_methods('conjugate', '', 'z', nloop=1000)\n-    compare_methods('transpose', '', nloop=1000)\n-    compare_methods('compressed', '', nloop=1000)\n-    compare_methods('__getitem__', '0', nloop=1000)\n-    compare_methods('__getitem__', '(0,0)', nloop=1000)\n-    compare_methods('__getitem__', '[0,-1]', nloop=1000)\n-    compare_methods('__setitem__', '0, 17', nloop=1000, test=False)\n-    compare_methods('__setitem__', '(0,0), 17', nloop=1000, test=False)\n-\n-    print(\"-\"*50)\n-    print(\"__setitem__ on small arrays\")\n-    timer('nmxs.__setitem__((-1,0),numpy.ma.masked)', 'numpy.ma   ', nloop=10000)\n-\n-    print(\"-\"*50)\n-    print(\"__setitem__ on large arrays\")\n-    timer('nmxl.__setitem__((-1,0),numpy.ma.masked)', 'numpy.ma   ', nloop=10000)\n-\n-    print(\"-\"*50)\n-    print(\"where on small arrays\")\n-    timer('numpy.ma.where(nmxs>2,nmxs,nmys)', 'numpy.ma   ', nloop=1000)\n-    print(\"-\"*50)\n-    print(\"where on large arrays\")\n-    timer('numpy.ma.where(nmxl>2,nmxl,nmyl)', 'numpy.ma   ', nloop=100)\n",
            "comment_added_diff": {},
            "comment_deleted_diff": {
                "1": "#!/usr/bin/env python3",
                "7": "###############################################################################",
                "8": "#                               Global variables                              #",
                "9": "###############################################################################",
                "12": "# Small arrays",
                "22": "# Big arrays",
                "33": "###############################################################################",
                "34": "#                                 Functions                                   #",
                "35": "###############################################################################"
            },
            "comment_modified_diff": {}
        }
    ],
    "nanfunctions.py": [
        {
            "commit": "ab2178b47c0ee834180c318db196976623710691",
            "timestamp": "2023-07-07T08:57:37+03:00",
            "author": "Ronald van Elburg",
            "commit_message": "ENH: add mean keyword to std and var (#24126)\n\n* Add mean keyword to std and var functions.\r\n\r\n* Add releae note for mean keyword to std and var functions.\r\n\r\n* Update release note with PR number\r\n\r\n* Address lint issue.\r\n\r\n* Align nan signatures with new signatures.\r\n\r\n* Address lint issue.\r\n\r\n* Correct version numbers on keywords.\r\n\r\n* Put backticks on keyword argument in documentation string.\r\n\r\n* Cleanuup assert statements in tests\r\n\r\n* Move comparison of in and out arrays closer to the function call.\r\n\r\n* Remove clutter from example code in release note.\r\n\r\n* Add test for nanstd and fix error in nanvar\r\n\r\n* haqndle \"mean\" keyword for var and std on MaskedArrays.\r\n\r\n* Address lint issues.\r\n\r\n* update the dispatchers according to suggestions by Marten van Kerkwijk:\r\n\r\n(https://github.com/numpy/numpy/pull/24126#discussion_r1254314302) The dispatcher returns all arguments that can, in principle, contain something that is an array and hence that, if not a regular ndarray, can allow the function to be dealt with another package (say, dask). Since mean can be an array, it should be added (most similar to where).\r\n\r\n* Move and adjust example from release note to doc-strings. Reflow doc-string.\r\n\r\n* Improve doc-string. Shorter sentences and add type and label mean argument\r\n\r\n* Remove some of these pesky trailing white spaces\r\n\r\n* Make extra white lines more consistent.\r\n\r\n* Make sure code examples execute without Jupyter magic.\r\n\r\n* Fold lines to pass linter.\r\n\r\n* Update doc-string nanstd and nanvar.\r\n\r\n* Try to satisfy linter and apple requirements at the same time. Making the example code ugly, alas!\r\n\r\n* Make doctest skip resource dependent output",
            "additions": 36,
            "deletions": 14,
            "change_type": "MODIFY",
            "diff": "@@ -1610,13 +1610,13 @@ def _nanquantile_1d(arr1d, q, overwrite_input=False, method=\"linear\"):\n \n \n def _nanvar_dispatcher(a, axis=None, dtype=None, out=None, ddof=None,\n-                       keepdims=None, *, where=None):\n+                       keepdims=None, *, where=None, mean=None):\n     return (a, out)\n \n \n @array_function_dispatch(_nanvar_dispatcher)\n def nanvar(a, axis=None, dtype=None, out=None, ddof=0, keepdims=np._NoValue,\n-           *, where=np._NoValue):\n+           *, where=np._NoValue, mean=np._NoValue):\n     \"\"\"\n     Compute the variance along the specified axis, while ignoring NaNs.\n \n@@ -1659,6 +1659,14 @@ def nanvar(a, axis=None, dtype=None, out=None, ddof=0, keepdims=np._NoValue,\n \n         .. versionadded:: 1.22.0\n \n+    mean : array like, optional\n+        Provide the mean to prevent its recalculation. The mean should have\n+        a shape as if it was calculated with ``keepdims=True``.\n+        The axis for the calculation of the mean should be the same as used in\n+        the call to this var function.\n+\n+        .. versionadded:: 1.26.0\n+\n     Returns\n     -------\n     variance : ndarray, see dtype parameter above\n@@ -1713,7 +1721,7 @@ def nanvar(a, axis=None, dtype=None, out=None, ddof=0, keepdims=np._NoValue,\n     arr, mask = _replace_nan(a, 0)\n     if mask is None:\n         return np.var(arr, axis=axis, dtype=dtype, out=out, ddof=ddof,\n-                      keepdims=keepdims, where=where)\n+                      keepdims=keepdims, where=where, mean=mean)\n \n     if dtype is not None:\n         dtype = np.dtype(dtype)\n@@ -1727,15 +1735,21 @@ def nanvar(a, axis=None, dtype=None, out=None, ddof=0, keepdims=np._NoValue,\n         _keepdims = np._NoValue\n     else:\n         _keepdims = True\n-    # we need to special case matrix for reverse compatibility\n-    # in order for this to work, these sums need to be called with\n-    # keepdims=True, however matrix now raises an error in this case, but\n-    # the reason that it drops the keepdims kwarg is to force keepdims=True\n-    # so this used to work by serendipity.\n+\n     cnt = np.sum(~mask, axis=axis, dtype=np.intp, keepdims=_keepdims,\n-                 where=where)\n-    avg = np.sum(arr, axis=axis, dtype=dtype, keepdims=_keepdims, where=where)\n-    avg = _divide_by_count(avg, cnt)\n+                     where=where)\n+\n+    if mean is not np._NoValue:\n+        avg = mean\n+    else:\n+        # we need to special case matrix for reverse compatibility\n+        # in order for this to work, these sums need to be called with\n+        # keepdims=True, however matrix now raises an error in this case, but\n+        # the reason that it drops the keepdims kwarg is to force keepdims=True\n+        # so this used to work by serendipity.\n+        avg = np.sum(arr, axis=axis, dtype=dtype,\n+                     keepdims=_keepdims, where=where)\n+        avg = _divide_by_count(avg, cnt)\n \n     # Compute squared deviation from mean.\n     np.subtract(arr, avg, out=arr, casting='unsafe', where=where)\n@@ -1771,13 +1785,13 @@ def nanvar(a, axis=None, dtype=None, out=None, ddof=0, keepdims=np._NoValue,\n \n \n def _nanstd_dispatcher(a, axis=None, dtype=None, out=None, ddof=None,\n-                       keepdims=None, *, where=None):\n+                       keepdims=None, *, where=None, mean=None):\n     return (a, out)\n \n \n @array_function_dispatch(_nanstd_dispatcher)\n def nanstd(a, axis=None, dtype=None, out=None, ddof=0, keepdims=np._NoValue,\n-           *, where=np._NoValue):\n+           *, where=np._NoValue, mean=np._NoValue):\n     \"\"\"\n     Compute the standard deviation along the specified axis, while\n     ignoring NaNs.\n@@ -1827,6 +1841,14 @@ def nanstd(a, axis=None, dtype=None, out=None, ddof=0, keepdims=np._NoValue,\n \n         .. versionadded:: 1.22.0\n \n+    mean : array like, optional\n+        Provide the mean to prevent its recalculation. The mean should have\n+        a shape as if it was calculated with ``keepdims=True``.\n+        The axis for the calculation of the mean should be the same as used in\n+        the call to this std function.\n+\n+        .. versionadded:: 1.26.0\n+\n     Returns\n     -------\n     standard_deviation : ndarray, see dtype parameter above.\n@@ -1877,7 +1899,7 @@ def nanstd(a, axis=None, dtype=None, out=None, ddof=0, keepdims=np._NoValue,\n \n     \"\"\"\n     var = nanvar(a, axis=axis, dtype=dtype, out=out, ddof=ddof,\n-                 keepdims=keepdims, where=where)\n+                 keepdims=keepdims, where=where, mean=mean)\n     if isinstance(var, np.ndarray):\n         std = np.sqrt(var, out=var)\n     elif hasattr(var, 'dtype'):\n",
            "comment_added_diff": {
                "1745": "        # we need to special case matrix for reverse compatibility",
                "1746": "        # in order for this to work, these sums need to be called with",
                "1747": "        # keepdims=True, however matrix now raises an error in this case, but",
                "1748": "        # the reason that it drops the keepdims kwarg is to force keepdims=True",
                "1749": "        # so this used to work by serendipity."
            },
            "comment_deleted_diff": {
                "1730": "    # we need to special case matrix for reverse compatibility",
                "1731": "    # in order for this to work, these sums need to be called with",
                "1732": "    # keepdims=True, however matrix now raises an error in this case, but",
                "1733": "    # the reason that it drops the keepdims kwarg is to force keepdims=True",
                "1734": "    # so this used to work by serendipity."
            },
            "comment_modified_diff": {}
        }
    ],
    "test_nanfunctions.py": [
        {
            "commit": "91432a36a3611c2374ea9e2d45592f0ac5e71adb",
            "timestamp": "2022-12-05T15:53:32-07:00",
            "author": "Roy Smart",
            "commit_message": "BUG: `keepdims=True` is ignored if `out` is not `None` in `numpy.median()`, `numpy.percentile()`, and `numpy.quantile()`.\n\nCloses #22714, #22544.",
            "additions": 58,
            "deletions": 0,
            "change_type": "MODIFY",
            "diff": "@@ -3,6 +3,7 @@\n import inspect\n \n import numpy as np\n+from numpy.core.numeric import normalize_axis_tuple\n from numpy.lib.nanfunctions import _nan_mask, _replace_nan\n from numpy.testing import (\n     assert_, assert_equal, assert_almost_equal, assert_raises,\n@@ -807,6 +808,33 @@ def test_keepdims(self):\n             res = np.nanmedian(d, axis=(0, 1, 3), keepdims=True)\n             assert_equal(res.shape, (1, 1, 7, 1))\n \n+    @pytest.mark.parametrize(\n+        argnames='axis',\n+        argvalues=[\n+            None,\n+            1,\n+            (1, ),\n+            (0, 1),\n+            (-3, -1),\n+        ]\n+    )\n+    def test_keepdims_out(self, axis):\n+        d = np.ones((3, 5, 7, 11))\n+        # Randomly set some elements to NaN:\n+        w = np.random.random((4, 200)) * np.array(d.shape)[:, None]\n+        w = w.astype(np.intp)\n+        d[tuple(w)] = np.nan\n+        if axis is None:\n+            shape_out = (1,) * d.ndim\n+        else:\n+            axis_norm = normalize_axis_tuple(axis, d.ndim)\n+            shape_out = tuple(\n+                1 if i in axis_norm else d.shape[i] for i in range(d.ndim))\n+        out = np.empty(shape_out)\n+        result = np.nanmedian(d, axis=axis, keepdims=True, out=out)\n+        assert result is out\n+        assert_equal(result.shape, shape_out)\n+\n     def test_out(self):\n         mat = np.random.rand(3, 3)\n         nan_mat = np.insert(mat, [0, 2], np.nan, axis=1)\n@@ -982,6 +1010,36 @@ def test_keepdims(self):\n             res = np.nanpercentile(d, 90, axis=(0, 1, 3), keepdims=True)\n             assert_equal(res.shape, (1, 1, 7, 1))\n \n+    @pytest.mark.parametrize('q', [7, [1, 7]])\n+    @pytest.mark.parametrize(\n+        argnames='axis',\n+        argvalues=[\n+            None,\n+            1,\n+            (1,),\n+            (0, 1),\n+            (-3, -1),\n+        ]\n+    )\n+    def test_keepdims_out(self, q, axis):\n+        d = np.ones((3, 5, 7, 11))\n+        # Randomly set some elements to NaN:\n+        w = np.random.random((4, 200)) * np.array(d.shape)[:, None]\n+        w = w.astype(np.intp)\n+        d[tuple(w)] = np.nan\n+        if axis is None:\n+            shape_out = (1,) * d.ndim\n+        else:\n+            axis_norm = normalize_axis_tuple(axis, d.ndim)\n+            shape_out = tuple(\n+                1 if i in axis_norm else d.shape[i] for i in range(d.ndim))\n+        shape_out = np.shape(q) + shape_out\n+\n+        out = np.empty(shape_out)\n+        result = np.nanpercentile(d, q, axis=axis, keepdims=True, out=out)\n+        assert result is out\n+        assert_equal(result.shape, shape_out)\n+\n     def test_out(self):\n         mat = np.random.rand(3, 3)\n         nan_mat = np.insert(mat, [0, 2], np.nan, axis=1)\n",
            "comment_added_diff": {
                "823": "        # Randomly set some elements to NaN:",
                "1026": "        # Randomly set some elements to NaN:"
            },
            "comment_deleted_diff": {},
            "comment_modified_diff": {}
        },
        {
            "commit": "ab2178b47c0ee834180c318db196976623710691",
            "timestamp": "2023-07-07T08:57:37+03:00",
            "author": "Ronald van Elburg",
            "commit_message": "ENH: add mean keyword to std and var (#24126)\n\n* Add mean keyword to std and var functions.\r\n\r\n* Add releae note for mean keyword to std and var functions.\r\n\r\n* Update release note with PR number\r\n\r\n* Address lint issue.\r\n\r\n* Align nan signatures with new signatures.\r\n\r\n* Address lint issue.\r\n\r\n* Correct version numbers on keywords.\r\n\r\n* Put backticks on keyword argument in documentation string.\r\n\r\n* Cleanuup assert statements in tests\r\n\r\n* Move comparison of in and out arrays closer to the function call.\r\n\r\n* Remove clutter from example code in release note.\r\n\r\n* Add test for nanstd and fix error in nanvar\r\n\r\n* haqndle \"mean\" keyword for var and std on MaskedArrays.\r\n\r\n* Address lint issues.\r\n\r\n* update the dispatchers according to suggestions by Marten van Kerkwijk:\r\n\r\n(https://github.com/numpy/numpy/pull/24126#discussion_r1254314302) The dispatcher returns all arguments that can, in principle, contain something that is an array and hence that, if not a regular ndarray, can allow the function to be dealt with another package (say, dask). Since mean can be an array, it should be added (most similar to where).\r\n\r\n* Move and adjust example from release note to doc-strings. Reflow doc-string.\r\n\r\n* Improve doc-string. Shorter sentences and add type and label mean argument\r\n\r\n* Remove some of these pesky trailing white spaces\r\n\r\n* Make extra white lines more consistent.\r\n\r\n* Make sure code examples execute without Jupyter magic.\r\n\r\n* Fold lines to pass linter.\r\n\r\n* Update doc-string nanstd and nanvar.\r\n\r\n* Try to satisfy linter and apple requirements at the same time. Making the example code ugly, alas!\r\n\r\n* Make doctest skip resource dependent output",
            "additions": 35,
            "deletions": 0,
            "change_type": "MODIFY",
            "diff": "@@ -769,6 +769,41 @@ def test_where(self, dtype):\n             assert ret.dtype == dtype_reference\n             np.testing.assert_allclose(ret, reference)\n \n+    def test_nanstd_with_mean_keyword(self):\n+        # Setting the seed to make the test reproducable\n+        rng = np.random.RandomState(1234)\n+        A = rng.randn(10, 20, 5) + 0.5\n+        A[:, 5, :] = np.nan\n+\n+        mean_out = np.zeros((10, 1, 5))\n+        std_out = np.zeros((10, 1, 5))\n+\n+        mean = np.nanmean(A,\n+                       out=mean_out,\n+                       axis=1,\n+                       keepdims=True)\n+\n+        # The returned  object should be the object specified during calling\n+        assert mean_out is mean\n+\n+        std = np.nanstd(A,\n+                     out=std_out,\n+                     axis=1,\n+                     keepdims=True,\n+                     mean=mean)\n+\n+        # The returned  object should be the object specified during calling\n+        assert std_out is std\n+\n+        # Shape of returned mean and std should be same\n+        assert std.shape == mean.shape\n+        assert std.shape == (10, 1, 5)\n+\n+        # Output should be the same as from the individual algorithms\n+        std_old = np.nanstd(A, axis=1, keepdims=True)\n+\n+        assert std_old.shape == mean.shape\n+        assert_almost_equal(std, std_old)\n \n _TIME_UNITS = (\n     \"Y\", \"M\", \"W\", \"D\", \"h\", \"m\", \"s\", \"ms\", \"us\", \"ns\", \"ps\", \"fs\", \"as\"\n",
            "comment_added_diff": {
                "773": "        # Setting the seed to make the test reproducable",
                "786": "        # The returned  object should be the object specified during calling",
                "795": "        # The returned  object should be the object specified during calling",
                "798": "        # Shape of returned mean and std should be same",
                "802": "        # Output should be the same as from the individual algorithms"
            },
            "comment_deleted_diff": {},
            "comment_modified_diff": {}
        },
        {
            "commit": "a4c582d4b3c530176be393ac596dd00130367ddb",
            "timestamp": "2023-07-18T08:53:25+02:00",
            "author": "Dimitri Papadopoulos",
            "commit_message": "MAINT: Fix new or residual typos found by codespell",
            "additions": 1,
            "deletions": 1,
            "change_type": "MODIFY",
            "diff": "@@ -770,7 +770,7 @@ def test_where(self, dtype):\n             np.testing.assert_allclose(ret, reference)\n \n     def test_nanstd_with_mean_keyword(self):\n-        # Setting the seed to make the test reproducable\n+        # Setting the seed to make the test reproducible\n         rng = np.random.RandomState(1234)\n         A = rng.randn(10, 20, 5) + 0.5\n         A[:, 5, :] = np.nan\n",
            "comment_added_diff": {
                "773": "        # Setting the seed to make the test reproducible"
            },
            "comment_deleted_diff": {
                "773": "        # Setting the seed to make the test reproducable"
            },
            "comment_modified_diff": {
                "773": "        # Setting the seed to make the test reproducable"
            }
        }
    ],
    "extras.py": [
        {
            "commit": "13086bdf5e3cafb64c265f8de475b618a6a0252f",
            "timestamp": "2023-03-25T01:49:09+00:00",
            "author": "yuki",
            "commit_message": "MAINT: move `mask_rowcols` to `ma/extras.py`",
            "additions": 90,
            "deletions": 2,
            "change_type": "MODIFY",
            "diff": "@@ -27,8 +27,7 @@\n from .core import (\n     MaskedArray, MAError, add, array, asarray, concatenate, filled, count,\n     getmask, getmaskarray, make_mask_descr, masked, masked_array, mask_or,\n-    nomask, ones, sort, zeros, getdata, get_masked_subclass, dot,\n-    mask_rowcols\n+    nomask, ones, sort, zeros, getdata, get_masked_subclass, dot\n     )\n \n import numpy as np\n@@ -955,6 +954,95 @@ def compress_cols(a):\n     return compress_rowcols(a, 1)\n \n \n+def mask_rowcols(a, axis=None):\n+    \"\"\"\n+    Mask rows and/or columns of a 2D array that contain masked values.\n+\n+    Mask whole rows and/or columns of a 2D array that contain\n+    masked values.  The masking behavior is selected using the\n+    `axis` parameter.\n+\n+      - If `axis` is None, rows *and* columns are masked.\n+      - If `axis` is 0, only rows are masked.\n+      - If `axis` is 1 or -1, only columns are masked.\n+\n+    Parameters\n+    ----------\n+    a : array_like, MaskedArray\n+        The array to mask.  If not a MaskedArray instance (or if no array\n+        elements are masked), the result is a MaskedArray with `mask` set\n+        to `nomask` (False). Must be a 2D array.\n+    axis : int, optional\n+        Axis along which to perform the operation. If None, applies to a\n+        flattened version of the array.\n+\n+    Returns\n+    -------\n+    a : MaskedArray\n+        A modified version of the input array, masked depending on the value\n+        of the `axis` parameter.\n+\n+    Raises\n+    ------\n+    NotImplementedError\n+        If input array `a` is not 2D.\n+\n+    See Also\n+    --------\n+    mask_rows : Mask rows of a 2D array that contain masked values.\n+    mask_cols : Mask cols of a 2D array that contain masked values.\n+    masked_where : Mask where a condition is met.\n+\n+    Notes\n+    -----\n+    The input array's mask is modified by this function.\n+\n+    Examples\n+    --------\n+    >>> import numpy.ma as ma\n+    >>> a = np.zeros((3, 3), dtype=int)\n+    >>> a[1, 1] = 1\n+    >>> a\n+    array([[0, 0, 0],\n+           [0, 1, 0],\n+           [0, 0, 0]])\n+    >>> a = ma.masked_equal(a, 1)\n+    >>> a\n+    masked_array(\n+      data=[[0, 0, 0],\n+            [0, --, 0],\n+            [0, 0, 0]],\n+      mask=[[False, False, False],\n+            [False,  True, False],\n+            [False, False, False]],\n+      fill_value=1)\n+    >>> ma.mask_rowcols(a)\n+    masked_array(\n+      data=[[0, --, 0],\n+            [--, --, --],\n+            [0, --, 0]],\n+      mask=[[False,  True, False],\n+            [ True,  True,  True],\n+            [False,  True, False]],\n+      fill_value=1)\n+\n+    \"\"\"\n+    a = array(a, subok=False)\n+    if a.ndim != 2:\n+        raise NotImplementedError(\"mask_rowcols works for 2D arrays only.\")\n+    m = getmask(a)\n+    # Nothing is masked: return a\n+    if m is nomask or not m.any():\n+        return a\n+    maskedval = m.nonzero()\n+    a._mask = a._mask.copy()\n+    if not axis:\n+        a[np.unique(maskedval[0])] = masked\n+    if axis in [None, 1, -1]:\n+        a[:, np.unique(maskedval[1])] = masked\n+    return a\n+\n+\n def mask_rows(a, axis=np._NoValue):\n     \"\"\"\n     Mask rows of a 2D array that contain masked values.\n",
            "comment_added_diff": {
                "1034": "    # Nothing is masked: return a"
            },
            "comment_deleted_diff": {},
            "comment_modified_diff": {}
        }
    ],
    "multiarray.py": [
        {
            "commit": "11155a254fad85d7b13b7e9b4b8611be5aeaccb0",
            "timestamp": "2023-07-09T01:13:53-07:00",
            "author": "Mohit Kumar",
            "commit_message": "Added casting option's description to Glossary and casting option's link to numpy.concatenate",
            "additions": 3,
            "deletions": 28,
            "change_type": "MODIFY",
            "diff": "@@ -173,34 +173,9 @@ def concatenate(arrays, axis=None, out=None, *, dtype=None, casting=None):\n \n     casting : {'no', 'equiv', 'safe', 'same_kind', 'unsafe'}, optional\n         Controls what kind of data casting may occur. Defaults to 'same_kind'.\n-    \n-    \n-    - `'no'`: No casting is allowed. The data types should not be cast at all.\n-    Any mismatch in data types between the arrays being concatenated will raise a `TypeError`.\n-\n-    - `'equiv'`: Only byte-order changes are allowed. Casting between different \n-    byte orders is permitted. However, other types of casting, \n-    such as changing the data type or size, are not allowed.\n-\n-    - `'safe'`:  Only casts that can preserve values are allowed. \n-    Upcasting (e.g., from int to float) is allowed, but downcasting is not. \n-    It ensures that the values are not lost or corrupted during casting.\n-\n-    - `'same_kind'`: The `'same_kind'` casting option allows safe casts and casts within a kind. \n-    It means that it allows casting between data types that are of the same kind or category. \n-    For example, casting from float64 to float32 is considered safe because both are floating-point data types,\n-    and the casting only involves reducing the precision. Casting between different kinds, \n-    such as from float to int, is not allowed.\n-\n-    - `'unsafe'`: The `'unsafe'` casting option allows any data conversions to be performed, \n-    even if it may result in loss of precision or data corruption. It allows casting between different data types,\n-    sizes, and kinds without any restrictions. This casting option should be used with caution, \n-    as it can lead to unexpected results or data inconsistencies.\n-\n-    By specifying the appropriate casting option, you can control the behavior of casting \n-    when performing array concatenation, ensuring that it aligns with your desired requirements for \n-    preserving data integrity and precision.\n-\n+        For more information, see the `documentation <https://numpy.org/devdocs/glossary.html#glossary>`.\n+        .. versionadded:: 1.20.0\n+        \n     Returns\n     -------\n     res : ndarray\n",
            "comment_added_diff": {
                "176": "        For more information, see the `documentation <https://numpy.org/devdocs/glossary.html#glossary>`."
            },
            "comment_deleted_diff": {},
            "comment_modified_diff": {
                "176": ""
            }
        },
        {
            "commit": "090ac24d01635450fcf280850a24968cc5c442d5",
            "timestamp": "2023-07-09T01:13:53-07:00",
            "author": "Mohit Kumar",
            "commit_message": "Added casting option's description to Glossary and casting option's link to numpy.concatenate",
            "additions": 2,
            "deletions": 1,
            "change_type": "MODIFY",
            "diff": "@@ -173,7 +173,8 @@ def concatenate(arrays, axis=None, out=None, *, dtype=None, casting=None):\n \n     casting : {'no', 'equiv', 'safe', 'same_kind', 'unsafe'}, optional\n         Controls what kind of data casting may occur. Defaults to 'same_kind'.\n-        For more information, see the `documentation <https://numpy.org/devdocs/glossary.html#glossary>`.\n+        For more information:\n+        see the `documentation <https://numpy.org/devdocs/glossary.html#glossary>`.\n         .. versionadded:: 1.20.0\n         \n     Returns\n",
            "comment_added_diff": {
                "177": "        see the `documentation <https://numpy.org/devdocs/glossary.html#glossary>`."
            },
            "comment_deleted_diff": {
                "176": "        For more information, see the `documentation <https://numpy.org/devdocs/glossary.html#glossary>`."
            },
            "comment_modified_diff": {}
        },
        {
            "commit": "f7872d85d76357d7ae31c7f5db8964d1ab03ff5c",
            "timestamp": "2023-07-09T01:13:54-07:00",
            "author": "Mohit Kumar",
            "commit_message": "Update multiarray.py",
            "additions": 3,
            "deletions": 2,
            "change_type": "MODIFY",
            "diff": "@@ -173,8 +173,9 @@ def concatenate(arrays, axis=None, out=None, *, dtype=None, casting=None):\n \n     casting : {'no', 'equiv', 'safe', 'same_kind', 'unsafe'}, optional\n         Controls what kind of data casting may occur. Defaults to 'same_kind'.\n-        For more information:\n-        see the `documentation <https://numpy.org/devdocs/glossary.html#glossary>`.\n+        For more information\n+        `<https://numpy.org/devdocs/glossary.html#glossary>`.\n+        \n         .. versionadded:: 1.20.0\n \n     Returns\n",
            "comment_added_diff": {
                "177": "        `<https://numpy.org/devdocs/glossary.html#glossary>`."
            },
            "comment_deleted_diff": {
                "177": "        see the `documentation <https://numpy.org/devdocs/glossary.html#glossary>`."
            },
            "comment_modified_diff": {
                "177": "        see the `documentation <https://numpy.org/devdocs/glossary.html#glossary>`."
            }
        },
        {
            "commit": "19b7c06d8ebe24fa4bab50e7e06c941fa41387c0",
            "timestamp": "2023-07-09T01:13:54-07:00",
            "author": "Mohit Kumar",
            "commit_message": "Update numpy/core/multiarray.py\n\nCo-authored-by: Mukulika <60316606+Mukulikaa@users.noreply.github.com>",
            "additions": 0,
            "deletions": 1,
            "change_type": "MODIFY",
            "diff": "@@ -174,7 +174,6 @@ def concatenate(arrays, axis=None, out=None, *, dtype=None, casting=None):\n     casting : {'no', 'equiv', 'safe', 'same_kind', 'unsafe'}, optional\n         Controls what kind of data casting may occur. Defaults to 'same_kind'.\n         For a description of the options, please see :term:`casting`.\n-        `<https://numpy.org/devdocs/glossary.html#glossary>`.\n         \n         .. versionadded:: 1.20.0\n \n",
            "comment_added_diff": {},
            "comment_deleted_diff": {
                "177": "        `<https://numpy.org/devdocs/glossary.html#glossary>`."
            },
            "comment_modified_diff": {}
        }
    ],
    "math.h": [],
    "test_simd.py": [
        {
            "commit": "39fe90da02831699c09fda62865ad8b92021d487",
            "timestamp": "2022-12-07T01:50:38+02:00",
            "author": "Sayed Adel",
            "commit_message": "BUG, SIMD: Fix rounding large numbers >= 2^52 on SSE2\n\n  Before SSE41, there were no native instructions for rounding\n  operations on double precision. We usually emulate it by assuming\n  that the `MXCR` register is set to rounding, adding a\n  large number `2^52` to `X` and then subtracting it back to\n  eliminate any excess precision as long as `|X|` is less than `2^52`\n  otherwise returns `X.`\n\n  The current emulated intrinics `npyv_[rint,floor, ceil, trunc]_f64`\n  was not checking whether `|x|` equal or large `2^52` which leads\n  to losing accuracy on large numbers.",
            "additions": 10,
            "deletions": 0,
            "change_type": "MODIFY",
            "diff": "@@ -437,6 +437,16 @@ def test_rounding(self, intrin, func):\n                 _round = intrin(data)\n                 assert _round == data_round\n \n+        # test large numbers\n+        for i in (\n+            1.1529215045988576e+18, 4.6116860183954304e+18,\n+            5.902958103546122e+20, 2.3611832414184488e+21\n+        ):\n+            x = self.setall(i)\n+            y = intrin(x)\n+            data_round = [func(n) for n in x]\n+            assert y == data_round\n+\n         # signed zero\n         if intrin_name == \"floor\":\n             data_szero = (-0.0,)\n",
            "comment_added_diff": {
                "440": "        # test large numbers"
            },
            "comment_deleted_diff": {},
            "comment_modified_diff": {}
        },
        {
            "commit": "a437cc15a38f445b833b0b1be4f91b23c474064e",
            "timestamp": "2022-12-14T12:03:22+02:00",
            "author": "Sayed Adel",
            "commit_message": "ENH, SIMD: Add ordered comparison intrinsics guarantees non-signaling",
            "additions": 55,
            "deletions": 28,
            "change_type": "MODIFY",
            "diff": "@@ -2,9 +2,24 @@\n # may be involved in their functionality.\n import pytest, math, re\n import itertools\n-from numpy.core._simd import targets\n+import operator\n+from numpy.core._simd import targets, clear_floatstatus, get_floatstatus\n from numpy.core._multiarray_umath import __cpu_baseline__\n \n+def check_floatstatus(divbyzero=False, overflow=False,\n+                      underflow=False, invalid=False,\n+                      all=False):\n+    #define NPY_FPE_DIVIDEBYZERO  1\n+    #define NPY_FPE_OVERFLOW      2\n+    #define NPY_FPE_UNDERFLOW     4\n+    #define NPY_FPE_INVALID       8\n+    err = get_floatstatus()\n+    ret = (all or divbyzero) and (err & 1) != 0\n+    ret |= (all or overflow) and (err & 2) != 0\n+    ret |= (all or underflow) and (err & 4) != 0\n+    ret |= (all or invalid) and (err & 8) != 0\n+    return ret\n+\n class _Test_Utility:\n     # submodule of the desired SIMD extension, e.g. targets[\"AVX512F\"]\n     npyv = None\n@@ -553,13 +568,29 @@ def test_special_cases(self):\n         nnan = self.notnan(self.setall(self._nan()))\n         assert nnan == [0]*self.nlanes\n \n-    import operator\n+\n+    @pytest.mark.parametrize(\"intrin_name\", [\n+        \"cmpltq\", \"cmpleq\", \"cmpgtq\", \"cmpgeq\"\n+    ])\n+    def test_binary_invalid_fpexception(self, intrin_name):\n+        intrin = getattr(self, intrin_name)\n+        for d in [float(\"nan\"), float(\"inf\"), -float(\"inf\")]:\n+            a = self.setall(d)\n+            b = self.setall(1.0)\n+            clear_floatstatus()\n+            intrin(a, b)\n+            intrin(b, a)\n+            assert check_floatstatus(invalid=True) == False\n \n     @pytest.mark.parametrize('py_comp,np_comp', [\n         (operator.lt, \"cmplt\"),\n         (operator.le, \"cmple\"),\n         (operator.gt, \"cmpgt\"),\n         (operator.ge, \"cmpge\"),\n+        (operator.lt, \"cmpltq\"),\n+        (operator.le, \"cmpleq\"),\n+        (operator.gt, \"cmpgtq\"),\n+        (operator.ge, \"cmpgeq\"),\n         (operator.eq, \"cmpeq\"),\n         (operator.ne, \"cmpneq\")\n     ])\n@@ -571,7 +602,8 @@ def to_bool(vector):\n             return [lane == mask_true for lane in vector]\n \n         intrin = getattr(self, np_comp)\n-        cmp_cases = ((0, nan), (nan, 0), (nan, nan), (pinf, nan), (ninf, nan))\n+        cmp_cases = ((0, nan), (nan, 0), (nan, nan), (pinf, nan),\n+                     (ninf, nan), (-0.0, +0.0))\n         for case_operand1, case_operand2 in cmp_cases:\n             data_a = [case_operand1]*self.nlanes\n             data_b = [case_operand2]*self.nlanes\n@@ -881,41 +913,36 @@ def test_reorder_rev64(self):\n         rev64 = self.rev64(self.load(range(self.nlanes)))\n         assert rev64 == data_rev64\n \n-    def test_operators_comparison(self):\n+    @pytest.mark.parametrize('func, intrin, sup_sfx', [\n+        (operator.lt, \"cmplt\", []),\n+        (operator.le, \"cmple\", []),\n+        (operator.gt, \"cmpgt\", []),\n+        (operator.ge, \"cmpge\", []),\n+        (operator.eq, \"cmpeq\", []),\n+        (operator.ne, \"cmpneq\", (\"f32\", \"f64\")),\n+        (operator.lt, \"cmpltq\", (\"f32\", \"f64\")),\n+        (operator.le, \"cmpleq\", (\"f32\", \"f64\")),\n+        (operator.gt, \"cmpgtq\", (\"f32\", \"f64\")),\n+        (operator.ge, \"cmpgeq\", (\"f32\", \"f64\"))\n+    ])\n+    def test_operators_comparison(self, func, intrin, sup_sfx):\n+        if sup_sfx and self.sfx not in sup_sfx:\n+            return\n         if self._is_fp():\n             data_a = self._data()\n         else:\n             data_a = self._data(self._int_max() - self.nlanes)\n         data_b = self._data(self._int_min(), reverse=True)\n         vdata_a, vdata_b = self.load(data_a), self.load(data_b)\n+        intrin = getattr(self, intrin)\n \n         mask_true = self._true_mask()\n         def to_bool(vector):\n             return [lane == mask_true for lane in vector]\n-        # equal\n-        data_eq = [a == b for a, b in zip(data_a, data_b)]\n-        cmpeq = to_bool(self.cmpeq(vdata_a, vdata_b))\n-        assert cmpeq == data_eq\n-        # not equal\n-        data_neq = [a != b for a, b in zip(data_a, data_b)]\n-        cmpneq = to_bool(self.cmpneq(vdata_a, vdata_b))\n-        assert cmpneq == data_neq\n-        # greater than\n-        data_gt = [a > b for a, b in zip(data_a, data_b)]\n-        cmpgt = to_bool(self.cmpgt(vdata_a, vdata_b))\n-        assert cmpgt == data_gt\n-        # greater than and equal\n-        data_ge = [a >= b for a, b in zip(data_a, data_b)]\n-        cmpge = to_bool(self.cmpge(vdata_a, vdata_b))\n-        assert cmpge == data_ge\n-        # less than\n-        data_lt  = [a < b for a, b in zip(data_a, data_b)]\n-        cmplt = to_bool(self.cmplt(vdata_a, vdata_b))\n-        assert cmplt == data_lt\n-        # less than and equal\n-        data_le  = [a <= b for a, b in zip(data_a, data_b)]\n-        cmple = to_bool(self.cmple(vdata_a, vdata_b))\n-        assert cmple == data_le\n+\n+        data_cmp = [func(a, b) for a, b in zip(data_a, data_b)]\n+        cmp = to_bool(intrin(vdata_a, vdata_b))\n+        assert cmp == data_cmp\n \n     def test_operators_logical(self):\n         if self._is_fp():\n",
            "comment_added_diff": {
                "12": "    #define NPY_FPE_DIVIDEBYZERO  1",
                "13": "    #define NPY_FPE_OVERFLOW      2",
                "14": "    #define NPY_FPE_UNDERFLOW     4",
                "15": "    #define NPY_FPE_INVALID       8"
            },
            "comment_deleted_diff": {
                "895": "        # equal",
                "899": "        # not equal",
                "903": "        # greater than",
                "907": "        # greater than and equal",
                "911": "        # less than",
                "915": "        # less than and equal"
            },
            "comment_modified_diff": {}
        },
        {
            "commit": "de95f3cfbafb08674b6dfd13f780703ebd48bf10",
            "timestamp": "2023-01-29T13:02:39+02:00",
            "author": "Sayed Adel",
            "commit_message": "ENH: Implement intrinsics for shuffle over 128-bit lane and unzip\n\n   shuffle intrinsics support 32-bit/64-bit vector data types,\n   unzip(deinterleave) intrinsics supports all data types.",
            "additions": 20,
            "deletions": 0,
            "change_type": "MODIFY",
            "diff": "@@ -906,6 +906,26 @@ def test_reorder_rev64(self):\n         rev64 = self.rev64(self.load(range(self.nlanes)))\n         assert rev64 == data_rev64\n \n+    def test_reorder_permi128(self):\n+        \"\"\"\n+        Test permuting elements for each 128-bit lane.\n+        npyv_permi128_##sfx\n+        \"\"\"\n+        ssize = self._scalar_size()\n+        if ssize < 32:\n+            return\n+        data = self.load(self._data())\n+        permn = 128//ssize\n+        permd = permn-1\n+        nlane128 = self.nlanes//permn\n+\n+        shfl = [0, 1] if ssize == 64 else [0, 2, 4, 6]\n+        for i in range(permd):\n+            indices = [(i >> shf) & permd for shf in shfl]\n+            vperm = self.permi128(data, *indices)\n+            data_vperm = [data[j] for j in indices]\n+            assert vperm = data_vperm\n+\n     @pytest.mark.parametrize('func, intrin', [\n         (operator.lt, \"cmplt\"),\n         (operator.le, \"cmple\"),\n",
            "comment_added_diff": {
                "912": "        npyv_permi128_##sfx"
            },
            "comment_deleted_diff": {},
            "comment_modified_diff": {}
        },
        {
            "commit": "32af803704bff2cab984ded25d604b1caf703a94",
            "timestamp": "2023-01-29T13:02:39+02:00",
            "author": "Sayed Adel",
            "commit_message": "TST, SIMD: add test cases for the new intrinics",
            "additions": 186,
            "deletions": 87,
            "change_type": "MODIFY",
            "diff": "@@ -35,6 +35,9 @@ def __getattr__(self, attr):\n         \"\"\"\n         return getattr(self.npyv, attr + \"_\" + self.sfx)\n \n+    def _x2(self, intrin_name):\n+        return getattr(self.npyv, f\"{intrin_name}_{self.sfx}x2\")\n+\n     def _data(self, start=None, count=None, reverse=False):\n         \"\"\"\n         Create list of consecutive numbers according to number of vector's lanes.\n@@ -380,6 +383,11 @@ def test_arithmetic_fused(self):\n         nfms = self.nmulsub(vdata_a, vdata_b, vdata_c)\n         data_nfms = self.mul(data_fma, self.setall(-1))\n         assert nfms == data_nfms\n+        # multiply, add for odd elements and subtract even elements.\n+        # (a * b) -+ c\n+        fmas = list(self.muladdsub(vdata_a, vdata_b, vdata_c))\n+        assert fmas[0::2] == list(data_fms)[0::2]\n+        assert fmas[1::2] == list(data_fma)[1::2]\n \n     def test_abs(self):\n         pinf, ninf, nan = self._pinfinity(), self._ninfinity(), self._nan()\n@@ -678,25 +686,34 @@ def test_memory_store(self):\n         assert store_h[:self.nlanes//2] == data[self.nlanes//2:]\n         assert store_h != vdata  # detect overflow\n \n-    def test_memory_partial_load(self):\n-        if self.sfx in (\"u8\", \"s8\", \"u16\", \"s16\"):\n+    @pytest.mark.parametrize(\"intrin, elsizes, scale, fill\", [\n+        (\"self.load_tillz, self.load_till\", (32, 64), 1, [0xffff]),\n+        (\"self.load2_tillz, self.load2_till\", (32, 64), 2, [0xffff, 0x7fff]),\n+    ])\n+    def test_memory_partial_load(self, intrin, elsizes, scale, fill):\n+        if self._scalar_size() not in elsizes:\n             return\n-\n+        npyv_load_tillz, npyv_load_till = eval(intrin)\n         data = self._data()\n         lanes = list(range(1, self.nlanes + 1))\n         lanes += [self.nlanes**2, self.nlanes**4] # test out of range\n         for n in lanes:\n-            load_till  = self.load_till(data, n, 15)\n-            data_till  = data[:n] + [15] * (self.nlanes-n)\n+            load_till = npyv_load_till(data, n, *fill)\n+            load_tillz = npyv_load_tillz(data, n)\n+            n *= scale\n+            data_till = data[:n] + fill * ((self.nlanes-n) // scale)\n             assert load_till == data_till\n-            load_tillz = self.load_tillz(data, n)\n             data_tillz = data[:n] + [0] * (self.nlanes-n)\n             assert load_tillz == data_tillz\n \n-    def test_memory_partial_store(self):\n-        if self.sfx in (\"u8\", \"s8\", \"u16\", \"s16\"):\n+    @pytest.mark.parametrize(\"intrin, elsizes, scale\", [\n+        (\"self.store_till\", (32, 64), 1),\n+        (\"self.store2_till\", (32, 64), 2),\n+    ])\n+    def test_memory_partial_store(self, intrin, elsizes, scale):\n+        if self._scalar_size() not in elsizes:\n             return\n-\n+        npyv_store_till = eval(intrin)\n         data = self._data()\n         data_rev = self._data(reverse=True)\n         vdata = self.load(data)\n@@ -704,105 +721,159 @@ def test_memory_partial_store(self):\n         lanes += [self.nlanes**2, self.nlanes**4]\n         for n in lanes:\n             data_till = data_rev.copy()\n-            data_till[:n] = data[:n]\n+            data_till[:n*scale] = data[:n*scale]\n             store_till = self._data(reverse=True)\n-            self.store_till(store_till, n, vdata)\n+            npyv_store_till(store_till, n, vdata)\n             assert store_till == data_till\n \n-    def test_memory_noncont_load(self):\n-        if self.sfx in (\"u8\", \"s8\", \"u16\", \"s16\"):\n+    @pytest.mark.parametrize(\"intrin, elsizes, scale\", [\n+        (\"self.loadn\", (32, 64), 1),\n+        (\"self.loadn2\", (32, 64), 2),\n+    ])\n+    def test_memory_noncont_load(self, intrin, elsizes, scale):\n+        if self._scalar_size() not in elsizes:\n             return\n-\n-        for stride in range(1, 64):\n-            data = self._data(count=stride*self.nlanes)\n-            data_stride = data[::stride]\n-            loadn = self.loadn(data, stride)\n-            assert loadn == data_stride\n-\n-        for stride in range(-64, 0):\n-            data = self._data(stride, -stride*self.nlanes)\n-            data_stride = self.load(data[::stride]) # cast unsigned\n-            loadn = self.loadn(data, stride)\n+        npyv_loadn = eval(intrin)\n+        for stride in range(-64, 64):\n+            if stride < 0:\n+                data = self._data(stride, -stride*self.nlanes)\n+                data_stride = list(itertools.chain(\n+                    *zip(*[data[-i::stride] for i in range(scale, 0, -1)])\n+                ))\n+            elif stride == 0:\n+                data = self._data()\n+                data_stride = data[0:scale] * (self.nlanes//scale)\n+            else:\n+                data = self._data(count=stride*self.nlanes)\n+                data_stride = list(itertools.chain(\n+                    *zip(*[data[i::stride] for i in range(scale)]))\n+                )\n+            data_stride = self.load(data_stride)  # cast unsigned\n+            loadn = npyv_loadn(data, stride)\n             assert loadn == data_stride\n \n-    def test_memory_noncont_partial_load(self):\n-        if self.sfx in (\"u8\", \"s8\", \"u16\", \"s16\"):\n+    @pytest.mark.parametrize(\"intrin, elsizes, scale, fill\", [\n+        (\"self.loadn_tillz, self.loadn_till\", (32, 64), 1, [0xffff]),\n+        (\"self.loadn2_tillz, self.loadn2_till\", (32, 64), 2, [0xffff, 0x7fff]),\n+    ])\n+    def test_memory_noncont_partial_load(self, intrin, elsizes, scale, fill):\n+        if self._scalar_size() not in elsizes:\n             return\n-\n+        npyv_loadn_tillz, npyv_loadn_till = eval(intrin)\n         lanes = list(range(1, self.nlanes + 1))\n         lanes += [self.nlanes**2, self.nlanes**4]\n-        for stride in range(1, 64):\n-            data = self._data(count=stride*self.nlanes)\n-            data_stride = data[::stride]\n-            for n in lanes:\n-                data_stride_till = data_stride[:n] + [15] * (self.nlanes-n)\n-                loadn_till = self.loadn_till(data, stride, n, 15)\n-                assert loadn_till == data_stride_till\n-                data_stride_tillz = data_stride[:n] + [0] * (self.nlanes-n)\n-                loadn_tillz = self.loadn_tillz(data, stride, n)\n-                assert loadn_tillz == data_stride_tillz\n-\n-        for stride in range(-64, 0):\n-            data = self._data(stride, -stride*self.nlanes)\n-            data_stride = list(self.load(data[::stride])) # cast unsigned\n+        for stride in range(-64, 64):\n+            if stride < 0:\n+                data = self._data(stride, -stride*self.nlanes)\n+                data_stride = list(itertools.chain(\n+                    *zip(*[data[-i::stride] for i in range(scale, 0, -1)])\n+                ))\n+            elif stride == 0:\n+                data = self._data()\n+                data_stride = data[0:scale] * (self.nlanes//scale)\n+            else:\n+                data = self._data(count=stride*self.nlanes)\n+                data_stride = list(itertools.chain(\n+                    *zip(*[data[i::stride] for i in range(scale)])\n+                ))\n+            data_stride = list(self.load(data_stride))  # cast unsigned\n             for n in lanes:\n-                data_stride_till = data_stride[:n] + [15] * (self.nlanes-n)\n-                loadn_till = self.loadn_till(data, stride, n, 15)\n+                nscale = n * scale\n+                llanes = self.nlanes - nscale\n+                data_stride_till = (\n+                    data_stride[:nscale] + fill * (llanes//scale)\n+                )\n+                loadn_till = npyv_loadn_till(data, stride, n, *fill)\n                 assert loadn_till == data_stride_till\n-                data_stride_tillz = data_stride[:n] + [0] * (self.nlanes-n)\n-                loadn_tillz = self.loadn_tillz(data, stride, n)\n+                data_stride_tillz = data_stride[:nscale] + [0] * llanes\n+                loadn_tillz = npyv_loadn_tillz(data, stride, n)\n                 assert loadn_tillz == data_stride_tillz\n \n-    def test_memory_noncont_store(self):\n-        if self.sfx in (\"u8\", \"s8\", \"u16\", \"s16\"):\n+    @pytest.mark.parametrize(\"intrin, elsizes, scale\", [\n+        (\"self.storen\", (32, 64), 1),\n+        (\"self.storen2\", (32, 64), 2),\n+    ])\n+    def test_memory_noncont_store(self, intrin, elsizes, scale):\n+        if self._scalar_size() not in elsizes:\n             return\n-\n-        vdata = self.load(self._data())\n+        npyv_storen = eval(intrin)\n+        data = self._data()\n+        vdata = self.load(data)\n+        hlanes = self.nlanes // scale\n         for stride in range(1, 64):\n-            data = [15] * stride * self.nlanes\n-            data[::stride] = vdata\n-            storen = [15] * stride * self.nlanes\n-            storen += [127]*64\n-            self.storen(storen, stride, vdata)\n-            assert storen[:-64] == data\n-            assert storen[-64:] == [127]*64 # detect overflow\n+            data_storen = [0xff] * stride * self.nlanes\n+            for s in range(0, hlanes*stride, stride):\n+                i = (s//stride)*scale\n+                data_storen[s:s+scale] = data[i:i+scale]\n+            storen = [0xff] * stride * self.nlanes\n+            storen += [0x7f]*64\n+            npyv_storen(storen, stride, vdata)\n+            assert storen[:-64] == data_storen\n+            assert storen[-64:] == [0x7f]*64  # detect overflow\n \n         for stride in range(-64, 0):\n-            data = [15] * -stride * self.nlanes\n-            data[::stride] = vdata\n-            storen = [127]*64\n-            storen += [15] * -stride * self.nlanes\n-            self.storen(storen, stride, vdata)\n-            assert storen[64:] == data\n-            assert storen[:64] == [127]*64 # detect overflow\n-\n-    def test_memory_noncont_partial_store(self):\n-        if self.sfx in (\"u8\", \"s8\", \"u16\", \"s16\"):\n+            data_storen = [0xff] * -stride * self.nlanes\n+            for s in range(0, hlanes*stride, stride):\n+                i = (s//stride)*scale\n+                data_storen[s-scale:s or None] = data[i:i+scale]\n+            storen = [0x7f]*64\n+            storen += [0xff] * -stride * self.nlanes\n+            npyv_storen(storen, stride, vdata)\n+            assert storen[64:] == data_storen\n+            assert storen[:64] == [0x7f]*64  # detect overflow\n+        # stride 0\n+        data_storen = [0x7f] * self.nlanes\n+        storen = data_storen.copy()\n+        data_storen[0:scale] = data[-scale:]\n+        npyv_storen(storen, 0, vdata)\n+        assert storen == data_storen\n+\n+    @pytest.mark.parametrize(\"intrin, elsizes, scale\", [\n+        (\"self.storen_till\", (32, 64), 1),\n+        (\"self.storen2_till\", (32, 64), 2),\n+    ])\n+    def test_memory_noncont_partial_store(self, intrin, elsizes, scale):\n+        if self._scalar_size() not in elsizes:\n             return\n-\n+        npyv_storen_till = eval(intrin)\n         data = self._data()\n         vdata = self.load(data)\n         lanes = list(range(1, self.nlanes + 1))\n         lanes += [self.nlanes**2, self.nlanes**4]\n+        hlanes = self.nlanes // scale\n         for stride in range(1, 64):\n             for n in lanes:\n-                data_till = [15] * stride * self.nlanes\n-                data_till[::stride] = data[:n] + [15] * (self.nlanes-n)\n-                storen_till = [15] * stride * self.nlanes\n-                storen_till += [127]*64\n-                self.storen_till(storen_till, stride, n, vdata)\n+                data_till = [0xff] * stride * self.nlanes\n+                tdata = data[:n*scale] + [0xff] * (self.nlanes-n*scale)\n+                for s in range(0, hlanes*stride, stride)[:n]:\n+                    i = (s//stride)*scale\n+                    data_till[s:s+scale] = tdata[i:i+scale]\n+                storen_till = [0xff] * stride * self.nlanes\n+                storen_till += [0x7f]*64\n+                npyv_storen_till(storen_till, stride, n, vdata)\n                 assert storen_till[:-64] == data_till\n-                assert storen_till[-64:] == [127]*64 # detect overflow\n+                assert storen_till[-64:] == [0x7f]*64  # detect overflow\n \n         for stride in range(-64, 0):\n             for n in lanes:\n-                data_till = [15] * -stride * self.nlanes\n-                data_till[::stride] = data[:n] + [15] * (self.nlanes-n)\n-                storen_till = [127]*64\n-                storen_till += [15] * -stride * self.nlanes\n-                self.storen_till(storen_till, stride, n, vdata)\n+                data_till = [0xff] * -stride * self.nlanes\n+                tdata = data[:n*scale] + [0xff] * (self.nlanes-n*scale)\n+                for s in range(0, hlanes*stride, stride)[:n]:\n+                    i = (s//stride)*scale\n+                    data_till[s-scale:s or None] = tdata[i:i+scale]\n+                storen_till = [0x7f]*64\n+                storen_till += [0xff] * -stride * self.nlanes\n+                npyv_storen_till(storen_till, stride, n, vdata)\n                 assert storen_till[64:] == data_till\n-                assert storen_till[:64] == [127]*64 # detect overflow\n+                assert storen_till[:64] == [0x7f]*64  # detect overflow\n+\n+        # stride 0\n+        for n in lanes:\n+            data_till = [0x7f] * self.nlanes\n+            storen_till = data_till.copy()\n+            data_till[0:scale] = data[:n*scale][-scale:]\n+            npyv_storen_till(storen_till, 0, n, vdata)\n+            assert storen_till == data_till\n \n     @pytest.mark.parametrize(\"intrin, table_size, elsize\", [\n         (\"self.lut32\", 32, 32),\n@@ -886,13 +957,27 @@ def test_reorder(self):\n         combineh = self.combineh(vdata_a, vdata_b)\n         assert combineh == data_a_hi + data_b_hi\n         # combine x2\n-        combine  = self.combine(vdata_a, vdata_b)\n+        combine = self.combine(vdata_a, vdata_b)\n         assert combine == (data_a_lo + data_b_lo, data_a_hi + data_b_hi)\n+\n         # zip(interleave)\n-        data_zipl = [v for p in zip(data_a_lo, data_b_lo) for v in p]\n-        data_ziph = [v for p in zip(data_a_hi, data_b_hi) for v in p]\n-        vzip  = self.zip(vdata_a, vdata_b)\n+        data_zipl = self.load([\n+            v for p in zip(data_a_lo, data_b_lo) for v in p\n+        ])\n+        data_ziph = self.load([\n+            v for p in zip(data_a_hi, data_b_hi) for v in p\n+        ])\n+        vzip = self.zip(vdata_a, vdata_b)\n         assert vzip == (data_zipl, data_ziph)\n+        vzip = [0]*self.nlanes*2\n+        self._x2(\"store\")(vzip, (vdata_a, vdata_b))\n+        assert vzip == list(data_zipl) + list(data_ziph)\n+\n+        # unzip(deinterleave)\n+        unzip = self.unzip(data_zipl, data_ziph)\n+        assert unzip == (data_a, data_b)\n+        unzip = self._x2(\"load\")(list(data_zipl) + list(data_ziph))\n+        assert unzip == (data_a, data_b)\n \n     def test_reorder_rev64(self):\n         # Reverse elements of each 64-bit lane\n@@ -918,13 +1003,15 @@ def test_reorder_permi128(self):\n         permn = 128//ssize\n         permd = permn-1\n         nlane128 = self.nlanes//permn\n-\n         shfl = [0, 1] if ssize == 64 else [0, 2, 4, 6]\n-        for i in range(permd):\n+        for i in range(permn):\n             indices = [(i >> shf) & permd for shf in shfl]\n             vperm = self.permi128(data, *indices)\n-            data_vperm = [data[j] for j in indices]\n-            assert vperm = data_vperm\n+            data_vperm = [\n+                data[j + (e & -permn)]\n+                for e, j in enumerate(indices*nlane128)\n+            ]\n+            assert vperm == data_vperm\n \n     @pytest.mark.parametrize('func, intrin', [\n         (operator.lt, \"cmplt\"),\n@@ -1188,6 +1275,18 @@ def test_mask_conditional(self):\n         ifadd = self.ifadd(false_mask, vdata_a, vdata_b, vdata_b)\n         assert ifadd == vdata_b\n \n+        if not self._is_fp():\n+            return\n+        data_div = self.div(vdata_b, vdata_a)\n+        ifdiv = self.ifdiv(true_mask, vdata_b, vdata_a, vdata_b)\n+        assert ifdiv == data_div\n+        ifdivz = self.ifdivz(true_mask, vdata_b, vdata_a)\n+        assert ifdivz == data_div\n+        ifdiv = self.ifdiv(false_mask, vdata_a, vdata_b, vdata_b)\n+        assert ifdiv == vdata_b\n+        ifdivz = self.ifdivz(false_mask, vdata_a, vdata_b)\n+        assert ifdivz == self.zero()\n+\n bool_sfx = (\"b8\", \"b16\", \"b32\", \"b64\")\n int_sfx = (\"u8\", \"s8\", \"u16\", \"s16\", \"u32\", \"s32\", \"u64\", \"s64\")\n fp_sfx  = (\"f32\", \"f64\")\n",
            "comment_added_diff": {
                "386": "        # multiply, add for odd elements and subtract even elements.",
                "387": "        # (a * b) -+ c",
                "751": "            data_stride = self.load(data_stride)  # cast unsigned",
                "779": "            data_stride = list(self.load(data_stride))  # cast unsigned",
                "812": "            assert storen[-64:] == [0x7f]*64  # detect overflow",
                "823": "            assert storen[:64] == [0x7f]*64  # detect overflow",
                "824": "        # stride 0",
                "855": "                assert storen_till[-64:] == [0x7f]*64  # detect overflow",
                "868": "                assert storen_till[:64] == [0x7f]*64  # detect overflow",
                "870": "        # stride 0",
                "976": "        # unzip(deinterleave)"
            },
            "comment_deleted_diff": {
                "724": "            data_stride = self.load(data[::stride]) # cast unsigned",
                "747": "            data_stride = list(self.load(data[::stride])) # cast unsigned",
                "768": "            assert storen[-64:] == [127]*64 # detect overflow",
                "777": "            assert storen[:64] == [127]*64 # detect overflow",
                "795": "                assert storen_till[-64:] == [127]*64 # detect overflow",
                "805": "                assert storen_till[:64] == [127]*64 # detect overflow"
            },
            "comment_modified_diff": {
                "779": "    def test_memory_noncont_partial_store(self):"
            }
        }
    ],
    "array_api.rst": [],
    "_manipulation_functions.py": [
        {
            "commit": "a6740500576475a43a7121087e9f5f96d48ef1d2",
            "timestamp": "2022-12-06T17:48:32-07:00",
            "author": "Aaron Meurer",
            "commit_message": "DOC: Some updates to the array_api compat document (#22747)\n\n* Add reshape differences to the array API compat document\r\n\r\n* Add an item to the array API compat document about reverse broadcasting\r\n\r\n* Make some wording easier to read",
            "additions": 1,
            "deletions": 0,
            "change_type": "MODIFY",
            "diff": "@@ -52,6 +52,7 @@ def permute_dims(x: Array, /, axes: Tuple[int, ...]) -> Array:\n     return Array._new(np.transpose(x._array, axes))\n \n \n+# Note: the optional argument is called 'shape', not 'newshape'\n def reshape(x: Array, /, shape: Tuple[int, ...]) -> Array:\n     \"\"\"\n     Array API compatible wrapper for :py:func:`np.reshape <numpy.reshape>`.\n",
            "comment_added_diff": {
                "55": "# Note: the optional argument is called 'shape', not 'newshape'"
            },
            "comment_deleted_diff": {},
            "comment_modified_diff": {}
        }
    ],
    "loops_logical.dispatch.c.src": [],
    "22575.compatibility.rst": [],
    "_simd.c": [],
    "_simd.dispatch.c.src": [],
    "operators.h": [],
    "avx2.h": [],
    "avx512.h": [],
    "neon.h": [],
    "simd.h": [],
    "sse.h": [],
    "vec.h": [],
    "loops_trigonometric.dispatch.c.src": [],
    "tokenize.cpp": [],
    "tokenize.h": [],
    "test_loadtxt.py": [
        {
            "commit": "5e0ed03b6b7a6ff05843d846eedac730f20decb7",
            "timestamp": "2022-12-19T10:40:20-08:00",
            "author": "Sebastian Berg",
            "commit_message": "BUG: Ensure correct behavior for rows ending in delimiter in loadtxt (#22836)\n\nIf a row ends in a delimiter, `add_fields` can be called twice without\r\nany field actually being parsed.  This causes issues with the field\r\nbuffer setup.\r\n\r\ncloses gh-22833",
            "additions": 12,
            "deletions": 0,
            "change_type": "MODIFY",
            "diff": "@@ -1011,3 +1011,15 @@ def test_control_characters_as_bytes():\n     \"\"\"Byte control characters (comments, delimiter) are supported.\"\"\"\n     a = np.loadtxt(StringIO(\"#header\\n1,2,3\"), comments=b\"#\", delimiter=b\",\")\n     assert_equal(a, [1, 2, 3])\n+\n+\n+@pytest.mark.filterwarnings('ignore::UserWarning')\n+def test_field_growing_cases():\n+    # Test empty field appending/growing (each field still takes 1 character)\n+    # to see if the final field appending does not create issues.\n+    res = np.loadtxt([\"\"], delimiter=\",\", dtype=bytes)\n+    assert len(res) == 0\n+\n+    for i in range(1, 1024):\n+        res = np.loadtxt([\",\" * i], delimiter=\",\", dtype=bytes)\n+        assert len(res) == i+1\n",
            "comment_added_diff": {
                "1018": "    # Test empty field appending/growing (each field still takes 1 character)",
                "1019": "    # to see if the final field appending does not create issues."
            },
            "comment_deleted_diff": {},
            "comment_modified_diff": {}
        }
    ],
    "bug-report.yml": [],
    "test_strings.py": [],
    "arraysetops.py": [
        {
            "commit": "235dbe1f9ea0955c0119f79a5c6614cd0268ef05",
            "timestamp": "2022-12-25T11:43:43-07:00",
            "author": "Miles Cranmer",
            "commit_message": "BUG: Fix integer overflow in in1d for mixed integer dtypes #22877 (#22878)\n\n* TST: Mixed integer types for in1d\r\n\r\n* BUG: Fix mixed dtype overflows for in1d (#22877)\r\n\r\n* BUG: Type conversion for integer overflow check\r\n\r\n* MAINT: Fix linting issues in in1d\r\n\r\n* MAINT: ar1 overflow check only for non-empty array\r\n\r\n* MAINT: Expand bounds of overflow check\r\n\r\n* TST: Fix integer overflow in mixed boolean test\r\n\r\n* TST: Include test for overflow on mixed dtypes\r\n\r\n* MAINT: Less conservative overflow checks",
            "additions": 18,
            "deletions": 2,
            "change_type": "MODIFY",
            "diff": "@@ -649,8 +649,24 @@ def in1d(ar1, ar2, assume_unique=False, invert=False, *, kind=None):\n         ar2_range = int(ar2_max) - int(ar2_min)\n \n         # Constraints on whether we can actually use the table method:\n-        range_safe_from_overflow = ar2_range < np.iinfo(ar2.dtype).max\n+        #  1. Assert memory usage is not too large\n         below_memory_constraint = ar2_range <= 6 * (ar1.size + ar2.size)\n+        #  2. Check overflows for (ar2 - ar2_min); dtype=ar2.dtype\n+        range_safe_from_overflow = ar2_range <= np.iinfo(ar2.dtype).max\n+        #  3. Check overflows for (ar1 - ar2_min); dtype=ar1.dtype\n+        if ar1.size > 0:\n+            ar1_min = np.min(ar1)\n+            ar1_max = np.max(ar1)\n+\n+            # After masking, the range of ar1 is guaranteed to be\n+            # within the range of ar2:\n+            ar1_upper = min(int(ar1_max), int(ar2_max))\n+            ar1_lower = max(int(ar1_min), int(ar2_min))\n+\n+            range_safe_from_overflow &= all((\n+                ar1_upper - int(ar2_min) <= np.iinfo(ar1.dtype).max,\n+                ar1_lower - int(ar2_min) >= np.iinfo(ar1.dtype).min\n+            ))\n \n         # Optimal performance is for approximately\n         # log10(size) > (log10(range) - 2.27) / 0.927.\n@@ -687,7 +703,7 @@ def in1d(ar1, ar2, assume_unique=False, invert=False, *, kind=None):\n         elif kind == 'table':  # not range_safe_from_overflow\n             raise RuntimeError(\n                 \"You have specified kind='table', \"\n-                \"but the range of values in `ar2` exceeds the \"\n+                \"but the range of values in `ar2` or `ar1` exceed the \"\n                 \"maximum integer of the datatype. \"\n                 \"Please set `kind` to None or 'sort'.\"\n             )\n",
            "comment_added_diff": {
                "652": "        #  1. Assert memory usage is not too large",
                "654": "        #  2. Check overflows for (ar2 - ar2_min); dtype=ar2.dtype",
                "656": "        #  3. Check overflows for (ar1 - ar2_min); dtype=ar1.dtype",
                "661": "            # After masking, the range of ar1 is guaranteed to be",
                "662": "            # within the range of ar2:"
            },
            "comment_deleted_diff": {},
            "comment_modified_diff": {
                "652": "        range_safe_from_overflow = ar2_range < np.iinfo(ar2.dtype).max"
            }
        }
    ],
    "test_arraysetops.py": [
        {
            "commit": "2f0bd6e86a77e4401d0384d9a75edf9470c5deb6",
            "timestamp": "2023-09-05T21:58:22+02:00",
            "author": "Mateusz Sok\u00f3\u0142",
            "commit_message": "API: Cleaning `numpy/__init__.py` and main namespace - Part 4 [NEP 52] (#24445)\n\n[skip ci]",
            "additions": 66,
            "deletions": 90,
            "change_type": "MODIFY",
            "diff": "@@ -4,7 +4,7 @@\n import numpy as np\n \n from numpy import (\n-    ediff1d, intersect1d, setxor1d, union1d, setdiff1d, unique, in1d, isin\n+    ediff1d, intersect1d, setxor1d, union1d, setdiff1d, unique, isin\n     )\n from numpy.exceptions import AxisError\n from numpy.testing import (assert_array_equal, assert_equal,\n@@ -198,9 +198,6 @@ def test_ediff1d_scalar_handling(self,\n \n     @pytest.mark.parametrize(\"kind\", [None, \"sort\", \"table\"])\n     def test_isin(self, kind):\n-        # the tests for in1d cover most of isin's behavior\n-        # if in1d is removed, would need to change those tests to test\n-        # isin instead.\n         def _isin_slow(a, b):\n             b = np.asarray(b).flatten().tolist()\n             return a in b\n@@ -258,86 +255,86 @@ def assert_isin_equal(a, b):\n             assert_isin_equal(empty_array, empty_array)\n \n     @pytest.mark.parametrize(\"kind\", [None, \"sort\", \"table\"])\n-    def test_in1d(self, kind):\n+    def test_isin(self, kind):\n         # we use two different sizes for the b array here to test the\n-        # two different paths in in1d().\n+        # two different paths in isin().\n         for mult in (1, 10):\n             # One check without np.array to make sure lists are handled correct\n             a = [5, 7, 1, 2]\n             b = [2, 4, 3, 1, 5] * mult\n             ec = np.array([True, False, True, True])\n-            c = in1d(a, b, assume_unique=True, kind=kind)\n+            c = isin(a, b, assume_unique=True, kind=kind)\n             assert_array_equal(c, ec)\n \n             a[0] = 8\n             ec = np.array([False, False, True, True])\n-            c = in1d(a, b, assume_unique=True, kind=kind)\n+            c = isin(a, b, assume_unique=True, kind=kind)\n             assert_array_equal(c, ec)\n \n             a[0], a[3] = 4, 8\n             ec = np.array([True, False, True, False])\n-            c = in1d(a, b, assume_unique=True, kind=kind)\n+            c = isin(a, b, assume_unique=True, kind=kind)\n             assert_array_equal(c, ec)\n \n             a = np.array([5, 4, 5, 3, 4, 4, 3, 4, 3, 5, 2, 1, 5, 5])\n             b = [2, 3, 4] * mult\n             ec = [False, True, False, True, True, True, True, True, True,\n                   False, True, False, False, False]\n-            c = in1d(a, b, kind=kind)\n+            c = isin(a, b, kind=kind)\n             assert_array_equal(c, ec)\n \n             b = b + [5, 5, 4] * mult\n             ec = [True, True, True, True, True, True, True, True, True, True,\n                   True, False, True, True]\n-            c = in1d(a, b, kind=kind)\n+            c = isin(a, b, kind=kind)\n             assert_array_equal(c, ec)\n \n             a = np.array([5, 7, 1, 2])\n             b = np.array([2, 4, 3, 1, 5] * mult)\n             ec = np.array([True, False, True, True])\n-            c = in1d(a, b, kind=kind)\n+            c = isin(a, b, kind=kind)\n             assert_array_equal(c, ec)\n \n             a = np.array([5, 7, 1, 1, 2])\n             b = np.array([2, 4, 3, 3, 1, 5] * mult)\n             ec = np.array([True, False, True, True, True])\n-            c = in1d(a, b, kind=kind)\n+            c = isin(a, b, kind=kind)\n             assert_array_equal(c, ec)\n \n             a = np.array([5, 5])\n             b = np.array([2, 2] * mult)\n             ec = np.array([False, False])\n-            c = in1d(a, b, kind=kind)\n+            c = isin(a, b, kind=kind)\n             assert_array_equal(c, ec)\n \n         a = np.array([5])\n         b = np.array([2])\n         ec = np.array([False])\n-        c = in1d(a, b, kind=kind)\n+        c = isin(a, b, kind=kind)\n         assert_array_equal(c, ec)\n \n         if kind in {None, \"sort\"}:\n-            assert_array_equal(in1d([], [], kind=kind), [])\n+            assert_array_equal(isin([], [], kind=kind), [])\n \n-    def test_in1d_char_array(self):\n+    def test_isin_char_array(self):\n         a = np.array(['a', 'b', 'c', 'd', 'e', 'c', 'e', 'b'])\n         b = np.array(['a', 'c'])\n \n         ec = np.array([True, False, True, False, False, True, False, False])\n-        c = in1d(a, b)\n+        c = isin(a, b)\n \n         assert_array_equal(c, ec)\n \n     @pytest.mark.parametrize(\"kind\", [None, \"sort\", \"table\"])\n-    def test_in1d_invert(self, kind):\n-        \"Test in1d's invert parameter\"\n+    def test_isin_invert(self, kind):\n+        \"Test isin's invert parameter\"\n         # We use two different sizes for the b array here to test the\n-        # two different paths in in1d().\n+        # two different paths in isin().\n         for mult in (1, 10):\n             a = np.array([5, 4, 5, 3, 4, 4, 3, 4, 3, 5, 2, 1, 5, 5])\n             b = [2, 3, 4] * mult\n-            assert_array_equal(np.invert(in1d(a, b, kind=kind)),\n-                               in1d(a, b, invert=True, kind=kind))\n+            assert_array_equal(np.invert(isin(a, b, kind=kind)),\n+                               isin(a, b, invert=True, kind=kind))\n \n         # float:\n         if kind in {None, \"sort\"}:\n@@ -346,74 +343,53 @@ def test_in1d_invert(self, kind):\n                             dtype=np.float32)\n                 b = [2, 3, 4] * mult\n                 b = np.array(b, dtype=np.float32)\n-                assert_array_equal(np.invert(in1d(a, b, kind=kind)),\n-                                   in1d(a, b, invert=True, kind=kind))\n+                assert_array_equal(np.invert(isin(a, b, kind=kind)),\n+                                   isin(a, b, invert=True, kind=kind))\n \n-    @pytest.mark.parametrize(\"kind\", [None, \"sort\", \"table\"])\n-    def test_in1d_ravel(self, kind):\n-        # Test that in1d ravels its input arrays. This is not documented\n-        # behavior however. The test is to ensure consistentency.\n-        a = np.arange(6).reshape(2, 3)\n-        b = np.arange(3, 9).reshape(3, 2)\n-        long_b = np.arange(3, 63).reshape(30, 2)\n-        ec = np.array([False, False, False, True, True, True])\n-\n-        assert_array_equal(in1d(a, b, assume_unique=True, kind=kind),\n-                           ec)\n-        assert_array_equal(in1d(a, b, assume_unique=False,\n-                                kind=kind),\n-                           ec)\n-        assert_array_equal(in1d(a, long_b, assume_unique=True,\n-                                kind=kind),\n-                           ec)\n-        assert_array_equal(in1d(a, long_b, assume_unique=False,\n-                                kind=kind),\n-                           ec)\n-\n-    def test_in1d_hit_alternate_algorithm(self):\n+    def test_isin_hit_alternate_algorithm(self):\n         \"\"\"Hit the standard isin code with integers\"\"\"\n         # Need extreme range to hit standard code\n         # This hits it without the use of kind='table'\n         a = np.array([5, 4, 5, 3, 4, 4, 1e9], dtype=np.int64)\n         b = np.array([2, 3, 4, 1e9], dtype=np.int64)\n         expected = np.array([0, 1, 0, 1, 1, 1, 1], dtype=bool)\n-        assert_array_equal(expected, in1d(a, b))\n-        assert_array_equal(np.invert(expected), in1d(a, b, invert=True))\n+        assert_array_equal(expected, isin(a, b))\n+        assert_array_equal(np.invert(expected), isin(a, b, invert=True))\n \n         a = np.array([5, 7, 1, 2], dtype=np.int64)\n         b = np.array([2, 4, 3, 1, 5, 1e9], dtype=np.int64)\n         ec = np.array([True, False, True, True])\n-        c = in1d(a, b, assume_unique=True)\n+        c = isin(a, b, assume_unique=True)\n         assert_array_equal(c, ec)\n \n     @pytest.mark.parametrize(\"kind\", [None, \"sort\", \"table\"])\n-    def test_in1d_boolean(self, kind):\n-        \"\"\"Test that in1d works for boolean input\"\"\"\n+    def test_isin_boolean(self, kind):\n+        \"\"\"Test that isin works for boolean input\"\"\"\n         a = np.array([True, False])\n         b = np.array([False, False, False])\n         expected = np.array([False, True])\n         assert_array_equal(expected,\n-                           in1d(a, b, kind=kind))\n+                           isin(a, b, kind=kind))\n         assert_array_equal(np.invert(expected),\n-                           in1d(a, b, invert=True, kind=kind))\n+                           isin(a, b, invert=True, kind=kind))\n \n     @pytest.mark.parametrize(\"kind\", [None, \"sort\"])\n-    def test_in1d_timedelta(self, kind):\n-        \"\"\"Test that in1d works for timedelta input\"\"\"\n+    def test_isin_timedelta(self, kind):\n+        \"\"\"Test that isin works for timedelta input\"\"\"\n         rstate = np.random.RandomState(0)\n         a = rstate.randint(0, 100, size=10)\n         b = rstate.randint(0, 100, size=10)\n-        truth = in1d(a, b)\n+        truth = isin(a, b)\n         a_timedelta = a.astype(\"timedelta64[s]\")\n         b_timedelta = b.astype(\"timedelta64[s]\")\n-        assert_array_equal(truth, in1d(a_timedelta, b_timedelta, kind=kind))\n+        assert_array_equal(truth, isin(a_timedelta, b_timedelta, kind=kind))\n \n-    def test_in1d_table_timedelta_fails(self):\n+    def test_isin_table_timedelta_fails(self):\n         a = np.array([0, 1, 2], dtype=\"timedelta64[s]\")\n         b = a\n         # Make sure it raises a value error:\n         with pytest.raises(ValueError):\n-            in1d(a, b, kind=\"table\")\n+            isin(a, b, kind=\"table\")\n \n     @pytest.mark.parametrize(\n         \"dtype1,dtype2\",\n@@ -427,8 +403,8 @@ def test_in1d_table_timedelta_fails(self):\n         ]\n     )\n     @pytest.mark.parametrize(\"kind\", [None, \"sort\", \"table\"])\n-    def test_in1d_mixed_dtype(self, dtype1, dtype2, kind):\n-        \"\"\"Test that in1d works as expected for mixed dtype input.\"\"\"\n+    def test_isin_mixed_dtype(self, dtype1, dtype2, kind):\n+        \"\"\"Test that isin works as expected for mixed dtype input.\"\"\"\n         is_dtype2_signed = np.issubdtype(dtype2, np.signedinteger)\n         ar1 = np.array([0, 0, 1, 1], dtype=dtype1)\n \n@@ -446,61 +422,61 @@ def test_in1d_mixed_dtype(self, dtype1, dtype2, kind):\n \n         if expect_failure:\n             with pytest.raises(RuntimeError, match=\"exceed the maximum\"):\n-                in1d(ar1, ar2, kind=kind)\n+                isin(ar1, ar2, kind=kind)\n         else:\n-            assert_array_equal(in1d(ar1, ar2, kind=kind), expected)\n+            assert_array_equal(isin(ar1, ar2, kind=kind), expected)\n \n     @pytest.mark.parametrize(\"kind\", [None, \"sort\", \"table\"])\n-    def test_in1d_mixed_boolean(self, kind):\n-        \"\"\"Test that in1d works as expected for bool/int input.\"\"\"\n+    def test_isin_mixed_boolean(self, kind):\n+        \"\"\"Test that isin works as expected for bool/int input.\"\"\"\n         for dtype in np.typecodes[\"AllInteger\"]:\n             a = np.array([True, False, False], dtype=bool)\n             b = np.array([0, 0, 0, 0], dtype=dtype)\n             expected = np.array([False, True, True], dtype=bool)\n-            assert_array_equal(in1d(a, b, kind=kind), expected)\n+            assert_array_equal(isin(a, b, kind=kind), expected)\n \n             a, b = b, a\n             expected = np.array([True, True, True, True], dtype=bool)\n-            assert_array_equal(in1d(a, b, kind=kind), expected)\n+            assert_array_equal(isin(a, b, kind=kind), expected)\n \n-    def test_in1d_first_array_is_object(self):\n+    def test_isin_first_array_is_object(self):\n         ar1 = [None]\n         ar2 = np.array([1]*10)\n         expected = np.array([False])\n-        result = np.in1d(ar1, ar2)\n+        result = np.isin(ar1, ar2)\n         assert_array_equal(result, expected)\n \n-    def test_in1d_second_array_is_object(self):\n+    def test_isin_second_array_is_object(self):\n         ar1 = 1\n         ar2 = np.array([None]*10)\n         expected = np.array([False])\n-        result = np.in1d(ar1, ar2)\n+        result = np.isin(ar1, ar2)\n         assert_array_equal(result, expected)\n \n-    def test_in1d_both_arrays_are_object(self):\n+    def test_isin_both_arrays_are_object(self):\n         ar1 = [None]\n         ar2 = np.array([None]*10)\n         expected = np.array([True])\n-        result = np.in1d(ar1, ar2)\n+        result = np.isin(ar1, ar2)\n         assert_array_equal(result, expected)\n \n-    def test_in1d_both_arrays_have_structured_dtype(self):\n+    def test_isin_both_arrays_have_structured_dtype(self):\n         # Test arrays of a structured data type containing an integer field\n         # and a field of dtype `object` allowing for arbitrary Python objects\n         dt = np.dtype([('field1', int), ('field2', object)])\n         ar1 = np.array([(1, None)], dtype=dt)\n         ar2 = np.array([(1, None)]*10, dtype=dt)\n         expected = np.array([True])\n-        result = np.in1d(ar1, ar2)\n+        result = np.isin(ar1, ar2)\n         assert_array_equal(result, expected)\n \n-    def test_in1d_with_arrays_containing_tuples(self):\n+    def test_isin_with_arrays_containing_tuples(self):\n         ar1 = np.array([(1,), 2], dtype=object)\n         ar2 = np.array([(1,), 2], dtype=object)\n         expected = np.array([True, True])\n-        result = np.in1d(ar1, ar2)\n+        result = np.isin(ar1, ar2)\n         assert_array_equal(result, expected)\n-        result = np.in1d(ar1, ar2, invert=True)\n+        result = np.isin(ar1, ar2, invert=True)\n         assert_array_equal(result, np.invert(expected))\n \n         # An integer is added at the end of the array to make sure\n@@ -513,32 +489,32 @@ def test_in1d_with_arrays_containing_tuples(self):\n         ar2 = np.array([(1,), (2, 1), 1], dtype=object)\n         ar2 = ar2[:-1]\n         expected = np.array([True, True])\n-        result = np.in1d(ar1, ar2)\n+        result = np.isin(ar1, ar2)\n         assert_array_equal(result, expected)\n-        result = np.in1d(ar1, ar2, invert=True)\n+        result = np.isin(ar1, ar2, invert=True)\n         assert_array_equal(result, np.invert(expected))\n \n         ar1 = np.array([(1,), (2, 3), 1], dtype=object)\n         ar1 = ar1[:-1]\n         ar2 = np.array([(1,), 2], dtype=object)\n         expected = np.array([True, False])\n-        result = np.in1d(ar1, ar2)\n+        result = np.isin(ar1, ar2)\n         assert_array_equal(result, expected)\n-        result = np.in1d(ar1, ar2, invert=True)\n+        result = np.isin(ar1, ar2, invert=True)\n         assert_array_equal(result, np.invert(expected))\n \n-    def test_in1d_errors(self):\n-        \"\"\"Test that in1d raises expected errors.\"\"\"\n+    def test_isin_errors(self):\n+        \"\"\"Test that isin raises expected errors.\"\"\"\n \n         # Error 1: `kind` is not one of 'sort' 'table' or None.\n         ar1 = np.array([1, 2, 3, 4, 5])\n         ar2 = np.array([2, 4, 6, 8, 10])\n-        assert_raises(ValueError, in1d, ar1, ar2, kind='quicksort')\n+        assert_raises(ValueError, isin, ar1, ar2, kind='quicksort')\n \n         # Error 2: `kind=\"table\"` does not work for non-integral arrays.\n         obj_ar1 = np.array([1, 'a', 3, 'b', 5], dtype=object)\n         obj_ar2 = np.array([1, 'a', 3, 'b', 5], dtype=object)\n-        assert_raises(ValueError, in1d, obj_ar1, obj_ar2, kind='table')\n+        assert_raises(ValueError, isin, obj_ar1, obj_ar2, kind='table')\n \n         for dtype in [np.int32, np.int64]:\n             ar1 = np.array([-1, 2, 3, 4, 5], dtype=dtype)\n@@ -550,15 +526,15 @@ def test_in1d_errors(self):\n             #  range of ar2\n             assert_raises(\n                 RuntimeError,\n-                in1d, ar1, overflow_ar2, kind='table'\n+                isin, ar1, overflow_ar2, kind='table'\n             )\n \n             # Non-error: `kind=None` will *not* trigger a runtime error\n             #  if there is an integer overflow, it will switch to\n             #  the `sort` algorithm.\n-            result = np.in1d(ar1, overflow_ar2, kind=None)\n+            result = np.isin(ar1, overflow_ar2, kind=None)\n             assert_array_equal(result, [True] + [False] * 4)\n-            result = np.in1d(ar1, overflow_ar2, kind='sort')\n+            result = np.isin(ar1, overflow_ar2, kind='sort')\n             assert_array_equal(result, [True] + [False] * 4)\n \n     def test_union1d(self):\n",
            "comment_added_diff": {
                "260": "        # two different paths in isin().",
                "332": "        # two different paths in isin()."
            },
            "comment_deleted_diff": {
                "201": "        # the tests for in1d cover most of isin's behavior",
                "202": "        # if in1d is removed, would need to change those tests to test",
                "203": "        # isin instead.",
                "263": "        # two different paths in in1d().",
                "335": "        # two different paths in in1d().",
                "354": "        # Test that in1d ravels its input arrays. This is not documented",
                "355": "        # behavior however. The test is to ensure consistentency."
            },
            "comment_modified_diff": {
                "332": "    def test_in1d_invert(self, kind):"
            }
        }
    ],
    "crackfortran.py": [
        {
            "commit": "fe73a8498417d762a2102d759fa4c88613b398ef",
            "timestamp": "2022-12-25T16:20:55-07:00",
            "author": "Rohit Goswami",
            "commit_message": "BUG: Use whole file for encoding checks with `charset_normalizer` [f2py] (#22872)\n\n* BUG: Use whole file for encoding checks [f2py]\r\n\r\n* DOC: Add a code comment\r\n\r\nCo-authored-by: melissawm <melissawm@gmail.com>\r\n\r\n* TST: Add a conditional unicode f2py test\r\n\r\n* MAINT: Add chardet as a test requirement\r\n\r\n* ENH: Cleanup and switch f2py to charset_normalizer\r\n\r\n* MAINT: Remove chardet for charset_normalizer\r\n\r\n* TST: Simplify UTF-8 encoding [f2py]\r\n\r\nCo-authored-by: melissawm <melissawm@gmail.com>",
            "additions": 26,
            "deletions": 21,
            "change_type": "MODIFY",
            "diff": "@@ -148,9 +148,9 @@\n import platform\n import codecs\n try:\n-    import chardet\n+    import charset_normalizer\n except ImportError:\n-    chardet = None\n+    charset_normalizer = None\n \n from . import __version__\n \n@@ -309,26 +309,31 @@ def getextension(name):\n def openhook(filename, mode):\n     \"\"\"Ensures that filename is opened with correct encoding parameter.\n \n-    This function uses chardet package, when available, for\n-    determining the encoding of the file to be opened. When chardet is\n-    not available, the function detects only UTF encodings, otherwise,\n-    ASCII encoding is used as fallback.\n+    This function uses charset_normalizer package, when available, for\n+    determining the encoding of the file to be opened. When charset_normalizer\n+    is not available, the function detects only UTF encodings, otherwise, ASCII\n+    encoding is used as fallback.\n     \"\"\"\n-    bytes = min(32, os.path.getsize(filename))\n-    with open(filename, 'rb') as f:\n-        raw = f.read(bytes)\n-    if raw.startswith(codecs.BOM_UTF8):\n-        encoding = 'UTF-8-SIG'\n-    elif raw.startswith((codecs.BOM_UTF32_LE, codecs.BOM_UTF32_BE)):\n-        encoding = 'UTF-32'\n-    elif raw.startswith((codecs.BOM_LE, codecs.BOM_BE)):\n-        encoding = 'UTF-16'\n+    # Reads in the entire file. Robust detection of encoding.\n+    # Correctly handles comments or late stage unicode characters\n+    # gh-22871\n+    if charset_normalizer is not None:\n+        encoding = charset_normalizer.from_path(filename).best().encoding\n     else:\n-        if chardet is not None:\n-            encoding = chardet.detect(raw)['encoding']\n-        else:\n-            # hint: install chardet to ensure correct encoding handling\n-            encoding = 'ascii'\n+        # hint: install charset_normalizer for correct encoding handling\n+        # No need to read the whole file for trying with startswith\n+        nbytes = min(32, os.path.getsize(filename))\n+        with open(filename, 'rb') as fhandle:\n+            raw = fhandle.read(nbytes)\n+            if raw.startswith(codecs.BOM_UTF8):\n+                encoding = 'UTF-8-SIG'\n+            elif raw.startswith((codecs.BOM_UTF32_LE, codecs.BOM_UTF32_BE)):\n+                encoding = 'UTF-32'\n+            elif raw.startswith((codecs.BOM_LE, codecs.BOM_BE)):\n+                encoding = 'UTF-16'\n+            else:\n+                # Fallback, without charset_normalizer\n+                encoding = 'ascii'\n     return open(filename, mode, encoding=encoding)\n \n \n@@ -394,7 +399,7 @@ def readfortrancode(ffile, dowithline=show, istop=1):\n         except UnicodeDecodeError as msg:\n             raise Exception(\n                 f'readfortrancode: reading {fin.filename()}#{fin.lineno()}'\n-                f' failed with\\n{msg}.\\nIt is likely that installing chardet'\n+                f' failed with\\n{msg}.\\nIt is likely that installing charset_normalizer'\n                 ' package will help f2py determine the input file encoding'\n                 ' correctly.')\n         if not l:\n",
            "comment_added_diff": {
                "317": "    # Reads in the entire file. Robust detection of encoding.",
                "318": "    # Correctly handles comments or late stage unicode characters",
                "319": "    # gh-22871",
                "323": "        # hint: install charset_normalizer for correct encoding handling",
                "324": "        # No need to read the whole file for trying with startswith",
                "335": "                # Fallback, without charset_normalizer"
            },
            "comment_deleted_diff": {
                "330": "            # hint: install chardet to ensure correct encoding handling"
            },
            "comment_modified_diff": {
                "317": "    bytes = min(32, os.path.getsize(filename))",
                "318": "    with open(filename, 'rb') as f:",
                "319": "        raw = f.read(bytes)",
                "323": "        encoding = 'UTF-32'",
                "324": "    elif raw.startswith((codecs.BOM_LE, codecs.BOM_BE)):"
            }
        },
        {
            "commit": "6b5cd92675139511b4b24ddfe822e96b03700edb",
            "timestamp": "2023-01-16T15:51:45+01:00",
            "author": "Rohit Goswami",
            "commit_message": "MAINT: `f2py` cleanup (#22885)\n\nUpdates the free format handling of .f90 and other common extensions (through a minor re-write). Also removes an unused function.\r\n\r\nThis disallows previously allowed (but highly unlikely to be present) code-paths, namely having fixed form F77 code in a fortran 90 file (with .f90).\r\n\r\nCo-authored-by: Sebastian Berg <sebastianb@nvidia.com>",
            "additions": 15,
            "deletions": 19,
            "change_type": "MODIFY",
            "diff": "@@ -147,6 +147,7 @@\n import copy\n import platform\n import codecs\n+from pathlib import Path\n try:\n     import charset_normalizer\n except ImportError:\n@@ -289,22 +290,15 @@ def undo_rmbadname(names):\n     return [undo_rmbadname1(_m) for _m in names]\n \n \n-def getextension(name):\n-    i = name.rfind('.')\n-    if i == -1:\n-        return ''\n-    if '\\\\' in name[i:]:\n-        return ''\n-    if '/' in name[i:]:\n-        return ''\n-    return name[i + 1:]\n-\n-is_f_file = re.compile(r'.*\\.(for|ftn|f77|f)\\Z', re.I).match\n _has_f_header = re.compile(r'-\\*-\\s*fortran\\s*-\\*-', re.I).search\n _has_f90_header = re.compile(r'-\\*-\\s*f90\\s*-\\*-', re.I).search\n _has_fix_header = re.compile(r'-\\*-\\s*fix\\s*-\\*-', re.I).search\n _free_f90_start = re.compile(r'[^c*]\\s*[^\\s\\d\\t]', re.I).match\n \n+# Extensions\n+COMMON_FREE_EXTENSIONS = ['.f90', '.f95', '.f03', '.f08']\n+COMMON_FIXED_EXTENSIONS = ['.for', '.ftn', '.f77', '.f']\n+\n \n def openhook(filename, mode):\n     \"\"\"Ensures that filename is opened with correct encoding parameter.\n@@ -337,26 +331,28 @@ def openhook(filename, mode):\n     return open(filename, mode, encoding=encoding)\n \n \n-def is_free_format(file):\n+def is_free_format(fname):\n     \"\"\"Check if file is in free format Fortran.\"\"\"\n     # f90 allows both fixed and free format, assuming fixed unless\n     # signs of free format are detected.\n-    result = 0\n-    with openhook(file, 'r') as f:\n-        line = f.readline()\n+    result = False\n+    if Path(fname).suffix.lower() in COMMON_FREE_EXTENSIONS:\n+        result = True\n+    with openhook(fname, 'r') as fhandle:\n+        line = fhandle.readline()\n         n = 15  # the number of non-comment lines to scan for hints\n         if _has_f_header(line):\n             n = 0\n         elif _has_f90_header(line):\n             n = 0\n-            result = 1\n+            result = True\n         while n > 0 and line:\n             if line[0] != '!' and line.strip():\n                 n -= 1\n                 if (line[0] != '\\t' and _free_f90_start(line[:5])) or line[-2:-1] == '&':\n-                    result = 1\n+                    result = True\n                     break\n-            line = f.readline()\n+            line = fhandle.readline()\n     return result\n \n \n@@ -412,7 +408,7 @@ def readfortrancode(ffile, dowithline=show, istop=1):\n             strictf77 = 0\n             sourcecodeform = 'fix'\n             ext = os.path.splitext(currentfilename)[1]\n-            if is_f_file(currentfilename) and \\\n+            if Path(currentfilename).suffix.lower() in COMMON_FIXED_EXTENSIONS and \\\n                     not (_has_f90_header(l) or _has_fix_header(l)):\n                 strictf77 = 1\n             elif is_free_format(currentfilename) and not _has_fix_header(l):\n",
            "comment_added_diff": {
                "298": "# Extensions"
            },
            "comment_deleted_diff": {},
            "comment_modified_diff": {
                "298": "    if '/' in name[i:]:"
            }
        },
        {
            "commit": "a97f209fe33fa5611f9d286a86e70fda230e6784",
            "timestamp": "2023-04-15T19:00:16+00:00",
            "author": "Rohit Goswami",
            "commit_message": "BUG: Handle data statements in pyf files correctly",
            "additions": 7,
            "deletions": 1,
            "change_type": "MODIFY",
            "diff": "@@ -1430,6 +1430,7 @@ def analyzeline(m, case, line):\n                     continue\n                 fc = 0\n                 vtype = vars[v].get('typespec')\n+                vdim = getdimension(vars[v])\n \n                 if (vtype == 'complex'):\n                     cmplxpat = r\"\\(.*?\\)\"\n@@ -1442,7 +1443,12 @@ def analyzeline(m, case, line):\n                 if '=' in vars[v] and not vars[v]['='] == matches[idx]:\n                     outmess('analyzeline: changing init expression of \"%s\" (\"%s\") to \"%s\"\\n' % (\n                         v, vars[v]['='], matches[idx]))\n-                vars[v]['='] = matches[idx]\n+\n+                if vdim is not None:\n+                    # Need to assign multiple values to one variable\n+                    vars[v]['='] = \"(/{}/)\".format(\", \".join(matches))\n+                else:\n+                    vars[v]['='] = matches[idx]\n                 last_name = v\n         groupcache[groupcounter]['vars'] = vars\n         if last_name is not None:\n",
            "comment_added_diff": {
                "1448": "                    # Need to assign multiple values to one variable"
            },
            "comment_deleted_diff": {},
            "comment_modified_diff": {}
        },
        {
            "commit": "7aa6d6dabe0b4b3e5bf378ec684778b46167cce4",
            "timestamp": "2023-04-15T20:08:56+00:00",
            "author": "Derek Homeier",
            "commit_message": "BUG: include macOS arm64 `machine()` value in `_selected_real_kind_func`",
            "additions": 4,
            "deletions": 4,
            "change_type": "MODIFY",
            "diff": "@@ -2387,19 +2387,19 @@ def _selected_int_kind_func(r):\n \n def _selected_real_kind_func(p, r=0, radix=0):\n     # XXX: This should be processor dependent\n-    # This is only good for 0 <= p <= 20\n+    # This is only verified for 0 <= p <= 20, possibly good for p <= 33 and above\n     if p < 7:\n         return 4\n     if p < 16:\n         return 8\n     machine = platform.machine().lower()\n-    if machine.startswith(('aarch64', 'power', 'ppc', 'riscv', 's390x', 'sparc', 'arm64')):\n-        if p <= 20:\n+    if machine.startswith(('aarch64', 'arm64', 'power', 'ppc', 'riscv', 's390x', 'sparc')):\n+        if p <= 33:\n             return 16\n     else:\n         if p < 19:\n             return 10\n-        elif p <= 20:\n+        elif p <= 33:\n             return 16\n     return -1\n \n",
            "comment_added_diff": {
                "2390": "    # This is only verified for 0 <= p <= 20, possibly good for p <= 33 and above"
            },
            "comment_deleted_diff": {
                "2390": "    # This is only good for 0 <= p <= 20"
            },
            "comment_modified_diff": {
                "2390": "    # This is only good for 0 <= p <= 20"
            }
        },
        {
            "commit": "b8af21ff494d1e93cbf6514283824da0c54ddc82",
            "timestamp": "2023-06-04T23:26:18+00:00",
            "author": "Rohit Goswami",
            "commit_message": "BUG: Hotfix for handling common blocks in f2py",
            "additions": 2,
            "deletions": 1,
            "change_type": "MODIFY",
            "diff": "@@ -614,7 +614,8 @@ def readfortrancode(ffile, dowithline=show, istop=1):\n              r'endinterface|endsubroutine|endfunction')\n endpattern = re.compile(\n     beforethisafter % ('', groupends, groupends, '.*'), re.I), 'end'\n-endifs = r'end\\s*(if|do|where|select|while|forall|associate|block|' + \\\n+# block, the Fortran 2008 construct needs special handling in the rest of the file\n+endifs = r'end\\s*(if|do|where|select|while|forall|associate|' + \\\n          r'critical|enum|team)'\n endifpattern = re.compile(\n     beforethisafter % (r'[\\w]*?', endifs, endifs, '.*'), re.I), 'endif'\n",
            "comment_added_diff": {
                "617": "# block, the Fortran 2008 construct needs special handling in the rest of the file"
            },
            "comment_deleted_diff": {},
            "comment_modified_diff": {
                "617": "endifs = r'end\\s*(if|do|where|select|while|forall|associate|block|' + \\"
            }
        },
        {
            "commit": "891c04e050174034824bb5c4dda5bf6a2297c80c",
            "timestamp": "2023-06-17T19:44:22+00:00",
            "author": "Rohit Goswami",
            "commit_message": "BUG: fix gh-23879 for private procedures\n\nBy adding them to skipfuncs",
            "additions": 6,
            "deletions": 0,
            "change_type": "MODIFY",
            "diff": "@@ -2179,6 +2179,12 @@ def analyzebody(block, args, tab=''):\n     global usermodules, skipfuncs, onlyfuncs, f90modulevars\n \n     setmesstext(block)\n+\n+    # Add private members to skipfuncs\n+    # Fixes gh-23879\n+    private_vars = {key: value for key, value in block['vars'].items() if 'attrspec' not in value or 'public' not in value['attrspec']}\n+    skipfuncs.extend(private_vars.keys())\n+\n     body = []\n     for b in block['body']:\n         b['parent_block'] = block\n",
            "comment_added_diff": {
                "2183": "    # Add private members to skipfuncs",
                "2184": "    # Fixes gh-23879"
            },
            "comment_deleted_diff": {},
            "comment_modified_diff": {}
        },
        {
            "commit": "67a9d3844ab7beab7f0beb5d3b1016d2fbd23f80",
            "timestamp": "2023-06-17T20:32:55+00:00",
            "author": "Rohit Goswami",
            "commit_message": "BUG: Fix overly aggressive skipfuncs additions",
            "additions": 4,
            "deletions": 4,
            "change_type": "MODIFY",
            "diff": "@@ -2180,10 +2180,7 @@ def analyzebody(block, args, tab=''):\n \n     setmesstext(block)\n \n-    # Add private members to skipfuncs\n-    # Fixes gh-23879\n-    private_vars = {key: value for key, value in block['vars'].items() if 'attrspec' not in value or 'public' not in value['attrspec']}\n-    skipfuncs.extend(private_vars.keys())\n+    maybe_private = {key: value for key, value in block['vars'].items() if 'attrspec' not in value or 'public' not in value['attrspec']}\n \n     body = []\n     for b in block['body']:\n@@ -2193,6 +2190,9 @@ def analyzebody(block, args, tab=''):\n                 continue\n             else:\n                 as_ = b['args']\n+            # Add private members to skipfuncs for gh-23879\n+            if b['name'] in maybe_private.keys():\n+                skipfuncs.append(b['name'])\n             if b['name'] in skipfuncs:\n                 continue\n             if onlyfuncs and b['name'] not in onlyfuncs:\n",
            "comment_added_diff": {
                "2193": "            # Add private members to skipfuncs for gh-23879"
            },
            "comment_deleted_diff": {
                "2183": "    # Add private members to skipfuncs",
                "2184": "    # Fixes gh-23879"
            },
            "comment_modified_diff": {}
        },
        {
            "commit": "66f8ba104c61c45a59442d3c85f8c4842177fba3",
            "timestamp": "2023-08-26T18:15:05+00:00",
            "author": "Rohit Goswami",
            "commit_message": "ENH: Rework bind(c) detection\n\nAlso prevents terrible name mangling / general function binding failures",
            "additions": 22,
            "deletions": 4,
            "change_type": "MODIFY",
            "diff": "@@ -696,7 +696,8 @@ def _simplifyargs(argsline):\n     return ','.join(a)\n \n crackline_re_1 = re.compile(r'\\s*(?P<result>\\b[a-z]+\\w*\\b)\\s*=.*', re.I)\n-\n+crackline_bind_1 = re.compile(r'\\s*(?P<bind>\\b[a-z]+\\w*\\b)\\s*=.*', re.I)\n+crackline_bindlang = re.compile(r'\\s*bind\\(\\s*(?P<lang>[^,]+)\\s*,\\s*name\\s*=\\s*\"(?P<lang_name>[^\"]+)\"\\s*\\)', re.I)\n \n def crackline(line, reset=0):\n     \"\"\"\n@@ -967,12 +968,22 @@ def _resolvetypedefpattern(line):\n         return m1.group('name'), attrs, m1.group('params')\n     return None, [], None\n \n+def parse_name_for_bind(line):\n+    pattern = re.compile(r'bind\\(\\s*(?P<lang>[^,]+)\\s*,\\s*name\\s*=\\s*\"(?P<name>[^\"]+)\"\\s*\\)', re.I)\n+    match = pattern.search(line)\n+    bind_statement = None\n+    if match:\n+        bind_statement = match.group(0)\n+        # Remove the 'bind' construct from the line.\n+        line = line[:match.start()] + line[match.end():]\n+    return line, bind_statement\n \n def _resolvenameargspattern(line):\n+    line, bind_cname = parse_name_for_bind(line)\n     line = markouterparen(line)\n     m1 = nameargspattern.match(line)\n     if m1:\n-        return m1.group('name'), m1.group('args'), m1.group('result'), m1.group('bind')\n+        return m1.group('name'), m1.group('args'), m1.group('result'), bind_cname\n     m1 = operatorpattern.match(line)\n     if m1:\n         name = m1.group('scheme') + '(' + m1.group('name') + ')'\n@@ -1022,7 +1033,7 @@ def analyzeline(m, case, line):\n             args = []\n             result = None\n         else:\n-            name, args, result, _ = _resolvenameargspattern(m.group('after'))\n+            name, args, result, bindcline = _resolvenameargspattern(m.group('after'))\n         if name is None:\n             if block == 'block data':\n                 name = '_BLOCK_DATA_'\n@@ -1140,6 +1151,13 @@ def analyzeline(m, case, line):\n             except Exception:\n                 pass\n         if block in ['function', 'subroutine']:  # set global attributes\n+            # name is fortran name\n+            bindcdat = re.search(crackline_bindlang, bindcline)\n+            if bindcdat:\n+                groupcache[groupcounter]['bindlang'] = {name : {}}\n+                groupcache[groupcounter]['bindlang'][name][\"lang\"] = bindcdat.group('lang')\n+                if bindcdat.group('lang_name'):\n+                    groupcache[groupcounter]['bindlang'][name][\"name\"] = bindcdat.group('lang_name')\n             try:\n                 groupcache[groupcounter]['vars'][name] = appenddecl(\n                     groupcache[groupcounter]['vars'][name], groupcache[groupcounter - 2]['vars'][''])\n@@ -1173,7 +1191,7 @@ def analyzeline(m, case, line):\n             groupcounter = groupcounter - 1  # end interface\n \n     elif case == 'entry':\n-        name, args, result, bind = _resolvenameargspattern(m.group('after'))\n+        name, args, result, _= _resolvenameargspattern(m.group('after'))\n         if name is not None:\n             if args:\n                 args = rmbadname([x.strip()\n",
            "comment_added_diff": {
                "977": "        # Remove the 'bind' construct from the line.",
                "1154": "            # name is fortran name"
            },
            "comment_deleted_diff": {},
            "comment_modified_diff": {}
        },
        {
            "commit": "98c9fc19019d614a57c5ff3bae4e5365ee9df520",
            "timestamp": "2023-09-21T11:57:40+00:00",
            "author": "Rohit Goswami",
            "commit_message": "TMP: Hide in a try except",
            "additions": 42,
            "deletions": 17,
            "change_type": "MODIFY",
            "diff": "@@ -1449,26 +1449,51 @@ def analyzeline(m, case, line):\n                     # wrapping.\n                     continue\n                 fc = 0\n-                vtype = vars[v].get('typespec')\n-                vdim = getdimension(vars[v])\n+                try:\n+                    vtype = vars[v].get('typespec') # can fail\n+                    vdim = getdimension(vars[v])\n \n-                if (vtype == 'complex'):\n-                    cmplxpat = r\"\\(.*?\\)\"\n-                    matches = re.findall(cmplxpat, l[1])\n-                else:\n-                    matches = l[1].split(',')\n+                    if (vtype == 'complex'):\n+                        cmplxpat = r\"\\(.*?\\)\"\n+                        matches = re.findall(cmplxpat, l[1])\n+                    else:\n+                        matches = l[1].split(',')\n \n-                if v not in vars:\n-                    vars[v] = {}\n-                if '=' in vars[v] and not vars[v]['='] == matches[idx]:\n-                    outmess('analyzeline: changing init expression of \"%s\" (\"%s\") to \"%s\"\\n' % (\n-                        v, vars[v]['='], matches[idx]))\n+                    if v not in vars:\n+                        vars[v] = {}\n+                    if '=' in vars[v] and not vars[v]['='] == matches[idx]:\n+                        outmess('analyzeline: changing init expression of \"%s\" (\"%s\") to \"%s\"\\n' % (\n+                            v, vars[v]['='], matches[idx]))\n \n-                if vdim is not None:\n-                    # Need to assign multiple values to one variable\n-                    vars[v]['='] = \"(/{}/)\".format(\", \".join(matches))\n-                else:\n-                    vars[v]['='] = matches[idx]\n+                    if vdim is not None:\n+                        # Need to assign multiple values to one variable\n+                        vars[v]['='] = \"(/{}/)\".format(\", \".join(matches))\n+                    else:\n+                        vars[v]['='] = matches[idx]\n+                except:\n+                    i = 0\n+                    j = 0\n+                    for v in rmbadname([x.strip() for x in markoutercomma(l[0]).split('@,@')]):\n+                        if v[0] == '(':\n+                            outmess(\n+                                'analyzeline: implied-DO list \"%s\" is not supported. Skipping.\\n' % v)\n+                            # XXX: subsequent init expressions may get wrong values.\n+                            # Ignoring since data statements are irrelevant for\n+                            # wrapping.\n+                            continue\n+                    fc = 0\n+                    while (i < llen) and (fc or not l[1][i] == ','):\n+                        if l[1][i] == \"'\":\n+                            fc = not fc\n+                        i = i + 1\n+                    i = i + 1\n+                    if v not in vars:\n+                         vars[v] = {}\n+                    if '=' in vars[v] and not vars[v]['='] == l[1][j:i - 1]:\n+                        outmess('analyzeline: changing init expression of \"%s\" (\"%s\") to \"%s\"\\n' % (\n+                            v, vars[v]['='], l[1][j:i - 1]))\n+                    vars[v]['='] = l[1][j:i - 1]\n+                    j = i\n                 last_name = v\n         groupcache[groupcounter]['vars'] = vars\n         if last_name is not None:\n",
            "comment_added_diff": {
                "1453": "                    vtype = vars[v].get('typespec') # can fail",
                "1469": "                        # Need to assign multiple values to one variable",
                "1480": "                            # XXX: subsequent init expressions may get wrong values.",
                "1481": "                            # Ignoring since data statements are irrelevant for",
                "1482": "                            # wrapping."
            },
            "comment_deleted_diff": {
                "1468": "                    # Need to assign multiple values to one variable"
            },
            "comment_modified_diff": {
                "1453": "                vdim = getdimension(vars[v])",
                "1469": "                    vars[v]['='] = \"(/{}/)\".format(\", \".join(matches))"
            }
        },
        {
            "commit": "00f92ccf338fee42c33be4b63ff4d877c053d1d9",
            "timestamp": "2023-09-22T16:01:34+00:00",
            "author": "Rohit Goswami",
            "commit_message": "BUG,MAINT: Fallback to old behavior on F77",
            "additions": 16,
            "deletions": 43,
            "change_type": "MODIFY",
            "diff": "@@ -1425,9 +1425,7 @@ def analyzeline(m, case, line):\n             if dl.startswith(','):\n                 dl = dl[1:].strip()\n             ll.append([dl, il])\n-        vars = {}\n-        if 'vars' in groupcache[groupcounter]:\n-            vars = groupcache[groupcounter]['vars']\n+        vars = groupcache[groupcounter].get('vars', {})\n         last_name = None\n         for l in ll:\n             l = [x.strip() for x in l]\n@@ -1452,48 +1450,23 @@ def analyzeline(m, case, line):\n                 try:\n                     vtype = vars[v].get('typespec') # can fail\n                     vdim = getdimension(vars[v])\n-\n-                    if (vtype == 'complex'):\n-                        cmplxpat = r\"\\(.*?\\)\"\n-                        matches = re.findall(cmplxpat, l[1])\n-                    else:\n-                        matches = l[1].split(',')\n-\n-                    if v not in vars:\n-                        vars[v] = {}\n-                    if '=' in vars[v] and not vars[v]['='] == matches[idx]:\n-                        outmess('analyzeline: changing init expression of \"%s\" (\"%s\") to \"%s\"\\n' % (\n-                            v, vars[v]['='], matches[idx]))\n-\n-                    if vdim is not None:\n-                        # Need to assign multiple values to one variable\n-                        vars[v]['='] = \"(/{}/)\".format(\", \".join(matches))\n-                    else:\n-                        vars[v]['='] = matches[idx]\n+                    matches = re.findall(r\"\\(.*?\\)\", l[1]) if vtype == 'complex' else l[1].split(',')\n+                    vars.setdefault(v, {})\n+                    if '=' in vars[v] and vars[v]['='] != matches[idx]:\n+                        outmess('analyzeline: changing init expression of \"%s\" (\"%s\") to \"%s\"\\n' % (v, vars[v]['='], matches[idx]))\n+                    vars[v]['='] = \"(/{}/)\".format(\", \".join(matches)) if vdim is not None else matches[idx]\n                 except:\n-                    i = 0\n-                    j = 0\n-                    for v in rmbadname([x.strip() for x in markoutercomma(l[0]).split('@,@')]):\n-                        if v[0] == '(':\n-                            outmess(\n-                                'analyzeline: implied-DO list \"%s\" is not supported. Skipping.\\n' % v)\n-                            # XXX: subsequent init expressions may get wrong values.\n-                            # Ignoring since data statements are irrelevant for\n-                            # wrapping.\n-                            continue\n-                    fc = 0\n-                    while (i < llen) and (fc or not l[1][i] == ','):\n-                        if l[1][i] == \"'\":\n+                    idy, jdx, fc = 0, 0, 0\n+                    while idy < llen and (fc or l[1][idy] != ','):\n+                        if l[1][idy] == \"'\":\n                             fc = not fc\n-                        i = i + 1\n-                    i = i + 1\n-                    if v not in vars:\n-                         vars[v] = {}\n-                    if '=' in vars[v] and not vars[v]['='] == l[1][j:i - 1]:\n-                        outmess('analyzeline: changing init expression of \"%s\" (\"%s\") to \"%s\"\\n' % (\n-                            v, vars[v]['='], l[1][j:i - 1]))\n-                    vars[v]['='] = l[1][j:i - 1]\n-                    j = i\n+                        idy += 1\n+                    idy += 1\n+                    vars.setdefault(v, {})\n+                    if '=' in vars[v] and vars[v]['='] != l[1][jdx:idy - 1]:\n+                        outmess('analyzeline: changing init expression of \"%s\" (\"%s\") to \"%s\"\\n' % (v, vars[v]['='], l[1][jdx:idy - 1]))\n+                    vars[v]['='] = l[1][jdx:idy - 1]\n+                    jdx = idy\n                 last_name = v\n         groupcache[groupcounter]['vars'] = vars\n         if last_name is not None:\n",
            "comment_added_diff": {},
            "comment_deleted_diff": {
                "1469": "                        # Need to assign multiple values to one variable",
                "1480": "                            # XXX: subsequent init expressions may get wrong values.",
                "1481": "                            # Ignoring since data statements are irrelevant for",
                "1482": "                            # wrapping."
            },
            "comment_modified_diff": {}
        },
        {
            "commit": "f54c3879bd73e84bd73c98225a08491e62786fce",
            "timestamp": "2023-09-22T16:09:27+00:00",
            "author": "Rohit Goswami",
            "commit_message": "MAINT: Minor cleanup",
            "additions": 21,
            "deletions": 24,
            "change_type": "MODIFY",
            "diff": "@@ -1428,48 +1428,45 @@ def analyzeline(m, case, line):\n         vars = groupcache[groupcounter].get('vars', {})\n         last_name = None\n         for l in ll:\n-            l = [x.strip() for x in l]\n-            if l[0][0] == ',':\n+            l[0], l[1] = l[0].strip(), l[1].strip()\n+            if l[0].startswith(','):\n                 l[0] = l[0][1:]\n-            if l[0][0] == '(':\n-                outmess(\n-                    'analyzeline: implied-DO list \"%s\" is not supported. Skipping.\\n' % l[0])\n+            if l[0].startswith('('):\n+                outmess('analyzeline: implied-DO list \"%s\" is not supported. Skipping.\\n' % l[0])\n                 continue\n-            llen = len(l[1])\n-            for idx, v in enumerate(rmbadname(\n-                    [x.strip() for x in markoutercomma(l[0]).split('@,@')])\n-                                    ):\n-                if v[0] == '(':\n-                    outmess(\n-                        'analyzeline: implied-DO list \"%s\" is not supported. Skipping.\\n' % v)\n+            for idx, v in enumerate(rmbadname([x.strip() for x in markoutercomma(l[0]).split('@,@')])):\n+                if v.startswith('('):\n+                    outmess('analyzeline: implied-DO list \"%s\" is not supported. Skipping.\\n' % v)\n                     # XXX: subsequent init expressions may get wrong values.\n                     # Ignoring since data statements are irrelevant for\n                     # wrapping.\n                     continue\n-                fc = 0\n                 try:\n-                    vtype = vars[v].get('typespec') # can fail\n+                    vtype = vars[v].get('typespec')\n                     vdim = getdimension(vars[v])\n                     matches = re.findall(r\"\\(.*?\\)\", l[1]) if vtype == 'complex' else l[1].split(',')\n                     vars.setdefault(v, {})\n-                    if '=' in vars[v] and vars[v]['='] != matches[idx]:\n-                        outmess('analyzeline: changing init expression of \"%s\" (\"%s\") to \"%s\"\\n' % (v, vars[v]['='], matches[idx]))\n-                    vars[v]['='] = \"(/{}/)\".format(\", \".join(matches)) if vdim is not None else matches[idx]\n+                    current_val = vars[v].get('=')\n+                    new_val = \"(/{}/)\".format(\", \".join(matches)) if vdim else matches[idx]\n+                    if current_val and current_val != new_val:\n+                        outmess('analyzeline: changing init expression of \"%s\" (\"%s\") to \"%s\"\\n' % (v, current_val, new_val))\n+                    vars[v]['='] = new_val\n                 except:\n                     idy, jdx, fc = 0, 0, 0\n-                    while idy < llen and (fc or l[1][idy] != ','):\n+                    while idy < len(l[1]) and (fc or l[1][idy] != ','):\n                         if l[1][idy] == \"'\":\n                             fc = not fc\n                         idy += 1\n-                    idy += 1\n                     vars.setdefault(v, {})\n-                    if '=' in vars[v] and vars[v]['='] != l[1][jdx:idy - 1]:\n-                        outmess('analyzeline: changing init expression of \"%s\" (\"%s\") to \"%s\"\\n' % (v, vars[v]['='], l[1][jdx:idy - 1]))\n-                    vars[v]['='] = l[1][jdx:idy - 1]\n-                    jdx = idy\n+                    current_val = vars[v].get('=')\n+                    new_val = l[1][jdx:idy]\n+                    if current_val and current_val != new_val:\n+                        outmess('analyzeline: changing init expression of \"%s\" (\"%s\") to \"%s\"\\n' % (v, current_val, new_val))\n+                    vars[v]['='] = new_val\n+                    jdx = idy + 1\n                 last_name = v\n         groupcache[groupcounter]['vars'] = vars\n-        if last_name is not None:\n+        if last_name:\n             previous_context = ('variable', last_name, groupcounter)\n     elif case == 'common':\n         line = m.group('after').strip()\n",
            "comment_added_diff": {},
            "comment_deleted_diff": {
                "1451": "                    vtype = vars[v].get('typespec') # can fail"
            },
            "comment_modified_diff": {}
        },
        {
            "commit": "c3b2845528103ae333e47761b513a4942a3c2041",
            "timestamp": "2023-09-22T16:17:36+00:00",
            "author": "Rohit Goswami",
            "commit_message": "MAINT: Refactor changes to be clearer",
            "additions": 6,
            "deletions": 11,
            "change_type": "MODIFY",
            "diff": "@@ -1445,25 +1445,20 @@ def analyzeline(m, case, line):\n                     vtype = vars[v].get('typespec')\n                     vdim = getdimension(vars[v])\n                     matches = re.findall(r\"\\(.*?\\)\", l[1]) if vtype == 'complex' else l[1].split(',')\n-                    vars.setdefault(v, {})\n-                    current_val = vars[v].get('=')\n                     new_val = \"(/{}/)\".format(\", \".join(matches)) if vdim else matches[idx]\n-                    if current_val and current_val != new_val:\n-                        outmess('analyzeline: changing init expression of \"%s\" (\"%s\") to \"%s\"\\n' % (v, current_val, new_val))\n-                    vars[v]['='] = new_val\n-                except:\n+                except: # gh-24746\n                     idy, jdx, fc = 0, 0, 0\n                     while idy < len(l[1]) and (fc or l[1][idy] != ','):\n                         if l[1][idy] == \"'\":\n                             fc = not fc\n                         idy += 1\n-                    vars.setdefault(v, {})\n-                    current_val = vars[v].get('=')\n                     new_val = l[1][jdx:idy]\n-                    if current_val and current_val != new_val:\n-                        outmess('analyzeline: changing init expression of \"%s\" (\"%s\") to \"%s\"\\n' % (v, current_val, new_val))\n-                    vars[v]['='] = new_val\n                     jdx = idy + 1\n+                vars.setdefault(v, {})\n+                current_val = vars[v].get('=')\n+                if current_val and current_val != new_val:\n+                    outmess('analyzeline: changing init expression of \"%s\" (\"%s\") to \"%s\"\\n' % (v, current_val, new_val))\n+                vars[v]['='] = new_val\n                 last_name = v\n         groupcache[groupcounter]['vars'] = vars\n         if last_name:\n",
            "comment_added_diff": {
                "1449": "                except: # gh-24746"
            },
            "comment_deleted_diff": {},
            "comment_modified_diff": {
                "1449": "                    current_val = vars[v].get('=')"
            }
        },
        {
            "commit": "fbbd6374cd7454c5d21f3a30b3848fdf428061ca",
            "timestamp": "2023-09-24T13:30:44+00:00",
            "author": "Rohit Goswami",
            "commit_message": "BUG: Fix handling of data statements",
            "additions": 9,
            "deletions": 0,
            "change_type": "MODIFY",
            "diff": "@@ -1441,6 +1441,15 @@ def analyzeline(m, case, line):\n                     # Ignoring since data statements are irrelevant for\n                     # wrapping.\n                     continue\n+                if '!' in l[1]:\n+                    # Fixes gh-24746 pyf generation\n+                    # XXX: This essentially ignores the value for generating the pyf which is fine:\n+                    # integer dimension(3) :: mytab\n+                    # common /mycom/ mytab\n+                    # Since in any case it is initialized in the Fortran code\n+                    outmess('Comment line in declaration \"%s\" is not supported. Skipping.\\n' % l[1])\n+                    new_val = 0\n+                    continue\n                 try:\n                     vtype = vars[v].get('typespec')\n                     vdim = getdimension(vars[v])\n",
            "comment_added_diff": {
                "1445": "                    # Fixes gh-24746 pyf generation",
                "1446": "                    # XXX: This essentially ignores the value for generating the pyf which is fine:",
                "1447": "                    # integer dimension(3) :: mytab",
                "1448": "                    # common /mycom/ mytab",
                "1449": "                    # Since in any case it is initialized in the Fortran code"
            },
            "comment_deleted_diff": {},
            "comment_modified_diff": {}
        },
        {
            "commit": "67127e4f2dc7d4d97cb7de072ea9f99447e40d6d",
            "timestamp": "2023-09-26T14:24:45+00:00",
            "author": "Rohit Goswami",
            "commit_message": "MAINT: Cleanup as per review\n\nCo-authored-by: jncots <jncots@users.noreply.github.com>",
            "additions": 5,
            "deletions": 15,
            "change_type": "MODIFY",
            "diff": "@@ -1448,24 +1448,14 @@ def analyzeline(m, case, line):\n                     # common /mycom/ mytab\n                     # Since in any case it is initialized in the Fortran code\n                     outmess('Comment line in declaration \"%s\" is not supported. Skipping.\\n' % l[1])\n-                    new_val = 0\n                     continue\n-                try:\n-                    vtype = vars[v].get('typespec')\n-                    vdim = getdimension(vars[v])\n-                    matches = re.findall(r\"\\(.*?\\)\", l[1]) if vtype == 'complex' else l[1].split(',')\n-                    new_val = \"(/{}/)\".format(\", \".join(matches)) if vdim else matches[idx]\n-                except: # gh-24746\n-                    idy, jdx, fc = 0, 0, 0\n-                    while idy < len(l[1]) and (fc or l[1][idy] != ','):\n-                        if l[1][idy] == \"'\":\n-                            fc = not fc\n-                        idy += 1\n-                    new_val = l[1][jdx:idy]\n-                    jdx = idy + 1\n                 vars.setdefault(v, {})\n+                vtype = vars[v].get('typespec')\n+                vdim = getdimension(vars[v])\n+                matches = re.findall(r\"\\(.*?\\)\", l[1]) if vtype == 'complex' else l[1].split(',')\n+                new_val = \"(/{}/)\".format(\", \".join(matches)) if vdim else matches[idx]\n                 current_val = vars[v].get('=')\n-                if current_val and current_val != new_val:\n+                if current_val and (current_val != new_val):\n                     outmess('analyzeline: changing init expression of \"%s\" (\"%s\") to \"%s\"\\n' % (v, current_val, new_val))\n                 vars[v]['='] = new_val\n                 last_name = v\n",
            "comment_added_diff": {},
            "comment_deleted_diff": {
                "1458": "                except: # gh-24746"
            },
            "comment_modified_diff": {}
        },
        {
            "commit": "7cc52e30dfc518d2f7bb2cd8e6448713c390ecdb",
            "timestamp": "2023-09-27T11:13:58+00:00",
            "author": "Rohit Goswami",
            "commit_message": "ENH: Fix DATA with * statements\n\nCo-authored-by: jncots <jncots@users.noreply.github.com>",
            "additions": 20,
            "deletions": 1,
            "change_type": "MODIFY",
            "diff": "@@ -1453,7 +1453,26 @@ def analyzeline(m, case, line):\n                 vtype = vars[v].get('typespec')\n                 vdim = getdimension(vars[v])\n                 matches = re.findall(r\"\\(.*?\\)\", l[1]) if vtype == 'complex' else l[1].split(',')\n-                new_val = \"(/{}/)\".format(\", \".join(matches)) if vdim else matches[idx]\n+                try:\n+                    new_val = \"(/{}/)\".format(\", \".join(matches)) if vdim else matches[idx]\n+                except IndexError:\n+                    # gh-24746\n+                    # Runs only if above code fails. Fixes the line\n+                    # DATA IVAR1, IVAR2, IVAR3, IVAR4, EVAR5 /4*0,0.0D0/\n+                    # by exanding to ['0', '0', '0', '0', '0.0d0']\n+                    if any(\"*\" in m for m in matches):\n+                        expanded_list = []\n+                        for match in matches:\n+                            if \"*\" in match:\n+                                try:\n+                                    multiplier, value = match.split(\"*\")\n+                                    expanded_list.extend([value.strip()] * int(multiplier))\n+                                except ValueError: # if int(multiplier) fails\n+                                    expanded_list.append(match.strip())\n+                            else:\n+                                expanded_list.append(match.strip())\n+                        matches = expanded_list\n+                    new_val = \"(/{}/)\".format(\", \".join(matches)) if vdim else matches[idx]\n                 current_val = vars[v].get('=')\n                 if current_val and (current_val != new_val):\n                     outmess('analyzeline: changing init expression of \"%s\" (\"%s\") to \"%s\"\\n' % (v, current_val, new_val))\n",
            "comment_added_diff": {
                "1459": "                    # gh-24746",
                "1460": "                    # Runs only if above code fails. Fixes the line",
                "1461": "                    # DATA IVAR1, IVAR2, IVAR3, IVAR4, EVAR5 /4*0,0.0D0/",
                "1462": "                    # by exanding to ['0', '0', '0', '0', '0.0d0']",
                "1470": "                                except ValueError: # if int(multiplier) fails"
            },
            "comment_deleted_diff": {},
            "comment_modified_diff": {}
        },
        {
            "commit": "1e6a322a9514c0050f4cc656aead1aeffba0b1b5",
            "timestamp": "2023-09-27T13:58:43+00:00",
            "author": "Rohit Goswami",
            "commit_message": "TST: Add check for multipliers in data [f2py]\n\nCo-authored-by: jncots <jncots@users.noreply.github.com>",
            "additions": 1,
            "deletions": 1,
            "change_type": "MODIFY",
            "diff": "@@ -1459,7 +1459,7 @@ def analyzeline(m, case, line):\n                     # gh-24746\n                     # Runs only if above code fails. Fixes the line\n                     # DATA IVAR1, IVAR2, IVAR3, IVAR4, EVAR5 /4*0,0.0D0/\n-                    # by exanding to ['0', '0', '0', '0', '0.0d0']\n+                    # by expanding to ['0', '0', '0', '0', '0.0d0']\n                     if any(\"*\" in m for m in matches):\n                         expanded_list = []\n                         for match in matches:\n",
            "comment_added_diff": {
                "1462": "                    # by expanding to ['0', '0', '0', '0', '0.0d0']"
            },
            "comment_deleted_diff": {
                "1462": "                    # by exanding to ['0', '0', '0', '0', '0.0d0']"
            },
            "comment_modified_diff": {
                "1462": "                    # by exanding to ['0', '0', '0', '0', '0.0d0']"
            }
        }
    ],
    "unicode_comment.f90": [],
    "test_crackfortran.py": [
        {
            "commit": "fe73a8498417d762a2102d759fa4c88613b398ef",
            "timestamp": "2022-12-25T16:20:55-07:00",
            "author": "Rohit Goswami",
            "commit_message": "BUG: Use whole file for encoding checks with `charset_normalizer` [f2py] (#22872)\n\n* BUG: Use whole file for encoding checks [f2py]\r\n\r\n* DOC: Add a code comment\r\n\r\nCo-authored-by: melissawm <melissawm@gmail.com>\r\n\r\n* TST: Add a conditional unicode f2py test\r\n\r\n* MAINT: Add chardet as a test requirement\r\n\r\n* ENH: Cleanup and switch f2py to charset_normalizer\r\n\r\n* MAINT: Remove chardet for charset_normalizer\r\n\r\n* TST: Simplify UTF-8 encoding [f2py]\r\n\r\nCo-authored-by: melissawm <melissawm@gmail.com>",
            "additions": 13,
            "deletions": 4,
            "change_type": "MODIFY",
            "diff": "@@ -1,4 +1,6 @@\n+import importlib\n import codecs\n+import unicodedata\n import pytest\n import numpy as np\n from numpy.f2py.crackfortran import markinnerspaces\n@@ -257,13 +259,20 @@ class TestFortranReader(util.F2PyTest):\n     def test_input_encoding(self, tmp_path, encoding):\n         # gh-635\n         f_path = tmp_path / f\"input_with_{encoding}_encoding.f90\"\n-        # explicit BOM is required for UTF8\n-        bom = {'utf-8': codecs.BOM_UTF8}.get(encoding, b'')\n         with f_path.open('w', encoding=encoding) as ff:\n-            ff.write(bom.decode(encoding) +\n-                     \"\"\"\n+            ff.write(\"\"\"\n                      subroutine foo()\n                      end subroutine foo\n                      \"\"\")\n         mod = crackfortran.crackfortran([str(f_path)])\n         assert mod[0]['name'] == 'foo'\n+\n+class TestUnicodeComment(util.F2PyTest):\n+    sources = [util.getpath(\"tests\", \"src\", \"crackfortran\", \"unicode_comment.f90\")]\n+\n+    @pytest.mark.skipif(\n+        (importlib.util.find_spec(\"charset_normalizer\") is None),\n+        reason=\"test requires charset_normalizer which is not installed\",\n+    )\n+    def test_encoding_comment(self):\n+        self.module.foo(3)\n",
            "comment_added_diff": {},
            "comment_deleted_diff": {
                "260": "        # explicit BOM is required for UTF8"
            },
            "comment_modified_diff": {}
        },
        {
            "commit": "39c380ccf12a15cf560260dac461649c26bd0e5e",
            "timestamp": "2023-03-26T13:09:14-07:00",
            "author": "molsonkiko",
            "commit_message": "initial fix for nameargspattern regex",
            "additions": 9,
            "deletions": 2,
            "change_type": "MODIFY",
            "diff": "@@ -279,7 +279,9 @@ def test_encoding_comment(self):\n         self.module.foo(3)\n \n class TestNameArgsPatternBacktracking:\n-    def test_nameargspattern_backtracking():\n+    def test_nameargspattern_backtracking(self):\n+        '''address ReDOS vulnerability:\n+        https://github.com/numpy/numpy/issues/23338'''\n         last_time = 0.\n         trials_per_count = 32\n         start_reps, end_reps = 10, 16\n@@ -291,5 +293,10 @@ def test_nameargspattern_backtracking():\n                 crackfortran.nameargspattern.search(atbindat)\n                 total_time += (time.perf_counter() - t0)\n             if ii > start_reps:\n-                assert total_time < 1.9 * last_time, f'Going from {ii - 1} to {ii} approximately doubled time'\n+                # the hallmark of exponentially catastrophic backtracking\n+                # is that runtime doubles for every added instance of\n+                # the problematic pattern.\n+                assert total_time < 1.9 * last_time\n+                # also try to rule out non-exponential but still bad cases\n+                assert total_time < 1\n             last_time = total_time\n\\ No newline at end of file\n",
            "comment_added_diff": {
                "296": "                # the hallmark of exponentially catastrophic backtracking",
                "297": "                # is that runtime doubles for every added instance of",
                "298": "                # the problematic pattern.",
                "300": "                # also try to rule out non-exponential but still bad cases"
            },
            "comment_deleted_diff": {},
            "comment_modified_diff": {}
        },
        {
            "commit": "fd0f29de9365dcceec26d6a2a7539f61b0b037cf",
            "timestamp": "2023-03-26T14:15:36-07:00",
            "author": "molsonkiko",
            "commit_message": "update test for less arbitrary time requirement",
            "additions": 2,
            "deletions": 1,
            "change_type": "MODIFY",
            "diff": "@@ -298,5 +298,6 @@ def test_nameargspattern_backtracking(self):\n                 # the problematic pattern.\n                 assert total_time < 1.9 * last_time\n                 # also try to rule out non-exponential but still bad cases\n-                assert total_time < 1\n+                # arbitrarily, we should set a hard limit of 10ms as too slow\n+                assert total_time < trials_per_count * 0.01\n             last_time = total_time\n\\ No newline at end of file\n",
            "comment_added_diff": {
                "301": "                # arbitrarily, we should set a hard limit of 10ms as too slow"
            },
            "comment_deleted_diff": {},
            "comment_modified_diff": {
                "301": "                assert total_time < 1"
            }
        },
        {
            "commit": "09c23ef73c839d3a7f31e755f40f2f06b9791b7f",
            "timestamp": "2023-03-26T17:45:01-07:00",
            "author": "molsonkiko",
            "commit_message": "make regex still match cases where OG fix failed\nMy first replacement regex would have failed to match\ncases like '@)@bind foo bar baz@(@@)@' which should\napparently be matched.\nAdded a test to make sure the regex does this.",
            "additions": 19,
            "deletions": 6,
            "change_type": "MODIFY",
            "diff": "@@ -4,7 +4,7 @@\n import unicodedata\n import pytest\n import numpy as np\n-from numpy.f2py.crackfortran import markinnerspaces\n+from numpy.f2py.crackfortran import markinnerspaces, nameargspattern\n from . import util\n from numpy.f2py import crackfortran\n import textwrap\n@@ -279,19 +279,32 @@ def test_encoding_comment(self):\n         self.module.foo(3)\n \n class TestNameArgsPatternBacktracking:\n-    def test_nameargspattern_backtracking(self):\n+    @pytest.mark.parametrize(\n+        ['adversary'],\n+        [\n+            ('@)@bind@(@',),\n+            ('@)@bind                         @(@',),\n+            ('@)@bind foo bar baz@(@',)\n+        ]\n+    )\n+    def test_nameargspattern_backtracking(self, adversary):\n         '''address ReDOS vulnerability:\n         https://github.com/numpy/numpy/issues/23338'''\n         last_time = 0.\n-        trials_per_count = 32\n-        start_reps, end_reps = 10, 16\n+        trials_per_count = 128\n+        start_reps, end_reps = 15, 25\n         for ii in range(start_reps, end_reps):\n-            atbindat = '@)@bind@(@' * ii\n+            repeated_adversary = adversary * ii\n             total_time = 0\n             for _ in range(trials_per_count):\n                 t0 = time.perf_counter()\n-                crackfortran.nameargspattern.search(atbindat)\n+                mtch = nameargspattern.search(repeated_adversary)\n                 total_time += (time.perf_counter() - t0)\n+            assert not mtch\n+            # if the adversary is capped with @)@, it becomes acceptable.\n+            # that should still be true.\n+            good_version_of_adversary = repeated_adversary + '@)@'\n+            assert nameargspattern.search(good_version_of_adversary)\n             if ii > start_reps:\n                 # the hallmark of exponentially catastrophic backtracking\n                 # is that runtime doubles for every added instance of\n",
            "comment_added_diff": {
                "304": "            # if the adversary is capped with @)@, it becomes acceptable.",
                "305": "            # that should still be true."
            },
            "comment_deleted_diff": {},
            "comment_modified_diff": {}
        },
        {
            "commit": "988283a7a18ad15f00db28902c643f653dfd7278",
            "timestamp": "2023-03-26T18:32:40-07:00",
            "author": "molsonkiko",
            "commit_message": "make time tests more resilient to random noise",
            "additions": 16,
            "deletions": 7,
            "change_type": "MODIFY",
            "diff": "@@ -290,18 +290,23 @@ class TestNameArgsPatternBacktracking:\n     def test_nameargspattern_backtracking(self, adversary):\n         '''address ReDOS vulnerability:\n         https://github.com/numpy/numpy/issues/23338'''\n-        last_time = 0.\n+        last_median = 0.\n         trials_per_count = 128\n         start_reps, end_reps = 15, 25\n+        times_median_doubled = 0\n         for ii in range(start_reps, end_reps):\n             repeated_adversary = adversary * ii\n-            total_time = 0\n+            times = []\n             for _ in range(trials_per_count):\n                 t0 = time.perf_counter()\n                 mtch = nameargspattern.search(repeated_adversary)\n-                total_time += (time.perf_counter() - t0)\n+                times.append(time.perf_counter() - t0)\n+            # We should use a measure of time that's resilient to outliers.\n+            # Times jump around a lot due to the CPU's scheduler.\n+            median = np.median(times)\n             assert not mtch\n-            # if the adversary is capped with @)@, it becomes acceptable.\n+            # if the adversary is capped with @)@, it becomes acceptable\n+            # according to the old version of the regex.\n             # that should still be true.\n             good_version_of_adversary = repeated_adversary + '@)@'\n             assert nameargspattern.search(good_version_of_adversary)\n@@ -309,8 +314,12 @@ def test_nameargspattern_backtracking(self, adversary):\n                 # the hallmark of exponentially catastrophic backtracking\n                 # is that runtime doubles for every added instance of\n                 # the problematic pattern.\n-                assert total_time < 1.9 * last_time\n+                times_median_doubled += median > 2 * last_median\n                 # also try to rule out non-exponential but still bad cases\n                 # arbitrarily, we should set a hard limit of 10ms as too slow\n-                assert total_time < trials_per_count * 0.01\n-            last_time = total_time\n\\ No newline at end of file\n+                assert median < trials_per_count * 0.01\n+            last_median = median\n+        # we accept that maybe the median might double once, due to\n+        # the CPU scheduler acting weird or whatever. More than that\n+        # seems suspicious.\n+        assert times_median_doubled < 2\n\\ No newline at end of file\n",
            "comment_added_diff": {
                "304": "            # We should use a measure of time that's resilient to outliers.",
                "305": "            # Times jump around a lot due to the CPU's scheduler.",
                "308": "            # if the adversary is capped with @)@, it becomes acceptable",
                "309": "            # according to the old version of the regex.",
                "322": "        # we accept that maybe the median might double once, due to",
                "323": "        # the CPU scheduler acting weird or whatever. More than that",
                "324": "        # seems suspicious."
            },
            "comment_deleted_diff": {
                "304": "            # if the adversary is capped with @)@, it becomes acceptable."
            },
            "comment_modified_diff": {
                "304": "            # if the adversary is capped with @)@, it becomes acceptable."
            }
        },
        {
            "commit": "f96b8daea959db191ff84b80dbc1ea948722fbaa",
            "timestamp": "2023-04-16T18:18:02+00:00",
            "author": "Rohit Goswami",
            "commit_message": "TST: Add a test for gh-23598",
            "additions": 9,
            "deletions": 1,
            "change_type": "MODIFY",
            "diff": "@@ -322,4 +322,12 @@ def test_nameargspattern_backtracking(self, adversary):\n         # we accept that maybe the median might double once, due to\n         # the CPU scheduler acting weird or whatever. More than that\n         # seems suspicious.\n-        assert times_median_doubled < 2\n\\ No newline at end of file\n+        assert times_median_doubled < 2\n+\n+\n+class TestFunctionReturn(util.F2PyTest):\n+    sources = [util.getpath(\"tests\", \"src\", \"crackfortran\", \"gh23598.f90\")]\n+\n+    def test_function_rettype(self):\n+        # gh-23598\n+        assert self.module.intproduct(3, 4) == 12\n",
            "comment_added_diff": {
                "332": "        # gh-23598"
            },
            "comment_deleted_diff": {},
            "comment_modified_diff": {}
        },
        {
            "commit": "46cc9556224ecba0fddd5f78be74289a5ffd64b4",
            "timestamp": "2023-04-25T20:46:56+02:00",
            "author": "molsonkiko",
            "commit_message": "TST: Remove crackfortran.nameargspattern time test that failed randomly (#23662)\n\nalso made the threshold for rejecting a regex as too slow\r\nmuch more lenient.\r\n200ms should be enough time even for a bad CPU on a bad day.\r\na bad regex should fail with near certainty",
            "additions": 15,
            "deletions": 25,
            "change_type": "MODIFY",
            "diff": "@@ -290,36 +290,26 @@ class TestNameArgsPatternBacktracking:\n     def test_nameargspattern_backtracking(self, adversary):\n         '''address ReDOS vulnerability:\n         https://github.com/numpy/numpy/issues/23338'''\n-        last_median = 0.\n-        trials_per_count = 128\n+        trials_per_batch = 12\n+        batches_per_regex = 4\n         start_reps, end_reps = 15, 25\n-        times_median_doubled = 0\n         for ii in range(start_reps, end_reps):\n             repeated_adversary = adversary * ii\n-            times = []\n-            for _ in range(trials_per_count):\n-                t0 = time.perf_counter()\n-                mtch = nameargspattern.search(repeated_adversary)\n-                times.append(time.perf_counter() - t0)\n-            # We should use a measure of time that's resilient to outliers.\n-            # Times jump around a lot due to the CPU's scheduler.\n-            median = np.median(times)\n+            # test times in small batches.\n+            # this gives us more chances to catch a bad regex\n+            # while still catching it before too long if it is bad\n+            for _ in range(batches_per_regex):\n+                times = []\n+                for _ in range(trials_per_batch):\n+                    t0 = time.perf_counter()\n+                    mtch = nameargspattern.search(repeated_adversary)\n+                    times.append(time.perf_counter() - t0)\n+                # our pattern should be much faster than 0.2s per search\n+                # it's unlikely that a bad regex will pass even on fast CPUs\n+                assert np.median(times) < 0.2\n             assert not mtch\n             # if the adversary is capped with @)@, it becomes acceptable\n             # according to the old version of the regex.\n             # that should still be true.\n             good_version_of_adversary = repeated_adversary + '@)@'\n-            assert nameargspattern.search(good_version_of_adversary)\n-            if ii > start_reps:\n-                # the hallmark of exponentially catastrophic backtracking\n-                # is that runtime doubles for every added instance of\n-                # the problematic pattern.\n-                times_median_doubled += median > 2 * last_median\n-                # also try to rule out non-exponential but still bad cases\n-                # arbitrarily, we should set a hard limit of 10ms as too slow\n-                assert median < trials_per_count * 0.01\n-            last_median = median\n-        # we accept that maybe the median might double once, due to\n-        # the CPU scheduler acting weird or whatever. More than that\n-        # seems suspicious.\n-        assert times_median_doubled < 2\n\\ No newline at end of file\n+            assert nameargspattern.search(good_version_of_adversary)\n\\ No newline at end of file\n",
            "comment_added_diff": {
                "298": "            # test times in small batches.",
                "299": "            # this gives us more chances to catch a bad regex",
                "300": "            # while still catching it before too long if it is bad",
                "307": "                # our pattern should be much faster than 0.2s per search",
                "308": "                # it's unlikely that a bad regex will pass even on fast CPUs"
            },
            "comment_deleted_diff": {
                "304": "            # We should use a measure of time that's resilient to outliers.",
                "305": "            # Times jump around a lot due to the CPU's scheduler.",
                "314": "                # the hallmark of exponentially catastrophic backtracking",
                "315": "                # is that runtime doubles for every added instance of",
                "316": "                # the problematic pattern.",
                "318": "                # also try to rule out non-exponential but still bad cases",
                "319": "                # arbitrarily, we should set a hard limit of 10ms as too slow",
                "322": "        # we accept that maybe the median might double once, due to",
                "323": "        # the CPU scheduler acting weird or whatever. More than that",
                "324": "        # seems suspicious."
            },
            "comment_modified_diff": {
                "299": "            times = []",
                "300": "            for _ in range(trials_per_count):"
            }
        },
        {
            "commit": "aa5b9d6665709d5ca8a098c2e4f9ce2f5c8a25b7",
            "timestamp": "2023-05-12T17:41:22+00:00",
            "author": "Rohit Goswami",
            "commit_message": "TST: Add a test for gh-23533",
            "additions": 27,
            "deletions": 0,
            "change_type": "MODIFY",
            "diff": "@@ -135,6 +135,7 @@ def test_multiple_relevant_spaces(self):\n         assert markinnerspaces(\"a 'b c' 'd e'\") == \"a 'b@_@c' 'd@_@e'\"\n         assert markinnerspaces(r'a \"b c\" \"d e\"') == r'a \"b@_@c\" \"d@_@e\"'\n \n+\n class TestDimSpec(util.F2PyTest):\n     \"\"\"This test suite tests various expressions that are used as dimension\n     specifications.\n@@ -244,6 +245,7 @@ def test_dependencies(self, tmp_path):\n         assert len(mod) == 1\n         assert mod[0][\"vars\"][\"abar\"][\"=\"] == \"bar('abar')\"\n \n+\n class TestEval(util.F2PyTest):\n     def test_eval_scalar(self):\n         eval_scalar = crackfortran._eval_scalar\n@@ -268,6 +270,7 @@ def test_input_encoding(self, tmp_path, encoding):\n         mod = crackfortran.crackfortran([str(f_path)])\n         assert mod[0]['name'] == 'foo'\n \n+\n class TestUnicodeComment(util.F2PyTest):\n     sources = [util.getpath(\"tests\", \"src\", \"crackfortran\", \"unicode_comment.f90\")]\n \n@@ -278,6 +281,7 @@ class TestUnicodeComment(util.F2PyTest):\n     def test_encoding_comment(self):\n         self.module.foo(3)\n \n+\n class TestNameArgsPatternBacktracking:\n     @pytest.mark.parametrize(\n         ['adversary'],\n@@ -313,6 +317,19 @@ def test_nameargspattern_backtracking(self, adversary):\n             # that should still be true.\n             good_version_of_adversary = repeated_adversary + '@)@'\n             assert nameargspattern.search(good_version_of_adversary)\n+            if ii > start_reps:\n+                # the hallmark of exponentially catastrophic backtracking\n+                # is that runtime doubles for every added instance of\n+                # the problematic pattern.\n+                times_median_doubled += median > 2 * last_median\n+                # also try to rule out non-exponential but still bad cases\n+                # arbitrarily, we should set a hard limit of 10ms as too slow\n+                assert median < trials_per_count * 0.01\n+            last_median = median\n+        # we accept that maybe the median might double once, due to\n+        # the CPU scheduler acting weird or whatever. More than that\n+        # seems suspicious.\n+        assert times_median_doubled < 2\n \n \n class TestFunctionReturn(util.F2PyTest):\n@@ -321,3 +338,13 @@ class TestFunctionReturn(util.F2PyTest):\n     def test_function_rettype(self):\n         # gh-23598\n         assert self.module.intproduct(3, 4) == 12\n+\n+\n+class TestFortranGroupCounters(util.F2PyTest):\n+    def test_end_if_comment(self):\n+        # gh-23533\n+        fpath = util.getpath(\"tests\", \"src\", \"crackfortran\", \"gh23533.f\")\n+        try:\n+            crackfortran.crackfortran([str(fpath)])\n+        except Exception as exc:\n+            assert False, f\"'crackfortran.crackfortran' raised an exception {exc}\"\n",
            "comment_added_diff": {
                "321": "                # the hallmark of exponentially catastrophic backtracking",
                "322": "                # is that runtime doubles for every added instance of",
                "323": "                # the problematic pattern.",
                "325": "                # also try to rule out non-exponential but still bad cases",
                "326": "                # arbitrarily, we should set a hard limit of 10ms as too slow",
                "329": "        # we accept that maybe the median might double once, due to",
                "330": "        # the CPU scheduler acting weird or whatever. More than that",
                "331": "        # seems suspicious.",
                "345": "        # gh-23533"
            },
            "comment_deleted_diff": {},
            "comment_modified_diff": {}
        },
        {
            "commit": "60f28c52e508892482941acbe7809f015511baf0",
            "timestamp": "2023-05-12T17:54:05+00:00",
            "author": "Rohit Goswami",
            "commit_message": "MAINT: Fix merge error",
            "additions": 0,
            "deletions": 13,
            "change_type": "MODIFY",
            "diff": "@@ -317,19 +317,6 @@ def test_nameargspattern_backtracking(self, adversary):\n             # that should still be true.\n             good_version_of_adversary = repeated_adversary + '@)@'\n             assert nameargspattern.search(good_version_of_adversary)\n-            if ii > start_reps:\n-                # the hallmark of exponentially catastrophic backtracking\n-                # is that runtime doubles for every added instance of\n-                # the problematic pattern.\n-                times_median_doubled += median > 2 * last_median\n-                # also try to rule out non-exponential but still bad cases\n-                # arbitrarily, we should set a hard limit of 10ms as too slow\n-                assert median < trials_per_count * 0.01\n-            last_median = median\n-        # we accept that maybe the median might double once, due to\n-        # the CPU scheduler acting weird or whatever. More than that\n-        # seems suspicious.\n-        assert times_median_doubled < 2\n \n \n class TestFunctionReturn(util.F2PyTest):\n",
            "comment_added_diff": {},
            "comment_deleted_diff": {
                "321": "                # the hallmark of exponentially catastrophic backtracking",
                "322": "                # is that runtime doubles for every added instance of",
                "323": "                # the problematic pattern.",
                "325": "                # also try to rule out non-exponential but still bad cases",
                "326": "                # arbitrarily, we should set a hard limit of 10ms as too slow",
                "329": "        # we accept that maybe the median might double once, due to",
                "330": "        # the CPU scheduler acting weird or whatever. More than that",
                "331": "        # seems suspicious."
            },
            "comment_modified_diff": {}
        }
    ],
    "test_requirements.txt": [],
    "bench_ufunc.py": [
        {
            "commit": "3a1ffd8ff595f448caaba931d4d650a4b0184c5d",
            "timestamp": "2023-03-04T15:41:53+00:00",
            "author": "Rohit Goswami",
            "commit_message": "BENCH: Init benchmarks for 1 arg ndarray methods",
            "additions": 45,
            "deletions": 6,
            "change_type": "MODIFY",
            "diff": "@@ -1,4 +1,4 @@\n-from .common import Benchmark, get_squares_\n+from .common import Benchmark, get_squares_, TYPES1\n \n import numpy as np\n \n@@ -19,6 +19,31 @@\n           'rint', 'round', 'sign', 'signbit', 'sin', 'sinh', 'spacing', 'sqrt',\n           'square', 'subtract', 'tan', 'tanh', 'true_divide', 'trunc']\n \n+meth_0arg = ['__abs__', '__dlpack__', '__dlpack_device__',\n+             '__neg__', '__pos__']\n+\n+# Array arguments\n+meth_1arg_arr = ['__add__', '__eq__', '__ge__', '__gt__',\n+                 '__matmul__', '__le__', '__lt__', '__mul__',\n+                 '__ne__', '__pow__', '__sub__', '__truediv__']\n+\n+# Index arguments\n+meth_1arg_ind = ['__getitem__']\n+\n+# Valid for 0d arrays\n+meth_0d = ['__bool__', '__complex__', '__float__']\n+\n+# Valid for size-1 arrays\n+meth_1d = ['__int__']\n+\n+# type_only\n+meth_int_scalar = ['__index__', '__lshift__', '__rshift__']\n+meth_int_bool = ['__invert__', '__xor__']\n+meth_int_only = ['__and__', '__or__']\n+meth_no_complex = ['__mod__', '__floordiv__']\n+\n+# Special\n+meth_special = ['__setitem__']\n \n for name in dir(np):\n     if isinstance(getattr(np, name, None), np.ufunc) and name not in ufuncs:\n@@ -56,20 +81,34 @@ class UFunc(Benchmark):\n     def setup(self, ufuncname):\n         np.seterr(all='ignore')\n         try:\n-            self.f = getattr(np, ufuncname)\n+            self.ufn = getattr(np, ufuncname)\n         except AttributeError:\n             raise NotImplementedError()\n         self.args = []\n-        for t, a in get_squares_().items():\n-            arg = (a,) * self.f.nin\n+        for _, aarg in get_squares_().items():\n+            arg = (aarg,) * self.ufn.nin\n             try:\n-                self.f(*arg)\n+                self.ufn(*arg)\n             except TypeError:\n                 continue\n             self.args.append(arg)\n \n     def time_ufunc_types(self, ufuncname):\n-        [self.f(*arg) for arg in self.args]\n+        [self.ufn(*arg) for arg in self.args]\n+\n+class MethodsV1(Benchmark):\n+    params = [meth_1arg_arr, TYPES1]\n+    param_names = ['methods', 'npdtypes']\n+    timeout = 10\n+\n+    def setup(self, methname, npdtypes):\n+        values = get_squares_()\n+        self.xarg_one = values.get(npdtypes)[0]\n+        self.xarg_two = values.get(npdtypes)[1]\n+\n+    def time_ndarray_meth(self, methname, npdtypes):\n+        meth = getattr(self.xarg_one, methname)\n+        meth(self.xarg_two)\n \n class UFuncSmall(Benchmark):\n     \"\"\"  Benchmark for a selection of ufuncs on a small arrays and scalars \n",
            "comment_added_diff": {
                "25": "# Array arguments",
                "30": "# Index arguments",
                "33": "# Valid for 0d arrays",
                "36": "# Valid for size-1 arrays",
                "39": "# type_only",
                "45": "# Special"
            },
            "comment_deleted_diff": {},
            "comment_modified_diff": {}
        },
        {
            "commit": "bef1e9482a6ba9b681da1279a45c196ff1a04902",
            "timestamp": "2023-03-04T15:41:54+00:00",
            "author": "Rohit Goswami",
            "commit_message": "BENCH: Refactor and add 0-argument ndarray methods",
            "additions": 41,
            "deletions": 11,
            "change_type": "MODIFY",
            "diff": "@@ -1,4 +1,4 @@\n-from .common import Benchmark, get_squares_, TYPES1\n+from .common import Benchmark, get_squares_, TYPES1, DLPACK_TYPES\n \n import numpy as np\n \n@@ -19,14 +19,6 @@\n           'rint', 'round', 'sign', 'signbit', 'sin', 'sinh', 'spacing', 'sqrt',\n           'square', 'subtract', 'tan', 'tanh', 'true_divide', 'trunc']\n \n-meth_0arg = ['__abs__', '__dlpack__', '__dlpack_device__',\n-             '__neg__', '__pos__']\n-\n-# Array arguments\n-meth_1arg_arr = ['__add__', '__eq__', '__ge__', '__gt__',\n-                 '__matmul__', '__le__', '__lt__', '__mul__',\n-                 '__ne__', '__pow__', '__sub__', '__truediv__']\n-\n # Index arguments\n meth_1arg_ind = ['__getitem__']\n \n@@ -96,8 +88,28 @@ def setup(self, ufuncname):\n     def time_ufunc_types(self, ufuncname):\n         [self.ufn(*arg) for arg in self.args]\n \n+class MethodsV0(Benchmark):\n+    \"\"\" Benchmark for the methods which do not take any arguments\n+    \"\"\"\n+    params = [['__abs__', '__neg__', '__pos__'], TYPES1]\n+    param_names = ['methods', 'npdtypes']\n+    timeout = 10\n+\n+    def setup(self, methname, npdtypes):\n+        values = get_squares_()\n+        self.xarg = values.get(npdtypes)[0]\n+\n+    def time_ndarray_meth(self, methname, npdtypes):\n+        meth = getattr(self.xarg, methname)\n+        meth()\n+\n class MethodsV1(Benchmark):\n-    params = [meth_1arg_arr, TYPES1]\n+    \"\"\" Benchmark for the methods which take an argument\n+    \"\"\"\n+    params = [['__add__', '__eq__', '__ge__', '__gt__',\n+                 '__matmul__', '__le__', '__lt__', '__mul__',\n+                 '__ne__', '__pow__', '__sub__', '__truediv__'],\n+              TYPES1]\n     param_names = ['methods', 'npdtypes']\n     timeout = 10\n \n@@ -110,8 +122,26 @@ def time_ndarray_meth(self, methname, npdtypes):\n         meth = getattr(self.xarg_one, methname)\n         meth(self.xarg_two)\n \n+class DLPMethods(Benchmark):\n+    \"\"\" Benchmark for DLPACK helpers\n+    \"\"\"\n+    params = [['__dlpack__', '__dlpack_device__'], DLPACK_TYPES]\n+    param_names = ['methods', 'npdtypes']\n+    timeout = 10\n+\n+    def setup(self, methname, npdtypes):\n+        values = get_squares_()\n+        if npdtypes == 'bool':\n+            self.xarg = values.get('int16')[0].astype('bool')\n+        else:\n+            self.xarg = values.get('int16')[0]\n+\n+    def time_ndarray_dlp(self, methname, npdtypes):\n+        meth = getattr(self.xarg, methname)\n+        meth()\n+\n class UFuncSmall(Benchmark):\n-    \"\"\"  Benchmark for a selection of ufuncs on a small arrays and scalars \n+    \"\"\"  Benchmark for a selection of ufuncs on a small arrays and scalars\n \n     Since the arrays and scalars are small, we are benchmarking the overhead \n     of the numpy ufunc functionality\n",
            "comment_added_diff": {},
            "comment_deleted_diff": {
                "25": "# Array arguments"
            },
            "comment_modified_diff": {}
        },
        {
            "commit": "a974be63b54db84ac4f5be5da0bb1363922174ac",
            "timestamp": "2023-03-04T15:41:54+00:00",
            "author": "Rohit Goswami",
            "commit_message": "BENCH: Add 0d ndarray benchmarks",
            "additions": 19,
            "deletions": 6,
            "change_type": "MODIFY",
            "diff": "@@ -22,12 +22,6 @@\n # Index arguments\n meth_1arg_ind = ['__getitem__']\n \n-# Valid for 0d arrays\n-meth_0d = ['__bool__', '__complex__', '__float__']\n-\n-# Valid for size-1 arrays\n-meth_1d = ['__int__']\n-\n # type_only\n meth_int_scalar = ['__index__', '__lshift__', '__rshift__']\n meth_int_bool = ['__invert__', '__xor__']\n@@ -103,6 +97,25 @@ def time_ndarray_meth(self, methname, npdtypes):\n         meth = getattr(self.xarg, methname)\n         meth()\n \n+class Methods0D(Benchmark):\n+    \"\"\"Zero dimension array methods\n+    \"\"\"\n+    params = [['__bool__', '__complex__',\n+               '__float__', '__int__'], TYPES1]\n+    param_names = ['methods', 'npdtypes']\n+    timeout = 10\n+\n+    def setup(self, methname, npdtypes):\n+        self.xarg = np.array(3, dtype=npdtypes)\n+\n+    def time_ndarray__0d__(self, methname, npdtypes):\n+        meth = getattr(self.xarg, methname)\n+        if npdtypes in ['complex64',\n+                        'complex128',\n+                        'complex256'] and methname in ['__float__', '__int__']:\n+            return\n+        meth()\n+\n class MethodsV1(Benchmark):\n     \"\"\" Benchmark for the methods which take an argument\n     \"\"\"\n",
            "comment_added_diff": {},
            "comment_deleted_diff": {
                "25": "# Valid for 0d arrays",
                "28": "# Valid for size-1 arrays"
            },
            "comment_modified_diff": {}
        },
        {
            "commit": "2bcdc5ddb251f4c64d58c6ff58c2807566bcfa76",
            "timestamp": "2023-03-04T15:41:54+00:00",
            "author": "Rohit Goswami",
            "commit_message": "BENCH: Add ndarray get,set items",
            "additions": 32,
            "deletions": 6,
            "change_type": "MODIFY",
            "diff": "@@ -19,18 +19,12 @@\n           'rint', 'round', 'sign', 'signbit', 'sin', 'sinh', 'spacing', 'sqrt',\n           'square', 'subtract', 'tan', 'tanh', 'true_divide', 'trunc']\n \n-# Index arguments\n-meth_1arg_ind = ['__getitem__']\n-\n # type_only\n meth_int_scalar = ['__index__', '__lshift__', '__rshift__']\n meth_int_bool = ['__invert__', '__xor__']\n meth_int_only = ['__and__', '__or__']\n meth_no_complex = ['__mod__', '__floordiv__']\n \n-# Special\n-meth_special = ['__setitem__']\n-\n for name in dir(np):\n     if isinstance(getattr(np, name, None), np.ufunc) and name not in ufuncs:\n         print(\"Missing ufunc %r\" % (name,))\n@@ -135,6 +129,38 @@ def time_ndarray_meth(self, methname, npdtypes):\n         meth = getattr(self.xarg_one, methname)\n         meth(self.xarg_two)\n \n+class NDArrayGetItem(Benchmark):\n+    param_names = ['margs', 'msize']\n+    params = [[0, (0, 0), (-1, 0), [0, -1]],\n+              ['small', 'big']]\n+\n+    def setup(self, margs, msize):\n+        self.xs = np.random.uniform(-1, 1, 6).reshape(2, 3)\n+        self.xl = np.random.uniform(-1, 1, 100*100).reshape(100, 100)\n+\n+    def time_methods_getitem(self, margs, msize):\n+        if msize == 'small':\n+            mdat = self.xs\n+        elif msize == 'big':\n+            mdat = self.xl\n+        getattr(mdat, '__getitem__')(margs)\n+\n+class NDArraySetItem(Benchmark):\n+    param_names = ['margs', 'msize']\n+    params = [[0, (0, 0), (-1, 0), [0, -1]],\n+              ['small', 'big']]\n+\n+    def setup(self, margs, msize):\n+        self.xs = np.random.uniform(-1, 1, 6).reshape(2, 3)\n+        self.xl = np.random.uniform(-1, 1, 100*100).reshape(100, 100)\n+\n+    def time_methods_setitem(self, margs, msize):\n+        if msize == 'small':\n+            mdat = self.xs\n+        elif msize == 'big':\n+            mdat = self.xl\n+        getattr(mdat, '__setitem__')(margs, 17)\n+\n class DLPMethods(Benchmark):\n     \"\"\" Benchmark for DLPACK helpers\n     \"\"\"\n",
            "comment_added_diff": {},
            "comment_deleted_diff": {
                "22": "# Index arguments",
                "31": "# Special"
            },
            "comment_modified_diff": {}
        },
        {
            "commit": "252a4bd62960deddf69e3b661f4e2e6e71be522a",
            "timestamp": "2023-03-04T15:42:55+00:00",
            "author": "Rohit Goswami",
            "commit_message": "MAINT,BENCH: Rework to skip benchmarks\n\nUsing the slightly less documented NotImplementedError in setup to skip",
            "additions": 5,
            "deletions": 4,
            "change_type": "MODIFY",
            "diff": "@@ -101,13 +101,14 @@ class Methods0D(Benchmark):\n \n     def setup(self, methname, npdtypes):\n         self.xarg = np.array(3, dtype=npdtypes)\n-\n-    def time_ndarray__0d__(self, methname, npdtypes):\n-        meth = getattr(self.xarg, methname)\n         if npdtypes in ['complex64',\n                         'complex128',\n                         'complex256'] and methname in ['__float__', '__int__']:\n-            return\n+            # Skip\n+            raise NotImplementedError\n+\n+    def time_ndarray__0d__(self, methname, npdtypes):\n+        meth = getattr(self.xarg, methname)\n         meth()\n \n class MethodsV1(Benchmark):\n",
            "comment_added_diff": {
                "107": "            # Skip"
            },
            "comment_deleted_diff": {},
            "comment_modified_diff": {}
        },
        {
            "commit": "49a2268af12611f23a9c7fce1d3632cb84f008ff",
            "timestamp": "2023-03-04T15:42:56+00:00",
            "author": "Rohit Goswami",
            "commit_message": "BENCH: Add remaining ndarray dunder methods\n\nWith the exception of __index__ which is benchmarked anyway",
            "additions": 35,
            "deletions": 16,
            "change_type": "MODIFY",
            "diff": "@@ -19,12 +19,6 @@\n           'rint', 'round', 'sign', 'signbit', 'sin', 'sinh', 'spacing', 'sqrt',\n           'square', 'subtract', 'tan', 'tanh', 'true_divide', 'trunc']\n \n-# type_only\n-meth_int_scalar = ['__index__', '__lshift__', '__rshift__']\n-meth_int_bool = ['__invert__', '__xor__']\n-meth_int_only = ['__and__', '__or__']\n-meth_no_complex = ['__mod__', '__floordiv__']\n-\n for name in dir(np):\n     if isinstance(getattr(np, name, None), np.ufunc) and name not in ufuncs:\n         print(\"Missing ufunc %r\" % (name,))\n@@ -91,19 +85,38 @@ def time_ndarray_meth(self, methname, npdtypes):\n         meth = getattr(self.xarg, methname)\n         meth()\n \n+class NDArrayLRShifts(Benchmark):\n+    \"\"\" Benchmark for the shift methods\n+    \"\"\"\n+    params = [['__lshift__', '__rshift__'],\n+              ['intp', 'int8', 'int16',\n+                'int32', 'int64', 'uint8',\n+                'uint16', 'uint32', 'uint64']]\n+    param_names = ['methods', 'npdtypes']\n+    timeout = 10\n+\n+    def setup(self, methname, npdtypes):\n+        self.vals = np.ones(1000,\n+                            dtype=getattr(np, npdtypes)) * \\\n+                            np.random.randint(9)\n+\n+    def time_ndarray_meth(self, methname, npdtypes):\n+        mshift = getattr(self.vals, methname)\n+        mshift(2)\n+\n class Methods0D(Benchmark):\n     \"\"\"Zero dimension array methods\n     \"\"\"\n-    params = [['__bool__', '__complex__',\n+    params = [['__bool__', '__complex__', '__invert__',\n                '__float__', '__int__'], TYPES1]\n     param_names = ['methods', 'npdtypes']\n     timeout = 10\n \n     def setup(self, methname, npdtypes):\n         self.xarg = np.array(3, dtype=npdtypes)\n-        if npdtypes in ['complex64',\n-                        'complex128',\n-                        'complex256'] and methname in ['__float__', '__int__']:\n+        if (npdtypes.startswith('complex') and \\\n+           methname in ['__float__', '__int__']) or \\\n+           (npdtypes.startswith('int') and methname == '__invert__'):\n             # Skip\n             raise NotImplementedError\n \n@@ -114,17 +127,23 @@ def time_ndarray__0d__(self, methname, npdtypes):\n class MethodsV1(Benchmark):\n     \"\"\" Benchmark for the methods which take an argument\n     \"\"\"\n-    params = [['__add__', '__eq__', '__ge__', '__gt__',\n-                 '__matmul__', '__le__', '__lt__', '__mul__',\n-                 '__ne__', '__pow__', '__sub__', '__truediv__'],\n+    params = [['__and__', '__add__', '__eq__', '__floordiv__', '__ge__',\n+               '__gt__', '__le__', '__lt__', '__matmul__',\n+               '__mod__', '__mul__', '__ne__', '__or__',\n+               '__pow__', '__sub__', '__truediv__', '__xor__'],\n               TYPES1]\n     param_names = ['methods', 'npdtypes']\n     timeout = 10\n \n     def setup(self, methname, npdtypes):\n-        values = get_squares_()\n-        self.xarg_one = values.get(npdtypes)[0]\n-        self.xarg_two = values.get(npdtypes)[1]\n+        if (npdtypes.startswith('complex') and \\\n+           methname in ['__floordiv__', '__mod__']) or \\\n+           (not npdtypes.startswith('int') and \\\n+            methname in ['__and__', '__or__', '__xor__']):\n+            raise NotImplementedError # skip\n+        values = get_squares_().get(npdtypes)\n+        self.xarg_one = values[0]\n+        self.xarg_two = values[1]\n \n     def time_ndarray_meth(self, methname, npdtypes):\n         meth = getattr(self.xarg_one, methname)\n",
            "comment_added_diff": {
                "143": "            raise NotImplementedError # skip"
            },
            "comment_deleted_diff": {
                "22": "# type_only"
            },
            "comment_modified_diff": {}
        },
        {
            "commit": "41c7fba65be6060ee1b7023ccc0634f8e935970a",
            "timestamp": "2023-03-04T15:42:56+00:00",
            "author": "Rohit Goswami",
            "commit_message": "MAINT: Fixup BENCH to appease linter",
            "additions": 5,
            "deletions": 5,
            "change_type": "MODIFY",
            "diff": "@@ -114,7 +114,7 @@ class Methods0D(Benchmark):\n \n     def setup(self, methname, npdtypes):\n         self.xarg = np.array(3, dtype=npdtypes)\n-        if (npdtypes.startswith('complex') and \\\n+        if (npdtypes.startswith('complex') and\n            methname in ['__float__', '__int__']) or \\\n            (npdtypes.startswith('int') and methname == '__invert__'):\n             # Skip\n@@ -136,11 +136,11 @@ class MethodsV1(Benchmark):\n     timeout = 10\n \n     def setup(self, methname, npdtypes):\n-        if (npdtypes.startswith('complex') and \\\n+        if (npdtypes.startswith('complex') and\n            methname in ['__floordiv__', '__mod__']) or \\\n-           (not npdtypes.startswith('int') and \\\n-            methname in ['__and__', '__or__', '__xor__']):\n-            raise NotImplementedError # skip\n+           (not npdtypes.startswith('int') and\n+             methname in ['__and__', '__or__', '__xor__']):\n+            raise NotImplementedError  # skip\n         values = get_squares_().get(npdtypes)\n         self.xarg_one = values[0]\n         self.xarg_two = values[1]\n",
            "comment_added_diff": {
                "143": "            raise NotImplementedError  # skip"
            },
            "comment_deleted_diff": {
                "143": "            raise NotImplementedError # skip"
            },
            "comment_modified_diff": {
                "143": "            raise NotImplementedError # skip"
            }
        },
        {
            "commit": "56d2fd5d8735e73f41ab30f42d139309df7fd758",
            "timestamp": "2023-03-04T16:02:03+00:00",
            "author": "Rohit Goswami",
            "commit_message": "BENCH: Reduce shapes and types for speed",
            "additions": 4,
            "deletions": 5,
            "change_type": "MODIFY",
            "diff": "@@ -163,7 +163,7 @@ class NDArrayGetItem(Benchmark):\n \n     def setup(self, margs, msize):\n         self.xs = np.random.uniform(-1, 1, 6).reshape(2, 3)\n-        self.xl = np.random.uniform(-1, 1, 100*100).reshape(100, 100)\n+        self.xl = np.random.uniform(-1, 1, 50*50).reshape(50, 50)\n \n     def time_methods_getitem(self, margs, msize):\n         if msize == 'small':\n@@ -232,11 +232,10 @@ def time_astype(self, typeconv):\n class UfuncsCreate(Benchmark):\n     \"\"\" Benchmark for creation functions\n     \"\"\"\n+    # (64, 64), (128, 128), (256, 256)\n+    # , (512, 512), (1024, 1024)\n     params = [[16, 32, 128, 256, 512,\n-               (16, 16), (32, 32),\n-               (64, 64), (128, 128),\n-               (256, 256), (512, 512),\n-               (1024, 1024)],\n+               (16, 16), (32, 32)],\n               ['C', 'F'],\n               TYPES1]\n     param_names = ['shape', 'order', 'npdtypes']\n",
            "comment_added_diff": {
                "235": "    # (64, 64), (128, 128), (256, 256)",
                "236": "    # , (512, 512), (1024, 1024)"
            },
            "comment_deleted_diff": {},
            "comment_modified_diff": {
                "236": "               (16, 16), (32, 32),"
            }
        },
        {
            "commit": "2043b4690186a67c589372cc2cf167b16b726657",
            "timestamp": "2023-03-04T17:08:05+00:00",
            "author": "Rohit Goswami",
            "commit_message": "BENCH: Rework to test real, round",
            "additions": 28,
            "deletions": 2,
            "change_type": "MODIFY",
            "diff": "@@ -18,16 +18,42 @@\n           'logaddexp2', 'logical_and', 'logical_not', 'logical_or',\n           'logical_xor', 'matmul', 'maximum', 'minimum', 'mod', 'modf',\n           'multiply', 'negative', 'nextafter', 'not_equal', 'positive',\n-          'power', 'rad2deg', 'radians', 'real', 'reciprocal', 'remainder',\n-          'right_shift', 'rint', 'round', 'sign', 'signbit', 'sin',\n+          'power', 'rad2deg', 'radians', 'reciprocal', 'remainder',\n+          'right_shift', 'rint', 'sign', 'signbit', 'sin',\n           'sinh', 'spacing', 'sqrt', 'square', 'subtract', 'tan', 'tanh',\n           'true_divide', 'trunc']\n+arrayfuncdisp = ['real', 'round']\n+\n \n for name in dir(np):\n     if isinstance(getattr(np, name, None), np.ufunc) and name not in ufuncs:\n         print(\"Missing ufunc %r\" % (name,))\n \n \n+class ArrayFunctionDispatcher(Benchmark):\n+    params = [arrayfuncdisp]\n+    param_names = ['func']\n+    timeout = 10\n+\n+    def setup(self, ufuncname):\n+        np.seterr(all='ignore')\n+        try:\n+            self.afdn = getattr(np, ufuncname)\n+        except AttributeError:\n+            raise NotImplementedError()\n+        self.args = []\n+        for _, aarg in get_squares_().items():\n+            arg = (aarg,) * 1 # no nin\n+            try:\n+                self.afdn(*arg)\n+            except TypeError:\n+                continue\n+            self.args.append(arg)\n+\n+    def time_afdn_types(self, ufuncname):\n+        [self.afdn(*arg) for arg in self.args]\n+\n+\n class Broadcast(Benchmark):\n     def setup(self):\n         self.d = np.ones((50000, 100), dtype=np.float64)\n",
            "comment_added_diff": {
                "46": "            arg = (aarg,) * 1 # no nin"
            },
            "comment_deleted_diff": {},
            "comment_modified_diff": {}
        },
        {
            "commit": "e014e7cc1d803b49c0a29a7731e7f78dbe1451d8",
            "timestamp": "2023-03-04T19:18:59+00:00",
            "author": "Rohit Goswami",
            "commit_message": "MAINT: Appease linter",
            "additions": 3,
            "deletions": 2,
            "change_type": "MODIFY",
            "diff": "@@ -43,7 +43,7 @@ def setup(self, ufuncname):\n             raise NotImplementedError()\n         self.args = []\n         for _, aarg in get_squares_().items():\n-            arg = (aarg,) * 1 # no nin\n+            arg = (aarg,) * 1  # no nin\n             try:\n                 self.afdn(*arg)\n             except TypeError:\n@@ -169,7 +169,8 @@ class MethodsV1(Benchmark):\n \n     def setup(self, methname, npdtypes):\n         if (\n-            npdtypes.startswith(\"complex\") and methname in [\"__floordiv__\", \"__mod__\"]\n+            npdtypes.startswith(\"complex\")\n+                and methname in [\"__floordiv__\", \"__mod__\"]\n         ) or (\n             not npdtypes.startswith(\"int\")\n             and methname in [\"__and__\", \"__or__\", \"__xor__\"]\n",
            "comment_added_diff": {
                "46": "            arg = (aarg,) * 1  # no nin"
            },
            "comment_deleted_diff": {
                "46": "            arg = (aarg,) * 1 # no nin"
            },
            "comment_modified_diff": {
                "46": "            arg = (aarg,) * 1 # no nin"
            }
        },
        {
            "commit": "1def667d1aa5e38e0cbb01a5d76b21c746904224",
            "timestamp": "2023-04-13T21:34:17+00:00",
            "author": "Rohit Goswami",
            "commit_message": "BENCH: Rework benchmarks",
            "additions": 0,
            "deletions": 77,
            "change_type": "MODIFY",
            "diff": "@@ -256,83 +256,6 @@ def time_astype(self, typeconv):\n         self.xarg.astype(typeconv[1])\n \n \n-class UfuncsCreate(Benchmark):\n-    \"\"\" Benchmark for creation functions\n-    \"\"\"\n-    # (64, 64), (128, 128), (256, 256)\n-    # , (512, 512), (1024, 1024)\n-    params = [[16, 32, 128, 256, 512,\n-               (16, 16), (32, 32)],\n-              ['C', 'F'],\n-              TYPES1]\n-    param_names = ['shape', 'order', 'npdtypes']\n-    timeout = 10\n-\n-    def setup(self, shape, order, npdtypes):\n-        values = get_squares_()\n-        self.xarg = values.get(npdtypes)[0]\n-\n-    def time_full(self, shape, order, npdtypes):\n-        np.full(shape, self.xarg[1], dtype=npdtypes, order=order)\n-\n-    def time_full_like(self, shape, order, npdtypes):\n-        np.full_like(self.xarg, self.xarg[0], order=order)\n-\n-    def time_ones(self, shape, order, npdtypes):\n-        np.ones(shape, dtype=npdtypes, order=order)\n-\n-    def time_ones_like(self, shape, order, npdtypes):\n-        np.ones_like(self.xarg, order=order)\n-\n-    def time_zeros(self, shape, order, npdtypes):\n-        np.zeros(shape, dtype=npdtypes, order=order)\n-\n-    def time_zeros_like(self, shape, order, npdtypes):\n-        np.zeros_like(self.xarg, order=order)\n-\n-    def time_empty(self, shape, order, npdtypes):\n-        np.empty(shape, dtype=npdtypes, order=order)\n-\n-    def time_empty_like(self, shape, order, npdtypes):\n-        np.empty_like(self.xarg, order=order)\n-\n-\n-class UfuncsFromDLP(Benchmark):\n-    \"\"\" Benchmark for creation functions\n-    \"\"\"\n-    params = [[16, 32, (16, 16),\n-               (32, 32), (64, 64)],\n-              TYPES1]\n-    param_names = ['shape', 'npdtypes']\n-    timeout = 10\n-\n-    def setup(self, shape, npdtypes):\n-        if npdtypes in ['longdouble', 'clongdouble']:\n-            raise NotImplementedError(\n-                'Only IEEE dtypes are supported')\n-        values = get_squares_()\n-        self.xarg = values.get(npdtypes)[0]\n-\n-    def time_from_dlpack(self, shape, npdtypes):\n-        np.from_dlpack(self.xarg)\n-\n-\n-class UFuncMeshGrid(Benchmark):\n-    \"\"\" Benchmark meshgrid generation\n-    \"\"\"\n-    params = [[16, 32],\n-              [2, 3, 4],\n-              ['ij', 'xy'], TYPES1]\n-    param_names = ['size', 'ndims', 'ind', 'ndtype']\n-    timeout = 10\n-\n-    def setup(self, size, ndims, ind, ndtype):\n-        self.grid_dims = [(np.random.ranf(size)).astype(ndtype) for\n-                          x in range(ndims)]\n-\n-    def time_meshgrid(self, size, ndims, ind, ndtype):\n-        np.meshgrid(*self.grid_dims, indexing=ind)\n-\n class UFuncSmall(Benchmark):\n     \"\"\"  Benchmark for a selection of ufuncs on a small arrays and scalars\n \n",
            "comment_added_diff": {},
            "comment_deleted_diff": {
                "262": "    # (64, 64), (128, 128), (256, 256)",
                "263": "    # , (512, 512), (1024, 1024)"
            },
            "comment_modified_diff": {}
        },
        {
            "commit": "8bb28e30d1a6c75788d1cf9e78c9928c2a6b344a",
            "timestamp": "2023-08-27T23:01:14+02:00",
            "author": "Ralf Gommers",
            "commit_message": "BENCH: fix issues with `operator` benchmarks in `bench_ufunc.py`",
            "additions": 38,
            "deletions": 12,
            "change_type": "MODIFY",
            "diff": "@@ -159,23 +159,49 @@ def time_ndarray__0d__(self, methname, npdtypes):\n class MethodsV1(Benchmark):\n     \"\"\" Benchmark for the methods which take an argument\n     \"\"\"\n-    params = [['__and__', '__add__', '__eq__', '__floordiv__', '__ge__',\n-               '__gt__', '__le__', '__lt__', '__matmul__',\n-               '__mod__', '__mul__', '__ne__', '__or__',\n-               '__pow__', '__sub__', '__truediv__', '__xor__'],\n+    params = [['__add__', '__eq__', '__ge__', '__gt__', '__le__',\n+               '__lt__', '__matmul__', '__mul__', '__ne__',\n+               '__pow__', '__sub__', '__truediv__'],\n               TYPES1]\n     param_names = ['methods', 'npdtypes']\n     timeout = 10\n \n     def setup(self, methname, npdtypes):\n-        if (\n-            npdtypes.startswith(\"complex\")\n-                and methname in [\"__floordiv__\", \"__mod__\"]\n-        ) or (\n-            not npdtypes.startswith(\"int\")\n-            and methname in [\"__and__\", \"__or__\", \"__xor__\"]\n-        ):\n-            raise NotImplementedError  # skip\n+        values = get_squares_().get(npdtypes)\n+        self.xargs = [values[0], values[1]]\n+        if np.issubdtype(npdtypes, np.inexact):\n+            # avoid overflow in __pow__/__matmul__ for low-precision dtypes\n+            self.xargs[1] *= 0.01\n+\n+    def time_ndarray_meth(self, methname, npdtypes):\n+        getattr(operator, methname)(*self.xargs)\n+\n+\n+class MethodsV1IntOnly(Benchmark):\n+    \"\"\" Benchmark for the methods which take an argument\n+    \"\"\"\n+    params = [['__and__', '__or__', '__xor__'],\n+              ['int16', 'int32', 'int64']]\n+    param_names = ['methods', 'npdtypes']\n+    timeout = 10\n+\n+    def setup(self, methname, npdtypes):\n+        values = get_squares_().get(npdtypes)\n+        self.xargs = [values[0], values[1]]\n+\n+    def time_ndarray_meth(self, methname, npdtypes):\n+        getattr(operator, methname)(*self.xargs)\n+\n+\n+class MethodsV1NoComplex(Benchmark):\n+    \"\"\" Benchmark for the methods which take an argument\n+    \"\"\"\n+    params = [['__floordiv__', '__mod__'],\n+              [dt for dt in TYPES1 if not dt.startswith('complex')]]\n+    param_names = ['methods', 'npdtypes']\n+    timeout = 10\n+\n+    def setup(self, methname, npdtypes):\n         values = get_squares_().get(npdtypes)\n         self.xargs = [values[0], values[1]]\n \n",
            "comment_added_diff": {
                "173": "            # avoid overflow in __pow__/__matmul__ for low-precision dtypes"
            },
            "comment_deleted_diff": {
                "178": "            raise NotImplementedError  # skip"
            },
            "comment_modified_diff": {
                "173": "                and methname in [\"__floordiv__\", \"__mod__\"]"
            }
        },
        {
            "commit": "618dcb742d225ef09a7a0a8fdfd32c6ae14eb7c4",
            "timestamp": "2023-08-27T23:01:14+02:00",
            "author": "Ralf Gommers",
            "commit_message": "BENCH: fix issues with Methods0D benchmarks in `bench_ufunc.py`",
            "additions": 35,
            "deletions": 8,
            "change_type": "MODIFY",
            "diff": "@@ -135,27 +135,54 @@ def time_ndarray_meth(self, methname, npdtypes):\n         getattr(operator, methname)(*[self.vals, 2])\n \n \n-class Methods0D(Benchmark):\n+class Methods0DBoolComplex(Benchmark):\n     \"\"\"Zero dimension array methods\n     \"\"\"\n-    params = [['__bool__', '__complex__', '__invert__',\n-               '__float__', '__int__'], TYPES1]\n+    params = [['__bool__', '__complex__'],\n+              TYPES1]\n     param_names = ['methods', 'npdtypes']\n     timeout = 10\n \n     def setup(self, methname, npdtypes):\n         self.xarg = np.array(3, dtype=npdtypes)\n-        if (npdtypes.startswith('complex') and\n-           methname in ['__float__', '__int__']) or \\\n-           (npdtypes.startswith('int') and methname == '__invert__'):\n-            # Skip\n-            raise NotImplementedError\n \n     def time_ndarray__0d__(self, methname, npdtypes):\n         meth = getattr(self.xarg, methname)\n         meth()\n \n \n+class Methods0DFloatInt(Benchmark):\n+    \"\"\"Zero dimension array methods\n+    \"\"\"\n+    params = [['__int__', '__float__'],\n+              [dt for dt in TYPES1 if not dt.startswith('complex')]\n+             ]\n+    param_names = ['methods', 'npdtypes']\n+    timeout = 10\n+\n+    def setup(self, methname, npdtypes):\n+        self.xarg = np.array(3, dtype=npdtypes)\n+\n+    def time_ndarray__0d__(self, methname, npdtypes):\n+        meth = getattr(self.xarg, methname)\n+        meth()\n+\n+\n+\n+class Methods0DInvert(Benchmark):\n+    \"\"\"Zero dimension array methods\n+    \"\"\"\n+    params = ['int16', 'int32', 'int64']\n+    param_names = ['npdtypes']\n+    timeout = 10\n+\n+    def setup(self, npdtypes):\n+        self.xarg = np.array(3, dtype=npdtypes)\n+\n+    def time_ndarray__0d__(self, npdtypes):\n+        self.xarg.__invert__()\n+\n+\n class MethodsV1(Benchmark):\n     \"\"\" Benchmark for the methods which take an argument\n     \"\"\"\n",
            "comment_added_diff": {},
            "comment_deleted_diff": {
                "151": "            # Skip"
            },
            "comment_modified_diff": {}
        }
    ],
    "npy_cpu_features.c": [],
    "npyio.py": [
        {
            "commit": "6d474f2dfe5e4b7ea2a6e7423a638316fa186227",
            "timestamp": "2023-01-01T13:38:57+01:00",
            "author": "dmbelov",
            "commit_message": "BUG: np.loadtxt cannot load text file with quoted fields separated by whitespace (#22906)\n\nFix issue with `delimiter=None` and quote character not working properly (not using whitespace delimiter mode).\r\n\r\nCloses gh-22899",
            "additions": 8,
            "deletions": 0,
            "change_type": "MODIFY",
            "diff": "@@ -1303,6 +1303,14 @@ def loadtxt(fname, dtype=float, comments='#', delimiter=None,\n     array([('alpha, #42', 10.), ('beta, #64',  2.)],\n           dtype=[('label', '<U12'), ('value', '<f8')])\n \n+    Quoted fields can be separated by multiple whitespace characters:\n+\n+    >>> s = StringIO('\"alpha, #42\"       10.0\\n\"beta, #64\" 2.0\\n')\n+    >>> dtype = np.dtype([(\"label\", \"U12\"), (\"value\", float)])\n+    >>> np.loadtxt(s, dtype=dtype, delimiter=None, quotechar='\"')\n+    array([('alpha, #42', 10.), ('beta, #64',  2.)],\n+          dtype=[('label', '<U12'), ('value', '<f8')])\n+\n     Two consecutive quote characters within a quoted field are treated as a\n     single escaped character:\n \n",
            "comment_added_diff": {
                "1308": "    >>> s = StringIO('\"alpha, #42\"       10.0\\n\"beta, #64\" 2.0\\n')",
                "1311": "    array([('alpha, #42', 10.), ('beta, #64',  2.)],"
            },
            "comment_deleted_diff": {},
            "comment_modified_diff": {}
        },
        {
            "commit": "a12a39378d342f368b2e02c1a7e300f5a95baff7",
            "timestamp": "2023-03-25T11:14:24+00:00",
            "author": "yuki",
            "commit_message": "MAINT: Fix reference roles of ast",
            "additions": 5,
            "deletions": 5,
            "change_type": "MODIFY",
            "diff": "@@ -142,7 +142,7 @@ class NpzFile(Mapping):\n     max_header_size : int, optional\n         Maximum allowed size of the header.  Large headers may not be safe\n         to load securely and thus require explicitly passing a larger value.\n-        See :py:meth:`ast.literal_eval()` for details.\n+        See :py:func:`ast.literal_eval()` for details.\n         This option is ignored when `allow_pickle` is passed.  In that case\n         the file is by definition trusted and the limit is unnecessary.\n \n@@ -309,7 +309,7 @@ def load(file, mmap_mode=None, allow_pickle=False, fix_imports=True,\n     max_header_size : int, optional\n         Maximum allowed size of the header.  Large headers may not be safe\n         to load securely and thus require explicitly passing a larger value.\n-        See :py:meth:`ast.literal_eval()` for details.\n+        See :py:func:`ast.literal_eval()` for details.\n         This option is ignored when `allow_pickle` is passed.  In that case\n         the file is by definition trusted and the limit is unnecessary.\n \n@@ -1159,10 +1159,10 @@ def loadtxt(fname, dtype=float, comments='#', delimiter=None,\n         while such lines are counted in `skiprows`.\n \n         .. versionadded:: 1.16.0\n-        \n+\n         .. versionchanged:: 1.23.0\n-            Lines containing no data, including comment lines (e.g., lines \n-            starting with '#' or as specified via `comments`) are not counted \n+            Lines containing no data, including comment lines (e.g., lines\n+            starting with '#' or as specified via `comments`) are not counted\n             towards `max_rows`.\n     quotechar : unicode character or None, optional\n         The character used to denote the start and end of a quoted item.\n",
            "comment_added_diff": {
                "1165": "            starting with '#' or as specified via `comments`) are not counted"
            },
            "comment_deleted_diff": {
                "1165": "            starting with '#' or as specified via `comments`) are not counted"
            },
            "comment_modified_diff": {
                "1165": "            starting with '#' or as specified via `comments`) are not counted"
            }
        },
        {
            "commit": "be0e502e47b72db8fbd52ab40124fcb07d45936f",
            "timestamp": "2023-04-06T12:34:33+02:00",
            "author": "Ganesh Kathiresan",
            "commit_message": "ENH: ``__repr__`` for NpzFile object (#23357)\n\nImproves the repr to include information about the arrays contained, e.g.:\r\n\r\n   >>> npzfile = np.load('arr.npz')\r\n   >>> npzfile\r\n   NpzFile 'arr.npz' with keys arr_0, arr_1, arr_2, arr_3, arr_4...\r\n\r\ncloses #23319\r\n\r\nCo-authored-by: Ross Barnowski <rossbar@berkeley.edu>",
            "additions": 16,
            "deletions": 0,
            "change_type": "MODIFY",
            "diff": "@@ -167,6 +167,8 @@ class NpzFile(Mapping):\n     >>> npz = np.load(outfile)\n     >>> isinstance(npz, np.lib.npyio.NpzFile)\n     True\n+    >>> npz\n+    NpzFile 'object' with keys x, y\n     >>> sorted(npz.files)\n     ['x', 'y']\n     >>> npz['x']  # getitem access\n@@ -178,6 +180,7 @@ class NpzFile(Mapping):\n     # Make __exit__ safe if zipfile_factory raises an exception\n     zip = None\n     fid = None\n+    _MAX_REPR_ARRAY_COUNT = 5\n \n     def __init__(self, fid, own_fid=False, allow_pickle=False,\n                  pickle_kwargs=None, *,\n@@ -259,6 +262,19 @@ def __getitem__(self, key):\n         else:\n             raise KeyError(\"%s is not a file in the archive\" % key)\n \n+    def __repr__(self):\n+        # Get filename or default to `object`\n+        if isinstance(self.fid, str):\n+            filename = self.fid\n+        else:\n+            filename = getattr(self.fid, \"name\", \"object\")\n+\n+        # Get the name of arrays\n+        array_names = ', '.join(self.files[:self._MAX_REPR_ARRAY_COUNT])\n+        if len(self.files) > self._MAX_REPR_ARRAY_COUNT:\n+            array_names += \"...\"\n+        return f\"NpzFile {filename!r} with keys: {array_names}\"\n+\n \n @set_module('numpy')\n def load(file, mmap_mode=None, allow_pickle=False, fix_imports=True,\n",
            "comment_added_diff": {
                "266": "        # Get filename or default to `object`",
                "272": "        # Get the name of arrays"
            },
            "comment_deleted_diff": {},
            "comment_modified_diff": {}
        },
        {
            "commit": "12efa8eec31bd476e0110bb90de43d603d0e76d3",
            "timestamp": "2023-06-14T19:30:01+02:00",
            "author": "Mateusz Sok\u00f3\u0142",
            "commit_message": "DEP: deprecate compat and selected lib utils (#23830)\n\n[skip ci]",
            "additions": 11,
            "deletions": 21,
            "change_type": "MODIFY",
            "diff": "@@ -8,6 +8,7 @@\n import operator\n from operator import itemgetter, index as opindex, methodcaller\n from collections.abc import Mapping\n+import pickle\n \n import numpy as np\n from . import format\n@@ -21,11 +22,7 @@\n     ConverterLockError, ConversionWarning, _is_string_like,\n     has_nested_fields, flatten_dtype, easy_dtype, _decode_line\n     )\n-\n-from numpy.compat import (\n-    asbytes, asstr, asunicode, os_fspath, os_PathLike,\n-    pickle\n-    )\n+from numpy._utils._convertions import asunicode, asbytes\n \n \n __all__ = [\n@@ -97,7 +94,7 @@ def zipfile_factory(file, *args, **kwargs):\n     constructor.\n     \"\"\"\n     if not hasattr(file, 'read'):\n-        file = os_fspath(file)\n+        file = os.fspath(file)\n     import zipfile\n     kwargs['allowZip64'] = True\n     return zipfile.ZipFile(file, *args, **kwargs)\n@@ -424,7 +421,7 @@ def load(file, mmap_mode=None, allow_pickle=False, fix_imports=True,\n             fid = file\n             own_fid = False\n         else:\n-            fid = stack.enter_context(open(os_fspath(file), \"rb\"))\n+            fid = stack.enter_context(open(os.fspath(file), \"rb\"))\n             own_fid = True\n \n         # Code to distinguish from NumPy binary files and pickles.\n@@ -536,7 +533,7 @@ def save(file, arr, allow_pickle=True, fix_imports=True):\n     if hasattr(file, 'write'):\n         file_ctx = contextlib.nullcontext(file)\n     else:\n-        file = os_fspath(file)\n+        file = os.fspath(file)\n         if not file.endswith('.npy'):\n             file = file + '.npy'\n         file_ctx = open(file, \"wb\")\n@@ -716,7 +713,7 @@ def _savez(file, args, kwds, compress, allow_pickle=True, pickle_kwargs=None):\n     import zipfile\n \n     if not hasattr(file, 'write'):\n-        file = os_fspath(file)\n+        file = os.fspath(file)\n         if not file.endswith('.npz'):\n             file = file + '.npz'\n \n@@ -1487,11 +1484,6 @@ def savetxt(fname, X, fmt='%.18e', delimiter=' ', newline='\\n', header='',\n \n     \"\"\"\n \n-    # Py3 conversions first\n-    if isinstance(fmt, bytes):\n-        fmt = asstr(fmt)\n-    delimiter = asstr(delimiter)\n-\n     class WriteWrap:\n         \"\"\"Convert to bytes on bytestream inputs.\n \n@@ -1526,8 +1518,8 @@ def first_write(self, v):\n                 self.write = self.write_bytes\n \n     own_fh = False\n-    if isinstance(fname, os_PathLike):\n-        fname = os_fspath(fname)\n+    if isinstance(fname, os.PathLike):\n+        fname = os.fspath(fname)\n     if _is_string_like(fname):\n         # datasource doesn't support creating a new file ...\n         open(fname, 'wt').close()\n@@ -1564,7 +1556,7 @@ def first_write(self, v):\n         if type(fmt) in (list, tuple):\n             if len(fmt) != ncol:\n                 raise AttributeError('fmt has wrong shape.  %s' % str(fmt))\n-            format = asstr(delimiter).join(map(asstr, fmt))\n+            format = delimiter.join(fmt)\n         elif isinstance(fmt, str):\n             n_fmt_chars = fmt.count('%')\n             error = ValueError('fmt has wrong number of %% formats:  %s' % fmt)\n@@ -1689,8 +1681,6 @@ def fromregex(file, regexp, dtype, encoding=None):\n         content = file.read()\n         if isinstance(content, bytes) and isinstance(regexp, str):\n             regexp = asbytes(regexp)\n-        elif isinstance(content, str) and isinstance(regexp, bytes):\n-            regexp = asstr(regexp)\n \n         if not hasattr(regexp, 'match'):\n             regexp = re.compile(regexp)\n@@ -1953,8 +1943,8 @@ def genfromtxt(fname, dtype=float, comments='#', delimiter=None,\n         byte_converters = False\n \n     # Initialize the filehandle, the LineSplitter and the NameValidator\n-    if isinstance(fname, os_PathLike):\n-        fname = os_fspath(fname)\n+    if isinstance(fname, os.PathLike):\n+        fname = os.fspath(fname)\n     if isinstance(fname, str):\n         fid = np.lib._datasource.open(fname, 'rt', encoding=encoding)\n         fid_ctx = contextlib.closing(fid)\n",
            "comment_added_diff": {},
            "comment_deleted_diff": {
                "1490": "    # Py3 conversions first"
            },
            "comment_modified_diff": {}
        },
        {
            "commit": "017cef318ad37ac05738830ebb3173c65c128759",
            "timestamp": "2023-07-11T22:25:10+02:00",
            "author": "Mateusz Sok\u00f3\u0142",
            "commit_message": "API: apply review comments",
            "additions": 21,
            "deletions": 3,
            "change_type": "MODIFY",
            "diff": "@@ -22,7 +22,7 @@\n     ConverterLockError, ConversionWarning, _is_string_like,\n     has_nested_fields, flatten_dtype, easy_dtype, _decode_line\n     )\n-from numpy._utils import asunicode, asbytes, deprecate\n+from numpy._utils import asunicode, asbytes\n \n \n __all__ = [\n@@ -2446,7 +2446,6 @@ def encode_unicode_cols(row_tup):\n _genfromtxt_with_like = array_function_dispatch()(genfromtxt)\n \n \n-@deprecate\n def recfromtxt(fname, **kwargs):\n     \"\"\"\n     Load ASCII data from a file and return it in a record array.\n@@ -2468,6 +2467,16 @@ def recfromtxt(fname, **kwargs):\n     array will be determined from the data.\n \n     \"\"\"\n+\n+    # Deprecated in NumPy 2.0, 2023-07-11\n+    warnings.warn(\n+        \"`recfromtxt` is deprecated, \"\n+        \"use `numpy.genfromtxt` instead.\"\n+        \"(deprecated in NumPy 2.0)\",\n+        DeprecationWarning,\n+        stacklevel=2\n+    )\n+\n     kwargs.setdefault(\"dtype\", None)\n     usemask = kwargs.get('usemask', False)\n     output = genfromtxt(fname, **kwargs)\n@@ -2479,7 +2488,6 @@ def recfromtxt(fname, **kwargs):\n     return output\n \n \n-@deprecate\n def recfromcsv(fname, **kwargs):\n     \"\"\"\n     Load ASCII data stored in a comma-separated file.\n@@ -2502,6 +2510,16 @@ def recfromcsv(fname, **kwargs):\n     array will be determined from the data.\n \n     \"\"\"\n+\n+    # Deprecated in NumPy 2.0, 2023-07-11\n+    warnings.warn(\n+        \"`recfromcsv` is deprecated, \"\n+        \"use `numpy.genfromtxt` with comma as `delimiter` instead. \"\n+        \"(deprecated in NumPy 2.0)\",\n+        DeprecationWarning,\n+        stacklevel=2\n+    )\n+\n     # Set default kwargs for genfromtxt as relevant to csv import.\n     kwargs.setdefault(\"case_sensitive\", \"lower\")\n     kwargs.setdefault(\"names\", True)\n",
            "comment_added_diff": {
                "2471": "    # Deprecated in NumPy 2.0, 2023-07-11",
                "2514": "    # Deprecated in NumPy 2.0, 2023-07-11"
            },
            "comment_deleted_diff": {},
            "comment_modified_diff": {}
        },
        {
            "commit": "63d9da80788d75a5c2abb8e79c34a6f6eb02ef7e",
            "timestamp": "2023-09-01T10:53:35+02:00",
            "author": "Mateusz Sok\u00f3\u0142",
            "commit_message": "API: Update `lib.polynomial` and `lib.npyio` namespaces (#24578)",
            "additions": 1,
            "deletions": 2541,
            "change_type": "MODIFY",
            "diff": "@@ -1,2541 +1 @@\n-import os\n-import re\n-import functools\n-import itertools\n-import warnings\n-import weakref\n-import contextlib\n-import operator\n-from operator import itemgetter, index as opindex, methodcaller\n-from collections.abc import Mapping\n-import pickle\n-\n-import numpy as np\n-from . import format\n-from ._datasource import DataSource\n-from numpy.core import overrides\n-from numpy.core.multiarray import packbits, unpackbits\n-from numpy.core._multiarray_umath import _load_from_filelike\n-from numpy.core.overrides import set_array_function_like_doc, set_module\n-from ._iotools import (\n-    LineSplitter, NameValidator, StringConverter, ConverterError,\n-    ConverterLockError, ConversionWarning, _is_string_like,\n-    has_nested_fields, flatten_dtype, easy_dtype, _decode_line\n-    )\n-from numpy._utils import asunicode, asbytes\n-\n-\n-__all__ = [\n-    'savetxt', 'loadtxt', 'genfromtxt', 'load', 'save', 'savez', 'DataSource',\n-    'savez_compressed', 'packbits', 'unpackbits', 'fromregex'\n-    ]\n-\n-\n-array_function_dispatch = functools.partial(\n-    overrides.array_function_dispatch, module='numpy')\n-\n-\n-class BagObj:\n-    \"\"\"\n-    BagObj(obj)\n-\n-    Convert attribute look-ups to getitems on the object passed in.\n-\n-    Parameters\n-    ----------\n-    obj : class instance\n-        Object on which attribute look-up is performed.\n-\n-    Examples\n-    --------\n-    >>> from numpy.lib.npyio import BagObj as BO\n-    >>> class BagDemo:\n-    ...     def __getitem__(self, key): # An instance of BagObj(BagDemo)\n-    ...                                 # will call this method when any\n-    ...                                 # attribute look-up is required\n-    ...         result = \"Doesn't matter what you want, \"\n-    ...         return result + \"you're gonna get this\"\n-    ...\n-    >>> demo_obj = BagDemo()\n-    >>> bagobj = BO(demo_obj)\n-    >>> bagobj.hello_there\n-    \"Doesn't matter what you want, you're gonna get this\"\n-    >>> bagobj.I_can_be_anything\n-    \"Doesn't matter what you want, you're gonna get this\"\n-\n-    \"\"\"\n-\n-    def __init__(self, obj):\n-        # Use weakref to make NpzFile objects collectable by refcount\n-        self._obj = weakref.proxy(obj)\n-\n-    def __getattribute__(self, key):\n-        try:\n-            return object.__getattribute__(self, '_obj')[key]\n-        except KeyError:\n-            raise AttributeError(key) from None\n-\n-    def __dir__(self):\n-        \"\"\"\n-        Enables dir(bagobj) to list the files in an NpzFile.\n-\n-        This also enables tab-completion in an interpreter or IPython.\n-        \"\"\"\n-        return list(object.__getattribute__(self, '_obj').keys())\n-\n-\n-def zipfile_factory(file, *args, **kwargs):\n-    \"\"\"\n-    Create a ZipFile.\n-\n-    Allows for Zip64, and the `file` argument can accept file, str, or\n-    pathlib.Path objects. `args` and `kwargs` are passed to the zipfile.ZipFile\n-    constructor.\n-    \"\"\"\n-    if not hasattr(file, 'read'):\n-        file = os.fspath(file)\n-    import zipfile\n-    kwargs['allowZip64'] = True\n-    return zipfile.ZipFile(file, *args, **kwargs)\n-\n-\n-class NpzFile(Mapping):\n-    \"\"\"\n-    NpzFile(fid)\n-\n-    A dictionary-like object with lazy-loading of files in the zipped\n-    archive provided on construction.\n-\n-    `NpzFile` is used to load files in the NumPy ``.npz`` data archive\n-    format. It assumes that files in the archive have a ``.npy`` extension,\n-    other files are ignored.\n-\n-    The arrays and file strings are lazily loaded on either\n-    getitem access using ``obj['key']`` or attribute lookup using\n-    ``obj.f.key``. A list of all files (without ``.npy`` extensions) can\n-    be obtained with ``obj.files`` and the ZipFile object itself using\n-    ``obj.zip``.\n-\n-    Attributes\n-    ----------\n-    files : list of str\n-        List of all files in the archive with a ``.npy`` extension.\n-    zip : ZipFile instance\n-        The ZipFile object initialized with the zipped archive.\n-    f : BagObj instance\n-        An object on which attribute can be performed as an alternative\n-        to getitem access on the `NpzFile` instance itself.\n-    allow_pickle : bool, optional\n-        Allow loading pickled data. Default: False\n-\n-        .. versionchanged:: 1.16.3\n-            Made default False in response to CVE-2019-6446.\n-\n-    pickle_kwargs : dict, optional\n-        Additional keyword arguments to pass on to pickle.load.\n-        These are only useful when loading object arrays saved on\n-        Python 2 when using Python 3.\n-    max_header_size : int, optional\n-        Maximum allowed size of the header.  Large headers may not be safe\n-        to load securely and thus require explicitly passing a larger value.\n-        See :py:func:`ast.literal_eval()` for details.\n-        This option is ignored when `allow_pickle` is passed.  In that case\n-        the file is by definition trusted and the limit is unnecessary.\n-\n-    Parameters\n-    ----------\n-    fid : file, str, or pathlib.Path\n-        The zipped archive to open. This is either a file-like object\n-        or a string containing the path to the archive.\n-    own_fid : bool, optional\n-        Whether NpzFile should close the file handle.\n-        Requires that `fid` is a file-like object.\n-\n-    Examples\n-    --------\n-    >>> from tempfile import TemporaryFile\n-    >>> outfile = TemporaryFile()\n-    >>> x = np.arange(10)\n-    >>> y = np.sin(x)\n-    >>> np.savez(outfile, x=x, y=y)\n-    >>> _ = outfile.seek(0)\n-\n-    >>> npz = np.load(outfile)\n-    >>> isinstance(npz, np.lib.npyio.NpzFile)\n-    True\n-    >>> npz\n-    NpzFile 'object' with keys x, y\n-    >>> sorted(npz.files)\n-    ['x', 'y']\n-    >>> npz['x']  # getitem access\n-    array([0, 1, 2, 3, 4, 5, 6, 7, 8, 9])\n-    >>> npz.f.x  # attribute lookup\n-    array([0, 1, 2, 3, 4, 5, 6, 7, 8, 9])\n-\n-    \"\"\"\n-    # Make __exit__ safe if zipfile_factory raises an exception\n-    zip = None\n-    fid = None\n-    _MAX_REPR_ARRAY_COUNT = 5\n-\n-    def __init__(self, fid, own_fid=False, allow_pickle=False,\n-                 pickle_kwargs=None, *,\n-                 max_header_size=format._MAX_HEADER_SIZE):\n-        # Import is postponed to here since zipfile depends on gzip, an\n-        # optional component of the so-called standard library.\n-        _zip = zipfile_factory(fid)\n-        self._files = _zip.namelist()\n-        self.files = []\n-        self.allow_pickle = allow_pickle\n-        self.max_header_size = max_header_size\n-        self.pickle_kwargs = pickle_kwargs\n-        for x in self._files:\n-            if x.endswith('.npy'):\n-                self.files.append(x[:-4])\n-            else:\n-                self.files.append(x)\n-        self.zip = _zip\n-        self.f = BagObj(self)\n-        if own_fid:\n-            self.fid = fid\n-\n-    def __enter__(self):\n-        return self\n-\n-    def __exit__(self, exc_type, exc_value, traceback):\n-        self.close()\n-\n-    def close(self):\n-        \"\"\"\n-        Close the file.\n-\n-        \"\"\"\n-        if self.zip is not None:\n-            self.zip.close()\n-            self.zip = None\n-        if self.fid is not None:\n-            self.fid.close()\n-            self.fid = None\n-        self.f = None  # break reference cycle\n-\n-    def __del__(self):\n-        self.close()\n-\n-    # Implement the Mapping ABC\n-    def __iter__(self):\n-        return iter(self.files)\n-\n-    def __len__(self):\n-        return len(self.files)\n-\n-    def __getitem__(self, key):\n-        # FIXME: This seems like it will copy strings around\n-        #   more than is strictly necessary.  The zipfile\n-        #   will read the string and then\n-        #   the format.read_array will copy the string\n-        #   to another place in memory.\n-        #   It would be better if the zipfile could read\n-        #   (or at least uncompress) the data\n-        #   directly into the array memory.\n-        member = False\n-        if key in self._files:\n-            member = True\n-        elif key in self.files:\n-            member = True\n-            key += '.npy'\n-        if member:\n-            bytes = self.zip.open(key)\n-            magic = bytes.read(len(format.MAGIC_PREFIX))\n-            bytes.close()\n-            if magic == format.MAGIC_PREFIX:\n-                bytes = self.zip.open(key)\n-                return format.read_array(bytes,\n-                                         allow_pickle=self.allow_pickle,\n-                                         pickle_kwargs=self.pickle_kwargs,\n-                                         max_header_size=self.max_header_size)\n-            else:\n-                return self.zip.read(key)\n-        else:\n-            raise KeyError(f\"{key} is not a file in the archive\")\n-\n-    def __contains__(self, key):\n-        return (key in self._files or key in self.files)\n-\n-    def __repr__(self):\n-        # Get filename or default to `object`\n-        if isinstance(self.fid, str):\n-            filename = self.fid\n-        else:\n-            filename = getattr(self.fid, \"name\", \"object\")\n-\n-        # Get the name of arrays\n-        array_names = ', '.join(self.files[:self._MAX_REPR_ARRAY_COUNT])\n-        if len(self.files) > self._MAX_REPR_ARRAY_COUNT:\n-            array_names += \"...\"\n-        return f\"NpzFile {filename!r} with keys: {array_names}\"\n-\n-\n-@set_module('numpy')\n-def load(file, mmap_mode=None, allow_pickle=False, fix_imports=True,\n-         encoding='ASCII', *, max_header_size=format._MAX_HEADER_SIZE):\n-    \"\"\"\n-    Load arrays or pickled objects from ``.npy``, ``.npz`` or pickled files.\n-\n-    .. warning:: Loading files that contain object arrays uses the ``pickle``\n-                 module, which is not secure against erroneous or maliciously\n-                 constructed data. Consider passing ``allow_pickle=False`` to\n-                 load data that is known not to contain object arrays for the\n-                 safer handling of untrusted sources.\n-\n-    Parameters\n-    ----------\n-    file : file-like object, string, or pathlib.Path\n-        The file to read. File-like objects must support the\n-        ``seek()`` and ``read()`` methods and must always\n-        be opened in binary mode.  Pickled files require that the\n-        file-like object support the ``readline()`` method as well.\n-    mmap_mode : {None, 'r+', 'r', 'w+', 'c'}, optional\n-        If not None, then memory-map the file, using the given mode (see\n-        `numpy.memmap` for a detailed description of the modes).  A\n-        memory-mapped array is kept on disk. However, it can be accessed\n-        and sliced like any ndarray.  Memory mapping is especially useful\n-        for accessing small fragments of large files without reading the\n-        entire file into memory.\n-    allow_pickle : bool, optional\n-        Allow loading pickled object arrays stored in npy files. Reasons for\n-        disallowing pickles include security, as loading pickled data can\n-        execute arbitrary code. If pickles are disallowed, loading object\n-        arrays will fail. Default: False\n-\n-        .. versionchanged:: 1.16.3\n-            Made default False in response to CVE-2019-6446.\n-\n-    fix_imports : bool, optional\n-        Only useful when loading Python 2 generated pickled files on Python 3,\n-        which includes npy/npz files containing object arrays. If `fix_imports`\n-        is True, pickle will try to map the old Python 2 names to the new names\n-        used in Python 3.\n-    encoding : str, optional\n-        What encoding to use when reading Python 2 strings. Only useful when\n-        loading Python 2 generated pickled files in Python 3, which includes\n-        npy/npz files containing object arrays. Values other than 'latin1',\n-        'ASCII', and 'bytes' are not allowed, as they can corrupt numerical\n-        data. Default: 'ASCII'\n-    max_header_size : int, optional\n-        Maximum allowed size of the header.  Large headers may not be safe\n-        to load securely and thus require explicitly passing a larger value.\n-        See :py:func:`ast.literal_eval()` for details.\n-        This option is ignored when `allow_pickle` is passed.  In that case\n-        the file is by definition trusted and the limit is unnecessary.\n-\n-    Returns\n-    -------\n-    result : array, tuple, dict, etc.\n-        Data stored in the file. For ``.npz`` files, the returned instance\n-        of NpzFile class must be closed to avoid leaking file descriptors.\n-\n-    Raises\n-    ------\n-    OSError\n-        If the input file does not exist or cannot be read.\n-    UnpicklingError\n-        If ``allow_pickle=True``, but the file cannot be loaded as a pickle.\n-    ValueError\n-        The file contains an object array, but ``allow_pickle=False`` given.\n-    EOFError\n-        When calling ``np.load`` multiple times on the same file handle,\n-        if all data has already been read\n-\n-    See Also\n-    --------\n-    save, savez, savez_compressed, loadtxt\n-    memmap : Create a memory-map to an array stored in a file on disk.\n-    lib.format.open_memmap : Create or load a memory-mapped ``.npy`` file.\n-\n-    Notes\n-    -----\n-    - If the file contains pickle data, then whatever object is stored\n-      in the pickle is returned.\n-    - If the file is a ``.npy`` file, then a single array is returned.\n-    - If the file is a ``.npz`` file, then a dictionary-like object is\n-      returned, containing ``{filename: array}`` key-value pairs, one for\n-      each file in the archive.\n-    - If the file is a ``.npz`` file, the returned value supports the\n-      context manager protocol in a similar fashion to the open function::\n-\n-        with load('foo.npz') as data:\n-            a = data['a']\n-\n-      The underlying file descriptor is closed when exiting the 'with'\n-      block.\n-\n-    Examples\n-    --------\n-    Store data to disk, and load it again:\n-\n-    >>> np.save('/tmp/123', np.array([[1, 2, 3], [4, 5, 6]]))\n-    >>> np.load('/tmp/123.npy')\n-    array([[1, 2, 3],\n-           [4, 5, 6]])\n-\n-    Store compressed data to disk, and load it again:\n-\n-    >>> a=np.array([[1, 2, 3], [4, 5, 6]])\n-    >>> b=np.array([1, 2])\n-    >>> np.savez('/tmp/123.npz', a=a, b=b)\n-    >>> data = np.load('/tmp/123.npz')\n-    >>> data['a']\n-    array([[1, 2, 3],\n-           [4, 5, 6]])\n-    >>> data['b']\n-    array([1, 2])\n-    >>> data.close()\n-\n-    Mem-map the stored array, and then access the second row\n-    directly from disk:\n-\n-    >>> X = np.load('/tmp/123.npy', mmap_mode='r')\n-    >>> X[1, :]\n-    memmap([4, 5, 6])\n-\n-    \"\"\"\n-    if encoding not in ('ASCII', 'latin1', 'bytes'):\n-        # The 'encoding' value for pickle also affects what encoding\n-        # the serialized binary data of NumPy arrays is loaded\n-        # in. Pickle does not pass on the encoding information to\n-        # NumPy. The unpickling code in numpy.core.multiarray is\n-        # written to assume that unicode data appearing where binary\n-        # should be is in 'latin1'. 'bytes' is also safe, as is 'ASCII'.\n-        #\n-        # Other encoding values can corrupt binary data, and we\n-        # purposefully disallow them. For the same reason, the errors=\n-        # argument is not exposed, as values other than 'strict'\n-        # result can similarly silently corrupt numerical data.\n-        raise ValueError(\"encoding must be 'ASCII', 'latin1', or 'bytes'\")\n-\n-    pickle_kwargs = dict(encoding=encoding, fix_imports=fix_imports)\n-\n-    with contextlib.ExitStack() as stack:\n-        if hasattr(file, 'read'):\n-            fid = file\n-            own_fid = False\n-        else:\n-            fid = stack.enter_context(open(os.fspath(file), \"rb\"))\n-            own_fid = True\n-\n-        # Code to distinguish from NumPy binary files and pickles.\n-        _ZIP_PREFIX = b'PK\\x03\\x04'\n-        _ZIP_SUFFIX = b'PK\\x05\\x06' # empty zip files start with this\n-        N = len(format.MAGIC_PREFIX)\n-        magic = fid.read(N)\n-        if not magic:\n-            raise EOFError(\"No data left in file\")\n-        # If the file size is less than N, we need to make sure not\n-        # to seek past the beginning of the file\n-        fid.seek(-min(N, len(magic)), 1)  # back-up\n-        if magic.startswith(_ZIP_PREFIX) or magic.startswith(_ZIP_SUFFIX):\n-            # zip-file (assume .npz)\n-            # Potentially transfer file ownership to NpzFile\n-            stack.pop_all()\n-            ret = NpzFile(fid, own_fid=own_fid, allow_pickle=allow_pickle,\n-                          pickle_kwargs=pickle_kwargs,\n-                          max_header_size=max_header_size)\n-            return ret\n-        elif magic == format.MAGIC_PREFIX:\n-            # .npy file\n-            if mmap_mode:\n-                if allow_pickle:\n-                    max_header_size = 2**64\n-                return format.open_memmap(file, mode=mmap_mode,\n-                                          max_header_size=max_header_size)\n-            else:\n-                return format.read_array(fid, allow_pickle=allow_pickle,\n-                                         pickle_kwargs=pickle_kwargs,\n-                                         max_header_size=max_header_size)\n-        else:\n-            # Try a pickle\n-            if not allow_pickle:\n-                raise ValueError(\"Cannot load file containing pickled data \"\n-                                 \"when allow_pickle=False\")\n-            try:\n-                return pickle.load(fid, **pickle_kwargs)\n-            except Exception as e:\n-                raise pickle.UnpicklingError(\n-                    f\"Failed to interpret file {file!r} as a pickle\") from e\n-\n-\n-def _save_dispatcher(file, arr, allow_pickle=None, fix_imports=None):\n-    return (arr,)\n-\n-\n-@array_function_dispatch(_save_dispatcher)\n-def save(file, arr, allow_pickle=True, fix_imports=True):\n-    \"\"\"\n-    Save an array to a binary file in NumPy ``.npy`` format.\n-\n-    Parameters\n-    ----------\n-    file : file, str, or pathlib.Path\n-        File or filename to which the data is saved.  If file is a file-object,\n-        then the filename is unchanged.  If file is a string or Path, a ``.npy``\n-        extension will be appended to the filename if it does not already\n-        have one.\n-    arr : array_like\n-        Array data to be saved.\n-    allow_pickle : bool, optional\n-        Allow saving object arrays using Python pickles. Reasons for disallowing\n-        pickles include security (loading pickled data can execute arbitrary\n-        code) and portability (pickled objects may not be loadable on different\n-        Python installations, for example if the stored objects require libraries\n-        that are not available, and not all pickled data is compatible between\n-        Python 2 and Python 3).\n-        Default: True\n-    fix_imports : bool, optional\n-        Only useful in forcing objects in object arrays on Python 3 to be\n-        pickled in a Python 2 compatible way. If `fix_imports` is True, pickle\n-        will try to map the new Python 3 names to the old module names used in\n-        Python 2, so that the pickle data stream is readable with Python 2.\n-\n-    See Also\n-    --------\n-    savez : Save several arrays into a ``.npz`` archive\n-    savetxt, load\n-\n-    Notes\n-    -----\n-    For a description of the ``.npy`` format, see :py:mod:`numpy.lib.format`.\n-\n-    Any data saved to the file is appended to the end of the file.\n-\n-    Examples\n-    --------\n-    >>> from tempfile import TemporaryFile\n-    >>> outfile = TemporaryFile()\n-\n-    >>> x = np.arange(10)\n-    >>> np.save(outfile, x)\n-\n-    >>> _ = outfile.seek(0) # Only needed here to simulate closing & reopening file\n-    >>> np.load(outfile)\n-    array([0, 1, 2, 3, 4, 5, 6, 7, 8, 9])\n-\n-\n-    >>> with open('test.npy', 'wb') as f:\n-    ...     np.save(f, np.array([1, 2]))\n-    ...     np.save(f, np.array([1, 3]))\n-    >>> with open('test.npy', 'rb') as f:\n-    ...     a = np.load(f)\n-    ...     b = np.load(f)\n-    >>> print(a, b)\n-    # [1 2] [1 3]\n-    \"\"\"\n-    if hasattr(file, 'write'):\n-        file_ctx = contextlib.nullcontext(file)\n-    else:\n-        file = os.fspath(file)\n-        if not file.endswith('.npy'):\n-            file = file + '.npy'\n-        file_ctx = open(file, \"wb\")\n-\n-    with file_ctx as fid:\n-        arr = np.asanyarray(arr)\n-        format.write_array(fid, arr, allow_pickle=allow_pickle,\n-                           pickle_kwargs=dict(fix_imports=fix_imports))\n-\n-\n-def _savez_dispatcher(file, *args, **kwds):\n-    yield from args\n-    yield from kwds.values()\n-\n-\n-@array_function_dispatch(_savez_dispatcher)\n-def savez(file, *args, **kwds):\n-    \"\"\"Save several arrays into a single file in uncompressed ``.npz`` format.\n-\n-    Provide arrays as keyword arguments to store them under the\n-    corresponding name in the output file: ``savez(fn, x=x, y=y)``.\n-\n-    If arrays are specified as positional arguments, i.e., ``savez(fn,\n-    x, y)``, their names will be `arr_0`, `arr_1`, etc.\n-\n-    Parameters\n-    ----------\n-    file : file, str, or pathlib.Path\n-        Either the filename (string) or an open file (file-like object)\n-        where the data will be saved. If file is a string or a Path, the\n-        ``.npz`` extension will be appended to the filename if it is not\n-        already there.\n-    args : Arguments, optional\n-        Arrays to save to the file. Please use keyword arguments (see\n-        `kwds` below) to assign names to arrays.  Arrays specified as\n-        args will be named \"arr_0\", \"arr_1\", and so on.\n-    kwds : Keyword arguments, optional\n-        Arrays to save to the file. Each array will be saved to the\n-        output file with its corresponding keyword name.\n-\n-    Returns\n-    -------\n-    None\n-\n-    See Also\n-    --------\n-    save : Save a single array to a binary file in NumPy format.\n-    savetxt : Save an array to a file as plain text.\n-    savez_compressed : Save several arrays into a compressed ``.npz`` archive\n-\n-    Notes\n-    -----\n-    The ``.npz`` file format is a zipped archive of files named after the\n-    variables they contain.  The archive is not compressed and each file\n-    in the archive contains one variable in ``.npy`` format. For a\n-    description of the ``.npy`` format, see :py:mod:`numpy.lib.format`.\n-\n-    When opening the saved ``.npz`` file with `load` a `NpzFile` object is\n-    returned. This is a dictionary-like object which can be queried for\n-    its list of arrays (with the ``.files`` attribute), and for the arrays\n-    themselves.\n-\n-    Keys passed in `kwds` are used as filenames inside the ZIP archive.\n-    Therefore, keys should be valid filenames; e.g., avoid keys that begin with\n-    ``/`` or contain ``.``.\n-\n-    When naming variables with keyword arguments, it is not possible to name a\n-    variable ``file``, as this would cause the ``file`` argument to be defined\n-    twice in the call to ``savez``.\n-\n-    Examples\n-    --------\n-    >>> from tempfile import TemporaryFile\n-    >>> outfile = TemporaryFile()\n-    >>> x = np.arange(10)\n-    >>> y = np.sin(x)\n-\n-    Using `savez` with \\\\*args, the arrays are saved with default names.\n-\n-    >>> np.savez(outfile, x, y)\n-    >>> _ = outfile.seek(0) # Only needed here to simulate closing & reopening file\n-    >>> npzfile = np.load(outfile)\n-    >>> npzfile.files\n-    ['arr_0', 'arr_1']\n-    >>> npzfile['arr_0']\n-    array([0, 1, 2, 3, 4, 5, 6, 7, 8, 9])\n-\n-    Using `savez` with \\\\**kwds, the arrays are saved with the keyword names.\n-\n-    >>> outfile = TemporaryFile()\n-    >>> np.savez(outfile, x=x, y=y)\n-    >>> _ = outfile.seek(0)\n-    >>> npzfile = np.load(outfile)\n-    >>> sorted(npzfile.files)\n-    ['x', 'y']\n-    >>> npzfile['x']\n-    array([0, 1, 2, 3, 4, 5, 6, 7, 8, 9])\n-\n-    \"\"\"\n-    _savez(file, args, kwds, False)\n-\n-\n-def _savez_compressed_dispatcher(file, *args, **kwds):\n-    yield from args\n-    yield from kwds.values()\n-\n-\n-@array_function_dispatch(_savez_compressed_dispatcher)\n-def savez_compressed(file, *args, **kwds):\n-    \"\"\"\n-    Save several arrays into a single file in compressed ``.npz`` format.\n-\n-    Provide arrays as keyword arguments to store them under the\n-    corresponding name in the output file: ``savez_compressed(fn, x=x, y=y)``.\n-\n-    If arrays are specified as positional arguments, i.e.,\n-    ``savez_compressed(fn, x, y)``, their names will be `arr_0`, `arr_1`, etc.\n-\n-    Parameters\n-    ----------\n-    file : file, str, or pathlib.Path\n-        Either the filename (string) or an open file (file-like object)\n-        where the data will be saved. If file is a string or a Path, the\n-        ``.npz`` extension will be appended to the filename if it is not\n-        already there.\n-    args : Arguments, optional\n-        Arrays to save to the file. Please use keyword arguments (see\n-        `kwds` below) to assign names to arrays.  Arrays specified as\n-        args will be named \"arr_0\", \"arr_1\", and so on.\n-    kwds : Keyword arguments, optional\n-        Arrays to save to the file. Each array will be saved to the\n-        output file with its corresponding keyword name.\n-\n-    Returns\n-    -------\n-    None\n-\n-    See Also\n-    --------\n-    numpy.save : Save a single array to a binary file in NumPy format.\n-    numpy.savetxt : Save an array to a file as plain text.\n-    numpy.savez : Save several arrays into an uncompressed ``.npz`` file format\n-    numpy.load : Load the files created by savez_compressed.\n-\n-    Notes\n-    -----\n-    The ``.npz`` file format is a zipped archive of files named after the\n-    variables they contain.  The archive is compressed with\n-    ``zipfile.ZIP_DEFLATED`` and each file in the archive contains one variable\n-    in ``.npy`` format. For a description of the ``.npy`` format, see\n-    :py:mod:`numpy.lib.format`.\n-\n-\n-    When opening the saved ``.npz`` file with `load` a `NpzFile` object is\n-    returned. This is a dictionary-like object which can be queried for\n-    its list of arrays (with the ``.files`` attribute), and for the arrays\n-    themselves.\n-\n-    Examples\n-    --------\n-    >>> test_array = np.random.rand(3, 2)\n-    >>> test_vector = np.random.rand(4)\n-    >>> np.savez_compressed('/tmp/123', a=test_array, b=test_vector)\n-    >>> loaded = np.load('/tmp/123.npz')\n-    >>> print(np.array_equal(test_array, loaded['a']))\n-    True\n-    >>> print(np.array_equal(test_vector, loaded['b']))\n-    True\n-\n-    \"\"\"\n-    _savez(file, args, kwds, True)\n-\n-\n-def _savez(file, args, kwds, compress, allow_pickle=True, pickle_kwargs=None):\n-    # Import is postponed to here since zipfile depends on gzip, an optional\n-    # component of the so-called standard library.\n-    import zipfile\n-\n-    if not hasattr(file, 'write'):\n-        file = os.fspath(file)\n-        if not file.endswith('.npz'):\n-            file = file + '.npz'\n-\n-    namedict = kwds\n-    for i, val in enumerate(args):\n-        key = 'arr_%d' % i\n-        if key in namedict.keys():\n-            raise ValueError(\n-                \"Cannot use un-named variables and keyword %s\" % key)\n-        namedict[key] = val\n-\n-    if compress:\n-        compression = zipfile.ZIP_DEFLATED\n-    else:\n-        compression = zipfile.ZIP_STORED\n-\n-    zipf = zipfile_factory(file, mode=\"w\", compression=compression)\n-\n-    for key, val in namedict.items():\n-        fname = key + '.npy'\n-        val = np.asanyarray(val)\n-        # always force zip64, gh-10776\n-        with zipf.open(fname, 'w', force_zip64=True) as fid:\n-            format.write_array(fid, val,\n-                               allow_pickle=allow_pickle,\n-                               pickle_kwargs=pickle_kwargs)\n-\n-    zipf.close()\n-\n-\n-def _ensure_ndmin_ndarray_check_param(ndmin):\n-    \"\"\"Just checks if the param ndmin is supported on\n-        _ensure_ndmin_ndarray. It is intended to be used as\n-        verification before running anything expensive.\n-        e.g. loadtxt, genfromtxt\n-    \"\"\"\n-    # Check correctness of the values of `ndmin`\n-    if ndmin not in [0, 1, 2]:\n-        raise ValueError(f\"Illegal value of ndmin keyword: {ndmin}\")\n-\n-def _ensure_ndmin_ndarray(a, *, ndmin: int):\n-    \"\"\"This is a helper function of loadtxt and genfromtxt to ensure\n-        proper minimum dimension as requested\n-\n-        ndim : int. Supported values 1, 2, 3\n-                    ^^ whenever this changes, keep in sync with\n-                       _ensure_ndmin_ndarray_check_param\n-    \"\"\"\n-    # Verify that the array has at least dimensions `ndmin`.\n-    # Tweak the size and shape of the arrays - remove extraneous dimensions\n-    if a.ndim > ndmin:\n-        a = np.squeeze(a)\n-    # and ensure we have the minimum number of dimensions asked for\n-    # - has to be in this order for the odd case ndmin=1, a.squeeze().ndim=0\n-    if a.ndim < ndmin:\n-        if ndmin == 1:\n-            a = np.atleast_1d(a)\n-        elif ndmin == 2:\n-            a = np.atleast_2d(a).T\n-\n-    return a\n-\n-\n-# amount of lines loadtxt reads in one chunk, can be overridden for testing\n-_loadtxt_chunksize = 50000\n-\n-\n-def _check_nonneg_int(value, name=\"argument\"):\n-    try:\n-        operator.index(value)\n-    except TypeError:\n-        raise TypeError(f\"{name} must be an integer\") from None\n-    if value < 0:\n-        raise ValueError(f\"{name} must be nonnegative\")\n-\n-\n-def _preprocess_comments(iterable, comments, encoding):\n-    \"\"\"\n-    Generator that consumes a line iterated iterable and strips out the\n-    multiple (or multi-character) comments from lines.\n-    This is a pre-processing step to achieve feature parity with loadtxt\n-    (we assume that this feature is a nieche feature).\n-    \"\"\"\n-    for line in iterable:\n-        if isinstance(line, bytes):\n-            # Need to handle conversion here, or the splitting would fail\n-            line = line.decode(encoding)\n-\n-        for c in comments:\n-            line = line.split(c, 1)[0]\n-\n-        yield line\n-\n-\n-# The number of rows we read in one go if confronted with a parametric dtype\n-_loadtxt_chunksize = 50000\n-\n-\n-def _read(fname, *, delimiter=',', comment='#', quote='\"',\n-          imaginary_unit='j', usecols=None, skiplines=0,\n-          max_rows=None, converters=None, ndmin=None, unpack=False,\n-          dtype=np.float64, encoding=\"bytes\"):\n-    r\"\"\"\n-    Read a NumPy array from a text file.\n-    This is a helper function for loadtxt.\n-\n-    Parameters\n-    ----------\n-    fname : file, str, or pathlib.Path\n-        The filename or the file to be read.\n-    delimiter : str, optional\n-        Field delimiter of the fields in line of the file.\n-        Default is a comma, ','.  If None any sequence of whitespace is\n-        considered a delimiter.\n-    comment : str or sequence of str or None, optional\n-        Character that begins a comment.  All text from the comment\n-        character to the end of the line is ignored.\n-        Multiple comments or multiple-character comment strings are supported,\n-        but may be slower and `quote` must be empty if used.\n-        Use None to disable all use of comments.\n-    quote : str or None, optional\n-        Character that is used to quote string fields. Default is '\"'\n-        (a double quote). Use None to disable quote support.\n-    imaginary_unit : str, optional\n-        Character that represent the imaginay unit `sqrt(-1)`.\n-        Default is 'j'.\n-    usecols : array_like, optional\n-        A one-dimensional array of integer column numbers.  These are the\n-        columns from the file to be included in the array.  If this value\n-        is not given, all the columns are used.\n-    skiplines : int, optional\n-        Number of lines to skip before interpreting the data in the file.\n-    max_rows : int, optional\n-        Maximum number of rows of data to read.  Default is to read the\n-        entire file.\n-    converters : dict or callable, optional\n-        A function to parse all columns strings into the desired value, or\n-        a dictionary mapping column number to a parser function.\n-        E.g. if column 0 is a date string: ``converters = {0: datestr2num}``.\n-        Converters can also be used to provide a default value for missing\n-        data, e.g. ``converters = lambda s: float(s.strip() or 0)`` will\n-        convert empty fields to 0.\n-        Default: None\n-    ndmin : int, optional\n-        Minimum dimension of the array returned.\n-        Allowed values are 0, 1 or 2.  Default is 0.\n-    unpack : bool, optional\n-        If True, the returned array is transposed, so that arguments may be\n-        unpacked using ``x, y, z = read(...)``.  When used with a structured\n-        data-type, arrays are returned for each field.  Default is False.\n-    dtype : numpy data type\n-        A NumPy dtype instance, can be a structured dtype to map to the\n-        columns of the file.\n-    encoding : str, optional\n-        Encoding used to decode the inputfile. The special value 'bytes'\n-        (the default) enables backwards-compatible behavior for `converters`,\n-        ensuring that inputs to the converter functions are encoded\n-        bytes objects. The special value 'bytes' has no additional effect if\n-        ``converters=None``. If encoding is ``'bytes'`` or ``None``, the\n-        default system encoding is used.\n-\n-    Returns\n-    -------\n-    ndarray\n-        NumPy array.\n-    \"\"\"\n-    # Handle special 'bytes' keyword for encoding\n-    byte_converters = False\n-    if encoding == 'bytes':\n-        encoding = None\n-        byte_converters = True\n-\n-    if dtype is None:\n-        raise TypeError(\"a dtype must be provided.\")\n-    dtype = np.dtype(dtype)\n-\n-    read_dtype_via_object_chunks = None\n-    if dtype.kind in 'SUM' and (\n-            dtype == \"S0\" or dtype == \"U0\" or dtype == \"M8\" or dtype == 'm8'):\n-        # This is a legacy \"flexible\" dtype.  We do not truly support\n-        # parametric dtypes currently (no dtype discovery step in the core),\n-        # but have to support these for backward compatibility.\n-        read_dtype_via_object_chunks = dtype\n-        dtype = np.dtype(object)\n-\n-    if usecols is not None:\n-        # Allow usecols to be a single int or a sequence of ints, the C-code\n-        # handles the rest\n-        try:\n-            usecols = list(usecols)\n-        except TypeError:\n-            usecols = [usecols]\n-\n-    _ensure_ndmin_ndarray_check_param(ndmin)\n-\n-    if comment is None:\n-        comments = None\n-    else:\n-        # assume comments are a sequence of strings\n-        if \"\" in comment:\n-            raise ValueError(\n-                \"comments cannot be an empty string. Use comments=None to \"\n-                \"disable comments.\"\n-            )\n-        comments = tuple(comment)\n-        comment = None\n-        if len(comments) == 0:\n-            comments = None  # No comments at all\n-        elif len(comments) == 1:\n-            # If there is only one comment, and that comment has one character,\n-            # the normal parsing can deal with it just fine.\n-            if isinstance(comments[0], str) and len(comments[0]) == 1:\n-                comment = comments[0]\n-                comments = None\n-        else:\n-            # Input validation if there are multiple comment characters\n-            if delimiter in comments:\n-                raise TypeError(\n-                    f\"Comment characters '{comments}' cannot include the \"\n-                    f\"delimiter '{delimiter}'\"\n-                )\n-\n-    # comment is now either a 1 or 0 character string or a tuple:\n-    if comments is not None:\n-        # Note: An earlier version support two character comments (and could\n-        #       have been extended to multiple characters, we assume this is\n-        #       rare enough to not optimize for.\n-        if quote is not None:\n-            raise ValueError(\n-                \"when multiple comments or a multi-character comment is \"\n-                \"given, quotes are not supported.  In this case quotechar \"\n-                \"must be set to None.\")\n-\n-    if len(imaginary_unit) != 1:\n-        raise ValueError('len(imaginary_unit) must be 1.')\n-\n-    _check_nonneg_int(skiplines)\n-    if max_rows is not None:\n-        _check_nonneg_int(max_rows)\n-    else:\n-        # Passing -1 to the C code means \"read the entire file\".\n-        max_rows = -1\n-\n-    fh_closing_ctx = contextlib.nullcontext()\n-    filelike = False\n-    try:\n-        if isinstance(fname, os.PathLike):\n-            fname = os.fspath(fname)\n-        if isinstance(fname, str):\n-            fh = np.lib._datasource.open(fname, 'rt', encoding=encoding)\n-            if encoding is None:\n-                encoding = getattr(fh, 'encoding', 'latin1')\n-\n-            fh_closing_ctx = contextlib.closing(fh)\n-            data = fh\n-            filelike = True\n-        else:\n-            if encoding is None:\n-                encoding = getattr(fname, 'encoding', 'latin1')\n-            data = iter(fname)\n-    except TypeError as e:\n-        raise ValueError(\n-            f\"fname must be a string, filehandle, list of strings,\\n\"\n-            f\"or generator. Got {type(fname)} instead.\") from e\n-\n-    with fh_closing_ctx:\n-        if comments is not None:\n-            if filelike:\n-                data = iter(data)\n-                filelike = False\n-            data = _preprocess_comments(data, comments, encoding)\n-\n-        if read_dtype_via_object_chunks is None:\n-            arr = _load_from_filelike(\n-                data, delimiter=delimiter, comment=comment, quote=quote,\n-                imaginary_unit=imaginary_unit,\n-                usecols=usecols, skiplines=skiplines, max_rows=max_rows,\n-                converters=converters, dtype=dtype,\n-                encoding=encoding, filelike=filelike,\n-                byte_converters=byte_converters)\n-\n-        else:\n-            # This branch reads the file into chunks of object arrays and then\n-            # casts them to the desired actual dtype.  This ensures correct\n-            # string-length and datetime-unit discovery (like `arr.astype()`).\n-            # Due to chunking, certain error reports are less clear, currently.\n-            if filelike:\n-                data = iter(data)  # cannot chunk when reading from file\n-\n-            c_byte_converters = False\n-            if read_dtype_via_object_chunks == \"S\":\n-                c_byte_converters = True  # Use latin1 rather than ascii\n-\n-            chunks = []\n-            while max_rows != 0:\n-                if max_rows < 0:\n-                    chunk_size = _loadtxt_chunksize\n-                else:\n-                    chunk_size = min(_loadtxt_chunksize, max_rows)\n-\n-                next_arr = _load_from_filelike(\n-                    data, delimiter=delimiter, comment=comment, quote=quote,\n-                    imaginary_unit=imaginary_unit,\n-                    usecols=usecols, skiplines=skiplines, max_rows=max_rows,\n-                    converters=converters, dtype=dtype,\n-                    encoding=encoding, filelike=filelike,\n-                    byte_converters=byte_converters,\n-                    c_byte_converters=c_byte_converters)\n-                # Cast here already.  We hope that this is better even for\n-                # large files because the storage is more compact.  It could\n-                # be adapted (in principle the concatenate could cast).\n-                chunks.append(next_arr.astype(read_dtype_via_object_chunks))\n-\n-                skiprows = 0  # Only have to skip for first chunk\n-                if max_rows >= 0:\n-                    max_rows -= chunk_size\n-                if len(next_arr) < chunk_size:\n-                    # There was less data than requested, so we are done.\n-                    break\n-\n-            # Need at least one chunk, but if empty, the last one may have\n-            # the wrong shape.\n-            if len(chunks) > 1 and len(chunks[-1]) == 0:\n-                del chunks[-1]\n-            if len(chunks) == 1:\n-                arr = chunks[0]\n-            else:\n-                arr = np.concatenate(chunks, axis=0)\n-\n-    # NOTE: ndmin works as advertised for structured dtypes, but normally\n-    #       these would return a 1D result plus the structured dimension,\n-    #       so ndmin=2 adds a third dimension even when no squeezing occurs.\n-    #       A `squeeze=False` could be a better solution (pandas uses squeeze).\n-    arr = _ensure_ndmin_ndarray(arr, ndmin=ndmin)\n-\n-    if arr.shape:\n-        if arr.shape[0] == 0:\n-            warnings.warn(\n-                f'loadtxt: input contained no data: \"{fname}\"',\n-                category=UserWarning,\n-                stacklevel=3\n-            )\n-\n-    if unpack:\n-        # Unpack structured dtypes if requested:\n-        dt = arr.dtype\n-        if dt.names is not None:\n-            # For structured arrays, return an array for each field.\n-            return [arr[field] for field in dt.names]\n-        else:\n-            return arr.T\n-    else:\n-        return arr\n-\n-\n-@set_array_function_like_doc\n-@set_module('numpy')\n-def loadtxt(fname, dtype=float, comments='#', delimiter=None,\n-            converters=None, skiprows=0, usecols=None, unpack=False,\n-            ndmin=0, encoding='bytes', max_rows=None, *, quotechar=None,\n-            like=None):\n-    r\"\"\"\n-    Load data from a text file.\n-\n-    Parameters\n-    ----------\n-    fname : file, str, pathlib.Path, list of str, generator\n-        File, filename, list, or generator to read.  If the filename\n-        extension is ``.gz`` or ``.bz2``, the file is first decompressed. Note\n-        that generators must return bytes or strings. The strings\n-        in a list or produced by a generator are treated as lines.\n-    dtype : data-type, optional\n-        Data-type of the resulting array; default: float.  If this is a\n-        structured data-type, the resulting array will be 1-dimensional, and\n-        each row will be interpreted as an element of the array.  In this\n-        case, the number of columns used must match the number of fields in\n-        the data-type.\n-    comments : str or sequence of str or None, optional\n-        The characters or list of characters used to indicate the start of a\n-        comment. None implies no comments. For backwards compatibility, byte\n-        strings will be decoded as 'latin1'. The default is '#'.\n-    delimiter : str, optional\n-        The character used to separate the values. For backwards compatibility,\n-        byte strings will be decoded as 'latin1'. The default is whitespace.\n-\n-        .. versionchanged:: 1.23.0\n-           Only single character delimiters are supported. Newline characters\n-           cannot be used as the delimiter.\n-\n-    converters : dict or callable, optional\n-        Converter functions to customize value parsing. If `converters` is\n-        callable, the function is applied to all columns, else it must be a\n-        dict that maps column number to a parser function.\n-        See examples for further details.\n-        Default: None.\n-\n-        .. versionchanged:: 1.23.0\n-           The ability to pass a single callable to be applied to all columns\n-           was added.\n-\n-    skiprows : int, optional\n-        Skip the first `skiprows` lines, including comments; default: 0.\n-    usecols : int or sequence, optional\n-        Which columns to read, with 0 being the first. For example,\n-        ``usecols = (1,4,5)`` will extract the 2nd, 5th and 6th columns.\n-        The default, None, results in all columns being read.\n-\n-        .. versionchanged:: 1.11.0\n-            When a single column has to be read it is possible to use\n-            an integer instead of a tuple. E.g ``usecols = 3`` reads the\n-            fourth column the same way as ``usecols = (3,)`` would.\n-    unpack : bool, optional\n-        If True, the returned array is transposed, so that arguments may be\n-        unpacked using ``x, y, z = loadtxt(...)``.  When used with a\n-        structured data-type, arrays are returned for each field.\n-        Default is False.\n-    ndmin : int, optional\n-        The returned array will have at least `ndmin` dimensions.\n-        Otherwise mono-dimensional axes will be squeezed.\n-        Legal values: 0 (default), 1 or 2.\n-\n-        .. versionadded:: 1.6.0\n-    encoding : str, optional\n-        Encoding used to decode the inputfile. Does not apply to input streams.\n-        The special value 'bytes' enables backward compatibility workarounds\n-        that ensures you receive byte arrays as results if possible and passes\n-        'latin1' encoded strings to converters. Override this value to receive\n-        unicode arrays and pass strings as input to converters.  If set to None\n-        the system default is used. The default value is 'bytes'.\n-\n-        .. versionadded:: 1.14.0\n-    max_rows : int, optional\n-        Read `max_rows` rows of content after `skiprows` lines. The default is\n-        to read all the rows. Note that empty rows containing no data such as\n-        empty lines and comment lines are not counted towards `max_rows`,\n-        while such lines are counted in `skiprows`.\n-\n-        .. versionadded:: 1.16.0\n-\n-        .. versionchanged:: 1.23.0\n-            Lines containing no data, including comment lines (e.g., lines\n-            starting with '#' or as specified via `comments`) are not counted\n-            towards `max_rows`.\n-    quotechar : unicode character or None, optional\n-        The character used to denote the start and end of a quoted item.\n-        Occurrences of the delimiter or comment characters are ignored within\n-        a quoted item. The default value is ``quotechar=None``, which means\n-        quoting support is disabled.\n-\n-        If two consecutive instances of `quotechar` are found within a quoted\n-        field, the first is treated as an escape character. See examples.\n-\n-        .. versionadded:: 1.23.0\n-    ${ARRAY_FUNCTION_LIKE}\n-\n-        .. versionadded:: 1.20.0\n-\n-    Returns\n-    -------\n-    out : ndarray\n-        Data read from the text file.\n-\n-    See Also\n-    --------\n-    load, fromstring, fromregex\n-    genfromtxt : Load data with missing values handled as specified.\n-    scipy.io.loadmat : reads MATLAB data files\n-\n-    Notes\n-    -----\n-    This function aims to be a fast reader for simply formatted files.  The\n-    `genfromtxt` function provides more sophisticated handling of, e.g.,\n-    lines with missing values.\n-\n-    Each row in the input text file must have the same number of values to be\n-    able to read all values. If all rows do not have same number of values, a\n-    subset of up to n columns (where n is the least number of values present\n-    in all rows) can be read by specifying the columns via `usecols`.\n-\n-    .. versionadded:: 1.10.0\n-\n-    The strings produced by the Python float.hex method can be used as\n-    input for floats.\n-\n-    Examples\n-    --------\n-    >>> from io import StringIO   # StringIO behaves like a file object\n-    >>> c = StringIO(\"0 1\\n2 3\")\n-    >>> np.loadtxt(c)\n-    array([[0., 1.],\n-           [2., 3.]])\n-\n-    >>> d = StringIO(\"M 21 72\\nF 35 58\")\n-    >>> np.loadtxt(d, dtype={'names': ('gender', 'age', 'weight'),\n-    ...                      'formats': ('S1', 'i4', 'f4')})\n-    array([(b'M', 21, 72.), (b'F', 35, 58.)],\n-          dtype=[('gender', 'S1'), ('age', '<i4'), ('weight', '<f4')])\n-\n-    >>> c = StringIO(\"1,0,2\\n3,0,4\")\n-    >>> x, y = np.loadtxt(c, delimiter=',', usecols=(0, 2), unpack=True)\n-    >>> x\n-    array([1., 3.])\n-    >>> y\n-    array([2., 4.])\n-\n-    The `converters` argument is used to specify functions to preprocess the\n-    text prior to parsing. `converters` can be a dictionary that maps\n-    preprocessing functions to each column:\n-\n-    >>> s = StringIO(\"1.618, 2.296\\n3.141, 4.669\\n\")\n-    >>> conv = {\n-    ...     0: lambda x: np.floor(float(x)),  # conversion fn for column 0\n-    ...     1: lambda x: np.ceil(float(x)),  # conversion fn for column 1\n-    ... }\n-    >>> np.loadtxt(s, delimiter=\",\", converters=conv)\n-    array([[1., 3.],\n-           [3., 5.]])\n-\n-    `converters` can be a callable instead of a dictionary, in which case it\n-    is applied to all columns:\n-\n-    >>> s = StringIO(\"0xDE 0xAD\\n0xC0 0xDE\")\n-    >>> import functools\n-    >>> conv = functools.partial(int, base=16)\n-    >>> np.loadtxt(s, converters=conv)\n-    array([[222., 173.],\n-           [192., 222.]])\n-\n-    This example shows how `converters` can be used to convert a field\n-    with a trailing minus sign into a negative number.\n-\n-    >>> s = StringIO('10.01 31.25-\\n19.22 64.31\\n17.57- 63.94')\n-    >>> def conv(fld):\n-    ...     return -float(fld[:-1]) if fld.endswith(b'-') else float(fld)\n-    ...\n-    >>> np.loadtxt(s, converters=conv)\n-    array([[ 10.01, -31.25],\n-           [ 19.22,  64.31],\n-           [-17.57,  63.94]])\n-\n-    Using a callable as the converter can be particularly useful for handling\n-    values with different formatting, e.g. floats with underscores:\n-\n-    >>> s = StringIO(\"1 2.7 100_000\")\n-    >>> np.loadtxt(s, converters=float)\n-    array([1.e+00, 2.7e+00, 1.e+05])\n-\n-    This idea can be extended to automatically handle values specified in\n-    many different formats:\n-\n-    >>> def conv(val):\n-    ...     try:\n-    ...         return float(val)\n-    ...     except ValueError:\n-    ...         return float.fromhex(val)\n-    >>> s = StringIO(\"1, 2.5, 3_000, 0b4, 0x1.4000000000000p+2\")\n-    >>> np.loadtxt(s, delimiter=\",\", converters=conv, encoding=None)\n-    array([1.0e+00, 2.5e+00, 3.0e+03, 1.8e+02, 5.0e+00])\n-\n-    Note that with the default ``encoding=\"bytes\"``, the inputs to the\n-    converter function are latin-1 encoded byte strings. To deactivate the\n-    implicit encoding prior to conversion, use ``encoding=None``\n-\n-    >>> s = StringIO('10.01 31.25-\\n19.22 64.31\\n17.57- 63.94')\n-    >>> conv = lambda x: -float(x[:-1]) if x.endswith('-') else float(x)\n-    >>> np.loadtxt(s, converters=conv, encoding=None)\n-    array([[ 10.01, -31.25],\n-           [ 19.22,  64.31],\n-           [-17.57,  63.94]])\n-\n-    Support for quoted fields is enabled with the `quotechar` parameter.\n-    Comment and delimiter characters are ignored when they appear within a\n-    quoted item delineated by `quotechar`:\n-\n-    >>> s = StringIO('\"alpha, #42\", 10.0\\n\"beta, #64\", 2.0\\n')\n-    >>> dtype = np.dtype([(\"label\", \"U12\"), (\"value\", float)])\n-    >>> np.loadtxt(s, dtype=dtype, delimiter=\",\", quotechar='\"')\n-    array([('alpha, #42', 10.), ('beta, #64',  2.)],\n-          dtype=[('label', '<U12'), ('value', '<f8')])\n-\n-    Quoted fields can be separated by multiple whitespace characters:\n-\n-    >>> s = StringIO('\"alpha, #42\"       10.0\\n\"beta, #64\" 2.0\\n')\n-    >>> dtype = np.dtype([(\"label\", \"U12\"), (\"value\", float)])\n-    >>> np.loadtxt(s, dtype=dtype, delimiter=None, quotechar='\"')\n-    array([('alpha, #42', 10.), ('beta, #64',  2.)],\n-          dtype=[('label', '<U12'), ('value', '<f8')])\n-\n-    Two consecutive quote characters within a quoted field are treated as a\n-    single escaped character:\n-\n-    >>> s = StringIO('\"Hello, my name is \"\"Monty\"\"!\"')\n-    >>> np.loadtxt(s, dtype=\"U\", delimiter=\",\", quotechar='\"')\n-    array('Hello, my name is \"Monty\"!', dtype='<U26')\n-\n-    Read subset of columns when all rows do not contain equal number of values:\n-\n-    >>> d = StringIO(\"1 2\\n2 4\\n3 9 12\\n4 16 20\")\n-    >>> np.loadtxt(d, usecols=(0, 1))\n-    array([[ 1.,  2.],\n-           [ 2.,  4.],\n-           [ 3.,  9.],\n-           [ 4., 16.]])\n-\n-    \"\"\"\n-\n-    if like is not None:\n-        return _loadtxt_with_like(\n-            like, fname, dtype=dtype, comments=comments, delimiter=delimiter,\n-            converters=converters, skiprows=skiprows, usecols=usecols,\n-            unpack=unpack, ndmin=ndmin, encoding=encoding,\n-            max_rows=max_rows\n-        )\n-\n-    if isinstance(delimiter, bytes):\n-        delimiter.decode(\"latin1\")\n-\n-    if dtype is None:\n-        dtype = np.float64\n-\n-    comment = comments\n-    # Control character type conversions for Py3 convenience\n-    if comment is not None:\n-        if isinstance(comment, (str, bytes)):\n-            comment = [comment]\n-        comment = [\n-            x.decode('latin1') if isinstance(x, bytes) else x for x in comment]\n-    if isinstance(delimiter, bytes):\n-        delimiter = delimiter.decode('latin1')\n-\n-    arr = _read(fname, dtype=dtype, comment=comment, delimiter=delimiter,\n-                converters=converters, skiplines=skiprows, usecols=usecols,\n-                unpack=unpack, ndmin=ndmin, encoding=encoding,\n-                max_rows=max_rows, quote=quotechar)\n-\n-    return arr\n-\n-\n-_loadtxt_with_like = array_function_dispatch()(loadtxt)\n-\n-\n-def _savetxt_dispatcher(fname, X, fmt=None, delimiter=None, newline=None,\n-                        header=None, footer=None, comments=None,\n-                        encoding=None):\n-    return (X,)\n-\n-\n-@array_function_dispatch(_savetxt_dispatcher)\n-def savetxt(fname, X, fmt='%.18e', delimiter=' ', newline='\\n', header='',\n-            footer='', comments='# ', encoding=None):\n-    \"\"\"\n-    Save an array to a text file.\n-\n-    Parameters\n-    ----------\n-    fname : filename, file handle or pathlib.Path\n-        If the filename ends in ``.gz``, the file is automatically saved in\n-        compressed gzip format.  `loadtxt` understands gzipped files\n-        transparently.\n-    X : 1D or 2D array_like\n-        Data to be saved to a text file.\n-    fmt : str or sequence of strs, optional\n-        A single format (%10.5f), a sequence of formats, or a\n-        multi-format string, e.g. 'Iteration %d -- %10.5f', in which\n-        case `delimiter` is ignored. For complex `X`, the legal options\n-        for `fmt` are:\n-\n-        * a single specifier, `fmt='%.4e'`, resulting in numbers formatted\n-          like `' (%s+%sj)' % (fmt, fmt)`\n-        * a full string specifying every real and imaginary part, e.g.\n-          `' %.4e %+.4ej %.4e %+.4ej %.4e %+.4ej'` for 3 columns\n-        * a list of specifiers, one per column - in this case, the real\n-          and imaginary part must have separate specifiers,\n-          e.g. `['%.3e + %.3ej', '(%.15e%+.15ej)']` for 2 columns\n-    delimiter : str, optional\n-        String or character separating columns.\n-    newline : str, optional\n-        String or character separating lines.\n-\n-        .. versionadded:: 1.5.0\n-    header : str, optional\n-        String that will be written at the beginning of the file.\n-\n-        .. versionadded:: 1.7.0\n-    footer : str, optional\n-        String that will be written at the end of the file.\n-\n-        .. versionadded:: 1.7.0\n-    comments : str, optional\n-        String that will be prepended to the ``header`` and ``footer`` strings,\n-        to mark them as comments. Default: '# ',  as expected by e.g.\n-        ``numpy.loadtxt``.\n-\n-        .. versionadded:: 1.7.0\n-    encoding : {None, str}, optional\n-        Encoding used to encode the outputfile. Does not apply to output\n-        streams. If the encoding is something other than 'bytes' or 'latin1'\n-        you will not be able to load the file in NumPy versions < 1.14. Default\n-        is 'latin1'.\n-\n-        .. versionadded:: 1.14.0\n-\n-\n-    See Also\n-    --------\n-    save : Save an array to a binary file in NumPy ``.npy`` format\n-    savez : Save several arrays into an uncompressed ``.npz`` archive\n-    savez_compressed : Save several arrays into a compressed ``.npz`` archive\n-\n-    Notes\n-    -----\n-    Further explanation of the `fmt` parameter\n-    (``%[flag]width[.precision]specifier``):\n-\n-    flags:\n-        ``-`` : left justify\n-\n-        ``+`` : Forces to precede result with + or -.\n-\n-        ``0`` : Left pad the number with zeros instead of space (see width).\n-\n-    width:\n-        Minimum number of characters to be printed. The value is not truncated\n-        if it has more characters.\n-\n-    precision:\n-        - For integer specifiers (eg. ``d,i,o,x``), the minimum number of\n-          digits.\n-        - For ``e, E`` and ``f`` specifiers, the number of digits to print\n-          after the decimal point.\n-        - For ``g`` and ``G``, the maximum number of significant digits.\n-        - For ``s``, the maximum number of characters.\n-\n-    specifiers:\n-        ``c`` : character\n-\n-        ``d`` or ``i`` : signed decimal integer\n-\n-        ``e`` or ``E`` : scientific notation with ``e`` or ``E``.\n-\n-        ``f`` : decimal floating point\n-\n-        ``g,G`` : use the shorter of ``e,E`` or ``f``\n-\n-        ``o`` : signed octal\n-\n-        ``s`` : string of characters\n-\n-        ``u`` : unsigned decimal integer\n-\n-        ``x,X`` : unsigned hexadecimal integer\n-\n-    This explanation of ``fmt`` is not complete, for an exhaustive\n-    specification see [1]_.\n-\n-    References\n-    ----------\n-    .. [1] `Format Specification Mini-Language\n-           <https://docs.python.org/library/string.html#format-specification-mini-language>`_,\n-           Python Documentation.\n-\n-    Examples\n-    --------\n-    >>> x = y = z = np.arange(0.0,5.0,1.0)\n-    >>> np.savetxt('test.out', x, delimiter=',')   # X is an array\n-    >>> np.savetxt('test.out', (x,y,z))   # x,y,z equal sized 1D arrays\n-    >>> np.savetxt('test.out', x, fmt='%1.4e')   # use exponential notation\n-\n-    \"\"\"\n-\n-    class WriteWrap:\n-        \"\"\"Convert to bytes on bytestream inputs.\n-\n-        \"\"\"\n-        def __init__(self, fh, encoding):\n-            self.fh = fh\n-            self.encoding = encoding\n-            self.do_write = self.first_write\n-\n-        def close(self):\n-            self.fh.close()\n-\n-        def write(self, v):\n-            self.do_write(v)\n-\n-        def write_bytes(self, v):\n-            if isinstance(v, bytes):\n-                self.fh.write(v)\n-            else:\n-                self.fh.write(v.encode(self.encoding))\n-\n-        def write_normal(self, v):\n-            self.fh.write(asunicode(v))\n-\n-        def first_write(self, v):\n-            try:\n-                self.write_normal(v)\n-                self.write = self.write_normal\n-            except TypeError:\n-                # input is probably a bytestream\n-                self.write_bytes(v)\n-                self.write = self.write_bytes\n-\n-    own_fh = False\n-    if isinstance(fname, os.PathLike):\n-        fname = os.fspath(fname)\n-    if _is_string_like(fname):\n-        # datasource doesn't support creating a new file ...\n-        open(fname, 'wt').close()\n-        fh = np.lib._datasource.open(fname, 'wt', encoding=encoding)\n-        own_fh = True\n-    elif hasattr(fname, 'write'):\n-        # wrap to handle byte output streams\n-        fh = WriteWrap(fname, encoding or 'latin1')\n-    else:\n-        raise ValueError('fname must be a string or file handle')\n-\n-    try:\n-        X = np.asarray(X)\n-\n-        # Handle 1-dimensional arrays\n-        if X.ndim == 0 or X.ndim > 2:\n-            raise ValueError(\n-                \"Expected 1D or 2D array, got %dD array instead\" % X.ndim)\n-        elif X.ndim == 1:\n-            # Common case -- 1d array of numbers\n-            if X.dtype.names is None:\n-                X = np.atleast_2d(X).T\n-                ncol = 1\n-\n-            # Complex dtype -- each field indicates a separate column\n-            else:\n-                ncol = len(X.dtype.names)\n-        else:\n-            ncol = X.shape[1]\n-\n-        iscomplex_X = np.iscomplexobj(X)\n-        # `fmt` can be a string with multiple insertion points or a\n-        # list of formats.  E.g. '%10.5f\\t%10d' or ('%10.5f', '$10d')\n-        if type(fmt) in (list, tuple):\n-            if len(fmt) != ncol:\n-                raise AttributeError('fmt has wrong shape.  %s' % str(fmt))\n-            format = delimiter.join(fmt)\n-        elif isinstance(fmt, str):\n-            n_fmt_chars = fmt.count('%')\n-            error = ValueError('fmt has wrong number of %% formats:  %s' % fmt)\n-            if n_fmt_chars == 1:\n-                if iscomplex_X:\n-                    fmt = [' (%s+%sj)' % (fmt, fmt), ] * ncol\n-                else:\n-                    fmt = [fmt, ] * ncol\n-                format = delimiter.join(fmt)\n-            elif iscomplex_X and n_fmt_chars != (2 * ncol):\n-                raise error\n-            elif ((not iscomplex_X) and n_fmt_chars != ncol):\n-                raise error\n-            else:\n-                format = fmt\n-        else:\n-            raise ValueError('invalid fmt: %r' % (fmt,))\n-\n-        if len(header) > 0:\n-            header = header.replace('\\n', '\\n' + comments)\n-            fh.write(comments + header + newline)\n-        if iscomplex_X:\n-            for row in X:\n-                row2 = []\n-                for number in row:\n-                    row2.append(number.real)\n-                    row2.append(number.imag)\n-                s = format % tuple(row2) + newline\n-                fh.write(s.replace('+-', '-'))\n-        else:\n-            for row in X:\n-                try:\n-                    v = format % tuple(row) + newline\n-                except TypeError as e:\n-                    raise TypeError(\"Mismatch between array dtype ('%s') and \"\n-                                    \"format specifier ('%s')\"\n-                                    % (str(X.dtype), format)) from e\n-                fh.write(v)\n-\n-        if len(footer) > 0:\n-            footer = footer.replace('\\n', '\\n' + comments)\n-            fh.write(comments + footer + newline)\n-    finally:\n-        if own_fh:\n-            fh.close()\n-\n-\n-@set_module('numpy')\n-def fromregex(file, regexp, dtype, encoding=None):\n-    r\"\"\"\n-    Construct an array from a text file, using regular expression parsing.\n-\n-    The returned array is always a structured array, and is constructed from\n-    all matches of the regular expression in the file. Groups in the regular\n-    expression are converted to fields of the structured array.\n-\n-    Parameters\n-    ----------\n-    file : file, str, or pathlib.Path\n-        Filename or file object to read.\n-\n-        .. versionchanged:: 1.22.0\n-            Now accepts `os.PathLike` implementations.\n-    regexp : str or regexp\n-        Regular expression used to parse the file.\n-        Groups in the regular expression correspond to fields in the dtype.\n-    dtype : dtype or list of dtypes\n-        Dtype for the structured array; must be a structured datatype.\n-    encoding : str, optional\n-        Encoding used to decode the inputfile. Does not apply to input streams.\n-\n-        .. versionadded:: 1.14.0\n-\n-    Returns\n-    -------\n-    output : ndarray\n-        The output array, containing the part of the content of `file` that\n-        was matched by `regexp`. `output` is always a structured array.\n-\n-    Raises\n-    ------\n-    TypeError\n-        When `dtype` is not a valid dtype for a structured array.\n-\n-    See Also\n-    --------\n-    fromstring, loadtxt\n-\n-    Notes\n-    -----\n-    Dtypes for structured arrays can be specified in several forms, but all\n-    forms specify at least the data type and field name. For details see\n-    `basics.rec`.\n-\n-    Examples\n-    --------\n-    >>> from io import StringIO\n-    >>> text = StringIO(\"1312 foo\\n1534  bar\\n444   qux\")\n-\n-    >>> regexp = r\"(\\d+)\\s+(...)\"  # match [digits, whitespace, anything]\n-    >>> output = np.fromregex(text, regexp,\n-    ...                       [('num', np.int64), ('key', 'S3')])\n-    >>> output\n-    array([(1312, b'foo'), (1534, b'bar'), ( 444, b'qux')],\n-          dtype=[('num', '<i8'), ('key', 'S3')])\n-    >>> output['num']\n-    array([1312, 1534,  444])\n-\n-    \"\"\"\n-    own_fh = False\n-    if not hasattr(file, \"read\"):\n-        file = os.fspath(file)\n-        file = np.lib._datasource.open(file, 'rt', encoding=encoding)\n-        own_fh = True\n-\n-    try:\n-        if not isinstance(dtype, np.dtype):\n-            dtype = np.dtype(dtype)\n-        if dtype.names is None:\n-            raise TypeError('dtype must be a structured datatype.')\n-\n-        content = file.read()\n-        if isinstance(content, bytes) and isinstance(regexp, str):\n-            regexp = asbytes(regexp)\n-\n-        if not hasattr(regexp, 'match'):\n-            regexp = re.compile(regexp)\n-        seq = regexp.findall(content)\n-        if seq and not isinstance(seq[0], tuple):\n-            # Only one group is in the regexp.\n-            # Create the new array as a single data-type and then\n-            #   re-interpret as a single-field structured array.\n-            newdtype = np.dtype(dtype[dtype.names[0]])\n-            output = np.array(seq, dtype=newdtype)\n-            output.dtype = dtype\n-        else:\n-            output = np.array(seq, dtype=dtype)\n-\n-        return output\n-    finally:\n-        if own_fh:\n-            file.close()\n-\n-\n-#####--------------------------------------------------------------------------\n-#---- --- ASCII functions ---\n-#####--------------------------------------------------------------------------\n-\n-\n-@set_array_function_like_doc\n-@set_module('numpy')\n-def genfromtxt(fname, dtype=float, comments='#', delimiter=None,\n-               skip_header=0, skip_footer=0, converters=None,\n-               missing_values=None, filling_values=None, usecols=None,\n-               names=None, excludelist=None,\n-               deletechars=''.join(sorted(NameValidator.defaultdeletechars)),\n-               replace_space='_', autostrip=False, case_sensitive=True,\n-               defaultfmt=\"f%i\", unpack=None, usemask=False, loose=True,\n-               invalid_raise=True, max_rows=None, encoding='bytes',\n-               *, ndmin=0, like=None):\n-    \"\"\"\n-    Load data from a text file, with missing values handled as specified.\n-\n-    Each line past the first `skip_header` lines is split at the `delimiter`\n-    character, and characters following the `comments` character are discarded.\n-\n-    Parameters\n-    ----------\n-    fname : file, str, pathlib.Path, list of str, generator\n-        File, filename, list, or generator to read.  If the filename\n-        extension is ``.gz`` or ``.bz2``, the file is first decompressed. Note\n-        that generators must return bytes or strings. The strings\n-        in a list or produced by a generator are treated as lines.\n-    dtype : dtype, optional\n-        Data type of the resulting array.\n-        If None, the dtypes will be determined by the contents of each\n-        column, individually.\n-    comments : str, optional\n-        The character used to indicate the start of a comment.\n-        All the characters occurring on a line after a comment are discarded.\n-    delimiter : str, int, or sequence, optional\n-        The string used to separate values.  By default, any consecutive\n-        whitespaces act as delimiter.  An integer or sequence of integers\n-        can also be provided as width(s) of each field.\n-    skiprows : int, optional\n-        `skiprows` was removed in numpy 1.10. Please use `skip_header` instead.\n-    skip_header : int, optional\n-        The number of lines to skip at the beginning of the file.\n-    skip_footer : int, optional\n-        The number of lines to skip at the end of the file.\n-    converters : variable, optional\n-        The set of functions that convert the data of a column to a value.\n-        The converters can also be used to provide a default value\n-        for missing data: ``converters = {3: lambda s: float(s or 0)}``.\n-    missing : variable, optional\n-        `missing` was removed in numpy 1.10. Please use `missing_values`\n-        instead.\n-    missing_values : variable, optional\n-        The set of strings corresponding to missing data.\n-    filling_values : variable, optional\n-        The set of values to be used as default when the data are missing.\n-    usecols : sequence, optional\n-        Which columns to read, with 0 being the first.  For example,\n-        ``usecols = (1, 4, 5)`` will extract the 2nd, 5th and 6th columns.\n-    names : {None, True, str, sequence}, optional\n-        If `names` is True, the field names are read from the first line after\n-        the first `skip_header` lines. This line can optionally be preceded\n-        by a comment delimiter. If `names` is a sequence or a single-string of\n-        comma-separated names, the names will be used to define the field names\n-        in a structured dtype. If `names` is None, the names of the dtype\n-        fields will be used, if any.\n-    excludelist : sequence, optional\n-        A list of names to exclude. This list is appended to the default list\n-        ['return','file','print']. Excluded names are appended with an\n-        underscore: for example, `file` would become `file_`.\n-    deletechars : str, optional\n-        A string combining invalid characters that must be deleted from the\n-        names.\n-    defaultfmt : str, optional\n-        A format used to define default field names, such as \"f%i\" or \"f_%02i\".\n-    autostrip : bool, optional\n-        Whether to automatically strip white spaces from the variables.\n-    replace_space : char, optional\n-        Character(s) used in replacement of white spaces in the variable\n-        names. By default, use a '_'.\n-    case_sensitive : {True, False, 'upper', 'lower'}, optional\n-        If True, field names are case sensitive.\n-        If False or 'upper', field names are converted to upper case.\n-        If 'lower', field names are converted to lower case.\n-    unpack : bool, optional\n-        If True, the returned array is transposed, so that arguments may be\n-        unpacked using ``x, y, z = genfromtxt(...)``.  When used with a\n-        structured data-type, arrays are returned for each field.\n-        Default is False.\n-    usemask : bool, optional\n-        If True, return a masked array.\n-        If False, return a regular array.\n-    loose : bool, optional\n-        If True, do not raise errors for invalid values.\n-    invalid_raise : bool, optional\n-        If True, an exception is raised if an inconsistency is detected in the\n-        number of columns.\n-        If False, a warning is emitted and the offending lines are skipped.\n-    max_rows : int,  optional\n-        The maximum number of rows to read. Must not be used with skip_footer\n-        at the same time.  If given, the value must be at least 1. Default is\n-        to read the entire file.\n-\n-        .. versionadded:: 1.10.0\n-    encoding : str, optional\n-        Encoding used to decode the inputfile. Does not apply when `fname` is\n-        a file object.  The special value 'bytes' enables backward compatibility\n-        workarounds that ensure that you receive byte arrays when possible\n-        and passes latin1 encoded strings to converters. Override this value to\n-        receive unicode arrays and pass strings as input to converters.  If set\n-        to None the system default is used. The default value is 'bytes'.\n-\n-        .. versionadded:: 1.14.0\n-    ndmin : int, optional\n-        Same parameter as `loadtxt`\n-\n-        .. versionadded:: 1.23.0\n-    ${ARRAY_FUNCTION_LIKE}\n-\n-        .. versionadded:: 1.20.0\n-\n-    Returns\n-    -------\n-    out : ndarray\n-        Data read from the text file. If `usemask` is True, this is a\n-        masked array.\n-\n-    See Also\n-    --------\n-    numpy.loadtxt : equivalent function when no data is missing.\n-\n-    Notes\n-    -----\n-    * When spaces are used as delimiters, or when no delimiter has been given\n-      as input, there should not be any missing data between two fields.\n-    * When the variables are named (either by a flexible dtype or with `names`),\n-      there must not be any header in the file (else a ValueError\n-      exception is raised).\n-    * Individual values are not stripped of spaces by default.\n-      When using a custom converter, make sure the function does remove spaces.\n-    * Custom converters may receive unexpected values due to dtype\n-      discovery. \n-\n-    References\n-    ----------\n-    .. [1] NumPy User Guide, section `I/O with NumPy\n-           <https://docs.scipy.org/doc/numpy/user/basics.io.genfromtxt.html>`_.\n-\n-    Examples\n-    --------\n-    >>> from io import StringIO\n-    >>> import numpy as np\n-\n-    Comma delimited file with mixed dtype\n-\n-    >>> s = StringIO(u\"1,1.3,abcde\")\n-    >>> data = np.genfromtxt(s, dtype=[('myint','i8'),('myfloat','f8'),\n-    ... ('mystring','S5')], delimiter=\",\")\n-    >>> data\n-    array((1, 1.3, b'abcde'),\n-          dtype=[('myint', '<i8'), ('myfloat', '<f8'), ('mystring', 'S5')])\n-\n-    Using dtype = None\n-\n-    >>> _ = s.seek(0) # needed for StringIO example only\n-    >>> data = np.genfromtxt(s, dtype=None,\n-    ... names = ['myint','myfloat','mystring'], delimiter=\",\")\n-    >>> data\n-    array((1, 1.3, b'abcde'),\n-          dtype=[('myint', '<i8'), ('myfloat', '<f8'), ('mystring', 'S5')])\n-\n-    Specifying dtype and names\n-\n-    >>> _ = s.seek(0)\n-    >>> data = np.genfromtxt(s, dtype=\"i8,f8,S5\",\n-    ... names=['myint','myfloat','mystring'], delimiter=\",\")\n-    >>> data\n-    array((1, 1.3, b'abcde'),\n-          dtype=[('myint', '<i8'), ('myfloat', '<f8'), ('mystring', 'S5')])\n-\n-    An example with fixed-width columns\n-\n-    >>> s = StringIO(u\"11.3abcde\")\n-    >>> data = np.genfromtxt(s, dtype=None, names=['intvar','fltvar','strvar'],\n-    ...     delimiter=[1,3,5])\n-    >>> data\n-    array((1, 1.3, b'abcde'),\n-          dtype=[('intvar', '<i8'), ('fltvar', '<f8'), ('strvar', 'S5')])\n-\n-    An example to show comments\n-\n-    >>> f = StringIO('''\n-    ... text,# of chars\n-    ... hello world,11\n-    ... numpy,5''')\n-    >>> np.genfromtxt(f, dtype='S12,S12', delimiter=',')\n-    array([(b'text', b''), (b'hello world', b'11'), (b'numpy', b'5')],\n-      dtype=[('f0', 'S12'), ('f1', 'S12')])\n-\n-    \"\"\"\n-\n-    if like is not None:\n-        return _genfromtxt_with_like(\n-            like, fname, dtype=dtype, comments=comments, delimiter=delimiter,\n-            skip_header=skip_header, skip_footer=skip_footer,\n-            converters=converters, missing_values=missing_values,\n-            filling_values=filling_values, usecols=usecols, names=names,\n-            excludelist=excludelist, deletechars=deletechars,\n-            replace_space=replace_space, autostrip=autostrip,\n-            case_sensitive=case_sensitive, defaultfmt=defaultfmt,\n-            unpack=unpack, usemask=usemask, loose=loose,\n-            invalid_raise=invalid_raise, max_rows=max_rows, encoding=encoding,\n-            ndmin=ndmin,\n-        )\n-\n-    _ensure_ndmin_ndarray_check_param(ndmin)\n-\n-    if max_rows is not None:\n-        if skip_footer:\n-            raise ValueError(\n-                    \"The keywords 'skip_footer' and 'max_rows' can not be \"\n-                    \"specified at the same time.\")\n-        if max_rows < 1:\n-            raise ValueError(\"'max_rows' must be at least 1.\")\n-\n-    if usemask:\n-        from numpy.ma import MaskedArray, make_mask_descr\n-    # Check the input dictionary of converters\n-    user_converters = converters or {}\n-    if not isinstance(user_converters, dict):\n-        raise TypeError(\n-            \"The input argument 'converter' should be a valid dictionary \"\n-            \"(got '%s' instead)\" % type(user_converters))\n-\n-    if encoding == 'bytes':\n-        encoding = None\n-        byte_converters = True\n-    else:\n-        byte_converters = False\n-\n-    # Initialize the filehandle, the LineSplitter and the NameValidator\n-    if isinstance(fname, os.PathLike):\n-        fname = os.fspath(fname)\n-    if isinstance(fname, str):\n-        fid = np.lib._datasource.open(fname, 'rt', encoding=encoding)\n-        fid_ctx = contextlib.closing(fid)\n-    else:\n-        fid = fname\n-        fid_ctx = contextlib.nullcontext(fid)\n-    try:\n-        fhd = iter(fid)\n-    except TypeError as e:\n-        raise TypeError(\n-            \"fname must be a string, a filehandle, a sequence of strings,\\n\"\n-            f\"or an iterator of strings. Got {type(fname)} instead.\"\n-        ) from e\n-    with fid_ctx:\n-        split_line = LineSplitter(delimiter=delimiter, comments=comments,\n-                                  autostrip=autostrip, encoding=encoding)\n-        validate_names = NameValidator(excludelist=excludelist,\n-                                       deletechars=deletechars,\n-                                       case_sensitive=case_sensitive,\n-                                       replace_space=replace_space)\n-\n-        # Skip the first `skip_header` rows\n-        try:\n-            for i in range(skip_header):\n-                next(fhd)\n-\n-            # Keep on until we find the first valid values\n-            first_values = None\n-\n-            while not first_values:\n-                first_line = _decode_line(next(fhd), encoding)\n-                if (names is True) and (comments is not None):\n-                    if comments in first_line:\n-                        first_line = (\n-                            ''.join(first_line.split(comments)[1:]))\n-                first_values = split_line(first_line)\n-        except StopIteration:\n-            # return an empty array if the datafile is empty\n-            first_line = ''\n-            first_values = []\n-            warnings.warn('genfromtxt: Empty input file: \"%s\"' % fname, stacklevel=2)\n-\n-        # Should we take the first values as names ?\n-        if names is True:\n-            fval = first_values[0].strip()\n-            if comments is not None:\n-                if fval in comments:\n-                    del first_values[0]\n-\n-        # Check the columns to use: make sure `usecols` is a list\n-        if usecols is not None:\n-            try:\n-                usecols = [_.strip() for _ in usecols.split(\",\")]\n-            except AttributeError:\n-                try:\n-                    usecols = list(usecols)\n-                except TypeError:\n-                    usecols = [usecols, ]\n-        nbcols = len(usecols or first_values)\n-\n-        # Check the names and overwrite the dtype.names if needed\n-        if names is True:\n-            names = validate_names([str(_.strip()) for _ in first_values])\n-            first_line = ''\n-        elif _is_string_like(names):\n-            names = validate_names([_.strip() for _ in names.split(',')])\n-        elif names:\n-            names = validate_names(names)\n-        # Get the dtype\n-        if dtype is not None:\n-            dtype = easy_dtype(dtype, defaultfmt=defaultfmt, names=names,\n-                               excludelist=excludelist,\n-                               deletechars=deletechars,\n-                               case_sensitive=case_sensitive,\n-                               replace_space=replace_space)\n-        # Make sure the names is a list (for 2.5)\n-        if names is not None:\n-            names = list(names)\n-\n-        if usecols:\n-            for (i, current) in enumerate(usecols):\n-                # if usecols is a list of names, convert to a list of indices\n-                if _is_string_like(current):\n-                    usecols[i] = names.index(current)\n-                elif current < 0:\n-                    usecols[i] = current + len(first_values)\n-            # If the dtype is not None, make sure we update it\n-            if (dtype is not None) and (len(dtype) > nbcols):\n-                descr = dtype.descr\n-                dtype = np.dtype([descr[_] for _ in usecols])\n-                names = list(dtype.names)\n-            # If `names` is not None, update the names\n-            elif (names is not None) and (len(names) > nbcols):\n-                names = [names[_] for _ in usecols]\n-        elif (names is not None) and (dtype is not None):\n-            names = list(dtype.names)\n-\n-        # Process the missing values ...............................\n-        # Rename missing_values for convenience\n-        user_missing_values = missing_values or ()\n-        if isinstance(user_missing_values, bytes):\n-            user_missing_values = user_missing_values.decode('latin1')\n-\n-        # Define the list of missing_values (one column: one list)\n-        missing_values = [list(['']) for _ in range(nbcols)]\n-\n-        # We have a dictionary: process it field by field\n-        if isinstance(user_missing_values, dict):\n-            # Loop on the items\n-            for (key, val) in user_missing_values.items():\n-                # Is the key a string ?\n-                if _is_string_like(key):\n-                    try:\n-                        # Transform it into an integer\n-                        key = names.index(key)\n-                    except ValueError:\n-                        # We couldn't find it: the name must have been dropped\n-                        continue\n-                # Redefine the key as needed if it's a column number\n-                if usecols:\n-                    try:\n-                        key = usecols.index(key)\n-                    except ValueError:\n-                        pass\n-                # Transform the value as a list of string\n-                if isinstance(val, (list, tuple)):\n-                    val = [str(_) for _ in val]\n-                else:\n-                    val = [str(val), ]\n-                # Add the value(s) to the current list of missing\n-                if key is None:\n-                    # None acts as default\n-                    for miss in missing_values:\n-                        miss.extend(val)\n-                else:\n-                    missing_values[key].extend(val)\n-        # We have a sequence : each item matches a column\n-        elif isinstance(user_missing_values, (list, tuple)):\n-            for (value, entry) in zip(user_missing_values, missing_values):\n-                value = str(value)\n-                if value not in entry:\n-                    entry.append(value)\n-        # We have a string : apply it to all entries\n-        elif isinstance(user_missing_values, str):\n-            user_value = user_missing_values.split(\",\")\n-            for entry in missing_values:\n-                entry.extend(user_value)\n-        # We have something else: apply it to all entries\n-        else:\n-            for entry in missing_values:\n-                entry.extend([str(user_missing_values)])\n-\n-        # Process the filling_values ...............................\n-        # Rename the input for convenience\n-        user_filling_values = filling_values\n-        if user_filling_values is None:\n-            user_filling_values = []\n-        # Define the default\n-        filling_values = [None] * nbcols\n-        # We have a dictionary : update each entry individually\n-        if isinstance(user_filling_values, dict):\n-            for (key, val) in user_filling_values.items():\n-                if _is_string_like(key):\n-                    try:\n-                        # Transform it into an integer\n-                        key = names.index(key)\n-                    except ValueError:\n-                        # We couldn't find it: the name must have been dropped,\n-                        continue\n-                # Redefine the key if it's a column number and usecols is defined\n-                if usecols:\n-                    try:\n-                        key = usecols.index(key)\n-                    except ValueError:\n-                        pass\n-                # Add the value to the list\n-                filling_values[key] = val\n-        # We have a sequence : update on a one-to-one basis\n-        elif isinstance(user_filling_values, (list, tuple)):\n-            n = len(user_filling_values)\n-            if (n <= nbcols):\n-                filling_values[:n] = user_filling_values\n-            else:\n-                filling_values = user_filling_values[:nbcols]\n-        # We have something else : use it for all entries\n-        else:\n-            filling_values = [user_filling_values] * nbcols\n-\n-        # Initialize the converters ................................\n-        if dtype is None:\n-            # Note: we can't use a [...]*nbcols, as we would have 3 times the same\n-            # ... converter, instead of 3 different converters.\n-            converters = [StringConverter(None, missing_values=miss, default=fill)\n-                          for (miss, fill) in zip(missing_values, filling_values)]\n-        else:\n-            dtype_flat = flatten_dtype(dtype, flatten_base=True)\n-            # Initialize the converters\n-            if len(dtype_flat) > 1:\n-                # Flexible type : get a converter from each dtype\n-                zipit = zip(dtype_flat, missing_values, filling_values)\n-                converters = [StringConverter(dt, locked=True,\n-                                              missing_values=miss, default=fill)\n-                              for (dt, miss, fill) in zipit]\n-            else:\n-                # Set to a default converter (but w/ different missing values)\n-                zipit = zip(missing_values, filling_values)\n-                converters = [StringConverter(dtype, locked=True,\n-                                              missing_values=miss, default=fill)\n-                              for (miss, fill) in zipit]\n-        # Update the converters to use the user-defined ones\n-        uc_update = []\n-        for (j, conv) in user_converters.items():\n-            # If the converter is specified by column names, use the index instead\n-            if _is_string_like(j):\n-                try:\n-                    j = names.index(j)\n-                    i = j\n-                except ValueError:\n-                    continue\n-            elif usecols:\n-                try:\n-                    i = usecols.index(j)\n-                except ValueError:\n-                    # Unused converter specified\n-                    continue\n-            else:\n-                i = j\n-            # Find the value to test - first_line is not filtered by usecols:\n-            if len(first_line):\n-                testing_value = first_values[j]\n-            else:\n-                testing_value = None\n-            if conv is bytes:\n-                user_conv = asbytes\n-            elif byte_converters:\n-                # converters may use decode to workaround numpy's old behaviour,\n-                # so encode the string again before passing to the user converter\n-                def tobytes_first(x, conv):\n-                    if type(x) is bytes:\n-                        return conv(x)\n-                    return conv(x.encode(\"latin1\"))\n-                user_conv = functools.partial(tobytes_first, conv=conv)\n-            else:\n-                user_conv = conv\n-            converters[i].update(user_conv, locked=True,\n-                                 testing_value=testing_value,\n-                                 default=filling_values[i],\n-                                 missing_values=missing_values[i],)\n-            uc_update.append((i, user_conv))\n-        # Make sure we have the corrected keys in user_converters...\n-        user_converters.update(uc_update)\n-\n-        # Fixme: possible error as following variable never used.\n-        # miss_chars = [_.missing_values for _ in converters]\n-\n-        # Initialize the output lists ...\n-        # ... rows\n-        rows = []\n-        append_to_rows = rows.append\n-        # ... masks\n-        if usemask:\n-            masks = []\n-            append_to_masks = masks.append\n-        # ... invalid\n-        invalid = []\n-        append_to_invalid = invalid.append\n-\n-        # Parse each line\n-        for (i, line) in enumerate(itertools.chain([first_line, ], fhd)):\n-            values = split_line(line)\n-            nbvalues = len(values)\n-            # Skip an empty line\n-            if nbvalues == 0:\n-                continue\n-            if usecols:\n-                # Select only the columns we need\n-                try:\n-                    values = [values[_] for _ in usecols]\n-                except IndexError:\n-                    append_to_invalid((i + skip_header + 1, nbvalues))\n-                    continue\n-            elif nbvalues != nbcols:\n-                append_to_invalid((i + skip_header + 1, nbvalues))\n-                continue\n-            # Store the values\n-            append_to_rows(tuple(values))\n-            if usemask:\n-                append_to_masks(tuple([v.strip() in m\n-                                       for (v, m) in zip(values,\n-                                                         missing_values)]))\n-            if len(rows) == max_rows:\n-                break\n-\n-    # Upgrade the converters (if needed)\n-    if dtype is None:\n-        for (i, converter) in enumerate(converters):\n-            current_column = [itemgetter(i)(_m) for _m in rows]\n-            try:\n-                converter.iterupgrade(current_column)\n-            except ConverterLockError:\n-                errmsg = \"Converter #%i is locked and cannot be upgraded: \" % i\n-                current_column = map(itemgetter(i), rows)\n-                for (j, value) in enumerate(current_column):\n-                    try:\n-                        converter.upgrade(value)\n-                    except (ConverterError, ValueError):\n-                        errmsg += \"(occurred line #%i for value '%s')\"\n-                        errmsg %= (j + 1 + skip_header, value)\n-                        raise ConverterError(errmsg)\n-\n-    # Check that we don't have invalid values\n-    nbinvalid = len(invalid)\n-    if nbinvalid > 0:\n-        nbrows = len(rows) + nbinvalid - skip_footer\n-        # Construct the error message\n-        template = \"    Line #%%i (got %%i columns instead of %i)\" % nbcols\n-        if skip_footer > 0:\n-            nbinvalid_skipped = len([_ for _ in invalid\n-                                     if _[0] > nbrows + skip_header])\n-            invalid = invalid[:nbinvalid - nbinvalid_skipped]\n-            skip_footer -= nbinvalid_skipped\n-#\n-#            nbrows -= skip_footer\n-#            errmsg = [template % (i, nb)\n-#                      for (i, nb) in invalid if i < nbrows]\n-#        else:\n-        errmsg = [template % (i, nb)\n-                  for (i, nb) in invalid]\n-        if len(errmsg):\n-            errmsg.insert(0, \"Some errors were detected !\")\n-            errmsg = \"\\n\".join(errmsg)\n-            # Raise an exception ?\n-            if invalid_raise:\n-                raise ValueError(errmsg)\n-            # Issue a warning ?\n-            else:\n-                warnings.warn(errmsg, ConversionWarning, stacklevel=2)\n-\n-    # Strip the last skip_footer data\n-    if skip_footer > 0:\n-        rows = rows[:-skip_footer]\n-        if usemask:\n-            masks = masks[:-skip_footer]\n-\n-    # Convert each value according to the converter:\n-    # We want to modify the list in place to avoid creating a new one...\n-    if loose:\n-        rows = list(\n-            zip(*[[conv._loose_call(_r) for _r in map(itemgetter(i), rows)]\n-                  for (i, conv) in enumerate(converters)]))\n-    else:\n-        rows = list(\n-            zip(*[[conv._strict_call(_r) for _r in map(itemgetter(i), rows)]\n-                  for (i, conv) in enumerate(converters)]))\n-\n-    # Reset the dtype\n-    data = rows\n-    if dtype is None:\n-        # Get the dtypes from the types of the converters\n-        column_types = [conv.type for conv in converters]\n-        # Find the columns with strings...\n-        strcolidx = [i for (i, v) in enumerate(column_types)\n-                     if v == np.str_]\n-\n-        if byte_converters and strcolidx:\n-            # convert strings back to bytes for backward compatibility\n-            warnings.warn(\n-                \"Reading unicode strings without specifying the encoding \"\n-                \"argument is deprecated. Set the encoding, use None for the \"\n-                \"system default.\",\n-                np.exceptions.VisibleDeprecationWarning, stacklevel=2)\n-            def encode_unicode_cols(row_tup):\n-                row = list(row_tup)\n-                for i in strcolidx:\n-                    row[i] = row[i].encode('latin1')\n-                return tuple(row)\n-\n-            try:\n-                data = [encode_unicode_cols(r) for r in data]\n-            except UnicodeEncodeError:\n-                pass\n-            else:\n-                for i in strcolidx:\n-                    column_types[i] = np.bytes_\n-\n-        # Update string types to be the right length\n-        sized_column_types = column_types[:]\n-        for i, col_type in enumerate(column_types):\n-            if np.issubdtype(col_type, np.character):\n-                n_chars = max(len(row[i]) for row in data)\n-                sized_column_types[i] = (col_type, n_chars)\n-\n-        if names is None:\n-            # If the dtype is uniform (before sizing strings)\n-            base = {\n-                c_type\n-                for c, c_type in zip(converters, column_types)\n-                if c._checked}\n-            if len(base) == 1:\n-                uniform_type, = base\n-                (ddtype, mdtype) = (uniform_type, bool)\n-            else:\n-                ddtype = [(defaultfmt % i, dt)\n-                          for (i, dt) in enumerate(sized_column_types)]\n-                if usemask:\n-                    mdtype = [(defaultfmt % i, bool)\n-                              for (i, dt) in enumerate(sized_column_types)]\n-        else:\n-            ddtype = list(zip(names, sized_column_types))\n-            mdtype = list(zip(names, [bool] * len(sized_column_types)))\n-        output = np.array(data, dtype=ddtype)\n-        if usemask:\n-            outputmask = np.array(masks, dtype=mdtype)\n-    else:\n-        # Overwrite the initial dtype names if needed\n-        if names and dtype.names is not None:\n-            dtype.names = names\n-        # Case 1. We have a structured type\n-        if len(dtype_flat) > 1:\n-            # Nested dtype, eg [('a', int), ('b', [('b0', int), ('b1', 'f4')])]\n-            # First, create the array using a flattened dtype:\n-            # [('a', int), ('b1', int), ('b2', float)]\n-            # Then, view the array using the specified dtype.\n-            if 'O' in (_.char for _ in dtype_flat):\n-                if has_nested_fields(dtype):\n-                    raise NotImplementedError(\n-                        \"Nested fields involving objects are not supported...\")\n-                else:\n-                    output = np.array(data, dtype=dtype)\n-            else:\n-                rows = np.array(data, dtype=[('', _) for _ in dtype_flat])\n-                output = rows.view(dtype)\n-            # Now, process the rowmasks the same way\n-            if usemask:\n-                rowmasks = np.array(\n-                    masks, dtype=np.dtype([('', bool) for t in dtype_flat]))\n-                # Construct the new dtype\n-                mdtype = make_mask_descr(dtype)\n-                outputmask = rowmasks.view(mdtype)\n-        # Case #2. We have a basic dtype\n-        else:\n-            # We used some user-defined converters\n-            if user_converters:\n-                ishomogeneous = True\n-                descr = []\n-                for i, ttype in enumerate([conv.type for conv in converters]):\n-                    # Keep the dtype of the current converter\n-                    if i in user_converters:\n-                        ishomogeneous &= (ttype == dtype.type)\n-                        if np.issubdtype(ttype, np.character):\n-                            ttype = (ttype, max(len(row[i]) for row in data))\n-                        descr.append(('', ttype))\n-                    else:\n-                        descr.append(('', dtype))\n-                # So we changed the dtype ?\n-                if not ishomogeneous:\n-                    # We have more than one field\n-                    if len(descr) > 1:\n-                        dtype = np.dtype(descr)\n-                    # We have only one field: drop the name if not needed.\n-                    else:\n-                        dtype = np.dtype(ttype)\n-            #\n-            output = np.array(data, dtype)\n-            if usemask:\n-                if dtype.names is not None:\n-                    mdtype = [(_, bool) for _ in dtype.names]\n-                else:\n-                    mdtype = bool\n-                outputmask = np.array(masks, dtype=mdtype)\n-    # Try to take care of the missing data we missed\n-    names = output.dtype.names\n-    if usemask and names:\n-        for (name, conv) in zip(names, converters):\n-            missing_values = [conv(_) for _ in conv.missing_values\n-                              if _ != '']\n-            for mval in missing_values:\n-                outputmask[name] |= (output[name] == mval)\n-    # Construct the final array\n-    if usemask:\n-        output = output.view(MaskedArray)\n-        output._mask = outputmask\n-\n-    output = _ensure_ndmin_ndarray(output, ndmin=ndmin)\n-\n-    if unpack:\n-        if names is None:\n-            return output.T\n-        elif len(names) == 1:\n-            # squeeze single-name dtypes too\n-            return output[names[0]]\n-        else:\n-            # For structured arrays with multiple fields,\n-            # return an array for each field.\n-            return [output[field] for field in names]\n-    return output\n-\n-\n-_genfromtxt_with_like = array_function_dispatch()(genfromtxt)\n-\n-\n-def recfromtxt(fname, **kwargs):\n-    \"\"\"\n-    Load ASCII data from a file and return it in a record array.\n-\n-    If ``usemask=False`` a standard `recarray` is returned,\n-    if ``usemask=True`` a MaskedRecords array is returned.\n-\n-    .. deprecated:: 2.0\n-        Use `numpy.genfromtxt` instead.\n-\n-    Parameters\n-    ----------\n-    fname, kwargs : For a description of input parameters, see `genfromtxt`.\n-\n-    See Also\n-    --------\n-    numpy.genfromtxt : generic function\n-\n-    Notes\n-    -----\n-    By default, `dtype` is None, which means that the data-type of the output\n-    array will be determined from the data.\n-\n-    \"\"\"\n-\n-    # Deprecated in NumPy 2.0, 2023-07-11\n-    warnings.warn(\n-        \"`recfromtxt` is deprecated, \"\n-        \"use `numpy.genfromtxt` instead.\"\n-        \"(deprecated in NumPy 2.0)\",\n-        DeprecationWarning,\n-        stacklevel=2\n-    )\n-\n-    kwargs.setdefault(\"dtype\", None)\n-    usemask = kwargs.get('usemask', False)\n-    output = genfromtxt(fname, **kwargs)\n-    if usemask:\n-        from numpy.ma.mrecords import MaskedRecords\n-        output = output.view(MaskedRecords)\n-    else:\n-        output = output.view(np.recarray)\n-    return output\n-\n-\n-def recfromcsv(fname, **kwargs):\n-    \"\"\"\n-    Load ASCII data stored in a comma-separated file.\n-\n-    The returned array is a record array (if ``usemask=False``, see\n-    `recarray`) or a masked record array (if ``usemask=True``,\n-    see `ma.mrecords.MaskedRecords`).\n-\n-    .. deprecated:: 2.0\n-        Use `numpy.genfromtxt` with comma as `delimiter` instead.\n-\n-    Parameters\n-    ----------\n-    fname, kwargs : For a description of input parameters, see `genfromtxt`.\n-\n-    See Also\n-    --------\n-    numpy.genfromtxt : generic function to load ASCII data.\n-\n-    Notes\n-    -----\n-    By default, `dtype` is None, which means that the data-type of the output\n-    array will be determined from the data.\n-\n-    \"\"\"\n-\n-    # Deprecated in NumPy 2.0, 2023-07-11\n-    warnings.warn(\n-        \"`recfromcsv` is deprecated, \"\n-        \"use `numpy.genfromtxt` with comma as `delimiter` instead. \"\n-        \"(deprecated in NumPy 2.0)\",\n-        DeprecationWarning,\n-        stacklevel=2\n-    )\n-\n-    # Set default kwargs for genfromtxt as relevant to csv import.\n-    kwargs.setdefault(\"case_sensitive\", \"lower\")\n-    kwargs.setdefault(\"names\", True)\n-    kwargs.setdefault(\"delimiter\", \",\")\n-    kwargs.setdefault(\"dtype\", None)\n-    output = genfromtxt(fname, **kwargs)\n-\n-    usemask = kwargs.get(\"usemask\", False)\n-    if usemask:\n-        from numpy.ma.mrecords import MaskedRecords\n-        output = output.view(MaskedRecords)\n-    else:\n-        output = output.view(np.recarray)\n-    return output\n+from ._npyio_impl import DataSource, NpzFile\n",
            "comment_added_diff": {},
            "comment_deleted_diff": {
                "53": "    ...     def __getitem__(self, key): # An instance of BagObj(BagDemo)",
                "54": "    ...                                 # will call this method when any",
                "55": "    ...                                 # attribute look-up is required",
                "69": "        # Use weakref to make NpzFile objects collectable by refcount",
                "170": "    >>> npz['x']  # getitem access",
                "172": "    >>> npz.f.x  # attribute lookup",
                "176": "    # Make __exit__ safe if zipfile_factory raises an exception",
                "184": "        # Import is postponed to here since zipfile depends on gzip, an",
                "185": "        # optional component of the so-called standard library.",
                "219": "        self.f = None  # break reference cycle",
                "224": "    # Implement the Mapping ABC",
                "232": "        # FIXME: This seems like it will copy strings around",
                "233": "        #   more than is strictly necessary.  The zipfile",
                "234": "        #   will read the string and then",
                "235": "        #   the format.read_array will copy the string",
                "236": "        #   to another place in memory.",
                "237": "        #   It would be better if the zipfile could read",
                "238": "        #   (or at least uncompress) the data",
                "239": "        #   directly into the array memory.",
                "265": "        # Get filename or default to `object`",
                "271": "        # Get the name of arrays",
                "403": "        # The 'encoding' value for pickle also affects what encoding",
                "404": "        # the serialized binary data of NumPy arrays is loaded",
                "405": "        # in. Pickle does not pass on the encoding information to",
                "406": "        # NumPy. The unpickling code in numpy.core.multiarray is",
                "407": "        # written to assume that unicode data appearing where binary",
                "408": "        # should be is in 'latin1'. 'bytes' is also safe, as is 'ASCII'.",
                "409": "        #",
                "410": "        # Other encoding values can corrupt binary data, and we",
                "411": "        # purposefully disallow them. For the same reason, the errors=",
                "412": "        # argument is not exposed, as values other than 'strict'",
                "413": "        # result can similarly silently corrupt numerical data.",
                "426": "        # Code to distinguish from NumPy binary files and pickles.",
                "428": "        _ZIP_SUFFIX = b'PK\\x05\\x06' # empty zip files start with this",
                "433": "        # If the file size is less than N, we need to make sure not",
                "434": "        # to seek past the beginning of the file",
                "435": "        fid.seek(-min(N, len(magic)), 1)  # back-up",
                "437": "            # zip-file (assume .npz)",
                "438": "            # Potentially transfer file ownership to NpzFile",
                "445": "            # .npy file",
                "456": "            # Try a pickle",
                "518": "    >>> _ = outfile.seek(0) # Only needed here to simulate closing & reopening file",
                "530": "    # [1 2] [1 3]",
                "616": "    >>> _ = outfile.seek(0) # Only needed here to simulate closing & reopening file",
                "710": "    # Import is postponed to here since zipfile depends on gzip, an optional",
                "711": "    # component of the so-called standard library.",
                "737": "        # always force zip64, gh-10776",
                "752": "    # Check correctness of the values of `ndmin`",
                "764": "    # Verify that the array has at least dimensions `ndmin`.",
                "765": "    # Tweak the size and shape of the arrays - remove extraneous dimensions",
                "768": "    # and ensure we have the minimum number of dimensions asked for",
                "769": "    # - has to be in this order for the odd case ndmin=1, a.squeeze().ndim=0",
                "779": "# amount of lines loadtxt reads in one chunk, can be overridden for testing",
                "801": "            # Need to handle conversion here, or the splitting would fail",
                "810": "# The number of rows we read in one go if confronted with a parametric dtype",
                "814": "def _read(fname, *, delimiter=',', comment='#', quote='\"',",
                "882": "    # Handle special 'bytes' keyword for encoding",
                "895": "        # This is a legacy \"flexible\" dtype.  We do not truly support",
                "896": "        # parametric dtypes currently (no dtype discovery step in the core),",
                "897": "        # but have to support these for backward compatibility.",
                "902": "        # Allow usecols to be a single int or a sequence of ints, the C-code",
                "903": "        # handles the rest",
                "914": "        # assume comments are a sequence of strings",
                "923": "            comments = None  # No comments at all",
                "925": "            # If there is only one comment, and that comment has one character,",
                "926": "            # the normal parsing can deal with it just fine.",
                "931": "            # Input validation if there are multiple comment characters",
                "938": "    # comment is now either a 1 or 0 character string or a tuple:",
                "940": "        # Note: An earlier version support two character comments (and could",
                "941": "        #       have been extended to multiple characters, we assume this is",
                "942": "        #       rare enough to not optimize for.",
                "956": "        # Passing -1 to the C code means \"read the entire file\".",
                "998": "            # This branch reads the file into chunks of object arrays and then",
                "999": "            # casts them to the desired actual dtype.  This ensures correct",
                "1000": "            # string-length and datetime-unit discovery (like `arr.astype()`).",
                "1001": "            # Due to chunking, certain error reports are less clear, currently.",
                "1003": "                data = iter(data)  # cannot chunk when reading from file",
                "1007": "                c_byte_converters = True  # Use latin1 rather than ascii",
                "1024": "                # Cast here already.  We hope that this is better even for",
                "1025": "                # large files because the storage is more compact.  It could",
                "1026": "                # be adapted (in principle the concatenate could cast).",
                "1029": "                skiprows = 0  # Only have to skip for first chunk",
                "1033": "                    # There was less data than requested, so we are done.",
                "1036": "            # Need at least one chunk, but if empty, the last one may have",
                "1037": "            # the wrong shape.",
                "1045": "    # NOTE: ndmin works as advertised for structured dtypes, but normally",
                "1046": "    #       these would return a 1D result plus the structured dimension,",
                "1047": "    #       so ndmin=2 adds a third dimension even when no squeezing occurs.",
                "1048": "    #       A `squeeze=False` could be a better solution (pandas uses squeeze).",
                "1060": "        # Unpack structured dtypes if requested:",
                "1063": "            # For structured arrays, return an array for each field.",
                "1073": "def loadtxt(fname, dtype=float, comments='#', delimiter=None,",
                "1096": "        strings will be decoded as 'latin1'. The default is '#'.",
                "1157": "            starting with '#' or as specified via `comments`) are not counted",
                "1202": "    >>> from io import StringIO   # StringIO behaves like a file object",
                "1227": "    ...     0: lambda x: np.floor(float(x)),  # conversion fn for column 0",
                "1228": "    ...     1: lambda x: np.ceil(float(x)),  # conversion fn for column 1",
                "1290": "    >>> s = StringIO('\"alpha, #42\", 10.0\\n\"beta, #64\", 2.0\\n')",
                "1293": "    array([('alpha, #42', 10.), ('beta, #64',  2.)],",
                "1298": "    >>> s = StringIO('\"alpha, #42\"       10.0\\n\"beta, #64\" 2.0\\n')",
                "1301": "    array([('alpha, #42', 10.), ('beta, #64',  2.)],",
                "1337": "    # Control character type conversions for Py3 convenience",
                "1365": "            footer='', comments='# ', encoding=None):",
                "1406": "        to mark them as comments. Default: '# ',  as expected by e.g.",
                "1474": "           <https://docs.python.org/library/string.html#format-specification-mini-language>`_,",
                "1480": "    >>> np.savetxt('test.out', x, delimiter=',')   # X is an array",
                "1481": "    >>> np.savetxt('test.out', (x,y,z))   # x,y,z equal sized 1D arrays",
                "1482": "    >>> np.savetxt('test.out', x, fmt='%1.4e')   # use exponential notation",
                "1515": "                # input is probably a bytestream",
                "1523": "        # datasource doesn't support creating a new file ...",
                "1528": "        # wrap to handle byte output streams",
                "1536": "        # Handle 1-dimensional arrays",
                "1541": "            # Common case -- 1d array of numbers",
                "1546": "            # Complex dtype -- each field indicates a separate column",
                "1553": "        # `fmt` can be a string with multiple insertion points or a",
                "1554": "        # list of formats.  E.g. '%10.5f\\t%10d' or ('%10.5f', '$10d')",
                "1658": "    >>> regexp = r\"(\\d+)\\s+(...)\"  # match [digits, whitespace, anything]",
                "1688": "            # Only one group is in the regexp.",
                "1689": "            # Create the new array as a single data-type and then",
                "1690": "            #   re-interpret as a single-field structured array.",
                "1703": "#####--------------------------------------------------------------------------",
                "1704": "#---- --- ASCII functions ---",
                "1705": "#####--------------------------------------------------------------------------",
                "1710": "def genfromtxt(fname, dtype=float, comments='#', delimiter=None,",
                "1868": "    >>> _ = s.seek(0) # needed for StringIO example only",
                "1896": "    ... text,# of chars",
                "1931": "    # Check the input dictionary of converters",
                "1944": "    # Initialize the filehandle, the LineSplitter and the NameValidator",
                "1968": "        # Skip the first `skip_header` rows",
                "1973": "            # Keep on until we find the first valid values",
                "1984": "            # return an empty array if the datafile is empty",
                "1989": "        # Should we take the first values as names ?",
                "1996": "        # Check the columns to use: make sure `usecols` is a list",
                "2007": "        # Check the names and overwrite the dtype.names if needed",
                "2015": "        # Get the dtype",
                "2022": "        # Make sure the names is a list (for 2.5)",
                "2028": "                # if usecols is a list of names, convert to a list of indices",
                "2033": "            # If the dtype is not None, make sure we update it",
                "2038": "            # If `names` is not None, update the names",
                "2044": "        # Process the missing values ...............................",
                "2045": "        # Rename missing_values for convenience",
                "2050": "        # Define the list of missing_values (one column: one list)",
                "2053": "        # We have a dictionary: process it field by field",
                "2055": "            # Loop on the items",
                "2057": "                # Is the key a string ?",
                "2060": "                        # Transform it into an integer",
                "2063": "                        # We couldn't find it: the name must have been dropped",
                "2065": "                # Redefine the key as needed if it's a column number",
                "2071": "                # Transform the value as a list of string",
                "2076": "                # Add the value(s) to the current list of missing",
                "2078": "                    # None acts as default",
                "2083": "        # We have a sequence : each item matches a column",
                "2089": "        # We have a string : apply it to all entries",
                "2094": "        # We have something else: apply it to all entries",
                "2099": "        # Process the filling_values ...............................",
                "2100": "        # Rename the input for convenience",
                "2104": "        # Define the default",
                "2106": "        # We have a dictionary : update each entry individually",
                "2111": "                        # Transform it into an integer",
                "2114": "                        # We couldn't find it: the name must have been dropped,",
                "2116": "                # Redefine the key if it's a column number and usecols is defined",
                "2122": "                # Add the value to the list",
                "2124": "        # We have a sequence : update on a one-to-one basis",
                "2131": "        # We have something else : use it for all entries",
                "2135": "        # Initialize the converters ................................",
                "2137": "            # Note: we can't use a [...]*nbcols, as we would have 3 times the same",
                "2138": "            # ... converter, instead of 3 different converters.",
                "2143": "            # Initialize the converters",
                "2145": "                # Flexible type : get a converter from each dtype",
                "2151": "                # Set to a default converter (but w/ different missing values)",
                "2156": "        # Update the converters to use the user-defined ones",
                "2159": "            # If the converter is specified by column names, use the index instead",
                "2170": "                    # Unused converter specified",
                "2174": "            # Find the value to test - first_line is not filtered by usecols:",
                "2182": "                # converters may use decode to workaround numpy's old behaviour,",
                "2183": "                # so encode the string again before passing to the user converter",
                "2196": "        # Make sure we have the corrected keys in user_converters...",
                "2199": "        # Fixme: possible error as following variable never used.",
                "2200": "        # miss_chars = [_.missing_values for _ in converters]",
                "2202": "        # Initialize the output lists ...",
                "2203": "        # ... rows",
                "2206": "        # ... masks",
                "2210": "        # ... invalid",
                "2214": "        # Parse each line",
                "2218": "            # Skip an empty line",
                "2222": "                # Select only the columns we need",
                "2231": "            # Store the values",
                "2240": "    # Upgrade the converters (if needed)",
                "2247": "                errmsg = \"Converter #%i is locked and cannot be upgraded: \" % i",
                "2253": "                        errmsg += \"(occurred line #%i for value '%s')\"",
                "2257": "    # Check that we don't have invalid values",
                "2261": "        # Construct the error message",
                "2262": "        template = \"    Line #%%i (got %%i columns instead of %i)\" % nbcols",
                "2268": "#",
                "2269": "#            nbrows -= skip_footer",
                "2270": "#            errmsg = [template % (i, nb)",
                "2271": "#                      for (i, nb) in invalid if i < nbrows]",
                "2272": "#        else:",
                "2278": "            # Raise an exception ?",
                "2281": "            # Issue a warning ?",
                "2285": "    # Strip the last skip_footer data",
                "2291": "    # Convert each value according to the converter:",
                "2292": "    # We want to modify the list in place to avoid creating a new one...",
                "2302": "    # Reset the dtype",
                "2305": "        # Get the dtypes from the types of the converters",
                "2307": "        # Find the columns with strings...",
                "2312": "            # convert strings back to bytes for backward compatibility",
                "2332": "        # Update string types to be the right length",
                "2340": "            # If the dtype is uniform (before sizing strings)",
                "2361": "        # Overwrite the initial dtype names if needed",
                "2364": "        # Case 1. We have a structured type",
                "2366": "            # Nested dtype, eg [('a', int), ('b', [('b0', int), ('b1', 'f4')])]",
                "2367": "            # First, create the array using a flattened dtype:",
                "2368": "            # [('a', int), ('b1', int), ('b2', float)]",
                "2369": "            # Then, view the array using the specified dtype.",
                "2379": "            # Now, process the rowmasks the same way",
                "2383": "                # Construct the new dtype",
                "2386": "        # Case #2. We have a basic dtype",
                "2388": "            # We used some user-defined converters",
                "2393": "                    # Keep the dtype of the current converter",
                "2401": "                # So we changed the dtype ?",
                "2403": "                    # We have more than one field",
                "2406": "                    # We have only one field: drop the name if not needed.",
                "2409": "            #",
                "2417": "    # Try to take care of the missing data we missed",
                "2425": "    # Construct the final array",
                "2436": "            # squeeze single-name dtypes too",
                "2439": "            # For structured arrays with multiple fields,",
                "2440": "            # return an array for each field.",
                "2473": "    # Deprecated in NumPy 2.0, 2023-07-11",
                "2519": "    # Deprecated in NumPy 2.0, 2023-07-11",
                "2528": "    # Set default kwargs for genfromtxt as relevant to csv import."
            },
            "comment_modified_diff": {}
        }
    ],
    "loops_unary_fp.dispatch.c.src": [],
    "loops_unary_fp_le.dispatch.c.src": [],
    "test_printing.py": [],
    "22863.new_feature.rst": [],
    "build_test.yml": [],
    "convert.c": [],
    "22963.new_feature.rst": [],
    "22998.expired.rst": [],
    "rows.c": [],
    "format.py": [
        {
            "commit": "2303556949b96c4220ed86fa4554f6a87dec3842",
            "timestamp": "2023-01-13T21:24:35+01:00",
            "author": "Michael",
            "commit_message": "ENH: Faster numpy.load (try/except _filter_header) (#22916)\n\nThis pull requests speeds up numpy.load. Since _filter_header is quite a bottleneck, we only run it if we must. Users will get a warning if they have a legacy Numpy file so that they can save it again for faster loading.\r\n\r\nMain discussion and benchmarks see #22898\r\n\r\nCo-authored-by: Sebastian Berg <sebastian@sipsolutions.net>",
            "additions": 18,
            "deletions": 4,
            "change_type": "MODIFY",
            "diff": "@@ -623,13 +623,27 @@ def _read_array_header(fp, version, max_header_size=_MAX_HEADER_SIZE):\n     #   \"descr\" : dtype.descr\n     # Versions (2, 0) and (1, 0) could have been created by a Python 2\n     # implementation before header filtering was implemented.\n-    if version <= (2, 0):\n-        header = _filter_header(header)\n+    #\n+    # For performance reasons, we try without _filter_header first though\n     try:\n         d = safe_eval(header)\n     except SyntaxError as e:\n-        msg = \"Cannot parse header: {!r}\"\n-        raise ValueError(msg.format(header)) from e\n+        if version <= (2, 0):\n+            header = _filter_header(header)\n+            try:\n+                d = safe_eval(header)\n+            except SyntaxError as e2:\n+                msg = \"Cannot parse header: {!r}\"\n+                raise ValueError(msg.format(header)) from e2\n+            else:\n+                warnings.warn(\n+                    \"Reading `.npy` or `.npz` file required additional \"\n+                    \"header parsing as it was created on Python 2. Save the \"\n+                    \"file again to speed up loading and avoid this warning.\",\n+                    UserWarning, stacklevel=4)\n+        else:\n+            msg = \"Cannot parse header: {!r}\"\n+            raise ValueError(msg.format(header)) from e\n     if not isinstance(d, dict):\n         msg = \"Header is not a dictionary: {!r}\"\n         raise ValueError(msg.format(d))\n",
            "comment_added_diff": {
                "626": "    #",
                "627": "    # For performance reasons, we try without _filter_header first though"
            },
            "comment_deleted_diff": {},
            "comment_modified_diff": {
                "626": "    if version <= (2, 0):",
                "627": "        header = _filter_header(header)"
            }
        },
        {
            "commit": "bd7242c84720e52d5e67211791f053d206d9affd",
            "timestamp": "2023-03-25T05:53:22+00:00",
            "author": "yuki",
            "commit_message": "MAINT: Fix reference roles of `ast`",
            "additions": 7,
            "deletions": 7,
            "change_type": "MODIFY",
            "diff": "@@ -437,15 +437,15 @@ def _write_array_header(fp, d, version=None):\n         header.append(\"'%s': %s, \" % (key, repr(value)))\n     header.append(\"}\")\n     header = \"\".join(header)\n-    \n+\n     # Add some spare space so that the array header can be modified in-place\n     # when changing the array size, e.g. when growing it by appending data at\n-    # the end. \n+    # the end.\n     shape = d['shape']\n     header += \" \" * ((GROWTH_AXIS_MAX_DIGITS - len(repr(\n         shape[-1 if d['fortran_order'] else 0]\n     ))) if len(shape) > 0 else 0)\n-    \n+\n     if version is None:\n         header = _wrap_header_guess_version(header)\n     else:\n@@ -505,7 +505,7 @@ def read_array_header_1_0(fp, max_header_size=_MAX_HEADER_SIZE):\n     max_header_size : int, optional\n         Maximum allowed size of the header.  Large headers may not be safe\n         to load securely and thus require explicitly passing a larger value.\n-        See :py:meth:`ast.literal_eval()` for details.\n+        See :py:func:`ast.literal_eval()` for details.\n \n     Raises\n     ------\n@@ -532,7 +532,7 @@ def read_array_header_2_0(fp, max_header_size=_MAX_HEADER_SIZE):\n     max_header_size : int, optional\n         Maximum allowed size of the header.  Large headers may not be safe\n         to load securely and thus require explicitly passing a larger value.\n-        See :py:meth:`ast.literal_eval()` for details.\n+        See :py:func:`ast.literal_eval()` for details.\n \n     Returns\n     -------\n@@ -764,7 +764,7 @@ def read_array(fp, allow_pickle=False, pickle_kwargs=None, *,\n     max_header_size : int, optional\n         Maximum allowed size of the header.  Large headers may not be safe\n         to load securely and thus require explicitly passing a larger value.\n-        See :py:meth:`ast.literal_eval()` for details.\n+        See :py:func:`ast.literal_eval()` for details.\n         This option is ignored when `allow_pickle` is passed.  In that case\n         the file is by definition trusted and the limit is unnecessary.\n \n@@ -883,7 +883,7 @@ def open_memmap(filename, mode='r+', dtype=None, shape=None,\n     max_header_size : int, optional\n         Maximum allowed size of the header.  Large headers may not be safe\n         to load securely and thus require explicitly passing a larger value.\n-        See :py:meth:`ast.literal_eval()` for details.\n+        See :py:func:`ast.literal_eval()` for details.\n \n     Returns\n     -------\n",
            "comment_added_diff": {
                "443": "    # the end."
            },
            "comment_deleted_diff": {
                "443": "    # the end."
            },
            "comment_modified_diff": {
                "443": "    # the end."
            }
        },
        {
            "commit": "b294b9f9b2ce52ef96708ac08dc21b2ef8d034b4",
            "timestamp": "2023-04-27T17:48:15+02:00",
            "author": "Sebastian Berg",
            "commit_message": "ENH: Refactor drop_metadata and use it for npy/npz saving",
            "additions": 7,
            "deletions": 13,
            "change_type": "MODIFY",
            "diff": "@@ -163,7 +163,7 @@\n \"\"\"\n import numpy\n import warnings\n-from numpy.lib.utils import safe_eval\n+from numpy.lib.utils import safe_eval, drop_metadata\n from numpy.compat import (\n     isfileobj, os_fspath, pickle\n     )\n@@ -239,15 +239,6 @@ def read_magic(fp):\n     major, minor = magic_str[-2:]\n     return major, minor\n \n-def _has_metadata(dt):\n-    if dt.metadata is not None:\n-        return True\n-    elif dt.names is not None:\n-        return any(_has_metadata(dt[k]) for k in dt.names)\n-    elif dt.subdtype is not None:\n-        return _has_metadata(dt.base)\n-    else:\n-        return False\n \n def dtype_to_descr(dtype):\n     \"\"\"\n@@ -272,9 +263,12 @@ def dtype_to_descr(dtype):\n         replicate the input dtype.\n \n     \"\"\"\n-    if _has_metadata(dtype):\n-        warnings.warn(\"metadata on a dtype may be saved or ignored, but will \"\n-                      \"raise if saved when read. Use another form of storage.\",\n+    # NOTE: that drop_metadata may not return the right dtype e.g. for user\n+    #       dtypes.  In that case our code below would fail the same, though.\n+    new_dtype = drop_metadata(dtype)\n+    if new_dtype is not dtype:\n+        warnings.warn(\"metadata on a dtype is not saved to an npy/npz. \"\n+                      \"Use another format (such as pickle) to store it.\",\n                       UserWarning, stacklevel=2)\n     if dtype.names is not None:\n         # This is a record array. The .descr is fine.  XXX: parts of the\n",
            "comment_added_diff": {
                "266": "    # NOTE: that drop_metadata may not return the right dtype e.g. for user",
                "267": "    #       dtypes.  In that case our code below would fail the same, though."
            },
            "comment_deleted_diff": {},
            "comment_modified_diff": {}
        },
        {
            "commit": "12efa8eec31bd476e0110bb90de43d603d0e76d3",
            "timestamp": "2023-06-14T19:30:01+02:00",
            "author": "Mateusz Sok\u00f3\u0142",
            "commit_message": "DEP: deprecate compat and selected lib utils (#23830)\n\n[skip ci]",
            "additions": 23,
            "deletions": 9,
            "change_type": "MODIFY",
            "diff": "@@ -161,12 +161,13 @@\n evolved with time and this document is more current.\n \n \"\"\"\n-import numpy\n+import io\n+import os\n+import pickle\n import warnings\n-from numpy.lib.utils import safe_eval, drop_metadata\n-from numpy.compat import (\n-    isfileobj, os_fspath, pickle\n-    )\n+\n+import numpy\n+from numpy.lib.utils import drop_metadata\n \n \n __all__ = []\n@@ -590,6 +591,7 @@ def _read_array_header(fp, version, max_header_size=_MAX_HEADER_SIZE):\n     \"\"\"\n     # Read an unsigned, little-endian short int which has the length of the\n     # header.\n+    import ast\n     import struct\n     hinfo = _header_size_info.get(version)\n     if hinfo is None:\n@@ -620,12 +622,12 @@ def _read_array_header(fp, version, max_header_size=_MAX_HEADER_SIZE):\n     #\n     # For performance reasons, we try without _filter_header first though\n     try:\n-        d = safe_eval(header)\n+        d = ast.literal_eval(header)\n     except SyntaxError as e:\n         if version <= (2, 0):\n             header = _filter_header(header)\n             try:\n-                d = safe_eval(header)\n+                d = ast.literal_eval(header)\n             except SyntaxError as e2:\n                 msg = \"Cannot parse header: {!r}\"\n                 raise ValueError(msg.format(header)) from e2\n@@ -916,12 +918,12 @@ def open_memmap(filename, mode='r+', dtype=None, shape=None,\n             shape=shape,\n         )\n         # If we got here, then it should be safe to create the file.\n-        with open(os_fspath(filename), mode+'b') as fp:\n+        with open(os.fspath(filename), mode+'b') as fp:\n             _write_array_header(fp, d, version)\n             offset = fp.tell()\n     else:\n         # Read the header of the file first.\n-        with open(os_fspath(filename), 'rb') as fp:\n+        with open(os.fspath(filename), 'rb') as fp:\n             version = read_magic(fp)\n             _check_version(version)\n \n@@ -974,3 +976,15 @@ def _read_bytes(fp, size, error_template=\"ran out of data\"):\n         raise ValueError(msg % (error_template, size, len(data)))\n     else:\n         return data\n+\n+\n+def isfileobj(f):\n+    if not isinstance(f, (io.FileIO, io.BufferedReader, io.BufferedWriter)):\n+        return False\n+    try:\n+        # BufferedReader/Writer may raise OSError when\n+        # fetching `fileno()` (e.g. when wrapping BytesIO).\n+        f.fileno()\n+        return True\n+    except OSError:\n+        return False\n",
            "comment_added_diff": {
                "985": "        # BufferedReader/Writer may raise OSError when",
                "986": "        # fetching `fileno()` (e.g. when wrapping BytesIO)."
            },
            "comment_deleted_diff": {},
            "comment_modified_diff": {}
        },
        {
            "commit": "c6a449c7972e97afd9401d098939fe29d3e7c891",
            "timestamp": "2023-07-12T10:33:01+02:00",
            "author": "Nathan Goldbaum",
            "commit_message": "ENH: Allow NEP 42 dtypes to use np.save and np.load (#24142)\n\nFixes #24110\r\n\r\nFirst, this makes it so that by default NEP 42 dtypes can't be pickled unless the dtype has a pickle implementation. Currently numpy will pickle them, but won't be able to unpickle them because the type code written to disk is invalid. Erroring is an improvement over writing corrupt files, I think.\r\n\r\nSecond, if a type can be pickled, this makes it so that np.save will save the array using pickle and will lie that the dtype is object (see @rkern's suggestion). I've made it so if this happens a UserWarning will get printed. Unfortunately there's no way to indicate in the file that this really isn't an object array, so I can't do much on the load side to detect when this happens. Hopefully the UserWarning at save time is enough? I think adding a way to indicate in the file that we're not really storing an object array would require a revision to the npy file format, which ideally I'd like to avoid.\r\n\r\nLast, added a pickle implementation to the scaled float test dtype and then added a test doing a round-trip save and load with a scale float array.",
            "additions": 26,
            "deletions": 3,
            "change_type": "MODIFY",
            "diff": "@@ -277,6 +277,23 @@ def dtype_to_descr(dtype):\n         # fiddled with. This needs to be fixed in the C implementation of\n         # dtype().\n         return dtype.descr\n+    elif not type(dtype)._legacy:\n+        # this must be a user-defined dtype since numpy does not yet expose any\n+        # non-legacy dtypes in the public API\n+        #\n+        # non-legacy dtypes don't yet have __array_interface__\n+        # support. Instead, as a hack, we use pickle to save the array, and lie\n+        # that the dtype is object. When the array is loaded, the descriptor is\n+        # unpickled with the array and the object dtype in the header is\n+        # discarded.\n+        #\n+        # a future NEP should define a way to serialize user-defined\n+        # descriptors and ideally work out the possible security implications\n+        warnings.warn(\"Custom dtypes are saved as python objects using the \"\n+                      \"pickle protocol. Loading this file requires \"\n+                      \"allow_pickle=True to be set.\",\n+                      UserWarning, stacklevel=2)\n+        return \"|O\"\n     else:\n         return dtype.str\n \n@@ -710,12 +727,18 @@ def write_array(fp, array, version=None, allow_pickle=True, pickle_kwargs=None):\n         # Set buffer size to 16 MiB to hide the Python loop overhead.\n         buffersize = max(16 * 1024 ** 2 // array.itemsize, 1)\n \n-    if array.dtype.hasobject:\n+    dtype_class = type(array.dtype)\n+\n+    if array.dtype.hasobject or not dtype_class._legacy:\n         # We contain Python objects so we cannot write out the data\n         # directly.  Instead, we will pickle it out\n         if not allow_pickle:\n-            raise ValueError(\"Object arrays cannot be saved when \"\n-                             \"allow_pickle=False\")\n+            if array.dtype.hasobject:\n+                raise ValueError(\"Object arrays cannot be saved when \"\n+                                 \"allow_pickle=False\")\n+            if not dtype_class._legacy:\n+                raise ValueError(\"User-defined dtypes cannot be saved \"\n+                                 \"when allow_pickle=False\")\n         if pickle_kwargs is None:\n             pickle_kwargs = {}\n         pickle.dump(array, fp, protocol=3, **pickle_kwargs)\n",
            "comment_added_diff": {
                "281": "        # this must be a user-defined dtype since numpy does not yet expose any",
                "282": "        # non-legacy dtypes in the public API",
                "283": "        #",
                "284": "        # non-legacy dtypes don't yet have __array_interface__",
                "285": "        # support. Instead, as a hack, we use pickle to save the array, and lie",
                "286": "        # that the dtype is object. When the array is loaded, the descriptor is",
                "287": "        # unpickled with the array and the object dtype in the header is",
                "288": "        # discarded.",
                "289": "        #",
                "290": "        # a future NEP should define a way to serialize user-defined",
                "291": "        # descriptors and ideally work out the possible security implications"
            },
            "comment_deleted_diff": {},
            "comment_modified_diff": {}
        }
    ],
    "cirrus_general.yml": [],
    "twodim_base.py": [],
    "_asarray.py": [],
    "arrayfunction_override.h": [],
    "bench_core.py": [
        {
            "commit": "3eb7c73df9fe7ad81f4863c6c0a91249257a46e4",
            "timestamp": "2023-03-04T15:42:56+00:00",
            "author": "Rohit Goswami",
            "commit_message": "BENCH: Add ndarray stats method calls",
            "additions": 36,
            "deletions": 10,
            "change_type": "MODIFY",
            "diff": "@@ -215,13 +215,39 @@ class Indices(Benchmark):\n     def time_indices(self):\n         np.indices((1000, 500))\n \n-class VarComplex(Benchmark):\n-    params = [10**n for n in range(0, 9)]\n-    def setup(self, n):\n-        self.arr = np.random.randn(n) + 1j * np.random.randn(n)\n-\n-    def teardown(self, n):\n-        del self.arr\n-\n-    def time_var(self, n):\n-        self.arr.var()\n+class StdStatsMethods(Benchmark):\n+    params = [['int8', 'uint8', 'int16', 'uint16', 'int32', 'uint32',\n+              'int64', 'uint64', 'float32', 'float64', 'intp',\n+              'complex64', 'complex128', 'complex256',\n+                'bool', 'float', 'int', 'complex'],\n+              [1000**n for n in range(0, 2)]]\n+    param_names = ['dtype', 'size']\n+\n+    def setup(self, dtype, size):\n+        try:\n+            self.data = np.ones(size, dtype=getattr(np, dtype))\n+        except AttributeError: # builtins throw AttributeError after 1.20\n+            self.data = np.ones(size, dtype=dtype)\n+        if dtype.startswith('complex'):\n+            self.data = np.random.randn(size) + 1j * np.random.randn(size)\n+\n+    def time_min(self, dtype, size):\n+        self.data.min()\n+\n+    def time_max(self, dtype, size):\n+        self.data.max()\n+\n+    def time_mean(self, dtype, size):\n+        self.data.mean()\n+\n+    def time_std(self, dtype, size):\n+        self.data.std()\n+\n+    def time_prod(self, dtype, size):\n+        self.data.prod()\n+\n+    def time_var(self, dtype, size):\n+        self.data.var()\n+\n+    def time_sum(self, dtype, size):\n+        self.data.sum()\n",
            "comment_added_diff": {
                "229": "        except AttributeError: # builtins throw AttributeError after 1.20"
            },
            "comment_deleted_diff": {},
            "comment_modified_diff": {}
        },
        {
            "commit": "41c7fba65be6060ee1b7023ccc0634f8e935970a",
            "timestamp": "2023-03-04T15:42:56+00:00",
            "author": "Rohit Goswami",
            "commit_message": "MAINT: Fixup BENCH to appease linter",
            "additions": 1,
            "deletions": 1,
            "change_type": "MODIFY",
            "diff": "@@ -226,7 +226,7 @@ class StdStatsMethods(Benchmark):\n     def setup(self, dtype, size):\n         try:\n             self.data = np.ones(size, dtype=getattr(np, dtype))\n-        except AttributeError: # builtins throw AttributeError after 1.20\n+        except AttributeError:  # builtins throw AttributeError after 1.20\n             self.data = np.ones(size, dtype=dtype)\n         if dtype.startswith('complex'):\n             self.data = np.random.randn(size) + 1j * np.random.randn(size)\n",
            "comment_added_diff": {
                "229": "        except AttributeError:  # builtins throw AttributeError after 1.20"
            },
            "comment_deleted_diff": {
                "229": "        except AttributeError: # builtins throw AttributeError after 1.20"
            },
            "comment_modified_diff": {
                "229": "        except AttributeError: # builtins throw AttributeError after 1.20"
            }
        },
        {
            "commit": "56d2fd5d8735e73f41ab30f42d139309df7fd758",
            "timestamp": "2023-03-04T16:02:03+00:00",
            "author": "Rohit Goswami",
            "commit_message": "BENCH: Reduce shapes and types for speed",
            "additions": 6,
            "deletions": 5,
            "change_type": "MODIFY",
            "diff": "@@ -217,11 +217,12 @@ def time_indices(self):\n \n \n class StatsMethods(Benchmark):\n-    params = [['int8', 'uint8', 'int16', 'uint16', 'int32', 'uint32',\n-              'int64', 'uint64', 'float32', 'float64', 'intp',\n-              'complex64', 'complex128', 'complex256',\n-                'bool', 'float', 'int', 'complex'],\n-              [1000**n for n in range(0, 2)]]\n+    # Not testing, but in array_api (redundant)\n+    # 8, 16, 32 bit variants, and 128 complexes\n+    params = [['int64', 'uint64', 'float64', 'intp',\n+               'complex64', 'bool', 'float', 'int',\n+               'complex', 'complex256'],\n+              [100**n for n in range(0, 2)]]\n     param_names = ['dtype', 'size']\n \n     def setup(self, dtype, size):\n",
            "comment_added_diff": {
                "220": "    # Not testing, but in array_api (redundant)",
                "221": "    # 8, 16, 32 bit variants, and 128 complexes"
            },
            "comment_deleted_diff": {},
            "comment_modified_diff": {
                "220": "    params = [['int8', 'uint8', 'int16', 'uint16', 'int32', 'uint32',",
                "221": "              'int64', 'uint64', 'float32', 'float64', 'intp',"
            }
        },
        {
            "commit": "8462f4fa01a506820edd525a89e438d5dd76b00d",
            "timestamp": "2023-08-27T22:40:43+02:00",
            "author": "Ralf Gommers",
            "commit_message": "BENCH: fix benchmarks in `bench_core.py`",
            "additions": 4,
            "deletions": 10,
            "change_type": "MODIFY",
            "diff": "@@ -217,19 +217,13 @@ def time_indices(self):\n \n \n class StatsMethods(Benchmark):\n-    # Not testing, but in array_api (redundant)\n-    # 8, 16, 32 bit variants, and 128 complexes\n-    params = [['int64', 'uint64', 'float64', 'intp',\n-               'complex64', 'bool', 'float', 'int',\n-               'complex', 'complex256'],\n-              [100**n for n in range(0, 2)]]\n+    params = [['int64', 'uint64', 'float32', 'float64',\n+               'complex64', 'bool_'],\n+              [100, 10000]]\n     param_names = ['dtype', 'size']\n \n     def setup(self, dtype, size):\n-        try:\n-            self.data = np.ones(size, dtype=getattr(np, dtype))\n-        except AttributeError:  # builtins throw AttributeError after 1.20\n-            self.data = np.ones(size, dtype=dtype)\n+        self.data = np.ones(size, dtype=dtype)\n         if dtype.startswith('complex'):\n             self.data = np.random.randn(size) + 1j * np.random.randn(size)\n \n",
            "comment_added_diff": {},
            "comment_deleted_diff": {
                "220": "    # Not testing, but in array_api (redundant)",
                "221": "    # 8, 16, 32 bit variants, and 128 complexes",
                "231": "        except AttributeError:  # builtins throw AttributeError after 1.20"
            },
            "comment_modified_diff": {}
        }
    ],
    "23019.expired.rst": [],
    "test_shape_base.py": [
        {
            "commit": "ff78e59f5a4ff5b4c5fbf58228a6be8dab9480a8",
            "timestamp": "2023-01-17T22:22:42+01:00",
            "author": "Sebastian Berg",
            "commit_message": "DEP: Finalize the non-sequence stacking deprecation\n\nThe `__array_function__` API currently will exhaust iterators so we\ncannot accept sequences reasonably.  Checking for `__getitem__` is presumably\nenough to reject that (and was what the deprecation used).\n\nFuture changes could allow this again, although it is not a useful API\nanyway, since we have to materialize the iterable in any case.",
            "additions": 8,
            "deletions": 7,
            "change_type": "MODIFY",
            "diff": "@@ -152,9 +152,9 @@ def test_2D_array(self):\n         assert_array_equal(res, desired)\n \n     def test_generator(self):\n-        with assert_warns(FutureWarning):\n+        with pytest.raises(TypeError, match=\"arrays to stack must be\"):\n             hstack((np.arange(3) for _ in range(2)))\n-        with assert_warns(FutureWarning):\n+        with pytest.raises(TypeError, match=\"arrays to stack must be\"):\n             hstack(map(lambda x: x, np.ones((3, 2))))\n \n     def test_casting_and_dtype(self):\n@@ -207,7 +207,7 @@ def test_2D_array2(self):\n         assert_array_equal(res, desired)\n \n     def test_generator(self):\n-        with assert_warns(FutureWarning):\n+        with pytest.raises(TypeError, match=\"arrays to stack must be\"):\n             vstack((np.arange(3) for _ in range(2)))\n \n     def test_casting_and_dtype(self):\n@@ -472,10 +472,11 @@ def test_stack():\n                         stack, [np.zeros((3, 3)), np.zeros(3)], axis=1)\n     assert_raises_regex(ValueError, 'must have the same shape',\n                         stack, [np.arange(2), np.arange(3)])\n-    # generator is deprecated\n-    with assert_warns(FutureWarning):\n-        result = stack((x for x in range(3)))\n-    assert_array_equal(result, np.array([0, 1, 2]))\n+\n+    # do not accept generators\n+    with pytest.raises(TypeError, match=\"arrays to stack must be\"):\n+        stack((x for x in range(3)))\n+\n     #casting and dtype test\n     a = np.array([1, 2, 3])\n     b = np.array([2.5, 3.5, 4.5])\n",
            "comment_added_diff": {
                "476": "    # do not accept generators"
            },
            "comment_deleted_diff": {
                "475": "    # generator is deprecated"
            },
            "comment_modified_diff": {
                "476": "    with assert_warns(FutureWarning):"
            }
        }
    ],
    "22776.improvement.rst": [],
    "core.pyi": [],
    "23011.deprecation.rst": [],
    "_dtype.py": [],
    "test_custom_dtypes.py": [
        {
            "commit": "07916b922a777130756b948a26d99328e0c56783",
            "timestamp": "2023-01-20T15:28:48+01:00",
            "author": "Sebastian Berg",
            "commit_message": "TST: Add test for wrapped ufuncs and reductions via ScaledFloat",
            "additions": 16,
            "deletions": 0,
            "change_type": "MODIFY",
            "diff": "@@ -202,3 +202,19 @@ def test_logical_ufuncs_casts_to_bool(self, ufunc):\n         # The output casting does not match the bool, bool -> bool loop:\n         with pytest.raises(TypeError):\n             ufunc(a, a, out=np.empty(a.shape, dtype=int), casting=\"equiv\")\n+\n+    def test_wrapped_and_wrapped_reductions(self):\n+        a = self._get_array(2.)\n+        float_equiv = a.astype(float)\n+\n+        expected = np.hypot(float_equiv, float_equiv)\n+        res = np.hypot(a, a)\n+        assert res.dtype == a.dtype\n+        res_float = res.view(np.float64) * 2\n+        assert_array_equal(res_float, expected)\n+\n+        # Also check reduction (keepdims, due to incorrect getitem)\n+        res = np.hypot.reduce(a, keepdims=True)\n+        assert res.dtype == a.dtype\n+        expected = np.hypot.reduce(float_equiv, keepdims=True)\n+        assert res.view(np.float64) * 2 == expected\n",
            "comment_added_diff": {
                "216": "        # Also check reduction (keepdims, due to incorrect getitem)"
            },
            "comment_deleted_diff": {},
            "comment_modified_diff": {}
        },
        {
            "commit": "96389f69e298729a33c2c6ad6bf994d338638d60",
            "timestamp": "2023-02-02T01:54:27+01:00",
            "author": "Sebastian Berg",
            "commit_message": "ENH: Allow trivial pickling of user DType (classes)\n\nThis also adds a `_legac` attribute, I am happy to rename it, but\nI suspect having something like it is useful.\n\nArguably, the best solution would be to give our DTypes a working\nmodule+name, but given that we are not there, this seems easy.\nWe could probably check for `__reduce__`, but I am not certain that\nwouldn't pre-empt successfully already, so this just restores the\ndefault for now.",
            "additions": 13,
            "deletions": 0,
            "change_type": "MODIFY",
            "diff": "@@ -218,3 +218,16 @@ def test_wrapped_and_wrapped_reductions(self):\n         assert res.dtype == a.dtype\n         expected = np.hypot.reduce(float_equiv, keepdims=True)\n         assert res.view(np.float64) * 2 == expected\n+\n+\n+def test_type_pickle():\n+    # can't actually unpickle, but we can pickle (if in namespace)\n+    import pickle\n+\n+    np._ScaledFloatTestDType = SF\n+\n+    s = pickle.dumps(SF)\n+    res = pickle.loads(s)\n+    assert res is SF\n+\n+    del np._ScaledFloatTestDType\n",
            "comment_added_diff": {
                "224": "    # can't actually unpickle, but we can pickle (if in namespace)"
            },
            "comment_deleted_diff": {},
            "comment_modified_diff": {}
        },
        {
            "commit": "a0c154a7f1d29c6d948af90d516c6cbe330d29cd",
            "timestamp": "2023-02-03T12:43:00+01:00",
            "author": "Sebastian Berg",
            "commit_message": "TST: Test new paths for `arr.astype()` to check it accepts DType classes",
            "additions": 10,
            "deletions": 0,
            "change_type": "MODIFY",
            "diff": "@@ -219,6 +219,16 @@ def test_wrapped_and_wrapped_reductions(self):\n         expected = np.hypot.reduce(float_equiv, keepdims=True)\n         assert res.view(np.float64) * 2 == expected\n \n+    def test_astype_class(self):\n+        # Very simple test that we accept `.astype()` also on the class.\n+        # ScaledFloat always returns the default descriptor, but it does\n+        # check the relevant code paths.\n+        arr = np.array([1., 2., 3.], dtype=object)\n+\n+        res = arr.astype(SF)  # passing the class class\n+        expected = arr.astype(SF(1.))  # above will have discovered 1. scaling\n+        assert_array_equal(res.view(np.float64), expected.view(np.float64))\n+\n \n def test_type_pickle():\n     # can't actually unpickle, but we can pickle (if in namespace)\n",
            "comment_added_diff": {
                "223": "        # Very simple test that we accept `.astype()` also on the class.",
                "224": "        # ScaledFloat always returns the default descriptor, but it does",
                "225": "        # check the relevant code paths.",
                "228": "        res = arr.astype(SF)  # passing the class class",
                "229": "        expected = arr.astype(SF(1.))  # above will have discovered 1. scaling"
            },
            "comment_deleted_diff": {},
            "comment_modified_diff": {}
        },
        {
            "commit": "aa8572c53ea0deecea6e920bb7ab9ab2bcf86eb3",
            "timestamp": "2023-06-09T16:08:36-06:00",
            "author": "Nathan Goldbaum",
            "commit_message": "ENH: let zeros, empty, and empty_like accept dtype classes",
            "additions": 8,
            "deletions": 0,
            "change_type": "MODIFY",
            "diff": "@@ -235,6 +235,14 @@ def test_creation_class(self):\n         arr2 = np.array([1., 2., 3.], dtype=SF(1.))\n         assert_array_equal(arr1.view(np.float64), arr2.view(np.float64))\n \n+        # empty and zeros don't support creating arrays using\n+        # a parametric dtype class\n+        with pytest.raises(RuntimeError):\n+            np.empty(3, dtype=SF)\n+\n+        with pytest.raises(RuntimeError):\n+            np.zeros(3, dtype=SF)\n+\n \n def test_type_pickle():\n     # can't actually unpickle, but we can pickle (if in namespace)\n",
            "comment_added_diff": {
                "238": "        # empty and zeros don't support creating arrays using",
                "239": "        # a parametric dtype class"
            },
            "comment_deleted_diff": {},
            "comment_modified_diff": {}
        },
        {
            "commit": "bb623847908e05eb16276fb38209b11cc0129b95",
            "timestamp": "2023-06-13T11:43:40-06:00",
            "author": "Nathan Goldbaum",
            "commit_message": "MAINT: refactor to use default_descr API hook",
            "additions": 7,
            "deletions": 7,
            "change_type": "MODIFY",
            "diff": "@@ -230,18 +230,18 @@ def test_astype_class(self):\n         assert_array_equal(res.view(np.float64), expected.view(np.float64))\n \n     def test_creation_class(self):\n+        # passing in a dtype class should return\n+        # the default descriptor\n         arr1 = np.array([1., 2., 3.], dtype=SF)\n         assert arr1.dtype == SF(1.)\n         arr2 = np.array([1., 2., 3.], dtype=SF(1.))\n         assert_array_equal(arr1.view(np.float64), arr2.view(np.float64))\n+        assert arr1.dtype == arr2.dtype\n \n-        # empty and zeros don't support creating arrays using\n-        # a parametric dtype class\n-        with pytest.raises(RuntimeError):\n-            np.empty(3, dtype=SF)\n-\n-        with pytest.raises(RuntimeError):\n-            np.zeros(3, dtype=SF)\n+        assert np.empty(3, dtype=SF).dtype == SF(1.)\n+        assert np.empty_like(arr1, dtype=SF).dtype == SF(1.)\n+        assert np.zeros(3, dtype=SF).dtype == SF(1.)\n+        assert np.zeros_like(arr1, dtype=SF).dtype == SF(1.)\n \n \n def test_type_pickle():\n",
            "comment_added_diff": {
                "233": "        # passing in a dtype class should return",
                "234": "        # the default descriptor"
            },
            "comment_deleted_diff": {
                "238": "        # empty and zeros don't support creating arrays using",
                "239": "        # a parametric dtype class"
            },
            "comment_modified_diff": {}
        },
        {
            "commit": "c6a449c7972e97afd9401d098939fe29d3e7c891",
            "timestamp": "2023-07-12T10:33:01+02:00",
            "author": "Nathan Goldbaum",
            "commit_message": "ENH: Allow NEP 42 dtypes to use np.save and np.load (#24142)\n\nFixes #24110\r\n\r\nFirst, this makes it so that by default NEP 42 dtypes can't be pickled unless the dtype has a pickle implementation. Currently numpy will pickle them, but won't be able to unpickle them because the type code written to disk is invalid. Erroring is an improvement over writing corrupt files, I think.\r\n\r\nSecond, if a type can be pickled, this makes it so that np.save will save the array using pickle and will lie that the dtype is object (see @rkern's suggestion). I've made it so if this happens a UserWarning will get printed. Unfortunately there's no way to indicate in the file that this really isn't an object array, so I can't do much on the load side to detect when this happens. Hopefully the UserWarning at save time is enough? I think adding a way to indicate in the file that we're not really storing an object array would require a revision to the npy file format, which ideally I'd like to avoid.\r\n\r\nLast, added a pickle implementation to the scaled float test dtype and then added a test doing a round-trip save and load with a scale float array.",
            "additions": 24,
            "deletions": 0,
            "change_type": "MODIFY",
            "diff": "@@ -1,3 +1,5 @@\n+from tempfile import NamedTemporaryFile\n+\n import pytest\n \n import numpy as np\n@@ -243,6 +245,28 @@ def test_creation_class(self):\n         assert np.zeros(3, dtype=SF).dtype == SF(1.)\n         assert np.zeros_like(arr1, dtype=SF).dtype == SF(1.)\n \n+    def test_np_save_load(self):\n+        # this monkeypatch is needed because pickle\n+        # uses the repr of a type to reconstruct it\n+        np._ScaledFloatTestDType = SF\n+\n+        arr = np.array([1.0, 2.0, 3.0], dtype=SF(1.0))\n+\n+        # adapted from RoundtripTest.roundtrip in np.save tests\n+        with NamedTemporaryFile(\"wb\", delete=False, suffix=\".npz\") as f:\n+            with pytest.warns(UserWarning) as record:\n+                np.savez(f.name, arr)\n+\n+        assert len(record) == 1\n+\n+        with np.load(f.name, allow_pickle=True) as data:\n+            larr = data[\"arr_0\"]\n+        assert_array_equal(arr.view(np.float64), larr.view(np.float64))\n+        assert larr.dtype == arr.dtype == SF(1.0)\n+\n+        del np._ScaledFloatTestDType\n+\n+\n \n def test_type_pickle():\n     # can't actually unpickle, but we can pickle (if in namespace)\n",
            "comment_added_diff": {
                "249": "        # this monkeypatch is needed because pickle",
                "250": "        # uses the repr of a type to reconstruct it",
                "255": "        # adapted from RoundtripTest.roundtrip in np.save tests"
            },
            "comment_deleted_diff": {},
            "comment_modified_diff": {}
        },
        {
            "commit": "62eec147f77a163dfd319b19b70161a6d449da26",
            "timestamp": "2023-09-20T10:13:44-06:00",
            "author": "Nathan Goldbaum",
            "commit_message": "ENH: fix printing structured dtypes with a non-legacy dtype member",
            "additions": 6,
            "deletions": 0,
            "change_type": "MODIFY",
            "diff": "@@ -51,6 +51,12 @@ def test_repr(self):\n     def test_dtype_name(self):\n         assert SF(1.).name == \"_ScaledFloatTestDType64\"\n \n+    def test_sfloat_structured_dtype_printing(self):\n+        dt = np.dtype([(\"id\", int), (\"value\", SF(0.5))])\n+        # repr of structured dtypes need special handling because the\n+        # implementation bypasses the object repr\n+        assert \"('value', '_ScaledFloatTestDType64')\" in repr(dt)\n+\n     @pytest.mark.parametrize(\"scaling\", [1., -1., 2.])\n     def test_sfloat_from_float(self, scaling):\n         a = np.array([1., 2., 3.]).astype(dtype=SF(scaling))\n",
            "comment_added_diff": {
                "56": "        # repr of structured dtypes need special handling because the",
                "57": "        # implementation bypasses the object repr"
            },
            "comment_deleted_diff": {},
            "comment_modified_diff": {}
        }
    ],
    "23041.expired.rst": [],
    "decorators.py": [
        {
            "commit": "f75bb0edb0e6eec2564de4bf798242984860a19b",
            "timestamp": "2023-01-19T14:35:44-07:00",
            "author": "Charles Harris",
            "commit_message": "MAINT: Remove all nose testing support.\n\nNumPy switched to using pytest in 2018 and nose has been unmaintained\nfor many years. We have kept NumPy's nose support to avoid breaking\ndownstream projects who might have been using it and not yet switched to\npytest or some other testing framework. With the arrival of Python 3.12,\nunpatched nose will raise an error. It it time to move on.\n\nDecorators removed\n\n- raises\n- slow\n- setastest\n- skipif\n- knownfailif\n- deprecated\n- parametrize\n- _needs_refcount\n\nThese are not to be confused with pytest versions with similar names,\ne.g., pytest.mark.slow, pytest.mark.skipif, pytest.mark.parametrize.\n\nFunctions removed\n\n- Tester\n- import_nose\n- run_module_suite",
            "additions": 0,
            "deletions": 331,
            "change_type": "DELETE",
            "diff": "@@ -1,331 +0,0 @@\n-\"\"\"\n-Decorators for labeling and modifying behavior of test objects.\n-\n-Decorators that merely return a modified version of the original\n-function object are straightforward. Decorators that return a new\n-function object need to use\n-::\n-\n-  nose.tools.make_decorator(original_function)(decorator)\n-\n-in returning the decorator, in order to preserve meta-data such as\n-function name, setup and teardown functions and so on - see\n-``nose.tools`` for more information.\n-\n-\"\"\"\n-import collections.abc\n-import warnings\n-\n-from .utils import SkipTest, assert_warns, HAS_REFCOUNT\n-\n-__all__ = ['slow', 'setastest', 'skipif', 'knownfailureif', 'deprecated',\n-           'parametrize', '_needs_refcount',]\n-\n-\n-def slow(t):\n-    \"\"\"\n-    .. deprecated:: 1.21\n-        This decorator is retained for compatibility with the nose testing framework, which is being phased out.\n-        Please use the nose2 or pytest frameworks instead.\n-\n-    Label a test as 'slow'.\n-\n-    The exact definition of a slow test is obviously both subjective and\n-    hardware-dependent, but in general any individual test that requires more\n-    than a second or two should be labeled as slow (the whole suite consists of\n-    thousands of tests, so even a second is significant).\n-\n-    Parameters\n-    ----------\n-    t : callable\n-        The test to label as slow.\n-\n-    Returns\n-    -------\n-    t : callable\n-        The decorated test `t`.\n-\n-    Examples\n-    --------\n-    The `numpy.testing` module includes ``import decorators as dec``.\n-    A test can be decorated as slow like this::\n-\n-      from numpy.testing import *\n-\n-      @dec.slow\n-      def test_big(self):\n-          print('Big, slow test')\n-\n-    \"\"\"\n-    # Numpy 1.21, 2020-12-20\n-    warnings.warn('the np.testing.dec decorators are included for nose support, and are '\n-                'deprecated since NumPy v1.21. Use the nose2 or pytest frameworks instead.', DeprecationWarning, stacklevel=2)\n-\n-    t.slow = True\n-    return t\n-\n-def setastest(tf=True):\n-    \"\"\"\n-    .. deprecated:: 1.21\n-        This decorator is retained for compatibility with the nose testing framework, which is being phased out.\n-        Please use the nose2 or pytest frameworks instead.\n-\n-    Signals to nose that this function is or is not a test.\n-\n-    Parameters\n-    ----------\n-    tf : bool\n-        If True, specifies that the decorated callable is a test.\n-        If False, specifies that the decorated callable is not a test.\n-        Default is True.\n-\n-    Notes\n-    -----\n-    This decorator can't use the nose namespace, because it can be\n-    called from a non-test module. See also ``istest`` and ``nottest`` in\n-    ``nose.tools``.\n-\n-    Examples\n-    --------\n-    `setastest` can be used in the following way::\n-\n-      from numpy.testing import dec\n-\n-      @dec.setastest(False)\n-      def func_with_test_in_name(arg1, arg2):\n-          pass\n-\n-    \"\"\"\n-    # Numpy 1.21, 2020-12-20\n-    warnings.warn('the np.testing.dec decorators are included for nose support, and are '\n-            'deprecated since NumPy v1.21. Use the nose2 or pytest frameworks instead.', DeprecationWarning, stacklevel=2)\n-    def set_test(t):\n-        t.__test__ = tf\n-        return t\n-    return set_test\n-\n-def skipif(skip_condition, msg=None):\n-    \"\"\"\n-    .. deprecated:: 1.21\n-        This decorator is retained for compatibility with the nose testing framework, which is being phased out.\n-        Please use the nose2 or pytest frameworks instead.\n-\n-    Make function raise SkipTest exception if a given condition is true.\n-\n-    If the condition is a callable, it is used at runtime to dynamically\n-    make the decision. This is useful for tests that may require costly\n-    imports, to delay the cost until the test suite is actually executed.\n-\n-    Parameters\n-    ----------\n-    skip_condition : bool or callable\n-        Flag to determine whether to skip the decorated test.\n-    msg : str, optional\n-        Message to give on raising a SkipTest exception. Default is None.\n-\n-    Returns\n-    -------\n-    decorator : function\n-        Decorator which, when applied to a function, causes SkipTest\n-        to be raised when `skip_condition` is True, and the function\n-        to be called normally otherwise.\n-\n-    Notes\n-    -----\n-    The decorator itself is decorated with the ``nose.tools.make_decorator``\n-    function in order to transmit function name, and various other metadata.\n-\n-    \"\"\"\n-\n-    def skip_decorator(f):\n-        # Local import to avoid a hard nose dependency and only incur the\n-        # import time overhead at actual test-time.\n-        import nose\n-\n-        # Numpy 1.21, 2020-12-20\n-        warnings.warn('the np.testing.dec decorators are included for nose support, and are '\n-            'deprecated since NumPy v1.21. Use the nose2 or pytest frameworks instead.', DeprecationWarning, stacklevel=2)\n-\n-        # Allow for both boolean or callable skip conditions.\n-        if isinstance(skip_condition, collections.abc.Callable):\n-            skip_val = lambda: skip_condition()\n-        else:\n-            skip_val = lambda: skip_condition\n-\n-        def get_msg(func,msg=None):\n-            \"\"\"Skip message with information about function being skipped.\"\"\"\n-            if msg is None:\n-                out = 'Test skipped due to test condition'\n-            else:\n-                out = msg\n-\n-            return f'Skipping test: {func.__name__}: {out}'\n-\n-        # We need to define *two* skippers because Python doesn't allow both\n-        # return with value and yield inside the same function.\n-        def skipper_func(*args, **kwargs):\n-            \"\"\"Skipper for normal test functions.\"\"\"\n-            if skip_val():\n-                raise SkipTest(get_msg(f, msg))\n-            else:\n-                return f(*args, **kwargs)\n-\n-        def skipper_gen(*args, **kwargs):\n-            \"\"\"Skipper for test generators.\"\"\"\n-            if skip_val():\n-                raise SkipTest(get_msg(f, msg))\n-            else:\n-                yield from f(*args, **kwargs)\n-\n-        # Choose the right skipper to use when building the actual decorator.\n-        if nose.util.isgenerator(f):\n-            skipper = skipper_gen\n-        else:\n-            skipper = skipper_func\n-\n-        return nose.tools.make_decorator(f)(skipper)\n-\n-    return skip_decorator\n-\n-\n-def knownfailureif(fail_condition, msg=None):\n-    \"\"\"\n-    .. deprecated:: 1.21\n-        This decorator is retained for compatibility with the nose testing framework, which is being phased out.\n-        Please use the nose2 or pytest frameworks instead.\n-\n-    Make function raise KnownFailureException exception if given condition is true.\n-\n-    If the condition is a callable, it is used at runtime to dynamically\n-    make the decision. This is useful for tests that may require costly\n-    imports, to delay the cost until the test suite is actually executed.\n-\n-    Parameters\n-    ----------\n-    fail_condition : bool or callable\n-        Flag to determine whether to mark the decorated test as a known\n-        failure (if True) or not (if False).\n-    msg : str, optional\n-        Message to give on raising a KnownFailureException exception.\n-        Default is None.\n-\n-    Returns\n-    -------\n-    decorator : function\n-        Decorator, which, when applied to a function, causes\n-        KnownFailureException to be raised when `fail_condition` is True,\n-        and the function to be called normally otherwise.\n-\n-    Notes\n-    -----\n-    The decorator itself is decorated with the ``nose.tools.make_decorator``\n-    function in order to transmit function name, and various other metadata.\n-\n-    \"\"\"\n-    # Numpy 1.21, 2020-12-20\n-    warnings.warn('the np.testing.dec decorators are included for nose support, and are '\n-            'deprecated since NumPy v1.21. Use the nose2 or pytest frameworks instead.', DeprecationWarning, stacklevel=2)\n-\n-    if msg is None:\n-        msg = 'Test skipped due to known failure'\n-\n-    # Allow for both boolean or callable known failure conditions.\n-    if isinstance(fail_condition, collections.abc.Callable):\n-        fail_val = lambda: fail_condition()\n-    else:\n-        fail_val = lambda: fail_condition\n-\n-    def knownfail_decorator(f):\n-        # Local import to avoid a hard nose dependency and only incur the\n-        # import time overhead at actual test-time.\n-        import nose\n-        from .noseclasses import KnownFailureException\n-\n-        def knownfailer(*args, **kwargs):\n-            if fail_val():\n-                raise KnownFailureException(msg)\n-            else:\n-                return f(*args, **kwargs)\n-        return nose.tools.make_decorator(f)(knownfailer)\n-\n-    return knownfail_decorator\n-\n-def deprecated(conditional=True):\n-    \"\"\"\n-    .. deprecated:: 1.21\n-        This decorator is retained for compatibility with the nose testing framework, which is being phased out.\n-        Please use the nose2 or pytest frameworks instead.\n-\n-    Filter deprecation warnings while running the test suite.\n-\n-    This decorator can be used to filter DeprecationWarning's, to avoid\n-    printing them during the test suite run, while checking that the test\n-    actually raises a DeprecationWarning.\n-\n-    Parameters\n-    ----------\n-    conditional : bool or callable, optional\n-        Flag to determine whether to mark test as deprecated or not. If the\n-        condition is a callable, it is used at runtime to dynamically make the\n-        decision. Default is True.\n-\n-    Returns\n-    -------\n-    decorator : function\n-        The `deprecated` decorator itself.\n-\n-    Notes\n-    -----\n-    .. versionadded:: 1.4.0\n-\n-    \"\"\"\n-    def deprecate_decorator(f):\n-        # Local import to avoid a hard nose dependency and only incur the\n-        # import time overhead at actual test-time.\n-        import nose\n-\n-        # Numpy 1.21, 2020-12-20\n-        warnings.warn('the np.testing.dec decorators are included for nose support, and are '\n-            'deprecated since NumPy v1.21. Use the nose2 or pytest frameworks instead.', DeprecationWarning, stacklevel=2)\n-\n-        def _deprecated_imp(*args, **kwargs):\n-            # Poor man's replacement for the with statement\n-            with assert_warns(DeprecationWarning):\n-                f(*args, **kwargs)\n-\n-        if isinstance(conditional, collections.abc.Callable):\n-            cond = conditional()\n-        else:\n-            cond = conditional\n-        if cond:\n-            return nose.tools.make_decorator(f)(_deprecated_imp)\n-        else:\n-            return f\n-    return deprecate_decorator\n-\n-\n-def parametrize(vars, input):\n-    \"\"\"\n-    .. deprecated:: 1.21\n-        This decorator is retained for compatibility with the nose testing framework, which is being phased out.\n-        Please use the nose2 or pytest frameworks instead.\n-\n-    Pytest compatibility class. This implements the simplest level of\n-    pytest.mark.parametrize for use in nose as an aid in making the transition\n-    to pytest. It achieves that by adding a dummy var parameter and ignoring\n-    the doc_func parameter of the base class. It does not support variable\n-    substitution by name, nor does it support nesting or classes. See the\n-    pytest documentation for usage.\n-\n-    .. versionadded:: 1.14.0\n-\n-    \"\"\"\n-    from .parameterized import parameterized\n-\n-    # Numpy 1.21, 2020-12-20\n-    warnings.warn('the np.testing.dec decorators are included for nose support, and are '\n-            'deprecated since NumPy v1.21. Use the nose2 or pytest frameworks instead.', DeprecationWarning, stacklevel=2)\n-\n-    return parameterized(input)\n-\n-_needs_refcount = skipif(not HAS_REFCOUNT, \"python has no sys.getrefcount\")\n",
            "comment_added_diff": {},
            "comment_deleted_diff": {
                "60": "    # Numpy 1.21, 2020-12-20",
                "99": "    # Numpy 1.21, 2020-12-20",
                "141": "        # Local import to avoid a hard nose dependency and only incur the",
                "142": "        # import time overhead at actual test-time.",
                "145": "        # Numpy 1.21, 2020-12-20",
                "149": "        # Allow for both boolean or callable skip conditions.",
                "164": "        # We need to define *two* skippers because Python doesn't allow both",
                "165": "        # return with value and yield inside the same function.",
                "180": "        # Choose the right skipper to use when building the actual decorator.",
                "225": "    # Numpy 1.21, 2020-12-20",
                "232": "    # Allow for both boolean or callable known failure conditions.",
                "239": "        # Local import to avoid a hard nose dependency and only incur the",
                "240": "        # import time overhead at actual test-time.",
                "283": "        # Local import to avoid a hard nose dependency and only incur the",
                "284": "        # import time overhead at actual test-time.",
                "287": "        # Numpy 1.21, 2020-12-20",
                "292": "            # Poor man's replacement for the with statement",
                "325": "    # Numpy 1.21, 2020-12-20"
            },
            "comment_modified_diff": {}
        }
    ],
    "noseclasses.py": [
        {
            "commit": "f75bb0edb0e6eec2564de4bf798242984860a19b",
            "timestamp": "2023-01-19T14:35:44-07:00",
            "author": "Charles Harris",
            "commit_message": "MAINT: Remove all nose testing support.\n\nNumPy switched to using pytest in 2018 and nose has been unmaintained\nfor many years. We have kept NumPy's nose support to avoid breaking\ndownstream projects who might have been using it and not yet switched to\npytest or some other testing framework. With the arrival of Python 3.12,\nunpatched nose will raise an error. It it time to move on.\n\nDecorators removed\n\n- raises\n- slow\n- setastest\n- skipif\n- knownfailif\n- deprecated\n- parametrize\n- _needs_refcount\n\nThese are not to be confused with pytest versions with similar names,\ne.g., pytest.mark.slow, pytest.mark.skipif, pytest.mark.parametrize.\n\nFunctions removed\n\n- Tester\n- import_nose\n- run_module_suite",
            "additions": 0,
            "deletions": 364,
            "change_type": "DELETE",
            "diff": "@@ -1,364 +0,0 @@\n-# These classes implement a doctest runner plugin for nose, a \"known failure\"\n-# error class, and a customized TestProgram for NumPy.\n-\n-# Because this module imports nose directly, it should not\n-# be used except by nosetester.py to avoid a general NumPy\n-# dependency on nose.\n-import os\n-import sys\n-import doctest\n-import inspect\n-\n-import numpy\n-import nose\n-from nose.plugins import doctests as npd\n-from nose.plugins.errorclass import ErrorClass, ErrorClassPlugin\n-from nose.plugins.base import Plugin\n-from nose.util import src\n-from .nosetester import get_package_name\n-from .utils import KnownFailureException, KnownFailureTest\n-\n-\n-# Some of the classes in this module begin with 'Numpy' to clearly distinguish\n-# them from the plethora of very similar names from nose/unittest/doctest\n-\n-#-----------------------------------------------------------------------------\n-# Modified version of the one in the stdlib, that fixes a python bug (doctests\n-# not found in extension modules, https://bugs.python.org/issue3158)\n-class NumpyDocTestFinder(doctest.DocTestFinder):\n-\n-    def _from_module(self, module, object):\n-        \"\"\"\n-        Return true if the given object is defined in the given\n-        module.\n-        \"\"\"\n-        if module is None:\n-            return True\n-        elif inspect.isfunction(object):\n-            return module.__dict__ is object.__globals__\n-        elif inspect.isbuiltin(object):\n-            return module.__name__ == object.__module__\n-        elif inspect.isclass(object):\n-            return module.__name__ == object.__module__\n-        elif inspect.ismethod(object):\n-            # This one may be a bug in cython that fails to correctly set the\n-            # __module__ attribute of methods, but since the same error is easy\n-            # to make by extension code writers, having this safety in place\n-            # isn't such a bad idea\n-            return module.__name__ == object.__self__.__class__.__module__\n-        elif inspect.getmodule(object) is not None:\n-            return module is inspect.getmodule(object)\n-        elif hasattr(object, '__module__'):\n-            return module.__name__ == object.__module__\n-        elif isinstance(object, property):\n-            return True  # [XX] no way not be sure.\n-        else:\n-            raise ValueError(\"object must be a class or function\")\n-\n-    def _find(self, tests, obj, name, module, source_lines, globs, seen):\n-        \"\"\"\n-        Find tests for the given object and any contained objects, and\n-        add them to `tests`.\n-        \"\"\"\n-\n-        doctest.DocTestFinder._find(self, tests, obj, name, module,\n-                                    source_lines, globs, seen)\n-\n-        # Below we re-run pieces of the above method with manual modifications,\n-        # because the original code is buggy and fails to correctly identify\n-        # doctests in extension modules.\n-\n-        # Local shorthands\n-        from inspect import (\n-            isroutine, isclass, ismodule, isfunction, ismethod\n-            )\n-\n-        # Look for tests in a module's contained objects.\n-        if ismodule(obj) and self._recurse:\n-            for valname, val in obj.__dict__.items():\n-                valname1 = f'{name}.{valname}'\n-                if ( (isroutine(val) or isclass(val))\n-                     and self._from_module(module, val)):\n-\n-                    self._find(tests, val, valname1, module, source_lines,\n-                               globs, seen)\n-\n-        # Look for tests in a class's contained objects.\n-        if isclass(obj) and self._recurse:\n-            for valname, val in obj.__dict__.items():\n-                # Special handling for staticmethod/classmethod.\n-                if isinstance(val, staticmethod):\n-                    val = getattr(obj, valname)\n-                if isinstance(val, classmethod):\n-                    val = getattr(obj, valname).__func__\n-\n-                # Recurse to methods, properties, and nested classes.\n-                if ((isfunction(val) or isclass(val) or\n-                     ismethod(val) or isinstance(val, property)) and\n-                      self._from_module(module, val)):\n-                    valname = f'{name}.{valname}'\n-                    self._find(tests, val, valname, module, source_lines,\n-                               globs, seen)\n-\n-\n-# second-chance checker; if the default comparison doesn't\n-# pass, then see if the expected output string contains flags that\n-# tell us to ignore the output\n-class NumpyOutputChecker(doctest.OutputChecker):\n-    def check_output(self, want, got, optionflags):\n-        ret = doctest.OutputChecker.check_output(self, want, got,\n-                                                 optionflags)\n-        if not ret:\n-            if \"#random\" in want:\n-                return True\n-\n-            # it would be useful to normalize endianness so that\n-            # bigendian machines don't fail all the tests (and there are\n-            # actually some bigendian examples in the doctests). Let's try\n-            # making them all little endian\n-            got = got.replace(\"'>\", \"'<\")\n-            want = want.replace(\"'>\", \"'<\")\n-\n-            # try to normalize out 32 and 64 bit default int sizes\n-            for sz in [4, 8]:\n-                got = got.replace(\"'<i%d'\" % sz, \"int\")\n-                want = want.replace(\"'<i%d'\" % sz, \"int\")\n-\n-            ret = doctest.OutputChecker.check_output(self, want,\n-                    got, optionflags)\n-\n-        return ret\n-\n-\n-# Subclass nose.plugins.doctests.DocTestCase to work around a bug in\n-# its constructor that blocks non-default arguments from being passed\n-# down into doctest.DocTestCase\n-class NumpyDocTestCase(npd.DocTestCase):\n-    def __init__(self, test, optionflags=0, setUp=None, tearDown=None,\n-                 checker=None, obj=None, result_var='_'):\n-        self._result_var = result_var\n-        self._nose_obj = obj\n-        doctest.DocTestCase.__init__(self, test,\n-                                     optionflags=optionflags,\n-                                     setUp=setUp, tearDown=tearDown,\n-                                     checker=checker)\n-\n-\n-print_state = numpy.get_printoptions()\n-\n-class NumpyDoctest(npd.Doctest):\n-    name = 'numpydoctest'   # call nosetests with --with-numpydoctest\n-    score = 1000  # load late, after doctest builtin\n-\n-    # always use whitespace and ellipsis options for doctests\n-    doctest_optflags = doctest.NORMALIZE_WHITESPACE | doctest.ELLIPSIS\n-\n-    # files that should be ignored for doctests\n-    doctest_ignore = ['generate_numpy_api.py',\n-                      'setup.py']\n-\n-    # Custom classes; class variables to allow subclassing\n-    doctest_case_class = NumpyDocTestCase\n-    out_check_class = NumpyOutputChecker\n-    test_finder_class = NumpyDocTestFinder\n-\n-    # Don't use the standard doctest option handler; hard-code the option values\n-    def options(self, parser, env=os.environ):\n-        Plugin.options(self, parser, env)\n-        # Test doctests in 'test' files / directories. Standard plugin default\n-        # is False\n-        self.doctest_tests = True\n-        # Variable name; if defined, doctest results stored in this variable in\n-        # the top-level namespace.  None is the standard default\n-        self.doctest_result_var = None\n-\n-    def configure(self, options, config):\n-        # parent method sets enabled flag from command line --with-numpydoctest\n-        Plugin.configure(self, options, config)\n-        self.finder = self.test_finder_class()\n-        self.parser = doctest.DocTestParser()\n-        if self.enabled:\n-            # Pull standard doctest out of plugin list; there's no reason to run\n-            # both.  In practice the Unplugger plugin above would cover us when\n-            # run from a standard numpy.test() call; this is just in case\n-            # someone wants to run our plugin outside the numpy.test() machinery\n-            config.plugins.plugins = [p for p in config.plugins.plugins\n-                                      if p.name != 'doctest']\n-\n-    def set_test_context(self, test):\n-        \"\"\" Configure `test` object to set test context\n-\n-        We set the numpy / scipy standard doctest namespace\n-\n-        Parameters\n-        ----------\n-        test : test object\n-            with ``globs`` dictionary defining namespace\n-\n-        Returns\n-        -------\n-        None\n-\n-        Notes\n-        -----\n-        `test` object modified in place\n-        \"\"\"\n-        # set the namespace for tests\n-        pkg_name = get_package_name(os.path.dirname(test.filename))\n-\n-        # Each doctest should execute in an environment equivalent to\n-        # starting Python and executing \"import numpy as np\", and,\n-        # for SciPy packages, an additional import of the local\n-        # package (so that scipy.linalg.basic.py's doctests have an\n-        # implicit \"from scipy import linalg\" as well).\n-        #\n-        # Note: __file__ allows the doctest in NoseTester to run\n-        # without producing an error\n-        test.globs = {'__builtins__':__builtins__,\n-                      '__file__':'__main__',\n-                      '__name__':'__main__',\n-                      'np':numpy}\n-        # add appropriate scipy import for SciPy tests\n-        if 'scipy' in pkg_name:\n-            p = pkg_name.split('.')\n-            p2 = p[-1]\n-            test.globs[p2] = __import__(pkg_name, test.globs, {}, [p2])\n-\n-    # Override test loading to customize test context (with set_test_context\n-    # method), set standard docstring options, and install our own test output\n-    # checker\n-    def loadTestsFromModule(self, module):\n-        if not self.matches(module.__name__):\n-            npd.log.debug(\"Doctest doesn't want module %s\", module)\n-            return\n-        try:\n-            tests = self.finder.find(module)\n-        except AttributeError:\n-            # nose allows module.__test__ = False; doctest does not and\n-            # throws AttributeError\n-            return\n-        if not tests:\n-            return\n-        tests.sort()\n-        module_file = src(module.__file__)\n-        for test in tests:\n-            if not test.examples:\n-                continue\n-            if not test.filename:\n-                test.filename = module_file\n-            # Set test namespace; test altered in place\n-            self.set_test_context(test)\n-            yield self.doctest_case_class(test,\n-                                          optionflags=self.doctest_optflags,\n-                                          checker=self.out_check_class(),\n-                                          result_var=self.doctest_result_var)\n-\n-    # Add an afterContext method to nose.plugins.doctests.Doctest in order\n-    # to restore print options to the original state after each doctest\n-    def afterContext(self):\n-        numpy.set_printoptions(**print_state)\n-\n-    # Ignore NumPy-specific build files that shouldn't be searched for tests\n-    def wantFile(self, file):\n-        bn = os.path.basename(file)\n-        if bn in self.doctest_ignore:\n-            return False\n-        return npd.Doctest.wantFile(self, file)\n-\n-\n-class Unplugger:\n-    \"\"\" Nose plugin to remove named plugin late in loading\n-\n-    By default it removes the \"doctest\" plugin.\n-    \"\"\"\n-    name = 'unplugger'\n-    enabled = True  # always enabled\n-    score = 4000  # load late in order to be after builtins\n-\n-    def __init__(self, to_unplug='doctest'):\n-        self.to_unplug = to_unplug\n-\n-    def options(self, parser, env):\n-        pass\n-\n-    def configure(self, options, config):\n-        # Pull named plugin out of plugins list\n-        config.plugins.plugins = [p for p in config.plugins.plugins\n-                                  if p.name != self.to_unplug]\n-\n-\n-class KnownFailurePlugin(ErrorClassPlugin):\n-    '''Plugin that installs a KNOWNFAIL error class for the\n-    KnownFailureClass exception.  When KnownFailure is raised,\n-    the exception will be logged in the knownfail attribute of the\n-    result, 'K' or 'KNOWNFAIL' (verbose) will be output, and the\n-    exception will not be counted as an error or failure.'''\n-    enabled = True\n-    knownfail = ErrorClass(KnownFailureException,\n-                           label='KNOWNFAIL',\n-                           isfailure=False)\n-\n-    def options(self, parser, env=os.environ):\n-        env_opt = 'NOSE_WITHOUT_KNOWNFAIL'\n-        parser.add_option('--no-knownfail', action='store_true',\n-                          dest='noKnownFail', default=env.get(env_opt, False),\n-                          help='Disable special handling of KnownFailure '\n-                               'exceptions')\n-\n-    def configure(self, options, conf):\n-        if not self.can_configure:\n-            return\n-        self.conf = conf\n-        disable = getattr(options, 'noKnownFail', False)\n-        if disable:\n-            self.enabled = False\n-\n-KnownFailure = KnownFailurePlugin   # backwards compat\n-\n-\n-class FPUModeCheckPlugin(Plugin):\n-    \"\"\"\n-    Plugin that checks the FPU mode before and after each test,\n-    raising failures if the test changed the mode.\n-    \"\"\"\n-\n-    def prepareTestCase(self, test):\n-        from numpy.core._multiarray_tests import get_fpu_mode\n-\n-        def run(result):\n-            old_mode = get_fpu_mode()\n-            test.test(result)\n-            new_mode = get_fpu_mode()\n-\n-            if old_mode != new_mode:\n-                try:\n-                    raise AssertionError(\n-                        \"FPU mode changed from {0:#x} to {1:#x} during the \"\n-                        \"test\".format(old_mode, new_mode))\n-                except AssertionError:\n-                    result.addFailure(test, sys.exc_info())\n-\n-        return run\n-\n-\n-# Class allows us to save the results of the tests in runTests - see runTests\n-# method docstring for details\n-class NumpyTestProgram(nose.core.TestProgram):\n-    def runTests(self):\n-        \"\"\"Run Tests. Returns true on success, false on failure, and\n-        sets self.success to the same value.\n-\n-        Because nose currently discards the test result object, but we need\n-        to return it to the user, override TestProgram.runTests to retain\n-        the result\n-        \"\"\"\n-        if self.testRunner is None:\n-            self.testRunner = nose.core.TextTestRunner(stream=self.config.stream,\n-                                                       verbosity=self.config.verbosity,\n-                                                       config=self.config)\n-        plug_runner = self.config.plugins.prepareTestRunner(self.testRunner)\n-        if plug_runner is not None:\n-            self.testRunner = plug_runner\n-        self.result = self.testRunner.run(self.test)\n-        self.success = self.result.wasSuccessful()\n-        return self.success\n",
            "comment_added_diff": {},
            "comment_deleted_diff": {
                "1": "# These classes implement a doctest runner plugin for nose, a \"known failure\"",
                "2": "# error class, and a customized TestProgram for NumPy.",
                "4": "# Because this module imports nose directly, it should not",
                "5": "# be used except by nosetester.py to avoid a general NumPy",
                "6": "# dependency on nose.",
                "22": "# Some of the classes in this module begin with 'Numpy' to clearly distinguish",
                "23": "# them from the plethora of very similar names from nose/unittest/doctest",
                "25": "#-----------------------------------------------------------------------------",
                "26": "# Modified version of the one in the stdlib, that fixes a python bug (doctests",
                "27": "# not found in extension modules, https://bugs.python.org/issue3158)",
                "44": "            # This one may be a bug in cython that fails to correctly set the",
                "45": "            # __module__ attribute of methods, but since the same error is easy",
                "46": "            # to make by extension code writers, having this safety in place",
                "47": "            # isn't such a bad idea",
                "54": "            return True  # [XX] no way not be sure.",
                "67": "        # Below we re-run pieces of the above method with manual modifications,",
                "68": "        # because the original code is buggy and fails to correctly identify",
                "69": "        # doctests in extension modules.",
                "71": "        # Local shorthands",
                "76": "        # Look for tests in a module's contained objects.",
                "86": "        # Look for tests in a class's contained objects.",
                "89": "                # Special handling for staticmethod/classmethod.",
                "95": "                # Recurse to methods, properties, and nested classes.",
                "104": "# second-chance checker; if the default comparison doesn't",
                "105": "# pass, then see if the expected output string contains flags that",
                "106": "# tell us to ignore the output",
                "112": "            if \"#random\" in want:",
                "115": "            # it would be useful to normalize endianness so that",
                "116": "            # bigendian machines don't fail all the tests (and there are",
                "117": "            # actually some bigendian examples in the doctests). Let's try",
                "118": "            # making them all little endian",
                "122": "            # try to normalize out 32 and 64 bit default int sizes",
                "133": "# Subclass nose.plugins.doctests.DocTestCase to work around a bug in",
                "134": "# its constructor that blocks non-default arguments from being passed",
                "135": "# down into doctest.DocTestCase",
                "150": "    name = 'numpydoctest'   # call nosetests with --with-numpydoctest",
                "151": "    score = 1000  # load late, after doctest builtin",
                "153": "    # always use whitespace and ellipsis options for doctests",
                "156": "    # files that should be ignored for doctests",
                "160": "    # Custom classes; class variables to allow subclassing",
                "165": "    # Don't use the standard doctest option handler; hard-code the option values",
                "168": "        # Test doctests in 'test' files / directories. Standard plugin default",
                "169": "        # is False",
                "171": "        # Variable name; if defined, doctest results stored in this variable in",
                "172": "        # the top-level namespace.  None is the standard default",
                "176": "        # parent method sets enabled flag from command line --with-numpydoctest",
                "181": "            # Pull standard doctest out of plugin list; there's no reason to run",
                "182": "            # both.  In practice the Unplugger plugin above would cover us when",
                "183": "            # run from a standard numpy.test() call; this is just in case",
                "184": "            # someone wants to run our plugin outside the numpy.test() machinery",
                "206": "        # set the namespace for tests",
                "209": "        # Each doctest should execute in an environment equivalent to",
                "210": "        # starting Python and executing \"import numpy as np\", and,",
                "211": "        # for SciPy packages, an additional import of the local",
                "212": "        # package (so that scipy.linalg.basic.py's doctests have an",
                "213": "        # implicit \"from scipy import linalg\" as well).",
                "214": "        #",
                "215": "        # Note: __file__ allows the doctest in NoseTester to run",
                "216": "        # without producing an error",
                "221": "        # add appropriate scipy import for SciPy tests",
                "227": "    # Override test loading to customize test context (with set_test_context",
                "228": "    # method), set standard docstring options, and install our own test output",
                "229": "    # checker",
                "237": "            # nose allows module.__test__ = False; doctest does not and",
                "238": "            # throws AttributeError",
                "249": "            # Set test namespace; test altered in place",
                "256": "    # Add an afterContext method to nose.plugins.doctests.Doctest in order",
                "257": "    # to restore print options to the original state after each doctest",
                "261": "    # Ignore NumPy-specific build files that shouldn't be searched for tests",
                "275": "    enabled = True  # always enabled",
                "276": "    score = 4000  # load late in order to be after builtins",
                "285": "        # Pull named plugin out of plugins list",
                "316": "KnownFailure = KnownFailurePlugin   # backwards compat",
                "336": "                        \"FPU mode changed from {0:#x} to {1:#x} during the \"",
                "344": "# Class allows us to save the results of the tests in runTests - see runTests",
                "345": "# method docstring for details"
            },
            "comment_modified_diff": {}
        }
    ],
    "nosetester.py": [
        {
            "commit": "f75bb0edb0e6eec2564de4bf798242984860a19b",
            "timestamp": "2023-01-19T14:35:44-07:00",
            "author": "Charles Harris",
            "commit_message": "MAINT: Remove all nose testing support.\n\nNumPy switched to using pytest in 2018 and nose has been unmaintained\nfor many years. We have kept NumPy's nose support to avoid breaking\ndownstream projects who might have been using it and not yet switched to\npytest or some other testing framework. With the arrival of Python 3.12,\nunpatched nose will raise an error. It it time to move on.\n\nDecorators removed\n\n- raises\n- slow\n- setastest\n- skipif\n- knownfailif\n- deprecated\n- parametrize\n- _needs_refcount\n\nThese are not to be confused with pytest versions with similar names,\ne.g., pytest.mark.slow, pytest.mark.skipif, pytest.mark.parametrize.\n\nFunctions removed\n\n- Tester\n- import_nose\n- run_module_suite",
            "additions": 0,
            "deletions": 545,
            "change_type": "DELETE",
            "diff": "@@ -1,545 +0,0 @@\n-\"\"\"\n-Nose test running.\n-\n-This module implements ``test()`` and ``bench()`` functions for NumPy modules.\n-\n-\"\"\"\n-import os\n-import sys\n-import warnings\n-import numpy as np\n-\n-from .utils import import_nose, suppress_warnings\n-\n-\n-__all__ = ['get_package_name', 'run_module_suite', 'NoseTester',\n-           '_numpy_tester', 'get_package_name', 'import_nose',\n-           'suppress_warnings']\n-\n-\n-def get_package_name(filepath):\n-    \"\"\"\n-    Given a path where a package is installed, determine its name.\n-\n-    Parameters\n-    ----------\n-    filepath : str\n-        Path to a file. If the determination fails, \"numpy\" is returned.\n-\n-    Examples\n-    --------\n-    >>> np.testing.nosetester.get_package_name('nonsense')\n-    'numpy'\n-\n-    \"\"\"\n-\n-    fullpath = filepath[:]\n-    pkg_name = []\n-    while 'site-packages' in filepath or 'dist-packages' in filepath:\n-        filepath, p2 = os.path.split(filepath)\n-        if p2 in ('site-packages', 'dist-packages'):\n-            break\n-        pkg_name.append(p2)\n-\n-    # if package name determination failed, just default to numpy/scipy\n-    if not pkg_name:\n-        if 'scipy' in fullpath:\n-            return 'scipy'\n-        else:\n-            return 'numpy'\n-\n-    # otherwise, reverse to get correct order and return\n-    pkg_name.reverse()\n-\n-    # don't include the outer egg directory\n-    if pkg_name[0].endswith('.egg'):\n-        pkg_name.pop(0)\n-\n-    return '.'.join(pkg_name)\n-\n-\n-def run_module_suite(file_to_run=None, argv=None):\n-    \"\"\"\n-    Run a test module.\n-\n-    Equivalent to calling ``$ nosetests <argv> <file_to_run>`` from\n-    the command line\n-\n-    Parameters\n-    ----------\n-    file_to_run : str, optional\n-        Path to test module, or None.\n-        By default, run the module from which this function is called.\n-    argv : list of strings\n-        Arguments to be passed to the nose test runner. ``argv[0]`` is\n-        ignored. All command line arguments accepted by ``nosetests``\n-        will work. If it is the default value None, sys.argv is used.\n-\n-        .. versionadded:: 1.9.0\n-\n-    Examples\n-    --------\n-    Adding the following::\n-\n-        if __name__ == \"__main__\" :\n-            run_module_suite(argv=sys.argv)\n-\n-    at the end of a test module will run the tests when that module is\n-    called in the python interpreter.\n-\n-    Alternatively, calling::\n-\n-    >>> run_module_suite(file_to_run=\"numpy/tests/test_matlib.py\")  # doctest: +SKIP\n-\n-    from an interpreter will run all the test routine in 'test_matlib.py'.\n-    \"\"\"\n-    if file_to_run is None:\n-        f = sys._getframe(1)\n-        file_to_run = f.f_locals.get('__file__', None)\n-        if file_to_run is None:\n-            raise AssertionError\n-\n-    if argv is None:\n-        argv = sys.argv + [file_to_run]\n-    else:\n-        argv = argv + [file_to_run]\n-\n-    nose = import_nose()\n-    from .noseclasses import KnownFailurePlugin\n-    nose.run(argv=argv, addplugins=[KnownFailurePlugin()])\n-\n-\n-class NoseTester:\n-    \"\"\"\n-    Nose test runner.\n-\n-    This class is made available as numpy.testing.Tester, and a test function\n-    is typically added to a package's __init__.py like so::\n-\n-      from numpy.testing import Tester\n-      test = Tester().test\n-\n-    Calling this test function finds and runs all tests associated with the\n-    package and all its sub-packages.\n-\n-    Attributes\n-    ----------\n-    package_path : str\n-        Full path to the package to test.\n-    package_name : str\n-        Name of the package to test.\n-\n-    Parameters\n-    ----------\n-    package : module, str or None, optional\n-        The package to test. If a string, this should be the full path to\n-        the package. If None (default), `package` is set to the module from\n-        which `NoseTester` is initialized.\n-    raise_warnings : None, str or sequence of warnings, optional\n-        This specifies which warnings to configure as 'raise' instead\n-        of being shown once during the test execution.  Valid strings are:\n-\n-          - \"develop\" : equals ``(Warning,)``\n-          - \"release\" : equals ``()``, don't raise on any warnings.\n-\n-        Default is \"release\".\n-    depth : int, optional\n-        If `package` is None, then this can be used to initialize from the\n-        module of the caller of (the caller of (...)) the code that\n-        initializes `NoseTester`. Default of 0 means the module of the\n-        immediate caller; higher values are useful for utility routines that\n-        want to initialize `NoseTester` objects on behalf of other code.\n-\n-    \"\"\"\n-    def __init__(self, package=None, raise_warnings=\"release\", depth=0,\n-                 check_fpu_mode=False):\n-        # Back-compat: 'None' used to mean either \"release\" or \"develop\"\n-        # depending on whether this was a release or develop version of\n-        # numpy. Those semantics were fine for testing numpy, but not so\n-        # helpful for downstream projects like scipy that use\n-        # numpy.testing. (They want to set this based on whether *they* are a\n-        # release or develop version, not whether numpy is.) So we continue to\n-        # accept 'None' for back-compat, but it's now just an alias for the\n-        # default \"release\".\n-        if raise_warnings is None:\n-            raise_warnings = \"release\"\n-\n-        package_name = None\n-        if package is None:\n-            f = sys._getframe(1 + depth)\n-            package_path = f.f_locals.get('__file__', None)\n-            if package_path is None:\n-                raise AssertionError\n-            package_path = os.path.dirname(package_path)\n-            package_name = f.f_locals.get('__name__', None)\n-        elif isinstance(package, type(os)):\n-            package_path = os.path.dirname(package.__file__)\n-            package_name = getattr(package, '__name__', None)\n-        else:\n-            package_path = str(package)\n-\n-        self.package_path = package_path\n-\n-        # Find the package name under test; this name is used to limit coverage\n-        # reporting (if enabled).\n-        if package_name is None:\n-            package_name = get_package_name(package_path)\n-        self.package_name = package_name\n-\n-        # Set to \"release\" in constructor in maintenance branches.\n-        self.raise_warnings = raise_warnings\n-\n-        # Whether to check for FPU mode changes\n-        self.check_fpu_mode = check_fpu_mode\n-\n-    def _test_argv(self, label, verbose, extra_argv):\n-        ''' Generate argv for nosetest command\n-\n-        Parameters\n-        ----------\n-        label : {'fast', 'full', '', attribute identifier}, optional\n-            see ``test`` docstring\n-        verbose : int, optional\n-            Verbosity value for test outputs, in the range 1-10. Default is 1.\n-        extra_argv : list, optional\n-            List with any extra arguments to pass to nosetests.\n-\n-        Returns\n-        -------\n-        argv : list\n-            command line arguments that will be passed to nose\n-        '''\n-        argv = [__file__, self.package_path, '-s']\n-        if label and label != 'full':\n-            if not isinstance(label, str):\n-                raise TypeError('Selection label should be a string')\n-            if label == 'fast':\n-                label = 'not slow'\n-            argv += ['-A', label]\n-        argv += ['--verbosity', str(verbose)]\n-\n-        # When installing with setuptools, and also in some other cases, the\n-        # test_*.py files end up marked +x executable. Nose, by default, does\n-        # not run files marked with +x as they might be scripts. However, in\n-        # our case nose only looks for test_*.py files under the package\n-        # directory, which should be safe.\n-        argv += ['--exe']\n-\n-        if extra_argv:\n-            argv += extra_argv\n-        return argv\n-\n-    def _show_system_info(self):\n-        nose = import_nose()\n-\n-        import numpy\n-        print(f'NumPy version {numpy.__version__}')\n-        relaxed_strides = numpy.ones((10, 1), order=\"C\").flags.f_contiguous\n-        print(\"NumPy relaxed strides checking option:\", relaxed_strides)\n-        npdir = os.path.dirname(numpy.__file__)\n-        print(f'NumPy is installed in {npdir}')\n-\n-        if 'scipy' in self.package_name:\n-            import scipy\n-            print(f'SciPy version {scipy.__version__}')\n-            spdir = os.path.dirname(scipy.__file__)\n-            print(f'SciPy is installed in {spdir}')\n-\n-        pyversion = sys.version.replace('\\n', '')\n-        print(f'Python version {pyversion}')\n-        print(\"nose version %d.%d.%d\" % nose.__versioninfo__)\n-\n-    def _get_custom_doctester(self):\n-        \"\"\" Return instantiated plugin for doctests\n-\n-        Allows subclassing of this class to override doctester\n-\n-        A return value of None means use the nose builtin doctest plugin\n-        \"\"\"\n-        from .noseclasses import NumpyDoctest\n-        return NumpyDoctest()\n-\n-    def prepare_test_args(self, label='fast', verbose=1, extra_argv=None,\n-                          doctests=False, coverage=False, timer=False):\n-        \"\"\"\n-        Run tests for module using nose.\n-\n-        This method does the heavy lifting for the `test` method. It takes all\n-        the same arguments, for details see `test`.\n-\n-        See Also\n-        --------\n-        test\n-\n-        \"\"\"\n-        # fail with nice error message if nose is not present\n-        import_nose()\n-        # compile argv\n-        argv = self._test_argv(label, verbose, extra_argv)\n-        # our way of doing coverage\n-        if coverage:\n-            argv += [f'--cover-package={self.package_name}', '--with-coverage',\n-                   '--cover-tests', '--cover-erase']\n-\n-        if timer:\n-            if timer is True:\n-                argv += ['--with-timer']\n-            elif isinstance(timer, int):\n-                argv += ['--with-timer', '--timer-top-n', str(timer)]\n-\n-        # construct list of plugins\n-        import nose.plugins.builtin\n-        from nose.plugins import EntryPointPluginManager\n-        from .noseclasses import (KnownFailurePlugin, Unplugger,\n-                                  FPUModeCheckPlugin)\n-        plugins = [KnownFailurePlugin()]\n-        plugins += [p() for p in nose.plugins.builtin.plugins]\n-        if self.check_fpu_mode:\n-            plugins += [FPUModeCheckPlugin()]\n-            argv += [\"--with-fpumodecheckplugin\"]\n-        try:\n-            # External plugins (like nose-timer)\n-            entrypoint_manager = EntryPointPluginManager()\n-            entrypoint_manager.loadPlugins()\n-            plugins += [p for p in entrypoint_manager.plugins]\n-        except ImportError:\n-            # Relies on pkg_resources, not a hard dependency\n-            pass\n-\n-        # add doctesting if required\n-        doctest_argv = '--with-doctest' in argv\n-        if doctests == False and doctest_argv:\n-            doctests = True\n-        plug = self._get_custom_doctester()\n-        if plug is None:\n-            # use standard doctesting\n-            if doctests and not doctest_argv:\n-                argv += ['--with-doctest']\n-        else:  # custom doctesting\n-            if doctest_argv:  # in fact the unplugger would take care of this\n-                argv.remove('--with-doctest')\n-            plugins += [Unplugger('doctest'), plug]\n-            if doctests:\n-                argv += ['--with-' + plug.name]\n-        return argv, plugins\n-\n-    def test(self, label='fast', verbose=1, extra_argv=None,\n-             doctests=False, coverage=False, raise_warnings=None,\n-             timer=False):\n-        \"\"\"\n-        Run tests for module using nose.\n-\n-        Parameters\n-        ----------\n-        label : {'fast', 'full', '', attribute identifier}, optional\n-            Identifies the tests to run. This can be a string to pass to\n-            the nosetests executable with the '-A' option, or one of several\n-            special values.  Special values are:\n-\n-            * 'fast' - the default - which corresponds to the ``nosetests -A``\n-              option of 'not slow'.\n-            * 'full' - fast (as above) and slow tests as in the\n-              'no -A' option to nosetests - this is the same as ''.\n-            * None or '' - run all tests.\n-            * attribute_identifier - string passed directly to nosetests as '-A'.\n-\n-        verbose : int, optional\n-            Verbosity value for test outputs, in the range 1-10. Default is 1.\n-        extra_argv : list, optional\n-            List with any extra arguments to pass to nosetests.\n-        doctests : bool, optional\n-            If True, run doctests in module. Default is False.\n-        coverage : bool, optional\n-            If True, report coverage of NumPy code. Default is False.\n-            (This requires the\n-            `coverage module <https://pypi.org/project/coverage/>`_).\n-        raise_warnings : None, str or sequence of warnings, optional\n-            This specifies which warnings to configure as 'raise' instead\n-            of being shown once during the test execution. Valid strings are:\n-\n-            * \"develop\" : equals ``(Warning,)``\n-            * \"release\" : equals ``()``, do not raise on any warnings.\n-        timer : bool or int, optional\n-            Timing of individual tests with ``nose-timer`` (which needs to be\n-            installed).  If True, time tests and report on all of them.\n-            If an integer (say ``N``), report timing results for ``N`` slowest\n-            tests.\n-\n-        Returns\n-        -------\n-        result : object\n-            Returns the result of running the tests as a\n-            ``nose.result.TextTestResult`` object.\n-\n-        Notes\n-        -----\n-        Each NumPy module exposes `test` in its namespace to run all tests for it.\n-        For example, to run all tests for numpy.lib:\n-\n-        >>> np.lib.test() #doctest: +SKIP\n-\n-        Examples\n-        --------\n-        >>> result = np.lib.test() #doctest: +SKIP\n-        Running unit tests for numpy.lib\n-        ...\n-        Ran 976 tests in 3.933s\n-\n-        OK\n-\n-        >>> result.errors #doctest: +SKIP\n-        []\n-        >>> result.knownfail #doctest: +SKIP\n-        []\n-        \"\"\"\n-\n-        # cap verbosity at 3 because nose becomes *very* verbose beyond that\n-        verbose = min(verbose, 3)\n-\n-        from . import utils\n-        utils.verbose = verbose\n-\n-        argv, plugins = self.prepare_test_args(\n-                label, verbose, extra_argv, doctests, coverage, timer)\n-\n-        if doctests:\n-            print(f'Running unit tests and doctests for {self.package_name}')\n-        else:\n-            print(f'Running unit tests for {self.package_name}')\n-\n-        self._show_system_info()\n-\n-        # reset doctest state on every run\n-        import doctest\n-        doctest.master = None\n-\n-        if raise_warnings is None:\n-            raise_warnings = self.raise_warnings\n-\n-        _warn_opts = dict(develop=(Warning,),\n-                          release=())\n-        if isinstance(raise_warnings, str):\n-            raise_warnings = _warn_opts[raise_warnings]\n-\n-        with suppress_warnings(\"location\") as sup:\n-            # Reset the warning filters to the default state,\n-            # so that running the tests is more repeatable.\n-            warnings.resetwarnings()\n-            # Set all warnings to 'warn', this is because the default 'once'\n-            # has the bad property of possibly shadowing later warnings.\n-            warnings.filterwarnings('always')\n-            # Force the requested warnings to raise\n-            for warningtype in raise_warnings:\n-                warnings.filterwarnings('error', category=warningtype)\n-            # Filter out annoying import messages.\n-            sup.filter(message='Not importing directory')\n-            sup.filter(message=\"numpy.dtype size changed\")\n-            sup.filter(message=\"numpy.ufunc size changed\")\n-            sup.filter(category=np.ModuleDeprecationWarning)\n-            # Filter out boolean '-' deprecation messages. This allows\n-            # older versions of scipy to test without a flood of messages.\n-            sup.filter(message=\".*boolean negative.*\")\n-            sup.filter(message=\".*boolean subtract.*\")\n-            # Filter out distutils cpu warnings (could be localized to\n-            # distutils tests). ASV has problems with top level import,\n-            # so fetch module for suppression here.\n-            with warnings.catch_warnings():\n-                warnings.simplefilter(\"always\")\n-                from ...distutils import cpuinfo\n-            sup.filter(category=UserWarning, module=cpuinfo)\n-            # Filter out some deprecation warnings inside nose 1.3.7 when run\n-            # on python 3.5b2. See\n-            #     https://github.com/nose-devs/nose/issues/929\n-            # Note: it is hard to filter based on module for sup (lineno could\n-            #       be implemented).\n-            warnings.filterwarnings(\"ignore\", message=\".*getargspec.*\",\n-                                    category=DeprecationWarning,\n-                                    module=r\"nose\\.\")\n-\n-            from .noseclasses import NumpyTestProgram\n-\n-            t = NumpyTestProgram(argv=argv, exit=False, plugins=plugins)\n-\n-        return t.result\n-\n-    def bench(self, label='fast', verbose=1, extra_argv=None):\n-        \"\"\"\n-        Run benchmarks for module using nose.\n-\n-        Parameters\n-        ----------\n-        label : {'fast', 'full', '', attribute identifier}, optional\n-            Identifies the benchmarks to run. This can be a string to pass to\n-            the nosetests executable with the '-A' option, or one of several\n-            special values.  Special values are:\n-\n-            * 'fast' - the default - which corresponds to the ``nosetests -A``\n-              option of 'not slow'.\n-            * 'full' - fast (as above) and slow benchmarks as in the\n-              'no -A' option to nosetests - this is the same as ''.\n-            * None or '' - run all tests.\n-            * attribute_identifier - string passed directly to nosetests as '-A'.\n-\n-        verbose : int, optional\n-            Verbosity value for benchmark outputs, in the range 1-10. Default is 1.\n-        extra_argv : list, optional\n-            List with any extra arguments to pass to nosetests.\n-\n-        Returns\n-        -------\n-        success : bool\n-            Returns True if running the benchmarks works, False if an error\n-            occurred.\n-\n-        Notes\n-        -----\n-        Benchmarks are like tests, but have names starting with \"bench\" instead\n-        of \"test\", and can be found under the \"benchmarks\" sub-directory of the\n-        module.\n-\n-        Each NumPy module exposes `bench` in its namespace to run all benchmarks\n-        for it.\n-\n-        Examples\n-        --------\n-        >>> success = np.lib.bench() #doctest: +SKIP\n-        Running benchmarks for numpy.lib\n-        ...\n-        using 562341 items:\n-        unique:\n-        0.11\n-        unique1d:\n-        0.11\n-        ratio: 1.0\n-        nUnique: 56230 == 56230\n-        ...\n-        OK\n-\n-        >>> success #doctest: +SKIP\n-        True\n-\n-        \"\"\"\n-\n-        print(f'Running benchmarks for {self.package_name}')\n-        self._show_system_info()\n-\n-        argv = self._test_argv(label, verbose, extra_argv)\n-        argv += ['--match', r'(?:^|[\\\\b_\\\\.%s-])[Bb]ench' % os.sep]\n-\n-        # import nose or make informative error\n-        nose = import_nose()\n-\n-        # get plugin to disable doctests\n-        from .noseclasses import Unplugger\n-        add_plugins = [Unplugger('doctest')]\n-\n-        return nose.run(argv=argv, addplugins=add_plugins)\n-\n-\n-def _numpy_tester():\n-    if hasattr(np, \"__version__\") and \".dev0\" in np.__version__:\n-        mode = \"develop\"\n-    else:\n-        mode = \"release\"\n-    return NoseTester(raise_warnings=mode, depth=1,\n-                      check_fpu_mode=True)\n",
            "comment_added_diff": {},
            "comment_deleted_diff": {
                "44": "    # if package name determination failed, just default to numpy/scipy",
                "51": "    # otherwise, reverse to get correct order and return",
                "54": "    # don't include the outer egg directory",
                "92": "    >>> run_module_suite(file_to_run=\"numpy/tests/test_matlib.py\")  # doctest: +SKIP",
                "156": "        # Back-compat: 'None' used to mean either \"release\" or \"develop\"",
                "157": "        # depending on whether this was a release or develop version of",
                "158": "        # numpy. Those semantics were fine for testing numpy, but not so",
                "159": "        # helpful for downstream projects like scipy that use",
                "160": "        # numpy.testing. (They want to set this based on whether *they* are a",
                "161": "        # release or develop version, not whether numpy is.) So we continue to",
                "162": "        # accept 'None' for back-compat, but it's now just an alias for the",
                "163": "        # default \"release\".",
                "183": "        # Find the package name under test; this name is used to limit coverage",
                "184": "        # reporting (if enabled).",
                "189": "        # Set to \"release\" in constructor in maintenance branches.",
                "192": "        # Whether to check for FPU mode changes",
                "221": "        # When installing with setuptools, and also in some other cases, the",
                "222": "        # test_*.py files end up marked +x executable. Nose, by default, does",
                "223": "        # not run files marked with +x as they might be scripts. However, in",
                "224": "        # our case nose only looks for test_*.py files under the package",
                "225": "        # directory, which should be safe.",
                "275": "        # fail with nice error message if nose is not present",
                "277": "        # compile argv",
                "279": "        # our way of doing coverage",
                "290": "        # construct list of plugins",
                "301": "            # External plugins (like nose-timer)",
                "306": "            # Relies on pkg_resources, not a hard dependency",
                "309": "        # add doctesting if required",
                "315": "            # use standard doctesting",
                "318": "        else:  # custom doctesting",
                "319": "            if doctest_argv:  # in fact the unplugger would take care of this",
                "379": "        >>> np.lib.test() #doctest: +SKIP",
                "383": "        >>> result = np.lib.test() #doctest: +SKIP",
                "390": "        >>> result.errors #doctest: +SKIP",
                "392": "        >>> result.knownfail #doctest: +SKIP",
                "396": "        # cap verbosity at 3 because nose becomes *very* verbose beyond that",
                "412": "        # reset doctest state on every run",
                "425": "            # Reset the warning filters to the default state,",
                "426": "            # so that running the tests is more repeatable.",
                "428": "            # Set all warnings to 'warn', this is because the default 'once'",
                "429": "            # has the bad property of possibly shadowing later warnings.",
                "431": "            # Force the requested warnings to raise",
                "434": "            # Filter out annoying import messages.",
                "439": "            # Filter out boolean '-' deprecation messages. This allows",
                "440": "            # older versions of scipy to test without a flood of messages.",
                "443": "            # Filter out distutils cpu warnings (could be localized to",
                "444": "            # distutils tests). ASV has problems with top level import,",
                "445": "            # so fetch module for suppression here.",
                "450": "            # Filter out some deprecation warnings inside nose 1.3.7 when run",
                "451": "            # on python 3.5b2. See",
                "452": "            #     https://github.com/nose-devs/nose/issues/929",
                "453": "            # Note: it is hard to filter based on module for sup (lineno could",
                "454": "            #       be implemented).",
                "505": "        >>> success = np.lib.bench() #doctest: +SKIP",
                "518": "        >>> success #doctest: +SKIP",
                "529": "        # import nose or make informative error",
                "532": "        # get plugin to disable doctests"
            },
            "comment_modified_diff": {}
        }
    ],
    "parameterized.py": [
        {
            "commit": "f75bb0edb0e6eec2564de4bf798242984860a19b",
            "timestamp": "2023-01-19T14:35:44-07:00",
            "author": "Charles Harris",
            "commit_message": "MAINT: Remove all nose testing support.\n\nNumPy switched to using pytest in 2018 and nose has been unmaintained\nfor many years. We have kept NumPy's nose support to avoid breaking\ndownstream projects who might have been using it and not yet switched to\npytest or some other testing framework. With the arrival of Python 3.12,\nunpatched nose will raise an error. It it time to move on.\n\nDecorators removed\n\n- raises\n- slow\n- setastest\n- skipif\n- knownfailif\n- deprecated\n- parametrize\n- _needs_refcount\n\nThese are not to be confused with pytest versions with similar names,\ne.g., pytest.mark.slow, pytest.mark.skipif, pytest.mark.parametrize.\n\nFunctions removed\n\n- Tester\n- import_nose\n- run_module_suite",
            "additions": 0,
            "deletions": 432,
            "change_type": "DELETE",
            "diff": "@@ -1,432 +0,0 @@\n-\"\"\"\n-tl;dr: all code is licensed under simplified BSD, unless stated otherwise.\n-\n-Unless stated otherwise in the source files, all code is copyright 2010 David\n-Wolever <david@wolever.net>. All rights reserved.\n-\n-Redistribution and use in source and binary forms, with or without\n-modification, are permitted provided that the following conditions are met:\n-\n-   1. Redistributions of source code must retain the above copyright notice,\n-   this list of conditions and the following disclaimer.\n-\n-   2. Redistributions in binary form must reproduce the above copyright notice,\n-   this list of conditions and the following disclaimer in the documentation\n-   and/or other materials provided with the distribution.\n-\n-THIS SOFTWARE IS PROVIDED BY <COPYRIGHT HOLDER> ``AS IS'' AND ANY EXPRESS OR\n-IMPLIED WARRANTIES, INCLUDING, BUT NOT LIMITED TO, THE IMPLIED WARRANTIES OF\n-MERCHANTABILITY AND FITNESS FOR A PARTICULAR PURPOSE ARE DISCLAIMED. IN NO\n-EVENT SHALL <COPYRIGHT HOLDER> OR CONTRIBUTORS BE LIABLE FOR ANY DIRECT,\n-INDIRECT, INCIDENTAL, SPECIAL, EXEMPLARY, OR CONSEQUENTIAL DAMAGES (INCLUDING,\n-BUT NOT LIMITED TO, PROCUREMENT OF SUBSTITUTE GOODS OR SERVICES; LOSS OF USE,\n-DATA, OR PROFITS; OR BUSINESS INTERRUPTION) HOWEVER CAUSED AND ON ANY THEORY OF\n-LIABILITY, WHETHER IN CONTRACT, STRICT LIABILITY, OR TORT (INCLUDING NEGLIGENCE\n-OR OTHERWISE) ARISING IN ANY WAY OUT OF THE USE OF THIS SOFTWARE, EVEN IF\n-ADVISED OF THE POSSIBILITY OF SUCH DAMAGE.\n-\n-The views and conclusions contained in the software and documentation are those\n-of the authors and should not be interpreted as representing official policies,\n-either expressed or implied, of David Wolever.\n-\n-\"\"\"\n-import re\n-import inspect\n-import warnings\n-from functools import wraps\n-from types import MethodType\n-from collections import namedtuple\n-\n-from unittest import TestCase\n-\n-_param = namedtuple(\"param\", \"args kwargs\")\n-\n-class param(_param):\n-    \"\"\" Represents a single parameter to a test case.\n-\n-        For example::\n-\n-            >>> p = param(\"foo\", bar=16)\n-            >>> p\n-            param(\"foo\", bar=16)\n-            >>> p.args\n-            ('foo', )\n-            >>> p.kwargs\n-            {'bar': 16}\n-\n-        Intended to be used as an argument to ``@parameterized``::\n-\n-            @parameterized([\n-                param(\"foo\", bar=16),\n-            ])\n-            def test_stuff(foo, bar=16):\n-                pass\n-        \"\"\"\n-\n-    def __new__(cls, *args , **kwargs):\n-        return _param.__new__(cls, args, kwargs)\n-\n-    @classmethod\n-    def explicit(cls, args=None, kwargs=None):\n-        \"\"\" Creates a ``param`` by explicitly specifying ``args`` and\n-            ``kwargs``::\n-\n-                >>> param.explicit([1,2,3])\n-                param(*(1, 2, 3))\n-                >>> param.explicit(kwargs={\"foo\": 42})\n-                param(*(), **{\"foo\": \"42\"})\n-            \"\"\"\n-        args = args or ()\n-        kwargs = kwargs or {}\n-        return cls(*args, **kwargs)\n-\n-    @classmethod\n-    def from_decorator(cls, args):\n-        \"\"\" Returns an instance of ``param()`` for ``@parameterized`` argument\n-            ``args``::\n-\n-                >>> param.from_decorator((42, ))\n-                param(args=(42, ), kwargs={})\n-                >>> param.from_decorator(\"foo\")\n-                param(args=(\"foo\", ), kwargs={})\n-            \"\"\"\n-        if isinstance(args, param):\n-            return args\n-        elif isinstance(args, (str,)):\n-            args = (args, )\n-        try:\n-            return cls(*args)\n-        except TypeError as e:\n-            if \"after * must be\" not in str(e):\n-                raise\n-            raise TypeError(\n-                \"Parameters must be tuples, but %r is not (hint: use '(%r, )')\"\n-                %(args, args),\n-            )\n-\n-    def __repr__(self):\n-        return \"param(*%r, **%r)\" %self\n-\n-\n-def parameterized_argument_value_pairs(func, p):\n-    \"\"\"Return tuples of parameterized arguments and their values.\n-\n-        This is useful if you are writing your own doc_func\n-        function and need to know the values for each parameter name::\n-\n-            >>> def func(a, foo=None, bar=42, **kwargs): pass\n-            >>> p = param(1, foo=7, extra=99)\n-            >>> parameterized_argument_value_pairs(func, p)\n-            [(\"a\", 1), (\"foo\", 7), (\"bar\", 42), (\"**kwargs\", {\"extra\": 99})]\n-\n-        If the function's first argument is named ``self`` then it will be\n-        ignored::\n-\n-            >>> def func(self, a): pass\n-            >>> p = param(1)\n-            >>> parameterized_argument_value_pairs(func, p)\n-            [(\"a\", 1)]\n-\n-        Additionally, empty ``*args`` or ``**kwargs`` will be ignored::\n-\n-            >>> def func(foo, *args): pass\n-            >>> p = param(1)\n-            >>> parameterized_argument_value_pairs(func, p)\n-            [(\"foo\", 1)]\n-            >>> p = param(1, 16)\n-            >>> parameterized_argument_value_pairs(func, p)\n-            [(\"foo\", 1), (\"*args\", (16, ))]\n-    \"\"\"\n-    argspec = inspect.getargspec(func)\n-    arg_offset = 1 if argspec.args[:1] == [\"self\"] else 0\n-\n-    named_args = argspec.args[arg_offset:]\n-\n-    result = list(zip(named_args, p.args))\n-    named_args = argspec.args[len(result) + arg_offset:]\n-    varargs = p.args[len(result):]\n-\n-    result.extend([\n-        (name, p.kwargs.get(name, default))\n-        for (name, default)\n-        in zip(named_args, argspec.defaults or [])\n-    ])\n-\n-    seen_arg_names = {n for (n, _) in result}\n-    keywords = dict(sorted([\n-        (name, p.kwargs[name])\n-        for name in p.kwargs\n-        if name not in seen_arg_names\n-    ]))\n-\n-    if varargs:\n-        result.append((\"*%s\" %(argspec.varargs, ), tuple(varargs)))\n-\n-    if keywords:\n-        result.append((\"**%s\" %(argspec.keywords, ), keywords))\n-\n-    return result\n-\n-def short_repr(x, n=64):\n-    \"\"\" A shortened repr of ``x`` which is guaranteed to be ``unicode``::\n-\n-            >>> short_repr(\"foo\")\n-            u\"foo\"\n-            >>> short_repr(\"123456789\", n=4)\n-            u\"12...89\"\n-    \"\"\"\n-\n-    x_repr = repr(x)\n-    if isinstance(x_repr, bytes):\n-        try:\n-            x_repr = str(x_repr, \"utf-8\")\n-        except UnicodeDecodeError:\n-            x_repr = str(x_repr, \"latin1\")\n-    if len(x_repr) > n:\n-        x_repr = x_repr[:n//2] + \"...\" + x_repr[len(x_repr) - n//2:]\n-    return x_repr\n-\n-def default_doc_func(func, num, p):\n-    if func.__doc__ is None:\n-        return None\n-\n-    all_args_with_values = parameterized_argument_value_pairs(func, p)\n-\n-    # Assumes that the function passed is a bound method.\n-    descs = [f'{n}={short_repr(v)}' for n, v in all_args_with_values]\n-\n-    # The documentation might be a multiline string, so split it\n-    # and just work with the first string, ignoring the period\n-    # at the end if there is one.\n-    first, nl, rest = func.__doc__.lstrip().partition(\"\\n\")\n-    suffix = \"\"\n-    if first.endswith(\".\"):\n-        suffix = \".\"\n-        first = first[:-1]\n-    args = \"%s[with %s]\" %(len(first) and \" \" or \"\", \", \".join(descs))\n-    return \"\".join([first.rstrip(), args, suffix, nl, rest])\n-\n-def default_name_func(func, num, p):\n-    base_name = func.__name__\n-    name_suffix = \"_%s\" %(num, )\n-    if len(p.args) > 0 and isinstance(p.args[0], (str,)):\n-        name_suffix += \"_\" + parameterized.to_safe_name(p.args[0])\n-    return base_name + name_suffix\n-\n-\n-# force nose for numpy purposes.\n-_test_runner_override = 'nose'\n-_test_runner_guess = False\n-_test_runners = set([\"unittest\", \"unittest2\", \"nose\", \"nose2\", \"pytest\"])\n-_test_runner_aliases = {\n-    \"_pytest\": \"pytest\",\n-}\n-\n-def set_test_runner(name):\n-    global _test_runner_override\n-    if name not in _test_runners:\n-        raise TypeError(\n-            \"Invalid test runner: %r (must be one of: %s)\"\n-            %(name, \", \".join(_test_runners)),\n-        )\n-    _test_runner_override = name\n-\n-def detect_runner():\n-    \"\"\" Guess which test runner we're using by traversing the stack and looking\n-        for the first matching module. This *should* be reasonably safe, as\n-        it's done during test discovery where the test runner should be the\n-        stack frame immediately outside. \"\"\"\n-    if _test_runner_override is not None:\n-        return _test_runner_override\n-    global _test_runner_guess\n-    if _test_runner_guess is False:\n-        stack = inspect.stack()\n-        for record in reversed(stack):\n-            frame = record[0]\n-            module = frame.f_globals.get(\"__name__\").partition(\".\")[0]\n-            if module in _test_runner_aliases:\n-                module = _test_runner_aliases[module]\n-            if module in _test_runners:\n-                _test_runner_guess = module\n-                break\n-        else:\n-            _test_runner_guess = None\n-    return _test_runner_guess\n-\n-class parameterized:\n-    \"\"\" Parameterize a test case::\n-\n-            class TestInt:\n-                @parameterized([\n-                    (\"A\", 10),\n-                    (\"F\", 15),\n-                    param(\"10\", 42, base=42)\n-                ])\n-                def test_int(self, input, expected, base=16):\n-                    actual = int(input, base=base)\n-                    assert_equal(actual, expected)\n-\n-            @parameterized([\n-                (2, 3, 5)\n-                (3, 5, 8),\n-            ])\n-            def test_add(a, b, expected):\n-                assert_equal(a + b, expected)\n-        \"\"\"\n-\n-    def __init__(self, input, doc_func=None):\n-        self.get_input = self.input_as_callable(input)\n-        self.doc_func = doc_func or default_doc_func\n-\n-    def __call__(self, test_func):\n-        self.assert_not_in_testcase_subclass()\n-\n-        @wraps(test_func)\n-        def wrapper(test_self=None):\n-            test_cls = test_self and type(test_self)\n-\n-            original_doc = wrapper.__doc__\n-            for num, args in enumerate(wrapper.parameterized_input):\n-                p = param.from_decorator(args)\n-                unbound_func, nose_tuple = self.param_as_nose_tuple(test_self, test_func, num, p)\n-                try:\n-                    wrapper.__doc__ = nose_tuple[0].__doc__\n-                    # Nose uses `getattr(instance, test_func.__name__)` to get\n-                    # a method bound to the test instance (as opposed to a\n-                    # method bound to the instance of the class created when\n-                    # tests were being enumerated). Set a value here to make\n-                    # sure nose can get the correct test method.\n-                    if test_self is not None:\n-                        setattr(test_cls, test_func.__name__, unbound_func)\n-                    yield nose_tuple\n-                finally:\n-                    if test_self is not None:\n-                        delattr(test_cls, test_func.__name__)\n-                    wrapper.__doc__ = original_doc\n-        wrapper.parameterized_input = self.get_input()\n-        wrapper.parameterized_func = test_func\n-        test_func.__name__ = \"_parameterized_original_%s\" %(test_func.__name__, )\n-        return wrapper\n-\n-    def param_as_nose_tuple(self, test_self, func, num, p):\n-        nose_func = wraps(func)(lambda *args: func(*args[:-1], **args[-1]))\n-        nose_func.__doc__ = self.doc_func(func, num, p)\n-        # Track the unbound function because we need to setattr the unbound\n-        # function onto the class for nose to work (see comments above), and\n-        # Python 3 doesn't let us pull the function out of a bound method.\n-        unbound_func = nose_func\n-        if test_self is not None:\n-            nose_func = MethodType(nose_func, test_self)\n-        return unbound_func, (nose_func, ) + p.args + (p.kwargs or {}, )\n-\n-    def assert_not_in_testcase_subclass(self):\n-        parent_classes = self._terrible_magic_get_defining_classes()\n-        if any(issubclass(cls, TestCase) for cls in parent_classes):\n-            raise Exception(\"Warning: '@parameterized' tests won't work \"\n-                            \"inside subclasses of 'TestCase' - use \"\n-                            \"'@parameterized.expand' instead.\")\n-\n-    def _terrible_magic_get_defining_classes(self):\n-        \"\"\" Returns the list of parent classes of the class currently being defined.\n-            Will likely only work if called from the ``parameterized`` decorator.\n-            This function is entirely @brandon_rhodes's fault, as he suggested\n-            the implementation: http://stackoverflow.com/a/8793684/71522\n-            \"\"\"\n-        stack = inspect.stack()\n-        if len(stack) <= 4:\n-            return []\n-        frame = stack[4]\n-        code_context = frame[4] and frame[4][0].strip()\n-        if not (code_context and code_context.startswith(\"class \")):\n-            return []\n-        _, _, parents = code_context.partition(\"(\")\n-        parents, _, _ = parents.partition(\")\")\n-        return eval(\"[\" + parents + \"]\", frame[0].f_globals, frame[0].f_locals)\n-\n-    @classmethod\n-    def input_as_callable(cls, input):\n-        if callable(input):\n-            return lambda: cls.check_input_values(input())\n-        input_values = cls.check_input_values(input)\n-        return lambda: input_values\n-\n-    @classmethod\n-    def check_input_values(cls, input_values):\n-        # Explicitly convert non-list inputs to a list so that:\n-        # 1. A helpful exception will be raised if they aren't iterable, and\n-        # 2. Generators are unwrapped exactly once (otherwise `nosetests\n-        #    --processes=n` has issues; see:\n-        #    https://github.com/wolever/nose-parameterized/pull/31)\n-        if not isinstance(input_values, list):\n-            input_values = list(input_values)\n-        return [ param.from_decorator(p) for p in input_values ]\n-\n-    @classmethod\n-    def expand(cls, input, name_func=None, doc_func=None, **legacy):\n-        \"\"\" A \"brute force\" method of parameterizing test cases. Creates new\n-            test cases and injects them into the namespace that the wrapped\n-            function is being defined in. Useful for parameterizing tests in\n-            subclasses of 'UnitTest', where Nose test generators don't work.\n-\n-            >>> @parameterized.expand([(\"foo\", 1, 2)])\n-            ... def test_add1(name, input, expected):\n-            ...     actual = add1(input)\n-            ...     assert_equal(actual, expected)\n-            ...\n-            >>> locals()\n-            ... 'test_add1_foo_0': <function ...> ...\n-            >>>\n-            \"\"\"\n-\n-        if \"testcase_func_name\" in legacy:\n-            warnings.warn(\"testcase_func_name= is deprecated; use name_func=\",\n-                          DeprecationWarning, stacklevel=2)\n-            if not name_func:\n-                name_func = legacy[\"testcase_func_name\"]\n-\n-        if \"testcase_func_doc\" in legacy:\n-            warnings.warn(\"testcase_func_doc= is deprecated; use doc_func=\",\n-                          DeprecationWarning, stacklevel=2)\n-            if not doc_func:\n-                doc_func = legacy[\"testcase_func_doc\"]\n-\n-        doc_func = doc_func or default_doc_func\n-        name_func = name_func or default_name_func\n-\n-        def parameterized_expand_wrapper(f, instance=None):\n-            stack = inspect.stack()\n-            frame = stack[1]\n-            frame_locals = frame[0].f_locals\n-\n-            parameters = cls.input_as_callable(input)()\n-            for num, p in enumerate(parameters):\n-                name = name_func(f, num, p)\n-                frame_locals[name] = cls.param_as_standalone_func(p, f, name)\n-                frame_locals[name].__doc__ = doc_func(f, num, p)\n-\n-            f.__test__ = False\n-        return parameterized_expand_wrapper\n-\n-    @classmethod\n-    def param_as_standalone_func(cls, p, func, name):\n-        @wraps(func)\n-        def standalone_func(*a):\n-            return func(*(a + p.args), **p.kwargs)\n-        standalone_func.__name__ = name\n-\n-        # place_as is used by py.test to determine what source file should be\n-        # used for this test.\n-        standalone_func.place_as = func\n-\n-        # Remove __wrapped__ because py.test will try to look at __wrapped__\n-        # to determine which parameters should be used with this test case,\n-        # and obviously we don't need it to do any parameterization.\n-        try:\n-            del standalone_func.__wrapped__\n-        except AttributeError:\n-            pass\n-        return standalone_func\n-\n-    @classmethod\n-    def to_safe_name(cls, s):\n-        return str(re.sub(\"[^a-zA-Z0-9_]+\", \"_\", s))\n",
            "comment_added_diff": {},
            "comment_deleted_diff": {
                "195": "    # Assumes that the function passed is a bound method.",
                "198": "    # The documentation might be a multiline string, so split it",
                "199": "    # and just work with the first string, ignoring the period",
                "200": "    # at the end if there is one.",
                "217": "# force nose for numpy purposes.",
                "294": "                    # Nose uses `getattr(instance, test_func.__name__)` to get",
                "295": "                    # a method bound to the test instance (as opposed to a",
                "296": "                    # method bound to the instance of the class created when",
                "297": "                    # tests were being enumerated). Set a value here to make",
                "298": "                    # sure nose can get the correct test method.",
                "314": "        # Track the unbound function because we need to setattr the unbound",
                "315": "        # function onto the class for nose to work (see comments above), and",
                "316": "        # Python 3 doesn't let us pull the function out of a bound method.",
                "355": "        # Explicitly convert non-list inputs to a list so that:",
                "356": "        # 1. A helpful exception will be raised if they aren't iterable, and",
                "357": "        # 2. Generators are unwrapped exactly once (otherwise `nosetests",
                "358": "        #    --processes=n` has issues; see:",
                "359": "        #    https://github.com/wolever/nose-parameterized/pull/31)",
                "417": "        # place_as is used by py.test to determine what source file should be",
                "418": "        # used for this test.",
                "421": "        # Remove __wrapped__ because py.test will try to look at __wrapped__",
                "422": "        # to determine which parameters should be used with this test case,",
                "423": "        # and obviously we don't need it to do any parameterization."
            },
            "comment_modified_diff": {}
        }
    ],
    "test_doctesting.py": [
        {
            "commit": "f75bb0edb0e6eec2564de4bf798242984860a19b",
            "timestamp": "2023-01-19T14:35:44-07:00",
            "author": "Charles Harris",
            "commit_message": "MAINT: Remove all nose testing support.\n\nNumPy switched to using pytest in 2018 and nose has been unmaintained\nfor many years. We have kept NumPy's nose support to avoid breaking\ndownstream projects who might have been using it and not yet switched to\npytest or some other testing framework. With the arrival of Python 3.12,\nunpatched nose will raise an error. It it time to move on.\n\nDecorators removed\n\n- raises\n- slow\n- setastest\n- skipif\n- knownfailif\n- deprecated\n- parametrize\n- _needs_refcount\n\nThese are not to be confused with pytest versions with similar names,\ne.g., pytest.mark.slow, pytest.mark.skipif, pytest.mark.parametrize.\n\nFunctions removed\n\n- Tester\n- import_nose\n- run_module_suite",
            "additions": 0,
            "deletions": 57,
            "change_type": "DELETE",
            "diff": "@@ -1,57 +0,0 @@\n-\"\"\" Doctests for NumPy-specific nose/doctest modifications\n-\n-\"\"\"\n-#FIXME: None of these tests is run, because 'check' is not a recognized\n-# testing prefix.\n-\n-# try the #random directive on the output line\n-def check_random_directive():\n-    '''\n-    >>> 2+2\n-    <BadExample object at 0x084D05AC>  #random: may vary on your system\n-    '''\n-\n-# check the implicit \"import numpy as np\"\n-def check_implicit_np():\n-    '''\n-    >>> np.array([1,2,3])\n-    array([1, 2, 3])\n-    '''\n-\n-# there's some extraneous whitespace around the correct responses\n-def check_whitespace_enabled():\n-    '''\n-    # whitespace after the 3\n-    >>> 1+2\n-    3\n-\n-    # whitespace before the 7\n-    >>> 3+4\n-     7\n-    '''\n-\n-def check_empty_output():\n-    \"\"\" Check that no output does not cause an error.\n-\n-    This is related to nose bug 445; the numpy plugin changed the\n-    doctest-result-variable default and therefore hit this bug:\n-    http://code.google.com/p/python-nose/issues/detail?id=445\n-\n-    >>> a = 10\n-    \"\"\"\n-\n-def check_skip():\n-    \"\"\" Check skip directive\n-\n-    The test below should not run\n-\n-    >>> 1/0 #doctest: +SKIP\n-    \"\"\"\n-\n-\n-if __name__ == '__main__':\n-    # Run tests outside numpy test rig\n-    import nose\n-    from numpy.testing.noseclasses import NumpyDoctest\n-    argv = ['', __file__, '--with-numpydoctest']\n-    nose.core.TestProgram(argv=argv, addplugins=[NumpyDoctest()])\n",
            "comment_added_diff": {},
            "comment_deleted_diff": {
                "4": "#FIXME: None of these tests is run, because 'check' is not a recognized",
                "5": "# testing prefix.",
                "7": "# try the #random directive on the output line",
                "11": "    <BadExample object at 0x084D05AC>  #random: may vary on your system",
                "14": "# check the implicit \"import numpy as np\"",
                "21": "# there's some extraneous whitespace around the correct responses",
                "24": "    # whitespace after the 3",
                "28": "    # whitespace before the 7",
                "48": "    >>> 1/0 #doctest: +SKIP",
                "53": "    # Run tests outside numpy test rig"
            },
            "comment_modified_diff": {}
        }
    ],
    "testing.pyi": [],
    "array_method.h": [],
    "reduction.c": [],
    "reduction.h": [],
    "legacy_array_method.c": [],
    "wrapping_array_method.c": [],
    "_scaled_float_dtype.c": [],
    "linux_musl.yml": [],
    "23060.expired.rst": [],
    "routines.testing.rst": [],
    "config.yml": [],
    "push_docs_to_repo.py": [],
    "compiled_base.h": [],
    "ufunc_object.h": [],
    "22769.improvement.rst": [],
    "test_numpy_config.py": [],
    ".gitattributes": [],
    "dlpack.h": [],
    "mixins.py": [],
    "23105.compatibility.rst": [],
    "ufunc_docstrings.py": [
        {
            "commit": "6a7a231e083cf0cb54986317d67e1b624e82001f",
            "timestamp": "2023-09-27T23:31:27+00:00",
            "author": "ganesh-k13",
            "commit_message": "DOC: Improved `bitwise_count` documentation\n\n* Added return type for integer inputs\n* Refined release note\n* Refined external references for popcount",
            "additions": 4,
            "deletions": 1,
            "change_type": "MODIFY",
            "diff": "@@ -4230,15 +4230,18 @@ def add_newdoc(place, name, doc):\n     -------\n     y : ndarray\n         The corresponding number of 1-bits in the input.\n+        Returns uint8 for all integer types\n         $OUT_SCALAR_1\n \n     References\n     ----------\n-    .. [1] https://stackoverflow.com/a/109025/5671364\n+    .. [1] https://graphics.stanford.edu/~seander/bithacks.html#CountBitsSetParallel\n \n     .. [2] Wikipedia, \"Hamming weight\",\n            https://en.wikipedia.org/wiki/Hamming_weight\n \n+    .. [3] http://aggregate.ee.engr.uky.edu/MAGIC/#Population%20Count%20(Ones%20Count)\n+\n     Examples\n     --------\n     >>> np.bitwise_count(1023)\n",
            "comment_added_diff": {
                "4238": "    .. [1] https://graphics.stanford.edu/~seander/bithacks.html#CountBitsSetParallel",
                "4243": "    .. [3] http://aggregate.ee.engr.uky.edu/MAGIC/#Population%20Count%20(Ones%20Count)"
            },
            "comment_deleted_diff": {},
            "comment_modified_diff": {}
        }
    ],
    "_simd_easyintrin.inc": [],
    "reorder.h": [],
    "bench_ufunc_strides.py": [],
    "loops_unary_complex.dispatch.c.src": [],
    "rebase_installed_dlls_cygwin.sh": [],
    "cygwin.yml": [],
    "argfunc.dispatch.c.src": [],
    "quicksort.cpp": [],
    "x86-qsort-skx.dispatch.cpp": [],
    "x86-qsort-skx.h": [],
    "x86-qsort.dispatch.cpp": [],
    "x86-qsort-icl.dispatch.cpp": [],
    "x86-qsort-icl.h": [],
    "bench_function_base.py": [
        {
            "commit": "9d08632f7f1e641ffde2dc21d245ca9d6bf8d3e2",
            "timestamp": "2023-05-16T11:06:17-06:00",
            "author": "Nathan Goldbaum",
            "commit_message": "BENCH: add benchmark for where slow path",
            "additions": 7,
            "deletions": 0,
            "change_type": "MODIFY",
            "diff": "@@ -308,7 +308,9 @@ def time_sort_worst(self):\n class Where(Benchmark):\n     def setup(self):\n         self.d = np.arange(20000)\n+        self.d_o = self.d.astype(object)\n         self.e = self.d.copy()\n+        self.e_o = self.d_o.copy()\n         self.cond = (self.d > 5000)\n         size = 1024 * 1024 // 8\n         rnd_array = np.random.rand(size)\n@@ -332,6 +334,11 @@ def time_1(self):\n     def time_2(self):\n         np.where(self.cond, self.d, self.e)\n \n+    def time_2_object(self):\n+        # object and byteswapped arrays have a\n+        # special slow path in the where internals\n+        np.where(self.cond, self.d_o, self.e_o)\n+\n     def time_2_broadcast(self):\n         np.where(self.cond, self.d, 0)\n \n",
            "comment_added_diff": {
                "338": "        # object and byteswapped arrays have a",
                "339": "        # special slow path in the where internals"
            },
            "comment_deleted_diff": {},
            "comment_modified_diff": {}
        },
        {
            "commit": "489acbb6c82a08d8ea689978a273659447ef6b71",
            "timestamp": "2023-09-18T10:35:05-07:00",
            "author": "Raghuveer Devulapalli",
            "commit_message": "Add benchmarks for np.partition",
            "additions": 31,
            "deletions": 3,
            "change_type": "MODIFY",
            "diff": "@@ -132,7 +132,7 @@ def memoize(f):\n     def wrapped(*args):\n         if args not in _memoized:\n             _memoized[args] = f(*args)\n-        \n+\n         return _memoized[args].copy()\n \n     return f\n@@ -154,7 +154,7 @@ def random(size, dtype):\n         arr = np.arange(size, dtype=dtype)\n         np.random.shuffle(arr)\n         return arr\n-    \n+\n     @staticmethod\n     @memoize\n     def ordered(size, dtype):\n@@ -237,7 +237,6 @@ def random_bubble(cls, size, dtype, bubble_num, bubble_size=None):\n \n         return cls.random_unsorted_area(size, dtype, frac, bubble_size)\n \n-\n class Sort(Benchmark):\n     \"\"\"\n     This benchmark tests sorting performance with several\n@@ -288,6 +287,35 @@ def time_argsort(self, kind, dtype, array_type):\n         np.argsort(self.arr, kind=kind)\n \n \n+class Partition(Benchmark):\n+    params = [\n+        ['float64', 'int64', 'float32', 'int32', 'int16', 'float16'],\n+        [\n+            ('random',),\n+            ('ordered',),\n+            ('reversed',),\n+            ('uniform',),\n+            ('sorted_block', 10),\n+            ('sorted_block', 100),\n+            ('sorted_block', 1000),\n+        ],\n+        [10, 100, 1000],\n+    ]\n+    param_names = ['dtype', 'array_type', 'k']\n+\n+    # The size of the benchmarked arrays.\n+    ARRAY_SIZE = 100000\n+\n+    def setup(self, dtype, array_type, k):\n+        np.random.seed(1234)\n+        array_class = array_type[0]\n+        self.arr = getattr(SortGenerator, array_class)(self.ARRAY_SIZE,\n+                dtype, *array_type[1:])\n+\n+    def time_partition(self, dtype, array_type, k):\n+        np.partition(self.arr, k)\n+\n+\n class SortWorst(Benchmark):\n     def setup(self):\n         # quicksort median of 3 worst case\n",
            "comment_added_diff": {
                "306": "    # The size of the benchmarked arrays."
            },
            "comment_deleted_diff": {},
            "comment_modified_diff": {}
        }
    ],
    "test_print.py": [
        {
            "commit": "79484648894c974f68b5bf1624ce1548b13eaabb",
            "timestamp": "2023-06-18T18:41:41+02:00",
            "author": "Ralf Gommers",
            "commit_message": "TST: disable longdouble string/print tests on Linux aarch64\n\nSee gh-23974",
            "additions": 14,
            "deletions": 6,
            "change_type": "MODIFY",
            "diff": "@@ -1,4 +1,5 @@\n import sys\n+import platform\n \n import pytest\n \n@@ -12,7 +13,14 @@\n _REF = {np.inf: 'inf', -np.inf: '-inf', np.nan: 'nan'}\n \n \n-@pytest.mark.parametrize('tp', [np.float32, np.double, np.longdouble])\n+# longdouble printing issue on aarch64, see gh-23974\n+if platform.machine() == 'aarch64':\n+    _real_dtypes = [np.float32, np.double]\n+    _complex_dtypes = [np.complex64, np.cdouble]\n+else:\n+    _real_dtypes = [np.float32, np.double, np.longdouble]\n+    _complex_dtypes = [np.complex64, np.cdouble, np.clongdouble]\n+@pytest.mark.parametrize('tp', _real_dtypes)\n def test_float_types(tp):\n     \"\"\" Check formatting.\n \n@@ -34,7 +42,7 @@ def test_float_types(tp):\n                      err_msg='Failed str formatting for type %s' % tp)\n \n \n-@pytest.mark.parametrize('tp', [np.float32, np.double, np.longdouble])\n+@pytest.mark.parametrize('tp', _real_dtypes)\n def test_nan_inf_float(tp):\n     \"\"\" Check formatting of nan & inf.\n \n@@ -48,7 +56,7 @@ def test_nan_inf_float(tp):\n                      err_msg='Failed str formatting for type %s' % tp)\n \n \n-@pytest.mark.parametrize('tp', [np.complex64, np.cdouble, np.clongdouble])\n+@pytest.mark.parametrize('tp', _complex_dtypes)\n def test_complex_types(tp):\n     \"\"\"Check formatting of complex types.\n \n@@ -74,7 +82,7 @@ def test_complex_types(tp):\n                      err_msg='Failed str formatting for type %s' % tp)\n \n \n-@pytest.mark.parametrize('dtype', [np.complex64, np.cdouble, np.clongdouble])\n+@pytest.mark.parametrize('dtype', _complex_dtypes)\n def test_complex_inf_nan(dtype):\n     \"\"\"Check inf/nan formatting of complex types.\"\"\"\n     TESTS = {\n@@ -119,7 +127,7 @@ def _test_redirected_print(x, tp, ref=None):\n                  err_msg='print failed for type%s' % tp)\n \n \n-@pytest.mark.parametrize('tp', [np.float32, np.double, np.longdouble])\n+@pytest.mark.parametrize('tp', _real_dtypes)\n def test_float_type_print(tp):\n     \"\"\"Check formatting when using print \"\"\"\n     for x in [0, 1, -1, 1e20]:\n@@ -135,7 +143,7 @@ def test_float_type_print(tp):\n         _test_redirected_print(float(1e16), tp, ref)\n \n \n-@pytest.mark.parametrize('tp', [np.complex64, np.cdouble, np.clongdouble])\n+@pytest.mark.parametrize('tp', _complex_dtypes)\n def test_complex_type_print(tp):\n     \"\"\"Check formatting when using print \"\"\"\n     # We do not create complex with inf/nan directly because the feature is\n",
            "comment_added_diff": {
                "16": "# longdouble printing issue on aarch64, see gh-23974"
            },
            "comment_deleted_diff": {},
            "comment_modified_diff": {}
        },
        {
            "commit": "ff9ecc0a69c8e2e654354344e9ccae6e56f0805d",
            "timestamp": "2023-06-19T08:45:49+02:00",
            "author": "Ralf Gommers",
            "commit_message": "Revert \"TST: disable longdouble string/print tests on Linux aarch64\"\n\nThis reverts commit 79484648894c974f68b5bf1624ce1548b13eaabb.\n\n[skip actions] [skip circle] [skip azp]",
            "additions": 6,
            "deletions": 14,
            "change_type": "MODIFY",
            "diff": "@@ -1,5 +1,4 @@\n import sys\n-import platform\n \n import pytest\n \n@@ -13,14 +12,7 @@\n _REF = {np.inf: 'inf', -np.inf: '-inf', np.nan: 'nan'}\n \n \n-# longdouble printing issue on aarch64, see gh-23974\n-if platform.machine() == 'aarch64':\n-    _real_dtypes = [np.float32, np.double]\n-    _complex_dtypes = [np.complex64, np.cdouble]\n-else:\n-    _real_dtypes = [np.float32, np.double, np.longdouble]\n-    _complex_dtypes = [np.complex64, np.cdouble, np.clongdouble]\n-@pytest.mark.parametrize('tp', _real_dtypes)\n+@pytest.mark.parametrize('tp', [np.float32, np.double, np.longdouble])\n def test_float_types(tp):\n     \"\"\" Check formatting.\n \n@@ -42,7 +34,7 @@ def test_float_types(tp):\n                      err_msg='Failed str formatting for type %s' % tp)\n \n \n-@pytest.mark.parametrize('tp', _real_dtypes)\n+@pytest.mark.parametrize('tp', [np.float32, np.double, np.longdouble])\n def test_nan_inf_float(tp):\n     \"\"\" Check formatting of nan & inf.\n \n@@ -56,7 +48,7 @@ def test_nan_inf_float(tp):\n                      err_msg='Failed str formatting for type %s' % tp)\n \n \n-@pytest.mark.parametrize('tp', _complex_dtypes)\n+@pytest.mark.parametrize('tp', [np.complex64, np.cdouble, np.clongdouble])\n def test_complex_types(tp):\n     \"\"\"Check formatting of complex types.\n \n@@ -82,7 +74,7 @@ def test_complex_types(tp):\n                      err_msg='Failed str formatting for type %s' % tp)\n \n \n-@pytest.mark.parametrize('dtype', _complex_dtypes)\n+@pytest.mark.parametrize('dtype', [np.complex64, np.cdouble, np.clongdouble])\n def test_complex_inf_nan(dtype):\n     \"\"\"Check inf/nan formatting of complex types.\"\"\"\n     TESTS = {\n@@ -127,7 +119,7 @@ def _test_redirected_print(x, tp, ref=None):\n                  err_msg='print failed for type%s' % tp)\n \n \n-@pytest.mark.parametrize('tp', _real_dtypes)\n+@pytest.mark.parametrize('tp', [np.float32, np.double, np.longdouble])\n def test_float_type_print(tp):\n     \"\"\"Check formatting when using print \"\"\"\n     for x in [0, 1, -1, 1e20]:\n@@ -143,7 +135,7 @@ def test_float_type_print(tp):\n         _test_redirected_print(float(1e16), tp, ref)\n \n \n-@pytest.mark.parametrize('tp', _complex_dtypes)\n+@pytest.mark.parametrize('tp', [np.complex64, np.cdouble, np.clongdouble])\n def test_complex_type_print(tp):\n     \"\"\"Check formatting when using print \"\"\"\n     # We do not create complex with inf/nan directly because the feature is\n",
            "comment_added_diff": {},
            "comment_deleted_diff": {
                "16": "# longdouble printing issue on aarch64, see gh-23974"
            },
            "comment_modified_diff": {}
        }
    ],
    "test_scalar_methods.py": [],
    "test_scalarprint.py": [
        {
            "commit": "20d397400d6325cff3decbba3d6195418e873237",
            "timestamp": "2023-02-01T18:43:05+11:00",
            "author": "Andrew Nelson",
            "commit_message": "WHL: musllinux wheels [wheel build]",
            "additions": 3,
            "deletions": 3,
            "change_type": "MODIFY",
            "diff": "@@ -8,7 +8,7 @@\n \n from tempfile import TemporaryFile\n import numpy as np\n-from numpy.testing import assert_, assert_equal, assert_raises\n+from numpy.testing import assert_, assert_equal, assert_raises, IS_MUSL\n \n class TestRealScalars:\n     def test_str(self):\n@@ -260,10 +260,10 @@ def test_dragon4(self):\n         assert_equal(fpos64('324', unique=False, precision=5,\n                                    fractional=False), \"324.00\")\n \n-\n     def test_dragon4_interface(self):\n         tps = [np.float16, np.float32, np.float64]\n-        if hasattr(np, 'float128'):\n+        # test is flaky for musllinux on np.float128\n+        if hasattr(np, 'float128') and not IS_MUSL:\n             tps.append(np.float128)\n \n         fpos = np.format_float_positional\n",
            "comment_added_diff": {
                "265": "        # test is flaky for musllinux on np.float128"
            },
            "comment_deleted_diff": {},
            "comment_modified_diff": {}
        },
        {
            "commit": "79484648894c974f68b5bf1624ce1548b13eaabb",
            "timestamp": "2023-06-18T18:41:41+02:00",
            "author": "Ralf Gommers",
            "commit_message": "TST: disable longdouble string/print tests on Linux aarch64\n\nSee gh-23974",
            "additions": 8,
            "deletions": 2,
            "change_type": "MODIFY",
            "diff": "@@ -13,7 +13,11 @@\n class TestRealScalars:\n     def test_str(self):\n         svals = [0.0, -0.0, 1, -1, np.inf, -np.inf, np.nan]\n-        styps = [np.float16, np.float32, np.float64, np.longdouble]\n+        # longdouble printing issue on aarch64, see gh-23974\n+        if platform.machine() == 'aarch64':\n+            styps = [np.float16, np.float32, np.float64]\n+        else:\n+            styps = [np.float16, np.float32, np.float64, np.longdouble]\n         wanted = [\n              ['0.0',  '0.0',  '0.0',  '0.0' ],\n              ['-0.0', '-0.0', '-0.0', '-0.0'],\n@@ -263,7 +267,9 @@ def test_dragon4(self):\n     def test_dragon4_interface(self):\n         tps = [np.float16, np.float32, np.float64]\n         # test is flaky for musllinux on np.float128\n-        if hasattr(np, 'float128') and not IS_MUSL:\n+        # also currently failing on Linux aarch64 with Meson (see gh-23974)\n+        is_aarch64 = platform.machine() == 'aarch64'\n+        if hasattr(np, 'float128') and not IS_MUSL and not is_aarch64:\n             tps.append(np.float128)\n \n         fpos = np.format_float_positional\n",
            "comment_added_diff": {
                "16": "        # longdouble printing issue on aarch64, see gh-23974",
                "270": "        # also currently failing on Linux aarch64 with Meson (see gh-23974)"
            },
            "comment_deleted_diff": {},
            "comment_modified_diff": {
                "16": "        styps = [np.float16, np.float32, np.float64, np.longdouble]"
            }
        },
        {
            "commit": "ff9ecc0a69c8e2e654354344e9ccae6e56f0805d",
            "timestamp": "2023-06-19T08:45:49+02:00",
            "author": "Ralf Gommers",
            "commit_message": "Revert \"TST: disable longdouble string/print tests on Linux aarch64\"\n\nThis reverts commit 79484648894c974f68b5bf1624ce1548b13eaabb.\n\n[skip actions] [skip circle] [skip azp]",
            "additions": 2,
            "deletions": 8,
            "change_type": "MODIFY",
            "diff": "@@ -13,11 +13,7 @@\n class TestRealScalars:\n     def test_str(self):\n         svals = [0.0, -0.0, 1, -1, np.inf, -np.inf, np.nan]\n-        # longdouble printing issue on aarch64, see gh-23974\n-        if platform.machine() == 'aarch64':\n-            styps = [np.float16, np.float32, np.float64]\n-        else:\n-            styps = [np.float16, np.float32, np.float64, np.longdouble]\n+        styps = [np.float16, np.float32, np.float64, np.longdouble]\n         wanted = [\n              ['0.0',  '0.0',  '0.0',  '0.0' ],\n              ['-0.0', '-0.0', '-0.0', '-0.0'],\n@@ -267,9 +263,7 @@ def test_dragon4(self):\n     def test_dragon4_interface(self):\n         tps = [np.float16, np.float32, np.float64]\n         # test is flaky for musllinux on np.float128\n-        # also currently failing on Linux aarch64 with Meson (see gh-23974)\n-        is_aarch64 = platform.machine() == 'aarch64'\n-        if hasattr(np, 'float128') and not IS_MUSL and not is_aarch64:\n+        if hasattr(np, 'float128') and not IS_MUSL:\n             tps.append(np.float128)\n \n         fpos = np.format_float_positional\n",
            "comment_added_diff": {},
            "comment_deleted_diff": {
                "16": "        # longdouble printing issue on aarch64, see gh-23974",
                "270": "        # also currently failing on Linux aarch64 with Meson (see gh-23974)"
            },
            "comment_modified_diff": {}
        }
    ],
    "_array_like.py": [
        {
            "commit": "c4c523296cdffb4e5f8c4cf6314d0bb177735e47",
            "timestamp": "2023-02-02T18:27:49+01:00",
            "author": "BvB93",
            "commit_message": "MAINT: Make use of the py39 `__class_getitem__` availability in the standard library",
            "additions": 28,
            "deletions": 25,
            "change_type": "MODIFY",
            "diff": "@@ -1,9 +1,7 @@\n from __future__ import annotations\n \n-# NOTE: Import `Sequence` from `typing` as we it is needed for a type-alias,\n-# not an annotation\n-from collections.abc import Collection, Callable\n-from typing import Any, Sequence, Protocol, Union, TypeVar, runtime_checkable\n+from collections.abc import Collection, Callable, Sequence\n+from typing import Any, Protocol, Union, TypeVar, runtime_checkable\n from numpy import (\n     ndarray,\n     dtype,\n@@ -25,8 +23,8 @@\n \n _T = TypeVar(\"_T\")\n _ScalarType = TypeVar(\"_ScalarType\", bound=generic)\n-_DType = TypeVar(\"_DType\", bound=\"dtype[Any]\")\n-_DType_co = TypeVar(\"_DType_co\", covariant=True, bound=\"dtype[Any]\")\n+_DType = TypeVar(\"_DType\", bound=dtype[Any])\n+_DType_co = TypeVar(\"_DType_co\", covariant=True, bound=dtype[Any])\n \n # The `_SupportsArray` protocol only cares about the default dtype\n # (i.e. `dtype=None` or no `dtype` parameter at all) of the to-be returned\n@@ -61,8 +59,8 @@ def __array_function__(\n \n # A subset of `npt.ArrayLike` that can be parametrized w.r.t. `np.generic`\n _ArrayLike = Union[\n-    _SupportsArray[\"dtype[_ScalarType]\"],\n-    _NestedSequence[_SupportsArray[\"dtype[_ScalarType]\"]],\n+    _SupportsArray[dtype[_ScalarType]],\n+    _NestedSequence[_SupportsArray[dtype[_ScalarType]]],\n ]\n \n # A union representing array-like objects; consists of two typevars:\n@@ -90,57 +88,62 @@ def __array_function__(\n # `ArrayLike<X>_co`: array-like objects that can be coerced into `X`\n # given the casting rules `same_kind`\n _ArrayLikeBool_co = _DualArrayLike[\n-    \"dtype[bool_]\",\n+    dtype[bool_],\n     bool,\n ]\n _ArrayLikeUInt_co = _DualArrayLike[\n-    \"dtype[Union[bool_, unsignedinteger[Any]]]\",\n+    dtype[Union[bool_, unsignedinteger[Any]]],\n     bool,\n ]\n _ArrayLikeInt_co = _DualArrayLike[\n-    \"dtype[Union[bool_, integer[Any]]]\",\n+    dtype[Union[bool_, integer[Any]]],\n     Union[bool, int],\n ]\n _ArrayLikeFloat_co = _DualArrayLike[\n-    \"dtype[Union[bool_, integer[Any], floating[Any]]]\",\n+    dtype[Union[bool_, integer[Any], floating[Any]]],\n     Union[bool, int, float],\n ]\n _ArrayLikeComplex_co = _DualArrayLike[\n-    \"dtype[Union[bool_, integer[Any], floating[Any], complexfloating[Any, Any]]]\",\n+    dtype[Union[\n+        bool_,\n+        integer[Any],\n+        floating[Any],\n+        complexfloating[Any, Any],\n+    ]],\n     Union[bool, int, float, complex],\n ]\n _ArrayLikeNumber_co = _DualArrayLike[\n-    \"dtype[Union[bool_, number[Any]]]\",\n+    dtype[Union[bool_, number[Any]]],\n     Union[bool, int, float, complex],\n ]\n _ArrayLikeTD64_co = _DualArrayLike[\n-    \"dtype[Union[bool_, integer[Any], timedelta64]]\",\n+    dtype[Union[bool_, integer[Any], timedelta64]],\n     Union[bool, int],\n ]\n _ArrayLikeDT64_co = Union[\n-    _SupportsArray[\"dtype[datetime64]\"],\n-    _NestedSequence[_SupportsArray[\"dtype[datetime64]\"]],\n+    _SupportsArray[dtype[datetime64]],\n+    _NestedSequence[_SupportsArray[dtype[datetime64]]],\n ]\n _ArrayLikeObject_co = Union[\n-    _SupportsArray[\"dtype[object_]\"],\n-    _NestedSequence[_SupportsArray[\"dtype[object_]\"]],\n+    _SupportsArray[dtype[object_]],\n+    _NestedSequence[_SupportsArray[dtype[object_]]],\n ]\n \n _ArrayLikeVoid_co = Union[\n-    _SupportsArray[\"dtype[void]\"],\n-    _NestedSequence[_SupportsArray[\"dtype[void]\"]],\n+    _SupportsArray[dtype[void]],\n+    _NestedSequence[_SupportsArray[dtype[void]]],\n ]\n _ArrayLikeStr_co = _DualArrayLike[\n-    \"dtype[str_]\",\n+    dtype[str_],\n     str,\n ]\n _ArrayLikeBytes_co = _DualArrayLike[\n-    \"dtype[bytes_]\",\n+    dtype[bytes_],\n     bytes,\n ]\n \n _ArrayLikeInt = _DualArrayLike[\n-    \"dtype[integer[Any]]\",\n+    dtype[integer[Any]],\n     int,\n ]\n \n@@ -153,6 +156,6 @@ class _UnknownType:\n \n \n _ArrayLikeUnknown = _DualArrayLike[\n-    \"dtype[_UnknownType]\",\n+    dtype[_UnknownType],\n     _UnknownType,\n ]\n",
            "comment_added_diff": {},
            "comment_deleted_diff": {
                "3": "# NOTE: Import `Sequence` from `typing` as we it is needed for a type-alias,",
                "4": "# not an annotation"
            },
            "comment_modified_diff": {}
        },
        {
            "commit": "9d5ac5c07e8000f32ae0b89eb385ff5b8919306f",
            "timestamp": "2023-09-14T15:52:20+02:00",
            "author": "Bas van Beek",
            "commit_message": "TYP: Use `collections.abc.Buffer` in the `npt.ArrayLike` definition",
            "additions": 14,
            "deletions": 11,
            "change_type": "MODIFY",
            "diff": "@@ -1,7 +1,9 @@\n from __future__ import annotations\n \n+import sys\n from collections.abc import Collection, Callable, Sequence\n from typing import Any, Protocol, Union, TypeVar, runtime_checkable\n+\n from numpy import (\n     ndarray,\n     dtype,\n@@ -76,17 +78,18 @@ def __array_function__(\n     _NestedSequence[_T],\n ]\n \n-# TODO: support buffer protocols once\n-#\n-# https://bugs.python.org/issue27501\n-#\n-# is resolved. See also the mypy issue:\n-#\n-# https://github.com/python/typing/issues/593\n-ArrayLike = _DualArrayLike[\n-    dtype[Any],\n-    Union[bool, int, float, complex, str, bytes],\n-]\n+if sys.version_info >= (3, 12):\n+    from collections.abc import Buffer\n+\n+    ArrayLike = Buffer | _DualArrayLike[\n+        dtype[Any],\n+        Union[bool, int, float, complex, str, bytes],\n+    ]\n+else:\n+    ArrayLike = _DualArrayLike[\n+        dtype[Any],\n+        Union[bool, int, float, complex, str, bytes],\n+    ]\n \n # `ArrayLike<X>_co`: array-like objects that can be coerced into `X`\n # given the casting rules `same_kind`\n",
            "comment_added_diff": {},
            "comment_deleted_diff": {
                "79": "# TODO: support buffer protocols once",
                "80": "#",
                "81": "# https://bugs.python.org/issue27501",
                "82": "#",
                "83": "# is resolved. See also the mypy issue:",
                "84": "#",
                "85": "# https://github.com/python/typing/issues/593"
            },
            "comment_modified_diff": {}
        }
    ],
    "_extended_precision.py": [],
    "descriptor.c": [],
    "methods.c": [],
    "test_arraymethod.py": [],
    "refguide_check.py": [
        {
            "commit": "a4c582d4b3c530176be393ac596dd00130367ddb",
            "timestamp": "2023-07-18T08:53:25+02:00",
            "author": "Dimitri Papadopoulos",
            "commit_message": "MAINT: Fix new or residual typos found by codespell",
            "additions": 1,
            "deletions": 1,
            "change_type": "MODIFY",
            "diff": "@@ -715,7 +715,7 @@ def check_output(self, want, got, optionflags):\n             # Maybe we're printing a numpy array? This produces invalid python\n             # code: `print(np.arange(3))` produces \"[0 1 2]\" w/o commas between\n             # values. So, reinsert commas and retry.\n-            # TODO: handle (1) abberivation (`print(np.arange(10000))`), and\n+            # TODO: handle (1) abbreviation (`print(np.arange(10000))`), and\n             #              (2) n-dim arrays with n > 1\n             s_want = want.strip()\n             s_got = got.strip()\n",
            "comment_added_diff": {
                "718": "            # TODO: handle (1) abbreviation (`print(np.arange(10000))`), and"
            },
            "comment_deleted_diff": {
                "718": "            # TODO: handle (1) abberivation (`print(np.arange(10000))`), and"
            },
            "comment_modified_diff": {
                "718": "            # TODO: handle (1) abberivation (`print(np.arange(10000))`), and"
            }
        }
    ],
    "_dtype_like.py": [
        {
            "commit": "c4c523296cdffb4e5f8c4cf6314d0bb177735e47",
            "timestamp": "2023-02-02T18:27:49+01:00",
            "author": "BvB93",
            "commit_message": "MAINT: Make use of the py39 `__class_getitem__` availability in the standard library",
            "additions": 50,
            "deletions": 53,
            "change_type": "MODIFY",
            "diff": "@@ -1,10 +1,8 @@\n+from collections.abc import Sequence\n from typing import (\n     Any,\n-    List,\n     Sequence,\n-    Tuple,\n     Union,\n-    Type,\n     TypeVar,\n     Protocol,\n     TypedDict,\n@@ -14,7 +12,6 @@\n import numpy as np\n \n from ._shape import _ShapeLike\n-from ._generic_alias import _DType as DType\n \n from ._char_codes import (\n     _BoolCodes,\n@@ -59,7 +56,7 @@\n )\n \n _SCT = TypeVar(\"_SCT\", bound=np.generic)\n-_DType_co = TypeVar(\"_DType_co\", covariant=True, bound=DType[Any])\n+_DType_co = TypeVar(\"_DType_co\", covariant=True, bound=np.dtype[Any])\n \n _DTypeLikeNested = Any  # TODO: wait for support for recursive types\n \n@@ -89,41 +86,41 @@ def dtype(self) -> _DType_co: ...\n \n # A subset of `npt.DTypeLike` that can be parametrized w.r.t. `np.generic`\n _DTypeLike = Union[\n-    \"np.dtype[_SCT]\",\n-    Type[_SCT],\n-    _SupportsDType[\"np.dtype[_SCT]\"],\n+    np.dtype[_SCT],\n+    type[_SCT],\n+    _SupportsDType[np.dtype[_SCT]],\n ]\n \n \n # Would create a dtype[np.void]\n _VoidDTypeLike = Union[\n     # (flexible_dtype, itemsize)\n-    Tuple[_DTypeLikeNested, int],\n+    tuple[_DTypeLikeNested, int],\n     # (fixed_dtype, shape)\n-    Tuple[_DTypeLikeNested, _ShapeLike],\n+    tuple[_DTypeLikeNested, _ShapeLike],\n     # [(field_name, field_dtype, field_shape), ...]\n     #\n     # The type here is quite broad because NumPy accepts quite a wide\n     # range of inputs inside the list; see the tests for some\n     # examples.\n-    List[Any],\n+    list[Any],\n     # {'names': ..., 'formats': ..., 'offsets': ..., 'titles': ...,\n     #  'itemsize': ...}\n     _DTypeDict,\n     # (base_dtype, new_dtype)\n-    Tuple[_DTypeLikeNested, _DTypeLikeNested],\n+    tuple[_DTypeLikeNested, _DTypeLikeNested],\n ]\n \n # Anything that can be coerced into numpy.dtype.\n # Reference: https://docs.scipy.org/doc/numpy/reference/arrays.dtypes.html\n DTypeLike = Union[\n-    DType[Any],\n+    np.dtype[Any],\n     # default data type (float64)\n     None,\n     # array-scalar types and generic types\n-    Type[Any],  # NOTE: We're stuck with `Type[Any]` due to object dtypes\n+    type[Any],  # NOTE: We're stuck with `type[Any]` due to object dtypes\n     # anything with a dtype attribute\n-    _SupportsDType[DType[Any]],\n+    _SupportsDType[np.dtype[Any]],\n     # character codes, type strings or comma-separated fields, e.g., 'float64'\n     str,\n     _VoidDTypeLike,\n@@ -139,16 +136,16 @@ def dtype(self) -> _DType_co: ...\n # Aliases for commonly used dtype-like objects.\n # Note that the precision of `np.number` subclasses is ignored herein.\n _DTypeLikeBool = Union[\n-    Type[bool],\n-    Type[np.bool_],\n-    DType[np.bool_],\n-    _SupportsDType[DType[np.bool_]],\n+    type[bool],\n+    type[np.bool_],\n+    np.dtype[np.bool_],\n+    _SupportsDType[np.dtype[np.bool_]],\n     _BoolCodes,\n ]\n _DTypeLikeUInt = Union[\n-    Type[np.unsignedinteger],\n-    DType[np.unsignedinteger],\n-    _SupportsDType[DType[np.unsignedinteger]],\n+    type[np.unsignedinteger],\n+    np.dtype[np.unsignedinteger],\n+    _SupportsDType[np.dtype[np.unsignedinteger]],\n     _UInt8Codes,\n     _UInt16Codes,\n     _UInt32Codes,\n@@ -161,10 +158,10 @@ def dtype(self) -> _DType_co: ...\n     _ULongLongCodes,\n ]\n _DTypeLikeInt = Union[\n-    Type[int],\n-    Type[np.signedinteger],\n-    DType[np.signedinteger],\n-    _SupportsDType[DType[np.signedinteger]],\n+    type[int],\n+    type[np.signedinteger],\n+    np.dtype[np.signedinteger],\n+    _SupportsDType[np.dtype[np.signedinteger]],\n     _Int8Codes,\n     _Int16Codes,\n     _Int32Codes,\n@@ -177,10 +174,10 @@ def dtype(self) -> _DType_co: ...\n     _LongLongCodes,\n ]\n _DTypeLikeFloat = Union[\n-    Type[float],\n-    Type[np.floating],\n-    DType[np.floating],\n-    _SupportsDType[DType[np.floating]],\n+    type[float],\n+    type[np.floating],\n+    np.dtype[np.floating],\n+    _SupportsDType[np.dtype[np.floating]],\n     _Float16Codes,\n     _Float32Codes,\n     _Float64Codes,\n@@ -190,10 +187,10 @@ def dtype(self) -> _DType_co: ...\n     _LongDoubleCodes,\n ]\n _DTypeLikeComplex = Union[\n-    Type[complex],\n-    Type[np.complexfloating],\n-    DType[np.complexfloating],\n-    _SupportsDType[DType[np.complexfloating]],\n+    type[complex],\n+    type[np.complexfloating],\n+    np.dtype[np.complexfloating],\n+    _SupportsDType[np.dtype[np.complexfloating]],\n     _Complex64Codes,\n     _Complex128Codes,\n     _CSingleCodes,\n@@ -201,42 +198,42 @@ def dtype(self) -> _DType_co: ...\n     _CLongDoubleCodes,\n ]\n _DTypeLikeDT64 = Union[\n-    Type[np.timedelta64],\n-    DType[np.timedelta64],\n-    _SupportsDType[DType[np.timedelta64]],\n+    type[np.timedelta64],\n+    np.dtype[np.timedelta64],\n+    _SupportsDType[np.dtype[np.timedelta64]],\n     _TD64Codes,\n ]\n _DTypeLikeTD64 = Union[\n-    Type[np.datetime64],\n-    DType[np.datetime64],\n-    _SupportsDType[DType[np.datetime64]],\n+    type[np.datetime64],\n+    np.dtype[np.datetime64],\n+    _SupportsDType[np.dtype[np.datetime64]],\n     _DT64Codes,\n ]\n _DTypeLikeStr = Union[\n-    Type[str],\n-    Type[np.str_],\n-    DType[np.str_],\n-    _SupportsDType[DType[np.str_]],\n+    type[str],\n+    type[np.str_],\n+    np.dtype[np.str_],\n+    _SupportsDType[np.dtype[np.str_]],\n     _StrCodes,\n ]\n _DTypeLikeBytes = Union[\n-    Type[bytes],\n-    Type[np.bytes_],\n-    DType[np.bytes_],\n-    _SupportsDType[DType[np.bytes_]],\n+    type[bytes],\n+    type[np.bytes_],\n+    np.dtype[np.bytes_],\n+    _SupportsDType[np.dtype[np.bytes_]],\n     _BytesCodes,\n ]\n _DTypeLikeVoid = Union[\n-    Type[np.void],\n-    DType[np.void],\n-    _SupportsDType[DType[np.void]],\n+    type[np.void],\n+    np.dtype[np.void],\n+    _SupportsDType[np.dtype[np.void]],\n     _VoidCodes,\n     _VoidDTypeLike,\n ]\n _DTypeLikeObject = Union[\n     type,\n-    DType[np.object_],\n-    _SupportsDType[DType[np.object_]],\n+    np.dtype[np.object_],\n+    _SupportsDType[np.dtype[np.object_]],\n     _ObjectCodes,\n ]\n \n",
            "comment_added_diff": {
                "121": "    type[Any],  # NOTE: We're stuck with `type[Any]` due to object dtypes"
            },
            "comment_deleted_diff": {
                "124": "    Type[Any],  # NOTE: We're stuck with `Type[Any]` due to object dtypes"
            },
            "comment_modified_diff": {}
        }
    ],
    "_nested_sequence.py": [],
    "_scalars.py": [],
    "_shape.py": [],
    "_add_docstring.py": [],
    "_callable.pyi": [],
    "_generic_alias.py": [
        {
            "commit": "cc5aaf2cc744dfbc0a16c1d20607853874307333",
            "timestamp": "2023-02-02T18:27:49+01:00",
            "author": "BvB93",
            "commit_message": "MAINT: Remove `npt._GenericAlias` in favor of py39 `types.GenericAlias`",
            "additions": 0,
            "deletions": 245,
            "change_type": "DELETE",
            "diff": "@@ -1,245 +0,0 @@\n-from __future__ import annotations\n-\n-import sys\n-import types\n-from collections.abc import Generator, Iterable, Iterator\n-from typing import (\n-    Any,\n-    ClassVar,\n-    NoReturn,\n-    TypeVar,\n-    TYPE_CHECKING,\n-)\n-\n-import numpy as np\n-\n-__all__ = [\"_GenericAlias\", \"NDArray\"]\n-\n-_T = TypeVar(\"_T\", bound=\"_GenericAlias\")\n-\n-\n-def _to_str(obj: object) -> str:\n-    \"\"\"Helper function for `_GenericAlias.__repr__`.\"\"\"\n-    if obj is Ellipsis:\n-        return '...'\n-    elif isinstance(obj, type) and not isinstance(obj, _GENERIC_ALIAS_TYPE):\n-        if obj.__module__ == 'builtins':\n-            return obj.__qualname__\n-        else:\n-            return f'{obj.__module__}.{obj.__qualname__}'\n-    else:\n-        return repr(obj)\n-\n-\n-def _parse_parameters(args: Iterable[Any]) -> Generator[TypeVar, None, None]:\n-    \"\"\"Search for all typevars and typevar-containing objects in `args`.\n-\n-    Helper function for `_GenericAlias.__init__`.\n-\n-    \"\"\"\n-    for i in args:\n-        if hasattr(i, \"__parameters__\"):\n-            yield from i.__parameters__\n-        elif isinstance(i, TypeVar):\n-            yield i\n-\n-\n-def _reconstruct_alias(alias: _T, parameters: Iterator[TypeVar]) -> _T:\n-    \"\"\"Recursively replace all typevars with those from `parameters`.\n-\n-    Helper function for `_GenericAlias.__getitem__`.\n-\n-    \"\"\"\n-    args = []\n-    for i in alias.__args__:\n-        if isinstance(i, TypeVar):\n-            value: Any = next(parameters)\n-        elif isinstance(i, _GenericAlias):\n-            value = _reconstruct_alias(i, parameters)\n-        elif hasattr(i, \"__parameters__\"):\n-            prm_tup = tuple(next(parameters) for _ in i.__parameters__)\n-            value = i[prm_tup]\n-        else:\n-            value = i\n-        args.append(value)\n-\n-    cls = type(alias)\n-    return cls(alias.__origin__, tuple(args), alias.__unpacked__)\n-\n-\n-class _GenericAlias:\n-    \"\"\"A python-based backport of the `types.GenericAlias` class.\n-\n-    E.g. for ``t = list[int]``, ``t.__origin__`` is ``list`` and\n-    ``t.__args__`` is ``(int,)``.\n-\n-    See Also\n-    --------\n-    :pep:`585`\n-        The PEP responsible for introducing `types.GenericAlias`.\n-\n-    \"\"\"\n-\n-    __slots__ = (\n-        \"__weakref__\",\n-        \"_origin\",\n-        \"_args\",\n-        \"_parameters\",\n-        \"_hash\",\n-        \"_starred\",\n-    )\n-\n-    @property\n-    def __origin__(self) -> type:\n-        return super().__getattribute__(\"_origin\")\n-\n-    @property\n-    def __args__(self) -> tuple[object, ...]:\n-        return super().__getattribute__(\"_args\")\n-\n-    @property\n-    def __parameters__(self) -> tuple[TypeVar, ...]:\n-        \"\"\"Type variables in the ``GenericAlias``.\"\"\"\n-        return super().__getattribute__(\"_parameters\")\n-\n-    @property\n-    def __unpacked__(self) -> bool:\n-        return super().__getattribute__(\"_starred\")\n-\n-    @property\n-    def __typing_unpacked_tuple_args__(self) -> tuple[object, ...] | None:\n-        # NOTE: This should return `__args__` if `__origin__` is a tuple,\n-        # which should never be the case with how `_GenericAlias` is used\n-        # within numpy\n-        return None\n-\n-    def __init__(\n-        self,\n-        origin: type,\n-        args: object | tuple[object, ...],\n-        starred: bool = False,\n-    ) -> None:\n-        self._origin = origin\n-        self._args = args if isinstance(args, tuple) else (args,)\n-        self._parameters = tuple(_parse_parameters(self.__args__))\n-        self._starred = starred\n-\n-    @property\n-    def __call__(self) -> type[Any]:\n-        return self.__origin__\n-\n-    def __reduce__(self: _T) -> tuple[\n-        type[_T],\n-        tuple[type[Any], tuple[object, ...], bool],\n-    ]:\n-        cls = type(self)\n-        return cls, (self.__origin__, self.__args__, self.__unpacked__)\n-\n-    def __mro_entries__(self, bases: Iterable[object]) -> tuple[type[Any]]:\n-        return (self.__origin__,)\n-\n-    def __dir__(self) -> list[str]:\n-        \"\"\"Implement ``dir(self)``.\"\"\"\n-        cls = type(self)\n-        dir_origin = set(dir(self.__origin__))\n-        return sorted(cls._ATTR_EXCEPTIONS | dir_origin)\n-\n-    def __hash__(self) -> int:\n-        \"\"\"Return ``hash(self)``.\"\"\"\n-        # Attempt to use the cached hash\n-        try:\n-            return super().__getattribute__(\"_hash\")\n-        except AttributeError:\n-            self._hash: int = (\n-                hash(self.__origin__) ^\n-                hash(self.__args__) ^\n-                hash(self.__unpacked__)\n-            )\n-            return super().__getattribute__(\"_hash\")\n-\n-    def __instancecheck__(self, obj: object) -> NoReturn:\n-        \"\"\"Check if an `obj` is an instance.\"\"\"\n-        raise TypeError(\"isinstance() argument 2 cannot be a \"\n-                        \"parameterized generic\")\n-\n-    def __subclasscheck__(self, cls: type) -> NoReturn:\n-        \"\"\"Check if a `cls` is a subclass.\"\"\"\n-        raise TypeError(\"issubclass() argument 2 cannot be a \"\n-                        \"parameterized generic\")\n-\n-    def __repr__(self) -> str:\n-        \"\"\"Return ``repr(self)``.\"\"\"\n-        args = \", \".join(_to_str(i) for i in self.__args__)\n-        origin = _to_str(self.__origin__)\n-        prefix = \"*\" if self.__unpacked__ else \"\"\n-        return f\"{prefix}{origin}[{args}]\"\n-\n-    def __getitem__(self: _T, key: object | tuple[object, ...]) -> _T:\n-        \"\"\"Return ``self[key]``.\"\"\"\n-        key_tup = key if isinstance(key, tuple) else (key,)\n-\n-        if len(self.__parameters__) == 0:\n-            raise TypeError(f\"There are no type variables left in {self}\")\n-        elif len(key_tup) > len(self.__parameters__):\n-            raise TypeError(f\"Too many arguments for {self}\")\n-        elif len(key_tup) < len(self.__parameters__):\n-            raise TypeError(f\"Too few arguments for {self}\")\n-\n-        key_iter = iter(key_tup)\n-        return _reconstruct_alias(self, key_iter)\n-\n-    def __eq__(self, value: object) -> bool:\n-        \"\"\"Return ``self == value``.\"\"\"\n-        if not isinstance(value, _GENERIC_ALIAS_TYPE):\n-            return NotImplemented\n-        return (\n-            self.__origin__ == value.__origin__ and\n-            self.__args__ == value.__args__ and\n-            self.__unpacked__ == getattr(\n-                value, \"__unpacked__\", self.__unpacked__\n-            )\n-        )\n-\n-    def __iter__(self: _T) -> Generator[_T, None, None]:\n-        \"\"\"Return ``iter(self)``.\"\"\"\n-        cls = type(self)\n-        yield cls(self.__origin__, self.__args__, True)\n-\n-    _ATTR_EXCEPTIONS: ClassVar[frozenset[str]] = frozenset({\n-        \"__origin__\",\n-        \"__args__\",\n-        \"__parameters__\",\n-        \"__mro_entries__\",\n-        \"__reduce__\",\n-        \"__reduce_ex__\",\n-        \"__copy__\",\n-        \"__deepcopy__\",\n-        \"__unpacked__\",\n-        \"__typing_unpacked_tuple_args__\",\n-        \"__class__\",\n-    })\n-\n-    def __getattribute__(self, name: str) -> Any:\n-        \"\"\"Return ``getattr(self, name)``.\"\"\"\n-        # Pull the attribute from `__origin__` unless its\n-        # name is in `_ATTR_EXCEPTIONS`\n-        cls = type(self)\n-        if name in cls._ATTR_EXCEPTIONS:\n-            return super().__getattribute__(name)\n-        return getattr(self.__origin__, name)\n-\n-\n-# See `_GenericAlias.__eq__`\n-if sys.version_info >= (3, 9):\n-    _GENERIC_ALIAS_TYPE = (_GenericAlias, types.GenericAlias)\n-else:\n-    _GENERIC_ALIAS_TYPE = (_GenericAlias,)\n-\n-ScalarType = TypeVar(\"ScalarType\", bound=np.generic, covariant=True)\n-\n-if TYPE_CHECKING or sys.version_info >= (3, 9):\n-    _DType = np.dtype[ScalarType]\n-    NDArray = np.ndarray[Any, np.dtype[ScalarType]]\n-else:\n-    _DType = _GenericAlias(np.dtype, (ScalarType,))\n-    NDArray = _GenericAlias(np.ndarray, (Any, _DType))\n",
            "comment_added_diff": {},
            "comment_deleted_diff": {
                "111": "        # NOTE: This should return `__args__` if `__origin__` is a tuple,",
                "112": "        # which should never be the case with how `_GenericAlias` is used",
                "113": "        # within numpy",
                "149": "        # Attempt to use the cached hash",
                "224": "        # Pull the attribute from `__origin__` unless its",
                "225": "        # name is in `_ATTR_EXCEPTIONS`",
                "232": "# See `_GenericAlias.__eq__`"
            },
            "comment_modified_diff": {}
        }
    ],
    "test_generic_alias.py": [
        {
            "commit": "cc5aaf2cc744dfbc0a16c1d20607853874307333",
            "timestamp": "2023-02-02T18:27:49+01:00",
            "author": "BvB93",
            "commit_message": "MAINT: Remove `npt._GenericAlias` in favor of py39 `types.GenericAlias`",
            "additions": 0,
            "deletions": 188,
            "change_type": "DELETE",
            "diff": "@@ -1,188 +0,0 @@\n-from __future__ import annotations\n-\n-import sys\n-import copy\n-import types\n-import pickle\n-import weakref\n-from typing import TypeVar, Any, Union, Callable\n-\n-import pytest\n-import numpy as np\n-from numpy._typing._generic_alias import _GenericAlias\n-from typing_extensions import Unpack\n-\n-ScalarType = TypeVar(\"ScalarType\", bound=np.generic, covariant=True)\n-T1 = TypeVar(\"T1\")\n-T2 = TypeVar(\"T2\")\n-DType = _GenericAlias(np.dtype, (ScalarType,))\n-NDArray = _GenericAlias(np.ndarray, (Any, DType))\n-\n-# NOTE: The `npt._GenericAlias` *class* isn't quite stable on python >=3.11.\n-# This is not a problem during runtime (as it's 3.8-exclusive), but we still\n-# need it for the >=3.9 in order to verify its semantics match\n-# `types.GenericAlias` replacement. xref numpy/numpy#21526\n-if sys.version_info >= (3, 9):\n-    DType_ref = types.GenericAlias(np.dtype, (ScalarType,))\n-    NDArray_ref = types.GenericAlias(np.ndarray, (Any, DType_ref))\n-    FuncType = Callable[[\"_GenericAlias | types.GenericAlias\"], Any]\n-else:\n-    DType_ref = Any\n-    NDArray_ref = Any\n-    FuncType = Callable[[\"_GenericAlias\"], Any]\n-\n-GETATTR_NAMES = sorted(set(dir(np.ndarray)) - _GenericAlias._ATTR_EXCEPTIONS)\n-\n-BUFFER = np.array([1], dtype=np.int64)\n-BUFFER.setflags(write=False)\n-\n-def _get_subclass_mro(base: type) -> tuple[type, ...]:\n-    class Subclass(base):  # type: ignore[misc,valid-type]\n-        pass\n-    return Subclass.__mro__[1:]\n-\n-\n-class TestGenericAlias:\n-    \"\"\"Tests for `numpy._typing._generic_alias._GenericAlias`.\"\"\"\n-\n-    @pytest.mark.parametrize(\"name,func\", [\n-        (\"__init__\", lambda n: n),\n-        (\"__init__\", lambda n: _GenericAlias(np.ndarray, Any)),\n-        (\"__init__\", lambda n: _GenericAlias(np.ndarray, (Any,))),\n-        (\"__init__\", lambda n: _GenericAlias(np.ndarray, (Any, Any))),\n-        (\"__init__\", lambda n: _GenericAlias(np.ndarray, T1)),\n-        (\"__init__\", lambda n: _GenericAlias(np.ndarray, (T1,))),\n-        (\"__init__\", lambda n: _GenericAlias(np.ndarray, (T1, T2))),\n-        (\"__origin__\", lambda n: n.__origin__),\n-        (\"__args__\", lambda n: n.__args__),\n-        (\"__parameters__\", lambda n: n.__parameters__),\n-        (\"__mro_entries__\", lambda n: n.__mro_entries__([object])),\n-        (\"__hash__\", lambda n: hash(n)),\n-        (\"__repr__\", lambda n: repr(n)),\n-        (\"__getitem__\", lambda n: n[np.float64]),\n-        (\"__getitem__\", lambda n: n[ScalarType][np.float64]),\n-        (\"__getitem__\", lambda n: n[Union[np.int64, ScalarType]][np.float64]),\n-        (\"__getitem__\", lambda n: n[Union[T1, T2]][np.float32, np.float64]),\n-        (\"__eq__\", lambda n: n == n),\n-        (\"__ne__\", lambda n: n != np.ndarray),\n-        (\"__call__\", lambda n: n((1,), np.int64, BUFFER)),\n-        (\"__call__\", lambda n: n(shape=(1,), dtype=np.int64, buffer=BUFFER)),\n-        (\"subclassing\", lambda n: _get_subclass_mro(n)),\n-        (\"pickle\", lambda n: n == pickle.loads(pickle.dumps(n))),\n-    ])\n-    def test_pass(self, name: str, func: FuncType) -> None:\n-        \"\"\"Compare `types.GenericAlias` with its numpy-based backport.\n-\n-        Checker whether ``func`` runs as intended and that both `GenericAlias`\n-        and `_GenericAlias` return the same result.\n-\n-        \"\"\"\n-        value = func(NDArray)\n-\n-        if sys.version_info >= (3, 9):\n-            value_ref = func(NDArray_ref)\n-            assert value == value_ref\n-\n-    @pytest.mark.parametrize(\"name,func\", [\n-        (\"__copy__\", lambda n: n == copy.copy(n)),\n-        (\"__deepcopy__\", lambda n: n == copy.deepcopy(n)),\n-    ])\n-    def test_copy(self, name: str, func: FuncType) -> None:\n-        value = func(NDArray)\n-\n-        # xref bpo-45167\n-        GE_398 = (\n-            sys.version_info[:2] == (3, 9) and sys.version_info >= (3, 9, 8)\n-        )\n-        if GE_398 or sys.version_info >= (3, 10, 1):\n-            value_ref = func(NDArray_ref)\n-            assert value == value_ref\n-\n-    def test_dir(self) -> None:\n-        value = dir(NDArray)\n-        if sys.version_info < (3, 9):\n-            return\n-\n-        # A number attributes only exist in `types.GenericAlias` in >= 3.11\n-        if sys.version_info < (3, 11, 0, \"beta\", 3):\n-            value.remove(\"__typing_unpacked_tuple_args__\")\n-        if sys.version_info < (3, 11, 0, \"beta\", 1):\n-            value.remove(\"__unpacked__\")\n-        assert value == dir(NDArray_ref)\n-\n-    @pytest.mark.parametrize(\"name,func,dev_version\", [\n-        (\"__iter__\", lambda n: len(list(n)), (\"beta\", 1)),\n-        (\"__iter__\", lambda n: next(iter(n)), (\"beta\", 1)),\n-        (\"__unpacked__\", lambda n: n.__unpacked__, (\"beta\", 1)),\n-        (\"Unpack\", lambda n: Unpack[n], (\"beta\", 1)),\n-\n-        # The right operand should now have `__unpacked__ = True`,\n-        # and they are thus now longer equivalent\n-        (\"__ne__\", lambda n: n != next(iter(n)), (\"beta\", 1)),\n-\n-        # >= beta3\n-        (\"__typing_unpacked_tuple_args__\",\n-         lambda n: n.__typing_unpacked_tuple_args__, (\"beta\", 3)),\n-\n-        # >= beta4\n-        (\"__class__\", lambda n: n.__class__ == type(n), (\"beta\", 4)),\n-    ])\n-    def test_py311_features(\n-        self,\n-        name: str,\n-        func: FuncType,\n-        dev_version: tuple[str, int],\n-    ) -> None:\n-        \"\"\"Test Python 3.11 features.\"\"\"\n-        value = func(NDArray)\n-\n-        if sys.version_info >= (3, 11, 0, *dev_version):\n-            value_ref = func(NDArray_ref)\n-            assert value == value_ref\n-\n-    def test_weakref(self) -> None:\n-        \"\"\"Test ``__weakref__``.\"\"\"\n-        value = weakref.ref(NDArray)()\n-\n-        if sys.version_info >= (3, 9, 1):  # xref bpo-42332\n-            value_ref = weakref.ref(NDArray_ref)()\n-            assert value == value_ref\n-\n-    @pytest.mark.parametrize(\"name\", GETATTR_NAMES)\n-    def test_getattr(self, name: str) -> None:\n-        \"\"\"Test that `getattr` wraps around the underlying type,\n-        aka ``__origin__``.\n-\n-        \"\"\"\n-        value = getattr(NDArray, name)\n-        value_ref1 = getattr(np.ndarray, name)\n-\n-        if sys.version_info >= (3, 9):\n-            value_ref2 = getattr(NDArray_ref, name)\n-            assert value == value_ref1 == value_ref2\n-        else:\n-            assert value == value_ref1\n-\n-    @pytest.mark.parametrize(\"name,exc_type,func\", [\n-        (\"__getitem__\", TypeError, lambda n: n[()]),\n-        (\"__getitem__\", TypeError, lambda n: n[Any, Any]),\n-        (\"__getitem__\", TypeError, lambda n: n[Any][Any]),\n-        (\"isinstance\", TypeError, lambda n: isinstance(np.array(1), n)),\n-        (\"issublass\", TypeError, lambda n: issubclass(np.ndarray, n)),\n-        (\"setattr\", AttributeError, lambda n: setattr(n, \"__origin__\", int)),\n-        (\"setattr\", AttributeError, lambda n: setattr(n, \"test\", int)),\n-        (\"getattr\", AttributeError, lambda n: getattr(n, \"test\")),\n-    ])\n-    def test_raise(\n-        self,\n-        name: str,\n-        exc_type: type[BaseException],\n-        func: FuncType,\n-    ) -> None:\n-        \"\"\"Test operations that are supposed to raise.\"\"\"\n-        with pytest.raises(exc_type):\n-            func(NDArray)\n-\n-        if sys.version_info >= (3, 9):\n-            with pytest.raises(exc_type):\n-                func(NDArray_ref)\n",
            "comment_added_diff": {},
            "comment_deleted_diff": {
                "21": "# NOTE: The `npt._GenericAlias` *class* isn't quite stable on python >=3.11.",
                "22": "# This is not a problem during runtime (as it's 3.8-exclusive), but we still",
                "23": "# need it for the >=3.9 in order to verify its semantics match",
                "24": "# `types.GenericAlias` replacement. xref numpy/numpy#21526",
                "40": "    class Subclass(base):  # type: ignore[misc,valid-type]",
                "93": "        # xref bpo-45167",
                "106": "        # A number attributes only exist in `types.GenericAlias` in >= 3.11",
                "119": "        # The right operand should now have `__unpacked__ = True`,",
                "120": "        # and they are thus now longer equivalent",
                "123": "        # >= beta3",
                "127": "        # >= beta4",
                "147": "        if sys.version_info >= (3, 9, 1):  # xref bpo-42332"
            },
            "comment_modified_diff": {}
        }
    ],
    "npy_cpu_features.h": [],
    "cpu_avx512_spr.c": [],
    "cpu_avx512fp16.c": [],
    "test_array_coercion.py": [
        {
            "commit": "a0c154a7f1d29c6d948af90d516c6cbe330d29cd",
            "timestamp": "2023-02-03T12:43:00+01:00",
            "author": "Sebastian Berg",
            "commit_message": "TST: Test new paths for `arr.astype()` to check it accepts DType classes",
            "additions": 2,
            "deletions": 0,
            "change_type": "MODIFY",
            "diff": "@@ -163,6 +163,8 @@ def test_basic_stringlength(self, obj):\n         assert np.array(arr, dtype=\"S\").dtype == expected\n         # Check that .astype() behaves identical\n         assert arr.astype(\"S\").dtype == expected\n+        # The DType class is accepted by `.astype()`\n+        assert arr.astype(type(np.dtype(\"S\"))).dtype == expected\n \n     @pytest.mark.parametrize(\"obj\",\n             [object(), 1.2, 10**43, None, \"string\"],\n",
            "comment_added_diff": {
                "166": "        # The DType class is accepted by `.astype()`"
            },
            "comment_deleted_diff": {},
            "comment_modified_diff": {}
        },
        {
            "commit": "e05361dc84532fb106ef1c4d1357df5517e126b6",
            "timestamp": "2023-03-20T15:27:18-06:00",
            "author": "Nathan Goldbaum",
            "commit_message": "MAINT: add test for string dtype discovery from a dtypemeta",
            "additions": 2,
            "deletions": 0,
            "change_type": "MODIFY",
            "diff": "@@ -161,6 +161,8 @@ def test_basic_stringlength(self, obj):\n         # A nested array is also discovered correctly\n         arr = np.array(obj, dtype=\"O\")\n         assert np.array(arr, dtype=\"S\").dtype == expected\n+        # Also if we use the dtype class\n+        assert np.array(arr, dtype=type(expected)).dtype == expected\n         # Check that .astype() behaves identical\n         assert arr.astype(\"S\").dtype == expected\n         # The DType class is accepted by `.astype()`\n",
            "comment_added_diff": {
                "164": "        # Also if we use the dtype class"
            },
            "comment_deleted_diff": {},
            "comment_modified_diff": {}
        },
        {
            "commit": "4c2f6f6c3c012b2158cc468a95bce91906f75f0e",
            "timestamp": "2023-03-21T22:06:02+00:00",
            "author": "Christian Veenhuis",
            "commit_message": "MAINT: Fix 'paramteric' typo",
            "additions": 8,
            "deletions": 8,
            "change_type": "MODIFY",
            "diff": "@@ -1,6 +1,6 @@\n \"\"\"\n Tests for array coercion, mainly through testing `np.array` results directly.\n-Note that other such tests exist e.g. in `test_api.py` and many corner-cases\n+Note that other such tests exist, e.g., in `test_api.py` and many corner-cases\n are tested (sometimes indirectly) elsewhere.\n \"\"\"\n \n@@ -20,8 +20,8 @@\n def arraylikes():\n     \"\"\"\n     Generator for functions converting an array into various array-likes.\n-    If full is True (default) includes array-likes not capable of handling\n-    all dtypes\n+    If full is True (default) it includes array-likes not capable of handling\n+    all dtypes.\n     \"\"\"\n     # base array:\n     def ndarray(a):\n@@ -40,8 +40,8 @@ def subclass(a):\n \n     class _SequenceLike():\n         # We are giving a warning that array-like's were also expected to be\n-        # sequence-like in `np.array([array_like])`, this can be removed\n-        # when the deprecation exired (started NumPy 1.20)\n+        # sequence-like in `np.array([array_like])`. This can be removed\n+        # when the deprecation expired (started NumPy 1.20).\n         def __len__(self):\n             raise TypeError\n \n@@ -259,7 +259,7 @@ def test_scalar_promotion(self):\n     @pytest.mark.parametrize(\"scalar\", scalar_instances())\n     def test_scalar_coercion(self, scalar):\n         # This tests various scalar coercion paths, mainly for the numerical\n-        # types.  It includes some paths not directly related to `np.array`\n+        # types. It includes some paths not directly related to `np.array`.\n         if isinstance(scalar, np.inexact):\n             # Ensure we have a full-precision number if available\n             scalar = type(scalar)((scalar * 2)**0.5)\n@@ -294,7 +294,7 @@ def test_scalar_coercion_same_as_cast_and_assignment(self, cast_to):\n            * `np.array(scalar, dtype=dtype)`\n            * `np.empty((), dtype=dtype)[()] = scalar`\n            * `np.array(scalar).astype(dtype)`\n-        should behave the same.  The only exceptions are paramteric dtypes\n+        should behave the same.  The only exceptions are parametric dtypes\n         (mainly datetime/timedelta without unit) and void without fields.\n         \"\"\"\n         dtype = cast_to.dtype  # use to parametrize only the target dtype\n@@ -386,7 +386,7 @@ def test_scalar_to_int_coerce_does_not_cast(self, dtype, scalar, error):\n         \"\"\"\n         dtype = np.dtype(dtype)\n \n-        # This is a special case using casting logic.  It warns for the NaN\n+        # This is a special case using casting logic. It warns for the NaN\n         # but allows the cast (giving undefined behaviour).\n         with np.errstate(invalid=\"ignore\"):\n             coerced = np.array(scalar, dtype=dtype)\n",
            "comment_added_diff": {
                "43": "        # sequence-like in `np.array([array_like])`. This can be removed",
                "44": "        # when the deprecation expired (started NumPy 1.20).",
                "262": "        # types. It includes some paths not directly related to `np.array`.",
                "389": "        # This is a special case using casting logic. It warns for the NaN"
            },
            "comment_deleted_diff": {
                "43": "        # sequence-like in `np.array([array_like])`, this can be removed",
                "44": "        # when the deprecation exired (started NumPy 1.20)",
                "262": "        # types.  It includes some paths not directly related to `np.array`",
                "389": "        # This is a special case using casting logic.  It warns for the NaN"
            },
            "comment_modified_diff": {
                "43": "        # sequence-like in `np.array([array_like])`, this can be removed",
                "44": "        # when the deprecation exired (started NumPy 1.20)",
                "262": "        # types.  It includes some paths not directly related to `np.array`",
                "389": "        # This is a special case using casting logic.  It warns for the NaN"
            }
        },
        {
            "commit": "a8face152ced3e951d067ee7276ff1fcb6be61b8",
            "timestamp": "2023-04-25T14:15:59+02:00",
            "author": "Sebastian Berg",
            "commit_message": "DEP: Finalize checking for sequence-like if something is array-like\n\nThis was always just a stop-gap for shapely basically, so there is\nno harm finalizing things.",
            "additions": 13,
            "deletions": 5,
            "change_type": "MODIFY",
            "diff": "@@ -39,9 +39,9 @@ def subclass(a):\n     yield subclass\n \n     class _SequenceLike():\n-        # We are giving a warning that array-like's were also expected to be\n-        # sequence-like in `np.array([array_like])`. This can be removed\n-        # when the deprecation expired (started NumPy 1.20).\n+        # Older NumPy versions, sometimes cared whether a protocol array was\n+        # also _SequenceLike.  This shouldn't matter, but keep it for now\n+        # for __array__ and not the others.\n         def __len__(self):\n             raise TypeError\n \n@@ -62,7 +62,7 @@ def __array__(self, dtype=None):\n     yield param(memoryview, id=\"memoryview\")\n \n     # Array-interface\n-    class ArrayInterface(_SequenceLike):\n+    class ArrayInterface:\n         def __init__(self, a):\n             self.a = a  # need to hold on to keep interface valid\n             self.__array_interface__ = a.__array_interface__\n@@ -70,7 +70,7 @@ def __init__(self, a):\n     yield param(ArrayInterface, id=\"__array_interface__\")\n \n     # Array-Struct\n-    class ArrayStruct(_SequenceLike):\n+    class ArrayStruct:\n         def __init__(self, a):\n             self.a = a  # need to hold on to keep struct valid\n             self.__array_struct__ = a.__array_struct__\n@@ -654,6 +654,14 @@ def test_0d_object_special_case(self, arraylike):\n         res = np.array([obj], dtype=object)\n         assert res[0] is obj\n \n+    @pytest.mark.parametrize(\"arraylike\", arraylikes())\n+    @pytest.mark.parametrize(\"arr\", [np.array(0.), np.arange(4)])\n+    def test_object_assignment_special_case(self, arraylike, arr):\n+        obj = arraylike(arr)\n+        empty = np.arange(1, dtype=object)\n+        empty[:] = [obj]\n+        assert empty[0] is obj\n+\n     def test_0d_generic_special_case(self):\n         class ArraySubclass(np.ndarray):\n             def __float__(self):\n",
            "comment_added_diff": {
                "42": "        # Older NumPy versions, sometimes cared whether a protocol array was",
                "43": "        # also _SequenceLike.  This shouldn't matter, but keep it for now",
                "44": "        # for __array__ and not the others."
            },
            "comment_deleted_diff": {
                "42": "        # We are giving a warning that array-like's were also expected to be",
                "43": "        # sequence-like in `np.array([array_like])`. This can be removed",
                "44": "        # when the deprecation expired (started NumPy 1.20)."
            },
            "comment_modified_diff": {
                "42": "        # We are giving a warning that array-like's were also expected to be",
                "43": "        # sequence-like in `np.array([array_like])`. This can be removed",
                "44": "        # when the deprecation expired (started NumPy 1.20)."
            }
        },
        {
            "commit": "23a6d794c000eeb069e59ac164afed18f7ea37cb",
            "timestamp": "2023-04-26T14:58:41+02:00",
            "author": "Sebastian Berg",
            "commit_message": "DEP: Finalize subarray from non-subarray creation futurewarning\n\nThis unfortunately switches over the C-only path when FromArray\nis called with a subarray dtype.\n\nTogether with the previous commit (which is very simple but in a sense\ndoes the heavy lifting):\n\nCloses gh-23083",
            "additions": 25,
            "deletions": 0,
            "change_type": "MODIFY",
            "diff": "@@ -845,3 +845,28 @@ def test_deprecated(self):\n             np.array(self.WeirdArrayLike())\n         with pytest.raises(RuntimeError):\n             np.array(self.WeirdArrayInterface())\n+\n+\n+def test_subarray_from_array_construction():\n+    # Arrays are more complex, since they \"broadcast\" on success:\n+    arr = np.array([1, 2])\n+\n+    res = arr.astype(\"(2)i,\")\n+    assert_array_equal(res, [[1, 1], [2, 2]])\n+\n+    res = np.array(arr, dtype=\"(2)i,\")\n+\n+    assert_array_equal(res, [[1, 1], [2, 2]])\n+\n+    res = np.array([[(1,), (2,)], arr], dtype=\"(2)i,\")\n+    assert_array_equal(res, [[[1, 1], [2, 2]], [[1, 1], [2, 2]]])\n+\n+    # Also try a multi-dimensional example:\n+    arr = np.arange(5 * 2).reshape(5, 2)\n+    expected = np.broadcast_to(arr[:, :, np.newaxis, np.newaxis], (5, 2, 2, 2))\n+\n+    res = arr.astype(\"(2,2)f\")\n+    assert_array_equal(res, expected)\n+\n+    res = np.array(arr, dtype=\"(2,2)f\")\n+    assert_array_equal(res, expected)\n",
            "comment_added_diff": {
                "851": "    # Arrays are more complex, since they \"broadcast\" on success:",
                "864": "    # Also try a multi-dimensional example:"
            },
            "comment_deleted_diff": {},
            "comment_modified_diff": {}
        },
        {
            "commit": "56d5dccd97182a193f244f4f68abb5a87eb160c3",
            "timestamp": "2023-07-03T21:18:22+02:00",
            "author": "Sebastian Berg",
            "commit_message": "BUG: Only replace dtype temporarily if dimensions changed\n\nFor strings this is (currently) arguably not correct... Now of course\nstrings shouldn't be changed to S0 probably, but that is a different\nmatter.\n\nIt seems there is something odd (possibly I just kept it as a corner\ncase) for coercion from object arrays.\n\nTests should cover the paths I think (lets see what CI says) but\nI couldn't reliably trigger failures for all paths.\n\nCloses gh-24043",
            "additions": 26,
            "deletions": 0,
            "change_type": "MODIFY",
            "diff": "@@ -870,3 +870,29 @@ def test_subarray_from_array_construction():\n \n     res = np.array(arr, dtype=\"(2,2)f\")\n     assert_array_equal(res, expected)\n+\n+\n+def test_empty_string():\n+    # Empty strings are unfortunately often converted to S1 and we need to\n+    # make sure we are filling the S1 and not the (possibly) detected S0\n+    # result.  This should likely just return S0 and if not maybe the decision\n+    # to return S1 should be moved.\n+    res = np.array([\"\"] * 10, dtype=\"S\")\n+    assert_array_equal(res, np.array(\"\\0\", \"S1\"))\n+    assert res.dtype == \"S1\"\n+\n+    arr = np.array([\"\"] * 10, dtype=object)\n+\n+    res = arr.astype(\"S\")\n+    assert_array_equal(res, b\"\")\n+    assert res.dtype == \"S1\"\n+\n+    res = np.array(arr, dtype=\"S\")\n+    assert_array_equal(res, b\"\")\n+    # TODO: This is arguably weird/wrong, but seems old:\n+    assert res.dtype == \"S8\"\n+\n+    res = np.array([[\"\"] * 10, arr], dtype=\"S\")\n+    assert_array_equal(res, b\"\")\n+    assert res.shape == (2, 10)\n+    assert res.dtype == \"S1\"\n",
            "comment_added_diff": {
                "876": "    # Empty strings are unfortunately often converted to S1 and we need to",
                "877": "    # make sure we are filling the S1 and not the (possibly) detected S0",
                "878": "    # result.  This should likely just return S0 and if not maybe the decision",
                "879": "    # to return S1 should be moved.",
                "892": "    # TODO: This is arguably weird/wrong, but seems old:"
            },
            "comment_deleted_diff": {},
            "comment_modified_diff": {}
        }
    ],
    "simd_qsort.dispatch.cpp": [],
    "simd_qsort.hpp": [],
    "simd_qsort_16bit.dispatch.cpp": [],
    "conftest.py": [
        {
            "commit": "790426e16466af4ef05ed295a27cbf5e7ff76a14",
            "timestamp": "2023-02-07T22:05:23+02:00",
            "author": "mattip",
            "commit_message": "TST, BUG: add a test for unary ufuncs, fix condition for indexed loops",
            "additions": 2,
            "deletions": 0,
            "change_type": "MODIFY",
            "diff": "@@ -40,6 +40,8 @@\n     \"numpy-profile\" if os.path.isfile(_pytest_ini) else \"np.test() profile\"\n )\n \n+# The experimentalAPI is used in _umath_tests\n+os.environ[\"NUMPY_EXPERIMENTAL_DTYPE_API\"] = \"1\"\n \n def pytest_configure(config):\n     config.addinivalue_line(\"markers\",\n",
            "comment_added_diff": {
                "43": "# The experimentalAPI is used in _umath_tests"
            },
            "comment_deleted_diff": {},
            "comment_modified_diff": {}
        }
    ],
    "_umath_tests.c.src": [],
    "test_character.py": [],
    "loops_minmax.dispatch.c.src": [],
    "test_argparse.py": [
        {
            "commit": "6b6b7914f86c6d45f2d589bde9fa0a74277e2854",
            "timestamp": "2023-06-10T21:30:00+02:00",
            "author": "Sebastian Berg",
            "commit_message": "TST: Add minimal test to cover many-argument path",
            "additions": 9,
            "deletions": 0,
            "change_type": "MODIFY",
            "diff": "@@ -60,3 +60,12 @@ def test_string_fallbacks():\n             match=\"got an unexpected keyword argument 'missing_arg'\"):\n         func(2, **{missing_arg: 3})\n \n+\n+def test_too_many_arguments_method_forwarding():\n+    # Not directly related to the standard argument parsing, but we sometimes\n+    # forward methods to Python: arr.mean() calls np.core._methods._mean()\n+    # This adds code coverage for this `npy_forward_method`.\n+    arr = np.arange(3)\n+    args = range(1000)\n+    with pytest.raises(TypeError):\n+        arr.mean(*args)\n",
            "comment_added_diff": {
                "65": "    # Not directly related to the standard argument parsing, but we sometimes",
                "66": "    # forward methods to Python: arr.mean() calls np.core._methods._mean()",
                "67": "    # This adds code coverage for this `npy_forward_method`."
            },
            "comment_deleted_diff": {},
            "comment_modified_diff": {}
        }
    ],
    "test_scalarbuffer.py": [],
    "_iotools.py": [],
    "dtype.py": [],
    "simple.py": [
        {
            "commit": "1621dfff71033c0e9ce2427809205e1b5e3bcfca",
            "timestamp": "2023-09-05T17:58:12+02:00",
            "author": "Bas van Beek",
            "commit_message": "TYP: Use stricter mypy settings\n\n* Disallow generic types lacking a parameter\n* Disallow importing from unknown modules",
            "additions": 4,
            "deletions": 5,
            "change_type": "MODIFY",
            "diff": "@@ -2,18 +2,18 @@\n import operator\n \n import numpy as np\n+import numpy.typing as npt\n from collections.abc import Iterable\n \n # Basic checks\n array = np.array([1, 2])\n \n \n-def ndarray_func(x):\n-    # type: (np.ndarray) -> np.ndarray\n+def ndarray_func(x: npt.NDArray[np.float64]) -> npt.NDArray[np.float64]:\n     return x\n \n \n-ndarray_func(np.array([1, 2]))\n+ndarray_func(np.array([1, 2], dtype=np.float64))\n array == 1\n array.dtype == float\n \n@@ -56,8 +56,7 @@ def ndarray_func(x):\n np.dtype(float) >= np.dtype((\"U\", 10))\n \n # Iteration and indexing\n-def iterable_func(x):\n-    # type: (Iterable) -> Iterable\n+def iterable_func(x: Iterable[object]) -> Iterable[object]:\n     return x\n \n \n",
            "comment_added_diff": {},
            "comment_deleted_diff": {
                "12": "    # type: (np.ndarray) -> np.ndarray",
                "60": "    # type: (Iterable) -> Iterable"
            },
            "comment_modified_diff": {}
        }
    ],
    "README.md": [],
    "global_state.rst": [],
    "multiarraymodule.h": [],
    "23089.change.rst": [],
    "scalar_string.f90": [],
    "heapsort.cpp": [],
    "mergesort.cpp": [],
    "timsort.cpp": [],
    "symbolic.py": [
        {
            "commit": "e1e487acf1d820cbab8a6f97986bf2fb451dfa8e",
            "timestamp": "2023-02-11T22:46:28+01:00",
            "author": "Dimitri Papadopoulos",
            "commit_message": "Fix typos found by copdespell",
            "additions": 2,
            "deletions": 2,
            "change_type": "MODIFY",
            "diff": "@@ -801,7 +801,7 @@ def normalize(obj):\n             else:\n                 _pairs_add(d, t, c)\n         if len(d) == 0:\n-            # TODO: deterimine correct kind\n+            # TODO: determine correct kind\n             return as_number(0)\n         elif len(d) == 1:\n             (t, c), = d.items()\n@@ -836,7 +836,7 @@ def normalize(obj):\n             else:\n                 _pairs_add(d, b, e)\n         if len(d) == 0 or coeff == 0:\n-            # TODO: deterimine correct kind\n+            # TODO: determine correct kind\n             assert isinstance(coeff, number_types)\n             return as_number(coeff)\n         elif len(d) == 1:\n",
            "comment_added_diff": {
                "804": "            # TODO: determine correct kind",
                "839": "            # TODO: determine correct kind"
            },
            "comment_deleted_diff": {
                "804": "            # TODO: deterimine correct kind",
                "839": "            # TODO: deterimine correct kind"
            },
            "comment_modified_diff": {
                "804": "            # TODO: deterimine correct kind",
                "839": "            # TODO: deterimine correct kind"
            }
        }
    ],
    "__init__.cython-30.pxd": [],
    "__init__.pxd": [],
    "checks.pyx": [],
    "_datetime.h": [],
    "datetime_strings.c": [],
    "datetime_strings.h": [],
    "_generator.pyi": [],
    "bit_generator.pyi": [],
    "bit_generator.pyx": [],
    "test_direct.py": [
        {
            "commit": "c1fa0d981258063e00e8976c41d34a6b94b12516",
            "timestamp": "2023-02-14T20:19:28+01:00",
            "author": "Sebastian Berg",
            "commit_message": "API: Add `rng.spawn()`, `bit_gen.spawn()`, and `bit_gen.seed_seq`\n\nThis makes the seed sequence interface more public facing by:\n1. Adding `BitGenerator.seed_seq` to give clear access to `_seed_seq`\n2. Add `spawn()` to both the generator and the bit generator as\n   convenience methods for spawning new instances.\n\nI somewhat remember that we always meant to consider making this\nmore public and adding such convenient methods, but did not do\nso originally.\nSo, now, I do wonder whether it is time to make this fully public?\n\nIt would be nice to follow up at some point with a bit of best practices.\nThis also doesn't add it to the `RandomState`, although doing it via\n`RandomState._bit_generator` is of course valid.\nCan we define as this kind of access as stable enough that downstream\nlibraries could use it?  I fear that backcompat with `RandomState`\nmight make adopting newer things like spawning hard for libraries?",
            "additions": 40,
            "deletions": 0,
            "change_type": "MODIFY",
            "diff": "@@ -148,6 +148,46 @@ def test_seedsequence():\n     assert len(dummy.spawn(10)) == 10\n \n \n+def test_generator_spawning():\n+    \"\"\" Test spawning new generators and bit_generators directly.\n+    \"\"\"\n+    rng = np.random.default_rng()\n+    seq = rng.bit_generator.seed_seq\n+    new_ss = seq.spawn(5)\n+    expected_keys = [seq.spawn_key + (i,) for i in range(5)]\n+    assert [c.spawn_key for c in new_ss] == expected_keys\n+\n+    new_bgs = rng.bit_generator.spawn(5)\n+    expected_keys = [seq.spawn_key + (i,) for i in range(5, 10)]\n+    assert [bg.seed_seq.spawn_key for bg in new_bgs] == expected_keys\n+\n+    new_rngs = rng.spawn(5)\n+    expected_keys = [seq.spawn_key + (i,) for i in range(10, 15)]\n+    found_keys = [rng.bit_generator.seed_seq.spawn_key for rng in new_rngs]\n+    assert found_keys == expected_keys\n+\n+    # Sanity check that streams are actually different:\n+    assert new_rngs[0].uniform() != new_rngs[1].uniform()\n+\n+\n+def test_non_spawnable():\n+    from numpy.random.bit_generator import ISeedSequence\n+\n+    class FakeSeedSequence:\n+        def generate_state(self, n_words, dtype=np.uint32):\n+            return np.zeros(n_words, dtype=dtype)\n+\n+    ISeedSequence.register(FakeSeedSequence)\n+\n+    rng = np.random.default_rng(FakeSeedSequence())\n+\n+    with pytest.raises(TypeError, match=\"The underlying SeedSequence\"):\n+        rng.spawn(5)\n+\n+    with pytest.raises(TypeError, match=\"The underlying SeedSequence\"):\n+        rng.bit_generator.spawn(5)\n+\n+\n class Base:\n     dtype = np.uint64\n     data2 = data1 = {}\n",
            "comment_added_diff": {
                "169": "    # Sanity check that streams are actually different:"
            },
            "comment_deleted_diff": {},
            "comment_modified_diff": {}
        }
    ],
    "test_getlimits.py": [
        {
            "commit": "a5f4e8e417d815cb577a8f2f9414b07a689d69b1",
            "timestamp": "2023-02-15T11:23:05+01:00",
            "author": "Pieter Eendebak",
            "commit_message": "ENH: Add some unit tests for finfo and iinfo (#23109)\n\n* ENH: Add some unit tests for finfo and iinfo\r\n\r\n* make tests more specific\r\n\r\n* refactor\r\n\r\n* lint\r\n\r\n* review comments\r\n\r\n* whitespace\r\n\r\n* add regression test\r\n\r\n* fix test",
            "additions": 49,
            "deletions": 11,
            "change_type": "MODIFY",
            "diff": "@@ -3,6 +3,7 @@\n \"\"\"\n import warnings\n import numpy as np\n+import pytest\n from numpy.core import finfo, iinfo\n from numpy import half, single, double, longdouble\n from numpy.testing import assert_equal, assert_, assert_raises\n@@ -40,20 +41,38 @@ def test_singleton(self):\n         ftype2 = finfo(longdouble)\n         assert_equal(id(ftype), id(ftype2))\n \n+def assert_finfo_equal(f1, f2):\n+    # assert two finfo instances have the same attributes\n+    for attr in ('bits', 'eps', 'epsneg', 'iexp', 'machep',\n+                 'max', 'maxexp', 'min', 'minexp', 'negep', 'nexp',\n+                 'nmant', 'precision', 'resolution', 'tiny',\n+                 'smallest_normal', 'smallest_subnormal'):\n+        assert_equal(getattr(f1, attr), getattr(f2, attr),\n+                     f'finfo instances {f1} and {f2} differ on {attr}')\n+\n+def assert_iinfo_equal(i1, i2):\n+    # assert two iinfo instances have the same attributes\n+    for attr in ('bits', 'min', 'max'):\n+        assert_equal(getattr(i1, attr), getattr(i2, attr),\n+                     f'iinfo instances {i1} and {i2} differ on {attr}')\n+\n class TestFinfo:\n     def test_basic(self):\n         dts = list(zip(['f2', 'f4', 'f8', 'c8', 'c16'],\n                        [np.float16, np.float32, np.float64, np.complex64,\n                         np.complex128]))\n         for dt1, dt2 in dts:\n-            for attr in ('bits', 'eps', 'epsneg', 'iexp', 'machep',\n-                         'max', 'maxexp', 'min', 'minexp', 'negep', 'nexp',\n-                         'nmant', 'precision', 'resolution', 'tiny',\n-                         'smallest_normal', 'smallest_subnormal'):\n-                assert_equal(getattr(finfo(dt1), attr),\n-                             getattr(finfo(dt2), attr), attr)\n+            assert_finfo_equal(finfo(dt1), finfo(dt2))\n+\n         assert_raises(ValueError, finfo, 'i4')\n \n+    def test_regression_gh23108(self):\n+        # np.float32(1.0) and np.float64(1.0) have the same hash and are\n+        # equal under the == operator\n+        f1 = np.finfo(np.float32(1.0))\n+        f2 = np.finfo(np.float64(1.0))\n+        assert f1 != f2\n+\n class TestIinfo:\n     def test_basic(self):\n         dts = list(zip(['i1', 'i2', 'i4', 'i8',\n@@ -61,9 +80,8 @@ def test_basic(self):\n                   [np.int8, np.int16, np.int32, np.int64,\n                    np.uint8, np.uint16, np.uint32, np.uint64]))\n         for dt1, dt2 in dts:\n-            for attr in ('bits', 'min', 'max'):\n-                assert_equal(getattr(iinfo(dt1), attr),\n-                             getattr(iinfo(dt2), attr), attr)\n+            assert_iinfo_equal(iinfo(dt1), iinfo(dt2))\n+\n         assert_raises(ValueError, iinfo, 'f4')\n \n     def test_unsigned_max(self):\n@@ -85,8 +103,28 @@ def test_finfo_repr(self):\n \n \n def test_instances():\n-    iinfo(10)\n-    finfo(3.0)\n+    # Test the finfo and iinfo results on numeric instances agree with\n+    # the results on the corresponding types\n+\n+    for c in [int, np.int16, np.int32, np.int64]:\n+        class_iinfo = iinfo(c)\n+        instance_iinfo = iinfo(c(12))\n+\n+        assert_iinfo_equal(class_iinfo, instance_iinfo)\n+\n+    for c in [float, np.float16, np.float32, np.float64]:\n+        class_finfo = finfo(c)\n+        instance_finfo = finfo(c(1.2))\n+        assert_finfo_equal(class_finfo, instance_finfo)\n+\n+    with pytest.raises(ValueError):\n+        iinfo(10.)\n+\n+    with pytest.raises(ValueError):\n+        iinfo('hi')\n+\n+    with pytest.raises(ValueError):\n+        finfo(np.int64(1))\n \n \n def assert_ma_equal(discovered, ma_like):\n",
            "comment_added_diff": {
                "45": "    # assert two finfo instances have the same attributes",
                "54": "    # assert two iinfo instances have the same attributes",
                "70": "        # np.float32(1.0) and np.float64(1.0) have the same hash and are",
                "71": "        # equal under the == operator",
                "106": "    # Test the finfo and iinfo results on numeric instances agree with",
                "107": "    # the results on the corresponding types"
            },
            "comment_deleted_diff": {},
            "comment_modified_diff": {
                "54": "                             getattr(finfo(dt2), attr), attr)"
            }
        }
    ],
    "_dtype_api.h": [],
    "dtype_traversal.c": [],
    "dtype_traversal.h": [],
    "nditer_api.c": [],
    "refcount.c": [],
    "refcount.h": [],
    "usertypes.c": [],
    "loops_autovec.dispatch.c.src": [],
    "common.py": [
        {
            "commit": "307198d56f1bd182fd61157c6487728b0aa00b76",
            "timestamp": "2023-03-04T19:00:16+00:00",
            "author": "Rohit Goswami",
            "commit_message": "MAINT: Cleanup with Path",
            "additions": 8,
            "deletions": 8,
            "change_type": "MODIFY",
            "diff": "@@ -2,6 +2,7 @@\n import random\n import os\n from functools import lru_cache\n+from pathlib import Path\n \n # Various pre-crafted datasets/variables for testing\n # !!! Must not be changed -- only appended !!!\n@@ -33,6 +34,9 @@\n     'complex128', 'bool',\n ]\n \n+# Path for caching\n+CACHE_ROOT = Path(__file__).resolve().parent.parent / 'env' / 'numpy_benchdata'\n+\n # values which will be used to construct our sample data matrices\n # replicate 10 times to speed up initial imports of this helper\n # and generate some redundancy\n@@ -109,10 +113,6 @@ def get_indexes_rand_():\n     return indexes_rand_\n \n \n-CACHE_ROOT = os.path.dirname(__file__)\n-CACHE_ROOT = os.path.abspath(\n-    os.path.join(CACHE_ROOT, '..', 'env', 'numpy_benchdata')\n-)\n \n \n @lru_cache(typed=True)\n@@ -147,8 +147,8 @@ def get_data(size, dtype, ip_num=0, zeros=False, finite=True, denormal=False):\n     if dtype.kind in 'fc':\n         cache_name += f'{int(finite)}{int(denormal)}'\n     cache_name += '.bin'\n-    cache_path = os.path.join(CACHE_ROOT, cache_name)\n-    if os.path.exists(cache_path):\n+    cache_path = CACHE_ROOT / cache_name\n+    if cache_path.exists():\n         return np.fromfile(cache_path, dtype)\n \n     array = np.ones(size, dtype)\n@@ -210,8 +210,8 @@ def get_data(size, dtype, ip_num=0, zeros=False, finite=True, denormal=False):\n         for start, r in enumerate(rands):\n             array[start:len(r)*stride:stride] = r\n \n-    if not os.path.exists(CACHE_ROOT):\n-        os.mkdir(CACHE_ROOT)\n+    if not CACHE_ROOT.exists():\n+        CACHE_ROOT.mkdir(parents=True)\n     array.tofile(cache_path)\n     return array\n \n",
            "comment_added_diff": {
                "37": "# Path for caching"
            },
            "comment_deleted_diff": {},
            "comment_modified_diff": {}
        }
    ],
    "bench_itemselection.py": [],
    "windows_meson.yml": [],
    "test_item_selection.py": [
        {
            "commit": "629d14f8000dc8cef3ca0097c684fe38e9a2343d",
            "timestamp": "2023-02-20T23:33:22+01:00",
            "author": "Sebastian Berg",
            "commit_message": "TST: Add at least some coverage for put and putmask",
            "additions": 58,
            "deletions": 0,
            "change_type": "MODIFY",
            "diff": "@@ -1,5 +1,7 @@\n import sys\n \n+import pytest\n+\n import numpy as np\n from numpy.testing import (\n     assert_, assert_raises, assert_array_equal, HAS_REFCOUNT\n@@ -84,3 +86,59 @@ def test_empty_argpartition(self):\n \n         b = np.array([0, 1, 2, 3, 4, 5])\n         assert_array_equal(a, b)\n+\n+\n+class TestPutMask:\n+    @pytest.mark.parametrize(\"dtype\", list(np.typecodes[\"All\"]) + [\"i,O\"])\n+    def test_simple(self, dtype):\n+        if dtype.lower() == \"m\":\n+            dtype += \"8[ns]\"\n+\n+        # putmask is weird and doesn't care about value length (even shorter)\n+        vals = np.arange(1001).astype(dtype=dtype)\n+\n+        mask = np.random.randint(2, size=1000).astype(bool)\n+        # Use vals.dtype in case of flexible dtype (i.e. string)\n+        arr = np.zeros(1000, dtype=vals.dtype)\n+        zeros = arr.copy()\n+\n+        np.putmask(arr, mask, vals)\n+        assert_array_equal(arr[mask], vals[:len(mask)][mask])\n+        assert_array_equal(arr[~mask], zeros[~mask])\n+\n+\n+class TestPut:\n+    @pytest.mark.parametrize(\"dtype\", list(np.typecodes[\"All\"])[1:] + [\"i,O\"])\n+    @pytest.mark.parametrize(\"mode\", [\"raise\", \"wrap\", \"clip\"])\n+    def test_simple(self, dtype, mode):\n+        if dtype.lower() == \"m\":\n+            dtype += \"8[ns]\"\n+\n+        # put is weird and doesn't care about value length (even shorter)\n+        vals = np.arange(1001).astype(dtype=dtype)\n+\n+        # Use vals.dtype in case of flexible dtype (i.e. string)\n+        arr = np.zeros(1000, dtype=vals.dtype)\n+        zeros = arr.copy()\n+\n+        if mode == \"clip\":\n+            # Special because 0 and -1 value are \"reserved\" for clip test\n+            indx = np.random.permutation(len(arr) - 2)[:-500] + 1\n+\n+            indx[-1] = 0\n+            indx[-2] = len(arr) - 1\n+            indx_put = indx.copy()\n+            indx_put[-1] = -1389\n+            indx_put[-2] = 1321\n+        else:\n+            # Avoid duplicates (for simplicity) and fill half only\n+            indx = np.random.permutation(len(arr) - 3)[:-500]\n+            indx_put = indx\n+            if mode == \"wrap\":\n+                indx_put = indx_put + len(arr)\n+\n+        np.put(arr, indx_put, vals, mode=mode)\n+        assert_array_equal(arr[indx], vals[:len(indx)])\n+        untouched = np.ones(len(arr), dtype=bool)\n+        untouched[indx] = False\n+        assert_array_equal(arr[untouched], zeros[:untouched.sum()])\n",
            "comment_added_diff": {
                "97": "        # putmask is weird and doesn't care about value length (even shorter)",
                "101": "        # Use vals.dtype in case of flexible dtype (i.e. string)",
                "117": "        # put is weird and doesn't care about value length (even shorter)",
                "120": "        # Use vals.dtype in case of flexible dtype (i.e. string)",
                "125": "            # Special because 0 and -1 value are \"reserved\" for clip test",
                "134": "            # Avoid duplicates (for simplicity) and fill half only"
            },
            "comment_deleted_diff": {},
            "comment_modified_diff": {}
        },
        {
            "commit": "ef336efec4c31da909f3091dce6f998dbe6e3861",
            "timestamp": "2023-02-22T17:20:16+01:00",
            "author": "Sebastian Berg",
            "commit_message": "TST: Test (weird) empty value put/putmask\n\nMainly to cover early return path which previously evaded initialization\nand thus crashed during cleanup",
            "additions": 21,
            "deletions": 0,
            "change_type": "MODIFY",
            "diff": "@@ -106,6 +106,17 @@ def test_simple(self, dtype):\n         assert_array_equal(arr[mask], vals[:len(mask)][mask])\n         assert_array_equal(arr[~mask], zeros[~mask])\n \n+    @pytest.mark.parametrize(\"dtype\", list(np.typecodes[\"All\"])[1:] + [\"i,O\"])\n+    @pytest.mark.parametrize(\"mode\", [\"raise\", \"wrap\", \"clip\"])\n+    def test_empty(self, dtype, mode):\n+        arr = np.zeros(1000, dtype=dtype)\n+        arr_copy = arr.copy()\n+        mask = np.random.randint(2, size=1000).astype(bool)\n+\n+        # Allowing empty values like this is weird...\n+        np.put(arr, mask, [])\n+        assert_array_equal(arr, arr_copy)\n+\n \n class TestPut:\n     @pytest.mark.parametrize(\"dtype\", list(np.typecodes[\"All\"])[1:] + [\"i,O\"])\n@@ -142,3 +153,13 @@ def test_simple(self, dtype, mode):\n         untouched = np.ones(len(arr), dtype=bool)\n         untouched[indx] = False\n         assert_array_equal(arr[untouched], zeros[:untouched.sum()])\n+\n+    @pytest.mark.parametrize(\"dtype\", list(np.typecodes[\"All\"])[1:] + [\"i,O\"])\n+    @pytest.mark.parametrize(\"mode\", [\"raise\", \"wrap\", \"clip\"])\n+    def test_empty(self, dtype, mode):\n+        arr = np.zeros(1000, dtype=dtype)\n+        arr_copy = arr.copy()\n+\n+        # Allowing empty values like this is weird...\n+        np.put(arr, [1, 2, 3], [])\n+        assert_array_equal(arr, arr_copy)\n",
            "comment_added_diff": {
                "116": "        # Allowing empty values like this is weird...",
                "163": "        # Allowing empty values like this is weird..."
            },
            "comment_deleted_diff": {},
            "comment_modified_diff": {}
        }
    ],
    "_asarray.pyi": [],
    "_type_aliases.pyi": [],
    "ctypeslib.pyi": [],
    "index_tricks.pyi": [],
    "nep-0013-ufunc-overrides.rst": [],
    "23240.compatibility.rst": [],
    "arrays.classes.rst": [],
    "override.c": [],
    "override.h": [],
    "_indexing_functions.py": [],
    "test_indexing_functions.py": [],
    "23302.deprecation.rst": [],
    "routines.math.rst": [],
    "22982.new_feature.rst": [],
    "npy_cpu.h": [],
    "fujitsuccompiler.py": [],
    "site.cfg.example": [],
    "23314.deprecations.rst": [],
    "test_type_check.py": [
        {
            "commit": "73d32d102e00a05990f23a44da09a79237013377",
            "timestamp": "2023-08-24T22:52:58+02:00",
            "author": "Mateusz Sok\u00f3\u0142",
            "commit_message": "API: Remove asfarray function",
            "additions": 1,
            "deletions": 14,
            "change_type": "MODIFY",
            "diff": "@@ -4,7 +4,7 @@\n     )\n from numpy.lib.type_check import (\n     common_type, mintypecode, isreal, iscomplex, isposinf, isneginf,\n-    nan_to_num, isrealobj, iscomplexobj, asfarray, real_if_close\n+    nan_to_num, isrealobj, iscomplexobj, real_if_close\n     )\n \n \n@@ -463,16 +463,3 @@ def test_basic(self):\n         assert_all(iscomplexobj(b))\n         b = real_if_close(a+1e-7j, tol=1e-6)\n         assert_all(isrealobj(b))\n-\n-\n-class TestArrayConversion:\n-\n-    def test_asfarray(self):\n-        a = asfarray(np.array([1, 2, 3]))\n-        assert_equal(a.__class__, np.ndarray)\n-        assert_(np.issubdtype(a.dtype, np.floating))\n-\n-        # previously this would infer dtypes from arrays, unlike every single\n-        # other numpy function\n-        assert_raises(TypeError,\n-            asfarray, np.array([1, 2, 3]), dtype=np.array(1.0))\n",
            "comment_added_diff": {},
            "comment_deleted_diff": {
                "475": "        # previously this would infer dtypes from arrays, unlike every single",
                "476": "        # other numpy function"
            },
            "comment_modified_diff": {}
        }
    ],
    "testutils.py": [],
    "bench_reduce.py": [
        {
            "commit": "77bcc98ff223d5f10f5c17dd0ff6fabd27706194",
            "timestamp": "2023-03-04T15:42:55+00:00",
            "author": "Rohit Goswami",
            "commit_message": "BENCH: Rework numpy stats reductions",
            "additions": 25,
            "deletions": 6,
            "change_type": "MODIFY",
            "diff": "@@ -45,19 +45,38 @@ def time_any_slow(self):\n         self.zeros.any()\n \n \n-class MinMax(Benchmark):\n-    params = [np.int8, np.uint8, np.int16, np.uint16, np.int32, np.uint32,\n-              np.int64, np.uint64, np.float32, np.float64, np.intp]\n+class StdStatsReductions(Benchmark):\n+    params = ['int8', 'uint8', 'int16', 'uint16', 'int32', 'uint32',\n+              'int64', 'uint64', 'float32', 'float64', 'intp',\n+              'complex64', 'complex128', 'complex256',\n+              'bool', 'float', 'int', 'complex']\n     param_names = ['dtype']\n \n     def setup(self, dtype):\n-        self.d = np.ones(20000, dtype=dtype)\n+        try:\n+            self.data = np.ones(20000, dtype=getattr(np, dtype))\n+        except AttributeError: # builtins throw AttributeError after 1.20\n+            self.data = np.ones(20000, dtype=dtype)\n+        if dtype.startswith('complex'):\n+            self.data = self.data * self.data.T*1j\n \n     def time_min(self, dtype):\n-        np.min(self.d)\n+        np.min(self.data)\n \n     def time_max(self, dtype):\n-        np.max(self.d)\n+        np.max(self.data)\n+\n+    def time_mean(self, dtype):\n+        np.mean(self.data)\n+\n+    def time_std(self, dtype):\n+        np.std(self.data)\n+\n+    def time_prod(self, dtype):\n+        np.prod(self.data)\n+\n+    def time_var(self, dtype):\n+        np.var(self.data)\n \n class FMinMax(Benchmark):\n     params = [np.float32, np.float64]\n",
            "comment_added_diff": {
                "58": "        except AttributeError: # builtins throw AttributeError after 1.20"
            },
            "comment_deleted_diff": {},
            "comment_modified_diff": {}
        },
        {
            "commit": "41c7fba65be6060ee1b7023ccc0634f8e935970a",
            "timestamp": "2023-03-04T15:42:56+00:00",
            "author": "Rohit Goswami",
            "commit_message": "MAINT: Fixup BENCH to appease linter",
            "additions": 1,
            "deletions": 1,
            "change_type": "MODIFY",
            "diff": "@@ -55,7 +55,7 @@ class StdStatsReductions(Benchmark):\n     def setup(self, dtype):\n         try:\n             self.data = np.ones(20000, dtype=getattr(np, dtype))\n-        except AttributeError: # builtins throw AttributeError after 1.20\n+        except AttributeError:  # builtins throw AttributeError after 1.20\n             self.data = np.ones(20000, dtype=dtype)\n         if dtype.startswith('complex'):\n             self.data = self.data * self.data.T*1j\n",
            "comment_added_diff": {
                "58": "        except AttributeError:  # builtins throw AttributeError after 1.20"
            },
            "comment_deleted_diff": {
                "58": "        except AttributeError: # builtins throw AttributeError after 1.20"
            },
            "comment_modified_diff": {
                "58": "        except AttributeError: # builtins throw AttributeError after 1.20"
            }
        },
        {
            "commit": "56d2fd5d8735e73f41ab30f42d139309df7fd758",
            "timestamp": "2023-03-04T16:02:03+00:00",
            "author": "Rohit Goswami",
            "commit_message": "BENCH: Reduce shapes and types for speed",
            "additions": 7,
            "deletions": 6,
            "change_type": "MODIFY",
            "diff": "@@ -46,17 +46,18 @@ def time_any_slow(self):\n \n \n class StatsReductions(Benchmark):\n-    params = ['int8', 'uint8', 'int16', 'uint16', 'int32', 'uint32',\n-              'int64', 'uint64', 'float32', 'float64', 'intp',\n-              'complex64', 'complex128', 'complex256',\n-              'bool', 'float', 'int', 'complex']\n+    # Not testing, but in array_api (redundant)\n+    # 8, 16, 32 bit variants, and 128 complexes\n+    params = ['int64', 'uint64', 'float64', 'intp',\n+               'complex64', 'bool', 'float', 'int',\n+               'complex', 'complex256'],\n     param_names = ['dtype']\n \n     def setup(self, dtype):\n         try:\n-            self.data = np.ones(20000, dtype=getattr(np, dtype))\n+            self.data = np.ones(200, dtype=getattr(np, dtype))\n         except AttributeError:  # builtins throw AttributeError after 1.20\n-            self.data = np.ones(20000, dtype=dtype)\n+            self.data = np.ones(200, dtype=dtype)\n         if dtype.startswith('complex'):\n             self.data = self.data * self.data.T*1j\n \n",
            "comment_added_diff": {
                "49": "    # Not testing, but in array_api (redundant)",
                "50": "    # 8, 16, 32 bit variants, and 128 complexes"
            },
            "comment_deleted_diff": {},
            "comment_modified_diff": {
                "49": "    params = ['int8', 'uint8', 'int16', 'uint16', 'int32', 'uint32',",
                "50": "              'int64', 'uint64', 'float32', 'float64', 'intp',"
            }
        },
        {
            "commit": "1cb9eeef3e17ce3f7ce686c4ff07d626feb0e427",
            "timestamp": "2023-08-27T23:01:14+02:00",
            "author": "Ralf Gommers",
            "commit_message": "BENCH: fix `bench_reduce.py` and `bench_shape_base.py`",
            "additions": 2,
            "deletions": 9,
            "change_type": "MODIFY",
            "diff": "@@ -46,18 +46,11 @@ def time_any_slow(self):\n \n \n class StatsReductions(Benchmark):\n-    # Not testing, but in array_api (redundant)\n-    # 8, 16, 32 bit variants, and 128 complexes\n-    params = ['int64', 'uint64', 'float64', 'intp',\n-               'complex64', 'bool', 'float', 'int',\n-               'complex', 'complex256'],\n+    params = ['int64', 'uint64', 'float32', 'float64', 'complex64', 'bool_'],\n     param_names = ['dtype']\n \n     def setup(self, dtype):\n-        try:\n-            self.data = np.ones(200, dtype=getattr(np, dtype))\n-        except AttributeError:  # builtins throw AttributeError after 1.20\n-            self.data = np.ones(200, dtype=dtype)\n+        self.data = np.ones(200, dtype=dtype)\n         if dtype.startswith('complex'):\n             self.data = self.data * self.data.T*1j\n \n",
            "comment_added_diff": {},
            "comment_deleted_diff": {
                "49": "    # Not testing, but in array_api (redundant)",
                "50": "    # 8, 16, 32 bit variants, and 128 complexes",
                "59": "        except AttributeError:  # builtins throw AttributeError after 1.20"
            },
            "comment_modified_diff": {}
        }
    ],
    "bench_lib.py": [],
    "bench_linalg.py": [
        {
            "commit": "56d2fd5d8735e73f41ab30f42d139309df7fd758",
            "timestamp": "2023-03-04T16:02:03+00:00",
            "author": "Rohit Goswami",
            "commit_message": "BENCH: Reduce shapes and types for speed",
            "additions": 4,
            "deletions": 3,
            "change_type": "MODIFY",
            "diff": "@@ -193,10 +193,11 @@ def time_einsum_noncon_contig_outstride0(self, dtype):\n \n \n class LinAlgTransposeVdot(Benchmark):\n+    # Smaller for speed\n+    # , (128, 128), (256, 256), (512, 512),\n+    # (1024, 1024)\n     params = [[(16, 16), (32, 32),\n-               (64, 64), (128, 128),\n-               (256, 256), (512, 512),\n-               (1024, 1024)], TYPES1]\n+               (64, 64)], TYPES1]\n     param_names = ['shape', 'npdtypes']\n \n     def setup(self, shape, npdtypes):\n",
            "comment_added_diff": {
                "196": "    # Smaller for speed",
                "197": "    # , (128, 128), (256, 256), (512, 512),",
                "198": "    # (1024, 1024)"
            },
            "comment_deleted_diff": {},
            "comment_modified_diff": {
                "197": "               (64, 64), (128, 128),",
                "198": "               (256, 256), (512, 512),"
            }
        },
        {
            "commit": "639b8a61d2bfc2feb45b54f86e7d6d40a2c8dea0",
            "timestamp": "2023-08-27T23:01:10+02:00",
            "author": "Ralf Gommers",
            "commit_message": "BENCH: fix linalg and itemselection benchmarks",
            "additions": 21,
            "deletions": 19,
            "change_type": "MODIFY",
            "diff": "@@ -72,37 +72,39 @@ def time_tensordot_a_b_axes_1_0_0_1(self):\n \n \n class Linalg(Benchmark):\n-    params = [['svd', 'pinv', 'det', 'norm'],\n-              TYPES1]\n-    param_names = ['op', 'type']\n+    params = set(TYPES1) - set(['float16'])\n+    param_names = ['dtype']\n \n-    def setup(self, op, typename):\n+    def setup(self, typename):\n         np.seterr(all='ignore')\n+        self.a = get_squares_()[typename]\n+\n+    def time_svd(self, typename):\n+        np.linalg.svd(self.a)\n+\n+    def time_pinv(self, typename):\n+        np.linalg.pinv(self.a)\n \n-        self.func = getattr(np.linalg, op)\n+    def time_det(self, typename):\n+        np.linalg.det(self.a)\n \n-        if op == 'cholesky':\n-            # we need a positive definite\n-            self.a = np.dot(get_squares_()[typename],\n-                            get_squares_()[typename].T)\n-        else:\n-            self.a = get_squares_()[typename]\n \n-        # check that dtype is supported at all\n-        try:\n-            self.func(self.a[:2, :2])\n-        except TypeError as e:\n-            raise NotImplementedError() from e\n+class LinalgNorm(Benchmark):\n+    params = TYPES1\n+    param_names = ['dtype']\n+\n+    def setup(self, typename):\n+        self.a = get_squares_()[typename]\n \n-    def time_op(self, op, typename):\n-        self.func(self.a)\n+    def time_norm(self, typename):\n+        np.linalg.norm(self.a)\n \n \n class LinalgSmallArrays(Benchmark):\n     \"\"\" Test overhead of linalg methods for small arrays \"\"\"\n     def setup(self):\n         self.array_5 = np.arange(5.)\n-        self.array_5_5 = np.reshape(np.arange(25.), (5., 5.))\n+        self.array_5_5 = np.reshape(np.arange(25.), (5, 5))\n \n     def time_norm_small_array(self):\n         np.linalg.norm(self.array_5)\n",
            "comment_added_diff": {},
            "comment_deleted_diff": {
                "85": "            # we need a positive definite",
                "91": "        # check that dtype is supported at all"
            },
            "comment_modified_diff": {}
        }
    ],
    "bench_manipulate.py": [
        {
            "commit": "56d2fd5d8735e73f41ab30f42d139309df7fd758",
            "timestamp": "2023-03-04T16:02:03+00:00",
            "author": "Rohit Goswami",
            "commit_message": "BENCH: Reduce shapes and types for speed",
            "additions": 2,
            "deletions": 3,
            "change_type": "MODIFY",
            "diff": "@@ -39,9 +39,8 @@ def time_broadcast_to(self, size, ndtype):\n \n \n class ConcatenateStackArrays(Benchmark):\n-    params = [[(16, 32), (32, 64),\n-               (64, 128), (128, 256),\n-               (256, 512)],\n+    # (64, 128), (128, 256), (256, 512)\n+    params = [[(16, 32), (32, 64)],\n               [2, 3, 4, 5],\n               TYPES1]\n     param_names = ['shape', 'narrays', 'ndtype']\n",
            "comment_added_diff": {
                "42": "    # (64, 128), (128, 256), (256, 512)"
            },
            "comment_deleted_diff": {},
            "comment_modified_diff": {
                "42": "    params = [[(16, 32), (32, 64),"
            }
        },
        {
            "commit": "f3f45cb9eb3c6f49dff54be08fefc5bac827499b",
            "timestamp": "2023-08-27T23:01:14+02:00",
            "author": "Ralf Gommers",
            "commit_message": "BENCH: reduce parametrization in `bench_manipulate.py`",
            "additions": 3,
            "deletions": 6,
            "change_type": "MODIFY",
            "diff": "@@ -4,9 +4,7 @@\n from collections import deque\n \n class BroadcastArrays(Benchmark):\n-    params = [[(16, 32), (32, 64),\n-               (64, 128), (128, 256),\n-               (256, 512), (512, 1024)],\n+    params = [[(16, 32), (128, 256), (512, 1024)],\n               TYPES1]\n     param_names = ['shape', 'ndtype']\n     timeout = 10\n@@ -22,7 +20,7 @@ def time_broadcast_arrays(self, shape, ndtype):\n \n \n class BroadcastArraysTo(Benchmark):\n-    params = [[16, 32, 64, 128, 256, 512],\n+    params = [[16, 64, 512],\n               TYPES1]\n     param_names = ['size', 'ndtype']\n     timeout = 10\n@@ -39,9 +37,8 @@ def time_broadcast_to(self, size, ndtype):\n \n \n class ConcatenateStackArrays(Benchmark):\n-    # (64, 128), (128, 256), (256, 512)\n     params = [[(16, 32), (32, 64)],\n-              [2, 3, 4, 5],\n+              [2, 5],\n               TYPES1]\n     param_names = ['shape', 'narrays', 'ndtype']\n     timeout = 10\n",
            "comment_added_diff": {},
            "comment_deleted_diff": {
                "42": "    # (64, 128), (128, 256), (256, 512)"
            },
            "comment_modified_diff": {}
        },
        {
            "commit": "a4120979d216cce00dcee511aad70bf7b45ef6e0",
            "timestamp": "2023-09-26T11:39:30+02:00",
            "author": "Brandon Smith",
            "commit_message": "BUG: Add size check for threaded array assignment (#24257)\n\nMany small array assignments releasing the GIL can cause slowdowns, and in multithreaded environments, can cause significant thread contention.\r\n\r\nAdd a size check to array assignment such that only assignments that exceed a certain number of elements (512) will release the GIL.\r\n\r\nCloses gh-24252.",
            "additions": 5,
            "deletions": 0,
            "change_type": "MODIFY",
            "diff": "@@ -63,6 +63,11 @@ def time_stack_ax1(self, size, narrays, ndtype):\n         np.stack(self.xarg, axis=1)\n \n \n+class ConcatenateNestedArrays(ConcatenateStackArrays):\n+    # Large number of small arrays to test GIL (non-)release\n+    params = [[(1, 1)], [1000, 100000], TYPES1]\n+\n+\n class DimsManipulations(Benchmark):\n     params = [\n         [(2, 1, 4), (2, 1), (5, 2, 3, 1)],\n",
            "comment_added_diff": {
                "67": "    # Large number of small arrays to test GIL (non-)release"
            },
            "comment_deleted_diff": {},
            "comment_modified_diff": {}
        }
    ],
    "23275.improvement.rst": [],
    "index.rst": [],
    "test_ufunclike.py": [
        {
            "commit": "ac6233b03df6562453ebda984f565f603e726710",
            "timestamp": "2023-03-12T22:10:11+00:00",
            "author": "Ralf Gommers",
            "commit_message": "DEP: expire deprecation for \"y as out\" in fix/isposinf/isneginf",
            "additions": 0,
            "deletions": 6,
            "change_type": "MODIFY",
            "diff": "@@ -80,12 +80,6 @@ def __array_finalize__(self, obj):\n         assert_(isinstance(f0d, MyArray))\n         assert_equal(f0d.metadata, 'bar')\n \n-    def test_deprecated(self):\n-        # NumPy 1.13.0, 2017-04-26\n-        assert_warns(DeprecationWarning, ufl.fix, [1, 2], y=nx.empty(2))\n-        assert_warns(DeprecationWarning, ufl.isposinf, [1, 2], y=nx.empty(2))\n-        assert_warns(DeprecationWarning, ufl.isneginf, [1, 2], y=nx.empty(2))\n-\n     def test_scalar(self):\n         x = np.inf\n         actual = np.isposinf(x)\n",
            "comment_added_diff": {},
            "comment_deleted_diff": {
                "84": "        # NumPy 1.13.0, 2017-04-26"
            },
            "comment_modified_diff": {}
        }
    ],
    "ufunclike.py": [
        {
            "commit": "ac6233b03df6562453ebda984f565f603e726710",
            "timestamp": "2023-03-12T22:10:11+00:00",
            "author": "Ralf Gommers",
            "commit_message": "DEP: expire deprecation for \"y as out\" in fix/isposinf/isneginf",
            "additions": 1,
            "deletions": 59,
            "change_type": "MODIFY",
            "diff": "@@ -6,72 +6,16 @@\n __all__ = ['fix', 'isneginf', 'isposinf']\n \n import numpy.core.numeric as nx\n-from numpy.core.overrides import (\n-    array_function_dispatch, ARRAY_FUNCTION_ENABLED,\n-)\n+from numpy.core.overrides import array_function_dispatch\n import warnings\n import functools\n \n \n-def _deprecate_out_named_y(f):\n-    \"\"\"\n-    Allow the out argument to be passed as the name `y` (deprecated)\n-\n-    In future, this decorator should be removed.\n-    \"\"\"\n-    @functools.wraps(f)\n-    def func(x, out=None, **kwargs):\n-        if 'y' in kwargs:\n-            if 'out' in kwargs:\n-                raise TypeError(\n-                    \"{} got multiple values for argument 'out'/'y'\"\n-                    .format(f.__name__)\n-                )\n-            out = kwargs.pop('y')\n-            # NumPy 1.13.0, 2017-04-26\n-            warnings.warn(\n-                \"The name of the out argument to {} has changed from `y` to \"\n-                \"`out`, to match other ufuncs.\".format(f.__name__),\n-                DeprecationWarning, stacklevel=3)\n-        return f(x, out=out, **kwargs)\n-\n-    return func\n-\n-\n-def _fix_out_named_y(f):\n-    \"\"\"\n-    Allow the out argument to be passed as the name `y` (deprecated)\n-\n-    This decorator should only be used if _deprecate_out_named_y is used on\n-    a corresponding dispatcher function.\n-    \"\"\"\n-    @functools.wraps(f)\n-    def func(x, out=None, **kwargs):\n-        if 'y' in kwargs:\n-            # we already did error checking in _deprecate_out_named_y\n-            out = kwargs.pop('y')\n-        return f(x, out=out, **kwargs)\n-\n-    return func\n-\n-\n-def _fix_and_maybe_deprecate_out_named_y(f):\n-    \"\"\"\n-    Use the appropriate decorator, depending upon if dispatching is being used.\n-    \"\"\"\n-    if ARRAY_FUNCTION_ENABLED:\n-        return _fix_out_named_y(f)\n-    else:\n-        return _deprecate_out_named_y(f)\n-\n-\n-@_deprecate_out_named_y\n def _dispatcher(x, out=None):\n     return (x, out)\n \n \n @array_function_dispatch(_dispatcher, verify=False, module='numpy')\n-@_fix_and_maybe_deprecate_out_named_y\n def fix(x, out=None):\n     \"\"\"\n     Round to nearest integer towards zero.\n@@ -125,7 +69,6 @@ def fix(x, out=None):\n \n \n @array_function_dispatch(_dispatcher, verify=False, module='numpy')\n-@_fix_and_maybe_deprecate_out_named_y\n def isposinf(x, out=None):\n     \"\"\"\n     Test element-wise for positive infinity, return result as bool array.\n@@ -197,7 +140,6 @@ def isposinf(x, out=None):\n \n \n @array_function_dispatch(_dispatcher, verify=False, module='numpy')\n-@_fix_and_maybe_deprecate_out_named_y\n def isneginf(x, out=None):\n     \"\"\"\n     Test element-wise for negative infinity, return result as bool array.\n",
            "comment_added_diff": {},
            "comment_deleted_diff": {
                "31": "            # NumPy 1.13.0, 2017-04-26",
                "51": "            # we already did error checking in _deprecate_out_named_y"
            },
            "comment_modified_diff": {}
        }
    ],
    "test_twodim_base.py": [],
    "INSTALL.rst": [],
    "building.rst": [],
    "_typing.py": [],
    "test_runtime.py": [],
    "_methods.py": [
        {
            "commit": "92e489eb3fea7ea4054e319b2d698e435e919704",
            "timestamp": "2023-03-16T22:15:10+11:00",
            "author": "Matti Picus",
            "commit_message": "DEP: remove deprecated casting in np.clip\n\nSigned-off-by: Matti Picus <matti.picus@gmail.com>",
            "additions": 4,
            "deletions": 67,
            "change_type": "MODIFY",
            "diff": "@@ -87,79 +87,16 @@ def _count_reduce_items(arr, axis, keepdims=False, where=True):\n                         keepdims)\n     return items\n \n-# Numpy 1.17.0, 2019-02-24\n-# Various clip behavior deprecations, marked with _clip_dep as a prefix.\n-\n-def _clip_dep_is_scalar_nan(a):\n-    # guarded to protect circular imports\n-    from numpy.core.fromnumeric import ndim\n-    if ndim(a) != 0:\n-        return False\n-    try:\n-        return um.isnan(a)\n-    except TypeError:\n-        return False\n-\n-def _clip_dep_is_byte_swapped(a):\n-    if isinstance(a, mu.ndarray):\n-        return not a.dtype.isnative\n-    return False\n-\n-def _clip_dep_invoke_with_casting(ufunc, *args, out=None, casting=None, **kwargs):\n-    # normal path\n-    if casting is not None:\n-        return ufunc(*args, out=out, casting=casting, **kwargs)\n-\n-    # try to deal with broken casting rules\n-    try:\n-        return ufunc(*args, out=out, **kwargs)\n-    except _exceptions._UFuncOutputCastingError as e:\n-        # Numpy 1.17.0, 2019-02-24\n-        warnings.warn(\n-            \"Converting the output of clip from {!r} to {!r} is deprecated. \"\n-            \"Pass `casting=\\\"unsafe\\\"` explicitly to silence this warning, or \"\n-            \"correct the type of the variables.\".format(e.from_, e.to),\n-            DeprecationWarning,\n-            stacklevel=2\n-        )\n-        return ufunc(*args, out=out, casting=\"unsafe\", **kwargs)\n-\n-def _clip(a, min=None, max=None, out=None, *, casting=None, **kwargs):\n+def _clip(a, min=None, max=None, out=None, **kwargs):\n     if min is None and max is None:\n         raise ValueError(\"One of max or min must be given\")\n \n-    # Numpy 1.17.0, 2019-02-24\n-    # This deprecation probably incurs a substantial slowdown for small arrays,\n-    # it will be good to get rid of it.\n-    if not _clip_dep_is_byte_swapped(a) and not _clip_dep_is_byte_swapped(out):\n-        using_deprecated_nan = False\n-        if _clip_dep_is_scalar_nan(min):\n-            min = -float('inf')\n-            using_deprecated_nan = True\n-        if _clip_dep_is_scalar_nan(max):\n-            max = float('inf')\n-            using_deprecated_nan = True\n-        if using_deprecated_nan:\n-            warnings.warn(\n-                \"Passing `np.nan` to mean no clipping in np.clip has always \"\n-                \"been unreliable, and is now deprecated. \"\n-                \"In future, this will always return nan, like it already does \"\n-                \"when min or max are arrays that contain nan. \"\n-                \"To skip a bound, pass either None or an np.inf of an \"\n-                \"appropriate sign.\",\n-                DeprecationWarning,\n-                stacklevel=2\n-            )\n-\n     if min is None:\n-        return _clip_dep_invoke_with_casting(\n-            um.minimum, a, max, out=out, casting=casting, **kwargs)\n+        return um.minimum(a, max, out=out, **kwargs)\n     elif max is None:\n-        return _clip_dep_invoke_with_casting(\n-            um.maximum, a, min, out=out, casting=casting, **kwargs)\n+        return um.maximum(a, min, out=out, **kwargs)\n     else:\n-        return _clip_dep_invoke_with_casting(\n-            um.clip, a, min, max, out=out, casting=casting, **kwargs)\n+        return um.clip(a, min, max, out=out, **kwargs)\n \n def _mean(a, axis=None, dtype=None, out=None, keepdims=False, *, where=True):\n     arr = asanyarray(a)\n",
            "comment_added_diff": {},
            "comment_deleted_diff": {
                "90": "# Numpy 1.17.0, 2019-02-24",
                "91": "# Various clip behavior deprecations, marked with _clip_dep as a prefix.",
                "94": "    # guarded to protect circular imports",
                "109": "    # normal path",
                "113": "    # try to deal with broken casting rules",
                "117": "        # Numpy 1.17.0, 2019-02-24",
                "131": "    # Numpy 1.17.0, 2019-02-24",
                "132": "    # This deprecation probably incurs a substantial slowdown for small arrays,",
                "133": "    # it will be good to get rid of it."
            },
            "comment_modified_diff": {}
        },
        {
            "commit": "ab2178b47c0ee834180c318db196976623710691",
            "timestamp": "2023-07-07T08:57:37+03:00",
            "author": "Ronald van Elburg",
            "commit_message": "ENH: add mean keyword to std and var (#24126)\n\n* Add mean keyword to std and var functions.\r\n\r\n* Add releae note for mean keyword to std and var functions.\r\n\r\n* Update release note with PR number\r\n\r\n* Address lint issue.\r\n\r\n* Align nan signatures with new signatures.\r\n\r\n* Address lint issue.\r\n\r\n* Correct version numbers on keywords.\r\n\r\n* Put backticks on keyword argument in documentation string.\r\n\r\n* Cleanuup assert statements in tests\r\n\r\n* Move comparison of in and out arrays closer to the function call.\r\n\r\n* Remove clutter from example code in release note.\r\n\r\n* Add test for nanstd and fix error in nanvar\r\n\r\n* haqndle \"mean\" keyword for var and std on MaskedArrays.\r\n\r\n* Address lint issues.\r\n\r\n* update the dispatchers according to suggestions by Marten van Kerkwijk:\r\n\r\n(https://github.com/numpy/numpy/pull/24126#discussion_r1254314302) The dispatcher returns all arguments that can, in principle, contain something that is an array and hence that, if not a regular ndarray, can allow the function to be dealt with another package (say, dask). Since mean can be an array, it should be added (most similar to where).\r\n\r\n* Move and adjust example from release note to doc-strings. Reflow doc-string.\r\n\r\n* Improve doc-string. Shorter sentences and add type and label mean argument\r\n\r\n* Remove some of these pesky trailing white spaces\r\n\r\n* Make extra white lines more consistent.\r\n\r\n* Make sure code examples execute without Jupyter magic.\r\n\r\n* Fold lines to pass linter.\r\n\r\n* Update doc-string nanstd and nanvar.\r\n\r\n* Try to satisfy linter and apple requirements at the same time. Making the example code ugly, alas!\r\n\r\n* Make doctest skip resource dependent output",
            "additions": 25,
            "deletions": 22,
            "change_type": "MODIFY",
            "diff": "@@ -134,7 +134,7 @@ def _mean(a, axis=None, dtype=None, out=None, keepdims=False, *, where=True):\n     return ret\n \n def _var(a, axis=None, dtype=None, out=None, ddof=0, keepdims=False, *,\n-         where=True):\n+         where=True, mean=None):\n     arr = asanyarray(a)\n \n     rcount = _count_reduce_items(arr, axis, keepdims=keepdims, where=where)\n@@ -147,26 +147,29 @@ def _var(a, axis=None, dtype=None, out=None, ddof=0, keepdims=False, *,\n     if dtype is None and issubclass(arr.dtype.type, (nt.integer, nt.bool_)):\n         dtype = mu.dtype('f8')\n \n-    # Compute the mean.\n-    # Note that if dtype is not of inexact type then arraymean will\n-    # not be either.\n-    arrmean = umr_sum(arr, axis, dtype, keepdims=True, where=where)\n-    # The shape of rcount has to match arrmean to not change the shape of out\n-    # in broadcasting. Otherwise, it cannot be stored back to arrmean.\n-    if rcount.ndim == 0:\n-        # fast-path for default case when where is True\n-        div = rcount\n+    if mean is not None:\n+        arrmean = mean\n     else:\n-        # matching rcount to arrmean when where is specified as array\n-        div = rcount.reshape(arrmean.shape)\n-    if isinstance(arrmean, mu.ndarray):\n-        with _no_nep50_warning():\n-            arrmean = um.true_divide(arrmean, div, out=arrmean,\n-                                     casting='unsafe', subok=False)\n-    elif hasattr(arrmean, \"dtype\"):\n-        arrmean = arrmean.dtype.type(arrmean / rcount)\n-    else:\n-        arrmean = arrmean / rcount\n+        # Compute the mean.\n+        # Note that if dtype is not of inexact type then arraymean will\n+        # not be either.\n+        arrmean = umr_sum(arr, axis, dtype, keepdims=True, where=where)\n+        # The shape of rcount has to match arrmean to not change the shape of\n+        # out in broadcasting. Otherwise, it cannot be stored back to arrmean.\n+        if rcount.ndim == 0:\n+            # fast-path for default case when where is True\n+            div = rcount\n+        else:\n+            # matching rcount to arrmean when where is specified as array\n+            div = rcount.reshape(arrmean.shape)\n+        if isinstance(arrmean, mu.ndarray):\n+            with _no_nep50_warning():\n+                arrmean = um.true_divide(arrmean, div, out=arrmean,\n+                                         casting='unsafe', subok=False)\n+        elif hasattr(arrmean, \"dtype\"):\n+            arrmean = arrmean.dtype.type(arrmean / rcount)\n+        else:\n+            arrmean = arrmean / rcount\n \n     # Compute sum of squared deviations from mean\n     # Note that x may not be inexact and that we need it to be an array,\n@@ -203,9 +206,9 @@ def _var(a, axis=None, dtype=None, out=None, ddof=0, keepdims=False, *,\n     return ret\n \n def _std(a, axis=None, dtype=None, out=None, ddof=0, keepdims=False, *,\n-         where=True):\n+         where=True, mean=None):\n     ret = _var(a, axis=axis, dtype=dtype, out=out, ddof=ddof,\n-               keepdims=keepdims, where=where)\n+               keepdims=keepdims, where=where, mean=mean)\n \n     if isinstance(ret, mu.ndarray):\n         ret = um.sqrt(ret, out=ret)\n",
            "comment_added_diff": {
                "153": "        # Compute the mean.",
                "154": "        # Note that if dtype is not of inexact type then arraymean will",
                "155": "        # not be either.",
                "157": "        # The shape of rcount has to match arrmean to not change the shape of",
                "158": "        # out in broadcasting. Otherwise, it cannot be stored back to arrmean.",
                "160": "            # fast-path for default case when where is True",
                "163": "            # matching rcount to arrmean when where is specified as array"
            },
            "comment_deleted_diff": {
                "150": "    # Compute the mean.",
                "151": "    # Note that if dtype is not of inexact type then arraymean will",
                "152": "    # not be either.",
                "154": "    # The shape of rcount has to match arrmean to not change the shape of out",
                "155": "    # in broadcasting. Otherwise, it cannot be stored back to arrmean.",
                "157": "        # fast-path for default case when where is True",
                "160": "        # matching rcount to arrmean when where is specified as array"
            },
            "comment_modified_diff": {
                "153": "    arrmean = umr_sum(arr, axis, dtype, keepdims=True, where=where)",
                "154": "    # The shape of rcount has to match arrmean to not change the shape of out",
                "155": "    # in broadcasting. Otherwise, it cannot be stored back to arrmean.",
                "157": "        # fast-path for default case when where is True",
                "158": "        div = rcount",
                "160": "        # matching rcount to arrmean when where is specified as array",
                "163": "        with _no_nep50_warning():"
            }
        }
    ],
    "ucsnarrow.c": [],
    "ucsnarrow.h": [],
    "ctors.h": [],
    "descriptor.h": [],
    "scalarapi.c": [],
    "spin": [],
    "23229.compatibility.rst": [],
    "datetime_busday.c": [],
    "absoft.py": [
        {
            "commit": "1dd6ef0114baee9301b8bd2a91c3c04870c8345a",
            "timestamp": "2023-03-24T22:04:15+00:00",
            "author": "Christian Veenhuis",
            "commit_message": "MAINT Fix broken links in absoft.py",
            "additions": 2,
            "deletions": 2,
            "change_type": "MODIFY",
            "diff": "@@ -1,6 +1,6 @@\n \n-# http://www.absoft.com/literature/osxuserguide.pdf\n-# http://www.absoft.com/documentation.html\n+# Absoft Corporation ceased operations on 12/31/2022.\n+# Thus, all links to <http://www.absoft.com> are invalid.\n \n # Notes:\n # - when using -g77 then use -DUNDERSCORE_G77 to compile f2py\n",
            "comment_added_diff": {
                "2": "# Absoft Corporation ceased operations on 12/31/2022.",
                "3": "# Thus, all links to <http://www.absoft.com> are invalid."
            },
            "comment_deleted_diff": {
                "2": "# http://www.absoft.com/literature/osxuserguide.pdf",
                "3": "# http://www.absoft.com/documentation.html"
            },
            "comment_modified_diff": {
                "2": "# http://www.absoft.com/literature/osxuserguide.pdf",
                "3": "# http://www.absoft.com/documentation.html"
            }
        }
    ],
    "23061.new_feature.rst": [],
    "23480.expired.rst": [],
    "routines.dual.rst": [],
    "dual.py": [
        {
            "commit": "8bdb8c327696d2f8c834b66f411e7d4216dc612c",
            "timestamp": "2023-03-28T23:51:36+01:00",
            "author": "Ralf Gommers",
            "commit_message": "DEP: remove deprecated `numpy.dual` module\n\nIt has been deprecated since numpy 1.20, and code search engines\ndon't turn up a reason to keep this around.\n\n[skip actions] [skip cirrus]",
            "additions": 0,
            "deletions": 83,
            "change_type": "DELETE",
            "diff": "@@ -1,83 +0,0 @@\n-\"\"\"\n-.. deprecated:: 1.20\n-\n-*This module is deprecated.  Instead of importing functions from*\n-``numpy.dual``, *the functions should be imported directly from NumPy\n-or SciPy*.\n-\n-Aliases for functions which may be accelerated by SciPy.\n-\n-SciPy_ can be built to use accelerated or otherwise improved libraries\n-for FFTs, linear algebra, and special functions. This module allows\n-developers to transparently support these accelerated functions when\n-SciPy is available but still support users who have only installed\n-NumPy.\n-\n-.. _SciPy : https://www.scipy.org\n-\n-\"\"\"\n-import warnings\n-\n-\n-warnings.warn('The module numpy.dual is deprecated.  Instead of using dual, '\n-              'use the functions directly from numpy or scipy.',\n-              category=DeprecationWarning,\n-              stacklevel=2)\n-\n-# This module should be used for functions both in numpy and scipy if\n-#  you want to use the numpy version if available but the scipy version\n-#  otherwise.\n-#  Usage  --- from numpy.dual import fft, inv\n-\n-__all__ = ['fft', 'ifft', 'fftn', 'ifftn', 'fft2', 'ifft2',\n-           'norm', 'inv', 'svd', 'solve', 'det', 'eig', 'eigvals',\n-           'eigh', 'eigvalsh', 'lstsq', 'pinv', 'cholesky', 'i0']\n-\n-import numpy.linalg as linpkg\n-import numpy.fft as fftpkg\n-from numpy.lib import i0\n-import sys\n-\n-\n-fft = fftpkg.fft\n-ifft = fftpkg.ifft\n-fftn = fftpkg.fftn\n-ifftn = fftpkg.ifftn\n-fft2 = fftpkg.fft2\n-ifft2 = fftpkg.ifft2\n-\n-norm = linpkg.norm\n-inv = linpkg.inv\n-svd = linpkg.svd\n-solve = linpkg.solve\n-det = linpkg.det\n-eig = linpkg.eig\n-eigvals = linpkg.eigvals\n-eigh = linpkg.eigh\n-eigvalsh = linpkg.eigvalsh\n-lstsq = linpkg.lstsq\n-pinv = linpkg.pinv\n-cholesky = linpkg.cholesky\n-\n-_restore_dict = {}\n-\n-def register_func(name, func):\n-    if name not in __all__:\n-        raise ValueError(\"{} not a dual function.\".format(name))\n-    f = sys._getframe(0).f_globals\n-    _restore_dict[name] = f[name]\n-    f[name] = func\n-\n-def restore_func(name):\n-    if name not in __all__:\n-        raise ValueError(\"{} not a dual function.\".format(name))\n-    try:\n-        val = _restore_dict[name]\n-    except KeyError:\n-        return\n-    else:\n-        sys._getframe(0).f_globals[name] = val\n-\n-def restore_all():\n-    for name in _restore_dict.keys():\n-        restore_func(name)\n",
            "comment_added_diff": {},
            "comment_deleted_diff": {
                "27": "# This module should be used for functions both in numpy and scipy if",
                "28": "#  you want to use the numpy version if available but the scipy version",
                "29": "#  otherwise.",
                "30": "#  Usage  --- from numpy.dual import fft, inv"
            },
            "comment_modified_diff": {}
        }
    ],
    "einsum_sumprod.c.src": [],
    "test_einsum_object.py": [
        {
            "commit": "4168b17169d88187a1f1527b297d6906ed1a60a7",
            "timestamp": "2023-03-29T14:06:52+03:00",
            "author": "iamsoto",
            "commit_message": "WIP: Adding Object dtype to einsum",
            "additions": 309,
            "deletions": 0,
            "change_type": "ADD",
            "diff": "@@ -0,0 +1,309 @@\n+import itertools\n+\n+import numpy as np\n+from numpy.testing import (\n+    assert_, assert_equal, assert_array_equal, assert_almost_equal,\n+    assert_raises, suppress_warnings, assert_raises_regex, assert_allclose\n+    )\n+\n+\n+def test_einsum_sums(dtype='object', do_opt=False):\n+    \"\"\"\n+        Different from test_einsum.py since object dtype cannot perform\n+        np.sum(a, axis=-1).astype(dtype) when len(a) == 1\n+    \"\"\"\n+\n+    # sum(a, axis=-1)\n+    for n in range(1, 17):\n+        a = np.arange(n, dtype=dtype)\n+        assert_equal(np.einsum(\"i->\", a, optimize=do_opt),\n+                     np.sum(a, axis=-1))\n+        assert_equal(np.einsum(a, [0], [], optimize=do_opt),\n+                     np.sum(a, axis=-1))\n+\n+\n+    for n in range(1, 17):\n+        a = np.arange(2*3*n, dtype=dtype).reshape(2, 3, n)\n+        assert_equal(np.einsum(\"...i->...\", a, optimize=do_opt),\n+                     np.sum(a, axis=-1))\n+        assert_equal(np.einsum(a, [Ellipsis, 0], [Ellipsis], optimize=do_opt),\n+                     np.sum(a, axis=-1))\n+\n+        \n+    # sum(a, axis=0)\n+    for n in range(1, 17):\n+        a = np.arange(2*n, dtype=dtype).reshape(2, n)\n+        assert_equal(np.einsum(\"i...->...\", a, optimize=do_opt),\n+                     np.sum(a, axis=0))\n+        assert_equal(np.einsum(a, [0, Ellipsis], [Ellipsis], optimize=do_opt),\n+                     np.sum(a, axis=0))\n+\n+    for n in range(1, 17):\n+        a = np.arange(2*3*n, dtype=dtype).reshape(2, 3, n)\n+        assert_equal(np.einsum(\"i...->...\", a, optimize=do_opt),\n+                     np.sum(a, axis=0))\n+        assert_equal(np.einsum(a, [0, Ellipsis], [Ellipsis], optimize=do_opt),\n+                     np.sum(a, axis=0))\n+\n+    # trace(a)\n+    for n in range(1, 17):\n+        a = np.arange(n*n, dtype=dtype).reshape(n, n)\n+        assert_equal(np.einsum(\"ii\", a, optimize=do_opt),\n+                     np.trace(a))\n+        assert_equal(np.einsum(a, [0, 0], optimize=do_opt),\n+                     np.trace(a))\n+\n+        # gh-15961: should accept numpy int64 type in subscript list\n+        np_array = np.asarray([0, 0])\n+        assert_equal(np.einsum(a, np_array, optimize=do_opt),\n+                     np.trace(a))\n+        assert_equal(np.einsum(a, list(np_array), optimize=do_opt),\n+                     np.trace(a))\n+\n+    # multiply(a, b)\n+    assert_equal(np.einsum(\"..., ...\", 3, 4), 12)  # scalar case\n+    for n in range(1, 17):\n+        a = np.arange(3 * n, dtype=dtype).reshape(3, n)\n+        b = np.arange(2 * 3 * n, dtype=dtype).reshape(2, 3, n)\n+        assert_equal(np.einsum(\"..., ...\", a, b, optimize=do_opt),\n+                     np.multiply(a, b))\n+        assert_equal(np.einsum(a, [Ellipsis], b, [Ellipsis], optimize=do_opt),\n+                     np.multiply(a, b))\n+\n+    # inner(a,b)\n+    for n in range(1, 17):\n+        a = np.arange(2 * 3 * n, dtype=dtype).reshape(2, 3, n)\n+        b = np.arange(n, dtype=dtype)\n+        assert_equal(np.einsum(\"...i, ...i\", a, b, optimize=do_opt), np.inner(a, b))\n+        assert_equal(np.einsum(a, [Ellipsis, 0], b, [Ellipsis, 0], optimize=do_opt),\n+                     np.inner(a, b))\n+\n+    for n in range(1, 11):\n+        a = np.arange(n * 3 * 2, dtype=dtype).reshape(n, 3, 2)\n+        b = np.arange(n, dtype=dtype)\n+        assert_equal(np.einsum(\"i..., i...\", a, b, optimize=do_opt),\n+                     np.inner(a.T, b.T).T)\n+        assert_equal(np.einsum(a, [0, Ellipsis], b, [0, Ellipsis], optimize=do_opt),\n+                     np.inner(a.T, b.T).T)\n+\n+    # outer(a,b)\n+    for n in range(1, 17):\n+        a = np.arange(3, dtype=dtype)+1\n+        b = np.arange(n, dtype=dtype)+1\n+        assert_equal(np.einsum(\"i,j\", a, b, optimize=do_opt),\n+                     np.outer(a, b))\n+        assert_equal(np.einsum(a, [0], b, [1], optimize=do_opt),\n+                     np.outer(a, b))\n+\n+    # Suppress the complex warnings for the 'as f8' tests\n+    with suppress_warnings() as sup:\n+        sup.filter(np.ComplexWarning)\n+\n+        # matvec(a,b) / a.dot(b) where a is matrix, b is vector\n+        for n in range(1, 17):\n+            a = np.arange(4*n, dtype=dtype).reshape(4, n)\n+            b = np.arange(n, dtype=dtype)\n+            assert_equal(np.einsum(\"ij, j\", a, b, optimize=do_opt),\n+                         np.dot(a, b))\n+            assert_equal(np.einsum(a, [0, 1], b, [1], optimize=do_opt),\n+                         np.dot(a, b))\n+\n+            c = np.arange(4, dtype=dtype)\n+            np.einsum(\"ij,j\", a, b, out=c,\n+                      dtype='f8', casting='unsafe', optimize=do_opt)\n+            assert_equal(c,\n+                         np.dot(a.astype('f8'),\n+                                b.astype('f8')).astype(dtype))\n+            c[...] = 0\n+            np.einsum(a, [0, 1], b, [1], out=c,\n+                      dtype='f8', casting='unsafe', optimize=do_opt)\n+            assert_equal(c,\n+                         np.dot(a.astype('f8'),\n+                                b.astype('f8')).astype(dtype))\n+\n+        for n in range(1, 17):\n+            a = np.arange(4*n, dtype=dtype).reshape(4, n)\n+            b = np.arange(n, dtype=dtype)\n+            assert_equal(np.einsum(\"ji,j\", a.T, b.T, optimize=do_opt),\n+                         np.dot(b.T, a.T))\n+            assert_equal(np.einsum(a.T, [1, 0], b.T, [1], optimize=do_opt),\n+                         np.dot(b.T, a.T))\n+\n+            c = np.arange(4, dtype=dtype)\n+            np.einsum(\"ji,j\", a.T, b.T, out=c,\n+                      dtype='f8', casting='unsafe', optimize=do_opt)\n+            assert_equal(c,\n+                         np.dot(b.T.astype('f8'),\n+                                a.T.astype('f8')).astype(dtype))\n+            c[...] = 0\n+            np.einsum(a.T, [1, 0], b.T, [1], out=c,\n+                      dtype='f8', casting='unsafe', optimize=do_opt)\n+            assert_equal(c,\n+                         np.dot(b.T.astype('f8'),\n+                                a.T.astype('f8')).astype(dtype))\n+\n+        # matmat(a,b) / a.dot(b) where a is matrix, b is matrix\n+        for n in range(1, 17):\n+            if n < 8 or dtype != 'f2':\n+                a = np.arange(4*n, dtype=dtype).reshape(4, n)\n+                b = np.arange(n*6, dtype=dtype).reshape(n, 6)\n+                assert_equal(np.einsum(\"ij,jk\", a, b, optimize=do_opt),\n+                             np.dot(a, b))\n+                assert_equal(np.einsum(a, [0, 1], b, [1, 2], optimize=do_opt),\n+                             np.dot(a, b))\n+\n+        for n in range(1, 17):\n+            a = np.arange(4*n, dtype=dtype).reshape(4, n)\n+            b = np.arange(n*6, dtype=dtype).reshape(n, 6)\n+            c = np.arange(24, dtype=dtype).reshape(4, 6)\n+            np.einsum(\"ij,jk\", a, b, out=c, dtype='f8', casting='unsafe',\n+                      optimize=do_opt)\n+            assert_equal(c,\n+                         np.dot(a.astype('f8'),\n+                                b.astype('f8')).astype(dtype))\n+            c[...] = 0\n+            np.einsum(a, [0, 1], b, [1, 2], out=c,\n+                      dtype='f8', casting='unsafe', optimize=do_opt)\n+            assert_equal(c,\n+                         np.dot(a.astype('f8'),\n+                                b.astype('f8')).astype(dtype))\n+\n+        # matrix triple product (note this is not currently an efficient\n+        # way to multiply 3 matrices)\n+        a = np.arange(12, dtype=dtype).reshape(3, 4)\n+        b = np.arange(20, dtype=dtype).reshape(4, 5)\n+        c = np.arange(30, dtype=dtype).reshape(5, 6)\n+        if dtype != 'f2':\n+            assert_equal(np.einsum(\"ij,jk,kl\", a, b, c, optimize=do_opt),\n+                         a.dot(b).dot(c))\n+            assert_equal(np.einsum(a, [0, 1], b, [1, 2], c, [2, 3],\n+                                   optimize=do_opt), a.dot(b).dot(c))\n+\n+        d = np.arange(18, dtype=dtype).reshape(3, 6)\n+        np.einsum(\"ij,jk,kl\", a, b, c, out=d,\n+                  dtype='f8', casting='unsafe', optimize=do_opt)\n+        tgt = a.astype('f8').dot(b.astype('f8'))\n+        tgt = tgt.dot(c.astype('f8')).astype(dtype)\n+        assert_equal(d, tgt)\n+\n+        d[...] = 0\n+        np.einsum(a, [0, 1], b, [1, 2], c, [2, 3], out=d,\n+                  dtype='f8', casting='unsafe', optimize=do_opt)\n+        tgt = a.astype('f8').dot(b.astype('f8'))\n+        tgt = tgt.dot(c.astype('f8')).astype(dtype)\n+        assert_equal(d, tgt)\n+\n+        # tensordot(a, b)\n+        if np.dtype(dtype) != np.dtype('f2'):\n+            a = np.arange(60, dtype=dtype).reshape(3, 4, 5)\n+            b = np.arange(24, dtype=dtype).reshape(4, 3, 2)\n+            assert_equal(np.einsum(\"ijk, jil -> kl\", a, b),\n+                         np.tensordot(a, b, axes=([1, 0], [0, 1])))\n+            assert_equal(np.einsum(a, [0, 1, 2], b, [1, 0, 3], [2, 3]),\n+                         np.tensordot(a, b, axes=([1, 0], [0, 1])))\n+\n+            c = np.arange(10, dtype=dtype).reshape(5, 2)\n+            np.einsum(\"ijk,jil->kl\", a, b, out=c,\n+                      dtype='f8', casting='unsafe', optimize=do_opt)\n+            assert_equal(c, np.tensordot(a.astype('f8'), b.astype('f8'),\n+                         axes=([1, 0], [0, 1])).astype(dtype))\n+            c[...] = 0\n+            np.einsum(a, [0, 1, 2], b, [1, 0, 3], [2, 3], out=c,\n+                      dtype='f8', casting='unsafe', optimize=do_opt)\n+            assert_equal(c, np.tensordot(a.astype('f8'), b.astype('f8'),\n+                         axes=([1, 0], [0, 1])).astype(dtype))\n+\n+    # logical_and(logical_and(a!=0, b!=0), c!=0)\n+    a = np.array([1,   3,   -2,   0,   12,  13,   0,   1], dtype=dtype)\n+    b = np.array([0,   3.5, 0.,   -2,  0,   1,    3,   12], dtype=dtype)\n+    c = np.array([True, True, False, True, True, False, True, True])\n+    assert_equal(np.einsum(\"i,i,i->i\", a, b, c,\n+                 dtype='?', casting='unsafe', optimize=do_opt),\n+                 np.logical_and(np.logical_and(a != 0, b != 0), c != 0))\n+    assert_equal(np.einsum(a, [0], b, [0], c, [0], [0],\n+                 dtype='?', casting='unsafe'),\n+                 np.logical_and(np.logical_and(a != 0, b != 0), c != 0))\n+\n+    a = np.arange(9, dtype=dtype)\n+    assert_equal(np.einsum(\",i->\", 3, a), 3*np.sum(a))\n+    assert_equal(np.einsum(3, [], a, [0], []), 3*np.sum(a))\n+    assert_equal(np.einsum(\"i,->\", a, 3), 3*np.sum(a))\n+    assert_equal(np.einsum(a, [0], 3, [], []), 3*np.sum(a))\n+\n+    # Various stride0, contiguous, and SSE aligned variants\n+    for n in range(1, 25):\n+        a = np.arange(n, dtype=dtype)\n+        if np.dtype(dtype).itemsize > 1:\n+            assert_equal(np.einsum(\"...,...\", a, a, optimize=do_opt),\n+                         np.multiply(a, a))\n+            assert_equal(np.einsum(\"i,i\", a, a, optimize=do_opt), np.dot(a, a))\n+            assert_equal(np.einsum(\"i,->i\", a, 2, optimize=do_opt), 2*a)\n+            assert_equal(np.einsum(\",i->i\", 2, a, optimize=do_opt), 2*a)\n+            assert_equal(np.einsum(\"i,->\", a, 2, optimize=do_opt), 2*np.sum(a))\n+            assert_equal(np.einsum(\",i->\", 2, a, optimize=do_opt), 2*np.sum(a))\n+\n+            assert_equal(np.einsum(\"...,...\", a[1:], a[:-1], optimize=do_opt),\n+                         np.multiply(a[1:], a[:-1]))\n+            assert_equal(np.einsum(\"i,i\", a[1:], a[:-1], optimize=do_opt),\n+                         np.dot(a[1:], a[:-1]))\n+            assert_equal(np.einsum(\"i,->i\", a[1:], 2, optimize=do_opt), 2*a[1:])\n+            assert_equal(np.einsum(\",i->i\", 2, a[1:], optimize=do_opt), 2*a[1:])\n+            assert_equal(np.einsum(\"i,->\", a[1:], 2, optimize=do_opt),\n+                         2*np.sum(a[1:]))\n+            assert_equal(np.einsum(\",i->\", 2, a[1:], optimize=do_opt),\n+                         2*np.sum(a[1:]))\n+\n+    # An object array, summed as the data type\n+    a = np.arange(9, dtype=object)\n+\n+    b = np.einsum(\"i->\", a, dtype=dtype, casting='unsafe')\n+    assert_equal(b, np.sum(a))\n+    assert_equal(b.dtype, np.dtype(dtype))\n+\n+    b = np.einsum(a, [0], [], dtype=dtype, casting='unsafe')\n+    assert_equal(b, np.sum(a))\n+    assert_equal(b.dtype, np.dtype(dtype))\n+\n+    # A case which was failing (ticket #1885)\n+    p = np.arange(2) + 1\n+    q = np.arange(4).reshape(2, 2) + 3\n+    r = np.arange(4).reshape(2, 2) + 7\n+    assert_equal(np.einsum('z,mz,zm->', p, q, r), 253)\n+\n+    # singleton dimensions broadcast (gh-10343)\n+    p = np.ones((10,2))\n+    q = np.ones((1,2))\n+    assert_array_equal(np.einsum('ij,ij->j', p, q, optimize=True),\n+                       np.einsum('ij,ij->j', p, q, optimize=False))\n+    assert_array_equal(np.einsum('ij,ij->j', p, q, optimize=True),\n+                       [10.] * 2)\n+\n+    # a blas-compatible contraction broadcasting case which was failing\n+    # for optimize=True (ticket #10930)\n+    x = np.array([2., 3.])\n+    y = np.array([4.])\n+    assert_array_equal(np.einsum(\"i, i\", x, y, optimize=False), 20.)\n+    assert_array_equal(np.einsum(\"i, i\", x, y, optimize=True), 20.)\n+\n+    # all-ones array was bypassing bug (ticket #10930)\n+    p = np.ones((1, 5)) / 2\n+    q = np.ones((5, 5)) / 2\n+    for optimize in (True, False):\n+        assert_array_equal(np.einsum(\"...ij,...jk->...ik\", p, p,\n+                                     optimize=optimize),\n+                           np.einsum(\"...ij,...jk->...ik\", p, q,\n+                                     optimize=optimize))\n+        assert_array_equal(np.einsum(\"...ij,...jk->...ik\", p, q,\n+                                     optimize=optimize),\n+                           np.full((1, 5), 1.25))\n+\n+    # Cases which were failing (gh-10899)\n+    x = np.eye(2, dtype=dtype)\n+    y = np.ones(2, dtype=dtype)\n+    assert_array_equal(np.einsum(\"ji,i->\", x, y, optimize=optimize),\n+                       [2.])  # contig_contig_outstride0_two\n+    assert_array_equal(np.einsum(\"i,ij->\", y, x, optimize=optimize),\n+                       [2.])  # stride0_contig_outstride0_two\n+    assert_array_equal(np.einsum(\"ij,i->\", x, y, optimize=optimize),\n+                       [2.])  # contig_stride0_outstride0_two\n+\n",
            "comment_added_diff": {
                "16": "    # sum(a, axis=-1)",
                "33": "    # sum(a, axis=0)",
                "48": "    # trace(a)",
                "56": "        # gh-15961: should accept numpy int64 type in subscript list",
                "63": "    # multiply(a, b)",
                "64": "    assert_equal(np.einsum(\"..., ...\", 3, 4), 12)  # scalar case",
                "73": "    # inner(a,b)",
                "89": "    # outer(a,b)",
                "98": "    # Suppress the complex warnings for the 'as f8' tests",
                "102": "        # matvec(a,b) / a.dot(b) where a is matrix, b is vector",
                "145": "        # matmat(a,b) / a.dot(b) where a is matrix, b is matrix",
                "171": "        # matrix triple product (note this is not currently an efficient",
                "172": "        # way to multiply 3 matrices)",
                "196": "        # tensordot(a, b)",
                "216": "    # logical_and(logical_and(a!=0, b!=0), c!=0)",
                "233": "    # Various stride0, contiguous, and SSE aligned variants",
                "256": "    # An object array, summed as the data type",
                "267": "    # A case which was failing (ticket #1885)",
                "273": "    # singleton dimensions broadcast (gh-10343)",
                "281": "    # a blas-compatible contraction broadcasting case which was failing",
                "282": "    # for optimize=True (ticket #10930)",
                "288": "    # all-ones array was bypassing bug (ticket #10930)",
                "300": "    # Cases which were failing (gh-10899)",
                "304": "                       [2.])  # contig_contig_outstride0_two",
                "306": "                       [2.])  # stride0_contig_outstride0_two",
                "308": "                       [2.])  # contig_stride0_outstride0_two"
            },
            "comment_deleted_diff": {},
            "comment_modified_diff": {}
        },
        {
            "commit": "53f7b55cefa7207240e8bf5b8ec0f661c7b36491",
            "timestamp": "2023-03-29T14:10:51+03:00",
            "author": "iamsoto",
            "commit_message": "PR_fixes_1",
            "additions": 0,
            "deletions": 309,
            "change_type": "DELETE",
            "diff": "@@ -1,309 +0,0 @@\n-import itertools\n-\n-import numpy as np\n-from numpy.testing import (\n-    assert_, assert_equal, assert_array_equal, assert_almost_equal,\n-    assert_raises, suppress_warnings, assert_raises_regex, assert_allclose\n-    )\n-\n-\n-def test_einsum_sums(dtype='object', do_opt=False):\n-    \"\"\"\n-        Different from test_einsum.py since object dtype cannot perform\n-        np.sum(a, axis=-1).astype(dtype) when len(a) == 1\n-    \"\"\"\n-\n-    # sum(a, axis=-1)\n-    for n in range(1, 17):\n-        a = np.arange(n, dtype=dtype)\n-        assert_equal(np.einsum(\"i->\", a, optimize=do_opt),\n-                     np.sum(a, axis=-1))\n-        assert_equal(np.einsum(a, [0], [], optimize=do_opt),\n-                     np.sum(a, axis=-1))\n-\n-\n-    for n in range(1, 17):\n-        a = np.arange(2*3*n, dtype=dtype).reshape(2, 3, n)\n-        assert_equal(np.einsum(\"...i->...\", a, optimize=do_opt),\n-                     np.sum(a, axis=-1))\n-        assert_equal(np.einsum(a, [Ellipsis, 0], [Ellipsis], optimize=do_opt),\n-                     np.sum(a, axis=-1))\n-\n-        \n-    # sum(a, axis=0)\n-    for n in range(1, 17):\n-        a = np.arange(2*n, dtype=dtype).reshape(2, n)\n-        assert_equal(np.einsum(\"i...->...\", a, optimize=do_opt),\n-                     np.sum(a, axis=0))\n-        assert_equal(np.einsum(a, [0, Ellipsis], [Ellipsis], optimize=do_opt),\n-                     np.sum(a, axis=0))\n-\n-    for n in range(1, 17):\n-        a = np.arange(2*3*n, dtype=dtype).reshape(2, 3, n)\n-        assert_equal(np.einsum(\"i...->...\", a, optimize=do_opt),\n-                     np.sum(a, axis=0))\n-        assert_equal(np.einsum(a, [0, Ellipsis], [Ellipsis], optimize=do_opt),\n-                     np.sum(a, axis=0))\n-\n-    # trace(a)\n-    for n in range(1, 17):\n-        a = np.arange(n*n, dtype=dtype).reshape(n, n)\n-        assert_equal(np.einsum(\"ii\", a, optimize=do_opt),\n-                     np.trace(a))\n-        assert_equal(np.einsum(a, [0, 0], optimize=do_opt),\n-                     np.trace(a))\n-\n-        # gh-15961: should accept numpy int64 type in subscript list\n-        np_array = np.asarray([0, 0])\n-        assert_equal(np.einsum(a, np_array, optimize=do_opt),\n-                     np.trace(a))\n-        assert_equal(np.einsum(a, list(np_array), optimize=do_opt),\n-                     np.trace(a))\n-\n-    # multiply(a, b)\n-    assert_equal(np.einsum(\"..., ...\", 3, 4), 12)  # scalar case\n-    for n in range(1, 17):\n-        a = np.arange(3 * n, dtype=dtype).reshape(3, n)\n-        b = np.arange(2 * 3 * n, dtype=dtype).reshape(2, 3, n)\n-        assert_equal(np.einsum(\"..., ...\", a, b, optimize=do_opt),\n-                     np.multiply(a, b))\n-        assert_equal(np.einsum(a, [Ellipsis], b, [Ellipsis], optimize=do_opt),\n-                     np.multiply(a, b))\n-\n-    # inner(a,b)\n-    for n in range(1, 17):\n-        a = np.arange(2 * 3 * n, dtype=dtype).reshape(2, 3, n)\n-        b = np.arange(n, dtype=dtype)\n-        assert_equal(np.einsum(\"...i, ...i\", a, b, optimize=do_opt), np.inner(a, b))\n-        assert_equal(np.einsum(a, [Ellipsis, 0], b, [Ellipsis, 0], optimize=do_opt),\n-                     np.inner(a, b))\n-\n-    for n in range(1, 11):\n-        a = np.arange(n * 3 * 2, dtype=dtype).reshape(n, 3, 2)\n-        b = np.arange(n, dtype=dtype)\n-        assert_equal(np.einsum(\"i..., i...\", a, b, optimize=do_opt),\n-                     np.inner(a.T, b.T).T)\n-        assert_equal(np.einsum(a, [0, Ellipsis], b, [0, Ellipsis], optimize=do_opt),\n-                     np.inner(a.T, b.T).T)\n-\n-    # outer(a,b)\n-    for n in range(1, 17):\n-        a = np.arange(3, dtype=dtype)+1\n-        b = np.arange(n, dtype=dtype)+1\n-        assert_equal(np.einsum(\"i,j\", a, b, optimize=do_opt),\n-                     np.outer(a, b))\n-        assert_equal(np.einsum(a, [0], b, [1], optimize=do_opt),\n-                     np.outer(a, b))\n-\n-    # Suppress the complex warnings for the 'as f8' tests\n-    with suppress_warnings() as sup:\n-        sup.filter(np.ComplexWarning)\n-\n-        # matvec(a,b) / a.dot(b) where a is matrix, b is vector\n-        for n in range(1, 17):\n-            a = np.arange(4*n, dtype=dtype).reshape(4, n)\n-            b = np.arange(n, dtype=dtype)\n-            assert_equal(np.einsum(\"ij, j\", a, b, optimize=do_opt),\n-                         np.dot(a, b))\n-            assert_equal(np.einsum(a, [0, 1], b, [1], optimize=do_opt),\n-                         np.dot(a, b))\n-\n-            c = np.arange(4, dtype=dtype)\n-            np.einsum(\"ij,j\", a, b, out=c,\n-                      dtype='f8', casting='unsafe', optimize=do_opt)\n-            assert_equal(c,\n-                         np.dot(a.astype('f8'),\n-                                b.astype('f8')).astype(dtype))\n-            c[...] = 0\n-            np.einsum(a, [0, 1], b, [1], out=c,\n-                      dtype='f8', casting='unsafe', optimize=do_opt)\n-            assert_equal(c,\n-                         np.dot(a.astype('f8'),\n-                                b.astype('f8')).astype(dtype))\n-\n-        for n in range(1, 17):\n-            a = np.arange(4*n, dtype=dtype).reshape(4, n)\n-            b = np.arange(n, dtype=dtype)\n-            assert_equal(np.einsum(\"ji,j\", a.T, b.T, optimize=do_opt),\n-                         np.dot(b.T, a.T))\n-            assert_equal(np.einsum(a.T, [1, 0], b.T, [1], optimize=do_opt),\n-                         np.dot(b.T, a.T))\n-\n-            c = np.arange(4, dtype=dtype)\n-            np.einsum(\"ji,j\", a.T, b.T, out=c,\n-                      dtype='f8', casting='unsafe', optimize=do_opt)\n-            assert_equal(c,\n-                         np.dot(b.T.astype('f8'),\n-                                a.T.astype('f8')).astype(dtype))\n-            c[...] = 0\n-            np.einsum(a.T, [1, 0], b.T, [1], out=c,\n-                      dtype='f8', casting='unsafe', optimize=do_opt)\n-            assert_equal(c,\n-                         np.dot(b.T.astype('f8'),\n-                                a.T.astype('f8')).astype(dtype))\n-\n-        # matmat(a,b) / a.dot(b) where a is matrix, b is matrix\n-        for n in range(1, 17):\n-            if n < 8 or dtype != 'f2':\n-                a = np.arange(4*n, dtype=dtype).reshape(4, n)\n-                b = np.arange(n*6, dtype=dtype).reshape(n, 6)\n-                assert_equal(np.einsum(\"ij,jk\", a, b, optimize=do_opt),\n-                             np.dot(a, b))\n-                assert_equal(np.einsum(a, [0, 1], b, [1, 2], optimize=do_opt),\n-                             np.dot(a, b))\n-\n-        for n in range(1, 17):\n-            a = np.arange(4*n, dtype=dtype).reshape(4, n)\n-            b = np.arange(n*6, dtype=dtype).reshape(n, 6)\n-            c = np.arange(24, dtype=dtype).reshape(4, 6)\n-            np.einsum(\"ij,jk\", a, b, out=c, dtype='f8', casting='unsafe',\n-                      optimize=do_opt)\n-            assert_equal(c,\n-                         np.dot(a.astype('f8'),\n-                                b.astype('f8')).astype(dtype))\n-            c[...] = 0\n-            np.einsum(a, [0, 1], b, [1, 2], out=c,\n-                      dtype='f8', casting='unsafe', optimize=do_opt)\n-            assert_equal(c,\n-                         np.dot(a.astype('f8'),\n-                                b.astype('f8')).astype(dtype))\n-\n-        # matrix triple product (note this is not currently an efficient\n-        # way to multiply 3 matrices)\n-        a = np.arange(12, dtype=dtype).reshape(3, 4)\n-        b = np.arange(20, dtype=dtype).reshape(4, 5)\n-        c = np.arange(30, dtype=dtype).reshape(5, 6)\n-        if dtype != 'f2':\n-            assert_equal(np.einsum(\"ij,jk,kl\", a, b, c, optimize=do_opt),\n-                         a.dot(b).dot(c))\n-            assert_equal(np.einsum(a, [0, 1], b, [1, 2], c, [2, 3],\n-                                   optimize=do_opt), a.dot(b).dot(c))\n-\n-        d = np.arange(18, dtype=dtype).reshape(3, 6)\n-        np.einsum(\"ij,jk,kl\", a, b, c, out=d,\n-                  dtype='f8', casting='unsafe', optimize=do_opt)\n-        tgt = a.astype('f8').dot(b.astype('f8'))\n-        tgt = tgt.dot(c.astype('f8')).astype(dtype)\n-        assert_equal(d, tgt)\n-\n-        d[...] = 0\n-        np.einsum(a, [0, 1], b, [1, 2], c, [2, 3], out=d,\n-                  dtype='f8', casting='unsafe', optimize=do_opt)\n-        tgt = a.astype('f8').dot(b.astype('f8'))\n-        tgt = tgt.dot(c.astype('f8')).astype(dtype)\n-        assert_equal(d, tgt)\n-\n-        # tensordot(a, b)\n-        if np.dtype(dtype) != np.dtype('f2'):\n-            a = np.arange(60, dtype=dtype).reshape(3, 4, 5)\n-            b = np.arange(24, dtype=dtype).reshape(4, 3, 2)\n-            assert_equal(np.einsum(\"ijk, jil -> kl\", a, b),\n-                         np.tensordot(a, b, axes=([1, 0], [0, 1])))\n-            assert_equal(np.einsum(a, [0, 1, 2], b, [1, 0, 3], [2, 3]),\n-                         np.tensordot(a, b, axes=([1, 0], [0, 1])))\n-\n-            c = np.arange(10, dtype=dtype).reshape(5, 2)\n-            np.einsum(\"ijk,jil->kl\", a, b, out=c,\n-                      dtype='f8', casting='unsafe', optimize=do_opt)\n-            assert_equal(c, np.tensordot(a.astype('f8'), b.astype('f8'),\n-                         axes=([1, 0], [0, 1])).astype(dtype))\n-            c[...] = 0\n-            np.einsum(a, [0, 1, 2], b, [1, 0, 3], [2, 3], out=c,\n-                      dtype='f8', casting='unsafe', optimize=do_opt)\n-            assert_equal(c, np.tensordot(a.astype('f8'), b.astype('f8'),\n-                         axes=([1, 0], [0, 1])).astype(dtype))\n-\n-    # logical_and(logical_and(a!=0, b!=0), c!=0)\n-    a = np.array([1,   3,   -2,   0,   12,  13,   0,   1], dtype=dtype)\n-    b = np.array([0,   3.5, 0.,   -2,  0,   1,    3,   12], dtype=dtype)\n-    c = np.array([True, True, False, True, True, False, True, True])\n-    assert_equal(np.einsum(\"i,i,i->i\", a, b, c,\n-                 dtype='?', casting='unsafe', optimize=do_opt),\n-                 np.logical_and(np.logical_and(a != 0, b != 0), c != 0))\n-    assert_equal(np.einsum(a, [0], b, [0], c, [0], [0],\n-                 dtype='?', casting='unsafe'),\n-                 np.logical_and(np.logical_and(a != 0, b != 0), c != 0))\n-\n-    a = np.arange(9, dtype=dtype)\n-    assert_equal(np.einsum(\",i->\", 3, a), 3*np.sum(a))\n-    assert_equal(np.einsum(3, [], a, [0], []), 3*np.sum(a))\n-    assert_equal(np.einsum(\"i,->\", a, 3), 3*np.sum(a))\n-    assert_equal(np.einsum(a, [0], 3, [], []), 3*np.sum(a))\n-\n-    # Various stride0, contiguous, and SSE aligned variants\n-    for n in range(1, 25):\n-        a = np.arange(n, dtype=dtype)\n-        if np.dtype(dtype).itemsize > 1:\n-            assert_equal(np.einsum(\"...,...\", a, a, optimize=do_opt),\n-                         np.multiply(a, a))\n-            assert_equal(np.einsum(\"i,i\", a, a, optimize=do_opt), np.dot(a, a))\n-            assert_equal(np.einsum(\"i,->i\", a, 2, optimize=do_opt), 2*a)\n-            assert_equal(np.einsum(\",i->i\", 2, a, optimize=do_opt), 2*a)\n-            assert_equal(np.einsum(\"i,->\", a, 2, optimize=do_opt), 2*np.sum(a))\n-            assert_equal(np.einsum(\",i->\", 2, a, optimize=do_opt), 2*np.sum(a))\n-\n-            assert_equal(np.einsum(\"...,...\", a[1:], a[:-1], optimize=do_opt),\n-                         np.multiply(a[1:], a[:-1]))\n-            assert_equal(np.einsum(\"i,i\", a[1:], a[:-1], optimize=do_opt),\n-                         np.dot(a[1:], a[:-1]))\n-            assert_equal(np.einsum(\"i,->i\", a[1:], 2, optimize=do_opt), 2*a[1:])\n-            assert_equal(np.einsum(\",i->i\", 2, a[1:], optimize=do_opt), 2*a[1:])\n-            assert_equal(np.einsum(\"i,->\", a[1:], 2, optimize=do_opt),\n-                         2*np.sum(a[1:]))\n-            assert_equal(np.einsum(\",i->\", 2, a[1:], optimize=do_opt),\n-                         2*np.sum(a[1:]))\n-\n-    # An object array, summed as the data type\n-    a = np.arange(9, dtype=object)\n-\n-    b = np.einsum(\"i->\", a, dtype=dtype, casting='unsafe')\n-    assert_equal(b, np.sum(a))\n-    assert_equal(b.dtype, np.dtype(dtype))\n-\n-    b = np.einsum(a, [0], [], dtype=dtype, casting='unsafe')\n-    assert_equal(b, np.sum(a))\n-    assert_equal(b.dtype, np.dtype(dtype))\n-\n-    # A case which was failing (ticket #1885)\n-    p = np.arange(2) + 1\n-    q = np.arange(4).reshape(2, 2) + 3\n-    r = np.arange(4).reshape(2, 2) + 7\n-    assert_equal(np.einsum('z,mz,zm->', p, q, r), 253)\n-\n-    # singleton dimensions broadcast (gh-10343)\n-    p = np.ones((10,2))\n-    q = np.ones((1,2))\n-    assert_array_equal(np.einsum('ij,ij->j', p, q, optimize=True),\n-                       np.einsum('ij,ij->j', p, q, optimize=False))\n-    assert_array_equal(np.einsum('ij,ij->j', p, q, optimize=True),\n-                       [10.] * 2)\n-\n-    # a blas-compatible contraction broadcasting case which was failing\n-    # for optimize=True (ticket #10930)\n-    x = np.array([2., 3.])\n-    y = np.array([4.])\n-    assert_array_equal(np.einsum(\"i, i\", x, y, optimize=False), 20.)\n-    assert_array_equal(np.einsum(\"i, i\", x, y, optimize=True), 20.)\n-\n-    # all-ones array was bypassing bug (ticket #10930)\n-    p = np.ones((1, 5)) / 2\n-    q = np.ones((5, 5)) / 2\n-    for optimize in (True, False):\n-        assert_array_equal(np.einsum(\"...ij,...jk->...ik\", p, p,\n-                                     optimize=optimize),\n-                           np.einsum(\"...ij,...jk->...ik\", p, q,\n-                                     optimize=optimize))\n-        assert_array_equal(np.einsum(\"...ij,...jk->...ik\", p, q,\n-                                     optimize=optimize),\n-                           np.full((1, 5), 1.25))\n-\n-    # Cases which were failing (gh-10899)\n-    x = np.eye(2, dtype=dtype)\n-    y = np.ones(2, dtype=dtype)\n-    assert_array_equal(np.einsum(\"ji,i->\", x, y, optimize=optimize),\n-                       [2.])  # contig_contig_outstride0_two\n-    assert_array_equal(np.einsum(\"i,ij->\", y, x, optimize=optimize),\n-                       [2.])  # stride0_contig_outstride0_two\n-    assert_array_equal(np.einsum(\"ij,i->\", x, y, optimize=optimize),\n-                       [2.])  # contig_stride0_outstride0_two\n-\n",
            "comment_added_diff": {},
            "comment_deleted_diff": {
                "16": "    # sum(a, axis=-1)",
                "33": "    # sum(a, axis=0)",
                "48": "    # trace(a)",
                "56": "        # gh-15961: should accept numpy int64 type in subscript list",
                "63": "    # multiply(a, b)",
                "64": "    assert_equal(np.einsum(\"..., ...\", 3, 4), 12)  # scalar case",
                "73": "    # inner(a,b)",
                "89": "    # outer(a,b)",
                "98": "    # Suppress the complex warnings for the 'as f8' tests",
                "102": "        # matvec(a,b) / a.dot(b) where a is matrix, b is vector",
                "145": "        # matmat(a,b) / a.dot(b) where a is matrix, b is matrix",
                "171": "        # matrix triple product (note this is not currently an efficient",
                "172": "        # way to multiply 3 matrices)",
                "196": "        # tensordot(a, b)",
                "216": "    # logical_and(logical_and(a!=0, b!=0), c!=0)",
                "233": "    # Various stride0, contiguous, and SSE aligned variants",
                "256": "    # An object array, summed as the data type",
                "267": "    # A case which was failing (ticket #1885)",
                "273": "    # singleton dimensions broadcast (gh-10343)",
                "281": "    # a blas-compatible contraction broadcasting case which was failing",
                "282": "    # for optimize=True (ticket #10930)",
                "288": "    # all-ones array was bypassing bug (ticket #10930)",
                "300": "    # Cases which were failing (gh-10899)",
                "304": "                       [2.])  # contig_contig_outstride0_two",
                "306": "                       [2.])  # stride0_contig_outstride0_two",
                "308": "                       [2.])  # contig_stride0_outstride0_two"
            },
            "comment_modified_diff": {}
        }
    ],
    "einsum.c.src": [],
    "memory.h": [],
    "README.rst": [],
    "einsumfunc.py": [
        {
            "commit": "5c46c11882946b07bcecff4c3557de17d7c68e9a",
            "timestamp": "2023-09-26T13:24:22-07:00",
            "author": "Dimitri Papadopoulos Orfanos",
            "commit_message": "DOC: explain why we avoid string.ascii_letters (#24811)\n\n* DOC: explain why we avoid string.ascii_letters",
            "additions": 2,
            "deletions": 0,
            "change_type": "MODIFY",
            "diff": "@@ -11,6 +11,8 @@\n \n __all__ = ['einsum', 'einsum_path']\n \n+# importing string for string.ascii_letters would be too slow\n+# the first import before caching has been measured to take 800 \u00b5s (#23777)\n einsum_symbols = 'abcdefghijklmnopqrstuvwxyzABCDEFGHIJKLMNOPQRSTUVWXYZ'\n einsum_symbols_set = set(einsum_symbols)\n \n",
            "comment_added_diff": {
                "14": "# importing string for string.ascii_letters would be too slow",
                "15": "# the first import before caching has been measured to take 800 \u00b5s (#23777)"
            },
            "comment_deleted_diff": {},
            "comment_modified_diff": {}
        }
    ],
    "npy_math_private.h": [],
    "build_clib.py": [
        {
            "commit": "16f73741f425fca39cb2a33f72e251432391f30b",
            "timestamp": "2023-08-21T01:42:41+04:00",
            "author": "Sayed Adel",
            "commit_message": "BUG: Fix meson build failure due to uncleaned inplace auto-generated dispatch config headers\n\n  Ensure that the distutils generated config files and wrapped sources,\n  derived from dispatch-able sources are consistently generated within the build directory\n  when the inplace build option is enabled.\n\n  This change is crucial to prevent conflicts with meson-generated config headers.\n  Given that `spin build --clean` does not remove these headers, which\n  requires cleaning up the numpy root via `git clean` otherwise\n  the build will fails.",
            "additions": 2,
            "deletions": 2,
            "change_type": "MODIFY",
            "diff": "@@ -320,8 +320,8 @@ def build_a_library(self, build_info, lib_name, libraries):\n             dispatch_hpath = os.path.join(\"numpy\", \"distutils\", \"include\")\n             dispatch_hpath = os.path.join(bsrc_dir, dispatch_hpath)\n             include_dirs.append(dispatch_hpath)\n-\n-            copt_build_src = None if self.inplace else bsrc_dir\n+            # copt_build_src = None if self.inplace else bsrc_dir\n+            copt_build_src = bsrc_dir\n             for _srcs, _dst, _ext in (\n                 ((c_sources,), copt_c_sources, ('.dispatch.c',)),\n                 ((c_sources, cxx_sources), copt_cxx_sources,\n",
            "comment_added_diff": {
                "323": "            # copt_build_src = None if self.inplace else bsrc_dir"
            },
            "comment_deleted_diff": {},
            "comment_modified_diff": {
                "323": ""
            }
        }
    ],
    "arrayscalars.h": [],
    "ufuncobject.h": [],
    "common.hpp": [],
    "float_status.hpp": [],
    "half.hpp": [],
    "half_private.hpp": [],
    "npdef.hpp": [],
    "npstd.hpp": [],
    "utils.hpp": [],
    "halffloat.c": [],
    "halffloat.cpp": [],
    "23357.improvement.rst": [],
    "npyio.pyi": [],
    "22137.new_feature.rst": [],
    "diagnose.py": [],
    "types.py": [
        {
            "commit": "03e5cf0b5697957c3f8345ed09a3662494633e7e",
            "timestamp": "2023-04-12T12:21:41+02:00",
            "author": "Sebastian Berg",
            "commit_message": "API: Add `numpy.types` module and fill it with DType classes",
            "additions": 73,
            "deletions": 0,
            "change_type": "ADD",
            "diff": "@@ -0,0 +1,73 @@\n+\"\"\"\n+Names of builtin NumPy Types (:mod:`numpy.types`)\n+==================================================\n+\n+Similar to the builtin ``types`` module, this submodule defines types (classes)\n+that are not widely used directly.\n+\n+.. versionadded:: NumPy 1.25\n+\n+    The types module is new in NumPy 1.25.  Older exceptions remain\n+    available through the main NumPy namespace for compatibility.\n+\n+\n+DType classes\n+-------------\n+\n+The following are the classes of the corresponding NumPy dtype instances and\n+NumPy scalar types.  The classe can be used for ``isisntance`` checks but are\n+otherwise not typically useful as of now.\n+\n+For general information see `numpy.dtype` and :ref:`arrays.dtypes`.\n+\n+.. list-table::\n+    :header-rows: 1\n+\n+    * - Group\n+      - DType class\n+\n+    * - Boolean\n+      - ``BoolDType``\n+\n+    * - Bit-sized integers\n+      - ``Int8DType``, ``UInt8DType``, ``Int16DType``, ``UInt16DType``,\n+        ``Int32DType``, ``UInt32DType``, ``Int64DType``, ``UInt64DType``\n+\n+    * - C-named integers (may be aliases)\n+      - ``ByteDType``, ``UByteDType``, ``ShortDType``, ``UShortDType``,\n+        ``IntDType``, ``UIntDType``, ``LongDType``, ``ULongDType``,\n+        ``LongLongDType``, ``ULongLongDType``\n+\n+    * - Floating point\n+      - ``Float16DType``, ``Float32DType``, ``Float64DType``,\n+        ``LongDoubleDType``\n+\n+    * - Complex\n+      - ``Complex64DType``, ``Complex128DType``, ``CLongDoubleDType``\n+\n+    * - Strings\n+      - ``BytesDType``, ``BytesDType``\n+\n+    * - Times\n+      - ``DateTime64DType``, ``TimeDelta64DType``\n+\n+    * - Others\n+      - ``ObjectDType``, ``VoidDType``\n+\n+\"\"\"\n+\n+__all__ = []\n+\n+\n+def _add_dtype_helper(DType, alias):\n+    # Function to add DTypes a bit more conveniently without channeling them\n+    # through `numpy.core._multiarray_umath` namespace or similar.\n+    from numpy import types\n+\n+    setattr(types, DType.__name__, DType)\n+    __all__.append(DType.__name__)\n+\n+    if alias:\n+        alias = alias.removeprefix(\"numpy.types.\")\n+        setattr(types, alias, DType)\n+        __all__.append(alias)\n",
            "comment_added_diff": {
                "63": "    # Function to add DTypes a bit more conveniently without channeling them",
                "64": "    # through `numpy.core._multiarray_umath` namespace or similar."
            },
            "comment_deleted_diff": {},
            "comment_modified_diff": {}
        }
    ],
    "types.pyi": [],
    "23358.improvement.rst": [],
    "dtypes.py": [],
    "dtypes.pyi": [],
    "bench_creation.py": [
        {
            "commit": "1def667d1aa5e38e0cbb01a5d76b21c746904224",
            "timestamp": "2023-04-13T21:34:17+00:00",
            "author": "Rohit Goswami",
            "commit_message": "BENCH: Rework benchmarks",
            "additions": 81,
            "deletions": 0,
            "change_type": "ADD",
            "diff": "@@ -0,0 +1,81 @@\n+from .common import Benchmark, TYPES1\n+\n+import numpy as np\n+\n+\n+class MeshGrid(Benchmark):\n+    \"\"\" Benchmark meshgrid generation\n+    \"\"\"\n+    params = [[16, 32],\n+              [2, 3, 4],\n+              ['ij', 'xy'], TYPES1]\n+    param_names = ['size', 'ndims', 'ind', 'ndtype']\n+    timeout = 10\n+\n+    def setup(self, size, ndims, ind, ndtype):\n+        self.grid_dims = [(np.random.ranf(size)).astype(ndtype) for\n+                          x in range(ndims)]\n+\n+    def time_meshgrid(self, size, ndims, ind, ndtype):\n+        np.meshgrid(*self.grid_dims, indexing=ind)\n+\n+\n+class Create(Benchmark):\n+    \"\"\" Benchmark for creation functions\n+    \"\"\"\n+    # (64, 64), (128, 128), (256, 256)\n+    # , (512, 512), (1024, 1024)\n+    params = [[16, 32, 128, 256, 512,\n+               (16, 16), (32, 32)],\n+              ['C', 'F'],\n+              TYPES1]\n+    param_names = ['shape', 'order', 'npdtypes']\n+    timeout = 10\n+\n+    def setup(self, shape, order, npdtypes):\n+        values = get_squares_()\n+        self.xarg = values.get(npdtypes)[0]\n+\n+    def time_full(self, shape, order, npdtypes):\n+        np.full(shape, self.xarg[1], dtype=npdtypes, order=order)\n+\n+    def time_full_like(self, shape, order, npdtypes):\n+        np.full_like(self.xarg, self.xarg[0], order=order)\n+\n+    def time_ones(self, shape, order, npdtypes):\n+        np.ones(shape, dtype=npdtypes, order=order)\n+\n+    def time_ones_like(self, shape, order, npdtypes):\n+        np.ones_like(self.xarg, order=order)\n+\n+    def time_zeros(self, shape, order, npdtypes):\n+        np.zeros(shape, dtype=npdtypes, order=order)\n+\n+    def time_zeros_like(self, shape, order, npdtypes):\n+        np.zeros_like(self.xarg, order=order)\n+\n+    def time_empty(self, shape, order, npdtypes):\n+        np.empty(shape, dtype=npdtypes, order=order)\n+\n+    def time_empty_like(self, shape, order, npdtypes):\n+        np.empty_like(self.xarg, order=order)\n+\n+\n+class UfuncsFromDLP(Benchmark):\n+    \"\"\" Benchmark for creation functions\n+    \"\"\"\n+    params = [[16, 32, (16, 16),\n+               (32, 32), (64, 64)],\n+              TYPES1]\n+    param_names = ['shape', 'npdtypes']\n+    timeout = 10\n+\n+    def setup(self, shape, npdtypes):\n+        if npdtypes in ['longdouble', 'clongdouble']:\n+            raise NotImplementedError(\n+                'Only IEEE dtypes are supported')\n+        values = get_squares_()\n+        self.xarg = values.get(npdtypes)[0]\n+\n+    def time_from_dlpack(self, shape, npdtypes):\n+        np.from_dlpack(self.xarg)\n",
            "comment_added_diff": {
                "26": "    # (64, 64), (128, 128), (256, 256)",
                "27": "    # , (512, 512), (1024, 1024)"
            },
            "comment_deleted_diff": {},
            "comment_modified_diff": {}
        },
        {
            "commit": "7b2169651adc1931e4458bc86e955af46351e22a",
            "timestamp": "2023-08-27T22:40:43+02:00",
            "author": "Ralf Gommers",
            "commit_message": "BENCH: fix benchmarks in `bench_creation.py`, remove longdouble types\n\nRuntime with --quick for this file is now down to a more reasonable\n20 seconds.\n\nWe shouldn't be benchmarking long double dtypes, that's a waste of\ntime and CI log space. CI is currently stopping at ~50,000 lines\nof output that cannot be suppressed, and the tests are heavily\nover-parametrized.",
            "additions": 21,
            "deletions": 29,
            "change_type": "MODIFY",
            "diff": "@@ -1,4 +1,4 @@\n-from .common import Benchmark, TYPES1\n+from .common import Benchmark, TYPES1, get_squares_\n \n import numpy as np\n \n@@ -23,57 +23,49 @@ def time_meshgrid(self, size, ndims, ind, ndtype):\n class Create(Benchmark):\n     \"\"\" Benchmark for creation functions\n     \"\"\"\n-    # (64, 64), (128, 128), (256, 256)\n-    # , (512, 512), (1024, 1024)\n-    params = [[16, 32, 128, 256, 512,\n-               (16, 16), (32, 32)],\n-              ['C', 'F'],\n+    params = [[16, 512, (32, 32)],\n               TYPES1]\n-    param_names = ['shape', 'order', 'npdtypes']\n+    param_names = ['shape', 'npdtypes']\n     timeout = 10\n \n-    def setup(self, shape, order, npdtypes):\n+    def setup(self, shape, npdtypes):\n         values = get_squares_()\n         self.xarg = values.get(npdtypes)[0]\n \n-    def time_full(self, shape, order, npdtypes):\n-        np.full(shape, self.xarg[1], dtype=npdtypes, order=order)\n+    def time_full(self, shape, npdtypes):\n+        np.full(shape, self.xarg[1], dtype=npdtypes)\n \n-    def time_full_like(self, shape, order, npdtypes):\n-        np.full_like(self.xarg, self.xarg[0], order=order)\n+    def time_full_like(self, shape, npdtypes):\n+        np.full_like(self.xarg, self.xarg[0])\n \n-    def time_ones(self, shape, order, npdtypes):\n-        np.ones(shape, dtype=npdtypes, order=order)\n+    def time_ones(self, shape, npdtypes):\n+        np.ones(shape, dtype=npdtypes)\n \n-    def time_ones_like(self, shape, order, npdtypes):\n-        np.ones_like(self.xarg, order=order)\n+    def time_ones_like(self, shape, npdtypes):\n+        np.ones_like(self.xarg)\n \n-    def time_zeros(self, shape, order, npdtypes):\n-        np.zeros(shape, dtype=npdtypes, order=order)\n+    def time_zeros(self, shape, npdtypes):\n+        np.zeros(shape, dtype=npdtypes)\n \n-    def time_zeros_like(self, shape, order, npdtypes):\n-        np.zeros_like(self.xarg, order=order)\n+    def time_zeros_like(self, shape, npdtypes):\n+        np.zeros_like(self.xarg)\n \n-    def time_empty(self, shape, order, npdtypes):\n-        np.empty(shape, dtype=npdtypes, order=order)\n+    def time_empty(self, shape, npdtypes):\n+        np.empty(shape, dtype=npdtypes)\n \n-    def time_empty_like(self, shape, order, npdtypes):\n-        np.empty_like(self.xarg, order=order)\n+    def time_empty_like(self, shape, npdtypes):\n+        np.empty_like(self.xarg)\n \n \n class UfuncsFromDLP(Benchmark):\n     \"\"\" Benchmark for creation functions\n     \"\"\"\n-    params = [[16, 32, (16, 16),\n-               (32, 32), (64, 64)],\n+    params = [[16, 32, (16, 16), (64, 64)],\n               TYPES1]\n     param_names = ['shape', 'npdtypes']\n     timeout = 10\n \n     def setup(self, shape, npdtypes):\n-        if npdtypes in ['longdouble', 'clongdouble']:\n-            raise NotImplementedError(\n-                'Only IEEE dtypes are supported')\n         values = get_squares_()\n         self.xarg = values.get(npdtypes)[0]\n \n",
            "comment_added_diff": {},
            "comment_deleted_diff": {
                "26": "    # (64, 64), (128, 128), (256, 256)",
                "27": "    # , (512, 512), (1024, 1024)"
            },
            "comment_modified_diff": {}
        }
    ],
    "data_stmts.f90": [],
    "test_data.py": [
        {
            "commit": "1d02e2ba1b780524b1fb9f9a2c1af9cd4df02312",
            "timestamp": "2023-04-15T18:04:25+00:00",
            "author": "Rohit Goswami",
            "commit_message": "TST: Add test for gh-23276",
            "additions": 33,
            "deletions": 0,
            "change_type": "ADD",
            "diff": "@@ -0,0 +1,33 @@\n+import os\n+import pytest\n+import numpy as np\n+\n+from . import util\n+from numpy.f2py.crackfortran import crackfortran\n+\n+\n+class TestData(util.F2PyTest):\n+    sources = [util.getpath(\"tests\", \"src\", \"crackfortran\", \"data_stmts.f90\")]\n+\n+    # For gh-23276\n+    def test_data_stmts(self):\n+        assert self.module.cmplxdat.i == 2\n+        assert self.module.cmplxdat.j == 3\n+        assert self.module.cmplxdat.x == 1.5\n+        assert self.module.cmplxdat.y == 2.0\n+        assert self.module.cmplxdat.medium_ref_index == np.array(1.+0.j)\n+        assert np.all(self.module.cmplxdat.z == np.array([3.5, 7.0]))\n+        assert np.all(self.module.cmplxdat.my_array == np.array([ 1.+2.j, -3.+4.j]))\n+        assert np.all(self.module.cmplxdat.my_real_array == np.array([ 1., 2., 3.]))\n+        assert np.all(self.module.cmplxdat.ref_index_one == np.array([13.0 + 21.0j]))\n+        assert np.all(self.module.cmplxdat.ref_index_two == np.array([-30.0 + 43.0j]))\n+\n+    def test_crackedlines(self):\n+        mod = crackfortran(self.sources)\n+        assert mod[0]['vars']['x']['='] == '1.5'\n+        assert mod[0]['vars']['y']['='] == '2.0'\n+        assert mod[0]['vars']['my_real_array']['='] == '(/1.0d0, 2.0d0, 3.0d0/)'\n+        assert mod[0]['vars']['ref_index_one']['='] == '(13.0d0, 21.0d0)'\n+        assert mod[0]['vars']['ref_index_two']['='] == '(-30.0d0, 43.0d0)'\n+        # assert mod[0]['vars']['my_array']['='] == '(1.0d0, 2.0d0), (-3.0d0, 4.0d0)'\n+        # assert mod[0]['vars']['z']['='] == '(/ 3.5, 7.0 /)'\n",
            "comment_added_diff": {
                "12": "    # For gh-23276",
                "32": "        # assert mod[0]['vars']['my_array']['='] == '(1.0d0, 2.0d0), (-3.0d0, 4.0d0)'",
                "33": "        # assert mod[0]['vars']['z']['='] == '(/ 3.5, 7.0 /)'"
            },
            "comment_deleted_diff": {},
            "comment_modified_diff": {}
        },
        {
            "commit": "e2288b3aeccd56aff298b7cc58535dda1fcc7934",
            "timestamp": "2023-04-15T18:59:50+00:00",
            "author": "Rohit Goswami",
            "commit_message": "TST: Add tests for edge case with data statements",
            "additions": 2,
            "deletions": 2,
            "change_type": "MODIFY",
            "diff": "@@ -29,5 +29,5 @@ def test_crackedlines(self):\n         assert mod[0]['vars']['my_real_array']['='] == '(/1.0d0, 2.0d0, 3.0d0/)'\n         assert mod[0]['vars']['ref_index_one']['='] == '(13.0d0, 21.0d0)'\n         assert mod[0]['vars']['ref_index_two']['='] == '(-30.0d0, 43.0d0)'\n-        # assert mod[0]['vars']['my_array']['='] == '(1.0d0, 2.0d0), (-3.0d0, 4.0d0)'\n-        # assert mod[0]['vars']['z']['='] == '(/ 3.5, 7.0 /)'\n+        assert mod[0]['vars']['my_array']['='] == '(/(1.0d0, 2.0d0), (-3.0d0, 4.0d0)/)'\n+        assert mod[0]['vars']['z']['='] == '(/3.5,  7.0/)'\n",
            "comment_added_diff": {},
            "comment_deleted_diff": {
                "32": "        # assert mod[0]['vars']['my_array']['='] == '(1.0d0, 2.0d0), (-3.0d0, 4.0d0)'",
                "33": "        # assert mod[0]['vars']['z']['='] == '(/ 3.5, 7.0 /)'"
            },
            "comment_modified_diff": {}
        },
        {
            "commit": "ae28b2183a6851bc076eb14faa77fc37397ecc74",
            "timestamp": "2023-09-22T15:40:05+00:00",
            "author": "Rohit Goswami",
            "commit_message": "TST: Add examples for gh-24746",
            "additions": 14,
            "deletions": 0,
            "change_type": "MODIFY",
            "diff": "@@ -15,6 +15,7 @@ def test_data_stmts(self):\n         assert self.module.cmplxdat.j == 3\n         assert self.module.cmplxdat.x == 1.5\n         assert self.module.cmplxdat.y == 2.0\n+        assert self.module.cmplxdat.pi == 3.1415926535897932384626433832795028841971693993751058209749445923078164062\n         assert self.module.cmplxdat.medium_ref_index == np.array(1.+0.j)\n         assert np.all(self.module.cmplxdat.z == np.array([3.5, 7.0]))\n         assert np.all(self.module.cmplxdat.my_array == np.array([ 1.+2.j, -3.+4.j]))\n@@ -26,8 +27,21 @@ def test_crackedlines(self):\n         mod = crackfortran(self.sources)\n         assert mod[0]['vars']['x']['='] == '1.5'\n         assert mod[0]['vars']['y']['='] == '2.0'\n+        assert mod[0]['vars']['pi']['='] == '3.1415926535897932384626433832795028841971693993751058209749445923078164062d0'\n         assert mod[0]['vars']['my_real_array']['='] == '(/1.0d0, 2.0d0, 3.0d0/)'\n         assert mod[0]['vars']['ref_index_one']['='] == '(13.0d0, 21.0d0)'\n         assert mod[0]['vars']['ref_index_two']['='] == '(-30.0d0, 43.0d0)'\n         assert mod[0]['vars']['my_array']['='] == '(/(1.0d0, 2.0d0), (-3.0d0, 4.0d0)/)'\n         assert mod[0]['vars']['z']['='] == '(/3.5,  7.0/)'\n+\n+class TestDataF77(util.F2PyTest):\n+    sources = [util.getpath(\"tests\", \"src\", \"crackfortran\", \"data_common.f\")]\n+\n+    # For gh-23276\n+    def test_data_stmts(self):\n+        assert self.module.mycom.mydata == 0\n+\n+    def test_crackedlines(self):\n+        mod = crackfortran(str(self.sources[0]))\n+        print(mod[0]['vars'])\n+        assert mod[0]['vars']['mydata']['='] == '0'\n",
            "comment_added_diff": {
                "40": "    # For gh-23276"
            },
            "comment_deleted_diff": {},
            "comment_modified_diff": {}
        },
        {
            "commit": "14bebd11f2941051ba5860eb179485fdec28a2bc",
            "timestamp": "2023-09-24T13:29:02+00:00",
            "author": "Rohit Goswami",
            "commit_message": "TST: More tests for gh-24746",
            "additions": 11,
            "deletions": 0,
            "change_type": "MODIFY",
            "diff": "@@ -45,3 +45,14 @@ def test_crackedlines(self):\n         mod = crackfortran(str(self.sources[0]))\n         print(mod[0]['vars'])\n         assert mod[0]['vars']['mydata']['='] == '0'\n+\n+\n+class TestDataWithCommentsF77(util.F2PyTest):\n+    sources = [util.getpath(\"tests\", \"src\", \"crackfortran\", \"data_with_comments.f\")]\n+\n+    # For gh-23276\n+    def test_data_stmts(self):\n+        assert len(self.module.mycom.mytab) == 3\n+        assert self.module.mycom.mytab[0] == 0\n+        assert self.module.mycom.mytab[1] == 4\n+        assert self.module.mycom.mytab[2] == 0\n",
            "comment_added_diff": {
                "53": "    # For gh-23276"
            },
            "comment_deleted_diff": {},
            "comment_modified_diff": {}
        },
        {
            "commit": "1e6a322a9514c0050f4cc656aead1aeffba0b1b5",
            "timestamp": "2023-09-27T13:58:43+00:00",
            "author": "Rohit Goswami",
            "commit_message": "TST: Add check for multipliers in data [f2py]\n\nCo-authored-by: jncots <jncots@users.noreply.github.com>",
            "additions": 12,
            "deletions": 0,
            "change_type": "MODIFY",
            "diff": "@@ -47,6 +47,18 @@ def test_crackedlines(self):\n         assert mod[0]['vars']['mydata']['='] == '0'\n \n \n+class TestDataMultiplierF77(util.F2PyTest):\n+    sources = [util.getpath(\"tests\", \"src\", \"crackfortran\", \"data_multiplier.f\")]\n+\n+    # For gh-23276\n+    def test_data_stmts(self):\n+        assert self.module.mycom.ivar1 == 3\n+        assert self.module.mycom.ivar2 == 3\n+        assert self.module.mycom.ivar3 == 2\n+        assert self.module.mycom.ivar4 == 2\n+        assert self.module.mycom.evar5 == 0\n+\n+\n class TestDataWithCommentsF77(util.F2PyTest):\n     sources = [util.getpath(\"tests\", \"src\", \"crackfortran\", \"data_with_comments.f\")]\n \n",
            "comment_added_diff": {
                "53": "    # For gh-23276"
            },
            "comment_deleted_diff": {},
            "comment_modified_diff": {}
        }
    ],
    "auxfuncs.py": [
        {
            "commit": "fedc834fecb7e1aca2917ad2e0fd1694400529e0",
            "timestamp": "2023-09-05T10:42:35-06:00",
            "author": "Rohit Goswami",
            "commit_message": "ENH: ``meson`` backend for ``f2py`` (#24532)\n\n* FIX: Import f2py2e rather than f2py for run_main\r\n\r\n* FIX: Import f2py2e instead of f2py\r\n\r\n* ENH: Add F2PY back-end work from gh-22225\r\n\r\nCo-authored-by: NamamiShanker <NamamiShanker@users.noreply.github.com>\r\n\r\n* ENH: Add meson skeleton from gh-2225\r\n\r\nCo-authored-by: NamamiShanker <NamamiShanker@users.noreply.github.com>\r\n\r\n* MAINT: Trim backend.py down to f2py2e flags\r\n\r\n* ENH: Add a factory function for backends\r\n\r\n* ENH: Add a distutils backend\r\n\r\n* ENH: Handle --backends in f2py\r\n\r\nDefaults to distutils for now\r\n\r\n* DOC: Add some minor comments in f2py2e\r\n\r\n* MAINT: Refactor and rework meson.build.src\r\n\r\n* MAINT: Add objects\r\n\r\n* MAINT: Cleanup distutils backend\r\n\r\n* MAINT: Refactor to add everything back to backend\r\n\r\nNecessary for the meson.build for now. Refactors / cleanup needs better\r\nargument handling in f2py2e\r\n\r\n* MAINT: Fix overly long line\r\n\r\n* BUG: Construct wrappers for meson backend\r\n\r\n* MAINT: Rework, simplify template massively\r\n\r\n* ENH: Truncate meson.build to skeleton only\r\n\r\n* MAINT: Minor backend housekeeping, name changes\r\n\r\n* MAINT: Less absolute paths, update setup.py [f2py]\r\n\r\n* MAINT: Move f2py module name functionality\r\n\r\nPreviously part of np.distutils\r\n\r\n* ENH: Handle .pyf files\r\n\r\n* TST: Fix typo in isoFortranEnvMap.f90\r\n\r\n* MAINT: Typo in f2py2e support for pyf files\r\n\r\n* DOC: Add release note for --backend\r\n\r\n* MAINT: Conditional switch for Python 3.12 [f2py]\r\n\r\n* MAINT: No absolute paths in backend [f2py-meson]\r\n\r\nThe files are copied over anyway, this makes it easier to extend the\r\ngenerated skeleton\r\n\r\n* MAINT: Prettier generated meson.build files [f2py]\r\n\r\n* ENH: Add meson's dependency(blah) to f2py\r\n\r\n* DOC: Document the new flag\r\n\r\n* MAINT: Simplify and rename backend template [f2py]\r\n\r\nCo-authored-by: rgommers <rgommers@users.noreply.github.com>\r\n\r\n* ENH: Support build_type via --debug [f2py-meson]\r\n\r\n* MAINT,DOC: Reduce warn,rework doc [f2py-meson]\r\n\r\nCo-authored-by: rgommers <rgommers@users.noreply.github.com>\r\n\r\n* ENH: Rework deps: to --dep calls [f2py-meson]\r\n\r\nAlso shows how incremental updates to the parser can be done.\r\n\r\n* MAINT,DOC: Add --backend to argparse, add docs\r\n\r\n* MAINT: Rename meson template [f2py-meson]\r\n\r\n* MAINT: Add meson.build for f2py\r\n\r\nShould address https://github.com/numpy/numpy/pull/22225#issuecomment-1697208937\r\n\r\n* BLD: remove duplicate f2py handling in meson.build files\r\n\r\n---------\r\n\r\nCo-authored-by: Namami Shanker <namami2011@gmail.com>\r\nCo-authored-by: NamamiShanker <NamamiShanker@users.noreply.github.com>\r\nCo-authored-by: rgommers <rgommers@users.noreply.github.com>\r\nCo-authored-by: Ralf Gommers <ralf.gommers@gmail.com>",
            "additions": 19,
            "deletions": 1,
            "change_type": "MODIFY",
            "diff": "@@ -16,6 +16,7 @@\n \"\"\"\n import pprint\n import sys\n+import re\n import types\n from functools import reduce\n from copy import deepcopy\n@@ -43,7 +44,7 @@\n     'ismodule', 'ismoduleroutine', 'isoptional', 'isprivate', 'isrequired',\n     'isroutine', 'isscalar', 'issigned_long_longarray', 'isstring',\n     'isstringarray', 'isstring_or_stringarray', 'isstringfunction',\n-    'issubroutine',\n+    'issubroutine', 'get_f2py_modulename',\n     'issubroutine_wrap', 'isthreadsafe', 'isunsigned', 'isunsigned_char',\n     'isunsigned_chararray', 'isunsigned_long_long',\n     'isunsigned_long_longarray', 'isunsigned_short',\n@@ -912,3 +913,20 @@ def deep_merge(dict1, dict2):\n         else:\n             merged_dict[key] = value\n     return merged_dict\n+\n+_f2py_module_name_match = re.compile(r'\\s*python\\s*module\\s*(?P<name>[\\w_]+)',\n+                                     re.I).match\n+_f2py_user_module_name_match = re.compile(r'\\s*python\\s*module\\s*(?P<name>[\\w_]*?'\n+                                          r'__user__[\\w_]*)', re.I).match\n+\n+def get_f2py_modulename(source):\n+    name = None\n+    with open(source) as f:\n+        for line in f:\n+            m = _f2py_module_name_match(line)\n+            if m:\n+                if _f2py_user_module_name_match(line): # skip *__user__* names\n+                    continue\n+                name = m.group('name')\n+                break\n+    return name\n",
            "comment_added_diff": {
                "928": "                if _f2py_user_module_name_match(line): # skip *__user__* names"
            },
            "comment_deleted_diff": {},
            "comment_modified_diff": {}
        }
    ],
    "test_kind.py": [],
    "gh23598.f90": [],
    "23601.change.rst": [],
    "func2subr.py": [
        {
            "commit": "2ac450e5c0c0ca47040c2fd4147f45f42010510c",
            "timestamp": "2023-04-17T00:41:10+00:00",
            "author": "Rohit Goswami",
            "commit_message": "BUG: Ensure function wrappers have consistent args",
            "additions": 6,
            "deletions": 0,
            "change_type": "MODIFY",
            "diff": "@@ -119,6 +119,12 @@ def add(line, ret=ret):\n \n     sargs = ', '.join(args)\n     if f90mode:\n+        # gh-23598 fix warning\n+        # Essentially, this gets called again with modules where the name of the\n+        # function is added to the arguments, which is not required, and removed\n+        sargs = sargs.replace(f\"{name}, \", '')\n+        args = [arg for arg in args if arg != name]\n+        rout['args'] = args\n         add('subroutine f2pywrap_%s_%s (%s)' %\n             (rout['modulename'], name, sargs))\n         if not signature:\n",
            "comment_added_diff": {
                "122": "        # gh-23598 fix warning",
                "123": "        # Essentially, this gets called again with modules where the name of the",
                "124": "        # function is added to the arguments, which is not required, and removed"
            },
            "comment_deleted_diff": {},
            "comment_modified_diff": {}
        }
    ],
    "gh23598Warn.f90": [],
    "test_f2py2e.py": [
        {
            "commit": "c7ff4118a345c966e2a0fc688e054e3fd9191e99",
            "timestamp": "2023-04-17T00:59:45+00:00",
            "author": "Rohit Goswami",
            "commit_message": "TST: Add a test for the f2py function wrapper file",
            "additions": 22,
            "deletions": 0,
            "change_type": "MODIFY",
            "diff": "@@ -62,6 +62,15 @@ def hello_world_f90(tmpdir_factory):\n     return fn\n \n \n+@pytest.fixture(scope=\"session\")\n+def gh23598_warn(tmpdir_factory):\n+    \"\"\"F90 file for testing warnings in gh23598\"\"\"\n+    fdat = util.getpath(\"tests\", \"src\", \"crackfortran\", \"gh23598Warn.f90\").read_text()\n+    fn = tmpdir_factory.getbasetemp() / \"gh23598Warn.f90\"\n+    fn.write_text(fdat, encoding=\"ascii\")\n+    return fn\n+\n+\n @pytest.fixture(scope=\"session\")\n def hello_world_f77(tmpdir_factory):\n     \"\"\"Generates a single f77 file for testing\"\"\"\n@@ -91,6 +100,19 @@ def f2cmap_f90(tmpdir_factory):\n     return fn\n \n \n+def test_gh23598_warn(capfd, gh23598_warn, monkeypatch):\n+    foutl = get_io_paths(gh23598_warn, mname=\"test\")\n+    ipath = foutl.f90inp\n+    monkeypatch.setattr(\n+        sys, \"argv\",\n+        f'f2py {ipath} -m test'.split())\n+\n+    with util.switchdir(ipath.parent):\n+        f2pycli()  # Generate files\n+        wrapper = foutl.wrap90.read_text()\n+        assert \"intproductf2pywrap, intpr\" not in wrapper\n+\n+\n def test_gen_pyf(capfd, hello_world_f90, monkeypatch):\n     \"\"\"Ensures that a signature file is generated via the CLI\n     CLI :: -h\n",
            "comment_added_diff": {
                "111": "        f2pycli()  # Generate files"
            },
            "comment_deleted_diff": {},
            "comment_modified_diff": {}
        }
    ],
    "23528.compatibility.rst": [],
    "depending_on_numpy.rst": [],
    "reviewer_guidelines.rst": [],
    "10615.deprecation.rst": [],
    "common.c": [],
    "23660.expired.rst": [],
    "loops_umath_fp.dispatch.c.src": [],
    "test_generator_mt19937_regressions.py": [
        {
            "commit": "40ae83411b275b68425b5c85a7243c69599edc59",
            "timestamp": "2023-04-30T23:28:44-04:00",
            "author": "warren",
            "commit_message": "BUG: random: Don't return negative values from Generator.geometric.\n\nIn C, when the integer part of a double exceeds the maximum int64, casting\nthe double to int64_t is undefined behavior.  On some platforms, the result\nis 2**64-1 and on others it is -2**64.  That results in rng.geometric()\nreturning -2**64 when p was sufficiently small on some platforms.\n\nThe fix is to never invoke undefined behavior by ensuring we don't attempt\nto cast very large double values to int64_t.",
            "additions": 7,
            "deletions": 0,
            "change_type": "MODIFY",
            "diff": "@@ -148,3 +148,10 @@ def test_gamma_0(self):\n         actual = mt19937.standard_gamma([0.0], dtype='float')\n         expected = np.array([0.], dtype=np.float32)\n         assert_array_equal(actual, expected)\n+\n+    def test_geometric_tiny_prob(self):\n+        # Regression test for gh-17007.\n+        # When p = 1e-30, the probability that a sample will exceed 2**63-1\n+        # is 0.9999999999907766, so we expect the result to be all 2**63-1.\n+        assert_array_equal(mt19937.geometric(p=1e-30, size=3),\n+                           np.iinfo(np.int64).max)\n",
            "comment_added_diff": {
                "153": "        # Regression test for gh-17007.",
                "154": "        # When p = 1e-30, the probability that a sample will exceed 2**63-1",
                "155": "        # is 0.9999999999907766, so we expect the result to be all 2**63-1."
            },
            "comment_deleted_diff": {},
            "comment_modified_diff": {}
        },
        {
            "commit": "f7e122354aa1f923521a495a721d11bfe534134e",
            "timestamp": "2023-07-17T15:10:52-04:00",
            "author": "warren",
            "commit_message": "BUG: random: Fix check for both uniform variates being 0 in random_beta()\n\nCloses gh-24203",
            "additions": 4,
            "deletions": 0,
            "change_type": "MODIFY",
            "diff": "@@ -75,6 +75,10 @@ def test_beta_small_parameters(self):\n         x = self.mt19937.beta(0.0001, 0.0001, size=100)\n         assert_(not np.any(np.isnan(x)), 'Nans in mt19937.beta')\n \n+    def test_beta_very_small_parameters(self):\n+        # gh-24203: beta would hang with very small parameters.\n+        self.mt19937.beta(1e-49, 1e-40)\n+\n     def test_choice_sum_of_probs_tolerance(self):\n         # The sum of probs should be 1.0 with some tolerance.\n         # For low precision dtypes the tolerance was too tight.\n",
            "comment_added_diff": {
                "79": "        # gh-24203: beta would hang with very small parameters."
            },
            "comment_deleted_diff": {},
            "comment_modified_diff": {}
        },
        {
            "commit": "a0b45c5d62aed45ed26b8ae4360ee71443878e84",
            "timestamp": "2023-07-26T13:35:19-04:00",
            "author": "warren",
            "commit_message": "BUG: random: Fix generation of nan by beta.\n\nThe implementation of Johnk's algorithm for beta(a, b) could generate nan\nvalues if both a and b were extremely small (i.e. subnormal or a small\nmultiple of the smallest normal double precision float).\n\nThe fix is to handle variate generation in this case by noting that when\nboth a and b are extremely small, the probability of generating a double\nprecision value that is not either 0 or 1 is also extremely small.  In\nparticular, if a and b are less than 3e-103, the probability of generating\na (double precision) value that is not 0 or 1 is less than approximately\n1e-100.  So instead of using Johnk's algorithm in this extreme case, we\ncan generate the values 0 or 1 as Bernoulli trials, with the probability\nof 1 being a/(a + b).\n\nCloses gh-24266.",
            "additions": 7,
            "deletions": 0,
            "change_type": "MODIFY",
            "diff": "@@ -79,6 +79,13 @@ def test_beta_very_small_parameters(self):\n         # gh-24203: beta would hang with very small parameters.\n         self.mt19937.beta(1e-49, 1e-40)\n \n+    def test_beta_ridiculously_small_parameters(self):\n+        # gh-24266: beta would generate nan when the parameters\n+        # were subnormal or a small multiple of the smallest normal.\n+        tiny = np.finfo(1.0).tiny\n+        x = self.mt19937.beta(tiny/32, tiny/40, size=50)\n+        assert not np.any(np.isnan(x))\n+\n     def test_choice_sum_of_probs_tolerance(self):\n         # The sum of probs should be 1.0 with some tolerance.\n         # For low precision dtypes the tolerance was too tight.\n",
            "comment_added_diff": {
                "83": "        # gh-24266: beta would generate nan when the parameters",
                "84": "        # were subnormal or a small multiple of the smallest normal."
            },
            "comment_deleted_diff": {},
            "comment_modified_diff": {}
        }
    ],
    "bench_indexing.py": [
        {
            "commit": "41fbcdbafb6fd87253feda230fb2f46735148d53",
            "timestamp": "2023-05-05T14:27:51+02:00",
            "author": "Sebastian Berg",
            "commit_message": "BENCH: Add more indeixng benchmarks (and make existing dtype specific)",
            "additions": 30,
            "deletions": 7,
            "change_type": "MODIFY",
            "diff": "@@ -1,4 +1,5 @@\n-from .common import Benchmark, get_squares_, get_indexes_, get_indexes_rand_\n+from .common import (\n+    Benchmark, get_square_, get_indexes_, get_indexes_rand_, TYPES1)\n \n from os.path import join as pjoin\n import shutil\n@@ -8,29 +9,51 @@\n \n \n class Indexing(Benchmark):\n-    params = [[\"indexes_\", \"indexes_rand_\"],\n+    params = [TYPES1 + [\"object\", \"O,i\"],\n+              [\"indexes_\", \"indexes_rand_\"],\n               ['I', ':,I', 'np.ix_(I, I)'],\n               ['', '=1']]\n-    param_names = ['indexes', 'sel', 'op']\n+    param_names = ['dtype', 'indexes', 'sel', 'op']\n \n-    def setup(self, indexes, sel, op):\n+    def setup(self, dtype, indexes, sel, op):\n         sel = sel.replace('I', indexes)\n \n-        ns = {'squares_': get_squares_((\"object\", \"O,i\")),\n+        ns = {'a': get_square_(dtype),\n               'np': np,\n               'indexes_': get_indexes_(),\n               'indexes_rand_': get_indexes_rand_()}\n \n-        code = \"def run():\\n    for a in squares_.values(): a[%s]%s\"\n+        code = \"def run():\\n    a[%s]%s\"\n         code = code % (sel, op)\n \n         exec(code, ns)\n         self.func = ns['run']\n \n-    def time_op(self, indexes, sel, op):\n+    def time_op(self, dtype, indexes, sel, op):\n         self.func()\n \n \n+class IndexingWith1DArr(Benchmark):\n+    # Benchmark similar to the take one\n+    params = [\n+        [(1000,), (1000, 1), (1000, 2), (2, 1000, 1), (1000, 3)],\n+        TYPES1 + [\"O\", \"i,O\"]]\n+    param_names = [\"shape\", \"dtype\"]\n+\n+    def setup(self, shape, dtype):\n+        self.arr = np.ones(shape, dtype)\n+        self.index = np.arange(1000)\n+        # index the second dimension:\n+        if len(shape) == 3:\n+            self.index = (slice(None), self.index)\n+\n+    def time_getitem_ordered(self, shape, dtype):\n+        self.arr[self.index]\n+\n+    def time_setitem_ordered(self, shape, dtype):\n+        self.arr[self.index] = 0\n+\n+\n class ScalarIndexing(Benchmark):\n     params = [[0, 1, 2]]\n     param_names = [\"ndim\"]\n",
            "comment_added_diff": {
                "37": "    # Benchmark similar to the take one",
                "46": "        # index the second dimension:"
            },
            "comment_deleted_diff": {},
            "comment_modified_diff": {}
        }
    ],
    "lowlevel_strided_loops.c.src": [],
    "py3k.py": [
        {
            "commit": "181c15b294d6dd164e4c41ddbb1c5feae9b5beee",
            "timestamp": "2023-05-09T12:09:39+02:00",
            "author": "Hongyang Peng",
            "commit_message": "BUG: fix the method for checking local files (#23728)\n\nBufferedReader and BufferedWriter cannot be used to determine local\r\nfiles. For example, users can implement CustomFile to operate on OSS\r\nfiles, and then use BufferedReader(CustomFile) to achieve the buffered\r\neffect. But fileno method can do it.",
            "additions": 9,
            "deletions": 1,
            "change_type": "MODIFY",
            "diff": "@@ -47,7 +47,15 @@ def asstr(s):\n     return str(s)\n \n def isfileobj(f):\n-    return isinstance(f, (io.FileIO, io.BufferedReader, io.BufferedWriter))\n+    if not isinstance(f, (io.FileIO, io.BufferedReader, io.BufferedWriter)):\n+        return False\n+    try:\n+        # BufferedReader/Writer may raise OSError when\n+        # fetching `fileno()` (e.g. when wrapping BytesIO).\n+        f.fileno()\n+        return True\n+    except OSError:\n+        return False\n \n def open_latin1(filename, mode='r'):\n     return open(filename, mode=mode, encoding='iso-8859-1')\n",
            "comment_added_diff": {
                "53": "        # BufferedReader/Writer may raise OSError when",
                "54": "        # fetching `fileno()` (e.g. when wrapping BytesIO)."
            },
            "comment_deleted_diff": {},
            "comment_modified_diff": {}
        }
    ],
    "test_compat.py": [],
    "chebyshev.py": [],
    "hermite.py": [],
    "hermite_e.py": [],
    "laguerre.py": [],
    "legendre.py": [],
    "bench_scalar.py": [
        {
            "commit": "670842b38005febf64259f268332e46d86233ef0",
            "timestamp": "2023-05-10T21:41:48+03:00",
            "author": "mattip",
            "commit_message": "add fast path for str(scalar_int)",
            "additions": 10,
            "deletions": 0,
            "change_type": "MODIFY",
            "diff": "@@ -65,3 +65,13 @@ def time_add_other_and_int32arr(self, typename):\n         other + int32\n         other + int32\n         other + int32\n+\n+class ScalarStr(Benchmark):\n+    # Test scalar to str conversion\n+    params = [TYPES1]\n+    param_names = [\"type\"]\n+    def setup(self, typename):\n+        self.a = np.array([100] * 100, dtype=typename)\n+\n+    def time_str_repr(self, typename):\n+        res = [str(x) for x in self.a]\n",
            "comment_added_diff": {
                "70": "    # Test scalar to str conversion"
            },
            "comment_deleted_diff": {},
            "comment_modified_diff": {}
        }
    ],
    "gh23533.f": [],
    "arrayobject.h": [],
    "linalg.pyi": [],
    "ctypeslib.py": [
        {
            "commit": "00f633b8579878fa058586e3e412a5d0f82027a1",
            "timestamp": "2023-05-19T08:56:04+03:00",
            "author": "mattip",
            "commit_message": "ENH: replace depcrecated distutils use in convenience function",
            "additions": 13,
            "deletions": 15,
            "change_type": "MODIFY",
            "diff": "@@ -120,28 +120,26 @@ def load_library(libname, loader_path):\n             If there is no library with the expected extension, or the\n             library is defective and cannot be loaded.\n         \"\"\"\n-        if ctypes.__version__ < '1.0.1':\n-            import warnings\n-            warnings.warn(\"All features of ctypes interface may not work \"\n-                          \"with ctypes < 1.0.1\", stacklevel=2)\n-\n         # Convert path-like objects into strings\n         libname = os.fsdecode(libname)\n         loader_path = os.fsdecode(loader_path)\n \n         ext = os.path.splitext(libname)[1]\n         if not ext:\n+            import sys\n+            import sysconfig\n             # Try to load library with platform-specific name, otherwise\n-            # default to libname.[so|pyd].  Sometimes, these files are built\n-            # erroneously on non-linux platforms.\n-            from numpy.distutils.misc_util import get_shared_lib_extension\n-            so_ext = get_shared_lib_extension()\n-            libname_ext = [libname + so_ext]\n-            # mac, windows and linux >= py3.2 shared library and loadable\n-            # module have different extensions so try both\n-            so_ext2 = get_shared_lib_extension(is_python_ext=True)\n-            if not so_ext2 == so_ext:\n-                libname_ext.insert(0, libname + so_ext2)\n+            # default to libname.[so|dll|dylib].  Sometimes, these files are\n+            # built erroneously on non-linux platforms.\n+            base_ext = \".so\"\n+            if sys.platform.startswith(\"darwin\"):\n+                base_ext = \".dylib\"\n+            elif sys.platform.startswith(\"win\"):\n+                base_ext = \".dll\"\n+            libname_ext = [libname + base_ext]\n+            so_ext = sysconfig.get_config_var(\"EXT_SUFFIX\")\n+            if not so_ext == base_ext:\n+                libname_ext.insert(0, libname + so_ext)\n         else:\n             libname_ext = [libname]\n \n",
            "comment_added_diff": {
                "132": "            # default to libname.[so|dll|dylib].  Sometimes, these files are",
                "133": "            # built erroneously on non-linux platforms."
            },
            "comment_deleted_diff": {
                "135": "            # default to libname.[so|pyd].  Sometimes, these files are built",
                "136": "            # erroneously on non-linux platforms.",
                "140": "            # mac, windows and linux >= py3.2 shared library and loadable",
                "141": "            # module have different extensions so try both"
            },
            "comment_modified_diff": {}
        }
    ],
    "test_manipulation_functions.py": [],
    "_string_helpers.py": [],
    "18053.new_feature.rst": [],
    "18535.improvement.rst": [],
    "21120.new_feature.rst": [],
    "22315.performance.rst": [],
    "22539.deprecation.rst": [],
    "22637.compatibility.rst": [],
    "22644.new_feature.rst": [],
    "22707.compatibility.rst": [],
    "22707.expired.rst": [],
    "22707.improvement.rst": [],
    "22786.new_feature.rst": [],
    "22997.improvement.rst": [],
    "23020.change.rst": [],
    "23020.performance.rst": [],
    "23113.improvement.rst": [],
    "23136.performance.rst": [],
    "23195.improvement.rst": [],
    "23314.deprecation.rst": [],
    "23322.improvement.rst": [],
    "23371.improvement.rst": [],
    "23376.expired.rst": [],
    "23403.expired.rst": [],
    "23652.improvement.rst": [],
    "23661.performance.rst": [],
    "23666.expired.rst": [],
    "23713.improvement.rst": [],
    "2.0.0-notes.rst": [],
    "umath_tests.py": [
        {
            "commit": "1366f8e52e4162ca16b223c9607c84450607f756",
            "timestamp": "2023-05-25T10:25:39-07:00",
            "author": "Brigitta Sip\u0151cz",
            "commit_message": "MAINT: removing the deprecated submodule",
            "additions": 0,
            "deletions": 13,
            "change_type": "DELETE",
            "diff": "@@ -1,13 +0,0 @@\n-\"\"\"\n-Shim for _umath_tests to allow a deprecation period for the new name.\n-\n-\"\"\"\n-import warnings\n-\n-# 2018-04-04, numpy 1.15.0\n-warnings.warn((\"numpy.core.umath_tests is an internal NumPy \"\n-               \"module and should not be imported. It will \"\n-               \"be removed in a future NumPy release.\"),\n-              category=DeprecationWarning, stacklevel=2)\n-\n-from ._umath_tests import *\n",
            "comment_added_diff": {},
            "comment_deleted_diff": {
                "7": "# 2018-04-04, numpy 1.15.0"
            },
            "comment_modified_diff": {}
        }
    ],
    "nep-0053-c-abi-evolution.md": [],
    "randomkit.h": [],
    "download-wheels.py": [],
    "23861.change.rst": [],
    "23729.improvement.rst": [],
    "gh22648.pyf": [],
    "_dtypes.py": [],
    "test_array_object.py": [
        {
            "commit": "8b63fc295bafea3efd3d115964200dbaac7be8a5",
            "timestamp": "2023-06-05T16:25:42-06:00",
            "author": "Aaron Meurer",
            "commit_message": "Update numpy.array_api magic methods for complex numbers\n\nUpdates from the v2022.12 version of the spec:\n\n- Add __complex__.\n- __float__, __int__, and __bool__ are now more lenient in what dtypes they\n  can operate on.\n- Support complex scalars and dtypes in all operators (except those that\n  should not operate on complex numbers).\n- Disallow integer scalars that are out of the bounds of the array dtype.\n- Update the tests accordingly.",
            "additions": 46,
            "deletions": 26,
            "change_type": "MODIFY",
            "diff": "@@ -1,6 +1,6 @@\n import operator\n \n-from numpy.testing import assert_raises\n+from numpy.testing import assert_raises, suppress_warnings\n import numpy as np\n import pytest\n \n@@ -9,9 +9,12 @@\n from .._dtypes import (\n     _all_dtypes,\n     _boolean_dtypes,\n+    _real_floating_dtypes,\n     _floating_dtypes,\n+    _complex_floating_dtypes,\n     _integer_dtypes,\n     _integer_or_boolean_dtypes,\n+    _real_numeric_dtypes,\n     _numeric_dtypes,\n     int8,\n     int16,\n@@ -85,13 +88,13 @@ def test_operators():\n         \"__add__\": \"numeric\",\n         \"__and__\": \"integer_or_boolean\",\n         \"__eq__\": \"all\",\n-        \"__floordiv__\": \"numeric\",\n-        \"__ge__\": \"numeric\",\n-        \"__gt__\": \"numeric\",\n-        \"__le__\": \"numeric\",\n+        \"__floordiv__\": \"real numeric\",\n+        \"__ge__\": \"real numeric\",\n+        \"__gt__\": \"real numeric\",\n+        \"__le__\": \"real numeric\",\n         \"__lshift__\": \"integer\",\n-        \"__lt__\": \"numeric\",\n-        \"__mod__\": \"numeric\",\n+        \"__lt__\": \"real numeric\",\n+        \"__mod__\": \"real numeric\",\n         \"__mul__\": \"numeric\",\n         \"__ne__\": \"all\",\n         \"__or__\": \"integer_or_boolean\",\n@@ -101,7 +104,6 @@ def test_operators():\n         \"__truediv__\": \"floating\",\n         \"__xor__\": \"integer_or_boolean\",\n     }\n-\n     # Recompute each time because of in-place ops\n     def _array_vals():\n         for d in _integer_dtypes:\n@@ -111,13 +113,15 @@ def _array_vals():\n         for d in _floating_dtypes:\n             yield asarray(1.0, dtype=d)\n \n+\n+    BIG_INT = int(1e30)\n     for op, dtypes in binary_op_dtypes.items():\n         ops = [op]\n         if op not in [\"__eq__\", \"__ne__\", \"__le__\", \"__ge__\", \"__lt__\", \"__gt__\"]:\n             rop = \"__r\" + op[2:]\n             iop = \"__i\" + op[2:]\n             ops += [rop, iop]\n-        for s in [1, 1.0, False]:\n+        for s in [1, 1.0, 1j, BIG_INT, False]:\n             for _op in ops:\n                 for a in _array_vals():\n                     # Test array op scalar. From the spec, the following combinations\n@@ -125,13 +129,12 @@ def _array_vals():\n \n                     # - Python bool for a bool array dtype,\n                     # - a Python int within the bounds of the given dtype for integer array dtypes,\n-                    # - a Python int or float for floating-point array dtypes\n-\n-                    # We do not do bounds checking for int scalars, but rather use the default\n-                    # NumPy behavior for casting in that case.\n+                    # - a Python int or float for real floating-point array dtypes\n+                    # - a Python int, float, or complex for complex floating-point array dtypes\n \n                     if ((dtypes == \"all\"\n                          or dtypes == \"numeric\" and a.dtype in _numeric_dtypes\n+                         or dtypes == \"real numeric\" and a.dtype in _real_numeric_dtypes\n                          or dtypes == \"integer\" and a.dtype in _integer_dtypes\n                          or dtypes == \"integer_or_boolean\" and a.dtype in _integer_or_boolean_dtypes\n                          or dtypes == \"boolean\" and a.dtype in _boolean_dtypes\n@@ -141,10 +144,18 @@ def _array_vals():\n                         # isinstance here.\n                         and (a.dtype in _boolean_dtypes and type(s) == bool\n                              or a.dtype in _integer_dtypes and type(s) == int\n-                             or a.dtype in _floating_dtypes and type(s) in [float, int]\n+                             or a.dtype in _real_floating_dtypes and type(s) in [float, int]\n+                             or a.dtype in _complex_floating_dtypes and type(s) in [complex, float, int]\n                         )):\n-                        # Only test for no error\n-                        getattr(a, _op)(s)\n+                        if a.dtype in _integer_dtypes and s == BIG_INT:\n+                            assert_raises(OverflowError, lambda: getattr(a, _op)(s))\n+                        else:\n+                            # Only test for no error\n+                            with suppress_warnings() as sup:\n+                                # ignore warnings from pow(BIG_INT)\n+                                sup.filter(RuntimeWarning,\n+                                           \"invalid value encountered in power\")\n+                                getattr(a, _op)(s)\n                     else:\n                         assert_raises(TypeError, lambda: getattr(a, _op)(s))\n \n@@ -174,8 +185,9 @@ def _array_vals():\n                             # Ensure only those dtypes that are required for every operator are allowed.\n                             elif (dtypes == \"all\" and (x.dtype in _boolean_dtypes and y.dtype in _boolean_dtypes\n                                                       or x.dtype in _numeric_dtypes and y.dtype in _numeric_dtypes)\n+                                or (dtypes == \"real numeric\" and x.dtype in _real_numeric_dtypes and y.dtype in _real_numeric_dtypes)\n                                 or (dtypes == \"numeric\" and x.dtype in _numeric_dtypes and y.dtype in _numeric_dtypes)\n-                                or dtypes == \"integer\" and x.dtype in _integer_dtypes and y.dtype in _numeric_dtypes\n+                                or dtypes == \"integer\" and x.dtype in _integer_dtypes and y.dtype in _integer_dtypes\n                                 or dtypes == \"integer_or_boolean\" and (x.dtype in _integer_dtypes and y.dtype in _integer_dtypes\n                                                                        or x.dtype in _boolean_dtypes and y.dtype in _boolean_dtypes)\n                                 or dtypes == \"boolean\" and x.dtype in _boolean_dtypes and y.dtype in _boolean_dtypes\n@@ -263,31 +275,39 @@ def test_python_scalar_construtors():\n     b = asarray(False)\n     i = asarray(0)\n     f = asarray(0.0)\n+    c = asarray(0j)\n \n     assert bool(b) == False\n     assert int(i) == 0\n     assert float(f) == 0.0\n     assert operator.index(i) == 0\n \n-    # bool/int/float should only be allowed on 0-D arrays.\n+    # bool/int/float/complex should only be allowed on 0-D arrays.\n     assert_raises(TypeError, lambda: bool(asarray([False])))\n     assert_raises(TypeError, lambda: int(asarray([0])))\n     assert_raises(TypeError, lambda: float(asarray([0.0])))\n+    assert_raises(TypeError, lambda: complex(asarray([0j])))\n     assert_raises(TypeError, lambda: operator.index(asarray([0])))\n \n-    # bool/int/float should only be allowed on arrays of the corresponding\n-    # dtype\n-    assert_raises(ValueError, lambda: bool(i))\n-    assert_raises(ValueError, lambda: bool(f))\n+    # bool should work on all types of arrays\n+    assert bool(b) is bool(i) is bool(f) is bool(c) is False\n+\n+    # int should fail on complex arrays\n+    assert int(b) == int(i) == int(f) == 0\n+    assert_raises(TypeError, lambda: int(c))\n \n-    assert_raises(ValueError, lambda: int(b))\n-    assert_raises(ValueError, lambda: int(f))\n+    # float should fail on complex arrays\n+    assert float(b) == float(i) == float(f) == 0.0\n+    assert_raises(TypeError, lambda: float(c))\n \n-    assert_raises(ValueError, lambda: float(b))\n-    assert_raises(ValueError, lambda: float(i))\n+    # complex should work on all types of arrays\n+    assert complex(b) == complex(i) == complex(f) == complex(c) == 0j\n \n+    # index should only work on integer arrays\n+    assert operator.index(i) == 0\n     assert_raises(TypeError, lambda: operator.index(b))\n     assert_raises(TypeError, lambda: operator.index(f))\n+    assert_raises(TypeError, lambda: operator.index(c))\n \n \n def test_device_property():\n",
            "comment_added_diff": {
                "132": "                    # - a Python int or float for real floating-point array dtypes",
                "133": "                    # - a Python int, float, or complex for complex floating-point array dtypes",
                "153": "                            # Only test for no error",
                "155": "                                # ignore warnings from pow(BIG_INT)",
                "285": "    # bool/int/float/complex should only be allowed on 0-D arrays.",
                "292": "    # bool should work on all types of arrays",
                "295": "    # int should fail on complex arrays",
                "299": "    # float should fail on complex arrays",
                "303": "    # complex should work on all types of arrays",
                "306": "    # index should only work on integer arrays"
            },
            "comment_deleted_diff": {
                "128": "                    # - a Python int or float for floating-point array dtypes",
                "130": "                    # We do not do bounds checking for int scalars, but rather use the default",
                "131": "                    # NumPy behavior for casting in that case.",
                "146": "                        # Only test for no error",
                "272": "    # bool/int/float should only be allowed on 0-D arrays.",
                "278": "    # bool/int/float should only be allowed on arrays of the corresponding",
                "279": "    # dtype"
            },
            "comment_modified_diff": {}
        }
    ],
    "_elementwise_functions.py": [],
    "test_elementwise_functions.py": [],
    "_statistical_functions.py": [
        {
            "commit": "e023bc611661bbed26292b098945170728e67d48",
            "timestamp": "2023-06-05T18:10:20-06:00",
            "author": "Aaron Meurer",
            "commit_message": "Update numpy.array_api sum() and prod() to handle complex dtypes",
            "additions": 17,
            "deletions": 10,
            "change_type": "MODIFY",
            "diff": "@@ -6,7 +6,7 @@\n )\n from ._array_object import Array\n from ._creation_functions import asarray\n-from ._dtypes import float32, float64\n+from ._dtypes import float32, float64, complex64, complex128\n \n from typing import TYPE_CHECKING, Optional, Tuple, Union\n \n@@ -62,10 +62,14 @@ def prod(\n ) -> Array:\n     if x.dtype not in _numeric_dtypes:\n         raise TypeError(\"Only numeric dtypes are allowed in prod\")\n-    # Note: sum() and prod() always upcast float32 to float64 for dtype=None\n-    # We need to do so here before computing the product to avoid overflow\n-    if dtype is None and x.dtype == float32:\n-        dtype = float64\n+    # Note: sum() and prod() always upcast for dtype=None. `np.prod` does that\n+    # for integers, but not for float32 or complex64, so we need to\n+    # special-case it here\n+    if dtype is None:\n+        if x.dtype == float32:\n+            dtype = float64\n+        elif x.dtype == complex64:\n+            dtype = complex128\n     return Array._new(np.prod(x._array, dtype=dtype, axis=axis, keepdims=keepdims))\n \n \n@@ -93,11 +97,14 @@ def sum(\n ) -> Array:\n     if x.dtype not in _numeric_dtypes:\n         raise TypeError(\"Only numeric dtypes are allowed in sum\")\n-    # Note: sum() and prod() always upcast integers to (u)int64 and float32 to\n-    # float64 for dtype=None. `np.sum` does that too for integers, but not for\n-    # float32, so we need to special-case it here\n-    if dtype is None and x.dtype == float32:\n-        dtype = float64\n+    # Note: sum() and prod() always upcast for dtype=None. `np.sum` does that\n+    # for integers, but not for float32 or complex64, so we need to\n+    # special-case it here\n+    if dtype is None:\n+        if x.dtype == float32:\n+            dtype = float64\n+        elif x.dtype == complex64:\n+            dtype = complex128\n     return Array._new(np.sum(x._array, axis=axis, dtype=dtype, keepdims=keepdims))\n \n \n",
            "comment_added_diff": {
                "65": "    # Note: sum() and prod() always upcast for dtype=None. `np.prod` does that",
                "66": "    # for integers, but not for float32 or complex64, so we need to",
                "67": "    # special-case it here",
                "100": "    # Note: sum() and prod() always upcast for dtype=None. `np.sum` does that",
                "101": "    # for integers, but not for float32 or complex64, so we need to",
                "102": "    # special-case it here"
            },
            "comment_deleted_diff": {
                "65": "    # Note: sum() and prod() always upcast float32 to float64 for dtype=None",
                "66": "    # We need to do so here before computing the product to avoid overflow",
                "96": "    # Note: sum() and prod() always upcast integers to (u)int64 and float32 to",
                "97": "    # float64 for dtype=None. `np.sum` does that too for integers, but not for",
                "98": "    # float32, so we need to special-case it here"
            },
            "comment_modified_diff": {
                "65": "    # Note: sum() and prod() always upcast float32 to float64 for dtype=None",
                "66": "    # We need to do so here before computing the product to avoid overflow",
                "67": "    if dtype is None and x.dtype == float32:",
                "100": "        dtype = float64"
            }
        }
    ],
    "_data_type_functions.py": [
        {
            "commit": "173fbc7009719ce802aa70634fb93031a0c00cfb",
            "timestamp": "2023-06-05T18:10:36-06:00",
            "author": "Aaron Meurer",
            "commit_message": "Add isdtype() to numpy.array_api\n\nThis is a new function in the v2022.12 version of the array API standard which\nis used for determining if a given dtype is part of a set of given dtype\ncategories. This will also eventually be added to the main NumPy namespace,\nbut for now only exists in numpy.array_api as a purely strict version.",
            "additions": 49,
            "deletions": 1,
            "change_type": "MODIFY",
            "diff": "@@ -1,7 +1,17 @@\n from __future__ import annotations\n \n from ._array_object import Array\n-from ._dtypes import _all_dtypes, _result_type\n+from ._dtypes import (\n+    _all_dtypes,\n+    _boolean_dtypes,\n+    _signed_integer_dtypes,\n+    _unsigned_integer_dtypes,\n+    _integer_dtypes,\n+    _real_floating_dtypes,\n+    _complex_floating_dtypes,\n+    _numeric_dtypes,\n+    _result_type,\n+)\n \n from dataclasses import dataclass\n from typing import TYPE_CHECKING, List, Tuple, Union\n@@ -117,6 +127,44 @@ def iinfo(type: Union[Dtype, Array], /) -> iinfo_object:\n     return iinfo_object(ii.bits, ii.max, ii.min)\n \n \n+# Note: isdtype is a new function from the 2022.12 array API specification.\n+def isdtype(\n+    dtype: Dtype, kind: Union[Dtype, str, Tuple[Union[Dtype, str], ...]]\n+) -> bool:\n+    \"\"\"\n+    Returns a boolean indicating whether a provided dtype is of a specified data type ``kind``.\n+\n+    See\n+    https://data-apis.org/array-api/latest/API_specification/generated/array_api.isdtype.html\n+    for more details\n+    \"\"\"\n+    if isinstance(kind, tuple):\n+        # Disallow nested tuples\n+        if any(isinstance(k, tuple) for k in kind):\n+            raise TypeError(\"'kind' must be a dtype, str, or tuple of dtypes and strs\")\n+        return any(isdtype(dtype, k) for k in kind)\n+    elif isinstance(kind, str):\n+        if kind == 'bool':\n+            return dtype in _boolean_dtypes\n+        elif kind == 'signed integer':\n+            return dtype in _signed_integer_dtypes\n+        elif kind == 'unsigned integer':\n+            return dtype in _unsigned_integer_dtypes\n+        elif kind == 'integral':\n+            return dtype in _integer_dtypes\n+        elif kind == 'real floating':\n+            return dtype in _real_floating_dtypes\n+        elif kind == 'complex floating':\n+            return dtype in _complex_floating_dtypes\n+        elif kind == 'numeric':\n+            return dtype in _numeric_dtypes\n+        else:\n+            raise ValueError(f\"Unrecognized data type kind: {kind!r}\")\n+    elif kind in _all_dtypes:\n+        return dtype == kind\n+    else:\n+        raise TypeError(f\"'kind' must be a dtype, str, or tuple of dtypes and strs, not {type(kind).__name__}\")\n+\n def result_type(*arrays_and_dtypes: Union[Array, Dtype]) -> Dtype:\n     \"\"\"\n     Array API compatible wrapper for :py:func:`np.result_type <numpy.result_type>`.\n",
            "comment_added_diff": {
                "130": "# Note: isdtype is a new function from the 2022.12 array API specification.",
                "142": "        # Disallow nested tuples"
            },
            "comment_deleted_diff": {},
            "comment_modified_diff": {}
        }
    ],
    "test_data_type_functions.py": [
        {
            "commit": "173fbc7009719ce802aa70634fb93031a0c00cfb",
            "timestamp": "2023-06-05T18:10:36-06:00",
            "author": "Aaron Meurer",
            "commit_message": "Add isdtype() to numpy.array_api\n\nThis is a new function in the v2022.12 version of the array API standard which\nis used for determining if a given dtype is part of a set of given dtype\ncategories. This will also eventually be added to the main NumPy namespace,\nbut for now only exists in numpy.array_api as a purely strict version.",
            "additions": 13,
            "deletions": 1,
            "change_type": "MODIFY",
            "diff": "@@ -1,7 +1,8 @@\n import pytest\n \n+from numpy.testing import assert_raises\n from numpy import array_api as xp\n-\n+import numpy as np\n \n @pytest.mark.parametrize(\n     \"from_, to, expected\",\n@@ -17,3 +18,14 @@ def test_can_cast(from_, to, expected):\n     can_cast() returns correct result\n     \"\"\"\n     assert xp.can_cast(from_, to) == expected\n+\n+def test_isdtype_strictness():\n+    assert_raises(TypeError, lambda: xp.isdtype(xp.float64, 64))\n+    assert_raises(ValueError, lambda: xp.isdtype(xp.float64, 'f8'))\n+\n+    assert_raises(TypeError, lambda: xp.isdtype(xp.float64, (('integral',),)))\n+    assert_raises(TypeError, lambda: xp.isdtype(xp.float64, np.object_))\n+\n+    # TODO: These will require https://github.com/numpy/numpy/issues/23883\n+    # assert_raises(TypeError, lambda: xp.isdtype(xp.float64, None))\n+    # assert_raises(TypeError, lambda: xp.isdtype(xp.float64, np.float64))\n",
            "comment_added_diff": {
                "29": "    # TODO: These will require https://github.com/numpy/numpy/issues/23883",
                "30": "    # assert_raises(TypeError, lambda: xp.isdtype(xp.float64, None))",
                "31": "    # assert_raises(TypeError, lambda: xp.isdtype(xp.float64, np.float64))"
            },
            "comment_deleted_diff": {},
            "comment_modified_diff": {}
        }
    ],
    "_searching_functions.py": [],
    "_sorting_functions.py": [],
    "test_scalar_ctors.py": [],
    "23871.change.rst": [],
    "noprefix.h": [],
    "npy_interrupt.h": [],
    "23921.expired.rst": [],
    "data_memory.rst": [],
    "ufunc.rst": [],
    "extobj.c": [],
    "umath.py": [],
    "_ufunc.pyi": [],
    "fromnumeric.pyi": [],
    "ufuncs.py": [],
    "ufuncs.rst": [],
    "umathmodule.h": [],
    "extobj.h": [],
    "umathmodule.c": [],
    "constants.pyi": [],
    "23936.change.rst": [],
    "23936.improvement.rst": [],
    "_convertions.py": [],
    "methods.h": [],
    "lib_utils.pyi": [],
    "lib_utils.py": [],
    "routines.err.rst": [],
    "gh23879.f90": [],
    "repair_windows.sh": [],
    "_shell_utils.py": [],
    "security.rst": [],
    "test_return_integer.py": [],
    "test_return_real.py": [],
    "_pytesttester.py": [
        {
            "commit": "595e78a0bf0725c9e5b7c09d11a7eb06b93f551e",
            "timestamp": "2023-06-19T19:51:33+02:00",
            "author": "Ralf Gommers",
            "commit_message": "BLD: don't use distutils on python >= 3.12",
            "additions": 7,
            "deletions": 6,
            "change_type": "MODIFY",
            "diff": "@@ -135,12 +135,13 @@ def __call__(self, label='fast', verbose=1, extra_argv=None,\n         # offset verbosity. The \"-q\" cancels a \"-v\".\n         pytest_args += [\"-q\"]\n \n-        with warnings.catch_warnings():\n-            warnings.simplefilter(\"always\")\n-            # Filter out distutils cpu warnings (could be localized to\n-            # distutils tests). ASV has problems with top level import,\n-            # so fetch module for suppression here.\n-            from numpy.distutils import cpuinfo\n+        if sys.version_info < (3, 12):\n+            with warnings.catch_warnings():\n+                warnings.simplefilter(\"always\")\n+                # Filter out distutils cpu warnings (could be localized to\n+                # distutils tests). ASV has problems with top level import,\n+                # so fetch module for suppression here.\n+                from numpy.distutils import cpuinfo\n \n         with warnings.catch_warnings(record=True):\n             # Ignore the warning from importing the array_api submodule. This\n",
            "comment_added_diff": {
                "141": "                # Filter out distutils cpu warnings (could be localized to",
                "142": "                # distutils tests). ASV has problems with top level import,",
                "143": "                # so fetch module for suppression here."
            },
            "comment_deleted_diff": {
                "140": "            # Filter out distutils cpu warnings (could be localized to",
                "141": "            # distutils tests). ASV has problems with top level import,",
                "142": "            # so fetch module for suppression here."
            },
            "comment_modified_diff": {
                "141": "            # distutils tests). ASV has problems with top level import,",
                "142": "            # so fetch module for suppression here.",
                "143": "            from numpy.distutils import cpuinfo"
            }
        }
    ],
    "test_ctypeslib.py": [
        {
            "commit": "c929d623a103bae377ad378cc6f8579728d9b0b5",
            "timestamp": "2023-06-19T19:51:36+02:00",
            "author": "Ralf Gommers",
            "commit_message": "TST: remove usages of `numpy.distutils` from tests for py312",
            "additions": 6,
            "deletions": 8,
            "change_type": "MODIFY",
            "diff": "@@ -1,11 +1,12 @@\n import sys\n-import pytest\n+import sysconfig\n import weakref\n from pathlib import Path\n \n+import pytest\n+\n import numpy as np\n from numpy.ctypeslib import ndpointer, load_library, as_array\n-from numpy.distutils.misc_util import get_shared_lib_extension\n from numpy.testing import assert_, assert_array_equal, assert_raises, assert_equal\n \n try:\n@@ -52,12 +53,9 @@ def test_basic2(self):\n         # Regression for #801: load_library with a full library name\n         # (including extension) does not work.\n         try:\n-            try:\n-                so = get_shared_lib_extension(is_python_ext=True)\n-                # Should succeed\n-                load_library('_multiarray_umath%s' % so, np.core._multiarray_umath.__file__)\n-            except ImportError:\n-                print(\"No distutils available, skipping test.\")\n+            so_ext = sysconfig.get_config_var('EXT_SUFFIX')\n+            load_library('_multiarray_umath%s' % so_ext,\n+                         np.core._multiarray_umath.__file__)\n         except ImportError as e:\n             msg = (\"ctypes is not available on this python: skipping the test\"\n                    \" (import error was: %s)\" % str(e))\n",
            "comment_added_diff": {},
            "comment_deleted_diff": {
                "57": "                # Should succeed"
            },
            "comment_modified_diff": {}
        }
    ],
    "test_isfile.py": [],
    "test_array_interface.py": [
        {
            "commit": "e42fc93b54a6d41dab72d86921f96e5ebc4c4198",
            "timestamp": "2023-06-19T19:51:36+02:00",
            "author": "Ralf Gommers",
            "commit_message": "TST: skip memory allocator and `array_interface` tests on py312\n\nThey require numpy.distutils, which isn't available on >=3.12\nThe `numpy.testing.extbuild` utility will need changing to make this\nwork again. Could either use plain `setuptools` or `meson`.",
            "additions": 3,
            "deletions": 0,
            "change_type": "MODIFY",
            "diff": "@@ -128,6 +128,9 @@ def get_module(tmp_path):\n                                                more_init=more_init)\n \n \n+# FIXME: numpy.testing.extbuild uses `numpy.distutils`, so this won't work on\n+# Python 3.12 and up.\n+@pytest.mark.skipif(sys.version_info >= (3, 12), reason=\"no numpy.distutils\")\n @pytest.mark.slow\n def test_cstruct(get_module):\n \n",
            "comment_added_diff": {
                "131": "# FIXME: numpy.testing.extbuild uses `numpy.distutils`, so this won't work on",
                "132": "# Python 3.12 and up."
            },
            "comment_deleted_diff": {},
            "comment_modified_diff": {}
        },
        {
            "commit": "e290d1ca2568599b0cf434a1f016e54b4da9e8cb",
            "timestamp": "2023-07-13T16:48:36+03:00",
            "author": "Matti Picus",
            "commit_message": "TST: fix: using meson with windows, MSVC",
            "additions": 3,
            "deletions": 4,
            "change_type": "MODIFY",
            "diff": "@@ -1,7 +1,7 @@\n import sys\n import pytest\n import numpy as np\n-from numpy.testing import extbuild\n+from numpy.testing import extbuild, IS_WASM\n \n \n @pytest.fixture\n@@ -12,6 +12,8 @@ def get_module(tmp_path):\n \n     if not sys.platform.startswith('linux'):\n         pytest.skip('link fails on cygwin')\n+    if IS_WASM:\n+        pytest.skip(\"Can't build module inside Wasm\")\n \n     prologue = '''\n         #include <Python.h>\n@@ -128,9 +130,6 @@ def get_module(tmp_path):\n                                                more_init=more_init)\n \n \n-# FIXME: numpy.testing.extbuild uses `numpy.distutils`, so this won't work on\n-# Python 3.12 and up.\n-@pytest.mark.skipif(sys.version_info >= (3, 12), reason=\"no numpy.distutils\")\n @pytest.mark.slow\n def test_cstruct(get_module):\n \n",
            "comment_added_diff": {},
            "comment_deleted_diff": {
                "131": "# FIXME: numpy.testing.extbuild uses `numpy.distutils`, so this won't work on",
                "132": "# Python 3.12 and up."
            },
            "comment_modified_diff": {}
        }
    ],
    "23998.expired.rst": [],
    "functions_missing_types.py": [],
    "23762.new_function.rst": [],
    "getset.c": [],
    "shape.h": [],
    "test_arrayobject.py": [],
    "cmds.py": [
        {
            "commit": "da4929eb520a6dd2efee363d56266c0161afc8ac",
            "timestamp": "2023-07-01T10:57:07-07:00",
            "author": "Stefan van der Walt",
            "commit_message": "Satisfy linter",
            "additions": 1,
            "deletions": 2,
            "change_type": "MODIFY",
            "diff": "@@ -75,7 +75,6 @@ def docs(ctx, sphinx_target, clean, first_build, jobs, install_deps):\n     default=\"auto\",\n     help=\"Number of parallel jobs for testing; use all CPUs by default\"\n )\n-\n @click.pass_context\n def test(ctx, pytest_args, markexpr, n_jobs):\n     \"\"\"\ud83d\udd27 Run tests\n@@ -109,7 +108,7 @@ def test(ctx, pytest_args, markexpr, n_jobs):\n      spin test -- -k \"geometric and not rgeometric\"\n \n     For more, see `pytest --help`.\n-    \"\"\"\n+    \"\"\" # noqa: E501\n     if not pytest_args:\n         pytest_args = ('numpy',)\n \n",
            "comment_added_diff": {
                "111": "    \"\"\" # noqa: E501"
            },
            "comment_deleted_diff": {},
            "comment_modified_diff": {}
        },
        {
            "commit": "037bea06a60716dce2c907c7829053596ec09fac",
            "timestamp": "2023-07-01T11:04:45-07:00",
            "author": "Stefan van der Walt",
            "commit_message": "More satisfied linter",
            "additions": 1,
            "deletions": 1,
            "change_type": "MODIFY",
            "diff": "@@ -108,7 +108,7 @@ def test(ctx, pytest_args, markexpr, n_jobs):\n      spin test -- -k \"geometric and not rgeometric\"\n \n     For more, see `pytest --help`.\n-    \"\"\" # noqa: E501\n+    \"\"\"  # noqa: E501\n     if not pytest_args:\n         pytest_args = ('numpy',)\n \n",
            "comment_added_diff": {
                "111": "    \"\"\"  # noqa: E501"
            },
            "comment_deleted_diff": {
                "111": "    \"\"\" # noqa: E501"
            },
            "comment_modified_diff": {
                "111": "    \"\"\" # noqa: E501"
            }
        },
        {
            "commit": "cc1b2ecd9886105138b9eb59122812005300b66a",
            "timestamp": "2023-07-04T13:34:52+02:00",
            "author": "Stefan van der Walt",
            "commit_message": "ENH: Add `spin benchmark` command (#24107)\n\nThis is based off of SciPy's `dev.py spin` command, with slightly\r\nmodified argument parsing.\r\n\r\nIt also changes the ASV config to build using Meson.",
            "additions": 195,
            "deletions": 0,
            "change_type": "MODIFY",
            "diff": "@@ -152,3 +152,198 @@ def gdb(python_expr):\n          'python', '-P', '-c', python_expr],\n         replace=True\n     )\n+\n+\n+# From scipy: benchmarks/benchmarks/common.py\n+def _set_mem_rlimit(max_mem=None):\n+    \"\"\"\n+    Set address space rlimit\n+    \"\"\"\n+    import resource\n+    import psutil\n+\n+    mem = psutil.virtual_memory()\n+\n+    if max_mem is None:\n+        max_mem = int(mem.total * 0.7)\n+    cur_limit = resource.getrlimit(resource.RLIMIT_AS)\n+    if cur_limit[0] > 0:\n+        max_mem = min(max_mem, cur_limit[0])\n+\n+    try:\n+        resource.setrlimit(resource.RLIMIT_AS, (max_mem, cur_limit[1]))\n+    except ValueError:\n+        # on macOS may raise: current limit exceeds maximum limit\n+        pass\n+\n+\n+def _commit_to_sha(commit):\n+    p = util.run(['git', 'rev-parse', commit], output=False, echo=False)\n+    if p.returncode != 0:\n+        raise(\n+            click.ClickException(\n+                f'Could not find SHA matching commit `{commit}`'\n+            )\n+        )\n+\n+    return p.stdout.decode('ascii').strip()\n+\n+\n+def _dirty_git_working_dir():\n+    # Changes to the working directory\n+    p0 = util.run(['git', 'diff-files', '--quiet'])\n+\n+    # Staged changes\n+    p1 = util.run(['git', 'diff-index', '--quiet', '--cached', 'HEAD'])\n+\n+    return (p0.returncode != 0 or p1.returncode != 0)\n+\n+\n+def _run_asv(cmd):\n+    # Always use ccache, if installed\n+    PATH = os.environ['PATH']\n+    EXTRA_PATH = os.pathsep.join([\n+        '/usr/lib/ccache', '/usr/lib/f90cache',\n+        '/usr/local/lib/ccache', '/usr/local/lib/f90cache'\n+    ])\n+    env = os.environ\n+    env['PATH'] = f'EXTRA_PATH:{PATH}'\n+\n+    # Control BLAS/LAPACK threads\n+    env['OPENBLAS_NUM_THREADS'] = '1'\n+    env['MKL_NUM_THREADS'] = '1'\n+\n+    # Limit memory usage\n+    try:\n+        _set_mem_rlimit()\n+    except (ImportError, RuntimeError):\n+        pass\n+\n+    try:\n+        util.run(cmd, cwd='benchmarks', env=env, sys_exit=False)\n+    except FileNotFoundError:\n+        click.secho((\n+            \"Cannot find `asv`. \"\n+            \"Please install Airspeed Velocity:\\n\\n\"\n+            \"  https://asv.readthedocs.io/en/latest/installing.html\\n\"\n+            \"\\n\"\n+            \"Depending on your system, one of the following should work:\\n\\n\"\n+            \"  pip install asv\\n\"\n+            \"  conda install asv\\n\"\n+        ), fg=\"red\")\n+        sys.exit(1)\n+\n+\n+@click.command()\n+@click.option(\n+    '--tests', '-t',\n+    default=None, metavar='TESTS', multiple=True,\n+    help=\"Which tests to run\"\n+)\n+@click.option(\n+    '--compare', '-c',\n+    is_flag=True,\n+    default=False,\n+    help=\"Compare benchmarks between the current branch and main \"\n+         \"(unless other branches specified). \"\n+         \"The benchmarks are each executed in a new isolated \"\n+         \"environment.\"\n+)\n+@click.option(\n+    '--verbose', '-v', is_flag=True, default=False\n+)\n+@click.argument(\n+    'commits', metavar='',\n+    required=False,\n+    nargs=-1\n+)\n+@click.pass_context\n+def bench(ctx, tests, compare, verbose, commits):\n+    \"\"\"\ud83c\udfcb Run benchmarks.\n+\n+    \\b\n+    Examples:\n+\n+    \\b\n+    $ spin bench -t bench_lib\n+    $ spin bench -t bench_random.Random\n+    $ spin bench -t Random -t Shuffle\n+\n+    Two benchmark runs can be compared.\n+    By default, `HEAD` is compared to `main`.\n+    You can also specify the branches/commits to compare:\n+\n+    \\b\n+    $ spin bench --compare\n+    $ spin bench --compare main\n+    $ spin bench --compare main HEAD\n+\n+    You can also choose which benchmarks to run in comparison mode:\n+\n+    $ spin bench -t Random --compare\n+    \"\"\"\n+    if not commits:\n+        commits = ('main', 'HEAD')\n+    elif len(commits) == 1:\n+        commits = commits + ('HEAD',)\n+    elif len(commits) > 2:\n+        raise click.ClickException(\n+            'Need a maximum of two revisions to compare'\n+        )\n+\n+    bench_args = []\n+    for t in tests:\n+        bench_args += ['--bench', t]\n+\n+    if verbose:\n+        bench_args = ['-v'] + bench_args\n+\n+    if not compare:\n+        # No comparison requested; we build and benchmark the current version\n+\n+        click.secho(\n+            \"Invoking `build` prior to running benchmarks:\",\n+            bold=True, fg=\"bright_green\"\n+        )\n+        ctx.invoke(meson.build)\n+\n+        meson._set_pythonpath()\n+\n+        p = util.run(\n+            ['python', '-c', 'import numpy as np; print(np.__version__)'],\n+            cwd='benchmarks',\n+            echo=False,\n+            output=False\n+        )\n+        os.chdir('..')\n+\n+        np_ver = p.stdout.strip().decode('ascii')\n+        click.secho(\n+            f'Running benchmarks on NumPy {np_ver}',\n+            bold=True, fg=\"bright_green\"\n+        )\n+        cmd = [\n+            'asv', 'run', '--dry-run', '--show-stderr', '--python=same'\n+        ] + bench_args\n+\n+        _run_asv(cmd)\n+\n+    else:\n+        # Benchmark comparison\n+\n+        # Ensure that we don't have uncommited changes\n+        commit_a, commit_b = [_commit_to_sha(c) for c in commits]\n+\n+        if commit_b == 'HEAD':\n+            if _dirty_git_working_dir():\n+                click.secho(\n+                    \"WARNING: you have uncommitted changes --- \"\n+                    \"these will NOT be benchmarked!\",\n+                    fg=\"red\"\n+                )\n+\n+        cmd_compare = [\n+            'asv', 'continuous', '--factor', '1.05',\n+        ] + bench_args + [commit_a, commit_b]\n+\n+        _run_asv(cmd_compare)\n",
            "comment_added_diff": {
                "157": "# From scipy: benchmarks/benchmarks/common.py",
                "176": "        # on macOS may raise: current limit exceeds maximum limit",
                "193": "    # Changes to the working directory",
                "196": "    # Staged changes",
                "203": "    # Always use ccache, if installed",
                "212": "    # Control BLAS/LAPACK threads",
                "216": "    # Limit memory usage",
                "302": "        # No comparison requested; we build and benchmark the current version",
                "332": "        # Benchmark comparison",
                "334": "        # Ensure that we don't have uncommited changes"
            },
            "comment_deleted_diff": {},
            "comment_modified_diff": {}
        },
        {
            "commit": "27048ee514b340c1f95cf60061040516da578a28",
            "timestamp": "2023-07-05T10:44:10-07:00",
            "author": "Stefan van der Walt",
            "commit_message": "`import numpy as np` in `spin ipython`\n\nAlso run `build` before all run commands (`run`, `python`, `ipython`).",
            "additions": 82,
            "deletions": 0,
            "change_type": "MODIFY",
            "diff": "@@ -2,6 +2,9 @@\n import shutil\n import sys\n import argparse\n+import tempfile\n+import pathlib\n+import shutil\n \n import click\n from spin.cmds import meson\n@@ -347,3 +350,82 @@ def bench(ctx, tests, compare, verbose, commits):\n         ] + bench_args + [commit_a, commit_b]\n \n         _run_asv(cmd_compare)\n+\n+\n+@click.command(context_settings={\n+    'ignore_unknown_options': True\n+})\n+@click.argument(\"python_args\", metavar='', nargs=-1)\n+@click.pass_context\n+def python(ctx, python_args):\n+    \"\"\"\ud83d\udc0d Launch Python shell with PYTHONPATH set\n+\n+    OPTIONS are passed through directly to Python, e.g.:\n+\n+    spin python -c 'import sys; print(sys.path)'\n+    \"\"\"\n+    env = os.environ\n+    env['PYTHONWARNINGS'] = env.get('PYTHONWARNINGS', 'all')\n+    ctx.invoke(meson.build)\n+    ctx.forward(meson.python)\n+\n+\n+@click.command(context_settings={\n+    'ignore_unknown_options': True\n+})\n+@click.argument(\"ipython_args\", metavar='', nargs=-1)\n+@click.pass_context\n+def ipython(ctx, ipython_args):\n+    \"\"\"\ud83d\udcbb Launch IPython shell with PYTHONPATH set\n+\n+    OPTIONS are passed through directly to IPython, e.g.:\n+\n+    spin ipython -i myscript.py\n+    \"\"\"\n+    env = os.environ\n+    env['PYTHONWARNINGS'] = env.get('PYTHONWARNINGS', 'all')\n+\n+    ctx.invoke(meson.build)\n+\n+    ppath = meson._set_pythonpath()\n+\n+    # Get NumPy version\n+    p = util.run(\n+        [sys.executable, '-c', 'import sys; sys.path.pop(0); import numpy; print(numpy.__version__)'],\n+        output=False, echo=False\n+    )\n+    np_ver = p.stdout.strip().decode('ascii')\n+\n+    with tempfile.TemporaryDirectory() as d:\n+        profile_dir = os.path.join(d, f'numpy_{np_ver}')\n+        startup_dir = os.path.join(profile_dir, 'startup')\n+\n+        pathlib.Path(startup_dir).mkdir(parents=True)\n+        with open(os.path.join(startup_dir, '00_numpy.py'), 'w') as f:\n+            f.write('import numpy as np\\n')\n+\n+        print(f'\ud83d\udcbb Launching IPython with PYTHONPATH=\"{ppath}\"')\n+        util.run([\"ipython\", \"--profile-dir\", profile_dir, \"--ignore-cwd\"] + list(ipython_args))\n+\n+\n+@click.command(context_settings={\"ignore_unknown_options\": True})\n+@click.argument(\"args\", nargs=-1)\n+@click.pass_context\n+def run(ctx, args):\n+    \"\"\"\ud83c\udfc1 Run a shell command with PYTHONPATH set\n+\n+    \\b\n+    spin run make\n+    spin run 'echo $PYTHONPATH'\n+    spin run python -c 'import sys; del sys.path[0]; import mypkg'\n+\n+    If you'd like to expand shell variables, like `$PYTHONPATH` in the example\n+    above, you need to provide a single, quoted command to `run`:\n+\n+    spin run 'echo $SHELL && echo $PWD'\n+\n+    On Windows, all shell commands are run via Bash.\n+    Install Git for Windows if you don't have Bash already.\n+    \"\"\"\n+    ctx.invoke(meson.build)\n+    ctx.forward(meson.run)\n",
            "comment_added_diff": {
                "392": "    # Get NumPy version"
            },
            "comment_deleted_diff": {},
            "comment_modified_diff": {}
        },
        {
            "commit": "787f0287893c0be80741b46153e2f39318c18bc0",
            "timestamp": "2023-07-25T09:43:58+02:00",
            "author": "Sebastian Berg",
            "commit_message": "DEV: Use `exec_lines` and not profile dir for `spin ipython`\n\n`spin ipython` is annoying me by using a temporary profile dir which\nmeans that every invocation has its own history file.\n\nTrying things out, it seems like `--TerminalIPythonApp.exec_lines=`\nis a safe way to achieve the same thing (import numpy before anything\nelse) while not interfering with `-c \"code\"` (or even more exec lines\nstatements).",
            "additions": 6,
            "deletions": 19,
            "change_type": "MODIFY",
            "diff": "@@ -425,25 +425,12 @@ def ipython(ctx, ipython_args):\n \n     ppath = meson._set_pythonpath()\n \n-    # Get NumPy version\n-    p = util.run([\n-        sys.executable, '-c',\n-        'import sys; sys.path.pop(0); import numpy; print(numpy.__version__)'],\n-        output=False, echo=False\n-    )\n-    np_ver = p.stdout.strip().decode('ascii')\n-\n-    with tempfile.TemporaryDirectory() as d:\n-        profile_dir = os.path.join(d, f'numpy_{np_ver}')\n-        startup_dir = os.path.join(profile_dir, 'startup')\n-\n-        pathlib.Path(startup_dir).mkdir(parents=True)\n-        with open(os.path.join(startup_dir, '00_numpy.py'), 'w') as f:\n-            f.write('import numpy as np\\n')\n-\n-        print(f'\ud83d\udcbb Launching IPython with PYTHONPATH=\"{ppath}\"')\n-        util.run([\"ipython\", \"--profile-dir\", profile_dir, \"--ignore-cwd\"] +\n-                 list(ipython_args))\n+    print(f'\ud83d\udcbb Launching IPython with PYTHONPATH=\"{ppath}\"')\n+    preimport = (r\"import numpy as np; \"\n+                 r\"print(f'\\nPreimported NumPy {np.__version__} as np')\")\n+    util.run([\"ipython\", \"--ignore-cwd\",\n+              f\"--TerminalIPythonApp.exec_lines={preimport}\"] +\n+             list(ipython_args))\n \n \n @click.command(context_settings={\"ignore_unknown_options\": True})\n",
            "comment_added_diff": {},
            "comment_deleted_diff": {
                "428": "    # Get NumPy version"
            },
            "comment_modified_diff": {}
        },
        {
            "commit": "4d05cd1ef67f6c9b4f0bb63d69d388f7651c62bf",
            "timestamp": "2023-08-10T12:43:44+02:00",
            "author": "Ralf Gommers",
            "commit_message": "BLD: build with our numpy fork of meson",
            "additions": 48,
            "deletions": 0,
            "change_type": "MODIFY",
            "diff": "@@ -11,6 +11,54 @@\n from spin import util\n \n \n+# The numpy-vendored version of Meson. Put the directory that the executable\n+# `meson` is in at the front of the PATH.\n+curdir = pathlib.Path(__file__).parent.resolve()\n+meson_executable_dir = str(curdir.parent / 'vendored-meson' / 'entrypoint')\n+os.environ['PATH'] = meson_executable_dir + os.pathsep + os.environ['PATH']\n+\n+# Check that the meson git submodule is present\n+meson_import_dir = curdir.parent / 'vendored-meson' / 'meson' / 'mesonbuild'\n+if not meson_import_dir.exists():\n+    raise RuntimeError(\n+        'The `vendored-meson/meson` git submodule does not exist! ' +\n+        'Run `git submodule update --init` to fix this problem.'\n+    )\n+\n+\n+@click.command()\n+@click.option(\n+    \"-j\", \"--jobs\",\n+    help=\"Number of parallel tasks to launch\",\n+    type=int\n+)\n+@click.option(\n+    \"--clean\", is_flag=True,\n+    help=\"Clean build directory before build\"\n+)\n+@click.option(\n+    \"-v\", \"--verbose\", is_flag=True,\n+    help=\"Print all build output, even installation\"\n+)\n+@click.argument(\"meson_args\", nargs=-1)\n+@click.pass_context\n+def build(ctx, meson_args, jobs=None, clean=False, verbose=False):\n+    \"\"\"\ud83d\udd27 Build package with Meson/ninja and install\n+\n+    MESON_ARGS are passed through e.g.:\n+\n+    spin build -- -Dpkg_config_path=/lib64/pkgconfig\n+\n+    The package is installed to build-install\n+\n+    By default builds for release, to be able to use a debugger set CFLAGS\n+    appropriately. For example, for linux use\n+\n+    CFLAGS=\"-O0 -g\" spin build\n+    \"\"\"\n+    ctx.forward(meson.build)\n+\n+\n @click.command()\n @click.argument(\"sphinx_target\", default=\"html\")\n @click.option(\n",
            "comment_added_diff": {
                "14": "# The numpy-vendored version of Meson. Put the directory that the executable",
                "15": "# `meson` is in at the front of the PATH.",
                "20": "# Check that the meson git submodule is present"
            },
            "comment_deleted_diff": {},
            "comment_modified_diff": {}
        },
        {
            "commit": "2ecc508f2f09099f504c2f1b0959c7d81db1570e",
            "timestamp": "2023-08-11T15:55:59+02:00",
            "author": "Ralf Gommers",
            "commit_message": "DEV: vendor spin's meson.py, modified to use our vendored Meson\n\nNote that this is a bit messy. I tried to vendor spin's `meson.py`\nseparately, but it's not possible to do so cleanly as far as I can\ntell. The import machinery is unhappy bouncing around between\n`bin/spin`, `.spin/cmds.py and `.spin/meson.py`. So it was either\nfolding the `spin/cmds/meson.py` content all into cmds.py, or vendor all\nof the spin package. This seems to work.",
            "additions": 437,
            "deletions": 14,
            "change_type": "MODIFY",
            "diff": "@@ -5,11 +5,434 @@\n import tempfile\n import pathlib\n import shutil\n+import json\n+import pathlib\n \n import click\n-from spin.cmds import meson\n from spin import util\n \n+_run = util.run\n+\n+# START of spin/cmds/meson.py\n+install_dir = \"build-install\"\n+\n+# The numpy-vendored version of Meson\n+meson_cli = [sys.executable,\n+             str(pathlib.Path(__file__).parent.parent.resolve() /\n+                 'vendored-meson' / 'meson' / 'meson.py')\n+            ]\n+\n+\n+def _set_pythonpath(quiet=False):\n+    site_packages = _get_site_packages()\n+    env = os.environ\n+\n+    if \"PYTHONPATH\" in env:\n+        env[\"PYTHONPATH\"] = f\"{site_packages}{os.pathsep}{env['PYTHONPATH']}\"\n+    else:\n+        env[\"PYTHONPATH\"] = site_packages\n+\n+    if not quiet:\n+        click.secho(\n+            f'$ export PYTHONPATH=\"{site_packages}\"', bold=True, fg=\"bright_blue\"\n+        )\n+\n+    return env[\"PYTHONPATH\"]\n+\n+\n+def _get_site_packages():\n+    candidate_paths = []\n+    for root, dirs, _files in os.walk(install_dir):\n+        for subdir in dirs:\n+            if subdir == \"site-packages\" or subdir == \"dist-packages\":\n+                candidate_paths.append(os.path.abspath(os.path.join(root, subdir)))\n+\n+    X, Y = sys.version_info.major, sys.version_info.minor\n+\n+    site_packages = None\n+    if any(f\"python{X}.\" in p for p in candidate_paths):\n+        # We have a system that uses `python3.X/site-packages` or `python3.X/dist-packages`\n+        site_packages = [p for p in candidate_paths if f\"python{X}.{Y}\" in p]\n+        if len(site_packages) == 0:\n+            raise FileNotFoundError(\n+                f\"No site-packages found in {install_dir} for Python {X}.{Y}\"\n+            )\n+        else:\n+            site_packages = site_packages[0]\n+    else:\n+        # A naming scheme that does not encode the Python major/minor version is used, so return\n+        # whatever site-packages path was found\n+        if len(candidate_paths) > 1:\n+            raise FileNotFoundError(\n+                f\"Multiple `site-packages` found in `{install_dir}`, but cannot use Python version to disambiguate\"\n+            )\n+        elif len(candidate_paths) == 1:\n+            site_packages = candidate_paths[0]\n+\n+    if site_packages is None:\n+        raise FileNotFoundError(\n+            f\"No `site-packages` or `dist-packages` found under `{install_dir}`\"\n+        )\n+\n+    return site_packages\n+\n+\n+def _meson_version():\n+    try:\n+        p = _run(meson_cli + [\"--version\"], output=False, echo=False)\n+        return p.stdout.decode(\"ascii\").strip()\n+    except:\n+        pass\n+\n+\n+def _meson_version_configured():\n+    try:\n+        meson_info_fn = os.path.join(\"build\", \"meson-info\", \"meson-info.json\")\n+        meson_info = json.load(open(meson_info_fn))\n+        return meson_info[\"meson_version\"][\"full\"]\n+    except:\n+        pass\n+\n+\n+@click.command()\n+@click.option(\"-j\", \"--jobs\", help=\"Number of parallel tasks to launch\", type=int)\n+@click.option(\"--clean\", is_flag=True, help=\"Clean build directory before build\")\n+@click.option(\n+    \"-v\", \"--verbose\", is_flag=True, help=\"Print all build output, even installation\"\n+)\n+@click.argument(\"meson_args\", nargs=-1)\n+def meson_build(meson_args, jobs=None, clean=False, verbose=False):\n+    \"\"\"\ud83d\udd27 Build package with Meson/ninja and install\n+\n+    MESON_ARGS are passed through e.g.:\n+\n+    spin build -- -Dpkg_config_path=/lib64/pkgconfig\n+\n+    The package is installed to build-install\n+\n+    By default builds for release, to be able to use a debugger set CFLAGS\n+    appropriately. For example, for linux use\n+\n+    CFLAGS=\"-O0 -g\" spin build\n+    \"\"\"\n+    build_dir = \"build\"\n+    setup_cmd = meson_cli + [\"setup\", build_dir, \"--prefix=/usr\"] + list(meson_args)\n+\n+    if clean:\n+        print(f\"Removing `{build_dir}`\")\n+        if os.path.isdir(build_dir):\n+            shutil.rmtree(build_dir)\n+        print(f\"Removing `{install_dir}`\")\n+        if os.path.isdir(install_dir):\n+            shutil.rmtree(install_dir)\n+\n+    if not (os.path.exists(build_dir) and _meson_version_configured()):\n+        p = _run(setup_cmd, sys_exit=False)\n+        if p.returncode != 0:\n+            raise RuntimeError(\n+                \"Meson configuration failed; please try `spin build` again with the `--clean` flag.\"\n+            )\n+    else:\n+        # Build dir has been configured; check if it was configured by\n+        # current version of Meson\n+\n+        if _meson_version() != _meson_version_configured():\n+            _run(setup_cmd + [\"--reconfigure\"])\n+\n+        # Any other conditions that warrant a reconfigure?\n+\n+    p = _run(meson_cli + [\"compile\", \"-C\", build_dir], sys_exit=False)\n+    p = _run(meson_cli +\n+        [\n+            \"install\",\n+            \"--only-changed\",\n+            \"-C\",\n+            build_dir,\n+            \"--destdir\",\n+            f\"../{install_dir}\",\n+        ],\n+        output=verbose,\n+    )\n+\n+\n+def _get_configured_command(command_name):\n+    from spin.cmds.util import get_commands\n+    command_groups = get_commands()\n+    commands = [cmd for section in command_groups for cmd in command_groups[section]]\n+    return next((cmd for cmd in commands if cmd.name == command_name), None)\n+\n+\n+@click.command()\n+@click.argument(\"pytest_args\", nargs=-1)\n+@click.pass_context\n+def meson_test(ctx, pytest_args):\n+    \"\"\"\ud83d\udd27 Run tests\n+\n+    PYTEST_ARGS are passed through directly to pytest, e.g.:\n+\n+      spin test -- -v\n+\n+    To run tests on a directory or file:\n+\n+     \\b\n+     spin test numpy/linalg\n+     spin test numpy/linalg/tests/test_linalg.py\n+\n+    To run specific tests, by module, function, class, or method:\n+\n+     \\b\n+     spin test -- --pyargs numpy.random\n+     spin test -- --pyargs numpy.random.tests.test_generator_mt19937\n+     spin test -- --pyargs numpy.random.tests.test_generator_mt19937::TestMultivariateHypergeometric\n+     spin test -- --pyargs numpy.random.tests.test_generator_mt19937::TestMultivariateHypergeometric::test_edge_cases\n+\n+    To report the durations of the N slowest tests:\n+\n+      spin test -- --durations=N\n+\n+    To run tests that match a given pattern:\n+\n+     \\b\n+     spin test -- -k \"geometric\"\n+     spin test -- -k \"geometric and not rgeometric\"\n+\n+    To skip tests with a given marker:\n+\n+      spin test -- -m \"not slow\"\n+\n+    To parallelize test runs (requires `pytest-xdist`):\n+\n+      spin test -- -n NUM_JOBS\n+\n+    For more, see `pytest --help`.\n+\n+    \"\"\"\n+    from spin.cmds.util import get_config\n+    cfg = get_config()\n+\n+    build_cmd = _get_configured_command(\"build\")\n+    if build_cmd:\n+        click.secho(\n+            \"Invoking `build` prior to running tests:\", bold=True, fg=\"bright_green\"\n+        )\n+        ctx.invoke(build_cmd)\n+\n+    package = cfg.get(\"tool.spin.package\", None)\n+    if not pytest_args:\n+        pytest_args = (package,)\n+        if pytest_args == (None,):\n+            print(\n+                \"Please specify `package = packagename` under `tool.spin` section of `pyproject.toml`\"\n+            )\n+            sys.exit(1)\n+\n+    site_path = _set_pythonpath()\n+\n+    # Sanity check that library built properly\n+    if sys.version_info[:2] >= (3, 11):\n+        p = _run([sys.executable, \"-P\", \"-c\", f\"import {package}\"], sys_exit=False)\n+        if p.returncode != 0:\n+            print(f\"As a sanity check, we tried to import {package}.\")\n+            print(\"Stopping. Please investigate the build error.\")\n+            sys.exit(1)\n+\n+    print(f'$ export PYTHONPATH=\"{site_path}\"')\n+    _run(\n+        [sys.executable, \"-m\", \"pytest\", f\"--rootdir={site_path}\"] + list(pytest_args),\n+        cwd=site_path,\n+        replace=True,\n+    )\n+\n+\n+@click.command()\n+@click.argument(\"ipython_args\", nargs=-1)\n+def ipython(ipython_args):\n+    \"\"\"\ud83d\udcbb Launch IPython shell with PYTHONPATH set\n+\n+    IPYTHON_ARGS are passed through directly to IPython, e.g.:\n+\n+    spin ipython -- -i myscript.py\n+    \"\"\"\n+    p = _set_pythonpath()\n+    print(f'\ud83d\udcbb Launching IPython with PYTHONPATH=\"{p}\"')\n+    _run([\"ipython\", \"--ignore-cwd\"] + list(ipython_args), replace=True)\n+\n+\n+@click.command()\n+@click.argument(\"shell_args\", nargs=-1)\n+def meson_shell(shell_args=[]):\n+    \"\"\"\ud83d\udcbb Launch shell with PYTHONPATH set\n+\n+    SHELL_ARGS are passed through directly to the shell, e.g.:\n+\n+    spin shell -- -c 'echo $PYTHONPATH'\n+\n+    Ensure that your shell init file (e.g., ~/.zshrc) does not override\n+    the PYTHONPATH.\n+    \"\"\"\n+    p = _set_pythonpath()\n+    shell = os.environ.get(\"SHELL\", \"sh\")\n+    cmd = [shell] + list(shell_args)\n+    print(f'\ud83d\udcbb Launching shell with PYTHONPATH=\"{p}\"')\n+    print(\"\u26a0  Change directory to avoid importing source instead of built package\")\n+    print(\"\u26a0  Ensure that your ~/.shellrc does not unset PYTHONPATH\")\n+    _run(cmd, replace=True)\n+\n+\n+@click.command()\n+@click.argument(\"python_args\", nargs=-1)\n+def meson_python(python_args):\n+    \"\"\"\ud83d\udc0d Launch Python shell with PYTHONPATH set\n+\n+    PYTHON_ARGS are passed through directly to Python, e.g.:\n+\n+    spin python -- -c 'import sys; print(sys.path)'\n+    \"\"\"\n+    p = _set_pythonpath()\n+    v = sys.version_info\n+    if (v.major < 3) or (v.major == 3 and v.minor < 11):\n+        print(\"We're sorry, but this feature only works on Python 3.11 and greater \ud83d\ude22\")\n+        print()\n+        print(\n+            \"Why? Because we need the '-P' flag so the interpreter doesn't muck with PYTHONPATH\"\n+        )\n+        print()\n+        print(\"However! You can still launch your own interpreter:\")\n+        print()\n+        print(f\"  PYTHONPATH='{p}' python\")\n+        print()\n+        print(\"And then call:\")\n+        print()\n+        print(\"import sys; del(sys.path[0])\")\n+        sys.exit(-1)\n+\n+    print(f'\ud83d\udc0d Launching Python with PYTHONPATH=\"{p}\"')\n+\n+    _run([\"/usr/bin/env\", \"python\", \"-P\"] + list(python_args), replace=True)\n+\n+\n+@click.command(context_settings={\"ignore_unknown_options\": True})\n+@click.argument(\"args\", nargs=-1)\n+def meson_run(args):\n+    \"\"\"\ud83c\udfc1 Run a shell command with PYTHONPATH set\n+\n+    \\b\n+    spin run make\n+    spin run 'echo $PYTHONPATH'\n+    spin run python -c 'import sys; del sys.path[0]; import mypkg'\n+\n+    If you'd like to expand shell variables, like `$PYTHONPATH` in the example\n+    above, you need to provide a single, quoted command to `run`:\n+\n+    spin run 'echo $SHELL && echo $PWD'\n+\n+    On Windows, all shell commands are run via Bash.\n+    Install Git for Windows if you don't have Bash already.\n+    \"\"\"\n+    if not len(args) > 0:\n+        raise RuntimeError(\"No command given\")\n+\n+    is_posix = sys.platform in (\"linux\", \"darwin\")\n+    shell = len(args) == 1\n+    if shell:\n+        args = args[0]\n+\n+    if shell and not is_posix:\n+        # On Windows, we're going to try to use bash\n+        args = [\"bash\", \"-c\", args]\n+\n+    _set_pythonpath(quiet=True)\n+    _run(args, echo=False, shell=shell)\n+\n+\n+@click.command()\n+@click.argument(\"sphinx_target\", default=\"html\")\n+@click.option(\n+    \"--clean\",\n+    is_flag=True,\n+    default=False,\n+    help=\"Clean previously built docs before building\",\n+)\n+@click.option(\n+    \"--build/--no-build\",\n+    \"first_build\",\n+    default=True,\n+    help=\"Build numpy before generating docs\",\n+)\n+@click.option(\"--jobs\", \"-j\", default=\"auto\", help=\"Number of parallel build jobs\")\n+@click.pass_context\n+def meson_docs(ctx, sphinx_target, clean, first_build, jobs):\n+    \"\"\"\ud83d\udcd6 Build Sphinx documentation\n+\n+    By default, SPHINXOPTS=\"-W\", raising errors on warnings.\n+    To build without raising on warnings:\n+\n+      SPHINXOPTS=\"\" spin docs\n+\n+    To list all Sphinx targets:\n+\n+      spin docs targets\n+\n+    To build another Sphinx target:\n+\n+      spin docs TARGET\n+\n+    \"\"\"\n+    # Detect docs dir\n+    doc_dir_candidates = (\"doc\", \"docs\")\n+    doc_dir = next((d for d in doc_dir_candidates if os.path.exists(d)), None)\n+    if doc_dir is None:\n+        print(\n+            f\"No documentation folder found; one of {', '.join(doc_dir_candidates)} must exist\"\n+        )\n+        sys.exit(1)\n+\n+    if sphinx_target in (\"targets\", \"help\"):\n+        clean = False\n+        first_build = False\n+        sphinx_target = \"help\"\n+\n+    if clean:\n+        doc_dirs = [\n+            \"./doc/build/\",\n+            \"./doc/source/api/\",\n+            \"./doc/source/auto_examples/\",\n+            \"./doc/source/jupyterlite_contents/\",\n+        ]\n+        for doc_dir in doc_dirs:\n+            if os.path.isdir(doc_dir):\n+                print(f\"Removing {doc_dir!r}\")\n+                shutil.rmtree(doc_dir)\n+\n+    build_cmd = _get_configured_command(\"build\")\n+\n+    if build_cmd and first_build:\n+        click.secho(\n+            \"Invoking `build` prior to building docs:\", bold=True, fg=\"bright_green\"\n+        )\n+        ctx.invoke(build_cmd)\n+\n+    try:\n+        site_path = _get_site_packages()\n+    except FileNotFoundError:\n+        print(\"No built numpy found; run `spin build` first.\")\n+        sys.exit(1)\n+\n+    opts = os.environ.get(\"SPHINXOPTS\", \"-W\")\n+    os.environ[\"SPHINXOPTS\"] = f\"{opts} -j {jobs}\"\n+    click.secho(\n+        f\"$ export SPHINXOPTS={os.environ['SPHINXOPTS']}\", bold=True, fg=\"bright_blue\"\n+    )\n+\n+    os.environ[\"PYTHONPATH\"] = f'{site_path}{os.sep}:{os.environ.get(\"PYTHONPATH\", \"\")}'\n+    click.secho(\n+        f\"$ export PYTHONPATH={os.environ['PYTHONPATH']}\", bold=True, fg=\"bright_blue\"\n+    )\n+    _run([\"make\", \"-C\", \"doc\", sphinx_target], replace=True)\n+\n+\n+# END of spin/cmds/meson.py\n+\n \n # The numpy-vendored version of Meson. Put the directory that the executable\n # `meson` is in at the front of the PATH.\n@@ -56,7 +479,7 @@ def build(ctx, meson_args, jobs=None, clean=False, verbose=False):\n \n     CFLAGS=\"-O0 -g\" spin build\n     \"\"\"\n-    ctx.forward(meson.build)\n+    ctx.forward(meson_build)\n \n \n @click.command()\n@@ -105,9 +528,9 @@ def docs(ctx, sphinx_target, clean, first_build, jobs, install_deps):\n         if install_deps:\n             util.run(['pip', 'install', '-q', '-r', 'doc_requirements.txt'])\n \n-    meson.docs.ignore_unknown_options = True\n+    meson_docs.ignore_unknown_options = True\n     del ctx.params['install_deps']\n-    ctx.forward(meson.docs)\n+    ctx.forward(meson_docs)\n \n \n @click.command()\n@@ -193,7 +616,7 @@ def test(ctx, pytest_args, markexpr, n_jobs, tests, verbose):\n \n     for extra_param in ('markexpr', 'n_jobs', 'tests', 'verbose'):\n         del ctx.params[extra_param]\n-    ctx.forward(meson.test)\n+    ctx.forward(meson_test)\n \n \n @click.command()\n@@ -220,7 +643,7 @@ def gdb(code, gdb_args):\n      spin gdb my_tests.py\n      spin gdb -- my_tests.py --mytest-flag\n     \"\"\"\n-    meson._set_pythonpath()\n+    _set_pythonpath()\n     gdb_args = list(gdb_args)\n \n     if gdb_args and gdb_args[0].endswith('.py'):\n@@ -392,9 +815,9 @@ def bench(ctx, tests, compare, verbose, commits):\n             \"Invoking `build` prior to running benchmarks:\",\n             bold=True, fg=\"bright_green\"\n         )\n-        ctx.invoke(meson.build)\n+        ctx.invoke(build)\n \n-        meson._set_pythonpath()\n+        _set_pythonpath()\n \n         p = util.run(\n             ['python', '-c', 'import numpy as np; print(np.__version__)'],\n@@ -450,8 +873,8 @@ def python(ctx, python_args):\n     \"\"\"\n     env = os.environ\n     env['PYTHONWARNINGS'] = env.get('PYTHONWARNINGS', 'all')\n-    ctx.invoke(meson.build)\n-    ctx.forward(meson.python)\n+    ctx.invoke(build)\n+    ctx.forward(meson_python)\n \n \n @click.command(context_settings={\n@@ -469,9 +892,9 @@ def ipython(ctx, ipython_args):\n     env = os.environ\n     env['PYTHONWARNINGS'] = env.get('PYTHONWARNINGS', 'all')\n \n-    ctx.invoke(meson.build)\n+    ctx.invoke(build)\n \n-    ppath = meson._set_pythonpath()\n+    ppath = _set_pythonpath()\n \n     print(f'\ud83d\udcbb Launching IPython with PYTHONPATH=\"{ppath}\"')\n     preimport = (r\"import numpy as np; \"\n@@ -500,5 +923,5 @@ def run(ctx, args):\n     On Windows, all shell commands are run via Bash.\n     Install Git for Windows if you don't have Bash already.\n     \"\"\"\n-    ctx.invoke(meson.build)\n-    ctx.forward(meson.run)\n+    ctx.invoke(build)\n+    ctx.forward(meson_run)\n",
            "comment_added_diff": {
                "16": "# START of spin/cmds/meson.py",
                "19": "# The numpy-vendored version of Meson",
                "54": "        # We have a system that uses `python3.X/site-packages` or `python3.X/dist-packages`",
                "63": "        # A naming scheme that does not encode the Python major/minor version is used, so return",
                "64": "        # whatever site-packages path was found",
                "136": "        # Build dir has been configured; check if it was configured by",
                "137": "        # current version of Meson",
                "142": "        # Any other conditions that warrant a reconfigure?",
                "231": "    # Sanity check that library built properly",
                "341": "        # On Windows, we're going to try to use bash",
                "381": "    # Detect docs dir",
                "434": "# END of spin/cmds/meson.py"
            },
            "comment_deleted_diff": {},
            "comment_modified_diff": {}
        },
        {
            "commit": "5f7fd6aa3f3a634bfa168de31e28ee1e9e9a5ffa",
            "timestamp": "2023-08-18T17:45:32-07:00",
            "author": "Stefan van der Walt",
            "commit_message": "Upgrade to spin 0.5\n\nThis version of spin allows us to use a vendored meson CLI.",
            "additions": 12,
            "deletions": 439,
            "change_type": "MODIFY",
            "diff": "@@ -10,437 +10,11 @@\n \n import click\n from spin import util\n+from spin.cmds import meson\n \n-_run = util.run\n-\n-# START of spin/cmds/meson.py\n-install_dir = \"build-install\"\n-\n-# The numpy-vendored version of Meson\n-meson_cli = [sys.executable,\n-             str(pathlib.Path(__file__).parent.parent.resolve() /\n-                 'vendored-meson' / 'meson' / 'meson.py')\n-            ]\n-\n-\n-def _set_pythonpath(quiet=False):\n-    site_packages = _get_site_packages()\n-    env = os.environ\n-\n-    if \"PYTHONPATH\" in env:\n-        env[\"PYTHONPATH\"] = f\"{site_packages}{os.pathsep}{env['PYTHONPATH']}\"\n-    else:\n-        env[\"PYTHONPATH\"] = site_packages\n-\n-    if not quiet:\n-        click.secho(\n-            f'$ export PYTHONPATH=\"{site_packages}\"', bold=True, fg=\"bright_blue\"\n-        )\n-\n-    return env[\"PYTHONPATH\"]\n-\n-\n-def _get_site_packages():\n-    candidate_paths = []\n-    for root, dirs, _files in os.walk(install_dir):\n-        for subdir in dirs:\n-            if subdir == \"site-packages\" or subdir == \"dist-packages\":\n-                candidate_paths.append(os.path.abspath(os.path.join(root, subdir)))\n-\n-    X, Y = sys.version_info.major, sys.version_info.minor\n-\n-    site_packages = None\n-    if any(f\"python{X}.\" in p for p in candidate_paths):\n-        # We have a system that uses `python3.X/site-packages` or `python3.X/dist-packages`\n-        site_packages = [p for p in candidate_paths if f\"python{X}.{Y}\" in p]\n-        if len(site_packages) == 0:\n-            raise FileNotFoundError(\n-                f\"No site-packages found in {install_dir} for Python {X}.{Y}\"\n-            )\n-        else:\n-            site_packages = site_packages[0]\n-    else:\n-        # A naming scheme that does not encode the Python major/minor version is used, so return\n-        # whatever site-packages path was found\n-        if len(candidate_paths) > 1:\n-            raise FileNotFoundError(\n-                f\"Multiple `site-packages` found in `{install_dir}`, but cannot use Python version to disambiguate\"\n-            )\n-        elif len(candidate_paths) == 1:\n-            site_packages = candidate_paths[0]\n-\n-    if site_packages is None:\n-        raise FileNotFoundError(\n-            f\"No `site-packages` or `dist-packages` found under `{install_dir}`\"\n-        )\n-\n-    return site_packages\n-\n-\n-def _meson_version():\n-    try:\n-        p = _run(meson_cli + [\"--version\"], output=False, echo=False)\n-        return p.stdout.decode(\"ascii\").strip()\n-    except:\n-        pass\n-\n-\n-def _meson_version_configured():\n-    try:\n-        meson_info_fn = os.path.join(\"build\", \"meson-info\", \"meson-info.json\")\n-        meson_info = json.load(open(meson_info_fn))\n-        return meson_info[\"meson_version\"][\"full\"]\n-    except:\n-        pass\n-\n-\n-@click.command()\n-@click.option(\"-j\", \"--jobs\", help=\"Number of parallel tasks to launch\", type=int)\n-@click.option(\"--clean\", is_flag=True, help=\"Clean build directory before build\")\n-@click.option(\n-    \"-v\", \"--verbose\", is_flag=True, help=\"Print all build output, even installation\"\n-)\n-@click.argument(\"meson_args\", nargs=-1)\n-def meson_build(meson_args, jobs=None, clean=False, verbose=False):\n-    \"\"\"\ud83d\udd27 Build package with Meson/ninja and install\n-\n-    MESON_ARGS are passed through e.g.:\n-\n-    spin build -- -Dpkg_config_path=/lib64/pkgconfig\n-\n-    The package is installed to build-install\n-\n-    By default builds for release, to be able to use a debugger set CFLAGS\n-    appropriately. For example, for linux use\n-\n-    CFLAGS=\"-O0 -g\" spin build\n-    \"\"\"\n-    build_dir = \"build\"\n-    setup_cmd = meson_cli + [\"setup\", build_dir, \"--prefix=/usr\"] + list(meson_args)\n-\n-    if clean:\n-        print(f\"Removing `{build_dir}`\")\n-        if os.path.isdir(build_dir):\n-            shutil.rmtree(build_dir)\n-        print(f\"Removing `{install_dir}`\")\n-        if os.path.isdir(install_dir):\n-            shutil.rmtree(install_dir)\n-\n-    if not (os.path.exists(build_dir) and _meson_version_configured()):\n-        p = _run(setup_cmd, sys_exit=False)\n-        if p.returncode != 0:\n-            raise RuntimeError(\n-                \"Meson configuration failed; please try `spin build` again with the `--clean` flag.\"\n-            )\n-    else:\n-        # Build dir has been configured; check if it was configured by\n-        # current version of Meson\n-\n-        if _meson_version() != _meson_version_configured():\n-            _run(setup_cmd + [\"--reconfigure\"])\n-\n-        # Any other conditions that warrant a reconfigure?\n-\n-    p = _run(meson_cli + [\"compile\", \"-C\", build_dir], sys_exit=False)\n-    p = _run(meson_cli +\n-        [\n-            \"install\",\n-            \"--only-changed\",\n-            \"-C\",\n-            build_dir,\n-            \"--destdir\",\n-            f\"../{install_dir}\",\n-        ],\n-        output=verbose,\n-    )\n-\n-\n-def _get_configured_command(command_name):\n-    from spin.cmds.util import get_commands\n-    command_groups = get_commands()\n-    commands = [cmd for section in command_groups for cmd in command_groups[section]]\n-    return next((cmd for cmd in commands if cmd.name == command_name), None)\n-\n-\n-@click.command()\n-@click.argument(\"pytest_args\", nargs=-1)\n-@click.pass_context\n-def meson_test(ctx, pytest_args):\n-    \"\"\"\ud83d\udd27 Run tests\n-\n-    PYTEST_ARGS are passed through directly to pytest, e.g.:\n-\n-      spin test -- -v\n-\n-    To run tests on a directory or file:\n-\n-     \\b\n-     spin test numpy/linalg\n-     spin test numpy/linalg/tests/test_linalg.py\n-\n-    To run specific tests, by module, function, class, or method:\n-\n-     \\b\n-     spin test -- --pyargs numpy.random\n-     spin test -- --pyargs numpy.random.tests.test_generator_mt19937\n-     spin test -- --pyargs numpy.random.tests.test_generator_mt19937::TestMultivariateHypergeometric\n-     spin test -- --pyargs numpy.random.tests.test_generator_mt19937::TestMultivariateHypergeometric::test_edge_cases\n-\n-    To report the durations of the N slowest tests:\n-\n-      spin test -- --durations=N\n-\n-    To run tests that match a given pattern:\n-\n-     \\b\n-     spin test -- -k \"geometric\"\n-     spin test -- -k \"geometric and not rgeometric\"\n-\n-    To skip tests with a given marker:\n-\n-      spin test -- -m \"not slow\"\n-\n-    To parallelize test runs (requires `pytest-xdist`):\n-\n-      spin test -- -n NUM_JOBS\n-\n-    For more, see `pytest --help`.\n-\n-    \"\"\"\n-    from spin.cmds.util import get_config\n-    cfg = get_config()\n-\n-    build_cmd = _get_configured_command(\"build\")\n-    if build_cmd:\n-        click.secho(\n-            \"Invoking `build` prior to running tests:\", bold=True, fg=\"bright_green\"\n-        )\n-        ctx.invoke(build_cmd)\n-\n-    package = cfg.get(\"tool.spin.package\", None)\n-    if not pytest_args:\n-        pytest_args = (package,)\n-        if pytest_args == (None,):\n-            print(\n-                \"Please specify `package = packagename` under `tool.spin` section of `pyproject.toml`\"\n-            )\n-            sys.exit(1)\n-\n-    site_path = _set_pythonpath()\n-\n-    # Sanity check that library built properly\n-    if sys.version_info[:2] >= (3, 11):\n-        p = _run([sys.executable, \"-P\", \"-c\", f\"import {package}\"], sys_exit=False)\n-        if p.returncode != 0:\n-            print(f\"As a sanity check, we tried to import {package}.\")\n-            print(\"Stopping. Please investigate the build error.\")\n-            sys.exit(1)\n-\n-    print(f'$ export PYTHONPATH=\"{site_path}\"')\n-    _run(\n-        [sys.executable, \"-m\", \"pytest\", f\"--rootdir={site_path}\"] + list(pytest_args),\n-        cwd=site_path,\n-        replace=True,\n-    )\n-\n-\n-@click.command()\n-@click.argument(\"ipython_args\", nargs=-1)\n-def ipython(ipython_args):\n-    \"\"\"\ud83d\udcbb Launch IPython shell with PYTHONPATH set\n-\n-    IPYTHON_ARGS are passed through directly to IPython, e.g.:\n-\n-    spin ipython -- -i myscript.py\n-    \"\"\"\n-    p = _set_pythonpath()\n-    print(f'\ud83d\udcbb Launching IPython with PYTHONPATH=\"{p}\"')\n-    _run([\"ipython\", \"--ignore-cwd\"] + list(ipython_args), replace=True)\n-\n-\n-@click.command()\n-@click.argument(\"shell_args\", nargs=-1)\n-def meson_shell(shell_args=[]):\n-    \"\"\"\ud83d\udcbb Launch shell with PYTHONPATH set\n-\n-    SHELL_ARGS are passed through directly to the shell, e.g.:\n-\n-    spin shell -- -c 'echo $PYTHONPATH'\n-\n-    Ensure that your shell init file (e.g., ~/.zshrc) does not override\n-    the PYTHONPATH.\n-    \"\"\"\n-    p = _set_pythonpath()\n-    shell = os.environ.get(\"SHELL\", \"sh\")\n-    cmd = [shell] + list(shell_args)\n-    print(f'\ud83d\udcbb Launching shell with PYTHONPATH=\"{p}\"')\n-    print(\"\u26a0  Change directory to avoid importing source instead of built package\")\n-    print(\"\u26a0  Ensure that your ~/.shellrc does not unset PYTHONPATH\")\n-    _run(cmd, replace=True)\n-\n-\n-@click.command()\n-@click.argument(\"python_args\", nargs=-1)\n-def meson_python(python_args):\n-    \"\"\"\ud83d\udc0d Launch Python shell with PYTHONPATH set\n-\n-    PYTHON_ARGS are passed through directly to Python, e.g.:\n-\n-    spin python -- -c 'import sys; print(sys.path)'\n-    \"\"\"\n-    p = _set_pythonpath()\n-    v = sys.version_info\n-    if (v.major < 3) or (v.major == 3 and v.minor < 11):\n-        print(\"We're sorry, but this feature only works on Python 3.11 and greater \ud83d\ude22\")\n-        print()\n-        print(\n-            \"Why? Because we need the '-P' flag so the interpreter doesn't muck with PYTHONPATH\"\n-        )\n-        print()\n-        print(\"However! You can still launch your own interpreter:\")\n-        print()\n-        print(f\"  PYTHONPATH='{p}' python\")\n-        print()\n-        print(\"And then call:\")\n-        print()\n-        print(\"import sys; del(sys.path[0])\")\n-        sys.exit(-1)\n-\n-    print(f'\ud83d\udc0d Launching Python with PYTHONPATH=\"{p}\"')\n-\n-    _run([\"/usr/bin/env\", \"python\", \"-P\"] + list(python_args), replace=True)\n-\n-\n-@click.command(context_settings={\"ignore_unknown_options\": True})\n-@click.argument(\"args\", nargs=-1)\n-def meson_run(args):\n-    \"\"\"\ud83c\udfc1 Run a shell command with PYTHONPATH set\n-\n-    \\b\n-    spin run make\n-    spin run 'echo $PYTHONPATH'\n-    spin run python -c 'import sys; del sys.path[0]; import mypkg'\n-\n-    If you'd like to expand shell variables, like `$PYTHONPATH` in the example\n-    above, you need to provide a single, quoted command to `run`:\n-\n-    spin run 'echo $SHELL && echo $PWD'\n-\n-    On Windows, all shell commands are run via Bash.\n-    Install Git for Windows if you don't have Bash already.\n-    \"\"\"\n-    if not len(args) > 0:\n-        raise RuntimeError(\"No command given\")\n-\n-    is_posix = sys.platform in (\"linux\", \"darwin\")\n-    shell = len(args) == 1\n-    if shell:\n-        args = args[0]\n-\n-    if shell and not is_posix:\n-        # On Windows, we're going to try to use bash\n-        args = [\"bash\", \"-c\", args]\n-\n-    _set_pythonpath(quiet=True)\n-    _run(args, echo=False, shell=shell)\n-\n-\n-@click.command()\n-@click.argument(\"sphinx_target\", default=\"html\")\n-@click.option(\n-    \"--clean\",\n-    is_flag=True,\n-    default=False,\n-    help=\"Clean previously built docs before building\",\n-)\n-@click.option(\n-    \"--build/--no-build\",\n-    \"first_build\",\n-    default=True,\n-    help=\"Build numpy before generating docs\",\n-)\n-@click.option(\"--jobs\", \"-j\", default=\"auto\", help=\"Number of parallel build jobs\")\n-@click.pass_context\n-def meson_docs(ctx, sphinx_target, clean, first_build, jobs):\n-    \"\"\"\ud83d\udcd6 Build Sphinx documentation\n-\n-    By default, SPHINXOPTS=\"-W\", raising errors on warnings.\n-    To build without raising on warnings:\n-\n-      SPHINXOPTS=\"\" spin docs\n-\n-    To list all Sphinx targets:\n-\n-      spin docs targets\n-\n-    To build another Sphinx target:\n-\n-      spin docs TARGET\n-\n-    \"\"\"\n-    # Detect docs dir\n-    doc_dir_candidates = (\"doc\", \"docs\")\n-    doc_dir = next((d for d in doc_dir_candidates if os.path.exists(d)), None)\n-    if doc_dir is None:\n-        print(\n-            f\"No documentation folder found; one of {', '.join(doc_dir_candidates)} must exist\"\n-        )\n-        sys.exit(1)\n-\n-    if sphinx_target in (\"targets\", \"help\"):\n-        clean = False\n-        first_build = False\n-        sphinx_target = \"help\"\n-\n-    if clean:\n-        doc_dirs = [\n-            \"./doc/build/\",\n-            \"./doc/source/api/\",\n-            \"./doc/source/auto_examples/\",\n-            \"./doc/source/jupyterlite_contents/\",\n-        ]\n-        for doc_dir in doc_dirs:\n-            if os.path.isdir(doc_dir):\n-                print(f\"Removing {doc_dir!r}\")\n-                shutil.rmtree(doc_dir)\n-\n-    build_cmd = _get_configured_command(\"build\")\n-\n-    if build_cmd and first_build:\n-        click.secho(\n-            \"Invoking `build` prior to building docs:\", bold=True, fg=\"bright_green\"\n-        )\n-        ctx.invoke(build_cmd)\n-\n-    try:\n-        site_path = _get_site_packages()\n-    except FileNotFoundError:\n-        print(\"No built numpy found; run `spin build` first.\")\n-        sys.exit(1)\n-\n-    opts = os.environ.get(\"SPHINXOPTS\", \"-W\")\n-    os.environ[\"SPHINXOPTS\"] = f\"{opts} -j {jobs}\"\n-    click.secho(\n-        f\"$ export SPHINXOPTS={os.environ['SPHINXOPTS']}\", bold=True, fg=\"bright_blue\"\n-    )\n-\n-    os.environ[\"PYTHONPATH\"] = f'{site_path}{os.sep}:{os.environ.get(\"PYTHONPATH\", \"\")}'\n-    click.secho(\n-        f\"$ export PYTHONPATH={os.environ['PYTHONPATH']}\", bold=True, fg=\"bright_blue\"\n-    )\n-    _run([\"make\", \"-C\", \"doc\", sphinx_target], replace=True)\n-\n-\n-# END of spin/cmds/meson.py\n-\n-\n-# The numpy-vendored version of Meson. Put the directory that the executable\n-# `meson` is in at the front of the PATH.\n-curdir = pathlib.Path(__file__).parent.resolve()\n-meson_executable_dir = str(curdir.parent / 'vendored-meson' / 'entrypoint')\n-os.environ['PATH'] = meson_executable_dir + os.pathsep + os.environ['PATH']\n \n # Check that the meson git submodule is present\n+curdir = pathlib.Path(__file__).parent\n meson_import_dir = curdir.parent / 'vendored-meson' / 'meson' / 'mesonbuild'\n if not meson_import_dir.exists():\n     raise RuntimeError(\n@@ -479,7 +53,7 @@ def build(ctx, meson_args, jobs=None, clean=False, verbose=False):\n \n     CFLAGS=\"-O0 -g\" spin build\n     \"\"\"\n-    ctx.forward(meson_build)\n+    ctx.forward(meson.build)\n \n \n @click.command()\n@@ -528,9 +102,9 @@ def docs(ctx, sphinx_target, clean, first_build, jobs, install_deps):\n         if install_deps:\n             util.run(['pip', 'install', '-q', '-r', 'doc_requirements.txt'])\n \n-    meson_docs.ignore_unknown_options = True\n+    meson.docs.ignore_unknown_options = True\n     del ctx.params['install_deps']\n-    ctx.forward(meson_docs)\n+    ctx.forward(meson.docs)\n \n \n @click.command()\n@@ -616,7 +190,7 @@ def test(ctx, pytest_args, markexpr, n_jobs, tests, verbose):\n \n     for extra_param in ('markexpr', 'n_jobs', 'tests', 'verbose'):\n         del ctx.params[extra_param]\n-    ctx.forward(meson_test)\n+    ctx.forward(meson.test)\n \n \n @click.command()\n@@ -643,7 +217,7 @@ def gdb(code, gdb_args):\n      spin gdb my_tests.py\n      spin gdb -- my_tests.py --mytest-flag\n     \"\"\"\n-    _set_pythonpath()\n+    meson._set_pythonpath()\n     gdb_args = list(gdb_args)\n \n     if gdb_args and gdb_args[0].endswith('.py'):\n@@ -817,7 +391,7 @@ def bench(ctx, tests, compare, verbose, commits):\n         )\n         ctx.invoke(build)\n \n-        _set_pythonpath()\n+        meson._set_pythonpath()\n \n         p = util.run(\n             ['python', '-c', 'import numpy as np; print(np.__version__)'],\n@@ -874,7 +448,7 @@ def python(ctx, python_args):\n     env = os.environ\n     env['PYTHONWARNINGS'] = env.get('PYTHONWARNINGS', 'all')\n     ctx.invoke(build)\n-    ctx.forward(meson_python)\n+    ctx.forward(meson.python)\n \n \n @click.command(context_settings={\n@@ -894,7 +468,7 @@ def ipython(ctx, ipython_args):\n \n     ctx.invoke(build)\n \n-    ppath = _set_pythonpath()\n+    ppath = meson._set_pythonpath()\n \n     print(f'\ud83d\udcbb Launching IPython with PYTHONPATH=\"{ppath}\"')\n     preimport = (r\"import numpy as np; \"\n@@ -924,17 +498,16 @@ def run(ctx, args):\n     Install Git for Windows if you don't have Bash already.\n     \"\"\"\n     ctx.invoke(build)\n-    ctx.forward(meson_run)\n+    ctx.forward(meson.run)\n  \n \n @click.command(context_settings={\"ignore_unknown_options\": True})\n @click.pass_context\n def mypy(ctx):\n-    \"\"\"Run Mypy tests for NumPy\n+    \"\"\"\ud83e\udd86 Run Mypy tests for NumPy\n     \"\"\"\n     env = os.environ\n     env['NPY_RUN_MYPY_IN_TESTSUITE'] = '1'\n     ctx.params['pytest_args'] = [os.path.join('numpy', 'typing')]\n     ctx.params['markexpr'] = 'full'\n     ctx.forward(test)\n-\n",
            "comment_added_diff": {},
            "comment_deleted_diff": {
                "16": "# START of spin/cmds/meson.py",
                "19": "# The numpy-vendored version of Meson",
                "54": "        # We have a system that uses `python3.X/site-packages` or `python3.X/dist-packages`",
                "63": "        # A naming scheme that does not encode the Python major/minor version is used, so return",
                "64": "        # whatever site-packages path was found",
                "136": "        # Build dir has been configured; check if it was configured by",
                "137": "        # current version of Meson",
                "142": "        # Any other conditions that warrant a reconfigure?",
                "231": "    # Sanity check that library built properly",
                "341": "        # On Windows, we're going to try to use bash",
                "381": "    # Detect docs dir",
                "434": "# END of spin/cmds/meson.py",
                "437": "# The numpy-vendored version of Meson. Put the directory that the executable",
                "438": "# `meson` is in at the front of the PATH."
            },
            "comment_modified_diff": {}
        },
        {
            "commit": "c2c36c5d14012b6932b3ce8422b5494323401fc0",
            "timestamp": "2023-08-27T22:40:43+02:00",
            "author": "Ralf Gommers",
            "commit_message": "DEV: add a --quick flag to `spin bench`, and ensure correct exit code",
            "additions": 15,
            "deletions": 26,
            "change_type": "MODIFY",
            "diff": "@@ -303,19 +303,7 @@ def _run_asv(cmd):\n     except (ImportError, RuntimeError):\n         pass\n \n-    try:\n-        util.run(cmd, cwd='benchmarks', env=env, sys_exit=False)\n-    except FileNotFoundError:\n-        click.secho((\n-            \"Cannot find `asv`. \"\n-            \"Please install Airspeed Velocity:\\n\\n\"\n-            \"  https://asv.readthedocs.io/en/latest/installing.html\\n\"\n-            \"\\n\"\n-            \"Depending on your system, one of the following should work:\\n\\n\"\n-            \"  pip install asv\\n\"\n-            \"  conda install asv\\n\"\n-        ), fg=\"red\")\n-        sys.exit(1)\n+    util.run(cmd, cwd='benchmarks', env=env)\n \n \n @click.command()\n@@ -336,13 +324,17 @@ def _run_asv(cmd):\n @click.option(\n     '--verbose', '-v', is_flag=True, default=False\n )\n+@click.option(\n+    '--quick', '-q', is_flag=True, default=False,\n+    help=\"Run each benchmark only once (timings won't be accurate)\"\n+)\n @click.argument(\n     'commits', metavar='',\n     required=False,\n     nargs=-1\n )\n @click.pass_context\n-def bench(ctx, tests, compare, verbose, commits):\n+def bench(ctx, tests, compare, verbose, quick, commits):\n     \"\"\"\ud83c\udfcb Run benchmarks.\n \n     \\b\n@@ -382,6 +374,9 @@ def bench(ctx, tests, compare, verbose, commits):\n     if verbose:\n         bench_args = ['-v'] + bench_args\n \n+    if quick:\n+        bench_args = ['--quick'] + bench_args\n+\n     if not compare:\n         # No comparison requested; we build and benchmark the current version\n \n@@ -409,27 +404,21 @@ def bench(ctx, tests, compare, verbose, commits):\n         cmd = [\n             'asv', 'run', '--dry-run', '--show-stderr', '--python=same'\n         ] + bench_args\n-\n         _run_asv(cmd)\n-\n     else:\n-        # Benchmark comparison\n-\n         # Ensure that we don't have uncommited changes\n         commit_a, commit_b = [_commit_to_sha(c) for c in commits]\n \n-        if commit_b == 'HEAD':\n-            if _dirty_git_working_dir():\n-                click.secho(\n-                    \"WARNING: you have uncommitted changes --- \"\n-                    \"these will NOT be benchmarked!\",\n-                    fg=\"red\"\n-                )\n+        if commit_b == 'HEAD' and _dirty_git_working_dir():\n+            click.secho(\n+                \"WARNING: you have uncommitted changes --- \"\n+                \"these will NOT be benchmarked!\",\n+                fg=\"red\"\n+            )\n \n         cmd_compare = [\n             'asv', 'continuous', '--factor', '1.05',\n         ] + bench_args + [commit_a, commit_b]\n-\n         _run_asv(cmd_compare)\n \n \n",
            "comment_added_diff": {},
            "comment_deleted_diff": {
                "416": "        # Benchmark comparison"
            },
            "comment_modified_diff": {}
        },
        {
            "commit": "21d29c5a0df2ad53506f3b3855179ec776c18f08",
            "timestamp": "2023-10-04T12:42:46+02:00",
            "author": "Matti Picus",
            "commit_message": "BLD: use scipy-openblas wheel (#24839)",
            "additions": 47,
            "deletions": 2,
            "change_type": "MODIFY",
            "diff": "@@ -37,9 +37,14 @@\n     \"-v\", \"--verbose\", is_flag=True,\n     help=\"Print all build output, even installation\"\n )\n+@click.option(\n+    \"--with-scipy-openblas\", type=click.Choice([\"32\", \"64\"]),\n+    default=None,\n+    help=\"Build with pre-installed scipy-openblas32 or scipy-openblas64 wheel\"\n+)\n @click.argument(\"meson_args\", nargs=-1)\n @click.pass_context\n-def build(ctx, meson_args, jobs=None, clean=False, verbose=False, quiet=False):\n+def build(ctx, meson_args, with_scipy_openblas, jobs=None, clean=False, verbose=False, quiet=False):\n     \"\"\"\ud83d\udd27 Build package with Meson/ninja and install\n \n     MESON_ARGS are passed through e.g.:\n@@ -53,6 +58,10 @@ def build(ctx, meson_args, jobs=None, clean=False, verbose=False, quiet=False):\n \n     CFLAGS=\"-O0 -g\" spin build\n     \"\"\"\n+    # XXX keep in sync with upstream build\n+    if with_scipy_openblas:\n+        _config_openblas(with_scipy_openblas)\n+    ctx.params.pop(\"with_scipy_openblas\", None)\n     ctx.forward(meson.build)\n \n \n@@ -413,7 +422,7 @@ def ipython(ctx, ipython_args):\n     util.run([\"ipython\", \"--ignore-cwd\",\n               f\"--TerminalIPythonApp.exec_lines={preimport}\"] +\n              list(ipython_args))\n- \n+\n \n @click.command(context_settings={\"ignore_unknown_options\": True})\n @click.pass_context\n@@ -425,3 +434,39 @@ def mypy(ctx):\n     ctx.params['pytest_args'] = [os.path.join('numpy', 'typing')]\n     ctx.params['markexpr'] = 'full'\n     ctx.forward(test)\n+\n+@click.command(context_settings={\n+    'ignore_unknown_options': True\n+})\n+@click.option(\n+    \"--with-scipy-openblas\", type=click.Choice([\"32\", \"64\"]),\n+    default=None, required=True,\n+    help=\"Build with pre-installed scipy-openblas32 or scipy-openblas64 wheel\"\n+)\n+def config_openblas(with_scipy_openblas):\n+    \"\"\"\ud83d\udd27 Create .openblas/scipy-openblas.pc file\n+\n+    Also create _distributor_init_local.py\n+\n+    Requires a pre-installed scipy-openblas64 or scipy-openblas32\n+    \"\"\"\n+    _config_openblas(with_scipy_openblas)\n+\n+\n+def _config_openblas(blas_variant):\n+    import importlib\n+    basedir = os.getcwd()\n+    openblas_dir = os.path.join(basedir, \".openblas\")\n+    pkg_config_fname = os.path.join(openblas_dir, \"scipy-openblas.pc\")\n+    if blas_variant:\n+        module_name = f\"scipy_openblas{blas_variant}\"\n+        try:\n+            openblas = importlib.import_module(module_name)\n+        except ModuleNotFoundError:\n+            raise RuntimeError(f\"'pip install {module_name} first\")\n+        local = os.path.join(basedir, \"numpy\", \"_distributor_init_local.py\")\n+        with open(local, \"wt\", encoding=\"utf8\") as fid:\n+            fid.write(f\"import {module_name}\\n\")\n+        os.makedirs(openblas_dir, exist_ok=True)\n+        with open(pkg_config_fname, \"wt\", encoding=\"utf8\") as fid:\n+            fid.write(openblas.get_pkg_config().replace(\"\\\\\", \"/\"))\n",
            "comment_added_diff": {
                "61": "    # XXX keep in sync with upstream build"
            },
            "comment_deleted_diff": {},
            "comment_modified_diff": {}
        }
    ],
    "asv.conf.json": [],
    "24126.change.rst": [],
    "glossary.rst": [],
    "basics.io.genfromtxt.rst": [],
    "_deprecations.py": [
        {
            "commit": "ab17394de8f55534eadf741dffba6408fc608514",
            "timestamp": "2023-07-10T15:27:32+02:00",
            "author": "Mateusz Sok\u00f3\u0142",
            "commit_message": "API: deprecate undocumented functions",
            "additions": 172,
            "deletions": 0,
            "change_type": "ADD",
            "diff": "@@ -0,0 +1,172 @@\n+\"\"\"\n+Module used for marking functions deprecated.\n+\"\"\"\n+\n+import functools\n+import sys\n+import textwrap\n+import warnings\n+\n+__all__ = [\"deprecate\", \"deprecate_with_doc\"]\n+\n+\n+class _Deprecate:\n+    \"\"\"\n+    Decorator class to deprecate old functions.\n+\n+    Refer to `deprecate` for details.\n+\n+    See Also\n+    --------\n+    deprecate\n+\n+    \"\"\"\n+\n+    def __init__(self, old_name=None, new_name=None, message=None):\n+        self.old_name = old_name\n+        self.new_name = new_name\n+        self.message = message\n+\n+    def __call__(self, func, *args, **kwargs):\n+        \"\"\"\n+        Decorator call.  Refer to ``decorate``.\n+\n+        \"\"\"\n+        old_name = self.old_name\n+        new_name = self.new_name\n+        message = self.message\n+\n+        if old_name is None:\n+            old_name = func.__name__\n+        if new_name is None:\n+            depdoc = \"`%s` is deprecated!\" % old_name\n+        else:\n+            depdoc = \"`%s` is deprecated, use `%s` instead!\" % \\\n+                     (old_name, new_name)\n+\n+        if message is not None:\n+            depdoc += \"\\n\" + message\n+\n+        @functools.wraps(func)\n+        def newfunc(*args, **kwds):\n+            warnings.warn(depdoc, DeprecationWarning, stacklevel=2)\n+            return func(*args, **kwds)\n+\n+        newfunc.__name__ = old_name\n+        doc = func.__doc__\n+        if doc is None:\n+            doc = depdoc\n+        else:\n+            lines = doc.expandtabs().split('\\n')\n+            indent = _get_indent(lines[1:])\n+            if lines[0].lstrip():\n+                # Indent the original first line to let inspect.cleandoc()\n+                # dedent the docstring despite the deprecation notice.\n+                doc = indent * ' ' + doc\n+            else:\n+                # Remove the same leading blank lines as cleandoc() would.\n+                skip = len(lines[0]) + 1\n+                for line in lines[1:]:\n+                    if len(line) > indent:\n+                        break\n+                    skip += len(line) + 1\n+                doc = doc[skip:]\n+            depdoc = textwrap.indent(depdoc, ' ' * indent)\n+            doc = '\\n\\n'.join([depdoc, doc])\n+        newfunc.__doc__ = doc\n+\n+        return newfunc\n+\n+\n+def _get_indent(lines):\n+    \"\"\"\n+    Determines the leading whitespace that could be removed from all the lines.\n+    \"\"\"\n+    indent = sys.maxsize\n+    for line in lines:\n+        content = len(line.lstrip())\n+        if content:\n+            indent = min(indent, len(line) - content)\n+    if indent == sys.maxsize:\n+        indent = 0\n+    return indent\n+\n+\n+def deprecate(*args, **kwargs):\n+    \"\"\"\n+    Issues a DeprecationWarning, adds warning to `old_name`'s\n+    docstring, rebinds ``old_name.__name__`` and returns the new\n+    function object.\n+\n+    This function may also be used as a decorator.\n+\n+    Parameters\n+    ----------\n+    func : function\n+        The function to be deprecated.\n+    old_name : str, optional\n+        The name of the function to be deprecated. Default is None, in\n+        which case the name of `func` is used.\n+    new_name : str, optional\n+        The new name for the function. Default is None, in which case the\n+        deprecation message is that `old_name` is deprecated. If given, the\n+        deprecation message is that `old_name` is deprecated and `new_name`\n+        should be used instead.\n+    message : str, optional\n+        Additional explanation of the deprecation.  Displayed in the\n+        docstring after the warning.\n+\n+    Returns\n+    -------\n+    old_func : function\n+        The deprecated function.\n+\n+    Examples\n+    --------\n+    Note that ``olduint`` returns a value after printing Deprecation\n+    Warning:\n+\n+    >>> olduint = np.deprecate(np.uint)\n+    DeprecationWarning: `uint64` is deprecated! # may vary\n+    >>> olduint(6)\n+    6\n+\n+    \"\"\"\n+    # Deprecate may be run as a function or as a decorator\n+    # If run as a function, we initialise the decorator class\n+    # and execute its __call__ method.\n+\n+    if args:\n+        fn = args[0]\n+        args = args[1:]\n+\n+        return _Deprecate(*args, **kwargs)(fn)\n+    else:\n+        return _Deprecate(*args, **kwargs)\n+\n+\n+def deprecate_with_doc(msg):\n+    \"\"\"\n+    Deprecates a function and includes the deprecation in its docstring.\n+\n+    This function is used as a decorator. It returns an object that can be\n+    used to issue a DeprecationWarning, by passing the to-be decorated\n+    function as argument, this adds warning to the to-be decorated function's\n+    docstring and returns the new function object.\n+\n+    See Also\n+    --------\n+    deprecate : Decorate a function such that it issues a `DeprecationWarning`\n+\n+    Parameters\n+    ----------\n+    msg : str\n+        Additional explanation of the deprecation. Displayed in the\n+        docstring after the warning.\n+\n+    Returns\n+    -------\n+    obj : object\n+\n+    \"\"\"\n+    return _Deprecate(message=msg)\n",
            "comment_added_diff": {
                "63": "                # Indent the original first line to let inspect.cleandoc()",
                "64": "                # dedent the docstring despite the deprecation notice.",
                "67": "                # Remove the same leading blank lines as cleandoc() would.",
                "130": "    DeprecationWarning: `uint64` is deprecated! # may vary",
                "135": "    # Deprecate may be run as a function or as a decorator",
                "136": "    # If run as a function, we initialise the decorator class",
                "137": "    # and execute its __call__ method."
            },
            "comment_deleted_diff": {},
            "comment_modified_diff": {}
        },
        {
            "commit": "017cef318ad37ac05738830ebb3173c65c128759",
            "timestamp": "2023-07-11T22:25:10+02:00",
            "author": "Mateusz Sok\u00f3\u0142",
            "commit_message": "API: apply review comments",
            "additions": 0,
            "deletions": 172,
            "change_type": "DELETE",
            "diff": "@@ -1,172 +0,0 @@\n-\"\"\"\n-Module used for marking functions deprecated.\n-\"\"\"\n-\n-import functools\n-import sys\n-import textwrap\n-import warnings\n-\n-__all__ = [\"deprecate\", \"deprecate_with_doc\"]\n-\n-\n-class _Deprecate:\n-    \"\"\"\n-    Decorator class to deprecate old functions.\n-\n-    Refer to `deprecate` for details.\n-\n-    See Also\n-    --------\n-    deprecate\n-\n-    \"\"\"\n-\n-    def __init__(self, old_name=None, new_name=None, message=None):\n-        self.old_name = old_name\n-        self.new_name = new_name\n-        self.message = message\n-\n-    def __call__(self, func, *args, **kwargs):\n-        \"\"\"\n-        Decorator call.  Refer to ``decorate``.\n-\n-        \"\"\"\n-        old_name = self.old_name\n-        new_name = self.new_name\n-        message = self.message\n-\n-        if old_name is None:\n-            old_name = func.__name__\n-        if new_name is None:\n-            depdoc = \"`%s` is deprecated!\" % old_name\n-        else:\n-            depdoc = \"`%s` is deprecated, use `%s` instead!\" % \\\n-                     (old_name, new_name)\n-\n-        if message is not None:\n-            depdoc += \"\\n\" + message\n-\n-        @functools.wraps(func)\n-        def newfunc(*args, **kwds):\n-            warnings.warn(depdoc, DeprecationWarning, stacklevel=2)\n-            return func(*args, **kwds)\n-\n-        newfunc.__name__ = old_name\n-        doc = func.__doc__\n-        if doc is None:\n-            doc = depdoc\n-        else:\n-            lines = doc.expandtabs().split('\\n')\n-            indent = _get_indent(lines[1:])\n-            if lines[0].lstrip():\n-                # Indent the original first line to let inspect.cleandoc()\n-                # dedent the docstring despite the deprecation notice.\n-                doc = indent * ' ' + doc\n-            else:\n-                # Remove the same leading blank lines as cleandoc() would.\n-                skip = len(lines[0]) + 1\n-                for line in lines[1:]:\n-                    if len(line) > indent:\n-                        break\n-                    skip += len(line) + 1\n-                doc = doc[skip:]\n-            depdoc = textwrap.indent(depdoc, ' ' * indent)\n-            doc = '\\n\\n'.join([depdoc, doc])\n-        newfunc.__doc__ = doc\n-\n-        return newfunc\n-\n-\n-def _get_indent(lines):\n-    \"\"\"\n-    Determines the leading whitespace that could be removed from all the lines.\n-    \"\"\"\n-    indent = sys.maxsize\n-    for line in lines:\n-        content = len(line.lstrip())\n-        if content:\n-            indent = min(indent, len(line) - content)\n-    if indent == sys.maxsize:\n-        indent = 0\n-    return indent\n-\n-\n-def deprecate(*args, **kwargs):\n-    \"\"\"\n-    Issues a DeprecationWarning, adds warning to `old_name`'s\n-    docstring, rebinds ``old_name.__name__`` and returns the new\n-    function object.\n-\n-    This function may also be used as a decorator.\n-\n-    Parameters\n-    ----------\n-    func : function\n-        The function to be deprecated.\n-    old_name : str, optional\n-        The name of the function to be deprecated. Default is None, in\n-        which case the name of `func` is used.\n-    new_name : str, optional\n-        The new name for the function. Default is None, in which case the\n-        deprecation message is that `old_name` is deprecated. If given, the\n-        deprecation message is that `old_name` is deprecated and `new_name`\n-        should be used instead.\n-    message : str, optional\n-        Additional explanation of the deprecation.  Displayed in the\n-        docstring after the warning.\n-\n-    Returns\n-    -------\n-    old_func : function\n-        The deprecated function.\n-\n-    Examples\n-    --------\n-    Note that ``olduint`` returns a value after printing Deprecation\n-    Warning:\n-\n-    >>> olduint = np.deprecate(np.uint)\n-    DeprecationWarning: `uint64` is deprecated! # may vary\n-    >>> olduint(6)\n-    6\n-\n-    \"\"\"\n-    # Deprecate may be run as a function or as a decorator\n-    # If run as a function, we initialise the decorator class\n-    # and execute its __call__ method.\n-\n-    if args:\n-        fn = args[0]\n-        args = args[1:]\n-\n-        return _Deprecate(*args, **kwargs)(fn)\n-    else:\n-        return _Deprecate(*args, **kwargs)\n-\n-\n-def deprecate_with_doc(msg):\n-    \"\"\"\n-    Deprecates a function and includes the deprecation in its docstring.\n-\n-    This function is used as a decorator. It returns an object that can be\n-    used to issue a DeprecationWarning, by passing the to-be decorated\n-    function as argument, this adds warning to the to-be decorated function's\n-    docstring and returns the new function object.\n-\n-    See Also\n-    --------\n-    deprecate : Decorate a function such that it issues a `DeprecationWarning`\n-\n-    Parameters\n-    ----------\n-    msg : str\n-        Additional explanation of the deprecation. Displayed in the\n-        docstring after the warning.\n-\n-    Returns\n-    -------\n-    obj : object\n-\n-    \"\"\"\n-    return _Deprecate(message=msg)\n",
            "comment_added_diff": {},
            "comment_deleted_diff": {
                "63": "                # Indent the original first line to let inspect.cleandoc()",
                "64": "                # dedent the docstring despite the deprecation notice.",
                "67": "                # Remove the same leading blank lines as cleandoc() would.",
                "130": "    DeprecationWarning: `uint64` is deprecated! # may vary",
                "135": "    # Deprecate may be run as a function or as a decorator",
                "136": "    # If run as a function, we initialise the decorator class",
                "137": "    # and execute its __call__ method."
            },
            "comment_modified_diff": {}
        }
    ],
    "_deprecations.pyi": [],
    "utils.pyi": [],
    "lib_function_base.pyi": [],
    "shape_base.pyi": [],
    "cblasfuncs.c": [],
    "extbuild.py": [
        {
            "commit": "0a140163369eb4fe2768fa44e326e1952137a5ed",
            "timestamp": "2023-07-13T16:48:27+03:00",
            "author": "mattip",
            "commit_message": "TST: refactor testing/extbuild to use meson on-the-fly",
            "additions": 30,
            "deletions": 42,
            "change_type": "MODIFY",
            "diff": "@@ -6,8 +6,10 @@\n \n import os\n import pathlib\n+import subprocess\n import sys\n import sysconfig\n+import textwrap\n \n __all__ = ['build_and_import_extension', 'compile_extension_module']\n \n@@ -51,8 +53,6 @@ def build_and_import_extension(\n     >>> assert not mod.test_bytes(u'abc')\n     >>> assert mod.test_bytes(b'abc')\n     \"\"\"\n-    from distutils.errors import CompileError\n-\n     body = prologue + _make_methods(functions, modname)\n     init = \"\"\"PyObject *mod = PyModule_Create(&moduledef);\n            \"\"\"\n@@ -67,7 +67,7 @@ def build_and_import_extension(\n     try:\n         mod_so = compile_extension_module(\n             modname, build_dir, include_dirs, source_string)\n-    except CompileError as e:\n+    except Exception as e:\n         # shorten the exception chain\n         raise RuntimeError(f\"could not compile in {build_dir}:\") from e\n     import importlib.util\n@@ -186,9 +186,9 @@ def _c_compile(cfile, outputfilename, include_dirs=[], libraries=[],\n     elif sys.platform.startswith('linux'):\n         compile_extra = [\n             \"-O0\", \"-g\", \"-Werror=implicit-function-declaration\", \"-fPIC\"]\n-        link_extra = None\n+        link_extra = []\n     else:\n-        compile_extra = link_extra = None\n+        compile_extra = link_extra = []\n         pass\n     if sys.platform == 'win32':\n         link_extra = link_extra + ['/DEBUG']  # generate .pdb file\n@@ -202,49 +202,37 @@ def _c_compile(cfile, outputfilename, include_dirs=[], libraries=[],\n                 library_dirs.append(s + 'lib')\n \n     outputfilename = outputfilename.with_suffix(get_so_suffix())\n-    saved_environ = os.environ.copy()\n-    try:\n-        build(\n-            cfile, outputfilename,\n-            compile_extra, link_extra,\n-            include_dirs, libraries, library_dirs)\n-    finally:\n-        # workaround for a distutils bugs where some env vars can\n-        # become longer and longer every time it is used\n-        for key, value in saved_environ.items():\n-            if os.environ.get(key) != value:\n-                os.environ[key] = value\n+    build(\n+        cfile, outputfilename,\n+        compile_extra, link_extra,\n+        include_dirs, libraries, library_dirs)\n     return outputfilename\n \n \n def build(cfile, outputfilename, compile_extra, link_extra,\n           include_dirs, libraries, library_dirs):\n-    \"cd into the directory where the cfile is, use distutils to build\"\n-    from numpy.distutils.ccompiler import new_compiler\n-\n-    compiler = new_compiler(force=1, verbose=2)\n-    compiler.customize('')\n-    objects = []\n-\n-    old = os.getcwd()\n-    os.chdir(cfile.parent)\n-    try:\n-        res = compiler.compile(\n-            [str(cfile.name)],\n-            include_dirs=include_dirs,\n-            extra_preargs=compile_extra\n+    \"use meson to build\"\n+\n+    build_dir = cfile.parent / \"build\"\n+    os.makedirs(build_dir, exist_ok=True)\n+    so_name = outputfilename.parts[-1]\n+    with open(cfile.parent / \"meson.build\", \"wt\") as fid:\n+        includes = ['-I' + d for d in include_dirs]\n+        link_dirs = ['-L' + d for d in library_dirs]\n+        fid.write(textwrap.dedent(f\"\"\"\\\n+            project('foo', 'c')\n+            shared_module('{so_name}', '{cfile.parts[-1]}',\n+                c_args: {includes} + {compile_extra},\n+                link_args: {link_dirs} + {link_extra},\n+                link_with: {libraries},\n+                name_prefix: '',\n+                name_suffix: 'dummy',\n             )\n-        objects += [str(cfile.parent / r) for r in res]\n-    finally:\n-        os.chdir(old)\n-\n-    compiler.link_shared_object(\n-        objects, str(outputfilename),\n-        libraries=libraries,\n-        extra_preargs=link_extra,\n-        library_dirs=library_dirs)\n-\n-\n+        \"\"\"))\n+    subprocess.check_call([\"meson\", \"setup\", \"--vsenv\", \"..\"], cwd=build_dir)\n+    subprocess.check_call([\"meson\", \"compile\"], cwd=build_dir)\n+    os.rename(str(build_dir / so_name) + \".dummy\", cfile.parent / so_name)\n+        \n def get_so_suffix():\n     ret = sysconfig.get_config_var('EXT_SUFFIX')\n     assert ret\n",
            "comment_added_diff": {},
            "comment_deleted_diff": {
                "212": "        # workaround for a distutils bugs where some env vars can",
                "213": "        # become longer and longer every time it is used"
            },
            "comment_modified_diff": {}
        }
    ],
    "emscripten.yml": [],
    "f90mod_rules.py": [],
    "nep-0018-array-function-protocol.rst": [],
    "nep-0022-ndarray-duck-typing-overview.rst": [],
    "nep-0037-array-module.rst": [],
    "nep-0041-improved-dtype-support.rst": [],
    "basics.interoperability.rst": [],
    "c-info.python-as-glue.rst": [],
    "npy_cpu_dispatch.h": [],
    "emulate_maskop.h": [],
    "pcg64.orig.h": [],
    "arrays.datetime.rst": [],
    "maskedarray.generic.rst": [],
    "nep-0051-scalar-representation.rst": [],
    "22449.change.rst": [],
    "umath_linalg.cpp": [],
    "clip.cpp": [],
    "funcs.inc.src": [],
    "array_constructors.pyi": [],
    "routines.help.rst": [],
    "numpy-for-matlab-users.rst": [],
    "how-to-io.rst": [],
    "stride_tricks.py": [
        {
            "commit": "3293164353d2b5168f83039a1158d54648a584ce",
            "timestamp": "2023-08-30T11:09:37+02:00",
            "author": "Mateusz Sok\u00f3\u0142",
            "commit_message": "API: Update lib.stride_tricks namespace",
            "additions": 3,
            "deletions": 546,
            "change_type": "MODIFY",
            "diff": "@@ -1,546 +1,3 @@\n-\"\"\"\n-Utilities that manipulate strides to achieve desirable effects.\n-\n-An explanation of strides can be found in the :ref:`arrays.ndarray`.\n-\n-\"\"\"\n-import numpy as np\n-from numpy.core.numeric import normalize_axis_tuple\n-from numpy.core.overrides import array_function_dispatch, set_module\n-\n-__all__ = ['broadcast_to', 'broadcast_arrays', 'broadcast_shapes']\n-\n-\n-class DummyArray:\n-    \"\"\"Dummy object that just exists to hang __array_interface__ dictionaries\n-    and possibly keep alive a reference to a base array.\n-    \"\"\"\n-\n-    def __init__(self, interface, base=None):\n-        self.__array_interface__ = interface\n-        self.base = base\n-\n-\n-def _maybe_view_as_subclass(original_array, new_array):\n-    if type(original_array) is not type(new_array):\n-        # if input was an ndarray subclass and subclasses were OK,\n-        # then view the result as that subclass.\n-        new_array = new_array.view(type=type(original_array))\n-        # Since we have done something akin to a view from original_array, we\n-        # should let the subclass finalize (if it has it implemented, i.e., is\n-        # not None).\n-        if new_array.__array_finalize__:\n-            new_array.__array_finalize__(original_array)\n-    return new_array\n-\n-\n-def as_strided(x, shape=None, strides=None, subok=False, writeable=True):\n-    \"\"\"\n-    Create a view into the array with the given shape and strides.\n-\n-    .. warning:: This function has to be used with extreme care, see notes.\n-\n-    Parameters\n-    ----------\n-    x : ndarray\n-        Array to create a new.\n-    shape : sequence of int, optional\n-        The shape of the new array. Defaults to ``x.shape``.\n-    strides : sequence of int, optional\n-        The strides of the new array. Defaults to ``x.strides``.\n-    subok : bool, optional\n-        .. versionadded:: 1.10\n-\n-        If True, subclasses are preserved.\n-    writeable : bool, optional\n-        .. versionadded:: 1.12\n-\n-        If set to False, the returned array will always be readonly.\n-        Otherwise it will be writable if the original array was. It\n-        is advisable to set this to False if possible (see Notes).\n-\n-    Returns\n-    -------\n-    view : ndarray\n-\n-    See also\n-    --------\n-    broadcast_to : broadcast an array to a given shape.\n-    reshape : reshape an array.\n-    lib.stride_tricks.sliding_window_view :\n-        userfriendly and safe function for the creation of sliding window views.\n-\n-    Notes\n-    -----\n-    ``as_strided`` creates a view into the array given the exact strides\n-    and shape. This means it manipulates the internal data structure of\n-    ndarray and, if done incorrectly, the array elements can point to\n-    invalid memory and can corrupt results or crash your program.\n-    It is advisable to always use the original ``x.strides`` when\n-    calculating new strides to avoid reliance on a contiguous memory\n-    layout.\n-\n-    Furthermore, arrays created with this function often contain self\n-    overlapping memory, so that two elements are identical.\n-    Vectorized write operations on such arrays will typically be\n-    unpredictable. They may even give different results for small, large,\n-    or transposed arrays.\n-\n-    Since writing to these arrays has to be tested and done with great\n-    care, you may want to use ``writeable=False`` to avoid accidental write\n-    operations.\n-\n-    For these reasons it is advisable to avoid ``as_strided`` when\n-    possible.\n-    \"\"\"\n-    # first convert input to array, possibly keeping subclass\n-    x = np.array(x, copy=False, subok=subok)\n-    interface = dict(x.__array_interface__)\n-    if shape is not None:\n-        interface['shape'] = tuple(shape)\n-    if strides is not None:\n-        interface['strides'] = tuple(strides)\n-\n-    array = np.asarray(DummyArray(interface, base=x))\n-    # The route via `__interface__` does not preserve structured\n-    # dtypes. Since dtype should remain unchanged, we set it explicitly.\n-    array.dtype = x.dtype\n-\n-    view = _maybe_view_as_subclass(x, array)\n-\n-    if view.flags.writeable and not writeable:\n-        view.flags.writeable = False\n-\n-    return view\n-\n-\n-def _sliding_window_view_dispatcher(x, window_shape, axis=None, *,\n-                                    subok=None, writeable=None):\n-    return (x,)\n-\n-\n-@array_function_dispatch(_sliding_window_view_dispatcher)\n-def sliding_window_view(x, window_shape, axis=None, *,\n-                        subok=False, writeable=False):\n-    \"\"\"\n-    Create a sliding window view into the array with the given window shape.\n-\n-    Also known as rolling or moving window, the window slides across all\n-    dimensions of the array and extracts subsets of the array at all window\n-    positions.\n-    \n-    .. versionadded:: 1.20.0\n-\n-    Parameters\n-    ----------\n-    x : array_like\n-        Array to create the sliding window view from.\n-    window_shape : int or tuple of int\n-        Size of window over each axis that takes part in the sliding window.\n-        If `axis` is not present, must have same length as the number of input\n-        array dimensions. Single integers `i` are treated as if they were the\n-        tuple `(i,)`.\n-    axis : int or tuple of int, optional\n-        Axis or axes along which the sliding window is applied.\n-        By default, the sliding window is applied to all axes and\n-        `window_shape[i]` will refer to axis `i` of `x`.\n-        If `axis` is given as a `tuple of int`, `window_shape[i]` will refer to\n-        the axis `axis[i]` of `x`.\n-        Single integers `i` are treated as if they were the tuple `(i,)`.\n-    subok : bool, optional\n-        If True, sub-classes will be passed-through, otherwise the returned\n-        array will be forced to be a base-class array (default).\n-    writeable : bool, optional\n-        When true, allow writing to the returned view. The default is false,\n-        as this should be used with caution: the returned view contains the\n-        same memory location multiple times, so writing to one location will\n-        cause others to change.\n-\n-    Returns\n-    -------\n-    view : ndarray\n-        Sliding window view of the array. The sliding window dimensions are\n-        inserted at the end, and the original dimensions are trimmed as\n-        required by the size of the sliding window.\n-        That is, ``view.shape = x_shape_trimmed + window_shape``, where\n-        ``x_shape_trimmed`` is ``x.shape`` with every entry reduced by one less\n-        than the corresponding window size.\n-\n-    See Also\n-    --------\n-    lib.stride_tricks.as_strided: A lower-level and less safe routine for\n-        creating arbitrary views from custom shape and strides.\n-    broadcast_to: broadcast an array to a given shape.\n-\n-    Notes\n-    -----\n-    For many applications using a sliding window view can be convenient, but\n-    potentially very slow. Often specialized solutions exist, for example:\n-\n-    - `scipy.signal.fftconvolve`\n-\n-    - filtering functions in `scipy.ndimage`\n-\n-    - moving window functions provided by\n-      `bottleneck <https://github.com/pydata/bottleneck>`_.\n-\n-    As a rough estimate, a sliding window approach with an input size of `N`\n-    and a window size of `W` will scale as `O(N*W)` where frequently a special\n-    algorithm can achieve `O(N)`. That means that the sliding window variant\n-    for a window size of 100 can be a 100 times slower than a more specialized\n-    version.\n-\n-    Nevertheless, for small window sizes, when no custom algorithm exists, or\n-    as a prototyping and developing tool, this function can be a good solution.\n-\n-    Examples\n-    --------\n-    >>> x = np.arange(6)\n-    >>> x.shape\n-    (6,)\n-    >>> v = sliding_window_view(x, 3)\n-    >>> v.shape\n-    (4, 3)\n-    >>> v\n-    array([[0, 1, 2],\n-           [1, 2, 3],\n-           [2, 3, 4],\n-           [3, 4, 5]])\n-\n-    This also works in more dimensions, e.g.\n-\n-    >>> i, j = np.ogrid[:3, :4]\n-    >>> x = 10*i + j\n-    >>> x.shape\n-    (3, 4)\n-    >>> x\n-    array([[ 0,  1,  2,  3],\n-           [10, 11, 12, 13],\n-           [20, 21, 22, 23]])\n-    >>> shape = (2,2)\n-    >>> v = sliding_window_view(x, shape)\n-    >>> v.shape\n-    (2, 3, 2, 2)\n-    >>> v\n-    array([[[[ 0,  1],\n-             [10, 11]],\n-            [[ 1,  2],\n-             [11, 12]],\n-            [[ 2,  3],\n-             [12, 13]]],\n-           [[[10, 11],\n-             [20, 21]],\n-            [[11, 12],\n-             [21, 22]],\n-            [[12, 13],\n-             [22, 23]]]])\n-\n-    The axis can be specified explicitly:\n-\n-    >>> v = sliding_window_view(x, 3, 0)\n-    >>> v.shape\n-    (1, 4, 3)\n-    >>> v\n-    array([[[ 0, 10, 20],\n-            [ 1, 11, 21],\n-            [ 2, 12, 22],\n-            [ 3, 13, 23]]])\n-\n-    The same axis can be used several times. In that case, every use reduces\n-    the corresponding original dimension:\n-\n-    >>> v = sliding_window_view(x, (2, 3), (1, 1))\n-    >>> v.shape\n-    (3, 1, 2, 3)\n-    >>> v\n-    array([[[[ 0,  1,  2],\n-             [ 1,  2,  3]]],\n-           [[[10, 11, 12],\n-             [11, 12, 13]]],\n-           [[[20, 21, 22],\n-             [21, 22, 23]]]])\n-\n-    Combining with stepped slicing (`::step`), this can be used to take sliding\n-    views which skip elements:\n-\n-    >>> x = np.arange(7)\n-    >>> sliding_window_view(x, 5)[:, ::2]\n-    array([[0, 2, 4],\n-           [1, 3, 5],\n-           [2, 4, 6]])\n-\n-    or views which move by multiple elements\n-\n-    >>> x = np.arange(7)\n-    >>> sliding_window_view(x, 3)[::2, :]\n-    array([[0, 1, 2],\n-           [2, 3, 4],\n-           [4, 5, 6]])\n-\n-    A common application of `sliding_window_view` is the calculation of running\n-    statistics. The simplest example is the\n-    `moving average <https://en.wikipedia.org/wiki/Moving_average>`_:\n-\n-    >>> x = np.arange(6)\n-    >>> x.shape\n-    (6,)\n-    >>> v = sliding_window_view(x, 3)\n-    >>> v.shape\n-    (4, 3)\n-    >>> v\n-    array([[0, 1, 2],\n-           [1, 2, 3],\n-           [2, 3, 4],\n-           [3, 4, 5]])\n-    >>> moving_average = v.mean(axis=-1)\n-    >>> moving_average\n-    array([1., 2., 3., 4.])\n-\n-    Note that a sliding window approach is often **not** optimal (see Notes).\n-    \"\"\"\n-    window_shape = (tuple(window_shape)\n-                    if np.iterable(window_shape)\n-                    else (window_shape,))\n-    # first convert input to array, possibly keeping subclass\n-    x = np.array(x, copy=False, subok=subok)\n-\n-    window_shape_array = np.array(window_shape)\n-    if np.any(window_shape_array < 0):\n-        raise ValueError('`window_shape` cannot contain negative values')\n-\n-    if axis is None:\n-        axis = tuple(range(x.ndim))\n-        if len(window_shape) != len(axis):\n-            raise ValueError(f'Since axis is `None`, must provide '\n-                             f'window_shape for all dimensions of `x`; '\n-                             f'got {len(window_shape)} window_shape elements '\n-                             f'and `x.ndim` is {x.ndim}.')\n-    else:\n-        axis = normalize_axis_tuple(axis, x.ndim, allow_duplicate=True)\n-        if len(window_shape) != len(axis):\n-            raise ValueError(f'Must provide matching length window_shape and '\n-                             f'axis; got {len(window_shape)} window_shape '\n-                             f'elements and {len(axis)} axes elements.')\n-\n-    out_strides = x.strides + tuple(x.strides[ax] for ax in axis)\n-\n-    # note: same axis can be windowed repeatedly\n-    x_shape_trimmed = list(x.shape)\n-    for ax, dim in zip(axis, window_shape):\n-        if x_shape_trimmed[ax] < dim:\n-            raise ValueError(\n-                'window shape cannot be larger than input array shape')\n-        x_shape_trimmed[ax] -= dim - 1\n-    out_shape = tuple(x_shape_trimmed) + window_shape\n-    return as_strided(x, strides=out_strides, shape=out_shape,\n-                      subok=subok, writeable=writeable)\n-\n-\n-def _broadcast_to(array, shape, subok, readonly):\n-    shape = tuple(shape) if np.iterable(shape) else (shape,)\n-    array = np.array(array, copy=False, subok=subok)\n-    if not shape and array.shape:\n-        raise ValueError('cannot broadcast a non-scalar to a scalar array')\n-    if any(size < 0 for size in shape):\n-        raise ValueError('all elements of broadcast shape must be non-'\n-                         'negative')\n-    extras = []\n-    it = np.nditer(\n-        (array,), flags=['multi_index', 'refs_ok', 'zerosize_ok'] + extras,\n-        op_flags=['readonly'], itershape=shape, order='C')\n-    with it:\n-        # never really has writebackifcopy semantics\n-        broadcast = it.itviews[0]\n-    result = _maybe_view_as_subclass(array, broadcast)\n-    # In a future version this will go away\n-    if not readonly and array.flags._writeable_no_warn:\n-        result.flags.writeable = True\n-        result.flags._warn_on_write = True\n-    return result\n-\n-\n-def _broadcast_to_dispatcher(array, shape, subok=None):\n-    return (array,)\n-\n-\n-@array_function_dispatch(_broadcast_to_dispatcher, module='numpy')\n-def broadcast_to(array, shape, subok=False):\n-    \"\"\"Broadcast an array to a new shape.\n-\n-    Parameters\n-    ----------\n-    array : array_like\n-        The array to broadcast.\n-    shape : tuple or int\n-        The shape of the desired array. A single integer ``i`` is interpreted\n-        as ``(i,)``.\n-    subok : bool, optional\n-        If True, then sub-classes will be passed-through, otherwise\n-        the returned array will be forced to be a base-class array (default).\n-\n-    Returns\n-    -------\n-    broadcast : array\n-        A readonly view on the original array with the given shape. It is\n-        typically not contiguous. Furthermore, more than one element of a\n-        broadcasted array may refer to a single memory location.\n-\n-    Raises\n-    ------\n-    ValueError\n-        If the array is not compatible with the new shape according to NumPy's\n-        broadcasting rules.\n-\n-    See Also\n-    --------\n-    broadcast\n-    broadcast_arrays\n-    broadcast_shapes\n-\n-    Notes\n-    -----\n-    .. versionadded:: 1.10.0\n-\n-    Examples\n-    --------\n-    >>> x = np.array([1, 2, 3])\n-    >>> np.broadcast_to(x, (3, 3))\n-    array([[1, 2, 3],\n-           [1, 2, 3],\n-           [1, 2, 3]])\n-    \"\"\"\n-    return _broadcast_to(array, shape, subok=subok, readonly=True)\n-\n-\n-def _broadcast_shape(*args):\n-    \"\"\"Returns the shape of the arrays that would result from broadcasting the\n-    supplied arrays against each other.\n-    \"\"\"\n-    # use the old-iterator because np.nditer does not handle size 0 arrays\n-    # consistently\n-    b = np.broadcast(*args[:32])\n-    # unfortunately, it cannot handle 32 or more arguments directly\n-    for pos in range(32, len(args), 31):\n-        # ironically, np.broadcast does not properly handle np.broadcast\n-        # objects (it treats them as scalars)\n-        # use broadcasting to avoid allocating the full array\n-        b = broadcast_to(0, b.shape)\n-        b = np.broadcast(b, *args[pos:(pos + 31)])\n-    return b.shape\n-\n-\n-@set_module('numpy')\n-def broadcast_shapes(*args):\n-    \"\"\"\n-    Broadcast the input shapes into a single shape.\n-\n-    :ref:`Learn more about broadcasting here <basics.broadcasting>`.\n-\n-    .. versionadded:: 1.20.0\n-\n-    Parameters\n-    ----------\n-    *args : tuples of ints, or ints\n-        The shapes to be broadcast against each other.\n-\n-    Returns\n-    -------\n-    tuple\n-        Broadcasted shape.\n-\n-    Raises\n-    ------\n-    ValueError\n-        If the shapes are not compatible and cannot be broadcast according\n-        to NumPy's broadcasting rules.\n-\n-    See Also\n-    --------\n-    broadcast\n-    broadcast_arrays\n-    broadcast_to\n-\n-    Examples\n-    --------\n-    >>> np.broadcast_shapes((1, 2), (3, 1), (3, 2))\n-    (3, 2)\n-\n-    >>> np.broadcast_shapes((6, 7), (5, 6, 1), (7,), (5, 1, 7))\n-    (5, 6, 7)\n-    \"\"\"\n-    arrays = [np.empty(x, dtype=[]) for x in args]\n-    return _broadcast_shape(*arrays)\n-\n-\n-def _broadcast_arrays_dispatcher(*args, subok=None):\n-    return args\n-\n-\n-@array_function_dispatch(_broadcast_arrays_dispatcher, module='numpy')\n-def broadcast_arrays(*args, subok=False):\n-    \"\"\"\n-    Broadcast any number of arrays against each other.\n-\n-    Parameters\n-    ----------\n-    *args : array_likes\n-        The arrays to broadcast.\n-\n-    subok : bool, optional\n-        If True, then sub-classes will be passed-through, otherwise\n-        the returned arrays will be forced to be a base-class array (default).\n-\n-    Returns\n-    -------\n-    broadcasted : list of arrays\n-        These arrays are views on the original arrays.  They are typically\n-        not contiguous.  Furthermore, more than one element of a\n-        broadcasted array may refer to a single memory location. If you need\n-        to write to the arrays, make copies first. While you can set the\n-        ``writable`` flag True, writing to a single output value may end up\n-        changing more than one location in the output array.\n-\n-        .. deprecated:: 1.17\n-            The output is currently marked so that if written to, a deprecation\n-            warning will be emitted. A future version will set the\n-            ``writable`` flag False so writing to it will raise an error.\n-\n-    See Also\n-    --------\n-    broadcast\n-    broadcast_to\n-    broadcast_shapes\n-\n-    Examples\n-    --------\n-    >>> x = np.array([[1,2,3]])\n-    >>> y = np.array([[4],[5]])\n-    >>> np.broadcast_arrays(x, y)\n-    [array([[1, 2, 3],\n-           [1, 2, 3]]), array([[4, 4, 4],\n-           [5, 5, 5]])]\n-\n-    Here is a useful idiom for getting contiguous copies instead of\n-    non-contiguous views.\n-\n-    >>> [np.array(a) for a in np.broadcast_arrays(x, y)]\n-    [array([[1, 2, 3],\n-           [1, 2, 3]]), array([[4, 4, 4],\n-           [5, 5, 5]])]\n-\n-    \"\"\"\n-    # nditer is not used here to avoid the limit of 32 arrays.\n-    # Otherwise, something like the following one-liner would suffice:\n-    # return np.nditer(args, flags=['multi_index', 'zerosize_ok'],\n-    #                  order='C').itviews\n-\n-    args = [np.array(_m, copy=False, subok=subok) for _m in args]\n-\n-    shape = _broadcast_shape(*args)\n-\n-    if all(array.shape == shape for array in args):\n-        # Common case where nothing needs to be broadcasted.\n-        return args\n-\n-    return [_broadcast_to(array, shape, subok=subok, readonly=False)\n-            for array in args]\n+from ._stride_tricks_impl import (\n+    DummyArray, as_strided, sliding_window_view\n+)\n",
            "comment_added_diff": {},
            "comment_deleted_diff": {
                "26": "        # if input was an ndarray subclass and subclasses were OK,",
                "27": "        # then view the result as that subclass.",
                "29": "        # Since we have done something akin to a view from original_array, we",
                "30": "        # should let the subclass finalize (if it has it implemented, i.e., is",
                "31": "        # not None).",
                "96": "    # first convert input to array, possibly keeping subclass",
                "105": "    # The route via `__interface__` does not preserve structured",
                "106": "    # dtypes. Since dtype should remain unchanged, we set it explicitly.",
                "304": "    # first convert input to array, possibly keeping subclass",
                "327": "    # note: same axis can be windowed repeatedly",
                "352": "        # never really has writebackifcopy semantics",
                "355": "    # In a future version this will go away",
                "419": "    # use the old-iterator because np.nditer does not handle size 0 arrays",
                "420": "    # consistently",
                "422": "    # unfortunately, it cannot handle 32 or more arguments directly",
                "424": "        # ironically, np.broadcast does not properly handle np.broadcast",
                "425": "        # objects (it treats them as scalars)",
                "426": "        # use broadcasting to avoid allocating the full array",
                "532": "    # nditer is not used here to avoid the limit of 32 arrays.",
                "533": "    # Otherwise, something like the following one-liner would suffice:",
                "534": "    # return np.nditer(args, flags=['multi_index', 'zerosize_ok'],",
                "535": "    #                  order='C').itviews",
                "542": "        # Common case where nothing needs to be broadcasted."
            },
            "comment_modified_diff": {}
        }
    ],
    "arrays.scalars.rst": [],
    "basics.rec.rst": [],
    "whatisnumpy.rst": [],
    "arrayterator.py": [],
    "test_classes.py": [],
    "test__exceptions.py": [],
    "test_casting_unittests.py": [],
    "test_financial_expired.py": [],
    "warnings_and_errors.pyi": [],
    "warnings_and_errors.py": [],
    "f2c.c": [],
    "f2c.h": [],
    "f2c_blas.c": [],
    "f2c_blas.c.patch": [],
    "f2c_c_lapack.c": [],
    "f2c_c_lapack.c.patch": [],
    "f2c_config.c": [],
    "f2c_lapack.c": [],
    "f2c_lapack.c.patch": [],
    "arrays.interface.rst": [],
    "arrays.ndarray.rst": [],
    "arrays.nditer.rst": [],
    "datetimes.rst": [],
    "routines.polynomials.classes.rst": [],
    "c-info.how-to-extend.rst": [],
    "meson": [],
    "LICENSE": [],
    "LICENSES_bundled.txt": [],
    "MANIFEST.in": [],
    "Makefile": [],
    "gitversion.py": [
        {
            "commit": "be896c9c652c010093d26bd0ff732dbfabcd6361",
            "timestamp": "2023-08-11T22:40:16+02:00",
            "author": "Stefan van der Walt",
            "commit_message": "MAINT: Remove versioneer (#24196)\n\nCo-authored-by: Ralf Gommers <ralf.gommers@gmail.com>",
            "additions": 98,
            "deletions": 0,
            "change_type": "ADD",
            "diff": "@@ -0,0 +1,98 @@\n+import os\n+import textwrap\n+\n+\n+def init_version():\n+    init = os.path.join(os.path.dirname(__file__), '../../pyproject.toml')\n+    data = open(init).readlines()\n+\n+    version_line = next(\n+        line for line in data if line.startswith('version =')\n+    )\n+\n+    version = version_line.strip().split(' = ')[1]\n+    version = version.replace('\"', '').replace(\"'\", '')\n+\n+    return version\n+\n+\n+def git_version(version):\n+    # Append last commit date and hash to dev version information,\n+    # if available\n+\n+    import subprocess\n+    import os.path\n+\n+    try:\n+        p = subprocess.Popen(\n+            ['git', 'log', '-1', '--format=\"%H %aI\"'],\n+            stdout=subprocess.PIPE,\n+            stderr=subprocess.PIPE,\n+            cwd=os.path.dirname(__file__),\n+        )\n+    except FileNotFoundError:\n+        pass\n+    else:\n+        out, err = p.communicate()\n+        if p.returncode == 0:\n+            git_hash, git_date = (\n+                out.decode('utf-8')\n+                .strip()\n+                .replace('\"', '')\n+                .split('T')[0]\n+                .replace('-', '')\n+                .split()\n+            )\n+\n+            # Only attach git tag to development versions\n+            if 'dev' in version:\n+                version += f'+git{git_date}.{git_hash[:7]}'\n+        else:\n+            git_hash = ''\n+\n+    return version, git_hash\n+\n+\n+if __name__ == \"__main__\":\n+    import argparse\n+\n+    parser = argparse.ArgumentParser()\n+    parser.add_argument('--write', help=\"Save version to this file\")\n+    parser.add_argument(\n+        '--meson-dist',\n+        help='Output path is relative to MESON_DIST_ROOT',\n+        action='store_true'\n+    )\n+    args = parser.parse_args()\n+\n+    version, git_hash = git_version(init_version())\n+\n+    # For NumPy 2.0, this should only have one field: `version`\n+    template = textwrap.dedent(f'''\n+        version = \"{version}\"\n+        __version__ = version\n+        full_version = version\n+\n+        git_revision = \"{git_hash}\"\n+        release = 'dev' not in version and '+' not in version\n+        short_version = version.split(\"+\")[0]\n+    ''')\n+\n+    if args.write:\n+        outfile = args.write\n+        if args.meson_dist:\n+            outfile = os.path.join(\n+                os.environ.get('MESON_DIST_ROOT', ''),\n+                outfile\n+            )\n+\n+        # Print human readable output path\n+        relpath = os.path.relpath(outfile)\n+        if relpath.startswith('.'):\n+            relpath = outfile\n+\n+        with open(outfile, 'w') as f:\n+            print(f'Saving version to {relpath}')\n+            f.write(template)\n+    else:\n+        print(version)\n",
            "comment_added_diff": {
                "20": "    # Append last commit date and hash to dev version information,",
                "21": "    # if available",
                "47": "            # Only attach git tag to development versions",
                "70": "    # For NumPy 2.0, this should only have one field: `version`",
                "89": "        # Print human readable output path"
            },
            "comment_deleted_diff": {},
            "comment_modified_diff": {}
        },
        {
            "commit": "e150c31194a3faff43791aa5ad3055db07f5e6e0",
            "timestamp": "2023-08-24T05:18:55+02:00",
            "author": "Ralf Gommers",
            "commit_message": "BUG: fix issue with git-version script, needs a shebang to run\n\nCloses gh-24514",
            "additions": 1,
            "deletions": 0,
            "change_type": "MODIFY",
            "diff": "@@ -1,3 +1,4 @@\n+#!/usr/bin/env python3\n import os\n import textwrap\n \n",
            "comment_added_diff": {
                "1": "#!/usr/bin/env python3"
            },
            "comment_deleted_diff": {},
            "comment_modified_diff": {}
        }
    ],
    "test_numpy_version.py": [],
    "modules.py": [],
    "modules.pyi": [],
    "version.pyi": [],
    "pyproject.toml.setuppy": [],
    "constants.rst": [],
    "function_base.pyi": [],
    "test_conversion_utils.py": [],
    "constants.py": [
        {
            "commit": "250e1479ce342d9d7ab8a592508f6ce892d4c98b",
            "timestamp": "2023-08-12T22:01:16+02:00",
            "author": "Mateusz Sok\u00f3\u0142",
            "commit_message": "API: Cleaning `numpy/__init__.py` and main namespace - Part 2 [NEP 52] (#24357)\n\n[skip ci]",
            "additions": 0,
            "deletions": 412,
            "change_type": "DELETE",
            "diff": "@@ -1,412 +0,0 @@\n-\"\"\"\n-=========\n-Constants\n-=========\n-\n-.. currentmodule:: numpy\n-\n-NumPy includes several constants:\n-\n-%(constant_list)s\n-\"\"\"\n-#\n-# Note: the docstring is autogenerated.\n-#\n-import re\n-import textwrap\n-\n-# Maintain same format as in numpy.add_newdocs\n-constants = []\n-def add_newdoc(module, name, doc):\n-    constants.append((name, doc))\n-\n-add_newdoc('numpy', 'pi',\n-    \"\"\"\n-    ``pi = 3.1415926535897932384626433...``\n-\n-    References\n-    ----------\n-    https://en.wikipedia.org/wiki/Pi\n-\n-    \"\"\")\n-\n-add_newdoc('numpy', 'e',\n-    \"\"\"\n-    Euler's constant, base of natural logarithms, Napier's constant.\n-\n-    ``e = 2.71828182845904523536028747135266249775724709369995...``\n-\n-    See Also\n-    --------\n-    exp : Exponential function\n-    log : Natural logarithm\n-\n-    References\n-    ----------\n-    https://en.wikipedia.org/wiki/E_%28mathematical_constant%29\n-\n-    \"\"\")\n-\n-add_newdoc('numpy', 'euler_gamma',\n-    \"\"\"\n-    ``\u03b3 = 0.5772156649015328606065120900824024310421...``\n-\n-    References\n-    ----------\n-    https://en.wikipedia.org/wiki/Euler-Mascheroni_constant\n-\n-    \"\"\")\n-\n-add_newdoc('numpy', 'inf',\n-    \"\"\"\n-    IEEE 754 floating point representation of (positive) infinity.\n-\n-    Returns\n-    -------\n-    y : float\n-        A floating point representation of positive infinity.\n-\n-    See Also\n-    --------\n-    isinf : Shows which elements are positive or negative infinity\n-\n-    isposinf : Shows which elements are positive infinity\n-\n-    isneginf : Shows which elements are negative infinity\n-\n-    isnan : Shows which elements are Not a Number\n-\n-    isfinite : Shows which elements are finite (not one of Not a Number,\n-    positive infinity and negative infinity)\n-\n-    Notes\n-    -----\n-    NumPy uses the IEEE Standard for Binary Floating-Point for Arithmetic\n-    (IEEE 754). This means that Not a Number is not equivalent to infinity.\n-    Also that positive infinity is not equivalent to negative infinity. But\n-    infinity is equivalent to positive infinity.\n-\n-    `Inf`, `Infinity`, `PINF` and `infty` are aliases for `inf`.\n-\n-    Examples\n-    --------\n-    >>> np.inf\n-    inf\n-    >>> np.array([1]) / 0.\n-    array([ Inf])\n-\n-    \"\"\")\n-\n-add_newdoc('numpy', 'nan',\n-    \"\"\"\n-    IEEE 754 floating point representation of Not a Number (NaN).\n-\n-    Returns\n-    -------\n-    y : A floating point representation of Not a Number.\n-\n-    See Also\n-    --------\n-    isnan : Shows which elements are Not a Number.\n-\n-    isfinite : Shows which elements are finite (not one of\n-    Not a Number, positive infinity and negative infinity)\n-\n-    Notes\n-    -----\n-    NumPy uses the IEEE Standard for Binary Floating-Point for Arithmetic\n-    (IEEE 754). This means that Not a Number is not equivalent to infinity.\n-\n-    `NaN` and `NAN` are aliases of `nan`.\n-\n-    Examples\n-    --------\n-    >>> np.nan\n-    nan\n-    >>> np.log(-1)\n-    nan\n-    >>> np.log([-1, 1, 2])\n-    array([        NaN,  0.        ,  0.69314718])\n-\n-    \"\"\")\n-\n-add_newdoc('numpy', 'newaxis',\n-    \"\"\"\n-    A convenient alias for None, useful for indexing arrays.\n-\n-    Examples\n-    --------\n-    >>> newaxis is None\n-    True\n-    >>> x = np.arange(3)\n-    >>> x\n-    array([0, 1, 2])\n-    >>> x[:, newaxis]\n-    array([[0],\n-    [1],\n-    [2]])\n-    >>> x[:, newaxis, newaxis]\n-    array([[[0]],\n-    [[1]],\n-    [[2]]])\n-    >>> x[:, newaxis] * x\n-    array([[0, 0, 0],\n-    [0, 1, 2],\n-    [0, 2, 4]])\n-\n-    Outer product, same as ``outer(x, y)``:\n-\n-    >>> y = np.arange(3, 6)\n-    >>> x[:, newaxis] * y\n-    array([[ 0,  0,  0],\n-    [ 3,  4,  5],\n-    [ 6,  8, 10]])\n-\n-    ``x[newaxis, :]`` is equivalent to ``x[newaxis]`` and ``x[None]``:\n-\n-    >>> x[newaxis, :].shape\n-    (1, 3)\n-    >>> x[newaxis].shape\n-    (1, 3)\n-    >>> x[None].shape\n-    (1, 3)\n-    >>> x[:, newaxis].shape\n-    (3, 1)\n-\n-    \"\"\")\n-\n-add_newdoc('numpy', 'NZERO',\n-    \"\"\"\n-    IEEE 754 floating point representation of negative zero.\n-\n-    Returns\n-    -------\n-    y : float\n-        A floating point representation of negative zero.\n-\n-    See Also\n-    --------\n-    PZERO : Defines positive zero.\n-\n-    isinf : Shows which elements are positive or negative infinity.\n-\n-    isposinf : Shows which elements are positive infinity.\n-\n-    isneginf : Shows which elements are negative infinity.\n-\n-    isnan : Shows which elements are Not a Number.\n-\n-    isfinite : Shows which elements are finite - not one of\n-               Not a Number, positive infinity and negative infinity.\n-\n-    Notes\n-    -----\n-    NumPy uses the IEEE Standard for Binary Floating-Point for Arithmetic\n-    (IEEE 754). Negative zero is considered to be a finite number.\n-\n-    Examples\n-    --------\n-    >>> np.NZERO\n-    -0.0\n-    >>> np.PZERO\n-    0.0\n-\n-    >>> np.isfinite([np.NZERO])\n-    array([ True])\n-    >>> np.isnan([np.NZERO])\n-    array([False])\n-    >>> np.isinf([np.NZERO])\n-    array([False])\n-\n-    \"\"\")\n-\n-add_newdoc('numpy', 'PZERO',\n-    \"\"\"\n-    IEEE 754 floating point representation of positive zero.\n-\n-    Returns\n-    -------\n-    y : float\n-        A floating point representation of positive zero.\n-\n-    See Also\n-    --------\n-    NZERO : Defines negative zero.\n-\n-    isinf : Shows which elements are positive or negative infinity.\n-\n-    isposinf : Shows which elements are positive infinity.\n-\n-    isneginf : Shows which elements are negative infinity.\n-\n-    isnan : Shows which elements are Not a Number.\n-\n-    isfinite : Shows which elements are finite - not one of\n-               Not a Number, positive infinity and negative infinity.\n-\n-    Notes\n-    -----\n-    NumPy uses the IEEE Standard for Binary Floating-Point for Arithmetic\n-    (IEEE 754). Positive zero is considered to be a finite number.\n-\n-    Examples\n-    --------\n-    >>> np.PZERO\n-    0.0\n-    >>> np.NZERO\n-    -0.0\n-\n-    >>> np.isfinite([np.PZERO])\n-    array([ True])\n-    >>> np.isnan([np.PZERO])\n-    array([False])\n-    >>> np.isinf([np.PZERO])\n-    array([False])\n-\n-    \"\"\")\n-\n-add_newdoc('numpy', 'NAN',\n-    \"\"\"\n-    IEEE 754 floating point representation of Not a Number (NaN).\n-\n-    `NaN` and `NAN` are equivalent definitions of `nan`. Please use\n-    `nan` instead of `NAN`.\n-\n-    See Also\n-    --------\n-    nan\n-\n-    \"\"\")\n-\n-add_newdoc('numpy', 'NaN',\n-    \"\"\"\n-    IEEE 754 floating point representation of Not a Number (NaN).\n-\n-    `NaN` and `NAN` are equivalent definitions of `nan`. Please use\n-    `nan` instead of `NaN`.\n-\n-    See Also\n-    --------\n-    nan\n-\n-    \"\"\")\n-\n-add_newdoc('numpy', 'NINF',\n-    \"\"\"\n-    IEEE 754 floating point representation of negative infinity.\n-\n-    Returns\n-    -------\n-    y : float\n-        A floating point representation of negative infinity.\n-\n-    See Also\n-    --------\n-    isinf : Shows which elements are positive or negative infinity\n-\n-    isposinf : Shows which elements are positive infinity\n-\n-    isneginf : Shows which elements are negative infinity\n-\n-    isnan : Shows which elements are Not a Number\n-\n-    isfinite : Shows which elements are finite (not one of Not a Number,\n-    positive infinity and negative infinity)\n-\n-    Notes\n-    -----\n-    NumPy uses the IEEE Standard for Binary Floating-Point for Arithmetic\n-    (IEEE 754). This means that Not a Number is not equivalent to infinity.\n-    Also that positive infinity is not equivalent to negative infinity. But\n-    infinity is equivalent to positive infinity.\n-\n-    Examples\n-    --------\n-    >>> np.NINF\n-    -inf\n-    >>> np.log(0)\n-    -inf\n-\n-    \"\"\")\n-\n-add_newdoc('numpy', 'PINF',\n-    \"\"\"\n-    IEEE 754 floating point representation of (positive) infinity.\n-\n-    Use `inf` because `Inf`, `Infinity`, `PINF` and `infty` are aliases for\n-    `inf`. For more details, see `inf`.\n-\n-    See Also\n-    --------\n-    inf\n-\n-    \"\"\")\n-\n-add_newdoc('numpy', 'infty',\n-    \"\"\"\n-    IEEE 754 floating point representation of (positive) infinity.\n-\n-    Use `inf` because `Inf`, `Infinity`, `PINF` and `infty` are aliases for\n-    `inf`. For more details, see `inf`.\n-\n-    See Also\n-    --------\n-    inf\n-\n-    \"\"\")\n-\n-add_newdoc('numpy', 'Inf',\n-    \"\"\"\n-    IEEE 754 floating point representation of (positive) infinity.\n-\n-    Use `inf` because `Inf`, `Infinity`, `PINF` and `infty` are aliases for\n-    `inf`. For more details, see `inf`.\n-\n-    See Also\n-    --------\n-    inf\n-\n-    \"\"\")\n-\n-add_newdoc('numpy', 'Infinity',\n-    \"\"\"\n-    IEEE 754 floating point representation of (positive) infinity.\n-\n-    Use `inf` because `Inf`, `Infinity`, `PINF` and `infty` are aliases for\n-    `inf`. For more details, see `inf`.\n-\n-    See Also\n-    --------\n-    inf\n-\n-    \"\"\")\n-\n-\n-if __doc__:\n-    constants_str = []\n-    constants.sort()\n-    for name, doc in constants:\n-        s = textwrap.dedent(doc).replace(\"\\n\", \"\\n    \")\n-\n-        # Replace sections by rubrics\n-        lines = s.split(\"\\n\")\n-        new_lines = []\n-        for line in lines:\n-            m = re.match(r'^(\\s+)[-=]+\\s*$', line)\n-            if m and new_lines:\n-                prev = textwrap.dedent(new_lines.pop())\n-                new_lines.append('%s.. rubric:: %s' % (m.group(1), prev))\n-                new_lines.append('')\n-            else:\n-                new_lines.append(line)\n-        s = \"\\n\".join(new_lines)\n-\n-        # Done.\n-        constants_str.append(\"\"\".. data:: %s\\n    %s\"\"\" % (name, s))\n-    constants_str = \"\\n\".join(constants_str)\n-\n-    __doc__ = __doc__ % dict(constant_list=constants_str)\n-    del constants_str, name, doc\n-    del line, lines, new_lines, m, s, prev\n-\n-del constants, add_newdoc\n",
            "comment_added_diff": {},
            "comment_deleted_diff": {
                "12": "#",
                "13": "# Note: the docstring is autogenerated.",
                "14": "#",
                "18": "# Maintain same format as in numpy.add_newdocs",
                "391": "        # Replace sections by rubrics",
                "404": "        # Done."
            },
            "comment_modified_diff": {}
        },
        {
            "commit": "5a5f5879c9d39d23dea3eefbf26831af1b404b82",
            "timestamp": "2023-08-24T22:50:13+02:00",
            "author": "Mateusz Sok\u00f3\u0142",
            "commit_message": "API: Remove np.mat from matrixlib module",
            "additions": 205,
            "deletions": 0,
            "change_type": "ADD",
            "diff": "@@ -0,0 +1,205 @@\n+\"\"\"\n+=========\n+Constants\n+=========\n+\n+.. currentmodule:: numpy\n+\n+NumPy includes several constants:\n+\n+%(constant_list)s\n+\"\"\"\n+#\n+# Note: the docstring is autogenerated.\n+#\n+import re\n+import textwrap\n+\n+# Maintain same format as in numpy.add_newdocs\n+constants = []\n+def add_newdoc(module, name, doc):\n+    constants.append((name, doc))\n+\n+\n+add_newdoc('numpy', 'pi',\n+    \"\"\"\n+    ``pi = 3.1415926535897932384626433...``\n+\n+    References\n+    ----------\n+    https://en.wikipedia.org/wiki/Pi\n+\n+    \"\"\")\n+\n+add_newdoc('numpy', 'e',\n+    \"\"\"\n+    Euler's constant, base of natural logarithms, Napier's constant.\n+\n+    ``e = 2.71828182845904523536028747135266249775724709369995...``\n+\n+    See Also\n+    --------\n+    exp : Exponential function\n+    log : Natural logarithm\n+\n+    References\n+    ----------\n+    https://en.wikipedia.org/wiki/E_%28mathematical_constant%29\n+\n+    \"\"\")\n+\n+add_newdoc('numpy', 'euler_gamma',\n+    \"\"\"\n+    ``\u03b3 = 0.5772156649015328606065120900824024310421...``\n+\n+    References\n+    ----------\n+    https://en.wikipedia.org/wiki/Euler-Mascheroni_constant\n+\n+    \"\"\")\n+\n+add_newdoc('numpy', 'inf',\n+    \"\"\"\n+    IEEE 754 floating point representation of (positive) infinity.\n+\n+    Returns\n+    -------\n+    y : float\n+        A floating point representation of positive infinity.\n+\n+    See Also\n+    --------\n+    isinf : Shows which elements are positive or negative infinity\n+\n+    isposinf : Shows which elements are positive infinity\n+\n+    isneginf : Shows which elements are negative infinity\n+\n+    isnan : Shows which elements are Not a Number\n+\n+    isfinite : Shows which elements are finite (not one of Not a Number,\n+    positive infinity and negative infinity)\n+\n+    Notes\n+    -----\n+    NumPy uses the IEEE Standard for Binary Floating-Point for Arithmetic\n+    (IEEE 754). This means that Not a Number is not equivalent to infinity.\n+    Also that positive infinity is not equivalent to negative infinity. But\n+    infinity is equivalent to positive infinity.\n+\n+    Examples\n+    --------\n+    >>> np.inf\n+    inf\n+    >>> np.array([1]) / 0.\n+    array([ Inf])\n+\n+    \"\"\")\n+\n+add_newdoc('numpy', 'nan',\n+    \"\"\"\n+    IEEE 754 floating point representation of Not a Number (NaN).\n+\n+    Returns\n+    -------\n+    y : A floating point representation of Not a Number.\n+\n+    See Also\n+    --------\n+    isnan : Shows which elements are Not a Number.\n+\n+    isfinite : Shows which elements are finite (not one of\n+    Not a Number, positive infinity and negative infinity)\n+\n+    Notes\n+    -----\n+    NumPy uses the IEEE Standard for Binary Floating-Point for Arithmetic\n+    (IEEE 754). This means that Not a Number is not equivalent to infinity.\n+\n+    `NaN` and `NAN` are aliases of `nan`.\n+\n+    Examples\n+    --------\n+    >>> np.nan\n+    nan\n+    >>> np.log(-1)\n+    nan\n+    >>> np.log([-1, 1, 2])\n+    array([        NaN,  0.        ,  0.69314718])\n+\n+    \"\"\")\n+\n+add_newdoc('numpy', 'newaxis',\n+    \"\"\"\n+    A convenient alias for None, useful for indexing arrays.\n+\n+    Examples\n+    --------\n+    >>> newaxis is None\n+    True\n+    >>> x = np.arange(3)\n+    >>> x\n+    array([0, 1, 2])\n+    >>> x[:, newaxis]\n+    array([[0],\n+    [1],\n+    [2]])\n+    >>> x[:, newaxis, newaxis]\n+    array([[[0]],\n+    [[1]],\n+    [[2]]])\n+    >>> x[:, newaxis] * x\n+    array([[0, 0, 0],\n+    [0, 1, 2],\n+    [0, 2, 4]])\n+\n+    Outer product, same as ``outer(x, y)``:\n+\n+    >>> y = np.arange(3, 6)\n+    >>> x[:, newaxis] * y\n+    array([[ 0,  0,  0],\n+    [ 3,  4,  5],\n+    [ 6,  8, 10]])\n+\n+    ``x[newaxis, :]`` is equivalent to ``x[newaxis]`` and ``x[None]``:\n+\n+    >>> x[newaxis, :].shape\n+    (1, 3)\n+    >>> x[newaxis].shape\n+    (1, 3)\n+    >>> x[None].shape\n+    (1, 3)\n+    >>> x[:, newaxis].shape\n+    (3, 1)\n+\n+    \"\"\")\n+\n+\n+if __doc__:\n+    constants_str = []\n+    constants.sort()\n+    for name, doc in constants:\n+        s = textwrap.dedent(doc).replace(\"\\n\", \"\\n    \")\n+\n+        # Replace sections by rubrics\n+        lines = s.split(\"\\n\")\n+        new_lines = []\n+        for line in lines:\n+            m = re.match(r'^(\\s+)[-=]+\\s*$', line)\n+            if m and new_lines:\n+                prev = textwrap.dedent(new_lines.pop())\n+                new_lines.append('%s.. rubric:: %s' % (m.group(1), prev))\n+                new_lines.append('')\n+            else:\n+                new_lines.append(line)\n+        s = \"\\n\".join(new_lines)\n+\n+        # Done.\n+        constants_str.append(\"\"\".. data:: %s\\n    %s\"\"\" % (name, s))\n+    constants_str = \"\\n\".join(constants_str)\n+\n+    __doc__ = __doc__ % dict(constant_list=constants_str)\n+    del constants_str, name, doc\n+    del line, lines, new_lines, m, s, prev\n+\n+del constants, add_newdoc\n",
            "comment_added_diff": {
                "12": "#",
                "13": "# Note: the docstring is autogenerated.",
                "14": "#",
                "18": "# Maintain same format as in numpy.add_newdocs",
                "184": "        # Replace sections by rubrics",
                "197": "        # Done."
            },
            "comment_deleted_diff": {},
            "comment_modified_diff": {}
        },
        {
            "commit": "84e4210e12d6289492cdee0d2b8de90cabad751b",
            "timestamp": "2023-08-24T22:50:13+02:00",
            "author": "Mateusz Sok\u00f3\u0142",
            "commit_message": "API: Apply review comments",
            "additions": 0,
            "deletions": 205,
            "change_type": "DELETE",
            "diff": "@@ -1,205 +0,0 @@\n-\"\"\"\n-=========\n-Constants\n-=========\n-\n-.. currentmodule:: numpy\n-\n-NumPy includes several constants:\n-\n-%(constant_list)s\n-\"\"\"\n-#\n-# Note: the docstring is autogenerated.\n-#\n-import re\n-import textwrap\n-\n-# Maintain same format as in numpy.add_newdocs\n-constants = []\n-def add_newdoc(module, name, doc):\n-    constants.append((name, doc))\n-\n-\n-add_newdoc('numpy', 'pi',\n-    \"\"\"\n-    ``pi = 3.1415926535897932384626433...``\n-\n-    References\n-    ----------\n-    https://en.wikipedia.org/wiki/Pi\n-\n-    \"\"\")\n-\n-add_newdoc('numpy', 'e',\n-    \"\"\"\n-    Euler's constant, base of natural logarithms, Napier's constant.\n-\n-    ``e = 2.71828182845904523536028747135266249775724709369995...``\n-\n-    See Also\n-    --------\n-    exp : Exponential function\n-    log : Natural logarithm\n-\n-    References\n-    ----------\n-    https://en.wikipedia.org/wiki/E_%28mathematical_constant%29\n-\n-    \"\"\")\n-\n-add_newdoc('numpy', 'euler_gamma',\n-    \"\"\"\n-    ``\u03b3 = 0.5772156649015328606065120900824024310421...``\n-\n-    References\n-    ----------\n-    https://en.wikipedia.org/wiki/Euler-Mascheroni_constant\n-\n-    \"\"\")\n-\n-add_newdoc('numpy', 'inf',\n-    \"\"\"\n-    IEEE 754 floating point representation of (positive) infinity.\n-\n-    Returns\n-    -------\n-    y : float\n-        A floating point representation of positive infinity.\n-\n-    See Also\n-    --------\n-    isinf : Shows which elements are positive or negative infinity\n-\n-    isposinf : Shows which elements are positive infinity\n-\n-    isneginf : Shows which elements are negative infinity\n-\n-    isnan : Shows which elements are Not a Number\n-\n-    isfinite : Shows which elements are finite (not one of Not a Number,\n-    positive infinity and negative infinity)\n-\n-    Notes\n-    -----\n-    NumPy uses the IEEE Standard for Binary Floating-Point for Arithmetic\n-    (IEEE 754). This means that Not a Number is not equivalent to infinity.\n-    Also that positive infinity is not equivalent to negative infinity. But\n-    infinity is equivalent to positive infinity.\n-\n-    Examples\n-    --------\n-    >>> np.inf\n-    inf\n-    >>> np.array([1]) / 0.\n-    array([ Inf])\n-\n-    \"\"\")\n-\n-add_newdoc('numpy', 'nan',\n-    \"\"\"\n-    IEEE 754 floating point representation of Not a Number (NaN).\n-\n-    Returns\n-    -------\n-    y : A floating point representation of Not a Number.\n-\n-    See Also\n-    --------\n-    isnan : Shows which elements are Not a Number.\n-\n-    isfinite : Shows which elements are finite (not one of\n-    Not a Number, positive infinity and negative infinity)\n-\n-    Notes\n-    -----\n-    NumPy uses the IEEE Standard for Binary Floating-Point for Arithmetic\n-    (IEEE 754). This means that Not a Number is not equivalent to infinity.\n-\n-    `NaN` and `NAN` are aliases of `nan`.\n-\n-    Examples\n-    --------\n-    >>> np.nan\n-    nan\n-    >>> np.log(-1)\n-    nan\n-    >>> np.log([-1, 1, 2])\n-    array([        NaN,  0.        ,  0.69314718])\n-\n-    \"\"\")\n-\n-add_newdoc('numpy', 'newaxis',\n-    \"\"\"\n-    A convenient alias for None, useful for indexing arrays.\n-\n-    Examples\n-    --------\n-    >>> newaxis is None\n-    True\n-    >>> x = np.arange(3)\n-    >>> x\n-    array([0, 1, 2])\n-    >>> x[:, newaxis]\n-    array([[0],\n-    [1],\n-    [2]])\n-    >>> x[:, newaxis, newaxis]\n-    array([[[0]],\n-    [[1]],\n-    [[2]]])\n-    >>> x[:, newaxis] * x\n-    array([[0, 0, 0],\n-    [0, 1, 2],\n-    [0, 2, 4]])\n-\n-    Outer product, same as ``outer(x, y)``:\n-\n-    >>> y = np.arange(3, 6)\n-    >>> x[:, newaxis] * y\n-    array([[ 0,  0,  0],\n-    [ 3,  4,  5],\n-    [ 6,  8, 10]])\n-\n-    ``x[newaxis, :]`` is equivalent to ``x[newaxis]`` and ``x[None]``:\n-\n-    >>> x[newaxis, :].shape\n-    (1, 3)\n-    >>> x[newaxis].shape\n-    (1, 3)\n-    >>> x[None].shape\n-    (1, 3)\n-    >>> x[:, newaxis].shape\n-    (3, 1)\n-\n-    \"\"\")\n-\n-\n-if __doc__:\n-    constants_str = []\n-    constants.sort()\n-    for name, doc in constants:\n-        s = textwrap.dedent(doc).replace(\"\\n\", \"\\n    \")\n-\n-        # Replace sections by rubrics\n-        lines = s.split(\"\\n\")\n-        new_lines = []\n-        for line in lines:\n-            m = re.match(r'^(\\s+)[-=]+\\s*$', line)\n-            if m and new_lines:\n-                prev = textwrap.dedent(new_lines.pop())\n-                new_lines.append('%s.. rubric:: %s' % (m.group(1), prev))\n-                new_lines.append('')\n-            else:\n-                new_lines.append(line)\n-        s = \"\\n\".join(new_lines)\n-\n-        # Done.\n-        constants_str.append(\"\"\".. data:: %s\\n    %s\"\"\" % (name, s))\n-    constants_str = \"\\n\".join(constants_str)\n-\n-    __doc__ = __doc__ % dict(constant_list=constants_str)\n-    del constants_str, name, doc\n-    del line, lines, new_lines, m, s, prev\n-\n-del constants, add_newdoc\n",
            "comment_added_diff": {},
            "comment_deleted_diff": {
                "12": "#",
                "13": "# Note: the docstring is autogenerated.",
                "14": "#",
                "18": "# Maintain same format as in numpy.add_newdocs",
                "184": "        # Replace sections by rubrics",
                "197": "        # Done."
            },
            "comment_modified_diff": {}
        }
    ],
    "arrayterator.pyi": [],
    "24321.python_removal.rst": [],
    "iterator.rst": [],
    "polyutils.py": [
        {
            "commit": "ced21a6fe3066b1b0198556e4212a970943a94ce",
            "timestamp": "2023-08-24T06:04:51+02:00",
            "author": "Pieter Eendebak",
            "commit_message": "DEP: Replace deprecation warning for non-integral arguments with exception (#24469)\n\n[skip ci]",
            "additions": 6,
            "deletions": 21,
            "change_type": "MODIFY",
            "diff": "@@ -716,41 +716,26 @@ def _pow(mul_f, c, pow, maxpower):\n         return prd\n \n \n-def _deprecate_as_int(x, desc):\n+def _as_int(x, desc):\n     \"\"\"\n-    Like `operator.index`, but emits a deprecation warning when passed a float\n+    Like `operator.index`, but emits a custom exception when passed an \n+    incorrect type\n \n     Parameters\n     ----------\n-    x : int-like, or float with integral value\n+    x : int-like\n         Value to interpret as an integer\n     desc : str\n         description to include in any error message\n \n     Raises\n     ------\n-    TypeError : if x is a non-integral float or non-numeric\n-    DeprecationWarning : if x is an integral float\n+    TypeError : if x is a float or non-numeric\n     \"\"\"\n     try:\n         return operator.index(x)\n     except TypeError as e:\n-        # Numpy 1.17.0, 2019-03-11\n-        try:\n-            ix = int(x)\n-        except TypeError:\n-            pass\n-        else:\n-            if ix == x:\n-                warnings.warn(\n-                    f\"In future, this will raise TypeError, as {desc} will \"\n-                    \"need to be an integer not just an integral float.\",\n-                    DeprecationWarning,\n-                    stacklevel=3\n-                )\n-                return ix\n-\n-        raise TypeError(f\"{desc} must be an integer\") from e\n+        raise TypeError(f\"{desc} must be an integer, received {x}\") from e\n \n \n def format_float(x, parens=False):\n",
            "comment_added_diff": {},
            "comment_deleted_diff": {
                "738": "        # Numpy 1.17.0, 2019-03-11"
            },
            "comment_modified_diff": {}
        },
        {
            "commit": "899e735c6144bfdcc7b4f4091314eaa6910fe9bb",
            "timestamp": "2023-08-24T06:47:40+02:00",
            "author": "Mateusz Sok\u00f3\u0142",
            "commit_message": "MAINT: Move `RankWarning` to exceptions module (#24476)",
            "additions": 2,
            "deletions": 17,
            "change_type": "MODIFY",
            "diff": "@@ -4,14 +4,6 @@\n This module provides: error and warning objects; a polynomial base class;\n and some routines used in both the `polynomial` and `chebyshev` modules.\n \n-Warning objects\n----------------\n-\n-.. autosummary::\n-   :toctree: generated/\n-\n-   RankWarning  raised in least-squares fit for rank-deficient matrix.\n-\n Functions\n ---------\n \n@@ -34,20 +26,12 @@\n \n from numpy.core.multiarray import dragon4_positional, dragon4_scientific\n from numpy.core.umath import absolute\n+from numpy.exceptions import RankWarning\n \n __all__ = [\n-    'RankWarning', 'as_series', 'trimseq',\n-    'trimcoef', 'getdomain', 'mapdomain', 'mapparms',\n+    'as_series', 'trimseq', 'trimcoef', 'getdomain', 'mapdomain', 'mapparms',\n     'format_float']\n \n-#\n-# Warnings and Exceptions\n-#\n-\n-class RankWarning(UserWarning):\n-    \"\"\"Issued by chebfit when the design matrix is rank deficient.\"\"\"\n-    pass\n-\n #\n # Helper functions to convert inputs to 1-D arrays\n #\n",
            "comment_added_diff": {},
            "comment_deleted_diff": {
                "43": "#",
                "44": "# Warnings and Exceptions",
                "45": "#"
            },
            "comment_modified_diff": {}
        }
    ],
    "test_polyutils.py": [],
    "1.16.0-notes.rst": [],
    "24494.expired.rst": [],
    "mypy.ini": [],
    "ufunc_config.py": [],
    "arithmetic.pyi": [],
    "bitwise_ops.pyi": [],
    "mod.pyi": [],
    "nbit_base_example.pyi": [],
    "type_check.pyi": [],
    "test_simd_module.py": [],
    "routines.polynomials.poly1d.rst": [],
    "polynomial.pyi": [],
    "polyutils.pyi": [],
    "_expired_attrs_2_0.py": [],
    "_histograms_impl.py": [],
    "_histograms_impl.pyi": [],
    "gh24008.f": [],
    "test_string.py": [],
    "arrays.dtypes.rst": [],
    "1.13.0-notes.rst": [],
    "basics.types.rst": [],
    "_char_codes.py": [],
    "timer_comparison.py": [],
    "hermite.pyi": [],
    "comparisons.py": [],
    "print_coercion_tables.py": [],
    "bench_shape_base.py": [],
    "test_defmatrix.py": [],
    "routines.array-manipulation.rst": [],
    "_nanfunctions_impl.py": [],
    "_nanfunctions_impl.pyi": [],
    "doc_requirements.txt": [],
    "iso_c_oddity.f90": [],
    "test_f2cmap.py": [
        {
            "commit": "a34d2bca3e3c53ca1ba2b4f01174bf5fcfa463f0",
            "timestamp": "2023-08-26T12:44:16+00:00",
            "author": "Rohit Goswami",
            "commit_message": "TST: Add a test for gh-24553",
            "additions": 11,
            "deletions": 0,
            "change_type": "MODIFY",
            "diff": "@@ -13,3 +13,14 @@ def test_long_long_map(self):\n         out = self.module.func1(inp)\n         exp_out = 3\n         assert out == exp_out\n+\n+class TestISOCmap(util.F2PyTest):\n+    sources = [\n+        util.getpath(\"tests\", \"src\", \"f2cmap\", \"iso_c_oddity.f90\"),\n+    ]\n+\n+    # gh-24553\n+    def test_c_double(self):\n+        out = self.module.coddity.c_add(1, 2)\n+        exp_out = 3\n+        assert  out == exp_out\n",
            "comment_added_diff": {
                "22": "    # gh-24553"
            },
            "comment_deleted_diff": {},
            "comment_modified_diff": {}
        },
        {
            "commit": "fab384365ef6fcf6574d2f7a3c9d8f9bcad57785",
            "timestamp": "2023-08-26T20:09:21+00:00",
            "author": "Rohit Goswami",
            "commit_message": "TST: Add more tests for iso_c_binding\n\nFunctions and subroutines",
            "additions": 0,
            "deletions": 11,
            "change_type": "MODIFY",
            "diff": "@@ -13,14 +13,3 @@ def test_long_long_map(self):\n         out = self.module.func1(inp)\n         exp_out = 3\n         assert out == exp_out\n-\n-class TestISOCmap(util.F2PyTest):\n-    sources = [\n-        util.getpath(\"tests\", \"src\", \"f2cmap\", \"iso_c_oddity.f90\"),\n-    ]\n-\n-    # gh-24553\n-    def test_c_double(self):\n-        out = self.module.coddity.c_add(1, 2)\n-        exp_out = 3\n-        assert  out == exp_out\n",
            "comment_added_diff": {},
            "comment_deleted_diff": {
                "22": "    # gh-24553"
            },
            "comment_modified_diff": {}
        }
    ],
    "_isocbind.py": [],
    "routines.array-creation.rst": [],
    "routines.io.rst": [],
    "isoCtests.f90": [],
    "test_isoc.py": [
        {
            "commit": "fab384365ef6fcf6574d2f7a3c9d8f9bcad57785",
            "timestamp": "2023-08-26T20:09:21+00:00",
            "author": "Rohit Goswami",
            "commit_message": "TST: Add more tests for iso_c_binding\n\nFunctions and subroutines",
            "additions": 19,
            "deletions": 0,
            "change_type": "ADD",
            "diff": "@@ -0,0 +1,19 @@\n+from . import util\n+import numpy as np\n+\n+class TestISOC(util.F2PyTest):\n+    sources = [\n+        util.getpath(\"tests\", \"src\", \"isocintrin\", \"isoCtests.f90\"),\n+    ]\n+\n+    # gh-24553\n+    def test_c_double(self):\n+        out = self.module.coddity.c_add(1, 2)\n+        exp_out = 3\n+        assert  out == exp_out\n+\n+    # gh-9693\n+    def test_bindc_function(self):\n+        out = self.module.coddity.wat(1, 20)\n+        exp_out = 8\n+        assert  out == exp_out\n",
            "comment_added_diff": {
                "9": "    # gh-24553",
                "15": "    # gh-9693"
            },
            "comment_deleted_diff": {},
            "comment_modified_diff": {}
        }
    ],
    "_function_base_impl.py": [
        {
            "commit": "2f0bd6e86a77e4401d0384d9a75edf9470c5deb6",
            "timestamp": "2023-09-05T21:58:22+02:00",
            "author": "Mateusz Sok\u00f3\u0142",
            "commit_message": "API: Cleaning `numpy/__init__.py` and main namespace - Part 4 [NEP 52] (#24445)\n\n[skip ci]",
            "additions": 11,
            "deletions": 0,
            "change_type": "MODIFY",
            "diff": "@@ -4783,6 +4783,9 @@ def trapz(y, x=None, dx=1.0, axis=-1):\n     r\"\"\"\n     Integrate along the given axis using the composite trapezoidal rule.\n \n+    .. deprecated:: 2.0\n+        Use `scipy.integrate.trapezoid` instead.\n+\n     If `x` is provided, the integration happens in sequence along its\n     elements - they are not sorted.\n \n@@ -4880,6 +4883,14 @@ def trapz(y, x=None, dx=1.0, axis=-1):\n     >>> np.trapz(a, axis=1)\n     array([2.,  8.])\n     \"\"\"\n+\n+    # Deprecated in NumPy 2.0, 2023-08-18\n+    warnings.warn(\n+        \"`trapz` is deprecated. Use `scipy.integrate.trapezoid` instead.\",\n+        DeprecationWarning,\n+        stacklevel=2\n+    )\n+\n     y = asanyarray(y)\n     if x is None:\n         d = dx\n",
            "comment_added_diff": {
                "4887": "    # Deprecated in NumPy 2.0, 2023-08-18"
            },
            "comment_deleted_diff": {},
            "comment_modified_diff": {}
        }
    ],
    "_function_base_impl.pyi": [],
    "_type_check_impl.py": [],
    "_type_check_impl.pyi": [],
    "scimath.py": [],
    "_twodim_base_impl.py": [],
    "_twodim_base_impl.pyi": [],
    "routines.set.rst": [],
    "_arraypad_impl.py": [],
    "_arraypad_impl.pyi": [],
    "_arraysetops_impl.py": [
        {
            "commit": "2f0bd6e86a77e4401d0384d9a75edf9470c5deb6",
            "timestamp": "2023-09-05T21:58:22+02:00",
            "author": "Mateusz Sok\u00f3\u0142",
            "commit_message": "API: Cleaning `numpy/__init__.py` and main namespace - Part 4 [NEP 52] (#24445)\n\n[skip ci]",
            "additions": 19,
            "deletions": 9,
            "change_type": "MODIFY",
            "diff": "@@ -15,6 +15,7 @@\n \n \"\"\"\n import functools\n+import warnings\n \n import numpy as np\n from numpy.core import overrides\n@@ -518,11 +519,12 @@ def in1d(ar1, ar2, assume_unique=False, invert=False, *, kind=None):\n     \"\"\"\n     Test whether each element of a 1-D array is also present in a second array.\n \n+    .. deprecated:: 2.0\n+        Use :func:`isin` instead of `in1d` for new code.\n+\n     Returns a boolean array the same length as `ar1` that is True\n     where an element of `ar1` is in `ar2` and False otherwise.\n \n-    We recommend using :func:`isin` instead of `in1d` for new code.\n-\n     Parameters\n     ----------\n     ar1 : (M,) array_like\n@@ -604,6 +606,18 @@ def in1d(ar1, ar2, assume_unique=False, invert=False, *, kind=None):\n     >>> test[mask]\n     array([1, 5])\n     \"\"\"\n+\n+    # Deprecated in NumPy 2.0, 2023-08-18\n+    warnings.warn(\n+        \"`in1d` is deprecated. Use `np.isin` instead.\",\n+        DeprecationWarning,\n+        stacklevel=2\n+    )\n+\n+    return _in1d(ar1, ar2, assume_unique, invert, kind=kind)\n+\n+\n+def _in1d(ar1, ar2, assume_unique=False, invert=False, *, kind=None):\n     # Ravel both arrays, behavior for the first array could be different\n     ar1 = np.asarray(ar1).ravel()\n     ar2 = np.asarray(ar2).ravel()\n@@ -805,10 +819,6 @@ def isin(element, test_elements, assume_unique=False, invert=False, *,\n         Has the same shape as `element`. The values `element[isin]`\n         are in `test_elements`.\n \n-    See Also\n-    --------\n-    in1d                  : Flattened version of this function.\n-\n     Notes\n     -----\n \n@@ -876,8 +886,8 @@ def isin(element, test_elements, assume_unique=False, invert=False, *,\n            [ True, False]])\n     \"\"\"\n     element = np.asarray(element)\n-    return in1d(element, test_elements, assume_unique=assume_unique,\n-                invert=invert, kind=kind).reshape(element.shape)\n+    return _in1d(element, test_elements, assume_unique=assume_unique,\n+                 invert=invert, kind=kind).reshape(element.shape)\n \n \n def _union1d_dispatcher(ar1, ar2):\n@@ -957,4 +967,4 @@ def setdiff1d(ar1, ar2, assume_unique=False):\n     else:\n         ar1 = unique(ar1)\n         ar2 = unique(ar2)\n-    return ar1[in1d(ar1, ar2, assume_unique=True, invert=True)]\n+    return ar1[_in1d(ar1, ar2, assume_unique=True, invert=True)]\n",
            "comment_added_diff": {
                "610": "    # Deprecated in NumPy 2.0, 2023-08-18"
            },
            "comment_deleted_diff": {},
            "comment_modified_diff": {}
        }
    ],
    "_arraysetops_impl.pyi": [],
    "_ufunclike_impl.py": [],
    "_ufunclike_impl.pyi": [],
    "_utils_impl.py": [
        {
            "commit": "52db499e7130239da0a66812b4970c4a92af3e35",
            "timestamp": "2023-09-06T10:20:42+02:00",
            "author": "Mateusz Sok\u00f3\u0142",
            "commit_message": "API: Add lib.array_utils namespace",
            "additions": 2,
            "deletions": 60,
            "change_type": "MODIFY",
            "diff": "@@ -7,12 +7,12 @@\n import functools\n import platform\n \n-from numpy.core import ndarray, asarray\n+from numpy.core import ndarray\n from numpy._utils import set_module\n import numpy as np\n \n __all__ = [\n-    'get_include', 'info', 'byte_bounds', 'show_runtime'\n+    'get_include', 'info', 'show_runtime'\n ]\n \n \n@@ -287,64 +287,6 @@ def deprecate_with_doc(msg):\n     return _Deprecate(message=msg)\n \n \n-#--------------------------------------------\n-# Determine if two arrays can share memory\n-#--------------------------------------------\n-\n-\n-@set_module('numpy')\n-def byte_bounds(a):\n-    \"\"\"\n-    Returns pointers to the end-points of an array.\n-\n-    Parameters\n-    ----------\n-    a : ndarray\n-        Input array. It must conform to the Python-side of the array\n-        interface.\n-\n-    Returns\n-    -------\n-    (low, high) : tuple of 2 integers\n-        The first integer is the first byte of the array, the second\n-        integer is just past the last byte of the array.  If `a` is not\n-        contiguous it will not use every byte between the (`low`, `high`)\n-        values.\n-\n-    Examples\n-    --------\n-    >>> I = np.eye(2, dtype='f'); I.dtype\n-    dtype('float32')\n-    >>> low, high = np.byte_bounds(I)\n-    >>> high - low == I.size*I.itemsize\n-    True\n-    >>> I = np.eye(2); I.dtype\n-    dtype('float64')\n-    >>> low, high = np.byte_bounds(I)\n-    >>> high - low == I.size*I.itemsize\n-    True\n-\n-    \"\"\"\n-    ai = a.__array_interface__\n-    a_data = ai['data'][0]\n-    astrides = ai['strides']\n-    ashape = ai['shape']\n-    bytes_a = asarray(a).dtype.itemsize\n-\n-    a_low = a_high = a_data\n-    if astrides is None:\n-        # contiguous case\n-        a_high += a.size * bytes_a\n-    else:\n-        for shape, stride in zip(ashape, astrides):\n-            if stride < 0:\n-                a_low += (shape-1)*stride\n-            else:\n-                a_high += (shape-1)*stride\n-        a_high += bytes_a\n-    return a_low, a_high\n-\n-\n #-----------------------------------------------------------------------------\n \n \n",
            "comment_added_diff": {},
            "comment_deleted_diff": {
                "290": "#--------------------------------------------",
                "291": "# Determine if two arrays can share memory",
                "292": "#--------------------------------------------",
                "336": "        # contiguous case"
            },
            "comment_modified_diff": {}
        }
    ],
    "_utils_impl.pyi": [],
    "24477.python_removal.rst": [],
    "_stride_tricks_impl.py": [
        {
            "commit": "3293164353d2b5168f83039a1158d54648a584ce",
            "timestamp": "2023-08-30T11:09:37+02:00",
            "author": "Mateusz Sok\u00f3\u0142",
            "commit_message": "API: Update lib.stride_tricks namespace",
            "additions": 546,
            "deletions": 0,
            "change_type": "ADD",
            "diff": "@@ -0,0 +1,546 @@\n+\"\"\"\n+Utilities that manipulate strides to achieve desirable effects.\n+\n+An explanation of strides can be found in the :ref:`arrays.ndarray`.\n+\n+\"\"\"\n+import numpy as np\n+from numpy.core.numeric import normalize_axis_tuple\n+from numpy.core.overrides import array_function_dispatch, set_module\n+\n+__all__ = ['broadcast_to', 'broadcast_arrays', 'broadcast_shapes']\n+\n+\n+class DummyArray:\n+    \"\"\"Dummy object that just exists to hang __array_interface__ dictionaries\n+    and possibly keep alive a reference to a base array.\n+    \"\"\"\n+\n+    def __init__(self, interface, base=None):\n+        self.__array_interface__ = interface\n+        self.base = base\n+\n+\n+def _maybe_view_as_subclass(original_array, new_array):\n+    if type(original_array) is not type(new_array):\n+        # if input was an ndarray subclass and subclasses were OK,\n+        # then view the result as that subclass.\n+        new_array = new_array.view(type=type(original_array))\n+        # Since we have done something akin to a view from original_array, we\n+        # should let the subclass finalize (if it has it implemented, i.e., is\n+        # not None).\n+        if new_array.__array_finalize__:\n+            new_array.__array_finalize__(original_array)\n+    return new_array\n+\n+\n+def as_strided(x, shape=None, strides=None, subok=False, writeable=True):\n+    \"\"\"\n+    Create a view into the array with the given shape and strides.\n+\n+    .. warning:: This function has to be used with extreme care, see notes.\n+\n+    Parameters\n+    ----------\n+    x : ndarray\n+        Array to create a new.\n+    shape : sequence of int, optional\n+        The shape of the new array. Defaults to ``x.shape``.\n+    strides : sequence of int, optional\n+        The strides of the new array. Defaults to ``x.strides``.\n+    subok : bool, optional\n+        .. versionadded:: 1.10\n+\n+        If True, subclasses are preserved.\n+    writeable : bool, optional\n+        .. versionadded:: 1.12\n+\n+        If set to False, the returned array will always be readonly.\n+        Otherwise it will be writable if the original array was. It\n+        is advisable to set this to False if possible (see Notes).\n+\n+    Returns\n+    -------\n+    view : ndarray\n+\n+    See also\n+    --------\n+    broadcast_to : broadcast an array to a given shape.\n+    reshape : reshape an array.\n+    lib.stride_tricks.sliding_window_view :\n+        userfriendly and safe function for a creation of sliding window views.\n+\n+    Notes\n+    -----\n+    ``as_strided`` creates a view into the array given the exact strides\n+    and shape. This means it manipulates the internal data structure of\n+    ndarray and, if done incorrectly, the array elements can point to\n+    invalid memory and can corrupt results or crash your program.\n+    It is advisable to always use the original ``x.strides`` when\n+    calculating new strides to avoid reliance on a contiguous memory\n+    layout.\n+\n+    Furthermore, arrays created with this function often contain self\n+    overlapping memory, so that two elements are identical.\n+    Vectorized write operations on such arrays will typically be\n+    unpredictable. They may even give different results for small, large,\n+    or transposed arrays.\n+\n+    Since writing to these arrays has to be tested and done with great\n+    care, you may want to use ``writeable=False`` to avoid accidental write\n+    operations.\n+\n+    For these reasons it is advisable to avoid ``as_strided`` when\n+    possible.\n+    \"\"\"\n+    # first convert input to array, possibly keeping subclass\n+    x = np.array(x, copy=False, subok=subok)\n+    interface = dict(x.__array_interface__)\n+    if shape is not None:\n+        interface['shape'] = tuple(shape)\n+    if strides is not None:\n+        interface['strides'] = tuple(strides)\n+\n+    array = np.asarray(DummyArray(interface, base=x))\n+    # The route via `__interface__` does not preserve structured\n+    # dtypes. Since dtype should remain unchanged, we set it explicitly.\n+    array.dtype = x.dtype\n+\n+    view = _maybe_view_as_subclass(x, array)\n+\n+    if view.flags.writeable and not writeable:\n+        view.flags.writeable = False\n+\n+    return view\n+\n+\n+def _sliding_window_view_dispatcher(x, window_shape, axis=None, *,\n+                                    subok=None, writeable=None):\n+    return (x,)\n+\n+\n+@array_function_dispatch(_sliding_window_view_dispatcher)\n+def sliding_window_view(x, window_shape, axis=None, *,\n+                        subok=False, writeable=False):\n+    \"\"\"\n+    Create a sliding window view into the array with the given window shape.\n+\n+    Also known as rolling or moving window, the window slides across all\n+    dimensions of the array and extracts subsets of the array at all window\n+    positions.\n+    \n+    .. versionadded:: 1.20.0\n+\n+    Parameters\n+    ----------\n+    x : array_like\n+        Array to create the sliding window view from.\n+    window_shape : int or tuple of int\n+        Size of window over each axis that takes part in the sliding window.\n+        If `axis` is not present, must have same length as the number of input\n+        array dimensions. Single integers `i` are treated as if they were the\n+        tuple `(i,)`.\n+    axis : int or tuple of int, optional\n+        Axis or axes along which the sliding window is applied.\n+        By default, the sliding window is applied to all axes and\n+        `window_shape[i]` will refer to axis `i` of `x`.\n+        If `axis` is given as a `tuple of int`, `window_shape[i]` will refer to\n+        the axis `axis[i]` of `x`.\n+        Single integers `i` are treated as if they were the tuple `(i,)`.\n+    subok : bool, optional\n+        If True, sub-classes will be passed-through, otherwise the returned\n+        array will be forced to be a base-class array (default).\n+    writeable : bool, optional\n+        When true, allow writing to the returned view. The default is false,\n+        as this should be used with caution: the returned view contains the\n+        same memory location multiple times, so writing to one location will\n+        cause others to change.\n+\n+    Returns\n+    -------\n+    view : ndarray\n+        Sliding window view of the array. The sliding window dimensions are\n+        inserted at the end, and the original dimensions are trimmed as\n+        required by the size of the sliding window.\n+        That is, ``view.shape = x_shape_trimmed + window_shape``, where\n+        ``x_shape_trimmed`` is ``x.shape`` with every entry reduced by one less\n+        than the corresponding window size.\n+\n+    See Also\n+    --------\n+    lib.stride_tricks.as_strided: A lower-level and less safe routine for\n+        creating arbitrary views from custom shape and strides.\n+    broadcast_to: broadcast an array to a given shape.\n+\n+    Notes\n+    -----\n+    For many applications using a sliding window view can be convenient, but\n+    potentially very slow. Often specialized solutions exist, for example:\n+\n+    - `scipy.signal.fftconvolve`\n+\n+    - filtering functions in `scipy.ndimage`\n+\n+    - moving window functions provided by\n+      `bottleneck <https://github.com/pydata/bottleneck>`_.\n+\n+    As a rough estimate, a sliding window approach with an input size of `N`\n+    and a window size of `W` will scale as `O(N*W)` where frequently a special\n+    algorithm can achieve `O(N)`. That means that the sliding window variant\n+    for a window size of 100 can be a 100 times slower than a more specialized\n+    version.\n+\n+    Nevertheless, for small window sizes, when no custom algorithm exists, or\n+    as a prototyping and developing tool, this function can be a good solution.\n+\n+    Examples\n+    --------\n+    >>> x = np.arange(6)\n+    >>> x.shape\n+    (6,)\n+    >>> v = sliding_window_view(x, 3)\n+    >>> v.shape\n+    (4, 3)\n+    >>> v\n+    array([[0, 1, 2],\n+           [1, 2, 3],\n+           [2, 3, 4],\n+           [3, 4, 5]])\n+\n+    This also works in more dimensions, e.g.\n+\n+    >>> i, j = np.ogrid[:3, :4]\n+    >>> x = 10*i + j\n+    >>> x.shape\n+    (3, 4)\n+    >>> x\n+    array([[ 0,  1,  2,  3],\n+           [10, 11, 12, 13],\n+           [20, 21, 22, 23]])\n+    >>> shape = (2,2)\n+    >>> v = sliding_window_view(x, shape)\n+    >>> v.shape\n+    (2, 3, 2, 2)\n+    >>> v\n+    array([[[[ 0,  1],\n+             [10, 11]],\n+            [[ 1,  2],\n+             [11, 12]],\n+            [[ 2,  3],\n+             [12, 13]]],\n+           [[[10, 11],\n+             [20, 21]],\n+            [[11, 12],\n+             [21, 22]],\n+            [[12, 13],\n+             [22, 23]]]])\n+\n+    The axis can be specified explicitly:\n+\n+    >>> v = sliding_window_view(x, 3, 0)\n+    >>> v.shape\n+    (1, 4, 3)\n+    >>> v\n+    array([[[ 0, 10, 20],\n+            [ 1, 11, 21],\n+            [ 2, 12, 22],\n+            [ 3, 13, 23]]])\n+\n+    The same axis can be used several times. In that case, every use reduces\n+    the corresponding original dimension:\n+\n+    >>> v = sliding_window_view(x, (2, 3), (1, 1))\n+    >>> v.shape\n+    (3, 1, 2, 3)\n+    >>> v\n+    array([[[[ 0,  1,  2],\n+             [ 1,  2,  3]]],\n+           [[[10, 11, 12],\n+             [11, 12, 13]]],\n+           [[[20, 21, 22],\n+             [21, 22, 23]]]])\n+\n+    Combining with stepped slicing (`::step`), this can be used to take sliding\n+    views which skip elements:\n+\n+    >>> x = np.arange(7)\n+    >>> sliding_window_view(x, 5)[:, ::2]\n+    array([[0, 2, 4],\n+           [1, 3, 5],\n+           [2, 4, 6]])\n+\n+    or views which move by multiple elements\n+\n+    >>> x = np.arange(7)\n+    >>> sliding_window_view(x, 3)[::2, :]\n+    array([[0, 1, 2],\n+           [2, 3, 4],\n+           [4, 5, 6]])\n+\n+    A common application of `sliding_window_view` is the calculation of running\n+    statistics. The simplest example is the\n+    `moving average <https://en.wikipedia.org/wiki/Moving_average>`_:\n+\n+    >>> x = np.arange(6)\n+    >>> x.shape\n+    (6,)\n+    >>> v = sliding_window_view(x, 3)\n+    >>> v.shape\n+    (4, 3)\n+    >>> v\n+    array([[0, 1, 2],\n+           [1, 2, 3],\n+           [2, 3, 4],\n+           [3, 4, 5]])\n+    >>> moving_average = v.mean(axis=-1)\n+    >>> moving_average\n+    array([1., 2., 3., 4.])\n+\n+    Note that a sliding window approach is often **not** optimal (see Notes).\n+    \"\"\"\n+    window_shape = (tuple(window_shape)\n+                    if np.iterable(window_shape)\n+                    else (window_shape,))\n+    # first convert input to array, possibly keeping subclass\n+    x = np.array(x, copy=False, subok=subok)\n+\n+    window_shape_array = np.array(window_shape)\n+    if np.any(window_shape_array < 0):\n+        raise ValueError('`window_shape` cannot contain negative values')\n+\n+    if axis is None:\n+        axis = tuple(range(x.ndim))\n+        if len(window_shape) != len(axis):\n+            raise ValueError(f'Since axis is `None`, must provide '\n+                             f'window_shape for all dimensions of `x`; '\n+                             f'got {len(window_shape)} window_shape elements '\n+                             f'and `x.ndim` is {x.ndim}.')\n+    else:\n+        axis = normalize_axis_tuple(axis, x.ndim, allow_duplicate=True)\n+        if len(window_shape) != len(axis):\n+            raise ValueError(f'Must provide matching length window_shape and '\n+                             f'axis; got {len(window_shape)} window_shape '\n+                             f'elements and {len(axis)} axes elements.')\n+\n+    out_strides = x.strides + tuple(x.strides[ax] for ax in axis)\n+\n+    # note: same axis can be windowed repeatedly\n+    x_shape_trimmed = list(x.shape)\n+    for ax, dim in zip(axis, window_shape):\n+        if x_shape_trimmed[ax] < dim:\n+            raise ValueError(\n+                'window shape cannot be larger than input array shape')\n+        x_shape_trimmed[ax] -= dim - 1\n+    out_shape = tuple(x_shape_trimmed) + window_shape\n+    return as_strided(x, strides=out_strides, shape=out_shape,\n+                      subok=subok, writeable=writeable)\n+\n+\n+def _broadcast_to(array, shape, subok, readonly):\n+    shape = tuple(shape) if np.iterable(shape) else (shape,)\n+    array = np.array(array, copy=False, subok=subok)\n+    if not shape and array.shape:\n+        raise ValueError('cannot broadcast a non-scalar to a scalar array')\n+    if any(size < 0 for size in shape):\n+        raise ValueError('all elements of broadcast shape must be non-'\n+                         'negative')\n+    extras = []\n+    it = np.nditer(\n+        (array,), flags=['multi_index', 'refs_ok', 'zerosize_ok'] + extras,\n+        op_flags=['readonly'], itershape=shape, order='C')\n+    with it:\n+        # never really has writebackifcopy semantics\n+        broadcast = it.itviews[0]\n+    result = _maybe_view_as_subclass(array, broadcast)\n+    # In a future version this will go away\n+    if not readonly and array.flags._writeable_no_warn:\n+        result.flags.writeable = True\n+        result.flags._warn_on_write = True\n+    return result\n+\n+\n+def _broadcast_to_dispatcher(array, shape, subok=None):\n+    return (array,)\n+\n+\n+@array_function_dispatch(_broadcast_to_dispatcher, module='numpy')\n+def broadcast_to(array, shape, subok=False):\n+    \"\"\"Broadcast an array to a new shape.\n+\n+    Parameters\n+    ----------\n+    array : array_like\n+        The array to broadcast.\n+    shape : tuple or int\n+        The shape of the desired array. A single integer ``i`` is interpreted\n+        as ``(i,)``.\n+    subok : bool, optional\n+        If True, then sub-classes will be passed-through, otherwise\n+        the returned array will be forced to be a base-class array (default).\n+\n+    Returns\n+    -------\n+    broadcast : array\n+        A readonly view on the original array with the given shape. It is\n+        typically not contiguous. Furthermore, more than one element of a\n+        broadcasted array may refer to a single memory location.\n+\n+    Raises\n+    ------\n+    ValueError\n+        If the array is not compatible with the new shape according to NumPy's\n+        broadcasting rules.\n+\n+    See Also\n+    --------\n+    broadcast\n+    broadcast_arrays\n+    broadcast_shapes\n+\n+    Notes\n+    -----\n+    .. versionadded:: 1.10.0\n+\n+    Examples\n+    --------\n+    >>> x = np.array([1, 2, 3])\n+    >>> np.broadcast_to(x, (3, 3))\n+    array([[1, 2, 3],\n+           [1, 2, 3],\n+           [1, 2, 3]])\n+    \"\"\"\n+    return _broadcast_to(array, shape, subok=subok, readonly=True)\n+\n+\n+def _broadcast_shape(*args):\n+    \"\"\"Returns the shape of the arrays that would result from broadcasting the\n+    supplied arrays against each other.\n+    \"\"\"\n+    # use the old-iterator because np.nditer does not handle size 0 arrays\n+    # consistently\n+    b = np.broadcast(*args[:32])\n+    # unfortunately, it cannot handle 32 or more arguments directly\n+    for pos in range(32, len(args), 31):\n+        # ironically, np.broadcast does not properly handle np.broadcast\n+        # objects (it treats them as scalars)\n+        # use broadcasting to avoid allocating the full array\n+        b = broadcast_to(0, b.shape)\n+        b = np.broadcast(b, *args[pos:(pos + 31)])\n+    return b.shape\n+\n+\n+@set_module('numpy')\n+def broadcast_shapes(*args):\n+    \"\"\"\n+    Broadcast the input shapes into a single shape.\n+\n+    :ref:`Learn more about broadcasting here <basics.broadcasting>`.\n+\n+    .. versionadded:: 1.20.0\n+\n+    Parameters\n+    ----------\n+    *args : tuples of ints, or ints\n+        The shapes to be broadcast against each other.\n+\n+    Returns\n+    -------\n+    tuple\n+        Broadcasted shape.\n+\n+    Raises\n+    ------\n+    ValueError\n+        If the shapes are not compatible and cannot be broadcast according\n+        to NumPy's broadcasting rules.\n+\n+    See Also\n+    --------\n+    broadcast\n+    broadcast_arrays\n+    broadcast_to\n+\n+    Examples\n+    --------\n+    >>> np.broadcast_shapes((1, 2), (3, 1), (3, 2))\n+    (3, 2)\n+\n+    >>> np.broadcast_shapes((6, 7), (5, 6, 1), (7,), (5, 1, 7))\n+    (5, 6, 7)\n+    \"\"\"\n+    arrays = [np.empty(x, dtype=[]) for x in args]\n+    return _broadcast_shape(*arrays)\n+\n+\n+def _broadcast_arrays_dispatcher(*args, subok=None):\n+    return args\n+\n+\n+@array_function_dispatch(_broadcast_arrays_dispatcher, module='numpy')\n+def broadcast_arrays(*args, subok=False):\n+    \"\"\"\n+    Broadcast any number of arrays against each other.\n+\n+    Parameters\n+    ----------\n+    *args : array_likes\n+        The arrays to broadcast.\n+\n+    subok : bool, optional\n+        If True, then sub-classes will be passed-through, otherwise\n+        the returned arrays will be forced to be a base-class array (default).\n+\n+    Returns\n+    -------\n+    broadcasted : list of arrays\n+        These arrays are views on the original arrays.  They are typically\n+        not contiguous.  Furthermore, more than one element of a\n+        broadcasted array may refer to a single memory location. If you need\n+        to write to the arrays, make copies first. While you can set the\n+        ``writable`` flag True, writing to a single output value may end up\n+        changing more than one location in the output array.\n+\n+        .. deprecated:: 1.17\n+            The output is currently marked so that if written to, a deprecation\n+            warning will be emitted. A future version will set the\n+            ``writable`` flag False so writing to it will raise an error.\n+\n+    See Also\n+    --------\n+    broadcast\n+    broadcast_to\n+    broadcast_shapes\n+\n+    Examples\n+    --------\n+    >>> x = np.array([[1,2,3]])\n+    >>> y = np.array([[4],[5]])\n+    >>> np.broadcast_arrays(x, y)\n+    [array([[1, 2, 3],\n+           [1, 2, 3]]), array([[4, 4, 4],\n+           [5, 5, 5]])]\n+\n+    Here is a useful idiom for getting contiguous copies instead of\n+    non-contiguous views.\n+\n+    >>> [np.array(a) for a in np.broadcast_arrays(x, y)]\n+    [array([[1, 2, 3],\n+           [1, 2, 3]]), array([[4, 4, 4],\n+           [5, 5, 5]])]\n+\n+    \"\"\"\n+    # nditer is not used here to avoid the limit of 32 arrays.\n+    # Otherwise, something like the following one-liner would suffice:\n+    # return np.nditer(args, flags=['multi_index', 'zerosize_ok'],\n+    #                  order='C').itviews\n+\n+    args = [np.array(_m, copy=False, subok=subok) for _m in args]\n+\n+    shape = _broadcast_shape(*args)\n+\n+    if all(array.shape == shape for array in args):\n+        # Common case where nothing needs to be broadcasted.\n+        return args\n+\n+    return [_broadcast_to(array, shape, subok=subok, readonly=False)\n+            for array in args]\n",
            "comment_added_diff": {
                "26": "        # if input was an ndarray subclass and subclasses were OK,",
                "27": "        # then view the result as that subclass.",
                "29": "        # Since we have done something akin to a view from original_array, we",
                "30": "        # should let the subclass finalize (if it has it implemented, i.e., is",
                "31": "        # not None).",
                "96": "    # first convert input to array, possibly keeping subclass",
                "105": "    # The route via `__interface__` does not preserve structured",
                "106": "    # dtypes. Since dtype should remain unchanged, we set it explicitly.",
                "304": "    # first convert input to array, possibly keeping subclass",
                "327": "    # note: same axis can be windowed repeatedly",
                "352": "        # never really has writebackifcopy semantics",
                "355": "    # In a future version this will go away",
                "419": "    # use the old-iterator because np.nditer does not handle size 0 arrays",
                "420": "    # consistently",
                "422": "    # unfortunately, it cannot handle 32 or more arguments directly",
                "424": "        # ironically, np.broadcast does not properly handle np.broadcast",
                "425": "        # objects (it treats them as scalars)",
                "426": "        # use broadcasting to avoid allocating the full array",
                "532": "    # nditer is not used here to avoid the limit of 32 arrays.",
                "533": "    # Otherwise, something like the following one-liner would suffice:",
                "534": "    # return np.nditer(args, flags=['multi_index', 'zerosize_ok'],",
                "535": "    #                  order='C').itviews",
                "542": "        # Common case where nothing needs to be broadcasted."
            },
            "comment_deleted_diff": {},
            "comment_modified_diff": {}
        }
    ],
    "_stride_tricks_impl.pyi": [],
    "stride_tricks.pyi": [],
    "test_stride_tricks.py": [],
    "_shape_base_impl.py": [
        {
            "commit": "e03d8ff35985bb3ce1bdb159793303c6b7b297fb",
            "timestamp": "2023-08-30T20:11:08+02:00",
            "author": "Mateusz Sok\u00f3\u0142",
            "commit_message": "API: Update lib.shape_base namespace",
            "additions": 13,
            "deletions": 2,
            "change_type": "RENAME",
            "diff": "@@ -16,7 +16,7 @@\n __all__ = [\n     'column_stack', 'row_stack', 'dstack', 'array_split', 'split',\n     'hsplit', 'vsplit', 'dsplit', 'apply_over_axes', 'expand_dims',\n-    'apply_along_axis', 'kron', 'tile', 'get_array_wrap', 'take_along_axis',\n+    'apply_along_axis', 'kron', 'tile', 'take_along_axis',\n     'put_along_axis'\n     ]\n \n@@ -1039,8 +1039,19 @@ def dsplit(ary, indices_or_sections):\n def get_array_prepare(*args):\n     \"\"\"Find the wrapper for the array with the highest priority.\n \n-    In case of ties, leftmost wins. If no wrapper is found, return None\n+    In case of ties, leftmost wins. If no wrapper is found, return None.\n+\n+    .. deprecated:: 2.0\n     \"\"\"\n+\n+    # Deprecated in NumPy 2.0, 2023-08-28\n+    warnings.warn(\n+        \"`get_array_prepare` is deprecated. \"\n+        \"(deprecated in NumPy 2.0)\",\n+        DeprecationWarning,\n+        stacklevel=2\n+    )\n+\n     wrappers = sorted((getattr(x, '__array_priority__', 0), -i,\n                  x.__array_prepare__) for i, x in enumerate(args)\n                                    if hasattr(x, '__array_prepare__'))\n",
            "comment_added_diff": {
                "1047": "    # Deprecated in NumPy 2.0, 2023-08-28"
            },
            "comment_deleted_diff": {},
            "comment_modified_diff": {}
        },
        {
            "commit": "4708dca39e8c678025bce8c8f1daf39d9fef7ae2",
            "timestamp": "2023-08-30T20:15:32+02:00",
            "author": "Mateusz Sok\u00f3\u0142",
            "commit_message": "MAINT: Remove get_array_prepare",
            "additions": 0,
            "deletions": 24,
            "change_type": "MODIFY",
            "diff": "@@ -1036,30 +1036,6 @@ def dsplit(ary, indices_or_sections):\n     return split(ary, indices_or_sections, 2)\n \n \n-def get_array_prepare(*args):\n-    \"\"\"Find the wrapper for the array with the highest priority.\n-\n-    In case of ties, leftmost wins. If no wrapper is found, return None.\n-\n-    .. deprecated:: 2.0\n-    \"\"\"\n-\n-    # Deprecated in NumPy 2.0, 2023-08-28\n-    warnings.warn(\n-        \"`get_array_prepare` is deprecated. \"\n-        \"(deprecated in NumPy 2.0)\",\n-        DeprecationWarning,\n-        stacklevel=2\n-    )\n-\n-    wrappers = sorted((getattr(x, '__array_priority__', 0), -i,\n-                 x.__array_prepare__) for i, x in enumerate(args)\n-                                   if hasattr(x, '__array_prepare__'))\n-    if wrappers:\n-        return wrappers[-1][-1]\n-    return None\n-\n-\n def get_array_wrap(*args):\n     \"\"\"Find the wrapper for the array with the highest priority.\n \n",
            "comment_added_diff": {},
            "comment_deleted_diff": {
                "1047": "    # Deprecated in NumPy 2.0, 2023-08-28"
            },
            "comment_modified_diff": {}
        },
        {
            "commit": "2f0bd6e86a77e4401d0384d9a75edf9470c5deb6",
            "timestamp": "2023-09-05T21:58:22+02:00",
            "author": "Mateusz Sok\u00f3\u0142",
            "commit_message": "API: Cleaning `numpy/__init__.py` and main namespace - Part 4 [NEP 52] (#24445)\n\n[skip ci]",
            "additions": 13,
            "deletions": 1,
            "change_type": "MODIFY",
            "diff": "@@ -604,7 +604,19 @@ def expand_dims(a, axis):\n     return a.reshape(shape)\n \n \n-row_stack = vstack\n+# TODO: Remove once deprecation period passes\n+def row_stack(tup, *, dtype=None, casting=\"same_kind\"):\n+    # Deprecated in NumPy 2.0, 2023-08-18\n+    warnings.warn(\n+        \"`row_stack` alias is deprecated. \"\n+        \"Use `np.vstack` directly.\",\n+        DeprecationWarning,\n+        stacklevel=2\n+    )\n+    return vstack(tup, dtype=dtype, casting=casting)\n+\n+\n+row_stack.__doc__ = vstack.__doc__\n \n \n def _column_stack_dispatcher(tup):\n",
            "comment_added_diff": {
                "607": "# TODO: Remove once deprecation period passes",
                "609": "    # Deprecated in NumPy 2.0, 2023-08-18"
            },
            "comment_deleted_diff": {},
            "comment_modified_diff": {
                "607": "row_stack = vstack"
            }
        },
        {
            "commit": "5312b6ed486a503e1917065197d5c6401e501400",
            "timestamp": "2023-09-17T20:02:44+02:00",
            "author": "Mateusz Sok\u00f3\u0142",
            "commit_message": "Apply review comments",
            "additions": 3,
            "deletions": 1,
            "change_type": "MODIFY",
            "diff": "@@ -8,6 +8,7 @@\n from numpy.core import overrides\n from numpy.core import vstack, atleast_3d\n from numpy.core.numeric import normalize_axis_tuple\n+from numpy.core.overrides import set_module\n from numpy.core.shape_base import _arrays_for_stack_dispatcher\n from numpy.lib._index_tricks_impl import ndindex\n from numpy.matrixlib.defmatrix import matrix  # this raises all the right alarm bells\n@@ -604,7 +605,8 @@ def expand_dims(a, axis):\n     return a.reshape(shape)\n \n \n-# TODO: Remove once deprecation period passes\n+# NOTE: Remove once deprecation period passes\n+@set_module(\"numpy\")\n def row_stack(tup, *, dtype=None, casting=\"same_kind\"):\n     # Deprecated in NumPy 2.0, 2023-08-18\n     warnings.warn(\n",
            "comment_added_diff": {
                "608": "# NOTE: Remove once deprecation period passes"
            },
            "comment_deleted_diff": {
                "607": "# TODO: Remove once deprecation period passes"
            },
            "comment_modified_diff": {}
        }
    ],
    "_shape_base_impl.pyi": [],
    "macos.yml": [],
    "24053.new_feature.rst": [],
    "_npyio_impl.py": [
        {
            "commit": "63d9da80788d75a5c2abb8e79c34a6f6eb02ef7e",
            "timestamp": "2023-09-01T10:53:35+02:00",
            "author": "Mateusz Sok\u00f3\u0142",
            "commit_message": "API: Update `lib.polynomial` and `lib.npyio` namespaces (#24578)",
            "additions": 2555,
            "deletions": 0,
            "change_type": "ADD",
            "diff": "@@ -0,0 +1,2555 @@\n+import os\n+import re\n+import functools\n+import itertools\n+import warnings\n+import weakref\n+import contextlib\n+import operator\n+from operator import itemgetter, index as opindex, methodcaller\n+from collections.abc import Mapping\n+import pickle\n+\n+import numpy as np\n+from . import format\n+from ._datasource import DataSource\n+from numpy.core import overrides\n+from numpy.core.multiarray import packbits, unpackbits\n+from numpy.core._multiarray_umath import _load_from_filelike\n+from numpy.core.overrides import set_array_function_like_doc, set_module\n+from ._iotools import (\n+    LineSplitter, NameValidator, StringConverter, ConverterError,\n+    ConverterLockError, ConversionWarning, _is_string_like,\n+    has_nested_fields, flatten_dtype, easy_dtype, _decode_line\n+    )\n+from numpy._utils import asunicode, asbytes\n+\n+\n+__all__ = [\n+    'savetxt', 'loadtxt', 'genfromtxt', 'load', 'save', 'savez',\n+    'savez_compressed', 'packbits', 'unpackbits', 'fromregex'\n+    ]\n+\n+\n+array_function_dispatch = functools.partial(\n+    overrides.array_function_dispatch, module='numpy')\n+\n+\n+class BagObj:\n+    \"\"\"\n+    BagObj(obj)\n+\n+    Convert attribute look-ups to getitems on the object passed in.\n+\n+    Parameters\n+    ----------\n+    obj : class instance\n+        Object on which attribute look-up is performed.\n+\n+    Examples\n+    --------\n+    >>> from numpy.lib._npyio_impl import BagObj as BO\n+    >>> class BagDemo:\n+    ...     def __getitem__(self, key): # An instance of BagObj(BagDemo)\n+    ...                                 # will call this method when any\n+    ...                                 # attribute look-up is required\n+    ...         result = \"Doesn't matter what you want, \"\n+    ...         return result + \"you're gonna get this\"\n+    ...\n+    >>> demo_obj = BagDemo()\n+    >>> bagobj = BO(demo_obj)\n+    >>> bagobj.hello_there\n+    \"Doesn't matter what you want, you're gonna get this\"\n+    >>> bagobj.I_can_be_anything\n+    \"Doesn't matter what you want, you're gonna get this\"\n+\n+    \"\"\"\n+\n+    def __init__(self, obj):\n+        # Use weakref to make NpzFile objects collectable by refcount\n+        self._obj = weakref.proxy(obj)\n+\n+    def __getattribute__(self, key):\n+        try:\n+            return object.__getattribute__(self, '_obj')[key]\n+        except KeyError:\n+            raise AttributeError(key) from None\n+\n+    def __dir__(self):\n+        \"\"\"\n+        Enables dir(bagobj) to list the files in an NpzFile.\n+\n+        This also enables tab-completion in an interpreter or IPython.\n+        \"\"\"\n+        return list(object.__getattribute__(self, '_obj').keys())\n+\n+\n+def zipfile_factory(file, *args, **kwargs):\n+    \"\"\"\n+    Create a ZipFile.\n+\n+    Allows for Zip64, and the `file` argument can accept file, str, or\n+    pathlib.Path objects. `args` and `kwargs` are passed to the zipfile.ZipFile\n+    constructor.\n+    \"\"\"\n+    if not hasattr(file, 'read'):\n+        file = os.fspath(file)\n+    import zipfile\n+    kwargs['allowZip64'] = True\n+    return zipfile.ZipFile(file, *args, **kwargs)\n+\n+\n+@set_module('numpy.lib.npyio')\n+class NpzFile(Mapping):\n+    \"\"\"\n+    NpzFile(fid)\n+\n+    A dictionary-like object with lazy-loading of files in the zipped\n+    archive provided on construction.\n+\n+    `NpzFile` is used to load files in the NumPy ``.npz`` data archive\n+    format. It assumes that files in the archive have a ``.npy`` extension,\n+    other files are ignored.\n+\n+    The arrays and file strings are lazily loaded on either\n+    getitem access using ``obj['key']`` or attribute lookup using\n+    ``obj.f.key``. A list of all files (without ``.npy`` extensions) can\n+    be obtained with ``obj.files`` and the ZipFile object itself using\n+    ``obj.zip``.\n+\n+    Attributes\n+    ----------\n+    files : list of str\n+        List of all files in the archive with a ``.npy`` extension.\n+    zip : ZipFile instance\n+        The ZipFile object initialized with the zipped archive.\n+    f : BagObj instance\n+        An object on which attribute can be performed as an alternative\n+        to getitem access on the `NpzFile` instance itself.\n+    allow_pickle : bool, optional\n+        Allow loading pickled data. Default: False\n+\n+        .. versionchanged:: 1.16.3\n+            Made default False in response to CVE-2019-6446.\n+\n+    pickle_kwargs : dict, optional\n+        Additional keyword arguments to pass on to pickle.load.\n+        These are only useful when loading object arrays saved on\n+        Python 2 when using Python 3.\n+    max_header_size : int, optional\n+        Maximum allowed size of the header.  Large headers may not be safe\n+        to load securely and thus require explicitly passing a larger value.\n+        See :py:func:`ast.literal_eval()` for details.\n+        This option is ignored when `allow_pickle` is passed.  In that case\n+        the file is by definition trusted and the limit is unnecessary.\n+\n+    Parameters\n+    ----------\n+    fid : file, str, or pathlib.Path\n+        The zipped archive to open. This is either a file-like object\n+        or a string containing the path to the archive.\n+    own_fid : bool, optional\n+        Whether NpzFile should close the file handle.\n+        Requires that `fid` is a file-like object.\n+\n+    Examples\n+    --------\n+    >>> from tempfile import TemporaryFile\n+    >>> outfile = TemporaryFile()\n+    >>> x = np.arange(10)\n+    >>> y = np.sin(x)\n+    >>> np.savez(outfile, x=x, y=y)\n+    >>> _ = outfile.seek(0)\n+\n+    >>> npz = np.load(outfile)\n+    >>> isinstance(npz, np.lib.npyio.NpzFile)\n+    True\n+    >>> npz\n+    NpzFile 'object' with keys x, y\n+    >>> sorted(npz.files)\n+    ['x', 'y']\n+    >>> npz['x']  # getitem access\n+    array([0, 1, 2, 3, 4, 5, 6, 7, 8, 9])\n+    >>> npz.f.x  # attribute lookup\n+    array([0, 1, 2, 3, 4, 5, 6, 7, 8, 9])\n+\n+    \"\"\"\n+    # Make __exit__ safe if zipfile_factory raises an exception\n+    zip = None\n+    fid = None\n+    _MAX_REPR_ARRAY_COUNT = 5\n+\n+    def __init__(self, fid, own_fid=False, allow_pickle=False,\n+                 pickle_kwargs=None, *,\n+                 max_header_size=format._MAX_HEADER_SIZE):\n+        # Import is postponed to here since zipfile depends on gzip, an\n+        # optional component of the so-called standard library.\n+        _zip = zipfile_factory(fid)\n+        self._files = _zip.namelist()\n+        self.files = []\n+        self.allow_pickle = allow_pickle\n+        self.max_header_size = max_header_size\n+        self.pickle_kwargs = pickle_kwargs\n+        for x in self._files:\n+            if x.endswith('.npy'):\n+                self.files.append(x[:-4])\n+            else:\n+                self.files.append(x)\n+        self.zip = _zip\n+        self.f = BagObj(self)\n+        if own_fid:\n+            self.fid = fid\n+\n+    def __enter__(self):\n+        return self\n+\n+    def __exit__(self, exc_type, exc_value, traceback):\n+        self.close()\n+\n+    def close(self):\n+        \"\"\"\n+        Close the file.\n+\n+        \"\"\"\n+        if self.zip is not None:\n+            self.zip.close()\n+            self.zip = None\n+        if self.fid is not None:\n+            self.fid.close()\n+            self.fid = None\n+        self.f = None  # break reference cycle\n+\n+    def __del__(self):\n+        self.close()\n+\n+    # Implement the Mapping ABC\n+    def __iter__(self):\n+        return iter(self.files)\n+\n+    def __len__(self):\n+        return len(self.files)\n+\n+    def __getitem__(self, key):\n+        # FIXME: This seems like it will copy strings around\n+        #   more than is strictly necessary.  The zipfile\n+        #   will read the string and then\n+        #   the format.read_array will copy the string\n+        #   to another place in memory.\n+        #   It would be better if the zipfile could read\n+        #   (or at least uncompress) the data\n+        #   directly into the array memory.\n+        member = False\n+        if key in self._files:\n+            member = True\n+        elif key in self.files:\n+            member = True\n+            key += '.npy'\n+        if member:\n+            bytes = self.zip.open(key)\n+            magic = bytes.read(len(format.MAGIC_PREFIX))\n+            bytes.close()\n+            if magic == format.MAGIC_PREFIX:\n+                bytes = self.zip.open(key)\n+                return format.read_array(bytes,\n+                                         allow_pickle=self.allow_pickle,\n+                                         pickle_kwargs=self.pickle_kwargs,\n+                                         max_header_size=self.max_header_size)\n+            else:\n+                return self.zip.read(key)\n+        else:\n+            raise KeyError(f\"{key} is not a file in the archive\")\n+\n+    def __contains__(self, key):\n+        return (key in self._files or key in self.files)\n+\n+    def __repr__(self):\n+        # Get filename or default to `object`\n+        if isinstance(self.fid, str):\n+            filename = self.fid\n+        else:\n+            filename = getattr(self.fid, \"name\", \"object\")\n+\n+        # Get the name of arrays\n+        array_names = ', '.join(self.files[:self._MAX_REPR_ARRAY_COUNT])\n+        if len(self.files) > self._MAX_REPR_ARRAY_COUNT:\n+            array_names += \"...\"\n+        return f\"NpzFile {filename!r} with keys: {array_names}\"\n+\n+\n+@set_module('numpy')\n+def load(file, mmap_mode=None, allow_pickle=False, fix_imports=True,\n+         encoding='ASCII', *, max_header_size=format._MAX_HEADER_SIZE):\n+    \"\"\"\n+    Load arrays or pickled objects from ``.npy``, ``.npz`` or pickled files.\n+\n+    .. warning:: Loading files that contain object arrays uses the ``pickle``\n+                 module, which is not secure against erroneous or maliciously\n+                 constructed data. Consider passing ``allow_pickle=False`` to\n+                 load data that is known not to contain object arrays for the\n+                 safer handling of untrusted sources.\n+\n+    Parameters\n+    ----------\n+    file : file-like object, string, or pathlib.Path\n+        The file to read. File-like objects must support the\n+        ``seek()`` and ``read()`` methods and must always\n+        be opened in binary mode.  Pickled files require that the\n+        file-like object support the ``readline()`` method as well.\n+    mmap_mode : {None, 'r+', 'r', 'w+', 'c'}, optional\n+        If not None, then memory-map the file, using the given mode (see\n+        `numpy.memmap` for a detailed description of the modes).  A\n+        memory-mapped array is kept on disk. However, it can be accessed\n+        and sliced like any ndarray.  Memory mapping is especially useful\n+        for accessing small fragments of large files without reading the\n+        entire file into memory.\n+    allow_pickle : bool, optional\n+        Allow loading pickled object arrays stored in npy files. Reasons for\n+        disallowing pickles include security, as loading pickled data can\n+        execute arbitrary code. If pickles are disallowed, loading object\n+        arrays will fail. Default: False\n+\n+        .. versionchanged:: 1.16.3\n+            Made default False in response to CVE-2019-6446.\n+\n+    fix_imports : bool, optional\n+        Only useful when loading Python 2 generated pickled files on Python 3,\n+        which includes npy/npz files containing object arrays. If `fix_imports`\n+        is True, pickle will try to map the old Python 2 names to the new names\n+        used in Python 3.\n+    encoding : str, optional\n+        What encoding to use when reading Python 2 strings. Only useful when\n+        loading Python 2 generated pickled files in Python 3, which includes\n+        npy/npz files containing object arrays. Values other than 'latin1',\n+        'ASCII', and 'bytes' are not allowed, as they can corrupt numerical\n+        data. Default: 'ASCII'\n+    max_header_size : int, optional\n+        Maximum allowed size of the header.  Large headers may not be safe\n+        to load securely and thus require explicitly passing a larger value.\n+        See :py:func:`ast.literal_eval()` for details.\n+        This option is ignored when `allow_pickle` is passed.  In that case\n+        the file is by definition trusted and the limit is unnecessary.\n+\n+    Returns\n+    -------\n+    result : array, tuple, dict, etc.\n+        Data stored in the file. For ``.npz`` files, the returned instance\n+        of NpzFile class must be closed to avoid leaking file descriptors.\n+\n+    Raises\n+    ------\n+    OSError\n+        If the input file does not exist or cannot be read.\n+    UnpicklingError\n+        If ``allow_pickle=True``, but the file cannot be loaded as a pickle.\n+    ValueError\n+        The file contains an object array, but ``allow_pickle=False`` given.\n+    EOFError\n+        When calling ``np.load`` multiple times on the same file handle,\n+        if all data has already been read\n+\n+    See Also\n+    --------\n+    save, savez, savez_compressed, loadtxt\n+    memmap : Create a memory-map to an array stored in a file on disk.\n+    lib.format.open_memmap : Create or load a memory-mapped ``.npy`` file.\n+\n+    Notes\n+    -----\n+    - If the file contains pickle data, then whatever object is stored\n+      in the pickle is returned.\n+    - If the file is a ``.npy`` file, then a single array is returned.\n+    - If the file is a ``.npz`` file, then a dictionary-like object is\n+      returned, containing ``{filename: array}`` key-value pairs, one for\n+      each file in the archive.\n+    - If the file is a ``.npz`` file, the returned value supports the\n+      context manager protocol in a similar fashion to the open function::\n+\n+        with load('foo.npz') as data:\n+            a = data['a']\n+\n+      The underlying file descriptor is closed when exiting the 'with'\n+      block.\n+\n+    Examples\n+    --------\n+    Store data to disk, and load it again:\n+\n+    >>> np.save('/tmp/123', np.array([[1, 2, 3], [4, 5, 6]]))\n+    >>> np.load('/tmp/123.npy')\n+    array([[1, 2, 3],\n+           [4, 5, 6]])\n+\n+    Store compressed data to disk, and load it again:\n+\n+    >>> a=np.array([[1, 2, 3], [4, 5, 6]])\n+    >>> b=np.array([1, 2])\n+    >>> np.savez('/tmp/123.npz', a=a, b=b)\n+    >>> data = np.load('/tmp/123.npz')\n+    >>> data['a']\n+    array([[1, 2, 3],\n+           [4, 5, 6]])\n+    >>> data['b']\n+    array([1, 2])\n+    >>> data.close()\n+\n+    Mem-map the stored array, and then access the second row\n+    directly from disk:\n+\n+    >>> X = np.load('/tmp/123.npy', mmap_mode='r')\n+    >>> X[1, :]\n+    memmap([4, 5, 6])\n+\n+    \"\"\"\n+    if encoding not in ('ASCII', 'latin1', 'bytes'):\n+        # The 'encoding' value for pickle also affects what encoding\n+        # the serialized binary data of NumPy arrays is loaded\n+        # in. Pickle does not pass on the encoding information to\n+        # NumPy. The unpickling code in numpy.core.multiarray is\n+        # written to assume that unicode data appearing where binary\n+        # should be is in 'latin1'. 'bytes' is also safe, as is 'ASCII'.\n+        #\n+        # Other encoding values can corrupt binary data, and we\n+        # purposefully disallow them. For the same reason, the errors=\n+        # argument is not exposed, as values other than 'strict'\n+        # result can similarly silently corrupt numerical data.\n+        raise ValueError(\"encoding must be 'ASCII', 'latin1', or 'bytes'\")\n+\n+    pickle_kwargs = dict(encoding=encoding, fix_imports=fix_imports)\n+\n+    with contextlib.ExitStack() as stack:\n+        if hasattr(file, 'read'):\n+            fid = file\n+            own_fid = False\n+        else:\n+            fid = stack.enter_context(open(os.fspath(file), \"rb\"))\n+            own_fid = True\n+\n+        # Code to distinguish from NumPy binary files and pickles.\n+        _ZIP_PREFIX = b'PK\\x03\\x04'\n+        _ZIP_SUFFIX = b'PK\\x05\\x06'  # empty zip files start with this\n+        N = len(format.MAGIC_PREFIX)\n+        magic = fid.read(N)\n+        if not magic:\n+            raise EOFError(\"No data left in file\")\n+        # If the file size is less than N, we need to make sure not\n+        # to seek past the beginning of the file\n+        fid.seek(-min(N, len(magic)), 1)  # back-up\n+        if magic.startswith(_ZIP_PREFIX) or magic.startswith(_ZIP_SUFFIX):\n+            # zip-file (assume .npz)\n+            # Potentially transfer file ownership to NpzFile\n+            stack.pop_all()\n+            ret = NpzFile(fid, own_fid=own_fid, allow_pickle=allow_pickle,\n+                          pickle_kwargs=pickle_kwargs,\n+                          max_header_size=max_header_size)\n+            return ret\n+        elif magic == format.MAGIC_PREFIX:\n+            # .npy file\n+            if mmap_mode:\n+                if allow_pickle:\n+                    max_header_size = 2**64\n+                return format.open_memmap(file, mode=mmap_mode,\n+                                          max_header_size=max_header_size)\n+            else:\n+                return format.read_array(fid, allow_pickle=allow_pickle,\n+                                         pickle_kwargs=pickle_kwargs,\n+                                         max_header_size=max_header_size)\n+        else:\n+            # Try a pickle\n+            if not allow_pickle:\n+                raise ValueError(\"Cannot load file containing pickled data \"\n+                                 \"when allow_pickle=False\")\n+            try:\n+                return pickle.load(fid, **pickle_kwargs)\n+            except Exception as e:\n+                raise pickle.UnpicklingError(\n+                    f\"Failed to interpret file {file!r} as a pickle\") from e\n+\n+\n+def _save_dispatcher(file, arr, allow_pickle=None, fix_imports=None):\n+    return (arr,)\n+\n+\n+@array_function_dispatch(_save_dispatcher)\n+def save(file, arr, allow_pickle=True, fix_imports=True):\n+    \"\"\"\n+    Save an array to a binary file in NumPy ``.npy`` format.\n+\n+    Parameters\n+    ----------\n+    file : file, str, or pathlib.Path\n+        File or filename to which the data is saved. If file is a file-object,\n+        then the filename is unchanged.  If file is a string or Path,\n+        a ``.npy`` extension will be appended to the filename if it does not\n+        already have one.\n+    arr : array_like\n+        Array data to be saved.\n+    allow_pickle : bool, optional\n+        Allow saving object arrays using Python pickles. Reasons for \n+        disallowing pickles include security (loading pickled data can execute\n+        arbitrary code) and portability (pickled objects may not be loadable \n+        on different Python installations, for example if the stored objects\n+        require libraries that are not available, and not all pickled data is\n+        compatible between Python 2 and Python 3).\n+        Default: True\n+    fix_imports : bool, optional\n+        Only useful in forcing objects in object arrays on Python 3 to be\n+        pickled in a Python 2 compatible way. If `fix_imports` is True, pickle\n+        will try to map the new Python 3 names to the old module names used in\n+        Python 2, so that the pickle data stream is readable with Python 2.\n+\n+    See Also\n+    --------\n+    savez : Save several arrays into a ``.npz`` archive\n+    savetxt, load\n+\n+    Notes\n+    -----\n+    For a description of the ``.npy`` format, see :py:mod:`numpy.lib.format`.\n+\n+    Any data saved to the file is appended to the end of the file.\n+\n+    Examples\n+    --------\n+    >>> from tempfile import TemporaryFile\n+    >>> outfile = TemporaryFile()\n+\n+    >>> x = np.arange(10)\n+    >>> np.save(outfile, x)\n+\n+    >>> _ = outfile.seek(0) # Only needed to simulate closing & reopening file\n+    >>> np.load(outfile)\n+    array([0, 1, 2, 3, 4, 5, 6, 7, 8, 9])\n+\n+\n+    >>> with open('test.npy', 'wb') as f:\n+    ...     np.save(f, np.array([1, 2]))\n+    ...     np.save(f, np.array([1, 3]))\n+    >>> with open('test.npy', 'rb') as f:\n+    ...     a = np.load(f)\n+    ...     b = np.load(f)\n+    >>> print(a, b)\n+    # [1 2] [1 3]\n+    \"\"\"\n+    if hasattr(file, 'write'):\n+        file_ctx = contextlib.nullcontext(file)\n+    else:\n+        file = os.fspath(file)\n+        if not file.endswith('.npy'):\n+            file = file + '.npy'\n+        file_ctx = open(file, \"wb\")\n+\n+    with file_ctx as fid:\n+        arr = np.asanyarray(arr)\n+        format.write_array(fid, arr, allow_pickle=allow_pickle,\n+                           pickle_kwargs=dict(fix_imports=fix_imports))\n+\n+\n+def _savez_dispatcher(file, *args, **kwds):\n+    yield from args\n+    yield from kwds.values()\n+\n+\n+@array_function_dispatch(_savez_dispatcher)\n+def savez(file, *args, **kwds):\n+    \"\"\"Save several arrays into a single file in uncompressed ``.npz`` format.\n+\n+    Provide arrays as keyword arguments to store them under the\n+    corresponding name in the output file: ``savez(fn, x=x, y=y)``.\n+\n+    If arrays are specified as positional arguments, i.e., ``savez(fn,\n+    x, y)``, their names will be `arr_0`, `arr_1`, etc.\n+\n+    Parameters\n+    ----------\n+    file : file, str, or pathlib.Path\n+        Either the filename (string) or an open file (file-like object)\n+        where the data will be saved. If file is a string or a Path, the\n+        ``.npz`` extension will be appended to the filename if it is not\n+        already there.\n+    args : Arguments, optional\n+        Arrays to save to the file. Please use keyword arguments (see\n+        `kwds` below) to assign names to arrays.  Arrays specified as\n+        args will be named \"arr_0\", \"arr_1\", and so on.\n+    kwds : Keyword arguments, optional\n+        Arrays to save to the file. Each array will be saved to the\n+        output file with its corresponding keyword name.\n+\n+    Returns\n+    -------\n+    None\n+\n+    See Also\n+    --------\n+    save : Save a single array to a binary file in NumPy format.\n+    savetxt : Save an array to a file as plain text.\n+    savez_compressed : Save several arrays into a compressed ``.npz`` archive\n+\n+    Notes\n+    -----\n+    The ``.npz`` file format is a zipped archive of files named after the\n+    variables they contain.  The archive is not compressed and each file\n+    in the archive contains one variable in ``.npy`` format. For a\n+    description of the ``.npy`` format, see :py:mod:`numpy.lib.format`.\n+\n+    When opening the saved ``.npz`` file with `load` a `NpzFile` object is\n+    returned. This is a dictionary-like object which can be queried for\n+    its list of arrays (with the ``.files`` attribute), and for the arrays\n+    themselves.\n+\n+    Keys passed in `kwds` are used as filenames inside the ZIP archive.\n+    Therefore, keys should be valid filenames; e.g., avoid keys that begin with\n+    ``/`` or contain ``.``.\n+\n+    When naming variables with keyword arguments, it is not possible to name a\n+    variable ``file``, as this would cause the ``file`` argument to be defined\n+    twice in the call to ``savez``.\n+\n+    Examples\n+    --------\n+    >>> from tempfile import TemporaryFile\n+    >>> outfile = TemporaryFile()\n+    >>> x = np.arange(10)\n+    >>> y = np.sin(x)\n+\n+    Using `savez` with \\\\*args, the arrays are saved with default names.\n+\n+    >>> np.savez(outfile, x, y)\n+    >>> _ = outfile.seek(0) # Only needed to simulate closing & reopening file\n+    >>> npzfile = np.load(outfile)\n+    >>> npzfile.files\n+    ['arr_0', 'arr_1']\n+    >>> npzfile['arr_0']\n+    array([0, 1, 2, 3, 4, 5, 6, 7, 8, 9])\n+\n+    Using `savez` with \\\\**kwds, the arrays are saved with the keyword names.\n+\n+    >>> outfile = TemporaryFile()\n+    >>> np.savez(outfile, x=x, y=y)\n+    >>> _ = outfile.seek(0)\n+    >>> npzfile = np.load(outfile)\n+    >>> sorted(npzfile.files)\n+    ['x', 'y']\n+    >>> npzfile['x']\n+    array([0, 1, 2, 3, 4, 5, 6, 7, 8, 9])\n+\n+    \"\"\"\n+    _savez(file, args, kwds, False)\n+\n+\n+def _savez_compressed_dispatcher(file, *args, **kwds):\n+    yield from args\n+    yield from kwds.values()\n+\n+\n+@array_function_dispatch(_savez_compressed_dispatcher)\n+def savez_compressed(file, *args, **kwds):\n+    \"\"\"\n+    Save several arrays into a single file in compressed ``.npz`` format.\n+\n+    Provide arrays as keyword arguments to store them under the\n+    corresponding name in the output file: ``savez_compressed(fn, x=x, y=y)``.\n+\n+    If arrays are specified as positional arguments, i.e.,\n+    ``savez_compressed(fn, x, y)``, their names will be `arr_0`, `arr_1`, etc.\n+\n+    Parameters\n+    ----------\n+    file : file, str, or pathlib.Path\n+        Either the filename (string) or an open file (file-like object)\n+        where the data will be saved. If file is a string or a Path, the\n+        ``.npz`` extension will be appended to the filename if it is not\n+        already there.\n+    args : Arguments, optional\n+        Arrays to save to the file. Please use keyword arguments (see\n+        `kwds` below) to assign names to arrays.  Arrays specified as\n+        args will be named \"arr_0\", \"arr_1\", and so on.\n+    kwds : Keyword arguments, optional\n+        Arrays to save to the file. Each array will be saved to the\n+        output file with its corresponding keyword name.\n+\n+    Returns\n+    -------\n+    None\n+\n+    See Also\n+    --------\n+    numpy.save : Save a single array to a binary file in NumPy format.\n+    numpy.savetxt : Save an array to a file as plain text.\n+    numpy.savez : Save several arrays into an uncompressed ``.npz`` file format\n+    numpy.load : Load the files created by savez_compressed.\n+\n+    Notes\n+    -----\n+    The ``.npz`` file format is a zipped archive of files named after the\n+    variables they contain.  The archive is compressed with\n+    ``zipfile.ZIP_DEFLATED`` and each file in the archive contains one variable\n+    in ``.npy`` format. For a description of the ``.npy`` format, see\n+    :py:mod:`numpy.lib.format`.\n+\n+\n+    When opening the saved ``.npz`` file with `load` a `NpzFile` object is\n+    returned. This is a dictionary-like object which can be queried for\n+    its list of arrays (with the ``.files`` attribute), and for the arrays\n+    themselves.\n+\n+    Examples\n+    --------\n+    >>> test_array = np.random.rand(3, 2)\n+    >>> test_vector = np.random.rand(4)\n+    >>> np.savez_compressed('/tmp/123', a=test_array, b=test_vector)\n+    >>> loaded = np.load('/tmp/123.npz')\n+    >>> print(np.array_equal(test_array, loaded['a']))\n+    True\n+    >>> print(np.array_equal(test_vector, loaded['b']))\n+    True\n+\n+    \"\"\"\n+    _savez(file, args, kwds, True)\n+\n+\n+def _savez(file, args, kwds, compress, allow_pickle=True, pickle_kwargs=None):\n+    # Import is postponed to here since zipfile depends on gzip, an optional\n+    # component of the so-called standard library.\n+    import zipfile\n+\n+    if not hasattr(file, 'write'):\n+        file = os.fspath(file)\n+        if not file.endswith('.npz'):\n+            file = file + '.npz'\n+\n+    namedict = kwds\n+    for i, val in enumerate(args):\n+        key = 'arr_%d' % i\n+        if key in namedict.keys():\n+            raise ValueError(\n+                \"Cannot use un-named variables and keyword %s\" % key)\n+        namedict[key] = val\n+\n+    if compress:\n+        compression = zipfile.ZIP_DEFLATED\n+    else:\n+        compression = zipfile.ZIP_STORED\n+\n+    zipf = zipfile_factory(file, mode=\"w\", compression=compression)\n+\n+    for key, val in namedict.items():\n+        fname = key + '.npy'\n+        val = np.asanyarray(val)\n+        # always force zip64, gh-10776\n+        with zipf.open(fname, 'w', force_zip64=True) as fid:\n+            format.write_array(fid, val,\n+                               allow_pickle=allow_pickle,\n+                               pickle_kwargs=pickle_kwargs)\n+\n+    zipf.close()\n+\n+\n+def _ensure_ndmin_ndarray_check_param(ndmin):\n+    \"\"\"Just checks if the param ndmin is supported on\n+        _ensure_ndmin_ndarray. It is intended to be used as\n+        verification before running anything expensive.\n+        e.g. loadtxt, genfromtxt\n+    \"\"\"\n+    # Check correctness of the values of `ndmin`\n+    if ndmin not in [0, 1, 2]:\n+        raise ValueError(f\"Illegal value of ndmin keyword: {ndmin}\")\n+\n+def _ensure_ndmin_ndarray(a, *, ndmin: int):\n+    \"\"\"This is a helper function of loadtxt and genfromtxt to ensure\n+        proper minimum dimension as requested\n+\n+        ndim : int. Supported values 1, 2, 3\n+                    ^^ whenever this changes, keep in sync with\n+                       _ensure_ndmin_ndarray_check_param\n+    \"\"\"\n+    # Verify that the array has at least dimensions `ndmin`.\n+    # Tweak the size and shape of the arrays - remove extraneous dimensions\n+    if a.ndim > ndmin:\n+        a = np.squeeze(a)\n+    # and ensure we have the minimum number of dimensions asked for\n+    # - has to be in this order for the odd case ndmin=1, a.squeeze().ndim=0\n+    if a.ndim < ndmin:\n+        if ndmin == 1:\n+            a = np.atleast_1d(a)\n+        elif ndmin == 2:\n+            a = np.atleast_2d(a).T\n+\n+    return a\n+\n+\n+# amount of lines loadtxt reads in one chunk, can be overridden for testing\n+_loadtxt_chunksize = 50000\n+\n+\n+def _check_nonneg_int(value, name=\"argument\"):\n+    try:\n+        operator.index(value)\n+    except TypeError:\n+        raise TypeError(f\"{name} must be an integer\") from None\n+    if value < 0:\n+        raise ValueError(f\"{name} must be nonnegative\")\n+\n+\n+def _preprocess_comments(iterable, comments, encoding):\n+    \"\"\"\n+    Generator that consumes a line iterated iterable and strips out the\n+    multiple (or multi-character) comments from lines.\n+    This is a pre-processing step to achieve feature parity with loadtxt\n+    (we assume that this feature is a nieche feature).\n+    \"\"\"\n+    for line in iterable:\n+        if isinstance(line, bytes):\n+            # Need to handle conversion here, or the splitting would fail\n+            line = line.decode(encoding)\n+\n+        for c in comments:\n+            line = line.split(c, 1)[0]\n+\n+        yield line\n+\n+\n+# The number of rows we read in one go if confronted with a parametric dtype\n+_loadtxt_chunksize = 50000\n+\n+\n+def _read(fname, *, delimiter=',', comment='#', quote='\"',\n+          imaginary_unit='j', usecols=None, skiplines=0,\n+          max_rows=None, converters=None, ndmin=None, unpack=False,\n+          dtype=np.float64, encoding=\"bytes\"):\n+    r\"\"\"\n+    Read a NumPy array from a text file.\n+    This is a helper function for loadtxt.\n+\n+    Parameters\n+    ----------\n+    fname : file, str, or pathlib.Path\n+        The filename or the file to be read.\n+    delimiter : str, optional\n+        Field delimiter of the fields in line of the file.\n+        Default is a comma, ','.  If None any sequence of whitespace is\n+        considered a delimiter.\n+    comment : str or sequence of str or None, optional\n+        Character that begins a comment.  All text from the comment\n+        character to the end of the line is ignored.\n+        Multiple comments or multiple-character comment strings are supported,\n+        but may be slower and `quote` must be empty if used.\n+        Use None to disable all use of comments.\n+    quote : str or None, optional\n+        Character that is used to quote string fields. Default is '\"'\n+        (a double quote). Use None to disable quote support.\n+    imaginary_unit : str, optional\n+        Character that represent the imaginay unit `sqrt(-1)`.\n+        Default is 'j'.\n+    usecols : array_like, optional\n+        A one-dimensional array of integer column numbers.  These are the\n+        columns from the file to be included in the array.  If this value\n+        is not given, all the columns are used.\n+    skiplines : int, optional\n+        Number of lines to skip before interpreting the data in the file.\n+    max_rows : int, optional\n+        Maximum number of rows of data to read.  Default is to read the\n+        entire file.\n+    converters : dict or callable, optional\n+        A function to parse all columns strings into the desired value, or\n+        a dictionary mapping column number to a parser function.\n+        E.g. if column 0 is a date string: ``converters = {0: datestr2num}``.\n+        Converters can also be used to provide a default value for missing\n+        data, e.g. ``converters = lambda s: float(s.strip() or 0)`` will\n+        convert empty fields to 0.\n+        Default: None\n+    ndmin : int, optional\n+        Minimum dimension of the array returned.\n+        Allowed values are 0, 1 or 2.  Default is 0.\n+    unpack : bool, optional\n+        If True, the returned array is transposed, so that arguments may be\n+        unpacked using ``x, y, z = read(...)``.  When used with a structured\n+        data-type, arrays are returned for each field.  Default is False.\n+    dtype : numpy data type\n+        A NumPy dtype instance, can be a structured dtype to map to the\n+        columns of the file.\n+    encoding : str, optional\n+        Encoding used to decode the inputfile. The special value 'bytes'\n+        (the default) enables backwards-compatible behavior for `converters`,\n+        ensuring that inputs to the converter functions are encoded\n+        bytes objects. The special value 'bytes' has no additional effect if\n+        ``converters=None``. If encoding is ``'bytes'`` or ``None``, the\n+        default system encoding is used.\n+\n+    Returns\n+    -------\n+    ndarray\n+        NumPy array.\n+    \"\"\"\n+    # Handle special 'bytes' keyword for encoding\n+    byte_converters = False\n+    if encoding == 'bytes':\n+        encoding = None\n+        byte_converters = True\n+\n+    if dtype is None:\n+        raise TypeError(\"a dtype must be provided.\")\n+    dtype = np.dtype(dtype)\n+\n+    read_dtype_via_object_chunks = None\n+    if dtype.kind in 'SUM' and (\n+            dtype == \"S0\" or dtype == \"U0\" or dtype == \"M8\" or dtype == 'm8'):\n+        # This is a legacy \"flexible\" dtype.  We do not truly support\n+        # parametric dtypes currently (no dtype discovery step in the core),\n+        # but have to support these for backward compatibility.\n+        read_dtype_via_object_chunks = dtype\n+        dtype = np.dtype(object)\n+\n+    if usecols is not None:\n+        # Allow usecols to be a single int or a sequence of ints, the C-code\n+        # handles the rest\n+        try:\n+            usecols = list(usecols)\n+        except TypeError:\n+            usecols = [usecols]\n+\n+    _ensure_ndmin_ndarray_check_param(ndmin)\n+\n+    if comment is None:\n+        comments = None\n+    else:\n+        # assume comments are a sequence of strings\n+        if \"\" in comment:\n+            raise ValueError(\n+                \"comments cannot be an empty string. Use comments=None to \"\n+                \"disable comments.\"\n+            )\n+        comments = tuple(comment)\n+        comment = None\n+        if len(comments) == 0:\n+            comments = None  # No comments at all\n+        elif len(comments) == 1:\n+            # If there is only one comment, and that comment has one character,\n+            # the normal parsing can deal with it just fine.\n+            if isinstance(comments[0], str) and len(comments[0]) == 1:\n+                comment = comments[0]\n+                comments = None\n+        else:\n+            # Input validation if there are multiple comment characters\n+            if delimiter in comments:\n+                raise TypeError(\n+                    f\"Comment characters '{comments}' cannot include the \"\n+                    f\"delimiter '{delimiter}'\"\n+                )\n+\n+    # comment is now either a 1 or 0 character string or a tuple:\n+    if comments is not None:\n+        # Note: An earlier version support two character comments (and could\n+        #       have been extended to multiple characters, we assume this is\n+        #       rare enough to not optimize for.\n+        if quote is not None:\n+            raise ValueError(\n+                \"when multiple comments or a multi-character comment is \"\n+                \"given, quotes are not supported.  In this case quotechar \"\n+                \"must be set to None.\")\n+\n+    if len(imaginary_unit) != 1:\n+        raise ValueError('len(imaginary_unit) must be 1.')\n+\n+    _check_nonneg_int(skiplines)\n+    if max_rows is not None:\n+        _check_nonneg_int(max_rows)\n+    else:\n+        # Passing -1 to the C code means \"read the entire file\".\n+        max_rows = -1\n+\n+    fh_closing_ctx = contextlib.nullcontext()\n+    filelike = False\n+    try:\n+        if isinstance(fname, os.PathLike):\n+            fname = os.fspath(fname)\n+        if isinstance(fname, str):\n+            fh = np.lib._datasource.open(fname, 'rt', encoding=encoding)\n+            if encoding is None:\n+                encoding = getattr(fh, 'encoding', 'latin1')\n+\n+            fh_closing_ctx = contextlib.closing(fh)\n+            data = fh\n+            filelike = True\n+        else:\n+            if encoding is None:\n+                encoding = getattr(fname, 'encoding', 'latin1')\n+            data = iter(fname)\n+    except TypeError as e:\n+        raise ValueError(\n+            f\"fname must be a string, filehandle, list of strings,\\n\"\n+            f\"or generator. Got {type(fname)} instead.\") from e\n+\n+    with fh_closing_ctx:\n+        if comments is not None:\n+            if filelike:\n+                data = iter(data)\n+                filelike = False\n+            data = _preprocess_comments(data, comments, encoding)\n+\n+        if read_dtype_via_object_chunks is None:\n+            arr = _load_from_filelike(\n+                data, delimiter=delimiter, comment=comment, quote=quote,\n+                imaginary_unit=imaginary_unit,\n+                usecols=usecols, skiplines=skiplines, max_rows=max_rows,\n+                converters=converters, dtype=dtype,\n+                encoding=encoding, filelike=filelike,\n+                byte_converters=byte_converters)\n+\n+        else:\n+            # This branch reads the file into chunks of object arrays and then\n+            # casts them to the desired actual dtype.  This ensures correct\n+            # string-length and datetime-unit discovery (like `arr.astype()`).\n+            # Due to chunking, certain error reports are less clear, currently.\n+            if filelike:\n+                data = iter(data)  # cannot chunk when reading from file\n+\n+            c_byte_converters = False\n+            if read_dtype_via_object_chunks == \"S\":\n+                c_byte_converters = True  # Use latin1 rather than ascii\n+\n+            chunks = []\n+            while max_rows != 0:\n+                if max_rows < 0:\n+                    chunk_size = _loadtxt_chunksize\n+                else:\n+                    chunk_size = min(_loadtxt_chunksize, max_rows)\n+\n+                next_arr = _load_from_filelike(\n+                    data, delimiter=delimiter, comment=comment, quote=quote,\n+                    imaginary_unit=imaginary_unit,\n+                    usecols=usecols, skiplines=skiplines, max_rows=max_rows,\n+                    converters=converters, dtype=dtype,\n+                    encoding=encoding, filelike=filelike,\n+                    byte_converters=byte_converters,\n+                    c_byte_converters=c_byte_converters)\n+                # Cast here already.  We hope that this is better even for\n+                # large files because the storage is more compact.  It could\n+                # be adapted (in principle the concatenate could cast).\n+                chunks.append(next_arr.astype(read_dtype_via_object_chunks))\n+\n+                skiprows = 0  # Only have to skip for first chunk\n+                if max_rows >= 0:\n+                    max_rows -= chunk_size\n+                if len(next_arr) < chunk_size:\n+                    # There was less data than requested, so we are done.\n+                    break\n+\n+            # Need at least one chunk, but if empty, the last one may have\n+            # the wrong shape.\n+            if len(chunks) > 1 and len(chunks[-1]) == 0:\n+                del chunks[-1]\n+            if len(chunks) == 1:\n+                arr = chunks[0]\n+            else:\n+                arr = np.concatenate(chunks, axis=0)\n+\n+    # NOTE: ndmin works as advertised for structured dtypes, but normally\n+    #       these would return a 1D result plus the structured dimension,\n+    #       so ndmin=2 adds a third dimension even when no squeezing occurs.\n+    #       A `squeeze=False` could be a better solution (pandas uses squeeze).\n+    arr = _ensure_ndmin_ndarray(arr, ndmin=ndmin)\n+\n+    if arr.shape:\n+        if arr.shape[0] == 0:\n+            warnings.warn(\n+                f'loadtxt: input contained no data: \"{fname}\"',\n+                category=UserWarning,\n+                stacklevel=3\n+            )\n+\n+    if unpack:\n+        # Unpack structured dtypes if requested:\n+        dt = arr.dtype\n+        if dt.names is not None:\n+            # For structured arrays, return an array for each field.\n+            return [arr[field] for field in dt.names]\n+        else:\n+            return arr.T\n+    else:\n+        return arr\n+\n+\n+@set_array_function_like_doc\n+@set_module('numpy')\n+def loadtxt(fname, dtype=float, comments='#', delimiter=None,\n+            converters=None, skiprows=0, usecols=None, unpack=False,\n+            ndmin=0, encoding='bytes', max_rows=None, *, quotechar=None,\n+            like=None):\n+    r\"\"\"\n+    Load data from a text file.\n+\n+    Parameters\n+    ----------\n+    fname : file, str, pathlib.Path, list of str, generator\n+        File, filename, list, or generator to read.  If the filename\n+        extension is ``.gz`` or ``.bz2``, the file is first decompressed. Note\n+        that generators must return bytes or strings. The strings\n+        in a list or produced by a generator are treated as lines.\n+    dtype : data-type, optional\n+        Data-type of the resulting array; default: float.  If this is a\n+        structured data-type, the resulting array will be 1-dimensional, and\n+        each row will be interpreted as an element of the array.  In this\n+        case, the number of columns used must match the number of fields in\n+        the data-type.\n+    comments : str or sequence of str or None, optional\n+        The characters or list of characters used to indicate the start of a\n+        comment. None implies no comments. For backwards compatibility, byte\n+        strings will be decoded as 'latin1'. The default is '#'.\n+    delimiter : str, optional\n+        The character used to separate the values. For backwards compatibility,\n+        byte strings will be decoded as 'latin1'. The default is whitespace.\n+\n+        .. versionchanged:: 1.23.0\n+           Only single character delimiters are supported. Newline characters\n+           cannot be used as the delimiter.\n+\n+    converters : dict or callable, optional\n+        Converter functions to customize value parsing. If `converters` is\n+        callable, the function is applied to all columns, else it must be a\n+        dict that maps column number to a parser function.\n+        See examples for further details.\n+        Default: None.\n+\n+        .. versionchanged:: 1.23.0\n+           The ability to pass a single callable to be applied to all columns\n+           was added.\n+\n+    skiprows : int, optional\n+        Skip the first `skiprows` lines, including comments; default: 0.\n+    usecols : int or sequence, optional\n+        Which columns to read, with 0 being the first. For example,\n+        ``usecols = (1,4,5)`` will extract the 2nd, 5th and 6th columns.\n+        The default, None, results in all columns being read.\n+\n+        .. versionchanged:: 1.11.0\n+            When a single column has to be read it is possible to use\n+            an integer instead of a tuple. E.g ``usecols = 3`` reads the\n+            fourth column the same way as ``usecols = (3,)`` would.\n+    unpack : bool, optional\n+        If True, the returned array is transposed, so that arguments may be\n+        unpacked using ``x, y, z = loadtxt(...)``.  When used with a\n+        structured data-type, arrays are returned for each field.\n+        Default is False.\n+    ndmin : int, optional\n+        The returned array will have at least `ndmin` dimensions.\n+        Otherwise mono-dimensional axes will be squeezed.\n+        Legal values: 0 (default), 1 or 2.\n+\n+        .. versionadded:: 1.6.0\n+    encoding : str, optional\n+        Encoding used to decode the inputfile. Does not apply to input streams.\n+        The special value 'bytes' enables backward compatibility workarounds\n+        that ensures you receive byte arrays as results if possible and passes\n+        'latin1' encoded strings to converters. Override this value to receive\n+        unicode arrays and pass strings as input to converters.  If set to None\n+        the system default is used. The default value is 'bytes'.\n+\n+        .. versionadded:: 1.14.0\n+    max_rows : int, optional\n+        Read `max_rows` rows of content after `skiprows` lines. The default is\n+        to read all the rows. Note that empty rows containing no data such as\n+        empty lines and comment lines are not counted towards `max_rows`,\n+        while such lines are counted in `skiprows`.\n+\n+        .. versionadded:: 1.16.0\n+\n+        .. versionchanged:: 1.23.0\n+            Lines containing no data, including comment lines (e.g., lines\n+            starting with '#' or as specified via `comments`) are not counted\n+            towards `max_rows`.\n+    quotechar : unicode character or None, optional\n+        The character used to denote the start and end of a quoted item.\n+        Occurrences of the delimiter or comment characters are ignored within\n+        a quoted item. The default value is ``quotechar=None``, which means\n+        quoting support is disabled.\n+\n+        If two consecutive instances of `quotechar` are found within a quoted\n+        field, the first is treated as an escape character. See examples.\n+\n+        .. versionadded:: 1.23.0\n+    ${ARRAY_FUNCTION_LIKE}\n+\n+        .. versionadded:: 1.20.0\n+\n+    Returns\n+    -------\n+    out : ndarray\n+        Data read from the text file.\n+\n+    See Also\n+    --------\n+    load, fromstring, fromregex\n+    genfromtxt : Load data with missing values handled as specified.\n+    scipy.io.loadmat : reads MATLAB data files\n+\n+    Notes\n+    -----\n+    This function aims to be a fast reader for simply formatted files.  The\n+    `genfromtxt` function provides more sophisticated handling of, e.g.,\n+    lines with missing values.\n+\n+    Each row in the input text file must have the same number of values to be\n+    able to read all values. If all rows do not have same number of values, a\n+    subset of up to n columns (where n is the least number of values present\n+    in all rows) can be read by specifying the columns via `usecols`.\n+\n+    .. versionadded:: 1.10.0\n+\n+    The strings produced by the Python float.hex method can be used as\n+    input for floats.\n+\n+    Examples\n+    --------\n+    >>> from io import StringIO   # StringIO behaves like a file object\n+    >>> c = StringIO(\"0 1\\n2 3\")\n+    >>> np.loadtxt(c)\n+    array([[0., 1.],\n+           [2., 3.]])\n+\n+    >>> d = StringIO(\"M 21 72\\nF 35 58\")\n+    >>> np.loadtxt(d, dtype={'names': ('gender', 'age', 'weight'),\n+    ...                      'formats': ('S1', 'i4', 'f4')})\n+    array([(b'M', 21, 72.), (b'F', 35, 58.)],\n+          dtype=[('gender', 'S1'), ('age', '<i4'), ('weight', '<f4')])\n+\n+    >>> c = StringIO(\"1,0,2\\n3,0,4\")\n+    >>> x, y = np.loadtxt(c, delimiter=',', usecols=(0, 2), unpack=True)\n+    >>> x\n+    array([1., 3.])\n+    >>> y\n+    array([2., 4.])\n+\n+    The `converters` argument is used to specify functions to preprocess the\n+    text prior to parsing. `converters` can be a dictionary that maps\n+    preprocessing functions to each column:\n+\n+    >>> s = StringIO(\"1.618, 2.296\\n3.141, 4.669\\n\")\n+    >>> conv = {\n+    ...     0: lambda x: np.floor(float(x)),  # conversion fn for column 0\n+    ...     1: lambda x: np.ceil(float(x)),  # conversion fn for column 1\n+    ... }\n+    >>> np.loadtxt(s, delimiter=\",\", converters=conv)\n+    array([[1., 3.],\n+           [3., 5.]])\n+\n+    `converters` can be a callable instead of a dictionary, in which case it\n+    is applied to all columns:\n+\n+    >>> s = StringIO(\"0xDE 0xAD\\n0xC0 0xDE\")\n+    >>> import functools\n+    >>> conv = functools.partial(int, base=16)\n+    >>> np.loadtxt(s, converters=conv)\n+    array([[222., 173.],\n+           [192., 222.]])\n+\n+    This example shows how `converters` can be used to convert a field\n+    with a trailing minus sign into a negative number.\n+\n+    >>> s = StringIO('10.01 31.25-\\n19.22 64.31\\n17.57- 63.94')\n+    >>> def conv(fld):\n+    ...     return -float(fld[:-1]) if fld.endswith(b'-') else float(fld)\n+    ...\n+    >>> np.loadtxt(s, converters=conv)\n+    array([[ 10.01, -31.25],\n+           [ 19.22,  64.31],\n+           [-17.57,  63.94]])\n+\n+    Using a callable as the converter can be particularly useful for handling\n+    values with different formatting, e.g. floats with underscores:\n+\n+    >>> s = StringIO(\"1 2.7 100_000\")\n+    >>> np.loadtxt(s, converters=float)\n+    array([1.e+00, 2.7e+00, 1.e+05])\n+\n+    This idea can be extended to automatically handle values specified in\n+    many different formats:\n+\n+    >>> def conv(val):\n+    ...     try:\n+    ...         return float(val)\n+    ...     except ValueError:\n+    ...         return float.fromhex(val)\n+    >>> s = StringIO(\"1, 2.5, 3_000, 0b4, 0x1.4000000000000p+2\")\n+    >>> np.loadtxt(s, delimiter=\",\", converters=conv, encoding=None)\n+    array([1.0e+00, 2.5e+00, 3.0e+03, 1.8e+02, 5.0e+00])\n+\n+    Note that with the default ``encoding=\"bytes\"``, the inputs to the\n+    converter function are latin-1 encoded byte strings. To deactivate the\n+    implicit encoding prior to conversion, use ``encoding=None``\n+\n+    >>> s = StringIO('10.01 31.25-\\n19.22 64.31\\n17.57- 63.94')\n+    >>> conv = lambda x: -float(x[:-1]) if x.endswith('-') else float(x)\n+    >>> np.loadtxt(s, converters=conv, encoding=None)\n+    array([[ 10.01, -31.25],\n+           [ 19.22,  64.31],\n+           [-17.57,  63.94]])\n+\n+    Support for quoted fields is enabled with the `quotechar` parameter.\n+    Comment and delimiter characters are ignored when they appear within a\n+    quoted item delineated by `quotechar`:\n+\n+    >>> s = StringIO('\"alpha, #42\", 10.0\\n\"beta, #64\", 2.0\\n')\n+    >>> dtype = np.dtype([(\"label\", \"U12\"), (\"value\", float)])\n+    >>> np.loadtxt(s, dtype=dtype, delimiter=\",\", quotechar='\"')\n+    array([('alpha, #42', 10.), ('beta, #64',  2.)],\n+          dtype=[('label', '<U12'), ('value', '<f8')])\n+\n+    Quoted fields can be separated by multiple whitespace characters:\n+\n+    >>> s = StringIO('\"alpha, #42\"       10.0\\n\"beta, #64\" 2.0\\n')\n+    >>> dtype = np.dtype([(\"label\", \"U12\"), (\"value\", float)])\n+    >>> np.loadtxt(s, dtype=dtype, delimiter=None, quotechar='\"')\n+    array([('alpha, #42', 10.), ('beta, #64',  2.)],\n+          dtype=[('label', '<U12'), ('value', '<f8')])\n+\n+    Two consecutive quote characters within a quoted field are treated as a\n+    single escaped character:\n+\n+    >>> s = StringIO('\"Hello, my name is \"\"Monty\"\"!\"')\n+    >>> np.loadtxt(s, dtype=\"U\", delimiter=\",\", quotechar='\"')\n+    array('Hello, my name is \"Monty\"!', dtype='<U26')\n+\n+    Read subset of columns when all rows do not contain equal number of values:\n+\n+    >>> d = StringIO(\"1 2\\n2 4\\n3 9 12\\n4 16 20\")\n+    >>> np.loadtxt(d, usecols=(0, 1))\n+    array([[ 1.,  2.],\n+           [ 2.,  4.],\n+           [ 3.,  9.],\n+           [ 4., 16.]])\n+\n+    \"\"\"\n+\n+    if like is not None:\n+        return _loadtxt_with_like(\n+            like, fname, dtype=dtype, comments=comments, delimiter=delimiter,\n+            converters=converters, skiprows=skiprows, usecols=usecols,\n+            unpack=unpack, ndmin=ndmin, encoding=encoding,\n+            max_rows=max_rows\n+        )\n+\n+    if isinstance(delimiter, bytes):\n+        delimiter.decode(\"latin1\")\n+\n+    if dtype is None:\n+        dtype = np.float64\n+\n+    comment = comments\n+    # Control character type conversions for Py3 convenience\n+    if comment is not None:\n+        if isinstance(comment, (str, bytes)):\n+            comment = [comment]\n+        comment = [\n+            x.decode('latin1') if isinstance(x, bytes) else x for x in comment]\n+    if isinstance(delimiter, bytes):\n+        delimiter = delimiter.decode('latin1')\n+\n+    arr = _read(fname, dtype=dtype, comment=comment, delimiter=delimiter,\n+                converters=converters, skiplines=skiprows, usecols=usecols,\n+                unpack=unpack, ndmin=ndmin, encoding=encoding,\n+                max_rows=max_rows, quote=quotechar)\n+\n+    return arr\n+\n+\n+_loadtxt_with_like = array_function_dispatch()(loadtxt)\n+\n+\n+def _savetxt_dispatcher(fname, X, fmt=None, delimiter=None, newline=None,\n+                        header=None, footer=None, comments=None,\n+                        encoding=None):\n+    return (X,)\n+\n+\n+@array_function_dispatch(_savetxt_dispatcher)\n+def savetxt(fname, X, fmt='%.18e', delimiter=' ', newline='\\n', header='',\n+            footer='', comments='# ', encoding=None):\n+    \"\"\"\n+    Save an array to a text file.\n+\n+    Parameters\n+    ----------\n+    fname : filename, file handle or pathlib.Path\n+        If the filename ends in ``.gz``, the file is automatically saved in\n+        compressed gzip format.  `loadtxt` understands gzipped files\n+        transparently.\n+    X : 1D or 2D array_like\n+        Data to be saved to a text file.\n+    fmt : str or sequence of strs, optional\n+        A single format (%10.5f), a sequence of formats, or a\n+        multi-format string, e.g. 'Iteration %d -- %10.5f', in which\n+        case `delimiter` is ignored. For complex `X`, the legal options\n+        for `fmt` are:\n+\n+        * a single specifier, `fmt='%.4e'`, resulting in numbers formatted\n+          like `' (%s+%sj)' % (fmt, fmt)`\n+        * a full string specifying every real and imaginary part, e.g.\n+          `' %.4e %+.4ej %.4e %+.4ej %.4e %+.4ej'` for 3 columns\n+        * a list of specifiers, one per column - in this case, the real\n+          and imaginary part must have separate specifiers,\n+          e.g. `['%.3e + %.3ej', '(%.15e%+.15ej)']` for 2 columns\n+    delimiter : str, optional\n+        String or character separating columns.\n+    newline : str, optional\n+        String or character separating lines.\n+\n+        .. versionadded:: 1.5.0\n+    header : str, optional\n+        String that will be written at the beginning of the file.\n+\n+        .. versionadded:: 1.7.0\n+    footer : str, optional\n+        String that will be written at the end of the file.\n+\n+        .. versionadded:: 1.7.0\n+    comments : str, optional\n+        String that will be prepended to the ``header`` and ``footer`` strings,\n+        to mark them as comments. Default: '# ',  as expected by e.g.\n+        ``numpy.loadtxt``.\n+\n+        .. versionadded:: 1.7.0\n+    encoding : {None, str}, optional\n+        Encoding used to encode the outputfile. Does not apply to output\n+        streams. If the encoding is something other than 'bytes' or 'latin1'\n+        you will not be able to load the file in NumPy versions < 1.14. Default\n+        is 'latin1'.\n+\n+        .. versionadded:: 1.14.0\n+\n+\n+    See Also\n+    --------\n+    save : Save an array to a binary file in NumPy ``.npy`` format\n+    savez : Save several arrays into an uncompressed ``.npz`` archive\n+    savez_compressed : Save several arrays into a compressed ``.npz`` archive\n+\n+    Notes\n+    -----\n+    Further explanation of the `fmt` parameter\n+    (``%[flag]width[.precision]specifier``):\n+\n+    flags:\n+        ``-`` : left justify\n+\n+        ``+`` : Forces to precede result with + or -.\n+\n+        ``0`` : Left pad the number with zeros instead of space (see width).\n+\n+    width:\n+        Minimum number of characters to be printed. The value is not truncated\n+        if it has more characters.\n+\n+    precision:\n+        - For integer specifiers (eg. ``d,i,o,x``), the minimum number of\n+          digits.\n+        - For ``e, E`` and ``f`` specifiers, the number of digits to print\n+          after the decimal point.\n+        - For ``g`` and ``G``, the maximum number of significant digits.\n+        - For ``s``, the maximum number of characters.\n+\n+    specifiers:\n+        ``c`` : character\n+\n+        ``d`` or ``i`` : signed decimal integer\n+\n+        ``e`` or ``E`` : scientific notation with ``e`` or ``E``.\n+\n+        ``f`` : decimal floating point\n+\n+        ``g,G`` : use the shorter of ``e,E`` or ``f``\n+\n+        ``o`` : signed octal\n+\n+        ``s`` : string of characters\n+\n+        ``u`` : unsigned decimal integer\n+\n+        ``x,X`` : unsigned hexadecimal integer\n+\n+    This explanation of ``fmt`` is not complete, for an exhaustive\n+    specification see [1]_.\n+\n+    References\n+    ----------\n+    .. [1] `Format Specification Mini-Language\n+           <https://docs.python.org/library/string.html#format-specification-mini-language>`_,\n+           Python Documentation.\n+\n+    Examples\n+    --------\n+    >>> x = y = z = np.arange(0.0,5.0,1.0)\n+    >>> np.savetxt('test.out', x, delimiter=',')   # X is an array\n+    >>> np.savetxt('test.out', (x,y,z))   # x,y,z equal sized 1D arrays\n+    >>> np.savetxt('test.out', x, fmt='%1.4e')   # use exponential notation\n+\n+    \"\"\"\n+\n+    class WriteWrap:\n+        \"\"\"Convert to bytes on bytestream inputs.\n+\n+        \"\"\"\n+        def __init__(self, fh, encoding):\n+            self.fh = fh\n+            self.encoding = encoding\n+            self.do_write = self.first_write\n+\n+        def close(self):\n+            self.fh.close()\n+\n+        def write(self, v):\n+            self.do_write(v)\n+\n+        def write_bytes(self, v):\n+            if isinstance(v, bytes):\n+                self.fh.write(v)\n+            else:\n+                self.fh.write(v.encode(self.encoding))\n+\n+        def write_normal(self, v):\n+            self.fh.write(asunicode(v))\n+\n+        def first_write(self, v):\n+            try:\n+                self.write_normal(v)\n+                self.write = self.write_normal\n+            except TypeError:\n+                # input is probably a bytestream\n+                self.write_bytes(v)\n+                self.write = self.write_bytes\n+\n+    own_fh = False\n+    if isinstance(fname, os.PathLike):\n+        fname = os.fspath(fname)\n+    if _is_string_like(fname):\n+        # datasource doesn't support creating a new file ...\n+        open(fname, 'wt').close()\n+        fh = np.lib._datasource.open(fname, 'wt', encoding=encoding)\n+        own_fh = True\n+    elif hasattr(fname, 'write'):\n+        # wrap to handle byte output streams\n+        fh = WriteWrap(fname, encoding or 'latin1')\n+    else:\n+        raise ValueError('fname must be a string or file handle')\n+\n+    try:\n+        X = np.asarray(X)\n+\n+        # Handle 1-dimensional arrays\n+        if X.ndim == 0 or X.ndim > 2:\n+            raise ValueError(\n+                \"Expected 1D or 2D array, got %dD array instead\" % X.ndim)\n+        elif X.ndim == 1:\n+            # Common case -- 1d array of numbers\n+            if X.dtype.names is None:\n+                X = np.atleast_2d(X).T\n+                ncol = 1\n+\n+            # Complex dtype -- each field indicates a separate column\n+            else:\n+                ncol = len(X.dtype.names)\n+        else:\n+            ncol = X.shape[1]\n+\n+        iscomplex_X = np.iscomplexobj(X)\n+        # `fmt` can be a string with multiple insertion points or a\n+        # list of formats.  E.g. '%10.5f\\t%10d' or ('%10.5f', '$10d')\n+        if type(fmt) in (list, tuple):\n+            if len(fmt) != ncol:\n+                raise AttributeError('fmt has wrong shape.  %s' % str(fmt))\n+            format = delimiter.join(fmt)\n+        elif isinstance(fmt, str):\n+            n_fmt_chars = fmt.count('%')\n+            error = ValueError('fmt has wrong number of %% formats:  %s' % fmt)\n+            if n_fmt_chars == 1:\n+                if iscomplex_X:\n+                    fmt = [' (%s+%sj)' % (fmt, fmt), ] * ncol\n+                else:\n+                    fmt = [fmt, ] * ncol\n+                format = delimiter.join(fmt)\n+            elif iscomplex_X and n_fmt_chars != (2 * ncol):\n+                raise error\n+            elif ((not iscomplex_X) and n_fmt_chars != ncol):\n+                raise error\n+            else:\n+                format = fmt\n+        else:\n+            raise ValueError('invalid fmt: %r' % (fmt,))\n+\n+        if len(header) > 0:\n+            header = header.replace('\\n', '\\n' + comments)\n+            fh.write(comments + header + newline)\n+        if iscomplex_X:\n+            for row in X:\n+                row2 = []\n+                for number in row:\n+                    row2.append(number.real)\n+                    row2.append(number.imag)\n+                s = format % tuple(row2) + newline\n+                fh.write(s.replace('+-', '-'))\n+        else:\n+            for row in X:\n+                try:\n+                    v = format % tuple(row) + newline\n+                except TypeError as e:\n+                    raise TypeError(\"Mismatch between array dtype ('%s') and \"\n+                                    \"format specifier ('%s')\"\n+                                    % (str(X.dtype), format)) from e\n+                fh.write(v)\n+\n+        if len(footer) > 0:\n+            footer = footer.replace('\\n', '\\n' + comments)\n+            fh.write(comments + footer + newline)\n+    finally:\n+        if own_fh:\n+            fh.close()\n+\n+\n+@set_module('numpy')\n+def fromregex(file, regexp, dtype, encoding=None):\n+    r\"\"\"\n+    Construct an array from a text file, using regular expression parsing.\n+\n+    The returned array is always a structured array, and is constructed from\n+    all matches of the regular expression in the file. Groups in the regular\n+    expression are converted to fields of the structured array.\n+\n+    Parameters\n+    ----------\n+    file : file, str, or pathlib.Path\n+        Filename or file object to read.\n+\n+        .. versionchanged:: 1.22.0\n+            Now accepts `os.PathLike` implementations.\n+    regexp : str or regexp\n+        Regular expression used to parse the file.\n+        Groups in the regular expression correspond to fields in the dtype.\n+    dtype : dtype or list of dtypes\n+        Dtype for the structured array; must be a structured datatype.\n+    encoding : str, optional\n+        Encoding used to decode the inputfile. Does not apply to input streams.\n+\n+        .. versionadded:: 1.14.0\n+\n+    Returns\n+    -------\n+    output : ndarray\n+        The output array, containing the part of the content of `file` that\n+        was matched by `regexp`. `output` is always a structured array.\n+\n+    Raises\n+    ------\n+    TypeError\n+        When `dtype` is not a valid dtype for a structured array.\n+\n+    See Also\n+    --------\n+    fromstring, loadtxt\n+\n+    Notes\n+    -----\n+    Dtypes for structured arrays can be specified in several forms, but all\n+    forms specify at least the data type and field name. For details see\n+    `basics.rec`.\n+\n+    Examples\n+    --------\n+    >>> from io import StringIO\n+    >>> text = StringIO(\"1312 foo\\n1534  bar\\n444   qux\")\n+\n+    >>> regexp = r\"(\\d+)\\s+(...)\"  # match [digits, whitespace, anything]\n+    >>> output = np.fromregex(text, regexp,\n+    ...                       [('num', np.int64), ('key', 'S3')])\n+    >>> output\n+    array([(1312, b'foo'), (1534, b'bar'), ( 444, b'qux')],\n+          dtype=[('num', '<i8'), ('key', 'S3')])\n+    >>> output['num']\n+    array([1312, 1534,  444])\n+\n+    \"\"\"\n+    own_fh = False\n+    if not hasattr(file, \"read\"):\n+        file = os.fspath(file)\n+        file = np.lib._datasource.open(file, 'rt', encoding=encoding)\n+        own_fh = True\n+\n+    try:\n+        if not isinstance(dtype, np.dtype):\n+            dtype = np.dtype(dtype)\n+        if dtype.names is None:\n+            raise TypeError('dtype must be a structured datatype.')\n+\n+        content = file.read()\n+        if isinstance(content, bytes) and isinstance(regexp, str):\n+            regexp = asbytes(regexp)\n+\n+        if not hasattr(regexp, 'match'):\n+            regexp = re.compile(regexp)\n+        seq = regexp.findall(content)\n+        if seq and not isinstance(seq[0], tuple):\n+            # Only one group is in the regexp.\n+            # Create the new array as a single data-type and then\n+            #   re-interpret as a single-field structured array.\n+            newdtype = np.dtype(dtype[dtype.names[0]])\n+            output = np.array(seq, dtype=newdtype)\n+            output.dtype = dtype\n+        else:\n+            output = np.array(seq, dtype=dtype)\n+\n+        return output\n+    finally:\n+        if own_fh:\n+            file.close()\n+\n+\n+#####--------------------------------------------------------------------------\n+#---- --- ASCII functions ---\n+#####--------------------------------------------------------------------------\n+\n+\n+@set_array_function_like_doc\n+@set_module('numpy')\n+def genfromtxt(fname, dtype=float, comments='#', delimiter=None,\n+               skip_header=0, skip_footer=0, converters=None,\n+               missing_values=None, filling_values=None, usecols=None,\n+               names=None, excludelist=None,\n+               deletechars=''.join(sorted(NameValidator.defaultdeletechars)),\n+               replace_space='_', autostrip=False, case_sensitive=True,\n+               defaultfmt=\"f%i\", unpack=None, usemask=False, loose=True,\n+               invalid_raise=True, max_rows=None, encoding='bytes',\n+               *, ndmin=0, like=None):\n+    \"\"\"\n+    Load data from a text file, with missing values handled as specified.\n+\n+    Each line past the first `skip_header` lines is split at the `delimiter`\n+    character, and characters following the `comments` character are discarded.\n+\n+    Parameters\n+    ----------\n+    fname : file, str, pathlib.Path, list of str, generator\n+        File, filename, list, or generator to read.  If the filename\n+        extension is ``.gz`` or ``.bz2``, the file is first decompressed. Note\n+        that generators must return bytes or strings. The strings\n+        in a list or produced by a generator are treated as lines.\n+    dtype : dtype, optional\n+        Data type of the resulting array.\n+        If None, the dtypes will be determined by the contents of each\n+        column, individually.\n+    comments : str, optional\n+        The character used to indicate the start of a comment.\n+        All the characters occurring on a line after a comment are discarded.\n+    delimiter : str, int, or sequence, optional\n+        The string used to separate values.  By default, any consecutive\n+        whitespaces act as delimiter.  An integer or sequence of integers\n+        can also be provided as width(s) of each field.\n+    skiprows : int, optional\n+        `skiprows` was removed in numpy 1.10. Please use `skip_header` instead.\n+    skip_header : int, optional\n+        The number of lines to skip at the beginning of the file.\n+    skip_footer : int, optional\n+        The number of lines to skip at the end of the file.\n+    converters : variable, optional\n+        The set of functions that convert the data of a column to a value.\n+        The converters can also be used to provide a default value\n+        for missing data: ``converters = {3: lambda s: float(s or 0)}``.\n+    missing : variable, optional\n+        `missing` was removed in numpy 1.10. Please use `missing_values`\n+        instead.\n+    missing_values : variable, optional\n+        The set of strings corresponding to missing data.\n+    filling_values : variable, optional\n+        The set of values to be used as default when the data are missing.\n+    usecols : sequence, optional\n+        Which columns to read, with 0 being the first.  For example,\n+        ``usecols = (1, 4, 5)`` will extract the 2nd, 5th and 6th columns.\n+    names : {None, True, str, sequence}, optional\n+        If `names` is True, the field names are read from the first line after\n+        the first `skip_header` lines. This line can optionally be preceded\n+        by a comment delimiter. If `names` is a sequence or a single-string of\n+        comma-separated names, the names will be used to define the field names\n+        in a structured dtype. If `names` is None, the names of the dtype\n+        fields will be used, if any.\n+    excludelist : sequence, optional\n+        A list of names to exclude. This list is appended to the default list\n+        ['return','file','print']. Excluded names are appended with an\n+        underscore: for example, `file` would become `file_`.\n+    deletechars : str, optional\n+        A string combining invalid characters that must be deleted from the\n+        names.\n+    defaultfmt : str, optional\n+        A format used to define default field names, such as \"f%i\" or \"f_%02i\".\n+    autostrip : bool, optional\n+        Whether to automatically strip white spaces from the variables.\n+    replace_space : char, optional\n+        Character(s) used in replacement of white spaces in the variable\n+        names. By default, use a '_'.\n+    case_sensitive : {True, False, 'upper', 'lower'}, optional\n+        If True, field names are case sensitive.\n+        If False or 'upper', field names are converted to upper case.\n+        If 'lower', field names are converted to lower case.\n+    unpack : bool, optional\n+        If True, the returned array is transposed, so that arguments may be\n+        unpacked using ``x, y, z = genfromtxt(...)``.  When used with a\n+        structured data-type, arrays are returned for each field.\n+        Default is False.\n+    usemask : bool, optional\n+        If True, return a masked array.\n+        If False, return a regular array.\n+    loose : bool, optional\n+        If True, do not raise errors for invalid values.\n+    invalid_raise : bool, optional\n+        If True, an exception is raised if an inconsistency is detected in the\n+        number of columns.\n+        If False, a warning is emitted and the offending lines are skipped.\n+    max_rows : int,  optional\n+        The maximum number of rows to read. Must not be used with skip_footer\n+        at the same time.  If given, the value must be at least 1. Default is\n+        to read the entire file.\n+\n+        .. versionadded:: 1.10.0\n+    encoding : str, optional\n+        Encoding used to decode the inputfile. Does not apply when `fname`\n+        is a file object. The special value 'bytes' enables backward \n+        compatibility workarounds that ensure that you receive byte arrays\n+        when possible and passes latin1 encoded strings to converters. \n+        Override this value to receive unicode arrays and pass strings \n+        as input to converters.  If set to None the system default is used.\n+        The default value is 'bytes'.\n+\n+        .. versionadded:: 1.14.0\n+    ndmin : int, optional\n+        Same parameter as `loadtxt`\n+\n+        .. versionadded:: 1.23.0\n+    ${ARRAY_FUNCTION_LIKE}\n+\n+        .. versionadded:: 1.20.0\n+\n+    Returns\n+    -------\n+    out : ndarray\n+        Data read from the text file. If `usemask` is True, this is a\n+        masked array.\n+\n+    See Also\n+    --------\n+    numpy.loadtxt : equivalent function when no data is missing.\n+\n+    Notes\n+    -----\n+    * When spaces are used as delimiters, or when no delimiter has been given\n+      as input, there should not be any missing data between two fields.\n+    * When variables are named (either by a flexible dtype or with `names`),\n+      there must not be any header in the file (else a ValueError\n+      exception is raised).\n+    * Individual values are not stripped of spaces by default.\n+      When using a custom converter, make sure the function does remove spaces.\n+    * Custom converters may receive unexpected values due to dtype\n+      discovery. \n+\n+    References\n+    ----------\n+    .. [1] NumPy User Guide, section `I/O with NumPy\n+           <https://docs.scipy.org/doc/numpy/user/basics.io.genfromtxt.html>`_.\n+\n+    Examples\n+    --------\n+    >>> from io import StringIO\n+    >>> import numpy as np\n+\n+    Comma delimited file with mixed dtype\n+\n+    >>> s = StringIO(u\"1,1.3,abcde\")\n+    >>> data = np.genfromtxt(s, dtype=[('myint','i8'),('myfloat','f8'),\n+    ... ('mystring','S5')], delimiter=\",\")\n+    >>> data\n+    array((1, 1.3, b'abcde'),\n+          dtype=[('myint', '<i8'), ('myfloat', '<f8'), ('mystring', 'S5')])\n+\n+    Using dtype = None\n+\n+    >>> _ = s.seek(0) # needed for StringIO example only\n+    >>> data = np.genfromtxt(s, dtype=None,\n+    ... names = ['myint','myfloat','mystring'], delimiter=\",\")\n+    >>> data\n+    array((1, 1.3, b'abcde'),\n+          dtype=[('myint', '<i8'), ('myfloat', '<f8'), ('mystring', 'S5')])\n+\n+    Specifying dtype and names\n+\n+    >>> _ = s.seek(0)\n+    >>> data = np.genfromtxt(s, dtype=\"i8,f8,S5\",\n+    ... names=['myint','myfloat','mystring'], delimiter=\",\")\n+    >>> data\n+    array((1, 1.3, b'abcde'),\n+          dtype=[('myint', '<i8'), ('myfloat', '<f8'), ('mystring', 'S5')])\n+\n+    An example with fixed-width columns\n+\n+    >>> s = StringIO(u\"11.3abcde\")\n+    >>> data = np.genfromtxt(s, dtype=None, names=['intvar','fltvar','strvar'],\n+    ...     delimiter=[1,3,5])\n+    >>> data\n+    array((1, 1.3, b'abcde'),\n+          dtype=[('intvar', '<i8'), ('fltvar', '<f8'), ('strvar', 'S5')])\n+\n+    An example to show comments\n+\n+    >>> f = StringIO('''\n+    ... text,# of chars\n+    ... hello world,11\n+    ... numpy,5''')\n+    >>> np.genfromtxt(f, dtype='S12,S12', delimiter=',')\n+    array([(b'text', b''), (b'hello world', b'11'), (b'numpy', b'5')],\n+      dtype=[('f0', 'S12'), ('f1', 'S12')])\n+\n+    \"\"\"\n+\n+    if like is not None:\n+        return _genfromtxt_with_like(\n+            like, fname, dtype=dtype, comments=comments, delimiter=delimiter,\n+            skip_header=skip_header, skip_footer=skip_footer,\n+            converters=converters, missing_values=missing_values,\n+            filling_values=filling_values, usecols=usecols, names=names,\n+            excludelist=excludelist, deletechars=deletechars,\n+            replace_space=replace_space, autostrip=autostrip,\n+            case_sensitive=case_sensitive, defaultfmt=defaultfmt,\n+            unpack=unpack, usemask=usemask, loose=loose,\n+            invalid_raise=invalid_raise, max_rows=max_rows, encoding=encoding,\n+            ndmin=ndmin,\n+        )\n+\n+    _ensure_ndmin_ndarray_check_param(ndmin)\n+\n+    if max_rows is not None:\n+        if skip_footer:\n+            raise ValueError(\n+                    \"The keywords 'skip_footer' and 'max_rows' can not be \"\n+                    \"specified at the same time.\")\n+        if max_rows < 1:\n+            raise ValueError(\"'max_rows' must be at least 1.\")\n+\n+    if usemask:\n+        from numpy.ma import MaskedArray, make_mask_descr\n+    # Check the input dictionary of converters\n+    user_converters = converters or {}\n+    if not isinstance(user_converters, dict):\n+        raise TypeError(\n+            \"The input argument 'converter' should be a valid dictionary \"\n+            \"(got '%s' instead)\" % type(user_converters))\n+\n+    if encoding == 'bytes':\n+        encoding = None\n+        byte_converters = True\n+    else:\n+        byte_converters = False\n+\n+    # Initialize the filehandle, the LineSplitter and the NameValidator\n+    if isinstance(fname, os.PathLike):\n+        fname = os.fspath(fname)\n+    if isinstance(fname, str):\n+        fid = np.lib._datasource.open(fname, 'rt', encoding=encoding)\n+        fid_ctx = contextlib.closing(fid)\n+    else:\n+        fid = fname\n+        fid_ctx = contextlib.nullcontext(fid)\n+    try:\n+        fhd = iter(fid)\n+    except TypeError as e:\n+        raise TypeError(\n+            \"fname must be a string, a filehandle, a sequence of strings,\\n\"\n+            f\"or an iterator of strings. Got {type(fname)} instead.\"\n+        ) from e\n+    with fid_ctx:\n+        split_line = LineSplitter(delimiter=delimiter, comments=comments,\n+                                  autostrip=autostrip, encoding=encoding)\n+        validate_names = NameValidator(excludelist=excludelist,\n+                                       deletechars=deletechars,\n+                                       case_sensitive=case_sensitive,\n+                                       replace_space=replace_space)\n+\n+        # Skip the first `skip_header` rows\n+        try:\n+            for i in range(skip_header):\n+                next(fhd)\n+\n+            # Keep on until we find the first valid values\n+            first_values = None\n+\n+            while not first_values:\n+                first_line = _decode_line(next(fhd), encoding)\n+                if (names is True) and (comments is not None):\n+                    if comments in first_line:\n+                        first_line = (\n+                            ''.join(first_line.split(comments)[1:]))\n+                first_values = split_line(first_line)\n+        except StopIteration:\n+            # return an empty array if the datafile is empty\n+            first_line = ''\n+            first_values = []\n+            warnings.warn(\n+                'genfromtxt: Empty input file: \"%s\"' % fname, stacklevel=2\n+            )\n+\n+        # Should we take the first values as names ?\n+        if names is True:\n+            fval = first_values[0].strip()\n+            if comments is not None:\n+                if fval in comments:\n+                    del first_values[0]\n+\n+        # Check the columns to use: make sure `usecols` is a list\n+        if usecols is not None:\n+            try:\n+                usecols = [_.strip() for _ in usecols.split(\",\")]\n+            except AttributeError:\n+                try:\n+                    usecols = list(usecols)\n+                except TypeError:\n+                    usecols = [usecols, ]\n+        nbcols = len(usecols or first_values)\n+\n+        # Check the names and overwrite the dtype.names if needed\n+        if names is True:\n+            names = validate_names([str(_.strip()) for _ in first_values])\n+            first_line = ''\n+        elif _is_string_like(names):\n+            names = validate_names([_.strip() for _ in names.split(',')])\n+        elif names:\n+            names = validate_names(names)\n+        # Get the dtype\n+        if dtype is not None:\n+            dtype = easy_dtype(dtype, defaultfmt=defaultfmt, names=names,\n+                               excludelist=excludelist,\n+                               deletechars=deletechars,\n+                               case_sensitive=case_sensitive,\n+                               replace_space=replace_space)\n+        # Make sure the names is a list (for 2.5)\n+        if names is not None:\n+            names = list(names)\n+\n+        if usecols:\n+            for (i, current) in enumerate(usecols):\n+                # if usecols is a list of names, convert to a list of indices\n+                if _is_string_like(current):\n+                    usecols[i] = names.index(current)\n+                elif current < 0:\n+                    usecols[i] = current + len(first_values)\n+            # If the dtype is not None, make sure we update it\n+            if (dtype is not None) and (len(dtype) > nbcols):\n+                descr = dtype.descr\n+                dtype = np.dtype([descr[_] for _ in usecols])\n+                names = list(dtype.names)\n+            # If `names` is not None, update the names\n+            elif (names is not None) and (len(names) > nbcols):\n+                names = [names[_] for _ in usecols]\n+        elif (names is not None) and (dtype is not None):\n+            names = list(dtype.names)\n+\n+        # Process the missing values ...............................\n+        # Rename missing_values for convenience\n+        user_missing_values = missing_values or ()\n+        if isinstance(user_missing_values, bytes):\n+            user_missing_values = user_missing_values.decode('latin1')\n+\n+        # Define the list of missing_values (one column: one list)\n+        missing_values = [list(['']) for _ in range(nbcols)]\n+\n+        # We have a dictionary: process it field by field\n+        if isinstance(user_missing_values, dict):\n+            # Loop on the items\n+            for (key, val) in user_missing_values.items():\n+                # Is the key a string ?\n+                if _is_string_like(key):\n+                    try:\n+                        # Transform it into an integer\n+                        key = names.index(key)\n+                    except ValueError:\n+                        # We couldn't find it: the name must have been dropped\n+                        continue\n+                # Redefine the key as needed if it's a column number\n+                if usecols:\n+                    try:\n+                        key = usecols.index(key)\n+                    except ValueError:\n+                        pass\n+                # Transform the value as a list of string\n+                if isinstance(val, (list, tuple)):\n+                    val = [str(_) for _ in val]\n+                else:\n+                    val = [str(val), ]\n+                # Add the value(s) to the current list of missing\n+                if key is None:\n+                    # None acts as default\n+                    for miss in missing_values:\n+                        miss.extend(val)\n+                else:\n+                    missing_values[key].extend(val)\n+        # We have a sequence : each item matches a column\n+        elif isinstance(user_missing_values, (list, tuple)):\n+            for (value, entry) in zip(user_missing_values, missing_values):\n+                value = str(value)\n+                if value not in entry:\n+                    entry.append(value)\n+        # We have a string : apply it to all entries\n+        elif isinstance(user_missing_values, str):\n+            user_value = user_missing_values.split(\",\")\n+            for entry in missing_values:\n+                entry.extend(user_value)\n+        # We have something else: apply it to all entries\n+        else:\n+            for entry in missing_values:\n+                entry.extend([str(user_missing_values)])\n+\n+        # Process the filling_values ...............................\n+        # Rename the input for convenience\n+        user_filling_values = filling_values\n+        if user_filling_values is None:\n+            user_filling_values = []\n+        # Define the default\n+        filling_values = [None] * nbcols\n+        # We have a dictionary : update each entry individually\n+        if isinstance(user_filling_values, dict):\n+            for (key, val) in user_filling_values.items():\n+                if _is_string_like(key):\n+                    try:\n+                        # Transform it into an integer\n+                        key = names.index(key)\n+                    except ValueError:\n+                        # We couldn't find it: the name must have been dropped\n+                        continue\n+                # Redefine the key if it's a column number \n+                # and usecols is defined\n+                if usecols:\n+                    try:\n+                        key = usecols.index(key)\n+                    except ValueError:\n+                        pass\n+                # Add the value to the list\n+                filling_values[key] = val\n+        # We have a sequence : update on a one-to-one basis\n+        elif isinstance(user_filling_values, (list, tuple)):\n+            n = len(user_filling_values)\n+            if (n <= nbcols):\n+                filling_values[:n] = user_filling_values\n+            else:\n+                filling_values = user_filling_values[:nbcols]\n+        # We have something else : use it for all entries\n+        else:\n+            filling_values = [user_filling_values] * nbcols\n+\n+        # Initialize the converters ................................\n+        if dtype is None:\n+            # Note: we can't use a [...]*nbcols, as we would have 3 times\n+            # the same converter, instead of 3 different converters.\n+            converters = [\n+                StringConverter(None, missing_values=miss, default=fill)\n+                for (miss, fill) in zip(missing_values, filling_values)\n+            ]\n+        else:\n+            dtype_flat = flatten_dtype(dtype, flatten_base=True)\n+            # Initialize the converters\n+            if len(dtype_flat) > 1:\n+                # Flexible type : get a converter from each dtype\n+                zipit = zip(dtype_flat, missing_values, filling_values)\n+                converters = [StringConverter(dt, \n+                                              locked=True,\n+                                              missing_values=miss, \n+                                              default=fill)\n+                              for (dt, miss, fill) in zipit]\n+            else:\n+                # Set to a default converter (but w/ different missing values)\n+                zipit = zip(missing_values, filling_values)\n+                converters = [StringConverter(dtype, \n+                                              locked=True,\n+                                              missing_values=miss, \n+                                              default=fill)\n+                              for (miss, fill) in zipit]\n+        # Update the converters to use the user-defined ones\n+        uc_update = []\n+        for (j, conv) in user_converters.items():\n+            # If the converter is specified by column names, \n+            # use the index instead\n+            if _is_string_like(j):\n+                try:\n+                    j = names.index(j)\n+                    i = j\n+                except ValueError:\n+                    continue\n+            elif usecols:\n+                try:\n+                    i = usecols.index(j)\n+                except ValueError:\n+                    # Unused converter specified\n+                    continue\n+            else:\n+                i = j\n+            # Find the value to test - first_line is not filtered by usecols:\n+            if len(first_line):\n+                testing_value = first_values[j]\n+            else:\n+                testing_value = None\n+            if conv is bytes:\n+                user_conv = asbytes\n+            elif byte_converters:\n+                # Converters may use decode to workaround numpy's old \n+                # behavior, so encode the string again before passing \n+                # to the user converter.\n+                def tobytes_first(x, conv):\n+                    if type(x) is bytes:\n+                        return conv(x)\n+                    return conv(x.encode(\"latin1\"))\n+                user_conv = functools.partial(tobytes_first, conv=conv)\n+            else:\n+                user_conv = conv\n+            converters[i].update(user_conv, locked=True,\n+                                 testing_value=testing_value,\n+                                 default=filling_values[i],\n+                                 missing_values=missing_values[i],)\n+            uc_update.append((i, user_conv))\n+        # Make sure we have the corrected keys in user_converters...\n+        user_converters.update(uc_update)\n+\n+        # Fixme: possible error as following variable never used.\n+        # miss_chars = [_.missing_values for _ in converters]\n+\n+        # Initialize the output lists ...\n+        # ... rows\n+        rows = []\n+        append_to_rows = rows.append\n+        # ... masks\n+        if usemask:\n+            masks = []\n+            append_to_masks = masks.append\n+        # ... invalid\n+        invalid = []\n+        append_to_invalid = invalid.append\n+\n+        # Parse each line\n+        for (i, line) in enumerate(itertools.chain([first_line, ], fhd)):\n+            values = split_line(line)\n+            nbvalues = len(values)\n+            # Skip an empty line\n+            if nbvalues == 0:\n+                continue\n+            if usecols:\n+                # Select only the columns we need\n+                try:\n+                    values = [values[_] for _ in usecols]\n+                except IndexError:\n+                    append_to_invalid((i + skip_header + 1, nbvalues))\n+                    continue\n+            elif nbvalues != nbcols:\n+                append_to_invalid((i + skip_header + 1, nbvalues))\n+                continue\n+            # Store the values\n+            append_to_rows(tuple(values))\n+            if usemask:\n+                append_to_masks(tuple([v.strip() in m\n+                                       for (v, m) in zip(values,\n+                                                         missing_values)]))\n+            if len(rows) == max_rows:\n+                break\n+\n+    # Upgrade the converters (if needed)\n+    if dtype is None:\n+        for (i, converter) in enumerate(converters):\n+            current_column = [itemgetter(i)(_m) for _m in rows]\n+            try:\n+                converter.iterupgrade(current_column)\n+            except ConverterLockError:\n+                errmsg = \"Converter #%i is locked and cannot be upgraded: \" % i\n+                current_column = map(itemgetter(i), rows)\n+                for (j, value) in enumerate(current_column):\n+                    try:\n+                        converter.upgrade(value)\n+                    except (ConverterError, ValueError):\n+                        errmsg += \"(occurred line #%i for value '%s')\"\n+                        errmsg %= (j + 1 + skip_header, value)\n+                        raise ConverterError(errmsg)\n+\n+    # Check that we don't have invalid values\n+    nbinvalid = len(invalid)\n+    if nbinvalid > 0:\n+        nbrows = len(rows) + nbinvalid - skip_footer\n+        # Construct the error message\n+        template = \"    Line #%%i (got %%i columns instead of %i)\" % nbcols\n+        if skip_footer > 0:\n+            nbinvalid_skipped = len([_ for _ in invalid\n+                                     if _[0] > nbrows + skip_header])\n+            invalid = invalid[:nbinvalid - nbinvalid_skipped]\n+            skip_footer -= nbinvalid_skipped\n+#\n+#            nbrows -= skip_footer\n+#            errmsg = [template % (i, nb)\n+#                      for (i, nb) in invalid if i < nbrows]\n+#        else:\n+        errmsg = [template % (i, nb)\n+                  for (i, nb) in invalid]\n+        if len(errmsg):\n+            errmsg.insert(0, \"Some errors were detected !\")\n+            errmsg = \"\\n\".join(errmsg)\n+            # Raise an exception ?\n+            if invalid_raise:\n+                raise ValueError(errmsg)\n+            # Issue a warning ?\n+            else:\n+                warnings.warn(errmsg, ConversionWarning, stacklevel=2)\n+\n+    # Strip the last skip_footer data\n+    if skip_footer > 0:\n+        rows = rows[:-skip_footer]\n+        if usemask:\n+            masks = masks[:-skip_footer]\n+\n+    # Convert each value according to the converter:\n+    # We want to modify the list in place to avoid creating a new one...\n+    if loose:\n+        rows = list(\n+            zip(*[[conv._loose_call(_r) for _r in map(itemgetter(i), rows)]\n+                  for (i, conv) in enumerate(converters)]))\n+    else:\n+        rows = list(\n+            zip(*[[conv._strict_call(_r) for _r in map(itemgetter(i), rows)]\n+                  for (i, conv) in enumerate(converters)]))\n+\n+    # Reset the dtype\n+    data = rows\n+    if dtype is None:\n+        # Get the dtypes from the types of the converters\n+        column_types = [conv.type for conv in converters]\n+        # Find the columns with strings...\n+        strcolidx = [i for (i, v) in enumerate(column_types)\n+                     if v == np.str_]\n+\n+        if byte_converters and strcolidx:\n+            # convert strings back to bytes for backward compatibility\n+            warnings.warn(\n+                \"Reading unicode strings without specifying the encoding \"\n+                \"argument is deprecated. Set the encoding, use None for the \"\n+                \"system default.\",\n+                np.exceptions.VisibleDeprecationWarning, stacklevel=2)\n+            \n+            def encode_unicode_cols(row_tup):\n+                row = list(row_tup)\n+                for i in strcolidx:\n+                    row[i] = row[i].encode('latin1')\n+                return tuple(row)\n+\n+            try:\n+                data = [encode_unicode_cols(r) for r in data]\n+            except UnicodeEncodeError:\n+                pass\n+            else:\n+                for i in strcolidx:\n+                    column_types[i] = np.bytes_\n+\n+        # Update string types to be the right length\n+        sized_column_types = column_types[:]\n+        for i, col_type in enumerate(column_types):\n+            if np.issubdtype(col_type, np.character):\n+                n_chars = max(len(row[i]) for row in data)\n+                sized_column_types[i] = (col_type, n_chars)\n+\n+        if names is None:\n+            # If the dtype is uniform (before sizing strings)\n+            base = {\n+                c_type\n+                for c, c_type in zip(converters, column_types)\n+                if c._checked}\n+            if len(base) == 1:\n+                uniform_type, = base\n+                (ddtype, mdtype) = (uniform_type, bool)\n+            else:\n+                ddtype = [(defaultfmt % i, dt)\n+                          for (i, dt) in enumerate(sized_column_types)]\n+                if usemask:\n+                    mdtype = [(defaultfmt % i, bool)\n+                              for (i, dt) in enumerate(sized_column_types)]\n+        else:\n+            ddtype = list(zip(names, sized_column_types))\n+            mdtype = list(zip(names, [bool] * len(sized_column_types)))\n+        output = np.array(data, dtype=ddtype)\n+        if usemask:\n+            outputmask = np.array(masks, dtype=mdtype)\n+    else:\n+        # Overwrite the initial dtype names if needed\n+        if names and dtype.names is not None:\n+            dtype.names = names\n+        # Case 1. We have a structured type\n+        if len(dtype_flat) > 1:\n+            # Nested dtype, eg [('a', int), ('b', [('b0', int), ('b1', 'f4')])]\n+            # First, create the array using a flattened dtype:\n+            # [('a', int), ('b1', int), ('b2', float)]\n+            # Then, view the array using the specified dtype.\n+            if 'O' in (_.char for _ in dtype_flat):\n+                if has_nested_fields(dtype):\n+                    raise NotImplementedError(\n+                        \"Nested fields involving objects are not supported...\")\n+                else:\n+                    output = np.array(data, dtype=dtype)\n+            else:\n+                rows = np.array(data, dtype=[('', _) for _ in dtype_flat])\n+                output = rows.view(dtype)\n+            # Now, process the rowmasks the same way\n+            if usemask:\n+                rowmasks = np.array(\n+                    masks, dtype=np.dtype([('', bool) for t in dtype_flat]))\n+                # Construct the new dtype\n+                mdtype = make_mask_descr(dtype)\n+                outputmask = rowmasks.view(mdtype)\n+        # Case #2. We have a basic dtype\n+        else:\n+            # We used some user-defined converters\n+            if user_converters:\n+                ishomogeneous = True\n+                descr = []\n+                for i, ttype in enumerate([conv.type for conv in converters]):\n+                    # Keep the dtype of the current converter\n+                    if i in user_converters:\n+                        ishomogeneous &= (ttype == dtype.type)\n+                        if np.issubdtype(ttype, np.character):\n+                            ttype = (ttype, max(len(row[i]) for row in data))\n+                        descr.append(('', ttype))\n+                    else:\n+                        descr.append(('', dtype))\n+                # So we changed the dtype ?\n+                if not ishomogeneous:\n+                    # We have more than one field\n+                    if len(descr) > 1:\n+                        dtype = np.dtype(descr)\n+                    # We have only one field: drop the name if not needed.\n+                    else:\n+                        dtype = np.dtype(ttype)\n+            #\n+            output = np.array(data, dtype)\n+            if usemask:\n+                if dtype.names is not None:\n+                    mdtype = [(_, bool) for _ in dtype.names]\n+                else:\n+                    mdtype = bool\n+                outputmask = np.array(masks, dtype=mdtype)\n+    # Try to take care of the missing data we missed\n+    names = output.dtype.names\n+    if usemask and names:\n+        for (name, conv) in zip(names, converters):\n+            missing_values = [conv(_) for _ in conv.missing_values\n+                              if _ != '']\n+            for mval in missing_values:\n+                outputmask[name] |= (output[name] == mval)\n+    # Construct the final array\n+    if usemask:\n+        output = output.view(MaskedArray)\n+        output._mask = outputmask\n+\n+    output = _ensure_ndmin_ndarray(output, ndmin=ndmin)\n+\n+    if unpack:\n+        if names is None:\n+            return output.T\n+        elif len(names) == 1:\n+            # squeeze single-name dtypes too\n+            return output[names[0]]\n+        else:\n+            # For structured arrays with multiple fields,\n+            # return an array for each field.\n+            return [output[field] for field in names]\n+    return output\n+\n+\n+_genfromtxt_with_like = array_function_dispatch()(genfromtxt)\n+\n+\n+def recfromtxt(fname, **kwargs):\n+    \"\"\"\n+    Load ASCII data from a file and return it in a record array.\n+\n+    If ``usemask=False`` a standard `recarray` is returned,\n+    if ``usemask=True`` a MaskedRecords array is returned.\n+\n+    .. deprecated:: 2.0\n+        Use `numpy.genfromtxt` instead.\n+\n+    Parameters\n+    ----------\n+    fname, kwargs : For a description of input parameters, see `genfromtxt`.\n+\n+    See Also\n+    --------\n+    numpy.genfromtxt : generic function\n+\n+    Notes\n+    -----\n+    By default, `dtype` is None, which means that the data-type of the output\n+    array will be determined from the data.\n+\n+    \"\"\"\n+\n+    # Deprecated in NumPy 2.0, 2023-07-11\n+    warnings.warn(\n+        \"`recfromtxt` is deprecated, \"\n+        \"use `numpy.genfromtxt` instead.\"\n+        \"(deprecated in NumPy 2.0)\",\n+        DeprecationWarning,\n+        stacklevel=2\n+    )\n+\n+    kwargs.setdefault(\"dtype\", None)\n+    usemask = kwargs.get('usemask', False)\n+    output = genfromtxt(fname, **kwargs)\n+    if usemask:\n+        from numpy.ma.mrecords import MaskedRecords\n+        output = output.view(MaskedRecords)\n+    else:\n+        output = output.view(np.recarray)\n+    return output\n+\n+\n+def recfromcsv(fname, **kwargs):\n+    \"\"\"\n+    Load ASCII data stored in a comma-separated file.\n+\n+    The returned array is a record array (if ``usemask=False``, see\n+    `recarray`) or a masked record array (if ``usemask=True``,\n+    see `ma.mrecords.MaskedRecords`).\n+\n+    .. deprecated:: 2.0\n+        Use `numpy.genfromtxt` with comma as `delimiter` instead.\n+\n+    Parameters\n+    ----------\n+    fname, kwargs : For a description of input parameters, see `genfromtxt`.\n+\n+    See Also\n+    --------\n+    numpy.genfromtxt : generic function to load ASCII data.\n+\n+    Notes\n+    -----\n+    By default, `dtype` is None, which means that the data-type of the output\n+    array will be determined from the data.\n+\n+    \"\"\"\n+\n+    # Deprecated in NumPy 2.0, 2023-07-11\n+    warnings.warn(\n+        \"`recfromcsv` is deprecated, \"\n+        \"use `numpy.genfromtxt` with comma as `delimiter` instead. \"\n+        \"(deprecated in NumPy 2.0)\",\n+        DeprecationWarning,\n+        stacklevel=2\n+    )\n+\n+    # Set default kwargs for genfromtxt as relevant to csv import.\n+    kwargs.setdefault(\"case_sensitive\", \"lower\")\n+    kwargs.setdefault(\"names\", True)\n+    kwargs.setdefault(\"delimiter\", \",\")\n+    kwargs.setdefault(\"dtype\", None)\n+    output = genfromtxt(fname, **kwargs)\n+\n+    usemask = kwargs.get(\"usemask\", False)\n+    if usemask:\n+        from numpy.ma.mrecords import MaskedRecords\n+        output = output.view(MaskedRecords)\n+    else:\n+        output = output.view(np.recarray)\n+    return output\n",
            "comment_added_diff": {
                "53": "    ...     def __getitem__(self, key): # An instance of BagObj(BagDemo)",
                "54": "    ...                                 # will call this method when any",
                "55": "    ...                                 # attribute look-up is required",
                "69": "        # Use weakref to make NpzFile objects collectable by refcount",
                "171": "    >>> npz['x']  # getitem access",
                "173": "    >>> npz.f.x  # attribute lookup",
                "177": "    # Make __exit__ safe if zipfile_factory raises an exception",
                "185": "        # Import is postponed to here since zipfile depends on gzip, an",
                "186": "        # optional component of the so-called standard library.",
                "220": "        self.f = None  # break reference cycle",
                "225": "    # Implement the Mapping ABC",
                "233": "        # FIXME: This seems like it will copy strings around",
                "234": "        #   more than is strictly necessary.  The zipfile",
                "235": "        #   will read the string and then",
                "236": "        #   the format.read_array will copy the string",
                "237": "        #   to another place in memory.",
                "238": "        #   It would be better if the zipfile could read",
                "239": "        #   (or at least uncompress) the data",
                "240": "        #   directly into the array memory.",
                "266": "        # Get filename or default to `object`",
                "272": "        # Get the name of arrays",
                "404": "        # The 'encoding' value for pickle also affects what encoding",
                "405": "        # the serialized binary data of NumPy arrays is loaded",
                "406": "        # in. Pickle does not pass on the encoding information to",
                "407": "        # NumPy. The unpickling code in numpy.core.multiarray is",
                "408": "        # written to assume that unicode data appearing where binary",
                "409": "        # should be is in 'latin1'. 'bytes' is also safe, as is 'ASCII'.",
                "410": "        #",
                "411": "        # Other encoding values can corrupt binary data, and we",
                "412": "        # purposefully disallow them. For the same reason, the errors=",
                "413": "        # argument is not exposed, as values other than 'strict'",
                "414": "        # result can similarly silently corrupt numerical data.",
                "427": "        # Code to distinguish from NumPy binary files and pickles.",
                "429": "        _ZIP_SUFFIX = b'PK\\x05\\x06'  # empty zip files start with this",
                "434": "        # If the file size is less than N, we need to make sure not",
                "435": "        # to seek past the beginning of the file",
                "436": "        fid.seek(-min(N, len(magic)), 1)  # back-up",
                "438": "            # zip-file (assume .npz)",
                "439": "            # Potentially transfer file ownership to NpzFile",
                "446": "            # .npy file",
                "457": "            # Try a pickle",
                "519": "    >>> _ = outfile.seek(0) # Only needed to simulate closing & reopening file",
                "531": "    # [1 2] [1 3]",
                "617": "    >>> _ = outfile.seek(0) # Only needed to simulate closing & reopening file",
                "711": "    # Import is postponed to here since zipfile depends on gzip, an optional",
                "712": "    # component of the so-called standard library.",
                "738": "        # always force zip64, gh-10776",
                "753": "    # Check correctness of the values of `ndmin`",
                "765": "    # Verify that the array has at least dimensions `ndmin`.",
                "766": "    # Tweak the size and shape of the arrays - remove extraneous dimensions",
                "769": "    # and ensure we have the minimum number of dimensions asked for",
                "770": "    # - has to be in this order for the odd case ndmin=1, a.squeeze().ndim=0",
                "780": "# amount of lines loadtxt reads in one chunk, can be overridden for testing",
                "802": "            # Need to handle conversion here, or the splitting would fail",
                "811": "# The number of rows we read in one go if confronted with a parametric dtype",
                "815": "def _read(fname, *, delimiter=',', comment='#', quote='\"',",
                "883": "    # Handle special 'bytes' keyword for encoding",
                "896": "        # This is a legacy \"flexible\" dtype.  We do not truly support",
                "897": "        # parametric dtypes currently (no dtype discovery step in the core),",
                "898": "        # but have to support these for backward compatibility.",
                "903": "        # Allow usecols to be a single int or a sequence of ints, the C-code",
                "904": "        # handles the rest",
                "915": "        # assume comments are a sequence of strings",
                "924": "            comments = None  # No comments at all",
                "926": "            # If there is only one comment, and that comment has one character,",
                "927": "            # the normal parsing can deal with it just fine.",
                "932": "            # Input validation if there are multiple comment characters",
                "939": "    # comment is now either a 1 or 0 character string or a tuple:",
                "941": "        # Note: An earlier version support two character comments (and could",
                "942": "        #       have been extended to multiple characters, we assume this is",
                "943": "        #       rare enough to not optimize for.",
                "957": "        # Passing -1 to the C code means \"read the entire file\".",
                "999": "            # This branch reads the file into chunks of object arrays and then",
                "1000": "            # casts them to the desired actual dtype.  This ensures correct",
                "1001": "            # string-length and datetime-unit discovery (like `arr.astype()`).",
                "1002": "            # Due to chunking, certain error reports are less clear, currently.",
                "1004": "                data = iter(data)  # cannot chunk when reading from file",
                "1008": "                c_byte_converters = True  # Use latin1 rather than ascii",
                "1025": "                # Cast here already.  We hope that this is better even for",
                "1026": "                # large files because the storage is more compact.  It could",
                "1027": "                # be adapted (in principle the concatenate could cast).",
                "1030": "                skiprows = 0  # Only have to skip for first chunk",
                "1034": "                    # There was less data than requested, so we are done.",
                "1037": "            # Need at least one chunk, but if empty, the last one may have",
                "1038": "            # the wrong shape.",
                "1046": "    # NOTE: ndmin works as advertised for structured dtypes, but normally",
                "1047": "    #       these would return a 1D result plus the structured dimension,",
                "1048": "    #       so ndmin=2 adds a third dimension even when no squeezing occurs.",
                "1049": "    #       A `squeeze=False` could be a better solution (pandas uses squeeze).",
                "1061": "        # Unpack structured dtypes if requested:",
                "1064": "            # For structured arrays, return an array for each field.",
                "1074": "def loadtxt(fname, dtype=float, comments='#', delimiter=None,",
                "1097": "        strings will be decoded as 'latin1'. The default is '#'.",
                "1158": "            starting with '#' or as specified via `comments`) are not counted",
                "1203": "    >>> from io import StringIO   # StringIO behaves like a file object",
                "1228": "    ...     0: lambda x: np.floor(float(x)),  # conversion fn for column 0",
                "1229": "    ...     1: lambda x: np.ceil(float(x)),  # conversion fn for column 1",
                "1291": "    >>> s = StringIO('\"alpha, #42\", 10.0\\n\"beta, #64\", 2.0\\n')",
                "1294": "    array([('alpha, #42', 10.), ('beta, #64',  2.)],",
                "1299": "    >>> s = StringIO('\"alpha, #42\"       10.0\\n\"beta, #64\" 2.0\\n')",
                "1302": "    array([('alpha, #42', 10.), ('beta, #64',  2.)],",
                "1338": "    # Control character type conversions for Py3 convenience",
                "1366": "            footer='', comments='# ', encoding=None):",
                "1407": "        to mark them as comments. Default: '# ',  as expected by e.g.",
                "1475": "           <https://docs.python.org/library/string.html#format-specification-mini-language>`_,",
                "1481": "    >>> np.savetxt('test.out', x, delimiter=',')   # X is an array",
                "1482": "    >>> np.savetxt('test.out', (x,y,z))   # x,y,z equal sized 1D arrays",
                "1483": "    >>> np.savetxt('test.out', x, fmt='%1.4e')   # use exponential notation",
                "1516": "                # input is probably a bytestream",
                "1524": "        # datasource doesn't support creating a new file ...",
                "1529": "        # wrap to handle byte output streams",
                "1537": "        # Handle 1-dimensional arrays",
                "1542": "            # Common case -- 1d array of numbers",
                "1547": "            # Complex dtype -- each field indicates a separate column",
                "1554": "        # `fmt` can be a string with multiple insertion points or a",
                "1555": "        # list of formats.  E.g. '%10.5f\\t%10d' or ('%10.5f', '$10d')",
                "1659": "    >>> regexp = r\"(\\d+)\\s+(...)\"  # match [digits, whitespace, anything]",
                "1689": "            # Only one group is in the regexp.",
                "1690": "            # Create the new array as a single data-type and then",
                "1691": "            #   re-interpret as a single-field structured array.",
                "1704": "#####--------------------------------------------------------------------------",
                "1705": "#---- --- ASCII functions ---",
                "1706": "#####--------------------------------------------------------------------------",
                "1711": "def genfromtxt(fname, dtype=float, comments='#', delimiter=None,",
                "1870": "    >>> _ = s.seek(0) # needed for StringIO example only",
                "1898": "    ... text,# of chars",
                "1933": "    # Check the input dictionary of converters",
                "1946": "    # Initialize the filehandle, the LineSplitter and the NameValidator",
                "1970": "        # Skip the first `skip_header` rows",
                "1975": "            # Keep on until we find the first valid values",
                "1986": "            # return an empty array if the datafile is empty",
                "1993": "        # Should we take the first values as names ?",
                "2000": "        # Check the columns to use: make sure `usecols` is a list",
                "2011": "        # Check the names and overwrite the dtype.names if needed",
                "2019": "        # Get the dtype",
                "2026": "        # Make sure the names is a list (for 2.5)",
                "2032": "                # if usecols is a list of names, convert to a list of indices",
                "2037": "            # If the dtype is not None, make sure we update it",
                "2042": "            # If `names` is not None, update the names",
                "2048": "        # Process the missing values ...............................",
                "2049": "        # Rename missing_values for convenience",
                "2054": "        # Define the list of missing_values (one column: one list)",
                "2057": "        # We have a dictionary: process it field by field",
                "2059": "            # Loop on the items",
                "2061": "                # Is the key a string ?",
                "2064": "                        # Transform it into an integer",
                "2067": "                        # We couldn't find it: the name must have been dropped",
                "2069": "                # Redefine the key as needed if it's a column number",
                "2075": "                # Transform the value as a list of string",
                "2080": "                # Add the value(s) to the current list of missing",
                "2082": "                    # None acts as default",
                "2087": "        # We have a sequence : each item matches a column",
                "2093": "        # We have a string : apply it to all entries",
                "2098": "        # We have something else: apply it to all entries",
                "2103": "        # Process the filling_values ...............................",
                "2104": "        # Rename the input for convenience",
                "2108": "        # Define the default",
                "2110": "        # We have a dictionary : update each entry individually",
                "2115": "                        # Transform it into an integer",
                "2118": "                        # We couldn't find it: the name must have been dropped",
                "2120": "                # Redefine the key if it's a column number",
                "2121": "                # and usecols is defined",
                "2127": "                # Add the value to the list",
                "2129": "        # We have a sequence : update on a one-to-one basis",
                "2136": "        # We have something else : use it for all entries",
                "2140": "        # Initialize the converters ................................",
                "2142": "            # Note: we can't use a [...]*nbcols, as we would have 3 times",
                "2143": "            # the same converter, instead of 3 different converters.",
                "2150": "            # Initialize the converters",
                "2152": "                # Flexible type : get a converter from each dtype",
                "2160": "                # Set to a default converter (but w/ different missing values)",
                "2167": "        # Update the converters to use the user-defined ones",
                "2170": "            # If the converter is specified by column names,",
                "2171": "            # use the index instead",
                "2182": "                    # Unused converter specified",
                "2186": "            # Find the value to test - first_line is not filtered by usecols:",
                "2194": "                # Converters may use decode to workaround numpy's old",
                "2195": "                # behavior, so encode the string again before passing",
                "2196": "                # to the user converter.",
                "2209": "        # Make sure we have the corrected keys in user_converters...",
                "2212": "        # Fixme: possible error as following variable never used.",
                "2213": "        # miss_chars = [_.missing_values for _ in converters]",
                "2215": "        # Initialize the output lists ...",
                "2216": "        # ... rows",
                "2219": "        # ... masks",
                "2223": "        # ... invalid",
                "2227": "        # Parse each line",
                "2231": "            # Skip an empty line",
                "2235": "                # Select only the columns we need",
                "2244": "            # Store the values",
                "2253": "    # Upgrade the converters (if needed)",
                "2260": "                errmsg = \"Converter #%i is locked and cannot be upgraded: \" % i",
                "2266": "                        errmsg += \"(occurred line #%i for value '%s')\"",
                "2270": "    # Check that we don't have invalid values",
                "2274": "        # Construct the error message",
                "2275": "        template = \"    Line #%%i (got %%i columns instead of %i)\" % nbcols",
                "2281": "#",
                "2282": "#            nbrows -= skip_footer",
                "2283": "#            errmsg = [template % (i, nb)",
                "2284": "#                      for (i, nb) in invalid if i < nbrows]",
                "2285": "#        else:",
                "2291": "            # Raise an exception ?",
                "2294": "            # Issue a warning ?",
                "2298": "    # Strip the last skip_footer data",
                "2304": "    # Convert each value according to the converter:",
                "2305": "    # We want to modify the list in place to avoid creating a new one...",
                "2315": "    # Reset the dtype",
                "2318": "        # Get the dtypes from the types of the converters",
                "2320": "        # Find the columns with strings...",
                "2325": "            # convert strings back to bytes for backward compatibility",
                "2346": "        # Update string types to be the right length",
                "2354": "            # If the dtype is uniform (before sizing strings)",
                "2375": "        # Overwrite the initial dtype names if needed",
                "2378": "        # Case 1. We have a structured type",
                "2380": "            # Nested dtype, eg [('a', int), ('b', [('b0', int), ('b1', 'f4')])]",
                "2381": "            # First, create the array using a flattened dtype:",
                "2382": "            # [('a', int), ('b1', int), ('b2', float)]",
                "2383": "            # Then, view the array using the specified dtype.",
                "2393": "            # Now, process the rowmasks the same way",
                "2397": "                # Construct the new dtype",
                "2400": "        # Case #2. We have a basic dtype",
                "2402": "            # We used some user-defined converters",
                "2407": "                    # Keep the dtype of the current converter",
                "2415": "                # So we changed the dtype ?",
                "2417": "                    # We have more than one field",
                "2420": "                    # We have only one field: drop the name if not needed.",
                "2423": "            #",
                "2431": "    # Try to take care of the missing data we missed",
                "2439": "    # Construct the final array",
                "2450": "            # squeeze single-name dtypes too",
                "2453": "            # For structured arrays with multiple fields,",
                "2454": "            # return an array for each field.",
                "2487": "    # Deprecated in NumPy 2.0, 2023-07-11",
                "2533": "    # Deprecated in NumPy 2.0, 2023-07-11",
                "2542": "    # Set default kwargs for genfromtxt as relevant to csv import."
            },
            "comment_deleted_diff": {},
            "comment_modified_diff": {}
        }
    ],
    "_npyio_impl.pyi": [],
    "_polynomial_impl.py": [],
    "_polynomial_impl.pyi": [],
    "datasource.pyi": [],
    "_index_tricks_impl.py": [],
    "_index_tricks_impl.pyi": [],
    "test_index_tricks.py": [],
    "HOWTO_RELEASE.rst": [],
    "mypy_plugin.py": [],
    "arrayprint.pyi": [],
    "flatiter.pyi": [],
    "array_constructors.py": [],
    "array_like.py": [],
    "literal.py": [
        {
            "commit": "1621dfff71033c0e9ce2427809205e1b5e3bcfca",
            "timestamp": "2023-09-05T17:58:12+02:00",
            "author": "Bas van Beek",
            "commit_message": "TYP: Use stricter mypy settings\n\n* Disallow generic types lacking a parameter\n* Disallow importing from unknown modules",
            "additions": 3,
            "deletions": 2,
            "change_type": "MODIFY",
            "diff": "@@ -1,9 +1,10 @@\n from __future__ import annotations\n \n+from typing import Any\n from functools import partial\n from collections.abc import Callable\n \n-import pytest  # type: ignore\n+import pytest\n import numpy as np\n \n AR = np.array(0)\n@@ -13,7 +14,7 @@\n ACF = frozenset({None, \"A\", \"C\", \"F\"})\n CF = frozenset({None, \"C\", \"F\"})\n \n-order_list: list[tuple[frozenset, Callable]] = [\n+order_list: list[tuple[frozenset[str | None], Callable[..., Any]]] = [\n     (KACF, partial(np.ndarray, 1)),\n     (KACF, AR.tobytes),\n     (KACF, partial(AR.astype, int)),\n",
            "comment_added_diff": {},
            "comment_deleted_diff": {
                "6": "import pytest  # type: ignore"
            },
            "comment_modified_diff": {}
        }
    ],
    "dtype.pyi": [],
    "_internal.pyi": [],
    "einsumfunc.pyi": [],
    "chebyshev.pyi": [],
    "hermite_e.pyi": [],
    "laguerre.pyi": [],
    "legendre.pyi": [],
    "_mt19937.pyi": [],
    "_philox.pyi": [],
    "_sfc64.pyi": [],
    "mtrand.pyi": [],
    "comparisons.pyi": [],
    "ndarray_misc.pyi": [],
    "random.pyi": [],
    "ufunclike.pyi": [],
    "24532.new_feature.rst": [],
    "_backend.py": [],
    "_distutils.py": [],
    "_meson.py": [
        {
            "commit": "fedc834fecb7e1aca2917ad2e0fd1694400529e0",
            "timestamp": "2023-09-05T10:42:35-06:00",
            "author": "Rohit Goswami",
            "commit_message": "ENH: ``meson`` backend for ``f2py`` (#24532)\n\n* FIX: Import f2py2e rather than f2py for run_main\r\n\r\n* FIX: Import f2py2e instead of f2py\r\n\r\n* ENH: Add F2PY back-end work from gh-22225\r\n\r\nCo-authored-by: NamamiShanker <NamamiShanker@users.noreply.github.com>\r\n\r\n* ENH: Add meson skeleton from gh-2225\r\n\r\nCo-authored-by: NamamiShanker <NamamiShanker@users.noreply.github.com>\r\n\r\n* MAINT: Trim backend.py down to f2py2e flags\r\n\r\n* ENH: Add a factory function for backends\r\n\r\n* ENH: Add a distutils backend\r\n\r\n* ENH: Handle --backends in f2py\r\n\r\nDefaults to distutils for now\r\n\r\n* DOC: Add some minor comments in f2py2e\r\n\r\n* MAINT: Refactor and rework meson.build.src\r\n\r\n* MAINT: Add objects\r\n\r\n* MAINT: Cleanup distutils backend\r\n\r\n* MAINT: Refactor to add everything back to backend\r\n\r\nNecessary for the meson.build for now. Refactors / cleanup needs better\r\nargument handling in f2py2e\r\n\r\n* MAINT: Fix overly long line\r\n\r\n* BUG: Construct wrappers for meson backend\r\n\r\n* MAINT: Rework, simplify template massively\r\n\r\n* ENH: Truncate meson.build to skeleton only\r\n\r\n* MAINT: Minor backend housekeeping, name changes\r\n\r\n* MAINT: Less absolute paths, update setup.py [f2py]\r\n\r\n* MAINT: Move f2py module name functionality\r\n\r\nPreviously part of np.distutils\r\n\r\n* ENH: Handle .pyf files\r\n\r\n* TST: Fix typo in isoFortranEnvMap.f90\r\n\r\n* MAINT: Typo in f2py2e support for pyf files\r\n\r\n* DOC: Add release note for --backend\r\n\r\n* MAINT: Conditional switch for Python 3.12 [f2py]\r\n\r\n* MAINT: No absolute paths in backend [f2py-meson]\r\n\r\nThe files are copied over anyway, this makes it easier to extend the\r\ngenerated skeleton\r\n\r\n* MAINT: Prettier generated meson.build files [f2py]\r\n\r\n* ENH: Add meson's dependency(blah) to f2py\r\n\r\n* DOC: Document the new flag\r\n\r\n* MAINT: Simplify and rename backend template [f2py]\r\n\r\nCo-authored-by: rgommers <rgommers@users.noreply.github.com>\r\n\r\n* ENH: Support build_type via --debug [f2py-meson]\r\n\r\n* MAINT,DOC: Reduce warn,rework doc [f2py-meson]\r\n\r\nCo-authored-by: rgommers <rgommers@users.noreply.github.com>\r\n\r\n* ENH: Rework deps: to --dep calls [f2py-meson]\r\n\r\nAlso shows how incremental updates to the parser can be done.\r\n\r\n* MAINT,DOC: Add --backend to argparse, add docs\r\n\r\n* MAINT: Rename meson template [f2py-meson]\r\n\r\n* MAINT: Add meson.build for f2py\r\n\r\nShould address https://github.com/numpy/numpy/pull/22225#issuecomment-1697208937\r\n\r\n* BLD: remove duplicate f2py handling in meson.build files\r\n\r\n---------\r\n\r\nCo-authored-by: Namami Shanker <namami2011@gmail.com>\r\nCo-authored-by: NamamiShanker <NamamiShanker@users.noreply.github.com>\r\nCo-authored-by: rgommers <rgommers@users.noreply.github.com>\r\nCo-authored-by: Ralf Gommers <ralf.gommers@gmail.com>",
            "additions": 157,
            "deletions": 0,
            "change_type": "ADD",
            "diff": "@@ -0,0 +1,157 @@\n+from __future__ import annotations\n+\n+import errno\n+import shutil\n+import subprocess\n+from pathlib import Path\n+\n+from ._backend import Backend\n+from string import Template\n+\n+import warnings\n+\n+\n+class MesonTemplate:\n+    \"\"\"Template meson build file generation class.\"\"\"\n+\n+    def __init__(\n+        self,\n+        modulename: str,\n+        sources: list[Path],\n+        deps: list[str],\n+        object_files: list[Path],\n+        linker_args: list[str],\n+        c_args: list[str],\n+        build_type: str,\n+    ):\n+        self.modulename = modulename\n+        self.build_template_path = (\n+            Path(__file__).parent.absolute() / \"meson.build.template\"\n+        )\n+        self.sources = sources\n+        self.deps = deps\n+        self.substitutions = {}\n+        self.objects = object_files\n+        self.pipeline = [\n+            self.initialize_template,\n+            self.sources_substitution,\n+            self.deps_substitution,\n+        ]\n+        self.build_type = build_type\n+\n+    def meson_build_template(self) -> str:\n+        if not self.build_template_path.is_file():\n+            raise FileNotFoundError(\n+                errno.ENOENT,\n+                \"Meson build template\"\n+                f\" {self.build_template_path.absolute()}\"\n+                \" does not exist.\",\n+            )\n+        return self.build_template_path.read_text()\n+\n+    def initialize_template(self) -> None:\n+        self.substitutions[\"modulename\"] = self.modulename\n+        self.substitutions[\"buildtype\"] = self.build_type\n+\n+    def sources_substitution(self) -> None:\n+        indent = \" \" * 21\n+        self.substitutions[\"source_list\"] = f\",\\n{indent}\".join(\n+            [f\"'{source}'\" for source in self.sources]\n+        )\n+\n+    def deps_substitution(self) -> None:\n+        indent = \" \" * 21\n+        self.substitutions[\"dep_list\"] = f\",\\n{indent}\".join(\n+            [f\"dependency('{dep}')\" for dep in self.deps]\n+        )\n+\n+    def generate_meson_build(self):\n+        for node in self.pipeline:\n+            node()\n+        template = Template(self.meson_build_template())\n+        return template.substitute(self.substitutions)\n+\n+\n+class MesonBackend(Backend):\n+    def __init__(self, *args, **kwargs):\n+        super().__init__(*args, **kwargs)\n+        self.dependencies = self.extra_dat.get(\"dependencies\", [])\n+        self.meson_build_dir = \"bbdir\"\n+        self.build_type = (\n+            \"debug\" if any(\"debug\" in flag for flag in self.fc_flags) else \"release\"\n+        )\n+\n+    def _move_exec_to_root(self, build_dir: Path):\n+        walk_dir = Path(build_dir) / self.meson_build_dir\n+        path_objects = walk_dir.glob(f\"{self.modulename}*.so\")\n+        for path_object in path_objects:\n+            shutil.move(path_object, Path.cwd())\n+\n+    def _get_build_command(self):\n+        return [\n+            \"meson\",\n+            \"setup\",\n+            self.meson_build_dir,\n+        ]\n+\n+    def write_meson_build(self, build_dir: Path) -> None:\n+        \"\"\"Writes the meson build file at specified location\"\"\"\n+        meson_template = MesonTemplate(\n+            self.modulename,\n+            self.sources,\n+            self.dependencies,\n+            self.extra_objects,\n+            self.flib_flags,\n+            self.fc_flags,\n+            self.build_type,\n+        )\n+        src = meson_template.generate_meson_build()\n+        Path(build_dir).mkdir(parents=True, exist_ok=True)\n+        meson_build_file = Path(build_dir) / \"meson.build\"\n+        meson_build_file.write_text(src)\n+        return meson_build_file\n+\n+    def run_meson(self, build_dir: Path):\n+        completed_process = subprocess.run(self._get_build_command(), cwd=build_dir)\n+        if completed_process.returncode != 0:\n+            raise subprocess.CalledProcessError(\n+                completed_process.returncode, completed_process.args\n+            )\n+        completed_process = subprocess.run(\n+            [\"meson\", \"compile\", \"-C\", self.meson_build_dir], cwd=build_dir\n+        )\n+        if completed_process.returncode != 0:\n+            raise subprocess.CalledProcessError(\n+                completed_process.returncode, completed_process.args\n+            )\n+\n+    def compile(self) -> None:\n+        self.sources = _prepare_sources(self.modulename, self.sources, self.build_dir)\n+        self.write_meson_build(self.build_dir)\n+        self.run_meson(self.build_dir)\n+        self._move_exec_to_root(self.build_dir)\n+\n+\n+def _prepare_sources(mname, sources, bdir):\n+    extended_sources = sources.copy()\n+    Path(bdir).mkdir(parents=True, exist_ok=True)\n+    # Copy sources\n+    for source in sources:\n+        shutil.copy(source, bdir)\n+    generated_sources = [\n+        Path(f\"{mname}module.c\"),\n+        Path(f\"{mname}-f2pywrappers2.f90\"),\n+        Path(f\"{mname}-f2pywrappers.f\"),\n+    ]\n+    bdir = Path(bdir)\n+    for generated_source in generated_sources:\n+        if generated_source.exists():\n+            shutil.copy(generated_source, bdir / generated_source.name)\n+            extended_sources.append(generated_source.name)\n+            generated_source.unlink()\n+    extended_sources = [\n+        Path(source).name\n+        for source in extended_sources\n+        if not Path(source).suffix == \".pyf\"\n+    ]\n+    return extended_sources\n",
            "comment_added_diff": {
                "138": "    # Copy sources"
            },
            "comment_deleted_diff": {},
            "comment_modified_diff": {}
        }
    ],
    "meson.build.template": [],
    "f2py2e.py": [
        {
            "commit": "fedc834fecb7e1aca2917ad2e0fd1694400529e0",
            "timestamp": "2023-09-05T10:42:35-06:00",
            "author": "Rohit Goswami",
            "commit_message": "ENH: ``meson`` backend for ``f2py`` (#24532)\n\n* FIX: Import f2py2e rather than f2py for run_main\r\n\r\n* FIX: Import f2py2e instead of f2py\r\n\r\n* ENH: Add F2PY back-end work from gh-22225\r\n\r\nCo-authored-by: NamamiShanker <NamamiShanker@users.noreply.github.com>\r\n\r\n* ENH: Add meson skeleton from gh-2225\r\n\r\nCo-authored-by: NamamiShanker <NamamiShanker@users.noreply.github.com>\r\n\r\n* MAINT: Trim backend.py down to f2py2e flags\r\n\r\n* ENH: Add a factory function for backends\r\n\r\n* ENH: Add a distutils backend\r\n\r\n* ENH: Handle --backends in f2py\r\n\r\nDefaults to distutils for now\r\n\r\n* DOC: Add some minor comments in f2py2e\r\n\r\n* MAINT: Refactor and rework meson.build.src\r\n\r\n* MAINT: Add objects\r\n\r\n* MAINT: Cleanup distutils backend\r\n\r\n* MAINT: Refactor to add everything back to backend\r\n\r\nNecessary for the meson.build for now. Refactors / cleanup needs better\r\nargument handling in f2py2e\r\n\r\n* MAINT: Fix overly long line\r\n\r\n* BUG: Construct wrappers for meson backend\r\n\r\n* MAINT: Rework, simplify template massively\r\n\r\n* ENH: Truncate meson.build to skeleton only\r\n\r\n* MAINT: Minor backend housekeeping, name changes\r\n\r\n* MAINT: Less absolute paths, update setup.py [f2py]\r\n\r\n* MAINT: Move f2py module name functionality\r\n\r\nPreviously part of np.distutils\r\n\r\n* ENH: Handle .pyf files\r\n\r\n* TST: Fix typo in isoFortranEnvMap.f90\r\n\r\n* MAINT: Typo in f2py2e support for pyf files\r\n\r\n* DOC: Add release note for --backend\r\n\r\n* MAINT: Conditional switch for Python 3.12 [f2py]\r\n\r\n* MAINT: No absolute paths in backend [f2py-meson]\r\n\r\nThe files are copied over anyway, this makes it easier to extend the\r\ngenerated skeleton\r\n\r\n* MAINT: Prettier generated meson.build files [f2py]\r\n\r\n* ENH: Add meson's dependency(blah) to f2py\r\n\r\n* DOC: Document the new flag\r\n\r\n* MAINT: Simplify and rename backend template [f2py]\r\n\r\nCo-authored-by: rgommers <rgommers@users.noreply.github.com>\r\n\r\n* ENH: Support build_type via --debug [f2py-meson]\r\n\r\n* MAINT,DOC: Reduce warn,rework doc [f2py-meson]\r\n\r\nCo-authored-by: rgommers <rgommers@users.noreply.github.com>\r\n\r\n* ENH: Rework deps: to --dep calls [f2py-meson]\r\n\r\nAlso shows how incremental updates to the parser can be done.\r\n\r\n* MAINT,DOC: Add --backend to argparse, add docs\r\n\r\n* MAINT: Rename meson template [f2py-meson]\r\n\r\n* MAINT: Add meson.build for f2py\r\n\r\nShould address https://github.com/numpy/numpy/pull/22225#issuecomment-1697208937\r\n\r\n* BLD: remove duplicate f2py handling in meson.build files\r\n\r\n---------\r\n\r\nCo-authored-by: Namami Shanker <namami2011@gmail.com>\r\nCo-authored-by: NamamiShanker <NamamiShanker@users.noreply.github.com>\r\nCo-authored-by: rgommers <rgommers@users.noreply.github.com>\r\nCo-authored-by: Ralf Gommers <ralf.gommers@gmail.com>",
            "additions": 82,
            "deletions": 52,
            "change_type": "MODIFY",
            "diff": "@@ -19,6 +19,8 @@\n import pprint\n import re\n from pathlib import Path\n+from itertools import dropwhile\n+import argparse\n \n from . import crackfortran\n from . import rules\n@@ -28,6 +30,7 @@\n from . import f90mod_rules\n from . import __version__\n from . import capi_maps\n+from numpy.f2py._backends import f2py_build_generator\n \n f2py_version = __version__.version\n numpy_version = __version__.version\n@@ -126,7 +129,7 @@\n   -v               Print f2py version ID and exit.\n \n \n-numpy.distutils options (only effective with -c):\n+build backend options (only effective with -c):\n \n   --fcompiler=         Specify Fortran compiler type by vendor\n   --compiler=          Specify C compiler type (as defined by distutils)\n@@ -142,6 +145,22 @@\n   --noarch             Compile without arch-dependent optimization\n   --debug              Compile with debugging information\n \n+  --dep                <dependency>\n+                       Specify a meson dependency for the module. This may\n+                       be passed multiple times for multiple dependencies.\n+                       Dependencies are stored in a list for further processing.\n+\n+                       Example: --dep lapack --dep scalapack\n+                       This will identify \"lapack\" and \"scalapack\" as dependencies\n+                       and remove them from argv, leaving a dependencies list\n+                       containing [\"lapack\", \"scalapack\"].\n+\n+  --backend            <backend_type>\n+                       Specify the build backend for the compilation process.\n+                       The supported backends are 'meson' and 'distutils'.\n+                       If not specified, defaults to 'distutils'. On\n+                       Python 3.12 or higher, the default is 'meson'.\n+\n Extra options (only effective with -c):\n \n   --link-<resource>    Link extension module with <resource> as defined\n@@ -251,6 +270,8 @@ def scaninputline(inputline):\n                 'f2py option --include_paths is deprecated, use --include-paths instead.\\n')\n             f7 = 1\n         elif l[:15] in '--include-paths':\n+            # Similar to using -I with -c, however this is\n+            # also used during generation of wrappers\n             f7 = 1\n         elif l == '--skip-empty-wrappers':\n             emptygen = False\n@@ -501,6 +522,25 @@ def get_prefix(module):\n     p = os.path.dirname(os.path.dirname(module.__file__))\n     return p\n \n+def preparse_sysargv():\n+    # To keep backwards bug compatibility, newer flags are handled by argparse,\n+    # and `sys.argv` is passed to the rest of `f2py` as is.\n+    parser = argparse.ArgumentParser(add_help=False)\n+    parser.add_argument(\"--dep\", action=\"append\", dest=\"dependencies\")\n+    parser.add_argument(\"--backend\", choices=['meson', 'distutils'], default='distutils')\n+\n+    args, remaining_argv = parser.parse_known_args()\n+    sys.argv = [sys.argv[0]] + remaining_argv\n+\n+    backend_key = args.backend\n+    if sys.version_info >= (3, 12) and backend_key == 'distutils':\n+        outmess('Cannot use distutils backend with Python 3.12, using meson backend instead.')\n+        backend_key = 'meson'\n+\n+    return {\n+        \"dependencies\": args.dependencies or [],\n+        \"backend\": backend_key\n+    }\n \n def run_compile():\n     \"\"\"\n@@ -508,6 +548,13 @@ def run_compile():\n     \"\"\"\n     import tempfile\n \n+    # Collect dependency flags, preprocess sys.argv\n+    argy = preparse_sysargv()\n+    dependencies = argy[\"dependencies\"]\n+    backend_key = argy[\"backend\"]\n+    build_backend = f2py_build_generator(backend_key)\n+\n+\n     i = sys.argv.index('-c')\n     del sys.argv[i]\n \n@@ -546,7 +593,6 @@ def run_compile():\n     if f2py_flags2 and f2py_flags2[-1] != ':':\n         f2py_flags2.append(':')\n     f2py_flags.extend(f2py_flags2)\n-\n     sys.argv = [_m for _m in sys.argv if _m not in f2py_flags2]\n     _reg3 = re.compile(\n         r'--((f(90)?compiler(-exec|)|compiler)=|help-compiler)')\n@@ -598,17 +644,17 @@ def run_compile():\n             del sys.argv[i + 1], sys.argv[i]\n             sources = sys.argv[1:]\n \n+    pyf_files = []\n     if '-m' in sys.argv:\n         i = sys.argv.index('-m')\n         modulename = sys.argv[i + 1]\n         del sys.argv[i + 1], sys.argv[i]\n         sources = sys.argv[1:]\n     else:\n-        from numpy.distutils.command.build_src import get_f2py_modulename\n-        pyf_files, sources = filter_files('', '[.]pyf([.]src|)', sources)\n-        sources = pyf_files + sources\n+        pyf_files, _sources = filter_files('', '[.]pyf([.]src|)', sources)\n+        sources = pyf_files + _sources\n         for f in pyf_files:\n-            modulename = get_f2py_modulename(f)\n+            modulename = auxfuncs.get_f2py_modulename(f)\n             if modulename:\n                 break\n \n@@ -627,52 +673,36 @@ def run_compile():\n         else:\n             print('Invalid use of -D:', name_value)\n \n-    from numpy.distutils.system_info import get_info\n-\n-    num_info = {}\n-    if num_info:\n-        include_dirs.extend(num_info.get('include_dirs', []))\n-\n-    from numpy.distutils.core import setup, Extension\n-    ext_args = {'name': modulename, 'sources': sources,\n-                'include_dirs': include_dirs,\n-                'library_dirs': library_dirs,\n-                'libraries': libraries,\n-                'define_macros': define_macros,\n-                'undef_macros': undef_macros,\n-                'extra_objects': extra_objects,\n-                'f2py_options': f2py_flags,\n-                }\n-\n-    if sysinfo_flags:\n-        from numpy.distutils.misc_util import dict_append\n-        for n in sysinfo_flags:\n-            i = get_info(n)\n-            if not i:\n-                outmess('No %s resources found in system'\n-                        ' (try `f2py --help-link`)\\n' % (repr(n)))\n-            dict_append(ext_args, **i)\n-\n-    ext = Extension(**ext_args)\n-    sys.argv = [sys.argv[0]] + setup_flags\n-    sys.argv.extend(['build',\n-                     '--build-temp', build_dir,\n-                     '--build-base', build_dir,\n-                     '--build-platlib', '.',\n-                     # disable CCompilerOpt\n-                     '--disable-optimization'])\n-    if fc_flags:\n-        sys.argv.extend(['config_fc'] + fc_flags)\n-    if flib_flags:\n-        sys.argv.extend(['build_ext'] + flib_flags)\n-\n-    setup(ext_modules=[ext])\n-\n-    if remove_build_dir and os.path.exists(build_dir):\n-        import shutil\n-        outmess('Removing build directory %s\\n' % (build_dir))\n-        shutil.rmtree(build_dir)\n-\n+    # Construct wrappers / signatures / things\n+    if backend_key == 'meson':\n+        outmess('Using meson backend\\nWill pass --lower to f2py\\nSee https://numpy.org/doc/stable/f2py/buildtools/meson.html')\n+        f2py_flags.append('--lower')\n+        if pyf_files:\n+            run_main(f\" {' '.join(f2py_flags)} {' '.join(pyf_files)}\".split())\n+        else:\n+            run_main(f\" {' '.join(f2py_flags)} -m {modulename} {' '.join(sources)}\".split())\n+\n+    # Now use the builder\n+    builder = build_backend(\n+        modulename,\n+        sources,\n+        extra_objects,\n+        build_dir,\n+        include_dirs,\n+        library_dirs,\n+        libraries,\n+        define_macros,\n+        undef_macros,\n+        f2py_flags,\n+        sysinfo_flags,\n+        fc_flags,\n+        flib_flags,\n+        setup_flags,\n+        remove_build_dir,\n+        {\"dependencies\": dependencies},\n+    )\n+\n+    builder.compile()\n \n def main():\n     if '--help-link' in sys.argv[1:]:\n",
            "comment_added_diff": {
                "273": "            # Similar to using -I with -c, however this is",
                "274": "            # also used during generation of wrappers",
                "526": "    # To keep backwards bug compatibility, newer flags are handled by argparse,",
                "527": "    # and `sys.argv` is passed to the rest of `f2py` as is.",
                "551": "    # Collect dependency flags, preprocess sys.argv",
                "676": "    # Construct wrappers / signatures / things",
                "685": "    # Now use the builder"
            },
            "comment_deleted_diff": {
                "662": "                     # disable CCompilerOpt"
            },
            "comment_modified_diff": {}
        }
    ],
    "isoFortranEnvMap.f90": [],
    "24420.new_feature.rst": [],
    "build-options.rst": [],
    "main_config.h.in": [],
    "npy_cpu_dispatch.c": [],
    "npy_cpu_dispatch_distutils.h": [],
    "_umath_tests.dispatch.c": [],
    "introspect.py": [],
    "24445.deprecation.rst": [],
    "routines.ma.rst": [],
    "quickstart.rst": [],
    "test_interaction.py": [],
    "arraysetops.pyi": [],
    "array_utils.py": [
        {
            "commit": "52db499e7130239da0a66812b4970c4a92af3e35",
            "timestamp": "2023-09-06T10:20:42+02:00",
            "author": "Mateusz Sok\u00f3\u0142",
            "commit_message": "API: Add lib.array_utils namespace",
            "additions": 57,
            "deletions": 0,
            "change_type": "ADD",
            "diff": "@@ -0,0 +1,57 @@\n+from numpy.core import asarray\n+from numpy.core.numeric import normalize_axis_tuple, normalize_axis_index\n+\n+\n+__all__ = [\"byte_bounds\", \"normalize_axis_tuple\", \"normalize_axis_index\"]\n+\n+\n+def byte_bounds(a):\n+    \"\"\"\n+    Returns pointers to the end-points of an array.\n+\n+    Parameters\n+    ----------\n+    a : ndarray\n+        Input array. It must conform to the Python-side of the array\n+        interface.\n+\n+    Returns\n+    -------\n+    (low, high) : tuple of 2 integers\n+        The first integer is the first byte of the array, the second\n+        integer is just past the last byte of the array.  If `a` is not\n+        contiguous it will not use every byte between the (`low`, `high`)\n+        values.\n+\n+    Examples\n+    --------\n+    >>> I = np.eye(2, dtype='f'); I.dtype\n+    dtype('float32')\n+    >>> low, high = np.byte_bounds(I)\n+    >>> high - low == I.size*I.itemsize\n+    True\n+    >>> I = np.eye(2); I.dtype\n+    dtype('float64')\n+    >>> low, high = np.byte_bounds(I)\n+    >>> high - low == I.size*I.itemsize\n+    True\n+\n+    \"\"\"\n+    ai = a.__array_interface__\n+    a_data = ai['data'][0]\n+    astrides = ai['strides']\n+    ashape = ai['shape']\n+    bytes_a = asarray(a).dtype.itemsize\n+\n+    a_low = a_high = a_data\n+    if astrides is None:\n+        # contiguous case\n+        a_high += a.size * bytes_a\n+    else:\n+        for shape, stride in zip(ashape, astrides):\n+            if stride < 0:\n+                a_low += (shape-1)*stride\n+            else:\n+                a_high += (shape-1)*stride\n+        a_high += bytes_a\n+    return a_low, a_high\n",
            "comment_added_diff": {
                "48": "        # contiguous case"
            },
            "comment_deleted_diff": {},
            "comment_modified_diff": {}
        },
        {
            "commit": "f764f89a5cd24f81a6be9543db56f0c5e3fc0f5c",
            "timestamp": "2023-09-06T10:22:37+02:00",
            "author": "Mateusz Sok\u00f3\u0142",
            "commit_message": "Add *_impl file",
            "additions": 6,
            "deletions": 57,
            "change_type": "MODIFY",
            "diff": "@@ -1,57 +1,6 @@\n-from numpy.core import asarray\n-from numpy.core.numeric import normalize_axis_tuple, normalize_axis_index\n-\n-\n-__all__ = [\"byte_bounds\", \"normalize_axis_tuple\", \"normalize_axis_index\"]\n-\n-\n-def byte_bounds(a):\n-    \"\"\"\n-    Returns pointers to the end-points of an array.\n-\n-    Parameters\n-    ----------\n-    a : ndarray\n-        Input array. It must conform to the Python-side of the array\n-        interface.\n-\n-    Returns\n-    -------\n-    (low, high) : tuple of 2 integers\n-        The first integer is the first byte of the array, the second\n-        integer is just past the last byte of the array.  If `a` is not\n-        contiguous it will not use every byte between the (`low`, `high`)\n-        values.\n-\n-    Examples\n-    --------\n-    >>> I = np.eye(2, dtype='f'); I.dtype\n-    dtype('float32')\n-    >>> low, high = np.byte_bounds(I)\n-    >>> high - low == I.size*I.itemsize\n-    True\n-    >>> I = np.eye(2); I.dtype\n-    dtype('float64')\n-    >>> low, high = np.byte_bounds(I)\n-    >>> high - low == I.size*I.itemsize\n-    True\n-\n-    \"\"\"\n-    ai = a.__array_interface__\n-    a_data = ai['data'][0]\n-    astrides = ai['strides']\n-    ashape = ai['shape']\n-    bytes_a = asarray(a).dtype.itemsize\n-\n-    a_low = a_high = a_data\n-    if astrides is None:\n-        # contiguous case\n-        a_high += a.size * bytes_a\n-    else:\n-        for shape, stride in zip(ashape, astrides):\n-            if stride < 0:\n-                a_low += (shape-1)*stride\n-            else:\n-                a_high += (shape-1)*stride\n-        a_high += bytes_a\n-    return a_low, a_high\n+from ._array_utils_impl import (\n+    __all__,\n+    byte_bounds,\n+    normalize_axis_index,\n+    normalize_axis_tuple,\n+)\n",
            "comment_added_diff": {},
            "comment_deleted_diff": {
                "48": "        # contiguous case"
            },
            "comment_modified_diff": {}
        }
    ],
    "array_utils.pyi": [],
    "test_array_utils.py": [
        {
            "commit": "52db499e7130239da0a66812b4970c4a92af3e35",
            "timestamp": "2023-09-06T10:20:42+02:00",
            "author": "Mateusz Sok\u00f3\u0142",
            "commit_message": "API: Add lib.array_utils namespace",
            "additions": 33,
            "deletions": 0,
            "change_type": "ADD",
            "diff": "@@ -0,0 +1,33 @@\n+import numpy as np\n+\n+from numpy.lib import array_utils\n+from numpy.testing import assert_equal\n+\n+\n+class TestByteBounds:\n+    def test_byte_bounds(self):\n+        # pointer difference matches size * itemsize\n+        # due to contiguity\n+        a = np.arange(12).reshape(3, 4)\n+        low, high = array_utils.byte_bounds(a)\n+        assert_equal(high - low, a.size * a.itemsize)\n+\n+    def test_unusual_order_positive_stride(self):\n+        a = np.arange(12).reshape(3, 4)\n+        b = a.T\n+        low, high = array_utils.byte_bounds(b)\n+        assert_equal(high - low, b.size * b.itemsize)\n+\n+    def test_unusual_order_negative_stride(self):\n+        a = np.arange(12).reshape(3, 4)\n+        b = a.T[::-1]\n+        low, high = array_utils.byte_bounds(b)\n+        assert_equal(high - low, b.size * b.itemsize)\n+\n+    def test_strided(self):\n+        a = np.arange(12)\n+        b = a[::2]\n+        low, high = array_utils.byte_bounds(b)\n+        # the largest pointer address is lost (even numbers only in the\n+        # stride), and compensate addresses for striding by 2\n+        assert_equal(high - low, b.size * 2 * b.itemsize - b.itemsize)\n",
            "comment_added_diff": {
                "9": "        # pointer difference matches size * itemsize",
                "10": "        # due to contiguity",
                "31": "        # the largest pointer address is lost (even numbers only in the",
                "32": "        # stride), and compensate addresses for striding by 2"
            },
            "comment_deleted_diff": {},
            "comment_modified_diff": {}
        }
    ],
    "_array_utils_impl.py": [
        {
            "commit": "f764f89a5cd24f81a6be9543db56f0c5e3fc0f5c",
            "timestamp": "2023-09-06T10:22:37+02:00",
            "author": "Mateusz Sok\u00f3\u0142",
            "commit_message": "Add *_impl file",
            "additions": 57,
            "deletions": 0,
            "change_type": "ADD",
            "diff": "@@ -0,0 +1,57 @@\n+from numpy.core import asarray\n+from numpy.core.numeric import normalize_axis_tuple, normalize_axis_index\n+\n+\n+__all__ = [\"byte_bounds\", \"normalize_axis_tuple\", \"normalize_axis_index\"]\n+\n+\n+def byte_bounds(a):\n+    \"\"\"\n+    Returns pointers to the end-points of an array.\n+\n+    Parameters\n+    ----------\n+    a : ndarray\n+        Input array. It must conform to the Python-side of the array\n+        interface.\n+\n+    Returns\n+    -------\n+    (low, high) : tuple of 2 integers\n+        The first integer is the first byte of the array, the second\n+        integer is just past the last byte of the array.  If `a` is not\n+        contiguous it will not use every byte between the (`low`, `high`)\n+        values.\n+\n+    Examples\n+    --------\n+    >>> I = np.eye(2, dtype='f'); I.dtype\n+    dtype('float32')\n+    >>> low, high = np.byte_bounds(I)\n+    >>> high - low == I.size*I.itemsize\n+    True\n+    >>> I = np.eye(2); I.dtype\n+    dtype('float64')\n+    >>> low, high = np.byte_bounds(I)\n+    >>> high - low == I.size*I.itemsize\n+    True\n+\n+    \"\"\"\n+    ai = a.__array_interface__\n+    a_data = ai['data'][0]\n+    astrides = ai['strides']\n+    ashape = ai['shape']\n+    bytes_a = asarray(a).dtype.itemsize\n+\n+    a_low = a_high = a_data\n+    if astrides is None:\n+        # contiguous case\n+        a_high += a.size * bytes_a\n+    else:\n+        for shape, stride in zip(ashape, astrides):\n+            if stride < 0:\n+                a_low += (shape-1)*stride\n+            else:\n+                a_high += (shape-1)*stride\n+        a_high += bytes_a\n+    return a_low, a_high\n",
            "comment_added_diff": {
                "48": "        # contiguous case"
            },
            "comment_deleted_diff": {},
            "comment_modified_diff": {}
        }
    ],
    "_array_utils_impl.pyi": [],
    "1.20.0-notes.rst": [],
    "1.20.2-notes.rst": [],
    "1.7.0-notes.rst": [],
    "basics.indexing.rst": [],
    "24680.new_feature.rst": [],
    "byteswapping.rst": [],
    "ndarray_conversion.py": [
        {
            "commit": "65bb8ddd03763b8a931de2cd472d8ce8bd2f9fbd",
            "timestamp": "2023-09-11T18:51:42+02:00",
            "author": "Mateusz Sok\u00f3\u0142",
            "commit_message": "API: Remove ptp, setitem and newbyteorder from np.ndarray class",
            "additions": 0,
            "deletions": 7,
            "change_type": "MODIFY",
            "diff": "@@ -12,13 +12,6 @@\n nd.item(0, 1)\n nd.item((0, 1))\n \n-# tolist is pretty simple\n-\n-# itemset\n-scalar_array.itemset(3)\n-nd.itemset(3, 0)\n-nd.itemset((0, 0), 3)\n-\n # tobytes\n nd.tobytes()\n nd.tobytes(\"C\")\n",
            "comment_added_diff": {},
            "comment_deleted_diff": {
                "15": "# tolist is pretty simple",
                "17": "# itemset"
            },
            "comment_modified_diff": {}
        }
    ],
    "test_indexerrors.py": [],
    "numpy_2_0_migration_guide.rst": [],
    "action.yml": [],
    "feature_detection_cmath.h": [],
    "feature_detection_math.h": [],
    "setup.cfg": [],
    "_looper.py": [],
    "compat3.py": [],
    "license.txt": [],
    "travis-before-install.sh": [],
    "travis-sorter.py": [
        {
            "commit": "63a1fee8d1c1e442aa8ca08ac7cefb195f99b2a9",
            "timestamp": "2023-09-13T22:52:14+02:00",
            "author": "Ralf Gommers",
            "commit_message": "MAINT: remove all `setup.py`'s and related files for distutils builds",
            "additions": 0,
            "deletions": 287,
            "change_type": "DELETE",
            "diff": "@@ -1,287 +0,0 @@\n-#!/usr/bin/env python3\n-\"\"\"\n-Run with a repo/build number or list of Travis CI build times to show the optimal build\n-order to run faster and make full use of all available parallel build jobs.\n-\n-Requires the Travis Client CLI\n-\n-https://github.com/travis-ci/travis.rb#installation\n-\n-# Example\n-\n-$ # Check build 22 of hugovk/numpy, and skip the first job (it's a single stage)\n-$ travis-sorter.py hugovk/numpy 22 --skip 1\n-travis show -r hugovk/numpy 22\n-[8, 7, 8, 10, 9, 18, 8, 11, 8, 10, 8, 8, 17, 8, 26]\n-[7, 8, 10, 9, 18, 8, 11, 8, 10, 8, 8, 17, 8, 26]\n-Before:\n-\n-ID Duration in mins\n- 1 *******\n- 2 ********\n- 3 **********\n- 4 *********\n- 5 ******************\n- 6        ********\n- 7         ***********\n- 8          ********\n- 9           **********\n-10                ********\n-11                  ********\n-12                   *****************\n-13                    ********\n-14                     **************************\n-End: 46\n-   ----------------------------------------------\n-\n-After:\n-\n-ID Duration in mins\n-14 **************************\n- 5 ******************\n-12 *****************\n- 7 ***********\n- 3 **********\n- 9           **********\n- 4            *********\n- 2                  ********\n- 6                   ********\n- 8                     ********\n-10                     ********\n-11                          ********\n-13                           ********\n- 1                           *******\n-End: 34\n-   ----------------------------------\n-\n-# Example\n-\n-$ python travis-sorter.py 4 4 4 4 4 12 19\n-\n-Before:\n-\n-****\n-****\n-****\n-****\n-****\n-    ************\n-    *******************\n-12345678901234567890123 = 23 minutes\n-\n-After:\n-\n-*******************\n-************\n-****\n-****\n-****\n-    ****\n-    ****\n-1234567890123456789 = 19 minutes\n-\"\"\"\n-import argparse\n-import re\n-import subprocess\n-import sys\n-\n-count = 1\n-\n-\n-def summarise(jobs):\n-    end = 0\n-    print(\"ID Duration in mins\")\n-    for job in jobs:\n-        before = \" \" * job.started\n-        active = \"*\" * job.length\n-        print(\"{:2d} {}{}\".format(job.id, before, active))\n-        if job.started + job.length > end:\n-            end = job.started + job.length\n-    # for job in jobs:\n-    #     print(job)\n-    print(\"End:\", end)\n-    print(\"   \" + \"-\" * end)\n-\n-\n-class Job:\n-    def __init__(self, length):\n-        global count\n-        self.id = count\n-        count += 1\n-        self.length = length\n-        self.started = -1\n-        self.status = \"not started\"\n-        self.ended = False\n-\n-    def __str__(self):\n-        return \"{}\\tLength: {}\\tStarted: {}\\tEnded: {}\".format(\n-            self.id, self.length, self.started, self.ended\n-        )\n-\n-\n-def count_status(jobs, status):\n-    number = 0\n-    for job in jobs:\n-        if job.status == status:\n-            number += 1\n-    return number\n-\n-\n-def simulate(jobs, limit):\n-\n-    time = 0\n-\n-    # summarise(jobs)\n-\n-    while True:\n-        # Check if any have ended\n-        for job in jobs:\n-            if job.status == \"active\":\n-                if time >= job.started + job.length:\n-                    # print(\"{}/{} Finished:\".format(count_status(jobs, \"active\"), limit))\n-                    job.ended = time\n-                    job.status = \"finished\"\n-                    # print(job)\n-\n-        # Check if any can start\n-        for job in jobs:\n-            if job.status == \"not started\":\n-                if count_status(jobs, \"active\") < limit:\n-                    # print(\"{}/{} Starting:\".format(count_status(jobs, \"active\"), limit))\n-                    job.started = time\n-                    job.status = \"active\"\n-                    # print(job)\n-\n-        time += 1\n-\n-        # Exit loop?\n-        if count_status(jobs, \"finished\") == len(jobs):\n-            break\n-\n-    summarise(jobs)\n-\n-\n-def do_thing(repo, number):\n-    cmd = f\"travis show -r {repo} {number or ''}\"\n-    # cmd = f\"travis show --com -r {repo} {number or ''}\"\n-    print(cmd)\n-\n-    exitcode = 0\n-    # For offline testing\n-    output = \"\"\"Build #4:  Upgrade Python syntax with pyupgrade https://github.com/asottile/pyupgrade\n-State:         passed\n-Type:          push\n-Branch:        add-3.7\n-Compare URL:   https://github.com/hugovk/diff-cover/compare/4ae7cf97c6fa...7eeddb300175\n-Duration:      16 min 7 sec\n-Started:       2018-10-17 19:03:01\n-Finished:      2018-10-17 19:09:53\n-\n-#4.1 passed:     1 min          os: linux, env: TOXENV=py27, python: 2.7\n-#4.2 passed:     1 min 43 sec   os: linux, env: TOXENV=py34, python: 3.4\n-#4.3 passed:     1 min 52 sec   os: linux, env: TOXENV=py35, python: 3.5\n-#4.4 passed:     1 min 38 sec   os: linux, env: TOXENV=py36, python: 3.6\n-#4.5 passed:     1 min 47 sec   os: linux, env: TOXENV=py37, python: 3.7\n-#4.6 passed:     4 min 35 sec   os: linux, env: TOXENV=pypy, python: pypy\n-#4.7 passed:     3 min 17 sec   os: linux, env: TOXENV=pypy3, python: pypy3\"\"\"\n-\n-    # For offline testing\n-    output = \"\"\"Build #9:  :arrows_clockwise: [EngCom] Public Pull Requests - 2.3-develop\n-State:         errored\n-Type:          push\n-Branch:        2.3-develop\n-Compare URL:   https://github.com/hugovk/magento2/compare/80469a61e061...77af5d65ef4f\n-Duration:      4 hrs 12 min 13 sec\n-Started:       2018-10-27 17:50:51\n-Finished:      2018-10-27 18:54:14\n-\n-#9.1 passed:     3 min 30 sec   os: linux, env: TEST_SUITE=unit, php: 7.1\n-#9.2 passed:     3 min 35 sec   os: linux, env: TEST_SUITE=unit, php: 7.2\n-#9.3 passed:     3 min 41 sec   os: linux, env: TEST_SUITE=static, php: 7.2\n-#9.4 passed:     8 min 48 sec   os: linux, env: TEST_SUITE=js GRUNT_COMMAND=spec, php: 7.2\n-#9.5 passed:     3 min 24 sec   os: linux, env: TEST_SUITE=js GRUNT_COMMAND=static, php: 7.2\n-#9.6 errored:    50 min         os: linux, env: TEST_SUITE=integration INTEGRATION_INDEX=1, php: 7.1\n-#9.7 passed:     49 min 25 sec  os: linux, env: TEST_SUITE=integration INTEGRATION_INDEX=1, php: 7.2\n-#9.8 passed:     31 min 54 sec  os: linux, env: TEST_SUITE=integration INTEGRATION_INDEX=2, php: 7.1\n-#9.9 passed:     31 min 24 sec  os: linux, env: TEST_SUITE=integration INTEGRATION_INDEX=2, php: 7.2\n-#9.10 passed:    27 min 23 sec  os: linux, env: TEST_SUITE=integration INTEGRATION_INDEX=3, php: 7.1\n-#9.11 passed:    26 min 9 sec   os: linux, env: TEST_SUITE=integration INTEGRATION_INDEX=3, php: 7.2\n-#9.12 passed:    13 min         os: linux, env: TEST_SUITE=functional, php: 7.2\"\"\"\n-\n-    # Real use\n-    exitcode, output = subprocess.getstatusoutput(cmd)\n-\n-    # print(exitcode)\n-    # print(output)\n-    if exitcode != 0:\n-        print(output)\n-        sys.exit(exitcode)\n-\n-    minutes = []\n-    matches = re.findall(r\"(pass|fail|error)ed.* (\\d+) min (\\d+)? \", output)\n-    for match in matches:\n-        status, m, s = match\n-        s = 0 if s == \"\" else int(s)\n-        s += int(m) * 60\n-        minutes.append(round(s / 60))\n-\n-    # print(minutes)\n-    return minutes\n-\n-\n-if __name__ == \"__main__\":\n-    parser = argparse.ArgumentParser(\n-        description=\"Either give minutes for --jobs (3 5 3 2 5), \"\n-        \"or --repo slug (hugovk/test) and build --number (5)\",\n-        formatter_class=argparse.ArgumentDefaultsHelpFormatter,\n-    )\n-    parser.add_argument(\n-        \"input\",\n-        nargs=\"+\",\n-        help=\"Either: times for each build job (minutes), \"\n-        \"or an org/repo slug and optionally build number\",\n-    )\n-    parser.add_argument(\n-        \"-l\", \"--limit\", type=int, default=5, help=\"Concurrent jobs limit\"\n-    )\n-    parser.add_argument(\n-        \"-s\", \"--skip\", type=int, default=0, help=\"Skip X jobs at the start\"\n-    )\n-    args = parser.parse_args()\n-\n-    # If all ints\n-    try:\n-        for x in args.input:\n-            int(x)\n-        job_times = args.input\n-    except ValueError:\n-        try:\n-            number = args.input[1]\n-        except IndexError:\n-            number = None\n-        job_times = do_thing(args.input[0], number)\n-\n-    job_times = job_times[args.skip :]\n-    # print(job_times)\n-\n-    print(\"Before:\")\n-    print()\n-\n-    jobs = []\n-    for job_time in job_times:\n-        job = Job(job_time)\n-        jobs.append(job)\n-\n-    simulate(jobs, args.limit)\n-\n-    print()\n-    print(\"After:\")\n-    print()\n-\n-    # Sort with longest first\n-    jobs.sort(key=lambda job: job.length, reverse=True)\n-    # Reset status\n-    for job in jobs:\n-        job.status = \"not started\"\n-\n-    simulate(jobs, args.limit)\n",
            "comment_added_diff": {},
            "comment_deleted_diff": {
                "1": "#!/usr/bin/env python3",
                "8": "https://github.com/travis-ci/travis.rb#installation",
                "10": "# Example",
                "12": "$ # Check build 22 of hugovk/numpy, and skip the first job (it's a single stage)",
                "57": "# Example",
                "100": "    # for job in jobs:",
                "101": "    #     print(job)",
                "134": "    # summarise(jobs)",
                "137": "        # Check if any have ended",
                "141": "                    # print(\"{}/{} Finished:\".format(count_status(jobs, \"active\"), limit))",
                "144": "                    # print(job)",
                "146": "        # Check if any can start",
                "150": "                    # print(\"{}/{} Starting:\".format(count_status(jobs, \"active\"), limit))",
                "153": "                    # print(job)",
                "157": "        # Exit loop?",
                "166": "    # cmd = f\"travis show --com -r {repo} {number or ''}\"",
                "170": "    # For offline testing",
                "171": "    output = \"\"\"Build #4:  Upgrade Python syntax with pyupgrade https://github.com/asottile/pyupgrade",
                "180": "#4.1 passed:     1 min          os: linux, env: TOXENV=py27, python: 2.7",
                "181": "#4.2 passed:     1 min 43 sec   os: linux, env: TOXENV=py34, python: 3.4",
                "182": "#4.3 passed:     1 min 52 sec   os: linux, env: TOXENV=py35, python: 3.5",
                "183": "#4.4 passed:     1 min 38 sec   os: linux, env: TOXENV=py36, python: 3.6",
                "184": "#4.5 passed:     1 min 47 sec   os: linux, env: TOXENV=py37, python: 3.7",
                "185": "#4.6 passed:     4 min 35 sec   os: linux, env: TOXENV=pypy, python: pypy",
                "186": "#4.7 passed:     3 min 17 sec   os: linux, env: TOXENV=pypy3, python: pypy3\"\"\"",
                "188": "    # For offline testing",
                "189": "    output = \"\"\"Build #9:  :arrows_clockwise: [EngCom] Public Pull Requests - 2.3-develop",
                "198": "#9.1 passed:     3 min 30 sec   os: linux, env: TEST_SUITE=unit, php: 7.1",
                "199": "#9.2 passed:     3 min 35 sec   os: linux, env: TEST_SUITE=unit, php: 7.2",
                "200": "#9.3 passed:     3 min 41 sec   os: linux, env: TEST_SUITE=static, php: 7.2",
                "201": "#9.4 passed:     8 min 48 sec   os: linux, env: TEST_SUITE=js GRUNT_COMMAND=spec, php: 7.2",
                "202": "#9.5 passed:     3 min 24 sec   os: linux, env: TEST_SUITE=js GRUNT_COMMAND=static, php: 7.2",
                "203": "#9.6 errored:    50 min         os: linux, env: TEST_SUITE=integration INTEGRATION_INDEX=1, php: 7.1",
                "204": "#9.7 passed:     49 min 25 sec  os: linux, env: TEST_SUITE=integration INTEGRATION_INDEX=1, php: 7.2",
                "205": "#9.8 passed:     31 min 54 sec  os: linux, env: TEST_SUITE=integration INTEGRATION_INDEX=2, php: 7.1",
                "206": "#9.9 passed:     31 min 24 sec  os: linux, env: TEST_SUITE=integration INTEGRATION_INDEX=2, php: 7.2",
                "207": "#9.10 passed:    27 min 23 sec  os: linux, env: TEST_SUITE=integration INTEGRATION_INDEX=3, php: 7.1",
                "208": "#9.11 passed:    26 min 9 sec   os: linux, env: TEST_SUITE=integration INTEGRATION_INDEX=3, php: 7.2",
                "209": "#9.12 passed:    13 min         os: linux, env: TEST_SUITE=functional, php: 7.2\"\"\"",
                "211": "    # Real use",
                "214": "    # print(exitcode)",
                "215": "    # print(output)",
                "228": "    # print(minutes)",
                "252": "    # If all ints",
                "265": "    # print(job_times)",
                "281": "    # Sort with longest first",
                "283": "    # Reset status"
            },
            "comment_modified_diff": {}
        }
    ],
    "tox.ini": [],
    "linux.yml": [],
    "24717.change.rst": [],
    "RELEASE_WALKTHROUGH.rst": [],
    "defchararray.pyi": [],
    "records.pyi": [],
    "mrecords.py": [],
    "chararray.pyi": [],
    "rec.pyi": [],
    "24587.python_removal.rst": [],
    "char.pyi": [],
    "windows.yml": [],
    "cirrus_wheels.yml": [],
    "LICENSE_linux.txt": [],
    "LICENSE_osx.txt": [],
    "LICENSE_win32.txt": [],
    "24770.new_feature.rst": [],
    "test_cpu_dispatcher.py": [],
    "24775.new_feature.rst": [],
    "data_common.f": [],
    "development_environment.rst": [],
    "git_links.inc": [],
    "governance.rst": [],
    "howto-docs.rst": [],
    "howto_build_docs.rst": [],
    "internals.rst": [],
    "parallel.rst": [],
    "upgrading-pcg64.rst": [],
    "swig.interface-file.rst": [],
    "swig.testing.rst": [],
    "absolute_beginners.rst": [],
    "intdiv.h": [],
    "dragon4.c": [],
    "dragon4.h": [],
    "_mt19937.pyx": [],
    "_pcg64.pyx": [],
    "_sfc64.pyx": [],
    "mt19937-jump.h": [],
    "pcg64.c": [],
    "pcg64.h": [],
    "splitmix64.c": [],
    "HOWTO_C_COVERAGE.txt": [],
    "nep-0021-advanced-indexing.rst": [],
    "nep-0052-python-api-cleanup.rst": [],
    "dtype.rst": [],
    "1.22.0-notes.rst": [],
    "1.22.4-notes.rst": [],
    "1.25.2-notes.rst": [],
    "1.3.0-notes.rst": [],
    "basics.subclassing.rst": [],
    "data_with_comments.f": [],
    "array_assign_array.c": [],
    "how-to-how-to.rst": [],
    "data_multiplier.f": [],
    "19355.new_feature.rst": [],
    "ufuncs.pyi": [],
    "test_umath_accuracy.py": [
        {
            "commit": "08047d51e0771e2b42f51a75d2936bdd39a43338",
            "timestamp": "2023-09-27T23:31:27+00:00",
            "author": "ganesh-k13",
            "commit_message": "TST: Misc bitwise_count changes\n\n* Changed popcount -> bitwise_count\n* Added comment on why we remove certain function in float64 object\n  loops\n* Comment on when we skip bitwise_count",
            "additions": 2,
            "deletions": 0,
            "change_type": "MODIFY",
            "diff": "@@ -11,6 +11,8 @@\n UNARY_UFUNCS = [obj for obj in np.core.umath.__dict__.values() if\n         isinstance(obj, np.ufunc)]\n UNARY_OBJECT_UFUNCS = [uf for uf in UNARY_UFUNCS if \"O->O\" in uf.types]\n+\n+# Remove functions that do not support `floats`\n UNARY_OBJECT_UFUNCS.remove(getattr(np, 'invert'))\n UNARY_OBJECT_UFUNCS.remove(getattr(np, 'bitwise_count'))\n \n",
            "comment_added_diff": {
                "15": "# Remove functions that do not support `floats`"
            },
            "comment_deleted_diff": {},
            "comment_modified_diff": {}
        }
    ],
    "numpyos.c": [],
    "numpyos.h": [],
    "distutils.rst": [],
    "linux_blas.yml": [],
    "linux_compiler_sanitizers.yml": [],
    "_distributor_init.py": [],
    "24854.python_removal.rst": [],
    "test_return_complex.py": [
        {
            "commit": "01e33cded468ee764944476b59f0423a34b6a232",
            "timestamp": "2023-10-04T20:44:21+02:00",
            "author": "Mateusz Sok\u00f3\u0142",
            "commit_message": "MAINT: Remove 'a' dtype alias (#24854)",
            "additions": 1,
            "deletions": 1,
            "change_type": "MODIFY",
            "diff": "@@ -33,7 +33,7 @@ def check_function(self, t, tname):\n         assert abs(t(array([234 + 3j], \"F\")) - (234 + 3j)) <= err\n         assert abs(t(array([234], \"D\")) - 234.0) <= err\n \n-        # pytest.raises(TypeError, t, array([234], 'a1'))\n+        # pytest.raises(TypeError, t, array([234], 'S1'))\n         pytest.raises(TypeError, t, \"abc\")\n \n         pytest.raises(IndexError, t, [])\n",
            "comment_added_diff": {
                "36": "        # pytest.raises(TypeError, t, array([234], 'S1'))"
            },
            "comment_deleted_diff": {
                "36": "        # pytest.raises(TypeError, t, array([234], 'a1'))"
            },
            "comment_modified_diff": {
                "36": "        # pytest.raises(TypeError, t, array([234], 'a1'))"
            }
        }
    ],
    "replace_old_macros.sed": [],
    "typeinfo.c": [],
    "typeinfo.h": [],
    "test_array_from_pyobj.py": [
        {
            "commit": "7d169dbf8c7b323e7145137b238c4033b2318080",
            "timestamp": "2023-10-05T11:45:14+02:00",
            "author": "Mateusz Sok\u00f3\u0142",
            "commit_message": "MAINT: Refactor f2py/tests/test_array_from_pyobj.py",
            "additions": 100,
            "deletions": 66,
            "change_type": "MODIFY",
            "diff": "@@ -12,11 +12,8 @@\n \n wrap = None\n \n-# Extend core typeinfo with CHARACTER to test dtype('c')\n-_ti = _typeinfo['STRING']\n-typeinfo = dict(\n-    CHARACTER=type(_ti)(('c', _ti.num, 8, _ti.alignment, _ti.type)),\n-    **_typeinfo)\n+# Extend core typeinfo with CHAR to test dtype('c')\n+typeinfo = dict(char=np.dtype(\"c\"), **_typeinfo)\n \n \n def setup_module():\n@@ -110,49 +107,50 @@ def is_intent_exact(self, *names):\n intent = Intent()\n \n _type_names = [\n-    \"BOOL\",\n-    \"BYTE\",\n-    \"UBYTE\",\n-    \"SHORT\",\n-    \"USHORT\",\n-    \"INT\",\n-    \"UINT\",\n-    \"LONG\",\n-    \"ULONG\",\n-    \"LONGLONG\",\n-    \"ULONGLONG\",\n-    \"FLOAT\",\n-    \"DOUBLE\",\n-    \"CFLOAT\",\n-    \"STRING1\",\n-    \"STRING5\",\n-    \"CHARACTER\",\n+    \"bool_\",\n+    \"byte\",\n+    \"ubyte\",\n+    \"short\",\n+    \"ushort\",\n+    \"intc\",\n+    \"uintc\",\n+    \"int_\",\n+    \"uint\",\n+    \"longlong\",\n+    \"ulonglong\",\n+    \"float32\",\n+    \"float64\",\n+    \"complex64\",\n+    \"complex128\",\n+    \"bytes_\",\n+    \"char\",\n ]\n \n-_cast_dict = {\"BOOL\": [\"BOOL\"]}\n-_cast_dict[\"BYTE\"] = _cast_dict[\"BOOL\"] + [\"BYTE\"]\n-_cast_dict[\"UBYTE\"] = _cast_dict[\"BOOL\"] + [\"UBYTE\"]\n-_cast_dict[\"BYTE\"] = [\"BYTE\"]\n-_cast_dict[\"UBYTE\"] = [\"UBYTE\"]\n-_cast_dict[\"SHORT\"] = _cast_dict[\"BYTE\"] + [\"UBYTE\", \"SHORT\"]\n-_cast_dict[\"USHORT\"] = _cast_dict[\"UBYTE\"] + [\"BYTE\", \"USHORT\"]\n-_cast_dict[\"INT\"] = _cast_dict[\"SHORT\"] + [\"USHORT\", \"INT\"]\n-_cast_dict[\"UINT\"] = _cast_dict[\"USHORT\"] + [\"SHORT\", \"UINT\"]\n+_cast_dict = {\"bool_\": [\"bool_\"]}\n+_cast_dict[\"byte\"] = _cast_dict[\"bool_\"] + [\"byte\"]\n+_cast_dict[\"ubyte\"] = _cast_dict[\"bool_\"] + [\"ubyte\"]\n+_cast_dict[\"byte\"] = [\"byte\"]\n+_cast_dict[\"ubyte\"] = [\"ubyte\"]\n+_cast_dict[\"short\"] = _cast_dict[\"byte\"] + [\"ubyte\", \"short\"]\n+_cast_dict[\"ushort\"] = _cast_dict[\"ubyte\"] + [\"byte\", \"ushort\"]\n+_cast_dict[\"intc\"] = _cast_dict[\"short\"] + [\"ushort\", \"intc\"]\n+_cast_dict[\"uintc\"] = _cast_dict[\"ushort\"] + [\"short\", \"uintc\"]\n \n-_cast_dict[\"LONG\"] = _cast_dict[\"INT\"] + [\"LONG\"]\n-_cast_dict[\"ULONG\"] = _cast_dict[\"UINT\"] + [\"ULONG\"]\n+_cast_dict[\"int_\"] = _cast_dict[\"intc\"] + [\"int_\"]\n+_cast_dict[\"uint\"] = _cast_dict[\"uintc\"] + [\"uint\"]\n \n-_cast_dict[\"LONGLONG\"] = _cast_dict[\"LONG\"] + [\"LONGLONG\"]\n-_cast_dict[\"ULONGLONG\"] = _cast_dict[\"ULONG\"] + [\"ULONGLONG\"]\n+_cast_dict[\"longlong\"] = _cast_dict[\"int_\"] + [\"longlong\"]\n+_cast_dict[\"ulonglong\"] = _cast_dict[\"uint\"] + [\"ulonglong\"]\n \n-_cast_dict[\"FLOAT\"] = _cast_dict[\"SHORT\"] + [\"USHORT\", \"FLOAT\"]\n-_cast_dict[\"DOUBLE\"] = _cast_dict[\"INT\"] + [\"UINT\", \"FLOAT\", \"DOUBLE\"]\n+_cast_dict[\"float32\"] = _cast_dict[\"short\"] + [\"ushort\", \"float32\"]\n+_cast_dict[\"float64\"] = _cast_dict[\"intc\"] + [\"uintc\", \"float32\", \"float64\"]\n \n-_cast_dict[\"CFLOAT\"] = _cast_dict[\"FLOAT\"] + [\"CFLOAT\"]\n+_cast_dict[\"complex64\"] = _cast_dict[\"float32\"] + [\"complex64\"]\n+_cast_dict[\"complex128\"] = _cast_dict[\"float64\"] + [\"complex128\"]\n \n-_cast_dict['STRING1'] = ['STRING1']\n-_cast_dict['STRING5'] = ['STRING5']\n-_cast_dict['CHARACTER'] = ['CHARACTER']\n+_cast_dict['bytes_'] = ['bytes_']\n+_cast_dict['str_'] = ['str_']\n+_cast_dict['char'] = ['char']\n \n # 32 bit system malloc typically does not provide the alignment required by\n # 16 byte long double types this means the inout intent cannot be satisfied\n@@ -163,22 +161,59 @@ def is_intent_exact(self, *names):\n if ((np.intp().dtype.itemsize != 4 or np.clongdouble().dtype.alignment <= 8)\n         and sys.platform != \"win32\"\n         and (platform.system(), platform.processor()) != (\"Darwin\", \"arm\")):\n-    _type_names.extend([\"LONGDOUBLE\", \"CDOUBLE\", \"CLONGDOUBLE\"])\n-    _cast_dict[\"LONGDOUBLE\"] = _cast_dict[\"LONG\"] + [\n-        \"ULONG\",\n-        \"FLOAT\",\n-        \"DOUBLE\",\n-        \"LONGDOUBLE\",\n+    _type_names.extend([\"longdouble\", \"complex128\", \"clongdouble\"])\n+    _cast_dict[\"longdouble\"] = _cast_dict[\"int_\"] + [\n+        \"uint\",\n+        \"float32\",\n+        \"float64\",\n+        \"longdouble\",\n     ]\n-    _cast_dict[\"CLONGDOUBLE\"] = _cast_dict[\"LONGDOUBLE\"] + [\n-        \"CFLOAT\",\n-        \"CDOUBLE\",\n-        \"CLONGDOUBLE\",\n+    _cast_dict[\"clongdouble\"] = _cast_dict[\"longdouble\"] + [\n+        \"complex64\",\n+        \"complex128\",\n+        \"clongdouble\",\n     ]\n-    _cast_dict[\"CDOUBLE\"] = _cast_dict[\"DOUBLE\"] + [\"CFLOAT\", \"CDOUBLE\"]\n+    _cast_dict[\"complex128\"] = _cast_dict[\"float64\"] + [\"complex64\", \"complex128\"]\n \n \n class Type:\n+\n+    # dtype names in numpy/f2py/tests/src/array_from_obj/wrapmodule.c\n+    # are following C naming convention\n+    python_to_c_name_map = {\n+        \"bool_\": \"BOOL\",\n+        \"byte\": \"BYTE\",\n+        \"ubyte\": \"UBYTE\",\n+        \"short\": \"SHORT\",\n+        \"ushort\": \"USHORT\",\n+        \"intc\": \"INT\",\n+        \"uintc\": \"UINT\",\n+        \"int_\": \"LONG\",\n+        \"uint\": \"ULONG\",\n+        \"longlong\": \"LONGLONG\",\n+        \"ulonglong\": \"ULONGLONG\",\n+        \"float32\": \"FLOAT\",\n+        \"float64\": \"DOUBLE\",\n+        \"longdouble\": \"LONGDOUBLE\",\n+        \"complex64\": \"CFLOAT\",\n+        \"complex128\": \"CDOUBLE\",\n+        \"clongdouble\": \"CLONGDOUBLE\",\n+        \"object_\": \"OBJECT\",\n+        \"bytes_\": \"STRING\",\n+        \"char\": \"STRING\",\n+        \"str_\": \"UNICODE\",\n+        \"int8\": \"BYTE\",\n+        \"uint8\": \"UBYTE\",\n+        \"int16\": \"SHORT\",\n+        \"uint16\": \"USHORT\",\n+        \"int32\": \"INT\",\n+        \"uint32\": \"UINT\",\n+        \"int64\": \"LONG\",\n+        \"uint64\": \"ULONG\",\n+        \"intp\": \"LONG\",\n+        \"uintp\": \"ULONG\",\n+    }\n+\n     _type_cache = {}\n \n     def __new__(cls, name):\n@@ -189,31 +224,33 @@ def __new__(cls, name):\n                 if not isinstance(i, type) and dtype0.type is i.type:\n                     name = n\n                     break\n-        obj = cls._type_cache.get(name.upper(), None)\n+        obj = cls._type_cache.get(name, None)\n         if obj is not None:\n             return obj\n         obj = object.__new__(cls)\n         obj._init(name)\n-        cls._type_cache[name.upper()] = obj\n+        cls._type_cache[name] = obj\n         return obj\n \n     def _init(self, name):\n-        self.NAME = name.upper()\n+        self.NAME = name\n \n-        if self.NAME == 'CHARACTER':\n+        if self.NAME == 'char':\n             info = typeinfo[self.NAME]\n             self.type_num = getattr(wrap, 'NPY_STRING')\n             self.elsize = 1\n             self.dtype = np.dtype('c')\n-        elif self.NAME.startswith('STRING'):\n-            info = typeinfo[self.NAME[:6]]\n+        elif self.NAME == 'bytes_':\n+            info = typeinfo[self.NAME]\n             self.type_num = getattr(wrap, 'NPY_STRING')\n-            self.elsize = int(self.NAME[6:] or 0)\n+            self.elsize = int(5)\n             self.dtype = np.dtype(f'S{self.elsize}')\n         else:\n             info = typeinfo[self.NAME]\n-            self.type_num = getattr(wrap, 'NPY_' + self.NAME)\n-            self.elsize = info.bits // 8\n+            self.type_num = getattr(\n+                wrap, 'NPY_' + self.python_to_c_name_map[self.NAME]\n+            )\n+            self.elsize = info.itemsize\n             self.dtype = np.dtype(info.type)\n \n         assert self.type_num == info.num\n@@ -390,14 +427,14 @@ def setup_type(self, request):\n \n     @property\n     def num2seq(self):\n-        if self.type.NAME.startswith('STRING'):\n+        if self.type.NAME == 'bytes_':\n             elsize = self.type.elsize\n             return ['1' * elsize, '2' * elsize]\n         return [1, 2]\n \n     @property\n     def num23seq(self):\n-        if self.type.NAME.startswith('STRING'):\n+        if self.type.NAME == 'bytes_':\n             elsize = self.type.elsize\n             return [['1' * elsize, '2' * elsize, '3' * elsize],\n                     ['4' * elsize, '5' * elsize, '6' * elsize]]\n@@ -552,9 +589,6 @@ def test_in_cache_from_2casttype(self):\n \n     def test_in_cache_from_2casttype_failure(self):\n         for t in self.type.all_types():\n-            if t.NAME == 'STRING':\n-                # string elsize is 0, so skipping the test\n-                continue\n             if t.elsize >= self.type.elsize:\n                 continue\n             obj = np.array(self.num2seq, dtype=t.dtype)\n",
            "comment_added_diff": {
                "15": "# Extend core typeinfo with CHAR to test dtype('c')",
                "181": "    # dtype names in numpy/f2py/tests/src/array_from_obj/wrapmodule.c",
                "182": "    # are following C naming convention"
            },
            "comment_deleted_diff": {
                "15": "# Extend core typeinfo with CHARACTER to test dtype('c')",
                "556": "                # string elsize is 0, so skipping the test"
            },
            "comment_modified_diff": {
                "15": "# Extend core typeinfo with CHARACTER to test dtype('c')"
            }
        },
        {
            "commit": "3968009d8fa45cf6d13dc639007fde4af35fb593",
            "timestamp": "2023-10-05T13:34:09+02:00",
            "author": "Mateusz Sok\u00f3\u0142",
            "commit_message": "Include c-names mapping",
            "additions": 75,
            "deletions": 109,
            "change_type": "MODIFY",
            "diff": "@@ -7,13 +7,16 @@\n import numpy as np\n \n from numpy.testing import assert_, assert_equal\n-from numpy.core.multiarray import typeinfo as _typeinfo\n+from numpy.core._type_aliases import c_names_dict as _c_names_dict\n from . import util\n \n wrap = None\n \n-# Extend core typeinfo with CHAR to test dtype('c')\n-typeinfo = dict(char=np.dtype(\"c\"), **_typeinfo)\n+# Extend core typeinfo with CHARACTER to test dtype('c')\n+c_names_dict = dict(\n+    CHARACTER=np.dtype(\"c\"),\n+    **_c_names_dict\n+)\n \n \n def setup_module():\n@@ -107,50 +110,49 @@ def is_intent_exact(self, *names):\n intent = Intent()\n \n _type_names = [\n-    \"bool_\",\n-    \"byte\",\n-    \"ubyte\",\n-    \"short\",\n-    \"ushort\",\n-    \"intc\",\n-    \"uintc\",\n-    \"int_\",\n-    \"uint\",\n-    \"longlong\",\n-    \"ulonglong\",\n-    \"float32\",\n-    \"float64\",\n-    \"complex64\",\n-    \"complex128\",\n-    \"bytes_\",\n-    \"char\",\n+    \"BOOL\",\n+    \"BYTE\",\n+    \"UBYTE\",\n+    \"SHORT\",\n+    \"USHORT\",\n+    \"INT\",\n+    \"UINT\",\n+    \"LONG\",\n+    \"ULONG\",\n+    \"LONGLONG\",\n+    \"ULONGLONG\",\n+    \"FLOAT\",\n+    \"DOUBLE\",\n+    \"CFLOAT\",\n+    \"STRING1\",\n+    \"STRING5\",\n+    \"CHARACTER\",\n ]\n \n-_cast_dict = {\"bool_\": [\"bool_\"]}\n-_cast_dict[\"byte\"] = _cast_dict[\"bool_\"] + [\"byte\"]\n-_cast_dict[\"ubyte\"] = _cast_dict[\"bool_\"] + [\"ubyte\"]\n-_cast_dict[\"byte\"] = [\"byte\"]\n-_cast_dict[\"ubyte\"] = [\"ubyte\"]\n-_cast_dict[\"short\"] = _cast_dict[\"byte\"] + [\"ubyte\", \"short\"]\n-_cast_dict[\"ushort\"] = _cast_dict[\"ubyte\"] + [\"byte\", \"ushort\"]\n-_cast_dict[\"intc\"] = _cast_dict[\"short\"] + [\"ushort\", \"intc\"]\n-_cast_dict[\"uintc\"] = _cast_dict[\"ushort\"] + [\"short\", \"uintc\"]\n+_cast_dict = {\"BOOL\": [\"BOOL\"]}\n+_cast_dict[\"BYTE\"] = _cast_dict[\"BOOL\"] + [\"BYTE\"]\n+_cast_dict[\"UBYTE\"] = _cast_dict[\"BOOL\"] + [\"UBYTE\"]\n+_cast_dict[\"BYTE\"] = [\"BYTE\"]\n+_cast_dict[\"UBYTE\"] = [\"UBYTE\"]\n+_cast_dict[\"SHORT\"] = _cast_dict[\"BYTE\"] + [\"UBYTE\", \"SHORT\"]\n+_cast_dict[\"USHORT\"] = _cast_dict[\"UBYTE\"] + [\"BYTE\", \"USHORT\"]\n+_cast_dict[\"INT\"] = _cast_dict[\"SHORT\"] + [\"USHORT\", \"INT\"]\n+_cast_dict[\"UINT\"] = _cast_dict[\"USHORT\"] + [\"SHORT\", \"UINT\"]\n \n-_cast_dict[\"int_\"] = _cast_dict[\"intc\"] + [\"int_\"]\n-_cast_dict[\"uint\"] = _cast_dict[\"uintc\"] + [\"uint\"]\n+_cast_dict[\"LONG\"] = _cast_dict[\"INT\"] + [\"LONG\"]\n+_cast_dict[\"ULONG\"] = _cast_dict[\"UINT\"] + [\"ULONG\"]\n \n-_cast_dict[\"longlong\"] = _cast_dict[\"int_\"] + [\"longlong\"]\n-_cast_dict[\"ulonglong\"] = _cast_dict[\"uint\"] + [\"ulonglong\"]\n+_cast_dict[\"LONGLONG\"] = _cast_dict[\"LONG\"] + [\"LONGLONG\"]\n+_cast_dict[\"ULONGLONG\"] = _cast_dict[\"ULONG\"] + [\"ULONGLONG\"]\n \n-_cast_dict[\"float32\"] = _cast_dict[\"short\"] + [\"ushort\", \"float32\"]\n-_cast_dict[\"float64\"] = _cast_dict[\"intc\"] + [\"uintc\", \"float32\", \"float64\"]\n+_cast_dict[\"FLOAT\"] = _cast_dict[\"SHORT\"] + [\"USHORT\", \"FLOAT\"]\n+_cast_dict[\"DOUBLE\"] = _cast_dict[\"INT\"] + [\"UINT\", \"FLOAT\", \"DOUBLE\"]\n \n-_cast_dict[\"complex64\"] = _cast_dict[\"float32\"] + [\"complex64\"]\n-_cast_dict[\"complex128\"] = _cast_dict[\"float64\"] + [\"complex128\"]\n+_cast_dict[\"CFLOAT\"] = _cast_dict[\"FLOAT\"] + [\"CFLOAT\"]\n \n-_cast_dict['bytes_'] = ['bytes_']\n-_cast_dict['str_'] = ['str_']\n-_cast_dict['char'] = ['char']\n+_cast_dict['STRING1'] = ['STRING1']\n+_cast_dict['STRING5'] = ['STRING5']\n+_cast_dict['CHARACTER'] = ['CHARACTER']\n \n # 32 bit system malloc typically does not provide the alignment required by\n # 16 byte long double types this means the inout intent cannot be satisfied\n@@ -161,95 +163,56 @@ def is_intent_exact(self, *names):\n if ((np.intp().dtype.itemsize != 4 or np.clongdouble().dtype.alignment <= 8)\n         and sys.platform != \"win32\"\n         and (platform.system(), platform.processor()) != (\"Darwin\", \"arm\")):\n-    _type_names.extend([\"longdouble\", \"complex128\", \"clongdouble\"])\n-    _cast_dict[\"longdouble\"] = _cast_dict[\"int_\"] + [\n-        \"uint\",\n-        \"float32\",\n-        \"float64\",\n-        \"longdouble\",\n+    _type_names.extend([\"LONGDOUBLE\", \"CDOUBLE\", \"CLONGDOUBLE\"])\n+    _cast_dict[\"LONGDOUBLE\"] = _cast_dict[\"LONG\"] + [\n+        \"ULONG\",\n+        \"FLOAT\",\n+        \"DOUBLE\",\n+        \"LONGDOUBLE\",\n     ]\n-    _cast_dict[\"clongdouble\"] = _cast_dict[\"longdouble\"] + [\n-        \"complex64\",\n-        \"complex128\",\n-        \"clongdouble\",\n+    _cast_dict[\"CLONGDOUBLE\"] = _cast_dict[\"LONGDOUBLE\"] + [\n+        \"CFLOAT\",\n+        \"CDOUBLE\",\n+        \"CLONGDOUBLE\",\n     ]\n-    _cast_dict[\"complex128\"] = _cast_dict[\"float64\"] + [\"complex64\", \"complex128\"]\n+    _cast_dict[\"CDOUBLE\"] = _cast_dict[\"DOUBLE\"] + [\"CFLOAT\", \"CDOUBLE\"]\n \n \n class Type:\n-\n-    # dtype names in numpy/f2py/tests/src/array_from_obj/wrapmodule.c\n-    # are following C naming convention\n-    python_to_c_name_map = {\n-        \"bool_\": \"BOOL\",\n-        \"byte\": \"BYTE\",\n-        \"ubyte\": \"UBYTE\",\n-        \"short\": \"SHORT\",\n-        \"ushort\": \"USHORT\",\n-        \"intc\": \"INT\",\n-        \"uintc\": \"UINT\",\n-        \"int_\": \"LONG\",\n-        \"uint\": \"ULONG\",\n-        \"longlong\": \"LONGLONG\",\n-        \"ulonglong\": \"ULONGLONG\",\n-        \"float32\": \"FLOAT\",\n-        \"float64\": \"DOUBLE\",\n-        \"longdouble\": \"LONGDOUBLE\",\n-        \"complex64\": \"CFLOAT\",\n-        \"complex128\": \"CDOUBLE\",\n-        \"clongdouble\": \"CLONGDOUBLE\",\n-        \"object_\": \"OBJECT\",\n-        \"bytes_\": \"STRING\",\n-        \"char\": \"STRING\",\n-        \"str_\": \"UNICODE\",\n-        \"int8\": \"BYTE\",\n-        \"uint8\": \"UBYTE\",\n-        \"int16\": \"SHORT\",\n-        \"uint16\": \"USHORT\",\n-        \"int32\": \"INT\",\n-        \"uint32\": \"UINT\",\n-        \"int64\": \"LONG\",\n-        \"uint64\": \"ULONG\",\n-        \"intp\": \"INTP\",\n-        \"uintp\": \"UINTP\",\n-    }\n-\n     _type_cache = {}\n \n     def __new__(cls, name):\n         if isinstance(name, np.dtype):\n             dtype0 = name\n             name = None\n-            for n, i in typeinfo.items():\n+            for n, i in c_names_dict.items():\n                 if not isinstance(i, type) and dtype0.type is i.type:\n                     name = n\n                     break\n-        obj = cls._type_cache.get(name, None)\n+        obj = cls._type_cache.get(name.upper(), None)\n         if obj is not None:\n             return obj\n         obj = object.__new__(cls)\n         obj._init(name)\n-        cls._type_cache[name] = obj\n+        cls._type_cache[name.upper()] = obj\n         return obj\n \n     def _init(self, name):\n-        self.NAME = name\n+        self.NAME = name.upper()\n \n-        if self.NAME == 'char':\n-            info = typeinfo[self.NAME]\n+        if self.NAME == 'CHARACTER':\n+            info = c_names_dict[self.NAME]\n             self.type_num = getattr(wrap, 'NPY_STRING')\n             self.elsize = 1\n             self.dtype = np.dtype('c')\n-        elif self.NAME == 'bytes_':\n-            info = typeinfo[self.NAME]\n+        elif self.NAME.startswith('STRING'):\n+            info = c_names_dict[self.NAME[:6]]\n             self.type_num = getattr(wrap, 'NPY_STRING')\n-            self.elsize = int(5)\n+            self.elsize = int(self.NAME[6:] or 0)\n             self.dtype = np.dtype(f'S{self.elsize}')\n         else:\n-            info = typeinfo[self.NAME]\n-            self.type_num = getattr(\n-                wrap, 'NPY_' + self.python_to_c_name_map[self.NAME]\n-            )\n+            info = c_names_dict[self.NAME]\n+            self.type_num = getattr(wrap, 'NPY_' + self.NAME)\n             self.elsize = info.itemsize\n             self.dtype = np.dtype(info.type)\n \n@@ -270,28 +233,28 @@ def all_types(self):\n         return [self.__class__(_m) for _m in _type_names]\n \n     def smaller_types(self):\n-        bits = typeinfo[self.NAME].alignment\n+        bits = c_names_dict[self.NAME].alignment\n         types = []\n         for name in _type_names:\n-            if typeinfo[name].alignment < bits:\n+            if c_names_dict[name].alignment < bits:\n                 types.append(Type(name))\n         return types\n \n     def equal_types(self):\n-        bits = typeinfo[self.NAME].alignment\n+        bits = c_names_dict[self.NAME].alignment\n         types = []\n         for name in _type_names:\n             if name == self.NAME:\n                 continue\n-            if typeinfo[name].alignment == bits:\n+            if c_names_dict[name].alignment == bits:\n                 types.append(Type(name))\n         return types\n \n     def larger_types(self):\n-        bits = typeinfo[self.NAME].alignment\n+        bits = c_names_dict[self.NAME].alignment\n         types = []\n         for name in _type_names:\n-            if typeinfo[name].alignment > bits:\n+            if c_names_dict[name].alignment > bits:\n                 types.append(Type(name))\n         return types\n \n@@ -427,14 +390,14 @@ def setup_type(self, request):\n \n     @property\n     def num2seq(self):\n-        if self.type.NAME == 'bytes_':\n+        if self.type.NAME.startswith('STRING'):\n             elsize = self.type.elsize\n             return ['1' * elsize, '2' * elsize]\n         return [1, 2]\n \n     @property\n     def num23seq(self):\n-        if self.type.NAME == 'bytes_':\n+        if self.type.NAME.startswith('STRING'):\n             elsize = self.type.elsize\n             return [['1' * elsize, '2' * elsize, '3' * elsize],\n                     ['4' * elsize, '5' * elsize, '6' * elsize]]\n@@ -589,6 +552,9 @@ def test_in_cache_from_2casttype(self):\n \n     def test_in_cache_from_2casttype_failure(self):\n         for t in self.type.all_types():\n+            if t.NAME == 'STRING':\n+                # string elsize is 0, so skipping the test\n+                continue\n             if t.elsize >= self.type.elsize:\n                 continue\n             obj = np.array(self.num2seq, dtype=t.dtype)\n",
            "comment_added_diff": {
                "15": "# Extend core typeinfo with CHARACTER to test dtype('c')",
                "556": "                # string elsize is 0, so skipping the test"
            },
            "comment_deleted_diff": {
                "15": "# Extend core typeinfo with CHAR to test dtype('c')",
                "181": "    # dtype names in numpy/f2py/tests/src/array_from_obj/wrapmodule.c",
                "182": "    # are following C naming convention"
            },
            "comment_modified_diff": {
                "15": "# Extend core typeinfo with CHAR to test dtype('c')"
            }
        }
    ],
    "format.pyi": [],
    "24866.new_feature.rst": [],
    "cirrus_arm.yml": []
}