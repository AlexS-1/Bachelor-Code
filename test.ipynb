{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "image/png": "iVBORw0KGgoAAAANSUhEUgAAA1UAAAIjCAYAAADr8zGuAAAAOXRFWHRTb2Z0d2FyZQBNYXRwbG90bGliIHZlcnNpb24zLjguMiwgaHR0cHM6Ly9tYXRwbG90bGliLm9yZy8g+/7EAAAACXBIWXMAAA9hAAAPYQGoP6dpAACi70lEQVR4nOzdd3yT1dvH8U8no5S9954CKiogssSBe+BGBRwIOMCJqAju+YCK8hNRyhAVUEFAAQWRoQKyhwyBsgqUVaB7nuePk6YNbSGhI2n7ffM6r+S+cyf3leSk5MpZfoBBREREREREzou/twMQEREREREpzJRUiYiIiIiI5IKSKhERERERkVxQUiUiIiIiIpILSqpERERERERyQUmViIiIiIhILiipEhERERERyQUlVSIiIiIiIrmgpEpERERERCQXlFSJ+KCQkBDGjx/PoUOHMMYwevRo6tWrhzGGPn36eDs8n3Tttdeybt064uPjMcZQrlw5b4dU5FxyySX8+eefxMTEYIyhbdu23g5JirARI0ZgjPHa+RcvXszixYvz9RxhYWFER0e7dawxhhEjRji3+/TpgzGGevXqOfcVRMz5pWvXrhhj6Nq1q7dDETkvSqpE8kH6f3bt2rU7r/u/9NJL9O3bl//973/cf//9TJkyJY8jLFoqVqzI9OnTiY+P5/HHH+f+++8nNjY222PT35v0Eh8fz/bt2xkzZgxVq1Yt4MjzXosWLRgxYoTLF628EBgYyIwZM6hYsSJPP/00999/P3v37j3rfapWrcoHH3zA1q1biY2NJSYmhtWrV/Pyyy8r6XXo2LEjI0aMKJSvR/pnaPz48dne/uabbzqPqVSpUoHGVphf17xUo0YNRowYkec/gKQnvAX9vor4OqOiopK3pU+fPsYYY9q1a3de9//777/NsmXLXPbVq1fPGGNMnz59vP78fK1ce+21xhhjevTo4fZ788orr5jevXubhx9+2ISFhZmUlBSza9cuU6pUKa8/n9yUXr16GWOM6dq1a54+brNmzYwxxjz88MNuHX/JJZeYI0eOmLi4OPPFF1+Yxx57zDz22GNm/PjxJjo62ixYsMDrr5UvlGeffdYYY0y9evW8HounxRhj4uLizIkTJ0xQUFCW23ft2mXi4uKMMcZUqlTJ48cPCAgwJUqU8NrrGhQUlO3zyssSFhZmoqOj3Tq2RIkSJiAgwLmd/rcs83M8M+Z27drly/8bI0aMOO/3NafStWvXfPnbpaJSUEUtVSI+qGrVqpw8edLbYRQa6S1Mnrxm8+bNY+rUqXz11Vf069ePjz76iIYNG3LLLbfkOp5SpUrl+jF8jSevcbly5Zg5cyapqalcdNFF9O/fn3HjxjFu3DgeffRRGjVqxNKlS/M5YikI8+fPp2zZslx33XUu+zt27EjDhg35+eefz/uxU1NTSUxMzG2I5y05OZnk5GSvnf9MiYmJpKamnvUYX4tZpDhRUiVSQNL7ztesWZOZM2cSHR3NkSNH+OCDD/D3tx/F9D7lDRs25MYbb3R2ncmpK1dO/efDwsIIDw932efn58fgwYPZvHkz8fHxHD58mM8//5zy5cu7HBceHs6cOXPo1KkTK1euJD4+nl27dvHAAw9kOU+5cuUYNWoU4eHhJCQksH//fiZNmuTSJSQ4OJiRI0fy33//kZCQwL59+3jvvfcIDg5263W74447WL16NXFxcRw9epQpU6ZQs2ZNl9dg8uTJAKxevRpjDGFhYW49dma///47AA0aNHDu6927t/Pcx48f59tvv6V27dou91u8eDGbNm3i4osvZsmSJcTGxvL2228DUKJECUaMGMH27duJj4/n4MGD/PDDDzRs2NB5/7x8X/r06cP3338PwB9//OGsP+cao9C9e3eWLl1KTEwMUVFRzJo1i+bNmztvDwsLcyZB33//PcaYs47beOyxx6hduzbPPPMM27dvz3L7kSNHeOutt1z2DRw4kM2bN5OQkEBERASffvpplq5b6a9169at+eOPP4iNjeW///6jV69eAHTp0oUVK1YQFxfHtm3b6NGjh8v907ssNWnShClTpnDy5EmOHDnC66+/DkDt2rWZNWsWp06d4tChQzzzzDNZYne3PhtjGDNmDLfccgubNm0iISGBzZs3c+2117rE8+GHHwKwZ8+eLJ/3q666imXLlhEVFUV0dDTbtm3L8rplJyAggFdeeYWdO3eSkJBAeHg4b731VpYYPfms5yQiIoKlS5dy3333uezv3bs3GzduZPPmzVnuc8UVVzB9+nT27t3rfA1HjRpFyZIlXY7LbkxVXryuffv2ZdGiRURGRpKQkMCWLVsYMGBAljjP/Pua/vf5zjvv5KWXXmL//v3Ex8ezcOFCGjVqdF7PMV2DBg2YP38+MTExREREMHz48CzHnDmmKjuZY+7atSurV68GYOLEic7XoU+fPowcOZKkpCQqV66c5THGjRtHVFQUJUqUOOu5sjv3pk2baNGiBb///juxsbEcOHCA559/PsuxtWrVYubMmcTExBAZGcmoUaNyPN9ll13GvHnzOHnyJLGxsfzxxx9cfvnlztubN29OXFwckyZNcrlfp06dSElJ4d133/XoeYjkhteby1RUilrJrvtfWFiYiYuLM5s2bTJffvmleeyxx8yMGTOMMcYMGDDAAKZq1aqmd+/e5siRI2bt2rWmd+/epnfv3qZ06dLZdv9bvHixWbx4cZbzh4WFmfDwcJd9X3zxhUlKSjLjxo0z/fv3N++8846Jjo42K1euNIGBgc7jwsPDzdatW82hQ4fMm2++aQYNGmRWr15tUlNTTcuWLZ3HhYSEmI0bN5rk5GQzbtw489hjj5mXX37ZrFy50rRt29YAxs/Pz8yfP9/ExMSYUaNGmUcffdR88sknJikpycycOdPt13HlypVm8ODB5u233zaxsbFm9+7dply5cgYwV111lfn8889duvR16NDBo/cGME8++aQxxpj+/fsbwLz00ksmNTXVfPvtt2bAgAFm+PDh5siRIy7nTn8PDh48aCIjI83HH39sHn30UXPzzTcbf39/89tvvxljjPnmm2/MoEGDzNChQ83ChQvNzTffnC/vS4MGDcxHH31kjDHmzTffdNafqlWr5vh69OjRwyQlJZlt27aZ5557zvk8jx8/7uxW1KFDB/Pmm28aY4z56KOPTO/evc1VV12V42MuX77cxMbGut11Kr0r0a+//moef/xx88knn5jk5OQsr8HixYvNgQMHzN69e817771nHn/8cbN582aTnJxs7rrrLnPw4EHz6quvmqeeesrs37/fREVFmTJlymQ5z9q1a83UqVPNgAEDzJw5c4wxxgwZMsRs3brVfPbZZ2bAgAFm2bJlxhhjOnfu7Ly/J/XZGGPWrVtnIiIizMsvv2yeeuops3PnThMTE2MqVqxoANO6dWszdepUY4wxgwcPdvm8t2zZ0iQkJJhVq1aZJ5980vTv39+8//775o8//jjn6xkWFmaMMWb69Olm4MCBZuLEicYYY3788UeX49z9rOdUjDFmzJgx5pFHHjGxsbEmJCTEgO22FxkZaYYOHZptN7GPP/7YzJ0717z44ovm0UcfNePHjzfJyclm+vTp2daLvHxdAbNy5UozYcIEM3jwYPP444+b+fPnG2OMGTRokMu5zvz7mt49bc2aNeaff/4xgwcPNq+++qqJiYkxK1ascLmvu88x/f+F7du3m0mTJplBgwaZ2bNnG2OMee2117I89xEjRmT5W5a5+1/mmKtWrWpeeeUVY4wxn3/+ufN1aNCggWnUqJExxpjHH3/c5RxBQUHm+PHj5ssvv3TrM5v5fc38+Rw9erQZMGCAWbhwoTHGmJ49ezqPK1mypNm2bZuJi4sz7777rnnqqafMP//8Y9avX2+Mce3+1717d5OQkGD+/PNP8/TTT5vBgweb9evXm4SEBHPppZc6j0vv7nnTTTcZwJQuXdr8999/ZvPmzSY4ONitv0MqKnlQvB6AikqRKzklVelf/DMfm/4fdOZ94eHhZs6cOS77cpNUderUyRhjzL333uty3DXXXJNlf3h4uDHGmCuuuMK5r3LlyiY+Pt588MEHzn0jR440xhhz66235vg69O7d26SkpJhOnTq57O/fv78xxpiOHTvmeN/AwEBz+PBhs3HjRpdxFddff70xxpiRI0ee9fU+13tz5ZVXmkqVKplatWqZu+66yxw9etTExsaamjVrmrp165rk5GQzbNgwl/u2atXKJCUluexfvHixMSYjGUsvffv2dX5ZzymW/HhfPB1TtXbtWnP48GFToUIF577WrVublJQUM3HiROe+9C+UvXr1OudjHj9+3Kxbt86t81euXNkkJCSY+fPnGz8/P+f+QYMGGWOM6du3b5bX+p577nHua9q0qTHGmJSUFHPZZZc591999dVZPi/pXwQ///xz5z5/f3+zb98+k5qaal544QXn/nLlypnY2FgTFhZ2XvXZGGMSEhJMw4YNXV7XM7/I5jT2Z/DgwVm+tLpT2rRpY4wx5osvvnDZ//777xtjjOnWrZvHdSqnYoxNqsqXL28SEhJM7969DWCuu+46k5qaaurWrZvtl++SJUtmeayhQ4ea1NRUU6dOnSzv15nnzM3rmtP5582bZ3bu3OmyL6ekasuWLS4/GKT/INOqVSuPn2P6/wsff/yxy7Fz5swxCQkJLq+bMZ4lVXD2MVV//vmn+fvvv1323XrrrW79/cgpqTLGmPvvv9+5LygoyBw8eNDMmDHDue+pp54yxhhzxx13OPeVKlXK7NixI8u5t2/fbubNm5fl/du1a5fLuEw/Pz+zdOlSc+jQIVOxYkUzZswYk5SUdN7jmlVUzqeo+59IAfv8889dtpctW+bSHSw/3HnnnZw8eZLffvuNSpUqOcuaNWuIjo6me/fuLsdv2bKF5cuXO7ePHTvG9u3bXeLs1asX69evZ9asWWc979atW9m2bZvLedO72p153swuueQSqlWrxtixY13GVfzyyy9s3bqVG264wdOXwcWiRYs4duwYBw4cYNq0acTExHDbbbdx8OBBbr/9dvz9/Zk+fbpL3IcPH+a///7LEndCQkKWLoe9evXi6NGjjBkzJscY8uN98UT16tW56KKLmDhxIlFRUc79mzZt4rfffuP6668/r8ctW7as29NEX3XVVZQoUYKPPvrIpavX+PHjOXXqVJb3OTo6mu+++865vWPHDqKioti6dSurVq1y7l+5ciVAtq/Nl19+6byelpbG6tWr8ff356uvvnLuP3XqVJbX1tP6vHDhQnbv3u3c3rRpE6dOnXLr/Uofu3bLLbfg5+d3zuPTpb9no0aNctn/f//3fwBZXs+8qFMnT55k/vz53HvvvQDcd999/PXXX+zbty/b4xMSEpzXS5cuTaVKlfjrr7/w9/fnoosuOuf5cvO6nnn+smXLUqlSJZYsWUKjRo0oW7bsOe8fFhbmMm5p2bJlgGtd8/Q5fvrpp1m2S5QowVVXXeXWczofkydPpkOHDi5x9+7dm3379rFkyZLzeszo6Gi+/vpr53ZycjKrVq1yOcf111/PwYMHnV2VAeLj4/niiy9cHuvCCy+kadOmfPPNNy6ft5CQEBYtWkSXLl2cnw1jDH379qVMmTLMmzePQYMG8c4777BmzZrzeh4i5yPQ2wGIFCfx8fEcO3bMZV9UVBQVK1bM1/M2adKE8uXLc/To0WxvP3Mq8ey+DEVFRVGhQgXndqNGjfjhhx/Oed6WLVtmec45nTez9PEP2Y3J2bZtG1dcccVZz30ugwYNYseOHaSkpBAZGcn27dudX+qbNGmCv78/O3fuzPa+Zw4Ej4iIyLKvUaNGbN++/awDy/PjffHE2V7jrVu30rNnT0qXLk1cXJxHj3v69GlCQ0NzFUNycjK7d+/OMp7wwIEDWR7j1KlT7N+/P0sMQLavzZmv46lTp4iPj+f48eNZ9mceH+hpfc7N+zVt2jQeeeQRvvrqK959910WLVrEjz/+6BzTlpN69eqRmpqape5GRkYSFRWV5fXMqzr1zTffMGXKFOrUqcOtt97KCy+8kOOxderU4fXXX+fmm2/O8rfPnSnQcxvz5ZdfzmuvvUbHjh0JCQnJcv70uuPu+dN/kMh8fk+eY2pqqkuSCPbHAoD69euf+wmdp2nTpvHRRx/Ru3dv3njjDcqWLcuNN97I6NGjz/sxs/t8RkVF0aZNG+d2vXr1sv3beubfgCZNmgA4x8xmp1y5cs4fIHbv3s3IkSP58MMP2bRpE2+88cb5PAWR86akSqQAnWvmJk8ZY7L9FTsgIMBl29/fn8jISHr37p3t45z5pT6nOD35xTz9vBs3bsx2wD+Q5YtwQVq1alWOv2L6+/uTlpbGddddl+1rERMT47IdHx9/XjF4633Jb9u2bePCCy8kKCgoz2ciy+k18OS1ye5Yd+7vaX3OzfuVkJBAly5d6N69OzfccAM9e/bknnvuYdGiRVxzzTWkpaWd9f5nS7zyKsbMZs+eTWJiIpMmTaJEiRJMnz492+P8/f357bffqFixIu+99x7btm0jNjaWWrVqMWnSJOekPfkVc8OGDVm0aBHbtm3jmWeeYf/+/SQlJXH99dfzzDPP5Mn58+I5FoSTJ08yd+5cZ1J1xx13ULJkSZeWJk/l5d+o9NfpueeeY/369dkec+bf4muuuQaAmjVrUqlSJSIjIz0+r8j5UlIlUohFRUVl2+XlzF+jd+3axVVXXcWff/7p0i0lN3bt2sUFF1xwzmPatm3LokWLPH789IVlmzVrlmWmuWbNmp1z4dnc2LVrF/7+/oSHh/Pff/+d92O0b9+ewMBAUlJScjwmr98Xd79Mg+trfKbmzZtz9OhRj1upAObMmcPll19Or169XLrqnSuGzDNWBgUF0aBBAxYuXOjx+fNLbupzTs72fhlj+P333/n999959tlnGTZsGG+//Tbdu3fPMYa9e/cSEBBAkyZN2LZtm3N/1apVqVChQr59bhISEpg1axYPPPAAv/zyS5ZWv3StW7emWbNmPPjggy6Lmud1N7ecXtebbrqJkiVLcvPNN7skwWfriuwpT59jQEAADRs2dPlb07RpU8DOXpgb5/p7MHnyZGbPns0ll1xC7969Wbt2Lf/++2+uznkue/fuzfb/jjP/Du3atQuwrc7ufOYee+wxrrnmGl566SWGDRvGuHHjuPXWW/MkZhF3+MbPJSJyXnbt2kXz5s1dpsVt06YNnTp1cjlu+vTpBAYGZjtNb0BAgFtdbs70ww8/cOGFF571P63p06dTu3ZtHn300Sy3lSxZktKlS+d439WrVxMZGcmAAQNcpoLu2bMnLVu2zNX6N+fy448/kpKSkuP0xe501/zhhx+oUqUKTzzxRI7H5Mf7EhsbC5BlSvbsHD58mHXr1tGnTx+Xc7Vq1YprrrmGX375xePzgx03ePDgQf7v//7P2YUnsypVqvDyyy8DdnxMYmIiTz31lMsxDz/8MOXLl8/X99lTuanPOcnp/cquK1v6r/Vnm+o6/T0bMmSIy/701rX8fD0//PBDRo4cedZuV+ktGWe2XAwePDhPY8npdc3u/GXLlqVfv355du7zeY5n/p144oknSEpKynUCf66/B/PmzePo0aMMHTqUrl275qqVyl2//PILtWrV4o477nDuK1WqFP3793c5bs2aNezcuZPnnnsuSzdNwOX/vfr16/PBBx/w/fff88477/Dcc89xyy23eLQ8gEhuqaVKpBCbMGECzzzzDAsWLOCrr76iatWqDBgwgC1btrgMuF66dCmff/45L730EhdeeCG//vorycnJNGnShDvvvJPBgwefc3zUmT744APuuOMOZsyYwYQJE1izZg0VK1bk5ptvZsCAAWzcuJEpU6Zw11138fnnn9O9e3f+/PNPAgICaN68OXfddRfXXnttjl3wUlJSGDp0KBMnTmTJkiV8++23VKtWjcGDBxMeHp6rfv/nsnv3bl555RXeffdd6tevz6xZs4iOjqZBgwbcdtttfPHFF86B/zmZPHkyDz74IKNHj+ayyy5j2bJlhISEcNVVVzF27Fhmz56dL+/L+vXrna9duXLlSExM5Pfff89x3Nbzzz/PvHnz+Pvvv/nqq68oVaoUTz75JKdOnWLkyJEenTvdyZMnue222/jll19Yv349X3/9tfN9vvjii7n33nv5+++/ATsxwjvvvMPIkSOZP38+s2fPplmzZgwaNIhVq1YVyJc8d+WmPuck/fi33nqL7777juTkZObMmcOrr75Kly5d+Pnnn9m7dy9Vq1Zl0KBB7N+/32ViiTNt3LiRiRMn8thjj1G+fHmWLFnCZZddRt++fZk5cyZ//PFHbl6Cs9q4cSMbN2486zHbtm1j586dfPjhh9SqVYvTp0/Tq1ev8x4XmJOcXtdff/2VxMRE5syZw7hx4yhTpgyPPvooR44ccVn/Ljc8fY7x8fH07NmTiRMnsnLlSq677jpuvPFG3nrrrRzH77lr165dREVFMWDAAKKjo4mNjWXlypXOFrCUlBS+++47nnzySVJSUvj2229zdT53jB8/nieeeILJkyfTrl07Dh06xAMPPJClVdwYwyOPPMK8efPYsmULYWFhREREUKtWLbp3787p06e5+eabAft/YXx8PAMHDgTgiy++oFevXnz88ccsXLiQQ4cO5fvzEgEfmIJQRaWolZymVI+Ojs5ybHbTBrs7pTpg7rvvPrNz506TkJBg1q5da66++ups16kCzCOPPGL++ecfExsba06dOmU2bNhg3n33XVO9evWznhuyn769QoUK5pNPPjH79+83CQkJZt++fSYsLMy5XgzYqdGff/55s2nTJhMfH2+OHz9u/vnnHzN8+HATGhp6ztfyzjvvNGvWrDHx8fHm2LFjZsqUKaZmzZrnfL09eW9yKrfddptZunSpiY6ONtHR0ebff/81Y8aMMU2aNHF5XTZt2pTt/UuWLGneeOMNs2vXLpOYmGgOHjxopk+fbho0aJCv78vDDz9sdu7caZKTk92aHvnKK680y5YtM7GxsebkyZPmp59+Ms2bN3c5xpMp1dNL9erVzf/93/8516SJiYkx//zzjxk2bFiW937QoEHm33//NYmJiebQoUPms88+c1kP7GyvdU6vTfqU32d+1s6cpjynz2Z253O3Pp957syxZp6mHTAvv/yy2b9/v0lJSXFOkd29e3czc+ZMc+DAAZOQkGAOHDhgpk6daho3bnzO1z0gIMAMHz7cWe/27t1r3nrrrSzr9XhSp7IrOT3HzCW717x58+bm119/NadPnzZHjhwx48aNc06Lnt0U+O6c093XFTA33nijWb9+vYmLizO7d+82zz//vHMJhLNNT57TZyC7v83uPsf0utegQQPnGmiHDh0yI0aMcFliIP25ezqlOmBuuukms3nzZpOUlJTt/yGXXHKJMcaY+fPnu/3ZzmlK9ew+n9n9f1SnTh0za9YsExMTY44cOWJGjx7tXErizL9Xbdu2Nd9//705evSoiY+PN+Hh4ea7774z3bt3N5Axpf1tt93mcr/atWubkydPmrlz57r9vFRUclP8HFdEREREpJhp06YNGzZs4IEHHvCplmGRwkZjqkRERESKqUcffZTo6Gh+/PFHb4ciUqhpTJWIiIhIMXPjjTfSsmVL+vfvz6effnpeM32KSAZ1/xMREREpZsLDw6lWrRoLFizggQceyLLmk4h4RkmViIiIiIhILmhMlYiIiIiISC4oqRIREREREckFTVSRjZo1axIdHe3tMERERERExMtCQ0M5ePDgWY9RUnWGmjVrEhER4e0wRERERETER9SqVeusiZWSqjOkt1DVqlXL661VoaGhRERE+EQsUjiozoinVGfEE6ov4inVGfGUr9WZ9HjOFYuSqhxER0f7xBsJvhWLFA6qM+Ip1RnxhOqLeEp1RjxV2OqMJqoQERERERHJBSVVIiIiIiIiuaCkSkREREREJBc0pspDfn5+lC9fntDQUPz8/PL1XCEhISQkJFCnTh1iY2Pz9VxSNKjOgDGG6OhoTp48iTHG2+GIiIhIMaCkygNVqlTh0UcfpXnz5gVyPn9/f1atWsVLL71EWlpagZxTCjfVmQzbtm1j/PjxHD161NuhiIiISBGnpMpNgYGBvPXWW8TExDB27FiOHDlCampqvp7T39+fFi1asHXr1mL/BVncozoDAQEBVK1albvuuou33nqLQYMGkZKS4u2wREREpAhTUuWmGjVqULJkST788EN27NhRIOf09/enYsWK7N27t9h+QRbPqM5Yu3fv5sSJE7zyyitUr16dAwcOeDskERERKcI0UYWb/P3tS5WYmOjlSETEHemf1YCAAC9HIiIiIkWdkioREREREZFcKDRJ1YsvvsiqVas4ffo0kZGRzJw5k6ZNm7ocU6JECT799FOOHTtGdHQ033//PVWrVvVSxCIiIiIiUhwUmqSqa9eufPbZZ3To0IGrr76aoKAgfv31V0qXLu08ZvTo0dx0003ceeeddO3alZo1a/Ljjz96MWrxdSNGjGDdunX5fp6uXbtijKFcuXL5fi4RERERKXimMJbKlSsbY4zp3LmzAUzZsmVNYmKi6dWrl/OYZs2aGWOMad++vduPGxoaaowxJjQ01GV/vXr1zOTJk029evUK7Dn6+/ubdu3aGX9//1y/VmPHjjV79+41CQkJ5tChQ2b+/Pnm8ssvdx4THh5usjN06FDn8zfGmLZt2+Z4nsWLF7vcJ3OZO3euMcaYESNGuBw/evToHB+vT58+zjhSUlLMiRMnzIoVK8zw4cNN2bJl8+Q1DgkJMRUrVszT9y275xUUFGSqVatWaOpMUSje+MwWxpLT3zwVleyK6ouKp0V1RsXT4mt1xt14Cu3sf+m/+J84cQKAdu3aERwczMKFC53HbN++nb1799KxY0dWrlyZ7eMEBwdTokQJ53ZoaKjLZbqQkBD8/f2dpSCkD7DP7UD7H374geDgYPr168fu3bupVq0aV155JVWqVHF5Lq+++ipffvmly32jo6NdnvO5nv++ffvo27cvH3zwgXNfzZo16dGjBwcPHsTPz8/l/mduZ+bv78+pU6do0aKFc9Hlyy+/nKFDh9KvXz86d+7MoUOHzus1Afu6xsfHEx8fn+fv6ZnPKzU1laNHj+Z73cmrOlMUpNfVkJCQLJ9nyZDT3zyR7Ki+iKdUZ8RTvlZn3I2jUCZVfn5+fPTRRyxfvpwtW7YAUL16dRITEzl16pTLsZGRkVSvXj3Hxxo2bBgjR47Msj8iIsJlOyEhgVWrVtGiRQsqVqwIgDGQllYyl8/m7FJToVWry7Ls9/dPwM/v3PcvU6YMXbp0oX///pw6dYpKlSqRkpLCr7/+CsBFF10E2OSyXLly1KxZM9vHqVGjBgDNmzfPMTEIDQ1l5cqVXH311fTt25cNGzYA0K9fP1atWkW1atWoUaOG85yhoaFUqVLFuX2munXr4u/v7xLTunXrGDhwINOnT+eLL77g1VdfBWyd6NOnD7fddhuVKlVi3759fPXVVyxatAiwSfe4ceN46qmnGDhwII0bN+aJJ56gXbt2dO3ald69e9O+fXtGjRrFtddeS0xMjPOczz77LI0bN2bgwIGUK1eOF154gYsuuoiyZcty4MABwsLCWLBgAWC7E3br1o1u3boxePBgAG666SZq1qzJuHHj6NatG8YYFixYwAsvvMBff/3lPE+3bt147bXXuOaaa0hMTKRatWoMGTKEDh06kJaWxvr16/nwww/dSiTbtGlzzmOKumrVqlGnTh3WrFlDyZL5+zktCs78mydyNqov4inVGfFUYaszhTKp+uyzz7jgggu44oorcv1Y77zzDqNGjXJuh4aGEhERQa1atYiOjnbur1OnDi+99BJbt25l7969ABhTGmOiszxmQfDzC8XPL+6cxwUEBBAdHU3Lli2ZMmUKSUlJ2R6XlJTEgQMHchxflN4iuG3bNmeydKbo6GgOHjzI5MmTufzyy5k4cSIA33zzDS+++CKvvvoqhw4dcp4jOjqao0eP5njONm3akJqamu3tkydPpl+/fmzYsIG0tDSGDRtGjx49eOSRR/jvv//o0qULY8eOZdWqVSxdupSyZcsC8PDDDzNkyBB2795NVFQUNWvWJD4+nnXr1rFhwwaGDx9OkyZNmDBhAmBbO7p3787w4cNZt24dNWvWZOHChQwbNozTp09z/fXXM2rUKBYtWsQ///zDgw8+yM8//8yWLVsYMWIEAEePHqVz584AbNy4kVOnTjF37lwuu+wyPvvsM+dzGjZsGDNnzmTFihUEBgYyefJkVqxYwXPPPUdKSgovv/wyH3zwARdeeCHJycnZvmYBAQG0adOGjRs35vvi1L6uXr167N+/n4EDB7J//35vh+OzcvqbJ5Id1RfxlOqMeMrX6kx6PO7wel9FT8qYMWPMvn37TP369V32d+/e3RhjTLly5Vz279mzxwwZMiTX/SazH59R2tj2Km+U0m4/p9tvv90cP37cxMXFmeXLl5u33nrLtG7d2uWY8PBwk5CQYKKjo13KFVdc4Xz+7oypGj16tGnTpo05deqUKV26tOncubM5fPiwCQgIMOvWrfN4TFVUVFS2tz322GPGGGOqVKligoODTUxMjOnQoYPLMePHjzdTp041gOnatasxxpibb77Z5ZgRI0aYdevWObdHjx5tFi5c6Ny++uqrTXx8fJZ6lbnMmTPHfPDBB2d9XunnT3+cW265xZw+fdqUKlXKWe/i4uLMtddeawDTu3dvs3XrVpfHCAoKMrGxsebqq6/OMRaNqTrXZ1blzOJrfddVfLuovqh4WlRnVDwtvlZniuSYqjFjxnDbbbfRrVs39uzZ43LbmjVrSEpKokePHs4Z/5o2bUq9evX4+++/8ymiOCAknx4b/Pz8adu2LRs2bMCYtGzO7Z4ff/yRn3/+mc6dO9OhQweuu+46XnjhBR555BEmTZrkPO6DDz5wti6lO5+m140bN/Lff/9xxx130L17d6ZMmZLnrSZ+jr6PxhgaN25MSEgIv/32m8sxwcHBWVq5Vq9efdbHnTp1KitWrKBGjRocOnSI3r178/PPPzu7lfr7+/PSSy9x1113UatWLeeYvLg4998PgF9++YXk5GRuvvlmpk2bRq9evTh9+rRzTGDbtm1p3Lhxll9oSpYsSaNGjbI8VxEREZGC4wcEOUrgGSUgh32ZS/o+f0cJcF4mJ4fwww+QllYT2F6Azyl3Ck1S9dlnn3Hfffdxyy23EB0dTbVq1QA4deoUCQkJnD59mq+++opRo0Zx4sQJTp8+zZgxY/jrr79ynKQib3j2ZdoTfn7+BAQk4OcXl01S5ZnExEQWLlzIwoULefPNNxk/fjyvvfaaS1J17Ngxdu3alduwAZgwYQKPP/44LVu25LLLso4Jy60WLVpw6tQpjh8/TsOGDQG44YYbsiSBiYmJLtuxsbFnfdzVq1eza9cu7rnnHv73v/9x22230bdvX+ftzz//PIMHD2bIkCFs2rSJ2NhYPvroI4KDgz2KPzk5me+//5777ruPadOmOS/Tk88yZcqwZs0aevfuneW+R48e9ehcIiIiUpgFAaUylZJnlDP3lch0eeb1EkBwNpeZS1A214POKPkw8ZZfGtRYS0LTudxxfxQl6YCSqnwwaNAgAJYsWeKyv2/fvs7E4OmnnyYtLY0ffviBEiVKsGDBAuf9xNW///7Lrbfemm+P/8033/Dhhx+yYcMGtm7dmqePXaVKFe677z5mzZqFMYZ///2XhIQE6taty9KlS3P9+FOnTqV3794cOHCAtLQ0fv75Z+dtnTp14qeffmLq1KmAbTFr2rQp//77r/OYpKQkt2bfmzp1Kr/99hstW7bkyiuv5JVXXnHetnbtWu6++26OHDniE/2JRUREJCelgDKOEnLG9cyldDbXSzmun1lKZbosLDP6pgEpZ5TUTJepZ2yn2cu6q6HtHGi6HEKPAdCsUnP2/XKi4J9CLhSapMrPjanuEhMTeeKJJ3jiiScKIKLCoWLFisyYMYMJEyawceNGoqOjueSSS3jhhRf46aefXI4NDQ11tgCmi4uLO68v9SdPnqRGjRo5TqjgLj8/P6pVq+acUr1jx4689NJLnDp1ihdffBGAmJgYPvzwQ0aPHo2/vz/Lly+nXLlydOrUidOnTzN58mSPzjl16lRee+01Xn75Zb7//nuXyT3SuzV27NiRqKgonnnmGapVq+aSVO3Zs4f27dtTr149YmJinJN8nGnp0qUcPnyYqVOnEh4ezqpVq1xieP755/npp5949dVXOXDgAPXq1eP222/n/fffL3Qz4oiIiPgOP2zSUw4om+myLBCaw2UZx/XsLgtmqR0rDkgA4h0l4YySeMb1xDOuJwJJjpL5evp2suN6cqb9yW6UVOzQIjeUc4Se/vWqDtDOcT0RAvcG8sX/1eTGX/9w+1XxBYUmqZLzExMTw8qVK3n66adp1KgRQUFB7N+/n/Hjx/P222+7HPvGG2/wxhtvuOz7/PPPGThw4Hmd+8zp7c9HuXLlOHz4MGlpaZw+fZrt27czadIkPv74Y5dkb/jw4Rw9epRhw4bRsGFDTp48ydq1a7M8R3fs2rWLlStX0r59e4YMGeJy25tvvknDhg1ZsGABcXFxfPHFF8yaNcu5bhrAhx9+yKRJk/j3338pXbo09evXz/Fc3377LUOHDuW1115z2R8fH0+XLl147733+PHHH50zzyxatIjTp097/JxERESKjkCgvKNUyHQ9fbuc43rmy/Tr6YlSfiRCsUBMpsv067HYZOjM63FnKfHZFNchDYWGH1ATaAY0BaoDPwIbHbdvw74t24G9UKp0KbpM7eKNSHPFD7fTyuIhNDSU06dPU7ZsWZcv7fXq1eONN95g+PDhzinV85u/vz8XXXQR69atIy0td2OqpHhQncngjc9sYZTT3zyR7Ki+iKfOXmdKAxWBSo7LM69XxCZJFTJdr4htHcoLycApR4nOdHn6jMv06zGO65kv06/Hoa/UmQQCjbFJVFNc37I0YBmwOPu7+trfGXfjUUuViIiIiORSSaAKUNnlMjGxFgMHQnz8RGxLUSXHbZUc98mN00AUcNJR0q+fynR55vX0chrbLU7yTAC2FyDYt/aeTLclAjuxrVE7yc953rxGSZWIiIiInCEIqApUc1ymlyrZXFYhpyVmkpLg888Bbs/hPEnACeD4GZcnsElS+uWZ10+R8Q1evMIPqEFGt744YIrjthhgC7YRbwewlyL/dimpEhERESkWMidK1TOVapku00uF83j8JOAYcNR5GRQUzbBhj/Lee8+TmBjh2H88U4nJ1TOSAhYINMQmUc2ww9PSpWCrWPocZTMKNjRvU1IlIiIiUqiVxDYZ1HSUGjmUyh4+bgpwxFEiscnSkTMu068fw3apOyOykqG89tqjjB49jsRE74+PkVy6E5tMpUvCdufbAfxHRkJVDCmpEhEREfFJ/thWo1pnlJpnlIoePGYyNkGKBA5nKpGZSnoSFYUmXyimqmCTp2bAdGw3PrAJVHXs2KgdwB5s7i1KqkREREQKXgD222kdoHY2l7WxrUvuflWLAyKAQ2eUw2dcP44SJcnCH6hLRiKVOU9vBqx2XF8D/FOwoRUWSqpERERE8lwFoB42SaqbzWVN3PsalopNhg5gk6bM5WCmojUM5TzVAe4DSmXalwLsxrZIbc+0v3iv1nJWSqpEREREPFYNqO8o9bIpoTndMZNkbHK0H5s0pZf9jv0HsN3wivi0aVJwypExU98Wx76jQAnsesQ7sEnUbux4KXGbkioRERGRLCoBDbBTnTUgI4Gqj02aSuVwv8wisQnSXsflPkdJvx6JuuJJvquO7cLXHNujFGwVTE+qEoBx2KF0qo7nTUmVADBixAhuvfVWLrroogI5X7169dizZw8XXnghGzZsKJBznkt4eDgfffQRH3/8MQDGGG699VZ++uknL0dWsM58HfJLWFgY5cuX57bbbsvX84iIZC8Ymxw1cpSGZCRQDYCy57h/KrYlaS92tP4+x/X0sg8tLitedRVwAXbN5XQGWzW3nXFsZAHFVIQpqSriZs+eTVBQENddd12W26644gqWLVtGmzZt+PDDDxkzZowXIsxZ/fr1eeutt+jWrRsVK1bk2LFjrFmzhqFDh7J9+/Z8T8yqV69OVFTUed8/Pb7U1FTq1q3LwYMHXR57//79BAYGUr9+ffbu3evW81m8eDHdunUDIDExkWPHjrF27VrCwsKYOXPmecea2aWXXkpsbGyePBbknEAPHjwYPz+/PDuPiEhWIdiEqXGmkp5E1cGOzj+bCCA8U9njKOHYhErTnomPKIFtRM08/qkSNqFKBnZhE6kd2K5/kueUVBVxX331FT/88AO1atUiIiLC5bZ+/frxzz//sGnTJoA8/SKdW4GBgfz2229s376d22+/nUOHDlG7dm2uu+46ypcvXyAxREbmzc82ERERPPjgg7z77rvOfX369CEiIoJ69ep5/HhffPEFr776KoGBgdSuXZvbbruN7777jokTJzJw4MDzjjMoKIjk5GSOHTt23o/hidOnNahaRPJCaWyy1BRo4ijpCVSNs9wP7CCSXY4SnulyN7a1SS1N4sPKkjFbXwPshJJjsBM8AvwFrMNWZ+X/+e5cP9GIO4LOUs5MWz041gQZUgNSMUHG9TgPzJ07l6NHj9K3b1+X/SEhIdx555189dVXgO3+t27dOuftXbt2ZeXKlcTExBAVFcXy5cupW7cuQLatIqNHj2bx4sXO7WuvvZZly5YRFRXFsWPHmDNnDg0bNnQ77latWtG4cWMGDRrEypUr2bdvH3/99RfDhw9n5cqVAOzZsweA9evXY4xxnn/x4sWMHj3a5fFmzpxJWFiYc7tKlSrMnj2buLg4du/ezX333ZclBmMMt9xyi3P7ggsuYNGiRcTFxXHs2DHGjRtHSEjIOZ/LpEmT6Nevn8u+fv36MWnSJPdejDPExcURGRlJREQEK1eu5MUXX+Sxxx6jf//+9OjRw3lc7dq1mTZtGlFRURw/fpxZs2a5JHHp7+NLL71EREQE27fbn7fCw8MZPHgwAFOnTuW7775zOX9gYCBHjx7lgQceAM79Xuf0PmWuR48++igRERFZWq5mzZrlrKMAN998M2vWrCE+Pp5du3bx6quvEhAQcF6vo4gUJoHYpOlG4FnsAJDfsa1FscAGYAbwNtAP6ExGQnUU+BuYAowE7gcux040UQZoC9zueNyxwDzsz/1KqMQHlQe6AI8CzwA3YH8/CMCuv1wm07H7sS1TSqgKhJKqvPDyWcpdZxz7/FmOvd/1UPOUYf316zHDjOtxHkhNTWXy5MlZkqo777yTgIAAvv322yz3CQgIYNasWSxZsoQ2bdrQsWNHvvjiC4xxf/RiSEgIo0aN4pJLLqFHjx6kpaUxc+ZMt7t7HT16lNTUVO644w78/bOvppdeeikAPXr0oHr16tx+++1uxzdx4kTq1KlD9+7dueOOOxg0aBBVq1bN8fjSpUuzYMECoqKiuPTSS7nzzju56qqr+PTTT895rtmzZ1OhQgU6deoEQKdOnahQoQJz5sxxO95zmTRpEidOnHCOTwoMDGTBggVER0fTuXNnOnXqRExMDPPnzycoKCMz79GjB82aNePqq6/mxhtvzPK4U6dO5aabbnJJHq+99lpKly7tTIjO9V678z7NmDGDSpUq0b17d+e+ChUq0LNnT6ZOnQrY7qqTJ0/m448/pmXLljz22GP07duXl1/28EMhIj6sItAJeBj4APgJ22cpDpvozAE+BPoD3bEL4YL9Nvk3MBkYDtwDXIKd1rwqNol6EHgNmOo49khBPCGR3PHDDv9LVw24Elv108dH/YZtofoU28AqXqHuf8XAhAkTeOGFF+jatStLliwBbEvJDz/8kG0XrLJly1K+fHnmzp3L7t27Adi27cwRjWf3448/umw/9NBDHDt2jJYtW7Jly5Yc7pXh4MGDPPXUU7z//vuMGDGC1atXs3jxYqZOnUp4eDhgEy+A48ePe9RVr0mTJlx//fVceumlrF5tV7N7+OGHz/oc77vvPkqWLMmDDz5IXFwcW7Zs4YknnmDOnDkMHTqUI0dy/s85OTmZr7/+moceeog///yThx56iK+//prk5GS3Yz4XYww7duygfv36ANx99934+/vzyCOPOI/p168fJ0+epFu3bvz222+A7fL5yCOP5BjLggULiI2N5bbbbuPrr78G7Gsxe/ZsYmJigHO/1+68TydPnmTevHncd999/P777wDccccdHDt2zNmyNWLECN59910mT54M2Ba14cOH8/777/P66697/JqJiLf4YddpauEozTNdVjnL/eLImO95R6byH3D+419FfE4gdthfc2wD7RpswyzYHqpbsdV+O7ahVnyCkqq88NZZbjuzcecD94/1+8SPtm3bsmHDBkza+c9xuX37dueX+SVLltCoUSO6dOninPDgTFFRUYSFhbFgwQJ+++03Fi5cyPTp0zl8+LDb52zcuDGvv/467du3p3Llys7Wprp167qVVAGMHTuWyZMn061bNzp06MCdd97JSy+9xM0338zChQvdjuVMLVq0IDk5mTVr1jj3bd++/ayTUrRo0YINGzYQF5cxuvPPP/8kICCAZs2anTWpApvY/vXXX7z00kvceeeddOzYkcDAvP34+fn5OVsT27RpQ+PGjYmOjnY5pmTJkjRq1MiZVG3atOmsyV1qairTp0+nd+/efP3115QuXZpbbrmFe+65x3lMXrzXYFvFxo8fz6BBg0hKSqJ379589913zufUtm1bOnXq5NIyFRAQQKlSpShVqhTx8fFun0tECoI/dja9lo7SItPl2bpO78W2Tm0/o0Sg+Z6lyCqFTaCaYxOqzK1T9TNdTwGmFVxY4j4lVXnBkwYHD471S/YjIDUAv2S/XCVVYCesGDNmDI8//jj9+vVj586dzlar7Dz00EN88skn9OzZk7vvvps333yTq6++mpUrV5KWlpalG1/mLmUAc+bMYe/evTz66KMcPHgQf39/tmzZQnBwMJ6IiYlh7ty5zJ07l1deeYUFCxbwyiuvnDWpcie+grZ582a2bdvGt99+y9atW9myZQtt27bNs8f39/enSZMmzpa3MmXKsGbNGnr37p3l2PSWI3BvcpKpU6eyZMkSqlSpwtVXX018fDzz58933p5X7/WcOXPw8/Pjhhtu4J9//qFz5848/fTTztvLlCnDiBEjsrSMASQkaOyDiPf4YUfJtzqjNCfntZySsK1MWx1lGxlTk+mndylm/IDHcR0PdZKMj8U+L8QkHlNSVUxMnz6djz/+mPvuu48HH3yQ//3vf+e8z/r161m/fj3vvvsuf/31F/fddx8rV67k6NGjXHDBBS7HXnjhhc4Wj4oVK9K8eXMeffRRli9fDuAcT5Rb27Zt4/LLLwcgKcku9X3mRAVHjx6lRo2MGZ/8/f254IILnN3Itm3bRlBQEO3atXMmIU2bNqVChQo5nnfr1q307duX0qVLO1urOnXqRGpqqnOCh3OZMGEC//vf/xgwYICbz9Z9ffr0oWLFis6EY926ddx1110cOXIkS2uVp/7++2/279/P3XffzXXXXceMGTNISbGjXt15r3N6n86UmJjIjz/+SO/evWncuDHbt293mTxl7dq1NGvWjF27duXq+YjI+UtLq8rChZCU9Dh2lr3W2NannFqe4rDfCv91lK2Oy13YdZ5Eipmq2N8bagPfOPYZYCd2kd70RMr9zkHiI5RUFROxsbFMmzaNd955h7JlyzJx4sQcj61fvz79+/dn9uzZHDx4kGbNmtGkSRPnWJbff/+d559/ngceeIC///6b+++/nwsuuMD5BTh9Frj+/ftz6NAh6tat6zKduDvatm3La6+9xpQpU/j3339JSkqia9euPPTQQ7z33nsAHDlyhLi4OHr27MmBAwdISEjg9OnT/P7774waNYrrr7+eXbt28cwzz7hMw75jxw7mzZvHuHHjGDhwICkpKXz00UcuXfvONHXqVF577TUmTZrEyJEjqVKlCmPGjGHKlCnn7PqXbvz48cyYMYOTJ0969FqcqXTp0lSrVs1lSvWnn36asWPH8scff3DRRRcxdepUnn32WX766SdeffVVDhw4QL169bj99tt5//33s0yvfy7ffPMNAwYMoGnTpi6TSbjzXuf0PmVn6tSpzJ07l1atWjnHcKV7/fXXmTt3Lvv27eP7778nLS2Ntm3bcsEFFzB8+HCPno+InEtJbLLUBjs7XhugNbGxVbj6aoB3zjg+AftNcEumshm7plNawYQs4ov8sAlU+tDBipluqwEcclyfg35nKAKMSkYJDQ01xhgTGhrqsr9evXpm8uTJpl69egUWi7+/v2nXrp3x9/fPk8fr0KGDMcaYuXPnZrltxIgRZt26dQYwVatWNT/++KOJiIgwCQkJJjw83IwcOdL4+fk5jx85cqQ5dOiQiYqKMv/3f/9nPvnkE7N48WLn7T169DBbtmwx8fHxZv369aZLly7GGGNuueUW5+tpjDFt27bNNtZKlSqZjz76yGzcuNGcPn3anDp1ymzYsME888wzLnE8/PDDZu/evSYlJcV5/sDAQPPZZ5+ZY8eOmcOHD5uhQ4eamTNnmrCwMOf9qlWrZubMmWPi4+PNnj17zP3332/Cw8PN4MGDncdkjhcwF1xwgVm0aJGJi4szx44dM+PGjTMhISE5vt7neo5t27Y1xhhnnTrX8YBZvHixSZeQkGAiIiLM7Nmzza233pqlzlSrVs1MnDjRHDlyxMTHx5udO3eacePGOet2WFiYmTlzZpZznPk6AKZ58+bGGGPCw8OzHH+u9zqn9ym78/v5+ZmIiAhjjDENGjTIcq5rrrnGLF++3MTGxpqTJ0+aFStWmEceeSTH17+gP7OFseT0N0+lOJWaBq43MMzAdwb+NZBiwGRTUk2TJsYEBs4yMMJALwNNDQT4wPNQ8cVSrP/GtMLwHIaRmcorGO7FcBGGkj4Qow8WX6szHsTj/WB9qRTlpEql6BfVmYyipMq94mv/eankZwkw0MrAfQbeN/CrgSOGbJMnY+CogYUGRhnoa+BiU6ZMVdUXFY9KsfkbE4yhJYaqmfY1wCZSL2K43XF7sA/E6uPF1+qMu/Go+5+IiEiRUwI73uli4CLHZWuynzgiBdt1b4OjbHRcZh3U4ecXmj/hihRGpYFmZMzYFwisANLnctqLXXN6D+raVwwoqRIRESnUSmHHPV0CtMMmUC3J/r/4aGA9Nmla7yhbsGOiROScArAfsxZAPezKAemOA5mHDKdh52SRYkFJlYiISKFREriQjASqHTaBym52zaPAOmBtpstd2J4qIuK20tiJLMG2OHUCyjm2D5GxKoB781ZJEaWkSkRExCcFAhdgE6hLHeUCILt19w4BaxxlraMcKJgwRYqi6tjWqBbY9aM+JGMiyz+xLVTbsOtJiaCkym3G2F/2AgP1kokUBumf1fTProjvawC0d5TLsGOhshsDdRhYTUYStZqMeZlF5LykT32enkhlXroyFbu+VPoww1UFG5oUDsoQ3HT8+HEAmjdvrsVHRQqB5s2bA3Ds2DEvRyKSnfLYxKmD4/IyoEo2x53EJk3/ZCpqgRLJc12A7pm2k4H/sF37/kPDDuWclFS5KTY2lj/++IO77roLgG3btpGSkpKv5/T396datWrUq1ePtDQtnijnpjpjW6iaN2/OXXfdxR9//HHWRZ1FCoY/0AqbQHUAOmJ/Cj9TInbiiJXYn8JXATvRGCiRPBQINMQORdxExkQS/2E/mtuxidQubGIl4iYlVR4ICwsD4O677y6Q8/n7+1OnTh32799fbL8gi2dUZzL88ccfzs+sSMEqh02eLneU9kB2U5H/h51/OT2J2gAkFVCMIsVIMNAYm0g1wa44APb3ivSk6iDwAZr6XM6bkioPGGOYMGEC3333HZUrV8bPzy9fzxcSEsKaNWsYOHAgsbGx+XouKRpUZ+zn9NixY2qhkgLUGDsdWHoS1RLXeZbBTmW+EptE/e24frwAYxQphoKAXtg1pDLP73IK2xq15YzjlVBJLiipOg9xcXHs27cv388TGhpKyZIl2b9/P9HR0fl+Pin8VGdE8lsgdkrzKzKVatkc9x/wF3aasBXYb2/Fu/VYJN+Vxn4cwx3byUAlbEJ1AvgXm0xFeCU6KeKUVImIiOSoFLYrXxdsAtUBO79yZonYCSTSk6i/sWtEiUi+K0PGjH31gRTgfcclwC/YNaYivRGcFCdKqkRERJzKYrvydXGUS8m6LtQJbPK03FHWYBMrESkQodheti2Butjp0NMdx36MTzi2wxEpEEqqRESkGCsPdAa6OcqFZB0PtR9Y6ijLsf2HNCOfiNe0Ba7KtH2AjK59UV6JSERJlYiIFCflOXcS9R+whIxEam9BBScimVUko0Xqb+wU6GCTpyZkJFKnvRKdiAslVSIiUoSFYJOoKx3lIrImUduAPxxlKXCo4MITEVeVyUikqmfan76uFNgufloxQ3yMkioRESlCSmAnk+iBTaIuI+uYqMxJ1BLgcMGFJyLZCwQexXUyzTTsmKh/sR9bER+mpEpERAoxPzIGWFyNbZUqdcYxu4HfHWUxSqJEfEBVbEvURsd2Cnbt61TsRzY9kYr3SnQiHlNSJSIihUw9bAJ1FbZFqvIZtx8CFpGRRO0pyOBEJBsGYxOpVtiufFWwCdQOIMFx0GzsOtkJ2T6EiE9TUiUiIj6uDHZSiWuAa4GmZ9weje3KtxD4DTtyXUR8QVqFNIb/Ppy4fnF24ol0KcAubMNyehKl5d2kEFNSJSIiPsYPOytfT2widTkQnOn2FGAFNoFaCKwiY6VPEfE6P5yrDqTUS+HNZW/ahCoF2Int2rcdLe8mRYqSKhER8QGVsAlUT2xrVLUzbt8J/Oooi9EcyiI+Jr1rXyvs9Odr7O7AHYH0HNSTBaMWkLAxQYmUFFlKqkRExAv8gUuA6xzlUlynOo/GjotagE2kdhd0gCJyLpkTqcxDG1vgTKr84/yZdc8syvYvS0KiBktJ0aWkSkRECkg5bCvUDdhEqsoZt28A5gPzgL+A5AKNTkTc5A/0x3UdqfSufVuwk0+IFDNKqkREJB+1xCZRNwCdcP1v5xR2XNQ8bDJ1sMCjExE3VALqAusc22lADBojJZKJkioREclDQUAX4CZHaXjG7f8CPzvKn2iCCREfVZGMrn3pLVI7sT1zAX4BYlEiJeLgf+5DfEfnzp2ZPXs2ERERGGO45ZZbXG4PCwvDGONS5s2b56VoRUSKiwpAb+A77JzIC4HB2IQqAdsS9QTQAPsN7QVgCUqoRHxMWWyDcn/gKewycNWx60n9B5TMdOwJlFCJZFKoWqpCQkLYsGEDEyZMYObMmdkeM2/ePPr16+fcTkzUJ15EJO/VB25xlM64/ncSCcwF5mC798UVdHAicj7qY9fVBtvFbzd2jNQ2IN5LMYkUEoUqqZo/fz7z588/6zGJiYlERkYWUEQiIsXJRdgk6lag7Rm3bcQmUbOBf3AuUiMivqcMdrhjK2zC9Ldj/3ZsF7+tjqLfQ0TcVqiSKnd069aNyMhIoqKi+P3333nllVc4ceJEjscHBwdTokQJ53ZoaKjLpTf5UixSOKjOiKfOVmeM8Sc1tQMpKTeTknIjxtTNdGsqAQF/Ehj4C4GBP+PvvzfTbWXyN2jxGv2NKbzSSqWR0jiFlGYppNZJdQ4A8S/hT8jmkIwDf3JcBgB58DarzoinfK3OuBtHpjWvCxdjDLfeeis//fSTc9/dd99NXFwc4eHhNGrUiLfffpuYmBg6duxIWlpato8zYsQIRo4cWUBRi4j4tqQk+P13+PFHmDULjh7NuK1UKejZE265BW64ASpXzvFhRMRHGGO4Y8YdzN4+m5S0jHGMl9W6jHta3cOdre6kdtnaXoxQpHAoW7Ys0dHROd5epJKqMzVo0IDdu3fTo0cPfv/992yPya6lKiIiglq1ap31hSsIvhSLFA6qM+Kp0NBQdu6MoFath4iOvpaUlGuB8pmOiCIwcB6BgbMJDFyMn58GVhRn+hvj+0ygIbVOKoHhGZ2R4m+MJ6VZCv5H/AncFkjQ9iD8TxfMXGWqM+IpX6sz6fGcK6kqct3/MgsPD+fo0aM0btw4x6QqKSmJpKSkLPujo6N94o0E34pFCgfVGTm3ksB1xMffR9WqEBMzIdNth4GZwI/AH6SkpJCiifokE/2N8TEBQGPgAqAZEAx8Chxz3L4Q+BXSjqeR5PhX0FRnxFOFrc4U6aSqVq1aVKpUiUOHDnk7FBERH1AKuB64A7gRKENKCsTEgJ/ffoz5HvgeWIGd+ktEfJY/dpWCC4AWZJ3uPJSMpOooIpLPClVSFRISQuPGjZ3bDRo0oG3btpw4cYITJ04wYsQIfvjhBw4fPkyjRo14//332blzJwsWLPBi1CIi3hQM9ATuBm7GdRKJvQQF/cTSpU9x9dUXEBNz2isRish5aALcm2n7NHb6881AhFciEinWClVSdckll/DHH384t0ePHg3AxIkTGThwIG3atKFPnz6UL1+egwcP8uuvvzJ8+PBsu/eJiBRdgcBV2ETqNqBcptv2ADMc5R9KlgylQ4en8PMrlMNrRYqH6kBr4BSwyrFvF3Acu5bUZmAfhXSUvEjRUKiSqiVLluDn55fj7T179izAaEREfIkfcAXQG9u9r1Km2w4A04FpZHwjExGfVgnbta81kD7T5nEyPsIpwBgvxCUi2SpUSZWIiJypLXAfth9QnUz7I7GtUdOAP9FP2CKFRDtHqZlpXwqwA9hEIZ63WaRoU1IlIlLo1McmUr2Blpn2nwR+AL4F/gBSCzguEfFYKSDzSgW1sQlVGraL32ZgG5BY8KGJiPuUVImIFArlgbuAB7Dd/NIlAHOBqcA89M1LpBAIwk593ho7FfpXwEHHbf9gJ5r4F4jzSnQich6UVImI+Kwg4DrgQewU6OkLlacBi7CJ1EzstF8i4tP8gUbYRKo5dmLOdA3ISKoOZrouIoWGkioREZ9zCdAXO3tf5Uz7NwJTgG/Qty6RQqQS8BAQkmnfCWzXvk1oHSmRIkBJlYiIT6gO3I9Nplpl2n8Q2yI1BfvtS0R8XhVsj93/HNsnsA3MMdi1pDaitaREihglVSIiXlMCuAmbSPUEAhz747ETTkzGdvNL80ZwIuKJstgp0NtgfyOJBkZhZ+ozwEQgCn2cRYooJVUiIgWuDfAwtmWqYqb9y7HfvGagcVIihUBJ7AScrbGTcqYvpZmKbYkqRcZkE8cLOjgRKUhKqkRECkQ57FpSD2PHTKXbD0zCtkr9l839RMRndQU6Ztreg+2l+y+u06SLSJGnpEpEJF91xSZSd2B/tgZIAmZh51FeiPoDifg4P6AutpF5A7DPsX8Tdka/jY7rp7wSnYj4ACVVIiJ5rhJ2nNSj2MVo0m3CJlJfo75AIoVAFWwi1Qbb2Aw2wUpPqg4CY70Ql4j4HCVVIiJ5phvQH7idjDWlooFvgS+xq3qKiE8LAC7DJlI1Mu1PwHbrW++FmETE5ympEhHJlfRWqf5A00z7/wHGAd8BsQUfloi4zw87Qx/YSSYuxc4hk4od6rgR2AGkeCU6ESkElFSJiJyX9sAg4C7sFGBgZ+ybCowH1nkpLhFxix/QENsiVR/4BJtEASwFgrBrSsVld2cREVdKqkRE3FYauA+bTF2Uaf8a4H+oVUqkEKiOTaRaA6GZ9jfCtkaBuviJiMeUVImInFNTbCLVl4zR6vHYJOp/aKyUSCFQD7geqJZpXxywGdu974A3ghKRokJJlYhItvyw38CeBK7NtP8/4HPsIr0nCj4sEXFPEHYVg/R1tGOxCVUKtkVqA7CTjC5/IiK5oKRKRMRFOeAh4HFsfyCw60jNBT7Fritlsr+riHiXH3Z8VFugJTZ5+t5x2zFgOrAbO5OfiEgeUlIlIgJAc+Ap4EEgxLEvCruu1Fgg3Etxicg5VcYmUpnXkwK7zlTmmf3+LeC4RKTYUFIlIsXcNcDTQM9M+zYBY7Az+WnqLxGfditwYabteOw4qQ1onJSIFBglVSJSDJUEHgCGYPsIge3iNws7r/ISr0QlIucQADTBduFLcuyLxI6L2omdtW8HGiclIgVOSZWIFCPVgSeAx7D9hcCOYv8K2zKlLn4iPqkGtjXqAmzv3JnYliiAtdjZ+7SagYh4kZIqESkGWgLPAvcDwY594dhWqa+AaC/FJSI5KoMdI3UhUDXT/mhsi1W6REcREfEiJVUiUoR1A54Dbsi0bzkwCvgJ2+VPRHxOaexQx/TkKQXYhu3etxt9dEXE5yipEpEiJgDoBTwPXOLYlwb8CPwfsMJLcYlIjmoBNclYRzsO2Iv9lrIB2IKmQRcRn6akSkSKiJJAX2wy1dCxLx4Iw7ZM7fJOWCKSvVAyuvdVwf72sRWIcdz+DbaFSkSkEFBSJSKFXDlgIHYmv2qOfUexC/WOxa74KSI+IQBoBlyEXVvb37E/GZtQZf5WooRKRAoRJVUiUkhVwyZSA8lY7XMP8CEwAdtKJSI+5ULgpkzb+4B12EV5NdmEiBRiSqpEpJCpBwwF+mG7/IFd6fNdYBr6eVvER4Rgu/edxLZCgR0bdbnjcj1wwhuBiYjkPSVVIlJINAaGYRftDXLs+wt4B/gZMF6KS0Sc/LGL814INMV29ztARlKVgF0STkSkiFFSJSI+riXwEnAPGfMr/wq8BSz1VlAiklkVbCLVFru+VLoD2BYpEZEiTkmViPiotsArwB2Z9s3BJlMrvRKRiOTgamzLFNjZ+zZix0od9VpEIiIFSkmViPiYNsBI4LZM+34A3kQ/eYt4l8GwZM8S4nvG2wbjU44b1mCnRF8H/IcW5xWRYkdJlYj4iNbACOzCvWC/lU3DJlP/eisoEQG7ptSFEHtxLN0mdYNWwBFgieP27Y4iIlJMKakSES9rhU2m7nRspydTb5Axul1ECpw/GWtKNbbbBkNocCgJqxNI3pHs3fhERHyIkioR8ZJmwGvYZCp9BdBpwOuoZUrEBwRie+EGO7b3Qsl/S3Jo6SFqVK5BcrSSKhGRdEqqRKSA1cO2TD1Ixmx+M7AJ1hZvBSVSvAVjG43rAbMc+5LImBNmHXACgkKDCAkOKfj4RER8nJIqESkg1YGXgf5k/PQ9C3gV2OSlmESKuVrAxcAFQAnHvlXAQcf1Rd4ISkSk8FFSJSL5rCLwAvAkUNqx7zfsdOmrvBWUSPFVCjvJ5sVAtUz7jwNrgZNeiElEpJBTUiUi+aQ0MBgYCpRz7PsL21r1h5diEhFqA9c5ridjhzCuBfZ6LSIRkUJPSZWI5LEA4CHsWlM1HfvWY1umfvZOSCLFVRngQuz4qPSG4V3YtaR2YHveJnglMhGRIkVJlYjkoduAt4Hmju3d2GTqO8B4KyiR4sUPOwX6xdhJNv2BaGA1dsWCNGCq16ITESmSlFSJSB64Angf6OjYPopdZ2oc9idyEcl3ZbGJ1EVk9LgF2Ift3ufnjaBERIoHJVUikgtNsMnUrY7tWOD/gA+xP42LSIHpDFzquB4HbMAmU0e9FpGISLGhpEpEzkNF7FTog4AgIAX4ErvW1GEvxiVSTJTHtkhtJ2P68zVAZcflNuzHUkRECoSSKhHxQDDwBHacVAXHvrnA89hvcSKSb/yBpsAlQCNsd75yZCzWexiY5JXIRESKPSVVIuKmXsB72G9zYGf0exb43VsBiRQP5YB22Jap0Ez7d6HfMkREfIS/twPwROfOnZk9ezYREREYY7jllluyHPPaa69x8OBB4uLi+O2332jcuLEXIhUpSi4ElgDfYxOqg0A/7Lc8JVQi+a4P0AWbUMUAy4GPgSkoqRIR8RGFKqkKCQlhw4YNPP7449ne/sILL/DUU08xYMAA2rdvT2xsLAsWLKBEiRIFHKlIUVAZ+Bw7QKMLduT7SGz/o4nYeZlFJE+VxU44EZBp31psq9R0YDSwEIgq+NBERCRnhar73/z585k/f36Otw8ZMoQ333yT2bNnA/Dggw8SGRnJrbfeyrRp0woqTJFCLhB4HJtAlXfs+wYYChzwTkgiRVn6ulKXYCfU9AeOA/86bl/uKCIi4rMKVVJ1Ng0aNKBGjRosXLjQue/06dOsXLmSjh075phUBQcHu7RkhYaGulx6ky/FIoVDbutMSsqVJCa+S1qaXbzX3389JUq8QGDgivQz5EWY4kP0d8Z70kLSSL4gmeQ2yZiyGYtjB+wLIJhgAkN9779o1RfxlOqMeMrX6oy7cfjeX+zzVL16dQAiIyNd9kdGRjpvy86wYcMYOXJklv0RERF5Gl9u+FIsUjh4Wmf27IEhQ+Cnn+x25crw9tvw0EMXEhDwa57HJ75Hf2cK1v5T+2n4SUNS0uy85xVKVqDvhX3p364/zSs393J056b6Ip5SnRFPFbY6U2SSqvP1zjvvMGrUKOd2aGgoERER1KpVi+ho7y5e6kuxSOHgaZ0xJpikpKdISnoOKA0kExQ0joSE93j22VM8+2y+hyxepr8zBcOUNKRWTyVwT8Z/u2n3pOHv50/w+mCS/0vmy5Qv+ZIvvRjluam+iKdUZ8RTvlZn0uM5lyKTVB0+bBccrVatmvN6+vb69etzvF9SUhJJSUlZ9kdHR/vEGwm+FYsUDu7VmauBT7ETTwAsBp4gOflfkpPzNTzxQfo7k09qAZcCrbBjp/4PiHfcNglIggQSvBTc+VN9EU+pzoinCludKVSz/51NeHg4hw4dokePHs59oaGhtG/fnr///tuLkYn4mtrADOBXbEJ1CLgPuJKMkfEict6CgIuB/sCj2FUJgoBj2Nn90mX9PU9ERAqpQtVSFRIS4rLuVIMGDWjbti0nTpxg//79fPTRR7zyyiv8999/hIeH88Ybb3Dw4EFmzZrlvaBFfEYgMAQYAZQBUoAxju3C80uQiE9rCNwFlHRspwBbgH/Q5JkiIkVYoUqqLrnkEv744w/n9ujRowGYOHEi/fr14/333yckJIQvvviC8uXLs3z5cnr27EliYqKXIhbxFZcBXwBtHdvLsNOmb/JaRCJFgj8QQsbvEpHYVqkTwGpgPXaJNxERKdIKVVK1ZMkS/Pz8znrMiBEjGDFiRAFFJOLrygJvAYPIWPzmOezivSJy3soA7RzlOHZ8FEAs9veLI4DJ/q4iIlL0FKqkSkTcYwzA7cAn2JHyYBOp57EDO0TkvNTDTjzRAghw7PPHdvdLn28iMpv7iYhIkaakSqSI2bcP4uOnAdc59uwABmBn9xOR89IMO5dLtUz79mHHSv0LpHojKBER8RVKqkSKDD+SkvrTsiWkpl6HnVrsXeBtQOMKRXKlBDahSsIORfwHOHzWe4iISDGipEqkSGgCfEViYmcSEyEg4G9SUx8Gtno7MJHCxQ+70sBlwH/ACsf+LdgufhuhEC4rJSIi+azIrFMlUjwFAC9gv+l1BmL49FMoVaonSqhEPFAa6AQMBu4FGmHHTqVLBVahhEpERLLlcUtVcHAw7du3p169epQuXZqjR4+ybt069uzZkw/hiUjOWgMTgEsc2wsICXmGxx/fwrBhmnZMxC3VgfbYj1P6/4hxwFrslOgiIiJucDupuvzyyxk8eDA33XQTQUFBnDp1ivj4eCpWrEiJEiXYvXs3X3zxBZ9//jkxMTH5GbNIMRcEvAy85LgeBTwNTMLfP9SbgYkUPlcAFziuH8S2Rm3GLtorIiLiJre6//30009MmzaNPXv2cM011xAaGkrlypWpU6cOISEhNGnShDfffJMePXqwY8cOrrrqqvyOW6SYugBYCYzAJlQ/Ai3JWCRHRHIUAnQBKmXatxI78cSX2PWl1qOESkREPOZWS9XPP/9Mr169SEnJ/n+a8PBwwsPDmTx5Mi1atKBGjRp5GqSIBGDXmHoNCMauNTUImOHNoEQKh5rYLn6tsP/rhQDzHLftdxQREZFccCup+uKLL9x+wK1bt7J1qwbIi+SdptiWqA6O7Z+A/sARr0Uk4vP8sQv0dgDqZNp/ANjjjYBERKQo05TqIj7LD3gSu9ZUKeAU8BQw2ZtBiRQO/bGTUICduW8zdrxUhNciEhGRIsztpOrEiRMYc+4ZxSpVqnTOY0TkXOpgW6e6O7Z/BR7G/swuIllUBY4C6f9N/QeUwS7SuwbQ/EkiIpKP3E6qhgwZko9hiEiGe4D/AeWBWOA54HNvBiTim/yAZtgufvWBKcAux23LgT+wrVQiIiL5zO2kavJkdTkSyV9lgc+A+x3bKxzXd+V4D5FiqSRwEXAZUMGxLw2oRsbHJdELcYmISLGlMVUiPqET8DX25/ZU4E1H0dzOIk5BwFXYhCrYsS8O273vH+C0l+ISEZFiz+2katcu934tb9So0XkHI1L8BAKvYhfyDQB2Y1un/vZmUCK+KRn7u0MwdvLLFcBG9NuDiIh4ndtJVf369dm7dy/ffPMNR45oKmeR3GsIfINdQAfsxBRPAtFei0jEZwRi17q+EJiKTajAztlisL8/iIiI+Ai3k6q7776bhx56iGeeeYZ58+YxYcIEfvnlF7dmBBSRM90JfIkdRxUFPIYW8hXBLsx7CXApdvY+gLbAasd1DTEUEREf5O/ugd9//z3XX389jRs3Zs2aNYwePZr9+/fzzjvv0Lhx4/yMUaQIKYmd2W86NqFajv3GqIRKirkqwM3A09iVBMpgl2b7FbvGlIiIiA9zO6lKd/DgQd5++22aNm3KfffdR/v27dm2bRvly5fPh/BEipLmwEpgAHaqsjeBbsB+L8Yk4gNCgYHAxdj+ExHY3xk+Bv4CErwXmoiIiDvOa/a/EiVKcMcdd/DQQw/Rvn17ZsyYQVxcXF7HJlKE9MFOlx4CRGIno1jo1YhEvCYAqAuEO7ajgW2O63+j3xlERKTQ8Sipuuyyy3j44Ye566672L17NxMmTKBXr16cPHkyn8ITKexCgLHAg47thdiEKtJrEYl4TWnseKnLHNc/AU46bpuBnYBCRESkEHI7qdq8eTNVq1blm2++oWvXrmzcuDE/4xIpApoBPwCtsGtPvQq8i+36J1KMVAI6YGfyC3LsOwWUJyOpUkIlIiKFmNtJVYsWLYiNjeXBBx/kgQceyPG4SpUq5UlgIoXbHcAE7GCRg8Dd2EkpRIqRcsB12N8X/Bz7DmK7+G1Bvy+IiEiR4XZS1a9fv/yMQ6SICATeA55xbC8G7sGuVCpSzCRgF+v1A7Zjk6k9XoxHREQkn7idVE2ePDk/4xApAmpgp0q/wrH9LvAKtuufSBEXhO3eV5+MFQISgdnY3xSOeSUqERGRAnFes/+JyJm6AtOAatjBIn2An7wakUiBCMFOPHEpdvIJsAv1ps/s9683ghIRESlYbq1TtWXLFu6++26CgoLOelzjxo0ZO3YsQ4cOzZPgRAqHp7Gz+lUDNmKnN1NCJUVcReAGYAj2N4XSwAngZ+CA98ISERHxBrdaqp588knee+89xo4dy2+//cbq1as5ePAgCQkJVKhQgZYtW3LFFVfQqlUrPv30U/73v//ld9wiPqAkMI6M6dInYxf2jfdaRCIFohbwCBmTTxzALtK7Fc3iJyIixZJbSdXvv//OpZdeSqdOnbj77rvp3bs39erVo1SpUhw7dox169YxefJkpk6dqjWrpJioCczE9ntKwU5MMcarEYnkGz/s9OdRju2D2DFSUcCfwF7vhCUiIuIrPBpT9eeff/Lnn3/mVywihUQH4EfsxBTHgTuxs/yJFDH+QGugE7Z730fY3xAM8AWQ7LXIREREfIomqhDxyEPAWKAEdvzUrWSMyBcpIoKAi4GO2BYqsDP5VSdjvJQSKhERESclVSJuCQRGAU86tr8H+gKx3gpIJO+VAtpje7Wmz+QXA6zAzuiX4KW4REREfJySKpFzKodNoq5ybA8H3kIj8qXICQW6Oa4fx04+sQHb5U9ERERypKRK5KwaYOeIboH9yb43djVTkSKgCiQ1TsrYPoKdeCICzeQnIiLiASVVIjnqCMwCqgL7gZuwP9uLFHK1gM5Ac0hMTWT/qf0Zt/3mraBEREQKL7cW/82sT58+2e4PCAjg7bffznVAIr7hHuB3bEK1BjvQRAmVFHINgT7Ao0BzwEDgrkCSUpPOfj8RERE5K4+Tqk8++YTp06dTvnx5576mTZuycuVK7r333ryMTcRLXgG+xS7uOwvoAhzyZkAiuVMFm0g9iO3RmgqsAz6DUnNK0ahiI29GJyIiUuh5nFRddNFF1K5dm02bNnHVVVcxaNAg1q5dy7Zt22jbtm1+xChSQIKBScAbju0PgF5AnNciEskTMdjEKhk7k9/HwE/YBXxFREQk1zweU7V79246derERx99xPz580lNTaVPnz589913+RGfSAEph22V6oad6mwQMN6L8Yicp0DgQqAudo1qgHhgOnAQ/UYgIiKSDzxuqQK44YYbuOeee/j77785efIkDz/8MDVq1Mjr2EQKSE1gKTahOgVchxIqKXSCsHOrDAZuBNoA9TLdvhMlVCIiIvnE46Tq888/Z8aMGbz33nt07tyZNm3akJSUxKZNm7jzzjvzI0aRfNQcuxhPG+zP+F2AhV6NSMQjJbAz+Q0BrsWuNXUK+AVbpUVERCTfedz9r1OnTrRv356NGzcCEBkZyQ033MCgQYOYMGECM2bMyPMgRfJHB2AuUAnYBvQE9no1IhGPVAUews6pAnACWAZsxE5GISIiIgXC46SqXbt2JCVlnX537NixLFyoX/ilsLgRmAaUxo7cvxE47tWIRNziD6Q5rh8DYoFobA/WLZluExERkQLjcfe/pKQkGjZsyBtvvME333xDlSpVAOjZsyeBgVpLWAqDh7GTUpTGtlT1QAmV+LxQbGPq40CAY18aMBkYC2xCCZWIiIiXeJxUdenShU2bNtG+fXtuv/12ypQpA0Dbtm157bXX8jxAkbz1MvAl9lvpV8CtaPS++LRywPXYCSg6YHurNs90+ynAeCEuERERcfI4qXr33Xd55ZVXuOaaa1y6Af7+++906NAhT4MTyVsfAG86rr8BPIIGnojPKo/tlfoUcBm2s/ZebMvUFu+FJSIiIll5nFS1bt2amTNnZtl/5MgRKleunCdBna8RI0ZgjHEpW7du9WpM4gv8gP8Bzzm2BwOvei8ckXMpBzwJXIJtVA0HJgJhwG7vhSUiIiLZ83gQ1MmTJ6lRowZ79uxx2X/RRRcRERGRV3Gdt82bN3PVVVc5t1NSUrwYjXhfAPbb6P3YASePYL+ZiviYkkCC4/opbPLkDywB9nkrKBEREXGHx0nVd999x3vvvcedd96JMQZ/f38uv/xyPvzwQyZPnpwfMXokJSWFyMhIb4chPiEY+A64DUjGJlbTvRqRSBYVscujtQTGYGfyAzs5pX4TEhERKRQ8TqpeeuklPvvsM/bv309AQAD//vsvAQEBfPPNN7z55pvnfoB81qRJEyIiIkhISODvv/9m2LBh7N+/P8fjg4ODKVGihHM7NDTU5dKbfCmWwsaYUsTHf0Nqag8ggVKlHiQwcD52CrWiS3Wm8Egrn0Zih0RSWqQ4O2KXaFOC4I3BBRqH6ox4QvVFPKU6I57ytTrjbhx+nOe8UXXq1OGCCy6gTJkyrFu3jp07d57Pw+Spnj17UqZMGbZv306NGjUYMWIEtWrV4oILLiAmJibb+4wYMYKRI0cWbKCSr06dghtvhOXLoXRpmD0bevTwdlQi1q4Tu3hj6Rt8vfFrUo2dKOWGJjfwatdXuazWZV6OTkRERLJTtmxZoqOjc7z9vJOqwqBcuXLs3buXZ555hgkTJmR7THYtVREREdSqVeusL1xB8KVYCgtjyhEX9xNpaRcDJyld+g4CAlZ5O6wCozrj20yQIeaxGHD8yQnYHUCJv0oQEBlw9jvmI9UZ8YTqi3hKdUY85Wt1Jj2ecyVVbnX/+7//+z+3T/zss8+6fWx+O3XqFDt27KBx48Y5HpOUlOQyNXy66Ohon3gjwbdi8W3lgB+Ai4FjwDXExa3zbkheojrjQ0KA2Ezba4HKwB+QGpFKnI+sk6Y6I55QfRFPqc6IpwpbnXErqbroootcti+++GICAwPZvn07AE2bNiU1NZU1a9bkfYS5EBISQqNGjZgyZYq3Q5F8VxZYgF3Q5xhwJbDJqxFJMVcWOwHFRdgJJw849v9KEe4fICIiUjy5lVRdeeWVzutPP/000dHR9OnTh5MnTwJQvnx5wsLCWLZsWb4E6a4PPviAOXPmsHfvXmrWrMlrr71Gamoq3377rVfjkvwWik2o2mMTqh4ooRKvKQN0BtqR8Re2GRlJlRIqERGRIsfj2f+effZZrrnmGmdCBXbtqldeeYVff/2VUaNG5WV8HqlduzbffvstlSpV4ujRoyxfvpwOHTpw7Ngxr8Uk+S0UmA90AI4DVwEbvRqRFFOlgSuAS4Egx749wGJgr5diEhERkQLhcVJVtmxZqlSpkmV/lSpVvD714b333uvV80tBKwPMAy4HTmATqg1ejUiKsb5AVcf1/cDvQLjXohEREZEC5HFSNXPmTMLCwnj22WdZtcrOqta+fXs++OADfvzxxzwPUCR76QlVJzISqvXeDEiKm2Ds4rxpju0V2C5/iwHvrzAhIiIiBcjjpGrAgAF8+OGHfPPNNwQF2T4uKSkpfPXVVzz//PN5HqBIVqWAn7F9raKAq4HiOcufeEEgtovfFdhJJ9IbR9dhZ/YTERGRYsfjpCo+Pp7HH3+c559/nkaNGgGwa9cu4uJ8Y1pgKeqCsNOmdwFOYhMqfZOVAhCAncmvC3ZmP4DWZCRVmoBCRESk2PI4qUoXFxfHpk2aYU0Kkj/wNXAdduGf6wDfmsZfiiA/oA3QDajg2HcSWIKG8ImIiAhwHklV6dKlefHFF+nRowdVq1bF39/f5fb01iuRvPc5cBeQBNyKHcQiks9uxrZQAUQDy7C5fKrXIhIREREf43FS9eWXX9K1a1emTJnCoUOHMEZ9XqQgvAs8iv0mey+w0LvhSNHmR0Z3vnVAc2A5sApI9lZQIiIi4qs8Tqquu+46brjhBv7666/8iEckG0MdBWxipVkmJZ/UxK4dfRBY5Ni3DxiFkikRERHJkcdJVVRUFCdOnMiPWESy8Ri2lQrgGSDMi7FIkVUJuBJo5diuCSwlI5FSQiUiIiJn4X/uQ1wNHz6c119/nVKlSuVHPCKZ3AOMdVx/ExjtxVikSAoFbgQexyZUBrvc2TiUSImIiIjbPG6pevbZZ2nUqBGRkZHs2bOH5GTXbx7t2rXLs+CkOOsBTMbm/Z8Cw70bjhQ9LYDbsbP0A2zHdvk74rWIREREpJDyOKmaNWtWPoQhktkF2LWogoBvgKe8G44UTRHYCSn2Yec92efdcERERKTw8jipev311/MjDhGHmsAvQDngD6AfWlVVci19rakawHzHvtPYWfqPeSsoERERKSrOe/FfkbxXBvgZqANsBW7DrkklkguNgauA6o7tTdhWKlBCJSIiInnC7aTqxIkTbq1JValSpVwFJMVVADAduBCIBK4HTnoxHin0agBXAw0d2wnYhXsjvRaRiIiIFFFuJ1VDhgzJxzBEPgOuA+Kw07Ht8Wo0UoiVBnpiu/sBpGAX7V0GxHsrKBERESnK3E6qJk+enJ9xSLE2FLseVRpwL7Dau+FI4ZYM1Hdc3wj8jho9RUREJF9pTJV42d1kLO47GJjtxVikUArAThi5ETunSTK2GsUCh7wYl4iIiBQbSqrEizoCkxzXR2PXoxLxQAvsJBTpQzk3OC53eiccERERKZ6UVImX1MSuRVUCmAk8591wpHCpBVwL1HVsxwCp3gtHREREijclVeIFJYAfsdOzbQQewI6nEjmHctiWqdaO7WTgL+BPNPu+iIiIeI2/p3fo1q1bPoQhxcv/gPbACeBW7OAXETfchk2oDLAO+ARYjBIqERER8SqPk6r58+ezc+dOXn75ZWrXrp0fMUmR9iTQD9tX6y4g3LvhiG/zw05EkW4RtsqMA34Cor0RlIiIiIgrj5OqWrVq8emnn3LHHXewe/du5s+fz5133klQUFB+xCdFSjdglOP689hvyCI5aICdab9Lpn37sXObHPZKRCIiIiLZ8jipOn78OB999BEXXXQR7du3Z8eOHYwdO5aDBw/y8ccf06ZNm3M/iBRD9YAZ2GF8U7Cz/YlkoyJwD9AHqA5chGtrlYiIiIiP8TipymzdunW88847fPrpp5QpU4aHHnqINWvWsHTpUlq2bJlXMUqhVwqYBVTGLuzb36vRiI8qgZ2EYhDQHDt3yUrsEDzN7CciIiI+7LySqsDAQHr16sXPP//M3r17ufbaa3niiSeoVq0ajRs3Zu/evcyYMSOvY5VCawJwIRCJnWkgwavRiA9qgB1udwW2MXMnNpmaB8R7MS4RERERN3g8pfonn3zCvffei5+fH1OmTOGFF15gy5Ytztv37t3Lc889x8GDB/M0UCmsnsb25UoG7gAOeDcc8U0nsC1Vx4EFwA7vhiMiIiLiCY+TqpYtW/Lkk0/y448/kpSU/TzGx44do3v37rkOTgq7S4H3HNeHAMu9F4r4lrJAU2xvUIBTwGTgIOrqJyIiIoWOx0nVa6+9xl9//UVqqus3n4CAAC6//HKWLVtGamoqS5cuzbMgpTAqB3wHBGEnqBjr3XDENwQCHYHOQDC2R+h+x237c7qTiIiIiG/zeEzV4sWLqVixYpb95cqVY/HixXkSlBQF44GGwG7gUS/HIj6hGXYSih7YhGofkOjViERERETyhMctVX5+fhhjsuyvVKkSsbGxeRKUFHYDgDuBJOBubN8uKbYqAz2Bxo7t08BvwCavRSQiIiKSp9xOqn744QcAjDFMnDiRxMSMn5gDAgJo06YNf/31V95HKIVMGzLWoBpKxqAZKZb8gQewvUFTgL+BZdh8W0RERKSIcDupOnXKtjb4+fkRHR1NfHzGPMdJSUmsWLGC8ePH532EUoiEANOBksAc4COvRiM+IA1YBFwAzMfO8iciIiJSxLidVD300EMA7Nmzhw8//JC4uLh8C0oKq7HYgTP7gb7eDUW8owpwHbCOjO59Gx1FREREpIjyeEzV66+/nh9xSKH3oKOkAPeiJoliJhjoBrQHAoDywGYg6/BLERERkSLHraRqzZo19OjRg5MnT7J27dpsJ6pI165duzwLTgqL5mRMmf4q8KcXY5EC1wq4Frv2FMA2bFc/JVQiIiJSTLiVVP3000/OiSlmzZqVn/FIoRMITMGOp/oVeNe74UjBqQjcADRybJ8AfgF2ei0iEREREa9wK6nK3OVP3f/E1YvAJdhv1H1Q80QxEopNqFKApcBfjusiIiIixYzHY6pEMrTFdvcDeBw47MVYpECUB046ru/FdvPbDkR5KR4RERERH+BWUnXixImzjqPKrFKlSrkKSAqLIGCi4/IH4DuvRiP5rCx2Vr9GwGdkrOe8wmsRiYiIiPgMt5KqIUOG5HMYUvi8AlwIHAUGejcUyT/+wGVAd6AEdt2pemiKdBEREZFM3EqqJk+enN9xSKFyMfCS4/ogbGIlRU5N4EbHJcA+YC5wxGsRiYiIiPgkt5Kq0NBQoqOjndfPJv04KaqCgUnYqvMd8L13w5H8cTXQEdtSFQ/8hl3QV/OQiIiIiGThVlIVFRVFjRo1OHr0KCdPnsx2fJWfnx/GGAIDNfdF0TYSuACIBJ7wbiiSf1KxCdVGYAEQ691wRERERHyZWxnQlVdeyYkTJwDo3r17vgYkvqw98ILj+gDguBdjkTwVim2ETH9LlwLhjiIiIiIiZ+VWUrV06dJsr0txUhI7218A8DUwy5vBSF7xA9phu/sdA77EdvFLQQmViIiIiJvOq69e+fLlefjhh2nRogUA//77L2FhYURFabGaoqkW8D+gOXAQeMq74UjeqALcBNR1bBugNOrqJyIiIuIhf0/v0LlzZ/bs2cNTTz1FhQoVqFChAk899RTh4eF07tw5P2L02KBBgwgPDyc+Pp4VK1Zw6aWXejukQioQeBbYhv32nQI8jFZ6LeQCgK7YHpx1gUTgF+ArlFCJiIiInAePW6o+++wzpk2bxsCBA0lLSwPA39+fsWPH8tlnn9GmTZs8D9ITd911F6NGjWLAgAGsXLmSIUOGsGDBApo1a8bRo5r6231dsau8tnJs/4WdPn2D1yKSPBAKPABUdWzvAH4mYzFfERERETkvxpMSFxdnmjZtmmV/06ZNTVxcnEePlR9lxYoVZsyYMc5tPz8/c+DAATN06FC37h8aGmqMMSY0NNTrz8U7sVQ38LUB4yhHDPQ14Of110Pl3OWcdcYPwyMYnsPQyvvxqni/+NLfPBXfL6ovKp4W1RkVT4uv1Rl34/G4pWrt2rW0aNGCHTt2uOxv0aIFGzZ4txUjKCiIdu3a8c477zj3GWNYuHAhHTt2zPY+wcHBlChRwrmdvg7Xudbjym+pqReSnPwyt9wCyckzCAxMyfdzGuNHauoVQFkgjaCgCZQo8QZ+flFAmXw/v+RedvU3pVYKAZEB+KX4AZA2Lw2/JD/8Evxsy5UUa77yN08KB9UX8ZTqjHjK1+qMu3G4lVS1bt3aef2TTz7h448/pnHjxqxYsQKADh068Pjjj/Piiy+eR6h5p3LlygQGBhIZGemyPzIykubNm2d7n2HDhjFy5Mgs+yMiIvIjRLfNmwfXXw+zZwNcW6Dnvuwy+Owzfy655BHgkQI9t+SNiIgITiWc4vnfnmf82vG8cPkLvHf1e94OS3yYt//mSeGi+iKeUp0RTxW2OuOHbbI6q9TUVIwx+Pn5nfU4by/+W6NGDQ4ePEjHjh2dCR/Ae++9R9euXenQoUOW+2TXUhUREUGtWrWIjo4ukLizk5ZWi8DAG/jwww957rnnSEhIKJDz+vsfISBgAX5+56wW4oPS62+lDpWIuiIKE2rfx6C1QZRYXAI/zv4ZluLHV/7mSeGg+iKeUp0RT/lanUmPp2zZsmeNx60MqEGDBnkWWH46duwYKSkpVKtWzWV/tWrVOHz4cLb3SUpKIikpKcv+6OhoL7+R2wgNjeCRRz7kmWe+IDHR+5VKfJ8paXhg5gOcuM4u1s1xYDYk700mmWSvxia+zft/86QwUX0RT6nOiKcKW51xK6nat29ffseRJ5KTk1mzZg09evTgp59+AsDPz48ePXrw6aefejk6kXxWB2LvjuXrjV9DGrACWAzKpURERETy13n31WvRogV169YlODjYZf+cOXNyHVRujBo1ikmTJrF69WpWrVrFkCFDCAkJISwszKtxieS702CCDM0qNWP/mP3E7YjzdkQiIiIixYLHSVWDBg2YOXMmrVu3dhlnZYwdu+HNMVUA06dPp0qVKrz++utUr16d9evX07NnT44cOeLVuETyRVUgvWqfgtI/lGbd5nVUe6na2e4lIiIiInnI39M7fPzxx4SHh1O1alXi4uJo1aoVXbp0YfXq1XTr1i0fQvTcZ599Rv369SlZsiQdOnRg1apV3g5JJG+VAG7GrsfcKGN3wKEASgWV8lJQIiIiIsWTx81KHTt25Morr+T48eOkpaWRlpbGn3/+ybBhw/jkk0+4+OKL8yNOEUnXALgVKIedu7MGsMubAYmIiIgUbx63VAUEBDhn4jh27Bg1a9YEYO/evTRr1ixvoxORDIFAT6APNqE6AYQBy70ZlIiIiIh43FK1efNm2rZty549e1i5ciUvvPACSUlJ9O/fn927d+dHjCJSE7gNqOLY/gf4Dci6GoCIiIiIFDCPk6o333yTkJAQAF599VXmzp3LsmXLOH78OHfffXeeBygiQAVsQnUamA3s9G44IiIiIpLB46Tq119/dV7ftWsXLVq0oEKFCkRFReVpYCLFXgCQ6ri+BSgNbAbivRaRiIiIiGTD4zFV2VFCJZLHLgOeBEIy7fsHJVQiIiIiPsjjlqrSpUvz4osv0qNHD6pWrYq/v2te1qhRoxzuKSLnVAa4BWji2G4HLPVeOCIiIiJybh4nVV9++SVdu3ZlypQpHDp0yLnor4jkUlNsQhUCpAC/AlpiTURERMTneZxUXXfdddxwww389ddf+RGPSPETBFwDXOrYPgz8ABz1WkQiIiIi4gGPk6qoqChOnDiRH7GIFE9dyEio/gIWkTFBhYiIiIj4PI8nqhg+fDivv/46pUqVyo94RIqf5cBeYDK2y58SKhEREZFCxeOWqmeffZZGjRoRGRnJnj17SE5Odrm9Xbt2eRacSJEUgusEFIlAmPfCEREREZHc8TipmjVrVj6EIVJMNAJuw87yl4AmohAREREpAjxOql5//fX8iEOkaAsAugNXOLYjgXDvhSMiIiIiecfjpEpEPFQBuAOo5dj+B1iAnTZdRERERAo9t5Kq48eP07RpU44fP86JEyfOujZVpUqV8iw4kUKvGXA7UAKIB2YDW70akYiIiIjkMbeSqqeffpro6GgAhgwZkp/xiBQtsdhP2T7s2lOnvBuOiIiIiOQ9t5KqyZMnZ3tdRLIRSEbXvgPAJMdlmtciEhEREZF8lKsxVSVKlCA4ONhlX3qLlkix1BK4DpgCHHHs2+e9cEREREQk/3m8+G/p0qUZM2YMkZGRxMbGEhUV5VJEiqUA4HrgLiAU6OjdcERERESk4HicVL3//vtceeWVDBw4kMTERB555BFGjBjBwYMHefDBB/MjRhHfVgF4GLjMsb0MmOO9cERERESkYHnc/e+mm27iwQcfZMmSJYSFhbFs2TJ27drF3r176d27N998801+xCnim5pgZ/crBcQBM4H/vBqRiIiIiBQwj1uqKlasyO7duwE4ffo0FStWBGD58uV06dIlb6MT8WUNgd7YhOoA8DlKqERERESKIY+Tqt27d9OgQQMAtm3bxl133QXYFqyTJ0/maXAiPm0PsBtYBYQBp70ajYiIiIh4icdJVVhYGG3btgXg3Xff5fHHHyc+Pp7Ro0fzwQcf5HmAIj6lGnZSCrBTpE8FfgFSvRaRiIiIiHiZx2OqPvroI+f1RYsW0bx5c9q1a8fOnTvZtGlTXsYm4lsuxs7wtw742bFPyZSIiIhIsedxS9UDDzzgsjbVvn37mDlzJtu2beOBBx7I0+BEfEIAcLOjBGKnTPf4kyMiIiIiRdV5df8rV65clv2hoaGEhYXlSVAiPiMU6IttpUoDFgLTHNdFRERERDiP7n9+fn4YY7Lsr127NqdOncqToER8Qm3gbmxiFQ98D+zyakQiIiIi4oPcTqrWrl2LMQZjDIsWLSIlJcV5W0BAAA0aNGD+/Pn5EqRIgQsC7gVCgCPAd8AJr0YkIiIiIj7K7aRq1qxZAFx44YUsWLCAmJgY521JSUns2bOHH374Ic8DFPGKZGAWcJHjMsmbwYiIiIiIL3M7qXr99dcB2LNnD9OmTSMxMTHfghLxihCgPBDh2P4PLeYrIiIiIufk8ZiqyZMnAxAUFETVqlXx93ed62L//v15E5lIQaqG7e4XDIwHorwbjoiIiIgUHh4nVY0bN2bChAlcfvnlLvvTJ7AIDPT4IUW8qynQCygBHEfTpYuIiIiIRzzOgCZOnEhKSgo33ngjhw4dynYmQJFCoyNwDeAH7AZmYGf6ExERERFxk8dJ1YUXXki7du3Yvn17fsQjUjACgBuw608BrAZ+QetPiYiIiIjHPO7o9O+//1K5cuX8iEWk4FxOxoK+84C5KKESERERkfPicVI1dOhQ3n//fbp27UrFihUJDQ11KSKFwt/ATuAbYKWXYxERERGRQs3j7n8LFy4EYNGiRS77NVGF+LyqwFHAACnA194NR0RERESKBo8zoO7du+dHHCL5qw1wC/AXsOgcx4qIiIiIeMDjpGrp0qX5EYdI/ukM9HBcr4id6U+TVoqIiIhIHnErqWrdujWbN2/GGEPr1q3PeuymTZvyJDCRXPPHzvDXzrH9J7AQJVQiIiIikqfcSqrWr19P9erVOXr0KOvXr8cYg5+fX5bjNKZKfEYwcBfQmIwZ/v7xakQiIiIiUkS5lQE1aNCAo0ePOq+L+DQ/4EGgNpAMfA9oWTURERERySduJVX79u3L9rqITzLYadLLY6dMP+jVaERERESkiDvvvnotWrSgbt26BAcHu+yfM2dOroMSOS+ZJ6DYhG2dSvJeOCIiIiJSPHicVDVo0ICZM2fSunVrl7FVxthvsxpTJV7RCLgGu/ZUtGOfEioRERERKQD+nt7h448/Jjw8nKpVqxIXF0erVq3o0qULq1evplu3bvkQosg5XADcB1QDrvByLCIiIiJS7HicVHXs2JFXX32V48ePk5aWRlpaGn/++SfDhg3jk08+yY8Y3RYeHo4xxqUMHTrUqzFJPrsM6AUEYLv8/erdcERERESk+PG4r15AQADR0bZ/1bFjx6hZsyY7duxg7969NGvWLM8D9NTw4cMZP368czs9VimCugNdHddXAvPRGlQiIiIiUuA8Tqo2b95M27Zt2bNnDytXruSFF14gKSmJ/v37s3v37vyI0SPR0dFERkZ6OwzJT37YRX0vcWz/Diz1XjgiIiIiUrx5nFS9+eabhISEAPDqq68yd+5cli1bxvHjx7n77rvzPEBPvfjiiwwfPpx9+/bxzTffMHr0aFJTU3M8Pjg4mBIlSji3Q0NDXS69yZdi8SWmhCGuQRxpJo0SC0sQvDEY9BIBqjPiOdUZ8YTqi3hKdUY85Wt1xt04Mk9Cfd4qVKhAVFRUbh8m155++mnWrl3LiRMnuPzyy3nnnXcICwvj2WefzfE+I0aMYOTIkQUXpOSJg9EH+SfiH25pfou3QxERERGRIq5s2bJnHVaUJ0lVfnrnnXd48cUXz3pM8+bN2b59e5b9/fr1Y9y4cZQpU4akpOzn186upSoiIoJatWp5fTyWL8XibSbAkFonlcA9mrL/bFRnxFOqM+IJ1RfxlOqMeMrX6kx6POdKqtz+hvrVV1+5ddzDDz/s7kO65f/+7/+YOHHiWY/JaSzXypUrCQoKon79+uzYsSPbY5KSkrJNuKKjo33ijQTfisUrgoB7gIbAD8Bm74ZTGBT7OiMeU50RT6i+iKdUZ8RTha3OuJ1U9e3bl71797Ju3Trngr8F4dixYxw7duy87nvhhReSmprKkSNH8jgqKTAlsGtQ1cMu5lt4PlsiIiIiUky4nVT973//495776VBgwaEhYXx9ddf+8Q4qnQdOnSgffv2LF68mOjoaDp27Mjo0aP5+uuvOXnypLfDk/NRCrgfqAUkAF8DB7wakYiIiIhIFm4v/vvEE09Qo0YN3n//fW666Sb279/PtGnTuOaaa/IzPrclJiZyzz33sGTJErZs2cLLL7/M6NGj6d+/v7dDk/NRGuiDTajigEkooRIRERERn+TRqP+kpCS+++47vvvuO+rWrUvfvn0ZO3YsgYGBtGrVitjY2PyK85zWrVtHx44dvXZ+yUPBwINAdSAGmAyoB6eIiIiI+KjznkotLS0NYwx+fn4EBATkZUxS3CUBu4EywETg/IbUiYiIiIgUCLe7/4Gdfvyee+7h119/ZceOHbRu3ZonnniCunXrerWVSoqgX4HPUUIlIiIiIj7P7Zaqzz77jHvuuYf9+/czYcIE7r33Xo4fP56fsUlxUgLoDPwBpDj2xXgtGhERERERt7mdVA0YMIB9+/axe/duunbtSteuXbM9rlevXnkWnBQTwUBvoC5QDrsWlYiIiIhIIeF2UjV58mSMMfkZixRHmROqeOAv74YjIiIiIuIpt5Oqfv365WccUhwFAfdiF/ZNAKYAh7wakYiIiIiIxzyaqEIkz/gDdwINgERsQnXQqxGJiIiIiJwXJVXiHTcCTYFk4GsgwrvhiIiIiIicLyVV4h3rgFhgGrDfy7GIiIiIiOTCeS/+K5Ir+4GPsQv9ioiIiIgUYmqpkoJzMVA907YSKhEREREpApRUScFoDdwM9MWuRSUiIiIiUkQoqZL81wS4zXF9PXDKe6GIiIiIiOQ1JVWSv+oAd2Fr2gZggXfDERERERHJa0qqJP9UxC7uGwTsAH4CjFcjEhERERHJc0qqJH+UAu4DSmPXoJoBpHk1IhERERGRfKEp1SV/pGHHTgUC32IX+RURERERKYKUVEn+SASmAqFAjJdjERERERHJR+r+J3mrdqbr6a1VIiIiIiJFmJIqyTttgUeA670diIiIiIhIwVFSJXmjHnZxX7Bd/0REREREigklVZJ7lYB7gABgC/C7d8MRERERESlISqokd0pgE6pSwH5gJlqLSkRERESKFSVVkju3AlWA08B3QIpXoxERERERKXBKquT81QGaYxOpaUCsd8MREREREfEGrVMl528/di2qECDCy7GIiIiIiHiJkirJnZ3eDkBERERExLvU/U88EwTcBlTwdiAiIiIiIr5BSZV45ibsIr+9AT8vxyIiIiIi4gOUVIn72gNtgDRgDpo6XUREREQEJVXirnrAtY7rvwJ7vRiLiIiIiIgPUVIl51YK6IWtLRuBFd4NR0RERETElyipknO7GSgLHMN2+xMRERERESclVXJ2JYFyQCrwA5Ds3XBERERERHyN1qmSs0sAvgJqA4e8HIuIiIiIiA9SS5WcWyqamEJEREREJAdKqiR7VwFXohoiIiIiInIO6v4nWTUErnBc3wPs9l4oIiIiIiK+Tu0Q4qo0cJvj+mqUUImIiIiInIOSKnF1MxAKHAUWeDkWEREREZFCQEmVZGgHNEfTp4uIiIiIeEBJlVhlgWsc1xcCh70Yi4iIiIhIIaKkSqwagB+wD1jh5VhERERERAoRzf4n1nZgLDbNNl6ORURERESkEFFSJRlOejsAEREREZHCR93/irsrgPreDkJEREREpPBSUlWc1QZ6AH2Byt4NRURERESksCo0SdVLL73En3/+SWxsLFFRUdkeU6dOHebOnUtsbCyRkZG8//77BAQEFHCkhYQ/cBN2cor1wDGvRiMiIiIiUmgVmjFVwcHBzJgxg7///puHH344y+3+/v78/PPPHD58mMsvv5waNWowefJkkpOTefnll70QsY+7HKgGxAK/ejkWEREREZFCrNC0VI38//buPDiqMvv/+CcLCVvQAJIoQhQIBMgYfkREBhwiDDgiCmqJM6ITGB0VHfeFbVhdkEEkiqCIiiKoKCpubIKoRQwomyBgWBMwhECAQCAr4fz+CPSXhiQkNnC7k/er6lT3fe7Tt093H4o+9dy+GTlSCQkJWrduXYn7u3fvrlatWunOO+/UL7/8ovnz52vYsGF68MEHVa1atfOcrZcLldT5+P0FknIczAUAAADwcT6zUnUmHTp00Lp167Rnzx7X2IIFC/T666+rdevWWrNmTYmPCwoKUnBwsGs7JCTE7dZJ5yIXkym3d66KqhUpIDVANbbXkF+I31k7PpzlTfUL30DNoCKoF1QUNYOK8raaKW8elaapCg8PV0ZGhtvYie3w8PBSHzd48GCNHDnytPG0tLSzmp8nzmYuC7cu1HUzrlP1wOpa9+I6NXu72Vk7NryHN9UvfAM1g4qgXlBR1AwqytdqxtGmasyYMRo0aFCZc6KiopScnHxOc3jppZdc2yEhIUpLS1PDhg2VnZ19zp63PM5FLiZT9ajqsuqmtmPbnpVjwnt4U/3CN1AzqAjqBRVFzaCivK1mTuRzJo42VePHj9c777xT5pxt27aV61i7d+/WVVdd5TYWFhbm2leagoICFRQUnDaenZ3tFR+kdA5y+bn4Jl/5Z++Y8CreVL/wDdQMKoJ6QUVRM6goX6sZR5uqzMxMZWaenWt5JyUlaejQobrooou0d+9eSVK3bt108OBBbdiw4aw8h08LlmSSTu8fAQAAAHjAZ35T1ahRI9WtW1eNGzdWQECAYmJiJElbtmzRkSNHtHDhQm3YsEHvvfeenn76aYWHh+vZZ5/VpEmTSlyJqnLiJP1J0lxJ9JgAAADAWeMzTdXo0aPVr18/1/aJq/nFxcXp+++/17Fjx9SzZ0+99tprSkpK0pEjR/Tuu+9q+PDhziTsTS6Q1E7FnzZn/AEAAABnlc80Vf3791f//v3LnLNjxw7dcMMN5ykjHxKn4k96u6StzqYCAAAAVDY+88d/8QddJCnm+P1FTiYCAAAAVE40VZVdFxV/yhsl+dbl/gEAAACfQFNVmTWU1FLSMUnfOpwLAAAAUEnRVFVmlx+//UXSXicTAQAAACovn7lQBf6ApZK2SMpxOhEAAACg8qKpqux2O50AAAAAULlx+l9ldKmkUKeTAAAAAKoGVqoqGz9JN0u6UNKHkjY7mg0AAABQ6bFSVdlESqonqUBSqsO5AAAAAFUATVVlc+Xx29UqbqwAAAAAnFM0VZXJBSpeqZKkFU4mAgAAAFQdNFWVSayKf1O1VdJ+h3MBAAAAqgiaqsoiQFLb4/dZpQIAAADOG5qqyqKBpCBJ2ZKSHc4FAAAAqEK4pHplkS5pvIqv/HfM4VwAAACAKoSVqsokX9Iup5MAAAAAqhaaqsqgjtMJAAAAAFUXTZWvC5Q0QNJ9kkIczgUAAACogmiqfF20pBqSqks67HAuAAAAQBVEU+Xrrjx+u1KSOZkIAAAAUDXRVPmycEmXSiqStNrhXAAAAIAqiqbKl51Ypdoo6YiTiQAAAABVF02VrwqWdMXx+z87mQgAAABQtdFU+apWkoIk7ZWU6nAuAAAAQBUW6HQC+IPWiFP+AAAAAC9AU+WrTNImp5MAAAAAwOl/AAAAAOABVqp80RWSLlbxVf92OJwLAAAAUMWxUuWLWkrqoOLGCgAAAICjaKp8UYPjt3sdzQIAAACAaKp8T6Ck0OP39ziZCAAAAACJpsr31Ffxp5Yj6bDDuQAAAACgqfI5nPoHAAAAeBWaKl9z0fFbTv0DAAAAvAJNla858XsqVqoAAAAAr8DfqfI1syXNl1TkdCIAAAAAJJoq38QFKgAAAACvwel/AAAAAOABmipfEiXpH5L+n9OJAAAAADiB0/98SSNJLSRlOZwHAAAAABdWqnzJicupc+U/AAAAwGvQVPmSE3/4l79RBQAAAHgNmipfESTpwuP3WakCAAAAvAZNla84cepftqRcJxMBAAAAcDKaKl/BqX8AAACAV6Kp8hXBkgrEqX8AAACAl+GS6r5imaTl4hMDAAAAvIzPrFQNGTJEiYmJOnLkiA4cOFDiHDM7LW6//fbznOk5ZJIKnU4CAAAAwMl8Zt0jKChIH3/8sZKSknT33XeXOq9fv36aP3++azsrK+s8ZAcAAACgqvKZpmrkyJGSpPj4+DLnZWVlKSMj4zxkdP4UXVwk/VPSdklznc4GAAAAwMl8pqkqr0mTJunNN9/Utm3b9Prrr2vatGllzg8KClJwcLBrOyQkxO3WSSdyqHZpNekiKeBwgGqG1HQ4K3gzb6pf+AZqBhVBvaCiqBlUlLfVTHnzqFRN1bBhw/Ttt98qJydH3bt31+TJk1W7dm1NnDix1McMHjzYtQp2srS0tHOYacXEPxmvV356RY/84xGNf2e80+nAB3hT/cI3UDOoCOoFFUXNoKJ8rWb8VHz5A0eMGTNGgwYNKnNOVFSUkpOTXdvx8fFKSEhQaGjoGY8/atQo9e/fX40bNy51TkkrVWlpaWrYsKGys7PL8SrOnRO5BP87WAWXFqj6/Oqqtr6aoznBu3lT/cI3UDOoCOoFFUXNoKK8rWZO5FOnTp0y83F0pWr8+PF65513ypyzbdu2P3z85cuXa/jw4QoKClJBQUGJcwoKCkrcl52d7RUfpCQVhhZf8i9vZ57ysvMczga+wJvqF76BmkFFUC+oKGoGFeVrNeNoU5WZmanMzMxzdvw2bdpo//79pTZUvmBfzj5ZreOLifzhXwAAAMDr+Mxvqho1aqS6deuqcePGCggIUExMjCRpy5YtOnLkiHr27KmwsDAtW7ZMeXl56tatm4YMGaIXX3zR4cw9s37v+uI7WZJ8tzcEAAAAKi2faapGjx6tfv36ubbXrFkjSYqLi9P333+vwsJCPfjgg5owYYL8/Py0ZcsWPf7445o6daozCZ8lBUUF8t/jr2P7jjmdCgAAAIASOHqhCm8UEhKiQ4cOnfHHaFUtF/gGagYVRc2gIqgXVBQ1g4rytpopbz7+5zEnAAAAAKh0aKq8XNGxIqdTAAAAAFAGmiovdqzGMdUeU1tH+h7hkwIAAAC8FF/Vvdix+seUdzRPVt0krlMBAAAAeCWaKi92rF5xJxWQGeBwJgAAAABKQ1PlxY7VL26q/PfxMQEAAADeim/rXuzESpV/Jh8TAAAA4K34tu7FiuoVX/mPlSoAAADAe/Ft3VuFSKoh+fv5y38/HxMAAADgrfi27q0CpcBNgeretLv8ivyczgYAAABAKWiqvNUBqcaXNTSv7zynMwEAAABQBpoqAAAAAPAATRUAAAAAeICmCgAAAAA8QFMFAAAAAB6gqQIAAAAAD9BUAQAAAIAHaKoAAAAAwAM0VQAAAADgAZoqAAAAAPAATRUAAAAAeICmCgAAAAA8QFMFAAAAAB6gqQIAAAAAD9BUAQAAAIAHaKoAAAAAwAM0VQAAAADgAZoqAAAAAPAATRUAAAAAeICmCgAAAAA8EOh0At4qJCTE6RRcOXhDLvAN1AwqippBRVAvqChqBhXlbTVT3jz8JNm5TcW3XHLJJUpLS3M6DQAAAABeomHDhtq1a1ep+2mqSnDJJZcoOzvb6TQUEhKitLQ0NWzY0CvygfejZlBR1AwqgnpBRVEzqChvrJmQkJAyGyqJ0/9KdKY37XzLzs72mqKCb6BmUFHUDCqCekFFUTOoKG+qmfLkwYUqAAAAAMADNFUAAAAA4AGaKi+Wn5+vkSNHKj8/3+lU4COoGVQUNYOKoF5QUdQMKspXa4YLVQAAAACAB1ipAgAAAAAP0FQBAAAAgAdoqgAAAADAAzRVAAAAAOABmiov9cADD2j79u3Kzc3VsmXL1K5dO6dTgpcYNGiQfvrpJx06dEgZGRn67LPP1Lx5c7c5wcHBevXVV5WZmans7GzNnj1bDRo0cChjeJuBAwfKzDRhwgTXGDWDU11yySV67733lJmZqZycHK1du1axsbFuc0aNGqVdu3YpJydH33zzjZo1a+ZQtnCav7+/Ro8erW3btiknJ0dbtmzRf//739PmUTNV1zXXXKMvvvhCaWlpMjP16tXrtDlnqo/Q0FDNmDFDBw8e1IEDB/Tmm2+qVq1a5+slnJER3hV9+vSxvLw869evn7Vs2dKmTJli+/fvt4suusjx3AjnY968eRYfH2+tWrWyK664wr766itLSUmxmjVruuZMnjzZUlNT7dprr7W2bdvajz/+aEuXLnU8d8L5uPLKK23btm22Zs0amzBhgmucmiFOjgsvvNC2b99ub7/9trVr184uu+wy69atmzVp0sQ15+mnn7YDBw7YTTfdZH/6059szpw5tnXrVgsODnY8f+L8x+DBg23v3r3Wo0cPi4iIsFtvvdUOHTpkDz30EDVDmCT729/+Zs8884z17t3bzMx69erltr889TF37lxbvXq1XXXVVdaxY0fbtGmTzZw50/HXdjwcT4A4JZYtW2YTJ050bfv5+dnvv/9uAwcOdDw3wvuifv36ZmZ2zTXXmCSrU6eO5efn26233uqa06JFCzMza9++veP5Es5FrVq1LDk52bp27WpLlixxNVXUDHFqjBkzxn744Ycy5+zatcueeOIJ13adOnUsNzfXbr/9dsfzJ85/fPnll/bmm2+6jc2ePdvee+891zY1Q5yIkpqqM9VHVFSUmZnFxsa65lx33XVWVFRkF198seOvidP/vEy1atUUGxurRYsWucbMTIsWLVKHDh0czAze6oILLpAk7d+/X5IUGxuroKAgtxpKTk5WamoqNVTFTZo0SV9//bUWL17sNk7N4FQ33XSTVqxYoY8++kgZGRlatWqV7rnnHtf+yy+/XBdffLFbzRw6dEjLly+nZqqoH3/8UV27dlVkZKQk6YorrlCnTp00b948SdQMylae+ujQoYMOHDiglStXuuYsWrRIx44dU/v27c97zqcKdDoBuKtfv74CAwOVkZHhNp6RkaGoqCiHsoK38vPzU0JCgpYuXar169dLksLDw5Wfn6+DBw+6zc3IyFB4eLgTacIL3H777Wrbtm2Jv8+kZnCqJk2aaMCAAXrppZf0/PPPq127dnrllVdUUFCg6dOnu+qipP+rqJmq6YUXXlCdOnX022+/qaioSAEBARo6dKjef/99SaJmUKby1Ed4eLj27Nnjtr+oqEj79+/3ihqiqQJ82KRJkxQdHa1OnTo5nQq82KWXXqqXX35Z3bp1U35+vtPpwAf4+/trxYoVGjp0qCRpzZo1io6O1v3336/p06c7nB28UZ8+fdS3b1/dcccdWr9+vdq0aaOEhATt2rWLmkGVwOl/XiYzM1NHjx5VWFiY23hYWJh2797tUFbwRhMnTlTPnj117bXXKi0tzTW+e/duBQcHu04LPIEaqrpiY2MVFhamVatWqbCwUIWFhYqLi9PDDz+swsJCZWRkUDNwk56erg0bNriNbdy4UY0bN5YkV13wfxVOGDdunF544QXNmjVLv/76q2bMmKEJEyZo8ODBkqgZlK089bF79+7TrkobEBCgunXrekUN0VR5mcLCQq1cuVJdu3Z1jfn5+alr165KSkpyMDN4k4kTJ+rmm29Wly5dlJKS4rZv5cqVKigocKuh5s2bKyIighqqohYvXqzo6Gi1adPGFT///LNmzpypNm3aaMWKFdQM3CQmJqpFixZuY82bN1dqaqokafv27UpPT3ermZCQELVv356aqaJq1qypY8eOuY0VFRXJ37/4qyY1g7KUpz6SkpIUGhqqtm3buuZ06dJF/v7+Wr58+XnPuSSOXy2DcI8+ffpYbm6u/fOf/7SoqCh7/fXXbf/+/dagQQPHcyOcj0mTJtmBAwfsL3/5i4WFhbmievXqrjmTJ0+2lJQUi4uLs7Zt21piYqIlJiY6njvhPXHy1f8kaoZwjyuvvNIKCgps8ODB1rRpU/vHP/5hhw8ftjvuuMM15+mnn7b9+/fbjTfeaNHR0fbZZ59xeewqHNOmTbOdO3e6Lqneu3dv27Nnj73wwgvUDGFS8RVoY2JiLCYmxszMHn30UYuJibFGjRqVuz7mzp1rK1eutHbt2tmf//xnS05O5pLqRNnx4IMPWkpKiuXl5dmyZcvsqquucjwnwjuiNPHx8a45wcHB9uqrr9q+ffvs8OHD9sknn1hYWJjjuRPeE6c2VdQMcWrccMMNtnbtWsvNzbUNGzbYPffcc9qcUaNGWXp6uuXm5to333xjkZGRjudNOBO1a9e2CRMmWEpKiuXk5NiWLVvsmWeesWrVqrnNo2aqbnTu3LnE7y/Tpk0rd32EhobazJkz7dChQ5aVlWVvvfWW1apVy/HXJsn8jt8BAAAAAPwB/KYKAAAAADxAUwUAAAAAHqCpAgAAAAAP0FQBAAAAgAdoqgAAAADAAzRVAAAAAOABmioAAAAA8ABNFQAAAAB4gKYKAODTRowYod27d8vM1KtXL6fTqZTq1q2rjIwMRURE/OFj1KtXTxkZGWrYsOFZzAwAvANNFQD4qGnTpsnMZGbKz8/X5s2bNWzYMAUEBDid2hmdrQYoKipKI0eO1H333afw8HDNmzev1Lm33HKLlixZoqysLGVnZ+uXX37RsGHDFBoa6nEe3mrJkiWaMGGCx8cZOnSoPv/8c6WmpkqSQkND9cUXXyg7O1urVq1SmzZt3Oa/+uqrevzxx93G9u3bp+nTp2vUqFEe5wMA3oamCgB82Lx58xQeHq7IyEiNHz9eI0eO1FNPPfWHjuXv7y8/P7+znOG51bRpU0nS559/royMDBUUFJQ479lnn9WsWbP0888/6/rrr1d0dLSeeOIJxcTE6K677jqfKfucGjVq6O6779Zbb73lGhs6dKhCQkLUtm1bfffdd5o6daprX/v27dW+fXslJCScdqxp06apb9++lbqRBVB1GUEQBOF7MW3aNPvss8/cxhYsWGA//vijSbKgoCAbN26c/f7773b48GFbtmyZde7c2TU3Pj7eDhw4YDfeeKOtX7/eCgsLLSIiwoKCguyFF16wHTt2WF5enm3evNn+9a9/uR7XunVrmzt3rmVnZ9vu3btt+vTpVq9ePdf+JUuW2Msvv2xjx461ffv2WXp6uo0YMcK1f/v27Xay7du3l/oao6OjbfHixZaTk2OZmZk2ZcoUq1WrlkmyESNG2KlKOka7du3MzOzhhx8ucf8FF1zgun///ffbli1bLD8/33777Te788473eaamd1777325Zdf2pEjR2zDhg129dVXW9OmTW3JkiV2+PBhS0xMtCZNmrgeM2LECFu9erX179/fUlNTLTs72yZNmmT+/v721FNPWXp6umVkZNiQIUNOy2vq1Km2Z88eO3jwoC1evNiuuOKK045755132vbt2y0rK8s++OADq127tqs+ThUREWEXXnihzZgxw/bs2WM5OTm2adMm69evX6mfwa233moZGRluY19//bXdd999JsmioqLs8OHDJskCAwNt9erVFhsbW+rxtm7d6lZPBEEQlSQcT4AgCIL4A1FSUzVnzhxbsWKFSbI33njDli5dap06dbImTZrYE088Ybm5udasWTOTipuq/Px8W7p0qXXo0MGaN29uNWrUsA8//NBSU1Otd+/edvnll1uXLl2sT58+JhV/0c/IyLDnnnvOWrRoYW3atLEFCxbY4sWLXTksWbLEsrKybPjw4dasWTO76667rKioyP7617+aJKtfv76ZmcXHx1tYWJjVr1+/xNdXs2ZNS0tLs9mzZ1vr1q3t2muvta1bt9q0adNMktWqVcvi4+PNzCwsLMzCwsJKPE5CQoIdOnTIAgMDy3w/e/fubfn5+TZgwACLjIy0xx57zAoLCy0uLs41x8xs586ddtttt1lkZKR9+umntm3bNlu0aJF1797doqKi7Mcff7S5c+e6HjNixAg7dOiQffTRR9ayZUvr2bOn5eXl2bx58+zll1+25s2bW79+/czM7KqrrnI9buHChfb5559bbGysNWvWzMaNG2d79+610NBQt+OeeH86depku3btsmeffdYkWZ06dSwxMdGmTJnien/8/f1t4sSJtmrVKouNjbWIiAjr2rWr9ezZs9T3JSEhwe31SLLnn3/eZs2aZQEBAfbII4+4GvkhQ4bYhAkTynyfP/jgA9dnSBAEUYnC8QQIgiCIPxCnNlVdu3a13Nxc+9///meNGjWywsJCu/jii90e880339hzzz1nklwNycmrH5GRkWZm1rVr1xKfc+jQoTZ//ny3sYYNG5qZWWRkpEnFTdUPP/zgNmf58uU2ZswY17aZWa9evcp8fffcc4/t27fPatas6Rq7/vrr7ejRo9agQQOTZL169Sp1hepEfP3117ZmzZozvp9Lly61KVOmuI3NmjXLvvrqK7e8R48e7dpu3769mZn179/fNXb77bdbTk6Oa3vEiBF2+PBh1wqSJJs3b55t27bN/Pz8XGMbN260gQMHmiTr2LGjZWVlWVBQkFs+mzdvtn//+9+lHnfs2LGWlJTk2l6yZMlpTc7nn39ub731Vrnr7LPPPrM333zTbaxOnTo2c+ZMS0lJse+++85atmxpzZo1s+TkZKtbt6699tprtnXrVps1a5bVqVPH7bHjx4+3b7/91vF/PwRBEGczAgUA8Fk9e/ZUdna2qlWrJn9/f73//vsaOXKk4uLiFBgYqE2bNrnNDw4O1r59+1zb+fn5Wrt2rWu7TZs2Onr0qL7//vsSny8mJkbXXnutsrOzT9vXtGlTbd68WZLcjilJ6enpatCgQYVeW8uWLfXLL78oJyfHNZaYmKiAgAC1aNFCe/bsKddxyvs7sZYtW+qNN95wG0tMTNQjjzziNnbya8vIyJAkrVu3zm2sRo0aCgkJcb1PKSkpOnz4sNucoqIiFfeD/zd24j2KiYlR7dq13T4rqfj3TSd+R1bSccvzPr/22mv65JNP1LZtWy1cuFBz5sxRUlJSqfNr1KihvLw8t7FDhw6pb9++bmOLFy/WU089pb59+6pJkyZq0aKFpk6dquHDh+vJJ590zcvNzVXNmjXLzBEAfA1NFQD4sCVLlmjAgAEqKCjQrl27VFRUJEmqXbu2jh49qtjYWNfYCSd/Cc/NzXXbd+r2qWrXrq0vv/xSAwcOPG1fenq6635hYaHbPjOTv78z10batGmTOnXqpMDAQB09etTj45382k40RSWNnfx6S3o/ynqPateurfT0dMXFxZ32/FlZWWUe90zv8/z58xUREaEePXqoW7duWrx4sSZNmlTqBU4yMzPPeGGJfv36KSsrS1988YU++eQTzZkzR0ePHtXHH3+s0aNHu82tW7eu9u7dW+bxAMDXcPU/APBhR44c0datW7Vz50635mn16tUKDAxUgwYNtHXrVrc4sbpSknXr1snf31+dO3cucf+qVavUunVrpaSknHbck1eUzqSgoOCMl37fuHGjYmJi3FY1OnbsqKKiIiUnJ5f7ud5//32FhITogQceKHH/BRdc4Hq+jh07uu3r2LGjNmzYUO7nOltWrVql8PBwHT169LT3+dTVq7KU9j5nZmZq+vTpuuuuu/Too4/q3nvvLfUYq1evVqtWrUrdX79+fQ0fPlwPPfSQJCkgIEDVqlWTJFWrVu2054+Ojtbq1avL/RoAwBfQVAFAJbR582bNmDFD06dP180336zLLrtM7dq106BBg9SjR49SH5eamqp3331Xb7/9tnr16qXLLrtMnTt31m233SZJmjRpkurWrasPPvhAV155pZo0aaLu3bvr7bffrtBKVEpKirp27aqwsDBdeOGFJc6ZOXOm8vLy9O6776p169aKi4vTxIkT9d5775X71D9J+umnnzR27FiNHz9eY8eO1dVXX63GjRurS5cu+uijjxQfHy9JGjdunPr166f7779fzZo102OPPaZbbrlFL774Yrmf62xZtGiRkpKSNGfOHHXr1k0RERHq0KGDnn32WcXGxpb7OCkpKWrfvr0iIiJUr149+fn5adSoUbrpppvUtGlTtWrVSj179tTGjRtLPcaCBQvUunXrUj+nhIQEjR8/Xrt27ZJUfMrkXXfdpaioKN17771KTEx0za1Ro4ZiY2O1cOHCcr8GAPAFNFUAUEn1799f06dP1/jx45WcnKw5c+aoXbt22rFjR5mPGzBggGbPnq3Jkyfrt99+09SpU1WrVi1Jxaf4dezYUQEBAVq4cKHWrVunhIQEZWVl6dixY+XO7YknnlC3bt20c+fOUlctcnNzdd1116lu3br6+eefNXv2bC1evFj/+c9/yv8mHDdo0CDdcccdat++vRYsWKD169frpZde0tq1a/Xuu+9KKv5bV4888oiefPJJrV+/Xvfdd5/69+9f6u/LzrUePXrohx9+0LRp07Rp0yZ9+OGHioiIKHOl8VQvvviiioqKtGHDBmVmZqpx48YqKCjQmDFjtHbtWv3www8qKirS3//+91KP8euvv2rVqlXq06fPafu6d++uZs2aafLkya6xV199Vdu2bdPy5csVFBTk9sd+e/XqpR07dmjp0qXlfg0A4Av8VHzFCgAAgBL16NFD48aNU3R0tNvFNSoqKSlJr7zyij744IOzmB0AOI8LVQAAgDLNnTtXkZGRatiwoX7//fc/dIx69erp008/paECUCmxUgUAAAAAHuA3VQAAAADgAZoqAAAAAPAATRUAAAAAeICmCgAAAAA8QFMFAAAAAB6gqQIAAAAAD9BUAQAAAIAHaKoAAAAAwAM0VQAAAADggf8PgT5OLGhkBL8AAAAASUVORK5CYII=",
      "text/plain": [
       "<Figure size 1000x600 with 1 Axes>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "\n",
    "# Define the percentage of comments (as a range from 0% to 100%)\n",
    "percent_comments = np.linspace(0, 100, 100)\n",
    "\n",
    "# Define constants for the SEI and Visual Studio MI formulas\n",
    "# These are placeholders; replace them with actual coefficients if available\n",
    "V = 1000  # Cyclomatic complexity\n",
    "G = 10   # Halstead effort\n",
    "L = 10000  # Lines of code\n",
    "C = percent_comments / 100  # Convert percentage to a fraction\n",
    "\n",
    "# Compute the SEI MI derivative\n",
    "MI_SEI = np.maximum(0, (171 - 5.2 * np.log(V) - 0.23 * G - 16.2 * np.log(L) + 50 * np.sin(np.sqrt(2.4 * C))) / 171 * 100)\n",
    "\n",
    "# Compute the Visual Studio MI derivative (simplified version)\n",
    "MI_VS = 171 - 5.2 * np.log(V) - 0.23 * G - 16.2 * np.log(L) + 36 * np.sqrt(C)\n",
    "\n",
    "# Plot the results\n",
    "plt.figure(figsize=(10, 6))\n",
    "plt.plot(percent_comments, MI_SEI, label=\"SEI MI Derivative\", color=\"blue\")\n",
    "plt.plot(percent_comments, MI_VS, label=\"Visual Studio MI Derivative\", color=\"green\", linestyle=\"--\")\n",
    "\n",
    "# Add labels, title, and legend\n",
    "plt.title(\"Influence of Percent of Comments on Maintainability Index\")\n",
    "plt.xlabel(\"Percent of Comments (%)\")\n",
    "plt.ylabel(\"Maintainability Index (MI)\")\n",
    "plt.legend()\n",
    "plt.grid(True)\n",
    "\n",
    "# Show the plot\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "from pylint.reporters.base_reporter import BaseReporter\n",
    "from pylint.reporters.ureports.nodes import Section\n",
    "\n",
    "class ScoreOnlyReporter(BaseReporter):\n",
    "    def __init__(self, output = None) -> None:\n",
    "        super().__init__(output)\n",
    "        self.name = \"score-only\"\n",
    "    \n",
    "    def handle_message(self, msg):\n",
    "        pass\n",
    "\n",
    "    def writeln(self, string = \"\"):\n",
    "        pass\n",
    "\n",
    "    def display_reports(self, layout):\n",
    "        pass\n",
    "\n",
    "    def _display(self, layout: Section):\n",
    "        pass\n",
    "\n",
    "    def on_close(self, stats, previous_stats):\n",
    "        pass"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "from pylint.lint import Run\n",
    "import os\n",
    "import subprocess\n",
    "from radon.metrics import mi_visit, h_visit\n",
    "from radon.raw import analyze\n",
    "import re\n",
    "\n",
    "def get_maintainability_index(source_code, filepath='temp_code.py'):\n",
    "    \"\"\"\n",
    "    Calculate the Maintainability Index of a given source code file.\n",
    "    Args:\n",
    "        source_code (str): The source code to analyze.\n",
    "        filepath (str): The path to the file to save the source code temporarily.\n",
    "    Returns:\n",
    "        int: The Maintainability Index of the source code.\n",
    "    \"\"\"\n",
    "    try:\n",
    "        return int(mi_visit(source_code, True))\n",
    "    except Exception as e:\n",
    "        filename = filepath.split('/')[-1]\n",
    "        with open(filename, \"w\") as f:\n",
    "            f.write(source_code)\n",
    "        result = subprocess.run(\n",
    "            ['python2', '-m' 'radon', \"mi\", \"-s\", filename],\n",
    "            stdout=subprocess.PIPE,\n",
    "            stderr=subprocess.PIPE,\n",
    "            text=True\n",
    "        )\n",
    "        maintainability_output = result.stdout\n",
    "        match_mi = re.search(r\"\\((\\d+)\\)\", maintainability_output)\n",
    "        if match_mi:\n",
    "            os.remove(filename)\n",
    "            return int(match_mi.group(1))\n",
    "        else:\n",
    "            return 0\n",
    "            raise Exception(f\"File at {filepath} does not compile: {result.stderr}\")\n",
    "        \n",
    "        \n",
    "def get_pylint_score(source_code, filepath='temp_code.py'):\n",
    "    filename = filepath.split(\"/\")[-1]\n",
    "    with open(filename, 'w') as f:\n",
    "        f.write(source_code)\n",
    "    try:\n",
    "        print(source_code)\n",
    "        pylint_results = Run([filename], ScoreOnlyReporter(), exit =False)\n",
    "        os.remove(filename)\n",
    "        return pylint_results.linter.stats.global_note\n",
    "    except Exception as e:\n",
    "        raise Exception(f\"File {filepath} does not compile {e}\")\n",
    "    \n",
    "class Python2LineMetics:\n",
    "    def __init__(self, loc, lloc, sloc, comments, single_comments, multi, blank):\n",
    "        self.loc = loc\n",
    "        self.lloc = lloc\n",
    "        self.sloc = sloc\n",
    "        self.comments = comments\n",
    "        self.single_comments = single_comments\n",
    "        self.multi = multi\n",
    "        self.blank = blank\n",
    "\n",
    "    def __repr__(self):\n",
    "        return f\"LOC: {self.loc}, LLOC: {self.lloc}, SLOC: {self.sloc}, Comments: {self.comments}, Single Comments: {self.single_comments}, Multi: {self.multi}, Blank: {self.blank}\"\n",
    "\n",
    "def get_line_metrics(source_code, filepath='temp_code.py'):\n",
    "    try:\n",
    "        return analyze(source_code)\n",
    "    except Exception as e:\n",
    "        filename = filepath.split(\"/\")[-1]\n",
    "        with open(filename, 'w') as f:\n",
    "            f.write(source_code)\n",
    "        result = subprocess.run(\n",
    "            ['python2', '-m' 'radon', \"raw\", filename],\n",
    "            stdout=subprocess.PIPE,\n",
    "            stderr=subprocess.PIPE,\n",
    "            text=True\n",
    "        )\n",
    "        line_metrics_output = result.stdout\n",
    "        loc = re.search(r\"LOC:\\s*(\\d+)\", line_metrics_output)\n",
    "        lloc = re.search(r\"LLOC:\\s*(\\d+)\", line_metrics_output)\n",
    "        sloc = re.search(r\"SLOC:\\s*(\\d+)\", line_metrics_output)\n",
    "        comments = re.search(r\"Comments:\\s*(\\d+)\", line_metrics_output)\n",
    "        single_comments = re.search(r\"Single comments:\\s*(\\d+)\", line_metrics_output)\n",
    "        multi = re.search(r\"Multi:\\s*(\\d+)\", line_metrics_output)\n",
    "        blank = re.search(r\"Blank:\\s*(\\d+)\", line_metrics_output)\n",
    "        if loc and lloc and sloc and comments and single_comments and multi and blank:\n",
    "            os.remove(filename)\n",
    "            return Python2LineMetics(\n",
    "                loc=int(loc.group(1)),\n",
    "                lloc=int(lloc.group(1)),\n",
    "                sloc=int(sloc.group(1)),\n",
    "                comments=int(comments.group(1)),\n",
    "                single_comments=int(single_comments.group(1)),\n",
    "                multi=int(multi.group(1)),\n",
    "                blank=int(blank.group(1))\n",
    "            )\n",
    "        else:\n",
    "            raise Exception(f\"File at {filepath} does not compile: {result.stderr}\")\n",
    "class Python2HelsteadTotal:\n",
    "    def __init__(self, h1, h2, N1, N2):\n",
    "        self.h1 = h1\n",
    "        self.h2 = h2\n",
    "        self.N1 = N1\n",
    "        self.N2 = N2\n",
    "\n",
    "class Python2HelsteadReport:\n",
    "    def __init__(self, h1, h2, N1, N2):\n",
    "        self.total = Python2HelsteadTotal(h1, h2, N1, N2)\n",
    "\n",
    "    def __repr__(self):\n",
    "        return f\"\"\"\n",
    "        HelsteadReport(total: \n",
    "        h1={self.total.h1}, \n",
    "        h2={self.total.h2}, \n",
    "        N1={self.total.N1}, \n",
    "        N2={self.total.N2})\"\"\"\n",
    "\n",
    "def get_halstead_metrics(source_code, filepath='temp_code.py'):\n",
    "    \"\"\"\n",
    "    Calculate the Halstead metrics of a given source code file.\n",
    "    Args:\n",
    "        source_code (str): The source code to analyze.\n",
    "        filepath (str): The path to the file to save the source code temporarily.\n",
    "    Returns:\n",
    "        dict: A dictionary containing the Halstead metrics.\n",
    "    \"\"\"\n",
    "    try:\n",
    "        return h_visit(source_code)\n",
    "    except Exception as e:\n",
    "        filename = filepath.split(\"/\")[-1]\n",
    "        with open(filename, 'w') as f:\n",
    "            f.write(source_code)\n",
    "        result = subprocess.run(\n",
    "            ['python2', '-m' 'radon', \"raw\", filename],\n",
    "            stdout=subprocess.PIPE,\n",
    "            stderr=subprocess.PIPE,\n",
    "            text=True\n",
    "        )\n",
    "        line_metrics_output = result.stdout\n",
    "        theta_1 = re.search(r\"h1:\\s*(\\d+)\", line_metrics_output)\n",
    "        theta_2 = re.search(r\"h1:\\s*(\\d+)\", line_metrics_output)\n",
    "        N1 = re.search(r\"N1:\\s*(\\d+)\", line_metrics_output)\n",
    "        N2 = re.search(r\"N2:\\s*(\\d+)\", line_metrics_output)\n",
    "        \n",
    "        if theta_1 and theta_2 and N1 and N2:\n",
    "            os.remove(filename)\n",
    "            return Python2HelsteadReport(\n",
    "                h1=int(theta_1.group(1)),\n",
    "                h2=int(theta_2.group(1)),\n",
    "                N1=int(N1.group(1)),\n",
    "                N2=int(N2.group(1))\n",
    "            )\n",
    "        else:\n",
    "            return Python2HelsteadReport(\n",
    "                h1=0,\n",
    "                h2=0,\n",
    "                N1=0,\n",
    "                N2=0\n",
    "            )\n",
    "            raise Exception(f\"File at {filepath} does not compile: {result.stderr}\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Previous HEAD position was fc46d27 Merge pull request #376 from JustinXre2020/bugfix_20250530\n",
      "HEAD is now at 3071e17 Updating Progress\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Previous HEAD position was 3071e17 Updating Progress\n",
      "HEAD is now at fc46d27 Merge pull request #376 from JustinXre2020/bugfix_20250530\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "File Progress/Keywords.py has no source code\n",
      "\n",
      "# Making a script for spacy's Rule based Matching\n",
      "\n",
      "import spacy\n",
      "import re\n",
      "\n",
      "nlp = spacy.load('en_core_web_sm')\n",
      "\n",
      "# Creating a Callback Pattern to return a string buffer is a pattern matches\n",
      "\n",
      "def match_pattern(pattern, text):\n",
      "\t\"\"\" \n",
      "\tThis takes in spacy.matcher.Matcher patterns and \n",
      "\t\"\"\"\n",
      "\n",
      "\tmatches = matcher.add('',None,text)\n",
      "\tif len(matches) == 0:\n",
      "\t\treturn False\n",
      "\telse:\n",
      "\t\treturn True\n",
      "\n",
      "import spacy\n",
      "from spacy.matcher import Matcher, PhraseMatcher\n",
      "import re\n",
      "\n",
      "# Making a script for spacy's Rule based Matching\n",
      "\n",
      "import spacy\n",
      "import re\n",
      "from spacy.matcher import Matcher, PhraseMatcher\n",
      "\n",
      "nlp = spacy.load('en_core_web_sm')\n",
      "\n",
      "# Creating a Callback Pattern to return a string buffer is a pattern matches\n",
      "\n",
      "def is_pattern(pattern, text):\n",
      "\t\"\"\" \n",
      "\tThis takes in spacy.matcher.Matcher patterns and if a pattern is found \n",
      "    it returns true.\n",
      "\t\"\"\"\n",
      "\n",
      "\tmatches = matcher.add('',None,text)\n",
      "\tif len(matches) == 0:\n",
      "\t\treturn False\n",
      "\telse:\n",
      "\t\treturn True \n",
      "\n",
      "import spacy\n",
      "from spacy.matcher import Matcher, PhraseMatcher\n",
      "import re\n",
      "\n",
      "nlp = spacy.load('en_core_web_md')\n",
      "\n",
      "matcher = Matcher(nlp.vocab)  # Matcher object should share the same vocabulary as the whole document\n",
      "\n",
      "def \n",
      "\n",
      "File Progress/first_app.py has no source code\n",
      "\n",
      "import spacy\n",
      "from spacy.matcher import Matcher, PhraseMatcher\n",
      "import re\n",
      "\n",
      "nlp = spacy.load('en_core_web_md')\n",
      "\n",
      "# Matcher object should share the same vocabulary as the whole document\n",
      "matcher = Matcher(nlp.vocab)\n",
      "\n",
      "import nltk\n",
      "import spacy\n",
      "import re\n",
      "\n",
      "from nltk.tokenize import word_tokenize, sent_tokenize\n",
      "from nltk.corpus import stopwords\n",
      "\n",
      "# Define english stopwords\n",
      "stop_words = stopwords.words('english')\n",
      "\n",
      "# load the spacy module and create a nlp object\n",
      "# This need the spacy en module to be present on the system.\n",
      "nlp = spacy.load('en_core_web_sm')\n",
      "# proces to remove stopwords form a file, takes an optional_word list\n",
      "# for the words that are not present in the stop words but the user wants them deleted.\n",
      "\n",
      "\n",
      "def remove_stopwords(text, stopwords=stop_words, optional_params=False, optional_words=[]):\n",
      "    if optional_params:\n",
      "        stopwords.append([a for a in optional_words])\n",
      "    return [word for word in text if word not in stopwords]\n",
      "\n",
      "\n",
      "def tokenize(text):\n",
      "    # Removes any useless punctuations from the text\n",
      "    text = re.sub(r'[^\\w\\s]', '', text)\n",
      "    return word_tokenize(text)\n",
      "\n",
      "\n",
      "def lemmatize(text):\n",
      "    # the input to this function is a list\n",
      "    str_text = nlp(\" \".join(text))\n",
      "    lemmatized_text = []\n",
      "    for word in str_text:\n",
      "        lemmatized_text.append(word.lemma_)\n",
      "    return lemmatized_text\n",
      "\n",
      "# internal fuction, useless right now.\n",
      "\n",
      "\n",
      "def _to_string(List):\n",
      "    # the input parameter must be a list\n",
      "    string = \" \"\n",
      "    return string.join(List)\n",
      "\n",
      "\n",
      "def remove_tags(text, postags=['NOUN', 'ADJ', 'VERB', 'ADV']):\n",
      "    \"\"\"\n",
      "    Takes in Tags which are allowed by the user and then elimnates the rest of the words\n",
      "    based on their Part of Speech (POS) Tags.\n",
      "    \"\"\"\n",
      "    filtered = []\n",
      "    str_text = nlp(\" \".join(text))\n",
      "    for token in str_text:\n",
      "        if token.pos_ in postags:\n",
      "            filtered.append(token.text)\n",
      "    return filtered\n",
      "\n",
      "import nltk\n",
      "import spacy\n",
      "import re\n",
      "\n",
      "from nltk.tokenize import word_tokenize, sent_tokenize\n",
      "from nltk.corpus import stopwords\n",
      "\n",
      "# Define english stopwords\n",
      "stop_words = stopwords.words('english')\n",
      "\n",
      "# load the spacy module and create a nlp object\n",
      "# This need the spacy en module to be present on the system.\n",
      "nlp = spacy.load('en_core_web_sm')\n",
      "# proces to remove stopwords form a file, takes an optional_word list\n",
      "# for the words that are not present in the stop words but the user wants them deleted.\n",
      "\n",
      "\n",
      "def remove_stopwords(text, stopwords=stop_words, optional_params=False, optional_words=[]):\n",
      "    if optional_params:\n",
      "        stopwords.append([a for a in optional_words])\n",
      "    return [word for word in text if word not in stopwords]\n",
      "\n",
      "\n",
      "def tokenize(text):\n",
      "    # Removes any useless punctuations from the text\n",
      "    text = re.sub(r'[^\\w\\s]', '', text)\n",
      "    return word_tokenize(text)\n",
      "\n",
      "\n",
      "def lemmatize(text):\n",
      "    # the input to this function is a list\n",
      "    str_text = nlp(\" \".join(text))\n",
      "    lemmatized_text = []\n",
      "    for word in str_text:\n",
      "        lemmatized_text.append(word.lemma_)\n",
      "    return lemmatized_text\n",
      "\n",
      "# internal fuction, useless right now.\n",
      "\n",
      "\n",
      "def _to_string(List):\n",
      "    # the input parameter must be a list\n",
      "    string = \" \"\n",
      "    return string.join(List)\n",
      "\n",
      "\n",
      "def remove_tags(text, postags=['NOUN', 'ADJ', 'VERB', 'ADV']):\n",
      "    \"\"\"\n",
      "    Takes in Tags which are allowed by the user and then elimnates the rest of the words\n",
      "    based on their Part of Speech (POS) Tags.\n",
      "    \"\"\"\n",
      "    filtered = []\n",
      "    str_text = nlp(\" \".join(text))\n",
      "    for token in str_text:\n",
      "        if token.pos_ in postags:\n",
      "            filtered.append(token.text)\n",
      "    return filtered\n",
      "\n",
      "import nltk\n",
      "import spacy\n",
      "import re\n",
      "\n",
      "from nltk.tokenize import word_tokenize, sent_tokenize\n",
      "from nltk.corpus import stopwords\n",
      "\n",
      "# Define english stopwords\n",
      "stop_words = stopwords.words('english')\n",
      "\n",
      "# load the spacy module and create a nlp object\n",
      "# This need the spacy en module to be present on the system.\n",
      "nlp = spacy.load('en_core_web_sm')\n",
      "# proces to remove stopwords form a file, takes an optional_word list\n",
      "# for the words that are not present in the stop words but the user wants them deleted.\n",
      "\n",
      "\n",
      "def remove_stopwords(text, stopwords=stop_words, optional_params=False, optional_words=[]):\n",
      "    if optional_params:\n",
      "        stopwords.append([a for a in optional_words])\n",
      "    return [word for word in text if word not in stopwords]\n",
      "\n",
      "\n",
      "def tokenize(text):\n",
      "    # Removes any useless punctuations from the text\n",
      "    text = re.sub(r'[^\\w\\s]', '', text)\n",
      "    return word_tokenize(text)\n",
      "\n",
      "\n",
      "def lemmatize(text):\n",
      "    # the input to this function is a list\n",
      "    str_text = nlp(\" \".join(text))\n",
      "    lemmatized_text = []\n",
      "    for word in str_text:\n",
      "        lemmatized_text.append(word.lemma_)\n",
      "    return lemmatized_text\n",
      "\n",
      "# internal fuction, useless right now.\n",
      "\n",
      "\n",
      "def _to_string(List):\n",
      "    # the input parameter must be a list\n",
      "    string = \" \"\n",
      "    return string.join(List)\n",
      "\n",
      "\n",
      "def remove_tags(text, postags=['PROPN', 'NOUN', 'ADJ', 'VERB', 'ADV']):\n",
      "    \"\"\"\n",
      "    Takes in Tags which are allowed by the user and then elimnates the rest of the words\n",
      "    based on their Part of Speech (POS) Tags.\n",
      "    \"\"\"\n",
      "    filtered = []\n",
      "    str_text = nlp(\" \".join(text))\n",
      "    for token in str_text:\n",
      "        if token.pos_ in postags:\n",
      "            filtered.append(token.text)\n",
      "    return filtered\n",
      "\n",
      "import spacy\n",
      "import Distill\n",
      "\n",
      "try:\n",
      "    nlp = spacy.load('en_core_web_sm')\n",
      "\n",
      "except ImportError:\n",
      "    print(\"Spacy's English Language Modules aren't present \\n Install them by doing \\n python -m spacy download en_core_web_sm\")\n",
      "\n",
      "\n",
      "def _base_clean(text):\n",
      "    \"\"\"\n",
      "    Takes in text read by the parser file and then does the text cleaning.\n",
      "    \"\"\"\n",
      "    text = Distill.tokenize(text)\n",
      "    text = Distill.remove_stopwords(text)\n",
      "    text = Distill.remove_tags(text)\n",
      "    text = Distill.lemmatize(text)\n",
      "    return text\n",
      "\n",
      "\n",
      "def _reduce_redundancy(text):\n",
      "    \"\"\"\n",
      "    Takes in text that has been cleaned by the _base_clean and uses set to reduce the repeating words\n",
      "    giving only a single word that is needed.\n",
      "    \"\"\"\n",
      "    return list(set(text))\n",
      "\n",
      "\n",
      "def _get_target_words(text):\n",
      "    \"\"\"\n",
      "    Takes in text and uses Spacy Tags on it, to extract the relevant Noun, Proper Noun words that contain words related to tech and JD. \n",
      "\n",
      "    \"\"\"\n",
      "    target = []\n",
      "    sent = \" \".join(text)\n",
      "    doc = nlp(sent)\n",
      "    for token in doc:\n",
      "        if token.tag_ in ['NN', 'NNP']:\n",
      "            target.append(token.text)\n",
      "    return target\n",
      "\n",
      "\n",
      "# https://towardsdatascience.com/overview-of-text-similarity-metrics-3397c4601f50\n",
      "# https://towardsdatascience.com/the-best-document-similarity-algorithm-in-2020-a-beginners-guide-a01b9ef8cf05\n",
      "\n",
      "def Cleaner(text):\n",
      "    sentence = []\n",
      "    sentence_cleaned = _base_clean(text)\n",
      "    sentence.append(sentence_cleaned)\n",
      "    sentence_reduced = _reduce_redundancy(sentence_cleaned)\n",
      "    sentence.append(sentence_reduced)\n",
      "    sentence_targetted = _get_target_words(sentence_reduced)\n",
      "    sentence.append(sentence_targetted)\n",
      "    return sentence\n",
      "\n",
      "import textdistance\n",
      "import abydos\n",
      "\n",
      "import textdistance as td\n",
      "import Cleaner\n",
      "\n",
      "def match(resume, job_des):\n",
      "    j = td.jaccard.similarity(resume, job_des)*100\n",
      "    s = td.sorensen_dice.similarity(resume, job_des)*100\n",
      "    c = td.cosine.similarity(resume, job_des)*100\n",
      "    o = td.overlap.normalized_similarity(resume, job_des)*100\n",
      "    total = (j+s+c+o)/4\n",
      "    return total\n",
      "\n",
      "# https://realpython.com/working-with-files-in-python/\n",
      "\n",
      "# https://support.dlink.ca/emulators/wbr2310/index.htm\n",
      "\n",
      "import os\n",
      "import pandas as pd\n",
      "import textract as tx\n",
      "import Similar\n",
      "import Cleaner\n",
      "\n",
      "\n",
      "resume_dir = \"Data/Resumes/\"\n",
      "job_desc_dir = \"Data/JobDesc/\"\n",
      "resume_names = os.listdir(resume_dir)\n",
      "document = []\n",
      "\n",
      "for res in resume_names:\n",
      "    temp = []\n",
      "    temp.append(res)\n",
      "    text = tx.process(resume_dir+res, encoding='ascii')\n",
      "    text = str(text, 'utf-8')\n",
      "    temp.append(text)\n",
      "    document.append(temp)\n",
      "\n",
      "df = pd.DataFrame(document, columns=['Name', 'Context'])\n",
      "\n",
      "# Only one Job Description should be present and in docx format\n",
      "job_docs = os.listdir(job_desc_dir)\n",
      "job_desc = tx.process(\n",
      "    job_desc_dir+job_docs[0], extension='docx', encoding='ascii')\n",
      "job_desc = str(job_desc, 'utf-8')\n",
      "job_des = Cleaner.Cleaner(job_desc[0])\n",
      "\n",
      "scores = []\n",
      "for text in df['Context']:\n",
      "    raw = Cleaner.Cleaner(text)\n",
      "    score = Similar.match(raw[2], job_des[2])\n",
      "    scores.append(score)\n",
      "\n",
      "df['Scores'] = scores\n",
      "df2 = df.sort_values(by=['Scores'], ascending=False)\n",
      "print(df2.iloc[0, 1])\n",
      "\n",
      "import textdistance as td\n",
      "import Cleaner\n",
      "\n",
      "\n",
      "def match(resume, job_des):\n",
      "    j = td.jaccard.similarity(resume, job_des)\n",
      "    s = td.sorensen_dice.similarity(resume, job_des)\n",
      "    c = td.cosine.similarity(resume, job_des)\n",
      "    o = td.overlap.normalized_similarity(resume, job_des)\n",
      "    total = (j+s+c+o)/4\n",
      "    return total*100\n",
      "\n",
      "\n",
      "\n",
      "import Cleaner\n",
      "import Similar\n",
      "import textract as tx\n",
      "import pandas as pd\n",
      "import os\n",
      "import streamlit as st\n",
      "\n",
      "st.title(\"Naive Resume Matcher\")\n",
      "st.markdown(\"\"\" ### Ranking **Resumes** based on the Matching Skills as provided by the required job description. This uses a **Token, String and Word Embedding** based algorithm created to generate a match score that ranks a resume.\"\"\")\n",
      "\n",
      "\n",
      "resume_dir = \"Data/Resumes/\"\n",
      "job_desc_dir = \"Data/JobDesc/\"\n",
      "resume_names = os.listdir(resume_dir)\n",
      "document = []\n",
      "\n",
      "for res in resume_names:\n",
      "    temp = []\n",
      "    temp.append(res)\n",
      "    text = tx.process(resume_dir+res, encoding='ascii')\n",
      "    text = str(text, 'utf-8')\n",
      "    temp.append(text)\n",
      "    document.append(temp)\n",
      "\n",
      "df = pd.DataFrame(document, columns=['Name', 'Context'])\n",
      "\n",
      "# Only one Job Description should be present and in docx format\n",
      "job_docs = os.listdir(job_desc_dir)\n",
      "job_desc = tx.process(\n",
      "    job_desc_dir+job_docs[1], extension='docx', encoding='ascii')\n",
      "job_desc = str(job_desc, 'utf-8')\n",
      "job_des = Cleaner.Cleaner(job_desc)\n",
      "\n",
      "st.subheader(\"Job Description\")\n",
      "st.markdown(\" --- \")\n",
      "st.write(job_desc)\n",
      "st.markdown(\" --- \")\n",
      "\n",
      "scores = []\n",
      "for text in df['Context']:\n",
      "    raw = Cleaner.Cleaner(text)\n",
      "    score = Similar.match(raw[2], job_des[2])\n",
      "    scores.append(score)\n",
      "st.write(scores)\n",
      "df['Scores'] = scores\n",
      "\n",
      "st.dataframe(df)\n",
      "df2 = df.sort_values(by=['Scores'], ascending=False)\n",
      "st.dataframe(df2)\n",
      "print(df2.iloc[0, 1])\n",
      "\n",
      "import os\n",
      "import pandas as pd\n",
      "import textract as tx\n",
      "import Similar\n",
      "import Cleaner\n",
      "\n",
      "\n",
      "resume_dir = \"Data/Resumes/\"\n",
      "job_desc_dir = \"Data/JobDesc/\"\n",
      "resume_names = os.listdir(resume_dir)\n",
      "document = []\n",
      "\n",
      "for res in resume_names:\n",
      "    temp = []\n",
      "    temp.append(res)\n",
      "    text = tx.process(resume_dir+res, encoding='ascii')\n",
      "    text = str(text, 'utf-8')\n",
      "    temp.append(text)\n",
      "    document.append(temp)\n",
      "\n",
      "df = pd.DataFrame(document, columns=['Name', 'Context'])\n",
      "\n",
      "# Only one Job Description should be present and in docx format\n",
      "job_docs = os.listdir(job_desc_dir)\n",
      "job_desc = tx.process(\n",
      "    job_desc_dir+job_docs[1], extension='docx', encoding='ascii')\n",
      "job_desc = str(job_desc, 'utf-8')\n",
      "job_des = Cleaner.Cleaner(job_desc[0])\n",
      "\n",
      "scores = []\n",
      "for text in df['Context']:\n",
      "    raw = Cleaner.Cleaner(text)\n",
      "    score = Similar.match(raw[2], job_des[2])\n",
      "    scores.append(score)\n",
      "\n",
      "df['Scores'] = scores\n",
      "df2 = df.sort_values(by=['Scores'], ascending=False)\n",
      "print(df2.iloc[0, 1])\n",
      "\n",
      "from wordcloud import STOPWORDS\n",
      "from operator import index\n",
      "from wordcloud import WordCloud\n",
      "from pandas._config.config import options\n",
      "import Cleaner\n",
      "import Similar\n",
      "import textract as tx\n",
      "import pandas as pd\n",
      "import os\n",
      "import streamlit as st\n",
      "import plotly.express as px\n",
      "import plotly.graph_objects as go\n",
      "import matplotlib.pyplot as plt\n",
      "\n",
      "st.title(\"Naive Resume Matcher\")\n",
      "st.markdown(\"\"\"\n",
      "A Machine Learning Based Resume Matcher, to compare Resumes with Job Descriptions.\n",
      "Create a score based on how good/similar a resume is to the particular Job Description.\\n\n",
      "Algorihms used:-\n",
      "- **String Matching**\n",
      "    - Monge Elkan\n",
      "\n",
      "- **Token Based**\n",
      "    - Jaccard\n",
      "    - Cosine\n",
      "    - Sorensen-Dice\n",
      "    - Overlap Coefficient\n",
      "\n",
      "Total Score calculate is the overall average of the 4 mentioned token based algorithms and string based.\n",
      "\"\"\")\n",
      "\n",
      "resume_dir = \"Data/Resumes/\"\n",
      "job_desc_dir = \"Data/JobDesc/\"\n",
      "resume_names = os.listdir(resume_dir)\n",
      "job_description_names = os.listdir(job_desc_dir)\n",
      "\n",
      "document = []\n",
      "\n",
      "st.write(\"Total Resumes found : \", len(resume_names))\n",
      "st.write(\"Total Job Descriptions found : \", len(job_description_names))\n",
      "\n",
      "# to read all the resumes in the directory as provided by the user\n",
      "\n",
      "\n",
      "def read_resumes(list_of_resumes, resume_directory):\n",
      "    placeholder = []\n",
      "    for res in list_of_resumes:\n",
      "        temp = []\n",
      "        temp.append(res)\n",
      "        text = tx.process(resume_directory+res, encoding='ascii')\n",
      "        text = str(text, 'utf-8')\n",
      "        temp.append(text)\n",
      "        placeholder.append(temp)\n",
      "    return placeholder\n",
      "\n",
      "\n",
      "document = read_resumes(resume_names, resume_dir)\n",
      "\n",
      "df = pd.DataFrame(document, columns=['Name', 'Context'])\n",
      "\n",
      "if len(job_description_names) <= 1:\n",
      "    st.write(\"There is only \", len(job_description_names),\n",
      "             \"present. It will be used to create scores.\")\n",
      "else:\n",
      "    st.write(\"There are \", len(job_description_names),\n",
      "             \"Job Descriptions available. Please select one.\")\n",
      "if len(job_description_names) > 1:\n",
      "    option_yn = st.selectbox(\n",
      "        \"Show the Job Description Names?\", options=['NO', 'YES'])\n",
      "    if option_yn == 'YES':\n",
      "        st.write(job_description_names)\n",
      "\n",
      "\n",
      "index = st.slider(\"Which JD to select ? : \", 0,\n",
      "                  len(job_description_names)-1, 1)\n",
      "\n",
      "\n",
      "def read_job_description(n, list_of_job_files, job_description_directory):\n",
      "    job_desc = tx.process(\n",
      "        job_description_directory+list_of_job_files[n], extension='docx', encoding='ascii')\n",
      "\n",
      "    job_desc = str(job_desc, 'utf-8')\n",
      "    job_description = Cleaner.Cleaner(job_desc)\n",
      "    return [job_desc, job_description]\n",
      "\n",
      "\n",
      "job = read_job_description(index, job_description_names, job_desc_dir)\n",
      "\n",
      "option_yn = st.selectbox(\"Show the Job Description ?\", options=['NO', 'YES'])\n",
      "if option_yn == 'YES':\n",
      "    st.markdown(\"---\")\n",
      "    st.markdown(\"### Job Description :\")\n",
      "    st.text(job[0])\n",
      "    st.markdown(\"---\")\n",
      "\n",
      "\n",
      "def get_cleaned_data(resumes):\n",
      "    data = []\n",
      "    for text in resumes:\n",
      "        raw = Cleaner.Cleaner(text)\n",
      "        data.append(raw)\n",
      "    return data\n",
      "\n",
      "\n",
      "cleaned_data = get_cleaned_data(df['Context'])\n",
      "\n",
      "cleaned_df = pd.DataFrame(cleaned_data, columns=[\n",
      "                          'Cleaned', 'Selective', 'Selective Reduced'])\n",
      "\n",
      "\n",
      "def calculate_scores(resumes, job_description, x=2, y=2):\n",
      "    scores = []\n",
      "    for text in resumes:\n",
      "        raw = Cleaner.Cleaner(text)\n",
      "        score = Similar.match(raw[x], job_description[y])\n",
      "        scores.append(score)\n",
      "    return scores\n",
      "\n",
      "\n",
      "df['Scores'] = calculate_scores(df['Context'], job[1])\n",
      "\n",
      "df_sorted = df.sort_values(\n",
      "    by=['Scores'], ascending=False).reset_index(drop=True)\n",
      "\n",
      "df_sorted['Rank'] = pd.DataFrame(\n",
      "    [i for i in range(1, len(df_sorted['Scores'])+1)])\n",
      "\n",
      "fig = go.Figure(data=[go.Table(\n",
      "    header=dict(values=[\"Rank\", \"Name\", \"Scores\"],\n",
      "                fill_color='#00416d',\n",
      "                align='center', font=dict(color='white', size=16)),\n",
      "    cells=dict(values=[df_sorted.Rank, df_sorted.Name, df_sorted.Scores],\n",
      "               fill_color='#d6e0f0',\n",
      "               align='left'))])\n",
      "\n",
      "fig.update_layout(width=700, height=1200)\n",
      "st.write(fig)\n",
      "# st.dataframe(df_sorted)\n",
      "\n",
      "fig = px.bar(df_sorted, x=df_sorted['Name'], y=df_sorted['Scores'])\n",
      "# fig.update_layout(width=700, height=700)\n",
      "st.write(fig)\n",
      "\n",
      "\n",
      "def get_text_from_df(df_iter):\n",
      "    output = \" \"\n",
      "    for _ in df_iter:\n",
      "        output += str(_)\n",
      "        return output\n",
      "\n",
      "\n",
      "text_wc = get_text_from_df(cleaned_df['Selective'])\n",
      "\n",
      "wordcloud = WordCloud(width=3000, height=2000, random_state=1, background_color='salmon',\n",
      "                      colormap='Pastel1', collocations=False, stopwords=STOPWORDS).generate(text_wc)\n",
      "st.write(plt.imshow(wordcloud))\n",
      "\n",
      "option_2 = st.selectbox(\"Show the Best Matching Resumes?\", options=[\n",
      "    'NO', 'YES'])\n",
      "\n",
      "\n",
      "if option_2 == 'YES':\n",
      "    indx = st.slider(\"Which resume to display ?:\", 1, len(resume_names), 1)\n",
      "    st.write(\"Displaying Resume with Rank: \", indx)\n",
      "    st.markdown(\"---\")\n",
      "    st.markdown(\"### Resume :\")\n",
      "    value = df_sorted.iloc[indx-1, 1]\n",
      "    st.write(\"With a Match Score of :\", df_sorted.iloc[indx-1, 2])\n",
      "    fig = go.Figure(data=[go.Table(\n",
      "        header=dict(values=[\"Resume\"],\n",
      "                    fill_color='#f0a500',\n",
      "                    align='center', font=dict(color='white', size=16)),\n",
      "        cells=dict(values=[str(value)],\n",
      "                   fill_color='#f4f4f4',\n",
      "                   align='left'))])\n",
      "\n",
      "    fig.update_layout(width=800, height=1200)\n",
      "    st.write(fig)\n",
      "    # st.text(df_sorted.iloc[indx-1, 1])\n",
      "    st.markdown(\"---\")\n",
      "\n",
      "from wordcloud import STOPWORDS\n",
      "from operator import index\n",
      "from wordcloud import WordCloud\n",
      "from pandas._config.config import options\n",
      "import pandas as pd\n",
      "import streamlit as st\n",
      "import plotly.express as px\n",
      "import plotly.graph_objects as go\n",
      "import matplotlib.pyplot as plt\n",
      "\n",
      "st.title(\"Naive Resume Matcher\")\n",
      "st.markdown(\"\"\"\n",
      "A Machine Learning Based Resume Matcher, to compare Resumes with Job Descriptions.\n",
      "Create a score based on how good/similar a resume is to the particular Job Description.\\n\n",
      "Algorihms used:-\n",
      "- **String Matching**\n",
      "    - Monge Elkan\n",
      "\n",
      "- **Token Based**\n",
      "    - Jaccard\n",
      "    - Cosine\n",
      "    - Sorensen-Dice\n",
      "    - Overlap Coefficient\n",
      "\n",
      "Total Score calculate is the overall average of the 4 mentioned token based algorithms and string based.\n",
      "\"\"\")\n",
      "\n",
      "\n",
      "# to read all the resumes in the directory as provided by the user\n",
      "\n",
      "if len(job_description_names) <= 1:\n",
      "    st.write(\"There is only \", len(job_description_names),\n",
      "             \"present. It will be used to create scores.\")\n",
      "else:\n",
      "    st.write(\"There are \", len(job_description_names),\n",
      "             \"Job Descriptions available. Please select one.\")\n",
      "if len(job_description_names) > 1:\n",
      "    option_yn = st.selectbox(\n",
      "        \"Show the Job Description Names?\", options=['NO', 'YES'])\n",
      "    if option_yn == 'YES':\n",
      "        st.write(job_description_names)\n",
      "\n",
      "\n",
      "index = st.slider(\"Which JD to select ? : \", 0,\n",
      "                  len(job_description_names)-1, 1)\n",
      "\n",
      "\n",
      "job = read_job_description(index, job_description_names, job_desc_dir)\n",
      "\n",
      "option_yn = st.selectbox(\"Show the Job Description ?\", options=['NO', 'YES'])\n",
      "if option_yn == 'YES':\n",
      "    st.markdown(\"---\")\n",
      "    st.markdown(\"### Job Description :\")\n",
      "    st.text(job[0])\n",
      "    st.markdown(\"---\")\n",
      "\n",
      "\n",
      "def get_cleaned_data(resumes):\n",
      "    data = []\n",
      "    for text in resumes:\n",
      "        raw = Cleaner.Cleaner(text)\n",
      "        data.append(raw)\n",
      "    return data\n",
      "\n",
      "\n",
      "cleaned_data = get_cleaned_data(df['Context'])\n",
      "\n",
      "cleaned_df = pd.DataFrame(cleaned_data, columns=[\n",
      "                          'Cleaned', 'Selective', 'Selective Reduced'])\n",
      "\n",
      "\n",
      "def calculate_scores(resumes, job_description, x=2, y=2):\n",
      "    scores = []\n",
      "    for text in resumes:\n",
      "        raw = Cleaner.Cleaner(text)\n",
      "        score = Similar.match(raw[x], job_description[y])\n",
      "        scores.append(score)\n",
      "    return scores\n",
      "\n",
      "\n",
      "df['Scores'] = calculate_scores(df['Context'], job[1])\n",
      "\n",
      "df_sorted = df.sort_values(\n",
      "    by=['Scores'], ascending=False).reset_index(drop=True)\n",
      "\n",
      "df_sorted['Rank'] = pd.DataFrame(\n",
      "    [i for i in range(1, len(df_sorted['Scores'])+1)])\n",
      "\n",
      "fig = go.Figure(data=[go.Table(\n",
      "    header=dict(values=[\"Rank\", \"Name\", \"Scores\"],\n",
      "                fill_color='#00416d',\n",
      "                align='center', font=dict(color='white', size=16)),\n",
      "    cells=dict(values=[df_sorted.Rank, df_sorted.Name, df_sorted.Scores],\n",
      "               fill_color='#d6e0f0',\n",
      "               align='left'))])\n",
      "\n",
      "fig.update_layout(width=700, height=1200)\n",
      "st.write(fig)\n",
      "# st.dataframe(df_sorted)\n",
      "\n",
      "fig = px.bar(df_sorted, x=df_sorted['Name'], y=df_sorted['Scores'])\n",
      "# fig.update_layout(width=700, height=700)\n",
      "st.write(fig)\n",
      "\n",
      "\n",
      "def get_text_from_df(df_iter):\n",
      "    output = \" \"\n",
      "    for _ in df_iter:\n",
      "        output += str(_)\n",
      "        return output\n",
      "\n",
      "\n",
      "option_2 = st.selectbox(\"Show the Best Matching Resumes?\", options=[\n",
      "    'NO', 'YES'])\n",
      "\n",
      "\n",
      "if option_2 == 'YES':\n",
      "    indx = st.slider(\"Which resume to display ?:\", 1, len(resume_names), 1)\n",
      "    st.write(\"Displaying Resume with Rank: \", indx)\n",
      "    st.markdown(\"---\")\n",
      "    st.markdown(\"### Resume :\")\n",
      "    value = df_sorted.iloc[indx-1, 1]\n",
      "    st.write(\"With a Match Score of :\", df_sorted.iloc[indx-1, 2])\n",
      "    fig = go.Figure(data=[go.Table(\n",
      "        header=dict(values=[\"Resume\"],\n",
      "                    fill_color='#f0a500',\n",
      "                    align='center', font=dict(color='white', size=16)),\n",
      "        cells=dict(values=[str(value)],\n",
      "                   fill_color='#f4f4f4',\n",
      "                   align='left'))])\n",
      "\n",
      "    fig.update_layout(width=800, height=1200)\n",
      "    st.write(fig)\n",
      "    # st.text(df_sorted.iloc[indx-1, 1])\n",
      "    st.markdown(\"---\")\n",
      "\n",
      "from wordcloud import STOPWORDS\n",
      "from operator import index\n",
      "from wordcloud import WordCloud\n",
      "from pandas._config.config import options\n",
      "import pandas as pd\n",
      "import streamlit as st\n",
      "import plotly.express as px\n",
      "import plotly.graph_objects as go\n",
      "import matplotlib.pyplot as plt\n",
      "import Similar\n",
      "\n",
      "# Reading the CSV files prepared by the fileReader.py\n",
      "Resumes = pd.read_csv('Resume_data.csv')\n",
      "Jobs = pd.read_csv('Job_Data.csv')\n",
      "\n",
      "\n",
      "# Checking for Multiple Job Descriptions\n",
      "# If more than one Job Descriptions are available, it asks user to select one as well.\n",
      "\n",
      "if len(Jobs['Name']) <= 1:\n",
      "    st.write(\n",
      "        \"There is only 1 Job Description present. It will be used to create scores.\")\n",
      "else:\n",
      "    st.write(\"There are \", len(Jobs['Name']),\n",
      "             \"Job Descriptions available. Please select one.\")\n",
      "\n",
      "\n",
      "# Asking to Print the Job Desciption Names\n",
      "if len(Jobs['Name']) > 1:\n",
      "    option_yn = st.selectbox(\n",
      "        \"Show the Job Description Names?\", options=['NO', 'YES'])\n",
      "    if option_yn == 'YES':\n",
      "        index = [a for a in range(len(Jobs['Name']))]\n",
      "        fig = go.Figure(data=[go.Table(header=dict(values=[\"Job No.\", \"Job Desc. Name\"], line_color='darkslategray',\n",
      "                                                   fill_color='lightskyblue'),\n",
      "                                       cells=dict(values=[index, Jobs['Name']], line_color='darkslategray',\n",
      "                                                  fill_color='cyan'))\n",
      "                              ])\n",
      "        st.write(fig)\n",
      "\n",
      "\n",
      "# Asking to chose the Job Description\n",
      "index = st.slider(\"Which JD to select ? : \", 0,\n",
      "                  len(Jobs['Name'])-1, 1)\n",
      "\n",
      "\n",
      "option_yn = st.selectbox(\"Show the Job Description ?\", options=['NO', 'YES'])\n",
      "if option_yn == 'YES':\n",
      "    st.markdown(\"---\")\n",
      "    st.markdown(\"### Job Description :\")\n",
      "    st.text(Jobs['Context'][index])\n",
      "    st.markdown(\"---\")\n",
      "\n",
      "\n",
      "# def calculate_scores(resumes, job_description, x=5, y=5):\n",
      "#     scores = []\n",
      "#     for text in resumes:\n",
      "#         score = Similar.match(esumes['TF_Based'][x], Jobs[])\n",
      "#         scores.append(score)\n",
      "#     return scores\n",
      "\n",
      "def Scoring()\n",
      "\n",
      "from operator import index\n",
      "from pandas._config.config import options\n",
      "import Cleaner\n",
      "import Similar\n",
      "import textract as tx\n",
      "import pandas as pd\n",
      "import os\n",
      "import tf_idf\n",
      "\n",
      "resume_dir = \"Data/Resumes/\"\n",
      "job_desc_dir = \"Data/JobDesc/\"\n",
      "resume_names = os.listdir(resume_dir)\n",
      "job_description_names = os.listdir(job_desc_dir)\n",
      "\n",
      "document = []\n",
      "\n",
      "def read_resumes(list_of_resumes, resume_directory):\n",
      "    placeholder = []\n",
      "    for res in list_of_resumes:\n",
      "        temp = []\n",
      "        temp.append(res)\n",
      "        text = tx.process(resume_directory+res, encoding='ascii')\n",
      "        text = str(text, 'utf-8')\n",
      "        temp.append(text)\n",
      "        placeholder.append(temp)\n",
      "    return placeholder\n",
      "\n",
      "document = read_resumes(resume_names, resume_dir)\n",
      "\n",
      "def get_cleaned_words(document):\n",
      "    for i in range(len(document)):\n",
      "        raw = Cleaner.Cleaner(document[i][1])\n",
      "        document[i].append(\" \".join(raw[0]))\n",
      "        document[i].append(\" \".join(raw[1]))\n",
      "        document[i].append(\" \".join(raw[2]))\n",
      "        sentence = tf_idf.do_tfidf(document[i][2].split(\" \"))\n",
      "        document[i].append(sentence)\n",
      "    return document\n",
      "\n",
      "Doc=get_cleaned_words(document)\n",
      "\n",
      "Database = pd.DataFrame(document,columns=[\"Name\",\"Context\",\"Cleaned\",\"Selective\",\"Selective_Reduced\",\"TF_Based\"])\n",
      "\n",
      "Database.to_csv(\"Resume_data.csv\", index=False)\n",
      "\n",
      "def read_jobdescriptions(job_description_names, job_desc_dir):\n",
      "    placeholder = []\n",
      "    for tes in job_description_names:\n",
      "        temp = []\n",
      "        temp.append(tes)\n",
      "        text = tx.process(job_desc_dir+tes, encoding='ascii')\n",
      "        text = str(text, 'utf-8')\n",
      "        temp.append(text)\n",
      "        placeholder.append(temp)\n",
      "    return placeholder\n",
      "\n",
      "job_document = read_jobdescriptions(job_description_names, job_desc_dir)\n",
      "\n",
      "Jd=get_cleaned_words(job_document)\n",
      "\n",
      "jd_database = pd.DataFrame(Jd,columns=[\"Name\",\"Context\",\"Cleaned\",\"Selective\",\"Selective_Reduced\",\"TF_Based\"])\n",
      "\n",
      "jd_database.to_csv(\"Job_Data.csv\",index=False)\n",
      "from wordcloud import WordCloud\n",
      "import matplotlib.pyplot as plt\n",
      "\n",
      "\n",
      "def generate_wordcloud(text):\n",
      "    wordcloud = WordCloud(width=800, height=800,\n",
      "                          background_color='white',\n",
      "                          colormap='viridis', collocations=False,\n",
      "                          min_font_size=10).generate(text)\n",
      "    plt.figure(figsize=(8, 8), facecolor=None)\n",
      "    plt.imshow(wordcloud)\n",
      "    plt.axis(\"off\")\n",
      "    plt.tight_layout(pad=0)\n",
      "\n",
      "    plt.show()\n",
      "\n",
      "import gensim\n",
      "import gensim.corpora as corpora\n",
      "\n",
      "\n",
      "def get_list_of_words(document):\n",
      "    Document = []\n",
      "    for a in document:\n",
      "        raw = a.split(\" \")\n",
      "        Document.append(raw)\n",
      "    return Document\n",
      "\n",
      "\n",
      "def LDA(document):\n",
      "    id2word = corpora.Dictionary(document)\n",
      "    corpus = [id2word.doc2bow(text) for text in document]\n",
      "    lda_model = gensim.models.ldamodel.LdaModel(corpus=corpus, id2word=id2word, num_topics=5, random_state=100,\n",
      "                                                update_every=1, chunksize=100, passes=50, alpha='auto', per_word_topics=True)\n",
      "    return lda_model[corpus]\n",
      "\n",
      "\n",
      "def format_topics_sentences(ldamodel=None, corpus=corpus, texts=Document):\n",
      "    sent_topics_df = []\n",
      "    for i, row_list in enumerate(ldamodel[corpus]):\n",
      "        row = row_list[0] if ldamodel.per_word_topics else row_list\n",
      "        row = sorted(row, key=lambda x: (x[1]), reverse=True)\n",
      "        for j, (topic_num, prop_topic) in enumerate(row):\n",
      "            if j == 0:\n",
      "                wp = ldamodel.show_topic(topic_num)\n",
      "                topic_keywords = \", \".join([word for word, prop in wp])\n",
      "                sent_topics_df.append(\n",
      "                    [i, int(topic_num), round(prop_topic, 4)*100, topic_keywords])\n",
      "            else:\n",
      "                break\n",
      "\n",
      "    return(sent_topics_df)\n",
      "\n",
      "from sklearn.feature_extraction.text import TfidfVectorizer\n",
      "\n",
      "def do_tfidf(token):\n",
      "    tfidf = TfidfVectorizer(max_df=0.05, min_df=0.001)\n",
      "    words = tfidf.fit_transform(token)\n",
      "    sentence = \" \".join(tfidf.get_feature_names())\n",
      "    return sentence\n",
      "import gensim\n",
      "import gensim.corpora as corpora\n",
      "from operator import index\n",
      "from wordcloud import WordCloud\n",
      "from pandas._config.config import options\n",
      "import pandas as pd\n",
      "import streamlit as st\n",
      "import plotly.express as px\n",
      "import plotly.graph_objects as go\n",
      "import matplotlib.pyplot as plt\n",
      "import Similar\n",
      "\n",
      "# Reading the CSV files prepared by the fileReader.py\n",
      "Resumes = pd.read_csv('Resume_data.csv')\n",
      "Jobs = pd.read_csv('Job_Data.csv')\n",
      "\n",
      "\n",
      "def get_list_of_words(document):\n",
      "    Document = []\n",
      "\n",
      "    for a in document:\n",
      "        raw = a.split(\" \")\n",
      "        Document.append(raw)\n",
      "\n",
      "    return Document\n",
      "\n",
      "\n",
      "document = get_list_of_words(Resumes['Cleaned'])\n",
      "\n",
      "id2word = corpora.Dictionary(document)\n",
      "corpus = [id2word.doc2bow(text) for text in document]\n",
      "\n",
      "lda_model = gensim.models.ldamodel.LdaModel(corpus=corpus, id2word=id2word, num_topics=5, random_state=100,\n",
      "                                            update_every=1, chunksize=100, passes=50, alpha='auto', per_word_topics=True)\n",
      "\n",
      "\n",
      "def format_topics_sentences(ldamodel, corpus):\n",
      "    sent_topics_df = []\n",
      "    for i, row_list in enumerate(ldamodel[corpus]):\n",
      "        row = row_list[0] if ldamodel.per_word_topics else row_list\n",
      "        row = sorted(row, key=lambda x: (x[1]), reverse=True)\n",
      "        for j, (topic_num, prop_topic) in enumerate(row):\n",
      "            if j == 0:\n",
      "                wp = ldamodel.show_topic(topic_num)\n",
      "                topic_keywords = \", \".join([word for word, prop in wp])\n",
      "                sent_topics_df.append(\n",
      "                    [i, int(topic_num), round(prop_topic, 4)*100, topic_keywords])\n",
      "            else:\n",
      "                break\n",
      "\n",
      "    return(sent_topics_df)\n",
      "\n",
      "\n",
      "df_topic_sents_keywords = format_topics_sentences(\n",
      "    ldamodel=lda_model, corpus=corpus)\n",
      "df_some = pd.DataFrame(df_topic_sents_keywords, columns=[\n",
      "                       'Document No', 'Dominant Topic', 'Topic % Contribution', 'Keywords'])\n",
      "df_some['Names'] = Resumes['Name']\n",
      "\n",
      "df = df_some\n",
      "fig = px.sunburst(df, path=['Dominant Topic', 'Names'], values='Topic % Contribution',\n",
      "                  color='Dominant Topic', color_continuous_scale='viridis', width=800, height=800, title=\"LOL XD\")\n",
      "st.write(fig)\n",
      "# Checking for Multiple Job Descriptions\n",
      "# If more than one Job Descriptions are available, it asks user to select one as well.\n",
      "if len(Jobs['Name']) <= 1:\n",
      "    st.write(\n",
      "        \"There is only 1 Job Description present. It will be used to create scores.\")\n",
      "else:\n",
      "    st.write(\"There are \", len(Jobs['Name']),\n",
      "             \"Job Descriptions available. Please select one.\")\n",
      "\n",
      "\n",
      "# Asking to Print the Job Desciption Names\n",
      "if len(Jobs['Name']) > 1:\n",
      "    option_yn = st.selectbox(\n",
      "        \"Show the Job Description Names?\", options=['NO', 'YES'])\n",
      "    if option_yn == 'YES':\n",
      "        index = [a for a in range(len(Jobs['Name']))]\n",
      "        fig = go.Figure(data=[go.Table(header=dict(values=[\"Job No.\", \"Job Desc. Name\"], line_color='darkslategray',\n",
      "                                                   fill_color='lightskyblue'),\n",
      "                                       cells=dict(values=[index, Jobs['Name']], line_color='darkslategray',\n",
      "                                                  fill_color='cyan'))\n",
      "                              ])\n",
      "        st.write(fig)\n",
      "\n",
      "\n",
      "# Asking to chose the Job Description\n",
      "index = st.slider(\"Which JD to select ? : \", 0,\n",
      "                  len(Jobs['Name'])-1, 1)\n",
      "\n",
      "\n",
      "option_yn = st.selectbox(\"Show the Job Description ?\", options=['NO', 'YES'])\n",
      "if option_yn == 'YES':\n",
      "    st.markdown(\"---\")\n",
      "    st.markdown(\"### Job Description :\")\n",
      "    st.text(Jobs['Context'][index])\n",
      "    st.markdown(\"---\")\n",
      "\n",
      "\n",
      "def calculate_scores(resumes, job_description):\n",
      "    scores = []\n",
      "    for x in range(resumes.shape[0]):\n",
      "        score = Similar.match(\n",
      "            resumes['TF_Based'][x], job_description['Cleaned'][index])\n",
      "        scores.append(score)\n",
      "    return scores\n",
      "\n",
      "\n",
      "Resumes['Scores'] = calculate_scores(Resumes, Jobs)\n",
      "\n",
      "Ranked_resumes = Resumes.sort_values(\n",
      "    by=['Scores'], ascending=False).reset_index(drop=True)\n",
      "\n",
      "Ranked_resumes['Rank'] = pd.DataFrame(\n",
      "    [i for i in range(1, len(Ranked_resumes['Scores'])+1)])\n",
      "\n",
      "fig = go.Figure(data=[go.Table(\n",
      "    header=dict(values=[\"Rank\", \"Name\", \"Scores\"],\n",
      "                fill_color='#00416d',\n",
      "                align='center', font=dict(color='white', size=16)),\n",
      "    cells=dict(values=[Ranked_resumes.Rank, Ranked_resumes.Name, Ranked_resumes.Scores],\n",
      "               fill_color='#d6e0f0',\n",
      "               align='left'))])\n",
      "\n",
      "fig.update_layout(title=\"Top Ranked Resumes\", width=700, height=1100)\n",
      "st.write(fig)\n",
      "\n",
      "st.markdown(\"---\")\n",
      "\n",
      "fig = px.bar(Ranked_resumes,\n",
      "             x=Ranked_resumes['Name'], y=Ranked_resumes['Scores'], color='Scores',\n",
      "             color_continuous_scale='haline', title=\"Score and Rank Distribution\")\n",
      "# fig.update_layout(width=700, height=700)\n",
      "st.write(fig)\n",
      "\n",
      "\n",
      "st.markdown(\"---\")\n",
      "\n",
      "\n",
      "def generate_wordcloud(text):\n",
      "    wordcloud = WordCloud(width=800, height=800,\n",
      "                          background_color='white',\n",
      "                          colormap='viridis', collocations=False,\n",
      "                          min_font_size=10).generate(text)\n",
      "    plt.figure(figsize=(8, 8), facecolor=None)\n",
      "    plt.imshow(wordcloud)\n",
      "    plt.axis(\"off\")\n",
      "    plt.tight_layout(pad=0)\n",
      "\n",
      "    plt.show()\n",
      "\n",
      "\n",
      "option_2 = st.selectbox(\"Show the Best Matching Resumes?\", options=[\n",
      "    'NO', 'YES'])\n",
      "if option_2 == 'YES':\n",
      "    indx = st.slider(\"Which resume to display ?:\",\n",
      "                     1, Ranked_resumes.shape[0], 1)\n",
      "    st.write(\"Displaying Resume with Rank: \", indx)\n",
      "    st.markdown(\"---\")\n",
      "    st.markdown(\"## **Resume** \")\n",
      "    value = Ranked_resumes.iloc[indx-1, 2]\n",
      "    st.markdown(\"#### The Word Cloud For the Resume\")\n",
      "    wordcloud = WordCloud(width=800, height=800,\n",
      "                          background_color='white',\n",
      "                          colormap='viridis', collocations=False,\n",
      "                          min_font_size=10).generate(value)\n",
      "    plt.figure(figsize=(7, 7), facecolor=None)\n",
      "    plt.imshow(wordcloud)\n",
      "    plt.axis(\"off\")\n",
      "    plt.tight_layout(pad=0)\n",
      "    st.pyplot()\n",
      "\n",
      "    st.write(\"With a Match Score of :\", Ranked_resumes.iloc[indx-1, 6])\n",
      "    fig = go.Figure(data=[go.Table(\n",
      "        header=dict(values=[\"Resume\"],\n",
      "                    fill_color='#f0a500',\n",
      "                    align='center', font=dict(color='white', size=16)),\n",
      "        cells=dict(values=[str(value)],\n",
      "                   fill_color='#f4f4f4',\n",
      "                   align='left'))])\n",
      "\n",
      "    fig.update_layout(width=800, height=1200)\n",
      "    st.write(fig)\n",
      "    # st.text(df_sorted.iloc[indx-1, 1])\n",
      "    st.markdown(\"---\")\n",
      "\n",
      "from wordcloud import WordCloud\n",
      "import matplotlib.pyplot as plt\n",
      "\n",
      "\n",
      "def generate_wordcloud(text):\n",
      "    wordcloud = WordCloud(width=800, height=800,\n",
      "                          background_color='white',\n",
      "                          colormap='viridis', collocations=False,\n",
      "                          min_font_size=10).generate(text)\n",
      "    plt.figure(figsize=(8, 8), facecolor=None)\n",
      "    plt.imshow(wordcloud)\n",
      "    plt.axis(\"off\")\n",
      "    plt.tight_layout(pad=0)\n",
      "\n",
      "    plt.show()\n",
      "    return plt\n",
      "\n",
      "import gensim\n",
      "import gensim.corpora as corpora\n",
      "\n",
      "\n",
      "def get_list_of_words(document):\n",
      "    Document = []\n",
      "\n",
      "    for a in document:\n",
      "        raw = a.split(\" \")\n",
      "        Document.append(raw)\n",
      "\n",
      "    id2word = corpora.Dictionary(document)\n",
      "    corpus = [id2word.doc2bow(text) for text in document]\n",
      "\n",
      "    lda_model = gensim.models.ldamodel.LdaModel(corpus=corpus, id2word=id2word, num_topics=5, random_state=100,\n",
      "                                                update_every=1, chunksize=100, passes=50, alpha='auto', per_word_topics=True)\n",
      "\n",
      "    return lda_model\n",
      "\n",
      "\n",
      "# def LDA(document):\n",
      "#     id2word = corpora.Dictionary(document)\n",
      "#     corpus = [id2word.doc2bow(text) for text in document]\n",
      "#     lda_model = gensim.models.ldamodel.LdaModel(corpus=corpus, id2word=id2word, num_topics=5, random_state=100,\n",
      "#                                                 update_every=1, chunksize=100, passes=50, alpha='auto', per_word_topics=True)\n",
      "#     return lda_model\n",
      "\n",
      "\n",
      "def format_topics_sentences(ldamodel, corpus, texts):\n",
      "    sent_topics_df = []\n",
      "    for i, row_list in enumerate(ldamodel[corpus]):\n",
      "        row = row_list[0] if ldamodel.per_word_topics else row_list\n",
      "        row = sorted(row, key=lambda x: (x[1]), reverse=True)\n",
      "        for j, (topic_num, prop_topic) in enumerate(row):\n",
      "            if j == 0:\n",
      "                wp = ldamodel.show_topic(topic_num)\n",
      "                topic_keywords = \", \".join([word for word, prop in wp])\n",
      "                sent_topics_df.append(\n",
      "                    [i, int(topic_num), round(prop_topic, 4)*100, topic_keywords])\n",
      "            else:\n",
      "                break\n",
      "\n",
      "    return(sent_topics_df)\n",
      "\n",
      "\n",
      "def if __name__ == \"__main__\":\n",
      "    pass\n",
      "\n",
      "File Progress/Archived Code/app.py has no source code\n",
      "\n",
      "File Progress/Archived Code/compute.py has no source code\n",
      "\n",
      "File Progress/Archived Code/generate_wordcloud.py has no source code\n",
      "\n",
      "File LOC changed from 48 to 48\n",
      "import gensim\n",
      "import gensim.corpora as corpora\n",
      "\n",
      "\n",
      "def get_list_of_words(document):\n",
      "    Document = []\n",
      "\n",
      "    for a in document:\n",
      "        raw = a.split(\" \")\n",
      "        Document.append(raw)\n",
      "\n",
      "    id2word = corpora.Dictionary(document)\n",
      "    corpus = [id2word.doc2bow(text) for text in document]\n",
      "\n",
      "    lda_model = gensim.models.ldamodel.LdaModel(corpus=corpus, id2word=id2word, num_topics=5, random_state=100,\n",
      "                                                update_every=1, chunksize=100, passes=50, alpha='auto', per_word_topics=True)\n",
      "\n",
      "    return lda_model\n",
      "\n",
      "\n",
      "# def LDA(document):\n",
      "#     id2word = corpora.Dictionary(document)\n",
      "#     corpus = [id2word.doc2bow(text) for text in document]\n",
      "#     lda_model = gensim.models.ldamodel.LdaModel(corpus=corpus, id2word=id2word, num_topics=5, random_state=100,\n",
      "#                                                 update_every=1, chunksize=100, passes=50, alpha='auto', per_word_topics=True)\n",
      "#     return lda_model\n",
      "\n",
      "\n",
      "def format_topics_sentences(ldamodel, corpus, texts):\n",
      "    sent_topics_df = []\n",
      "    for i, row_list in enumerate(ldamodel[corpus]):\n",
      "        row = row_list[0] if ldamodel.per_word_topics else row_list\n",
      "        row = sorted(row, key=lambda x: (x[1]), reverse=True)\n",
      "        for j, (topic_num, prop_topic) in enumerate(row):\n",
      "            if j == 0:\n",
      "                wp = ldamodel.show_topic(topic_num)\n",
      "                topic_keywords = \", \".join([word for word, prop in wp])\n",
      "                sent_topics_df.append(\n",
      "                    [i, int(topic_num), round(prop_topic, 4)*100, topic_keywords])\n",
      "            else:\n",
      "                break\n",
      "\n",
      "    return(sent_topics_df)\n",
      "\n",
      "\n",
      "# def if __name__ == \"__main__\":\n",
      "#     pass\n",
      "\n",
      "import matplotlib.colors as mcolors\n",
      "import gensim\n",
      "import gensim.corpora as corpora\n",
      "from operator import index\n",
      "from wordcloud import WordCloud\n",
      "from pandas._config.config import options\n",
      "import pandas as pd\n",
      "import streamlit as st\n",
      "import plotly.express as px\n",
      "import plotly.graph_objects as go\n",
      "import matplotlib.pyplot as plt\n",
      "import Similar\n",
      "from PIL import Image\n",
      "import time\n",
      "\n",
      "\n",
      "image = Image.open('NAIVE RESUME MATCHER.png')\n",
      "st.image(image, use_column_width=True)\n",
      "\n",
      "st.title(\"Naive Resume Matcher\")\n",
      "st.markdown(\"\"\"\n",
      "A Machine Learning Based Resume Matcher, to compare Resumes with Job Descriptions.\n",
      "Create a score based on how good/similar a resume is to the particular Job Description.\\n\n",
      "Documents are sorted based on Their TF-IDF Scores (Term Frequency-Inverse Document Frequency)\n",
      "\n",
      "Matching Algorihms used are :-\n",
      "- **String Matching**\n",
      "    - Monge Elkan\n",
      "\n",
      "- **Token Based**\n",
      "    - Jaccard\n",
      "    - Cosine\n",
      "    - Sorensen-Dice\n",
      "    - Overlap Coefficient\n",
      "\n",
      "Topic Modelling of Resumes is done to provide additional information about the resumes and what clusters/topics,\n",
      "the belong to. \n",
      "For this :-\n",
      "\n",
      "1. LSA (Latenet Semantic Analysis) (aka TruncatedSVD in sklearn) is done after TF-IDF.\n",
      "2. id2word, and doc2word algorithms are used on the Documents.\n",
      "3. LDA or Latent Dirichlet Allocation is done to extract the Topics from the Document set.(In this case Resumes)\n",
      "4. Additional Plots are done to gain more insights about the document.\n",
      "\n",
      "Total Score calculate is the overall average of the 4 mentioned token based algorithms and string based.\n",
      "\"\"\")\n",
      "\n",
      "\n",
      "# Reading the CSV files prepared by the fileReader.py\n",
      "Resumes = pd.read_csv('Resume_data.csv')\n",
      "Jobs = pd.read_csv('Job_Data.csv')\n",
      "\n",
      "\n",
      "############################### JOB DESCRIPTION CODE ######################################\n",
      "# Checking for Multiple Job Descriptions\n",
      "# If more than one Job Descriptions are available, it asks user to select one as well.\n",
      "if len(Jobs['Name']) <= 1:\n",
      "    st.write(\n",
      "        \"There is only 1 Job Description present. It will be used to create scores.\")\n",
      "else:\n",
      "    st.write(\"There are \", len(Jobs['Name']),\n",
      "             \"Job Descriptions available. Please select one.\")\n",
      "\n",
      "\n",
      "# Asking to Print the Job Desciption Names\n",
      "if len(Jobs['Name']) > 1:\n",
      "    option_yn = st.selectbox(\n",
      "        \"Show the Job Description Names?\", options=['YES', 'NO'])\n",
      "    if option_yn == 'YES':\n",
      "        index = [a for a in range(len(Jobs['Name']))]\n",
      "        fig = go.Figure(data=[go.Table(header=dict(values=[\"Job No.\", \"Job Desc. Name\"], line_color='darkslategray',\n",
      "                                                   fill_color='lightskyblue'),\n",
      "                                       cells=dict(values=[index, Jobs['Name']], line_color='darkslategray',\n",
      "                                                  fill_color='cyan'))\n",
      "                              ])\n",
      "        fig.update_layout(width=700, height=400)\n",
      "        st.write(fig)\n",
      "\n",
      "\n",
      "# Asking to chose the Job Description\n",
      "index = st.slider(\"Which JD to select ? : \", 0,\n",
      "                  len(Jobs['Name'])-1, 1)\n",
      "\n",
      "\n",
      "option_yn = st.selectbox(\"Show the Job Description ?\", options=['YES', 'NO'])\n",
      "if option_yn == 'YES':\n",
      "    st.markdown(\"---\")\n",
      "    st.markdown(\"### Job Description :\")\n",
      "    fig = go.Figure(data=[go.Table(\n",
      "        header=dict(values=[\"Job Description\"],\n",
      "                    fill_color='#f0a500',\n",
      "                    align='center', font=dict(color='white', size=16)),\n",
      "        cells=dict(values=[Jobs['Context'][index]],\n",
      "                   fill_color='#f4f4f4',\n",
      "                   align='left'))])\n",
      "\n",
      "    fig.update_layout(width=800, height=500)\n",
      "    st.write(fig)\n",
      "    st.markdown(\"---\")\n",
      "\n",
      "\n",
      "#################################### SCORE CALCUATION ################################\n",
      "\n",
      "def calculate_scores(resumes, job_description):\n",
      "    scores = []\n",
      "    for x in range(resumes.shape[0]):\n",
      "        score = Similar.match(\n",
      "            resumes['TF_Based'][x], job_description['Cleaned'][index])\n",
      "        scores.append(score)\n",
      "    return scores\n",
      "\n",
      "\n",
      "Resumes['Scores'] = calculate_scores(Resumes, Jobs)\n",
      "\n",
      "Ranked_resumes = Resumes.sort_values(\n",
      "    by=['Scores'], ascending=False).reset_index(drop=True)\n",
      "\n",
      "Ranked_resumes['Rank'] = pd.DataFrame(\n",
      "    [i for i in range(1, len(Ranked_resumes['Scores'])+1)])\n",
      "\n",
      "########################### SCORE TABLE PLOT ##############################\n",
      "\n",
      "fig1 = go.Figure(data=[go.Table(\n",
      "    header=dict(values=[\"Rank\", \"Name\", \"Scores\"],\n",
      "                fill_color='#00416d',\n",
      "                align='center', font=dict(color='white', size=16)),\n",
      "    cells=dict(values=[Ranked_resumes.Rank, Ranked_resumes.Name, Ranked_resumes.Scores],\n",
      "               fill_color='#d6e0f0',\n",
      "               align='left'))])\n",
      "\n",
      "fig1.update_layout(title=\"Top Ranked Resumes\", width=700, height=1100)\n",
      "st.write(fig1)\n",
      "\n",
      "st.markdown(\"---\")\n",
      "\n",
      "fig2 = px.bar(Ranked_resumes,\n",
      "              x=Ranked_resumes['Name'], y=Ranked_resumes['Scores'], color='Scores',\n",
      "              color_continuous_scale='haline', title=\"Score and Rank Distribution\")\n",
      "# fig.update_layout(width=700, height=700)\n",
      "st.write(fig2)\n",
      "\n",
      "\n",
      "st.markdown(\"---\")\n",
      "\n",
      "############################################ TF-IDF Code ###################################\n",
      "\n",
      "\n",
      "def get_list_of_words(document):\n",
      "    Document = []\n",
      "\n",
      "    for a in document:\n",
      "        raw = a.split(\" \")\n",
      "        Document.append(raw)\n",
      "\n",
      "    return Document\n",
      "\n",
      "\n",
      "document = get_list_of_words(Resumes['Cleaned'])\n",
      "\n",
      "id2word = corpora.Dictionary(document)\n",
      "corpus = [id2word.doc2bow(text) for text in document]\n",
      "\n",
      "lda_model = gensim.models.ldamodel.LdaModel(corpus=corpus, id2word=id2word, num_topics=5, random_state=100,\n",
      "                                            update_every=1, chunksize=100, passes=50, alpha='auto', per_word_topics=True)\n",
      "\n",
      "################################### LDA CODE ##############################################\n",
      "\n",
      "\n",
      "def format_topics_sentences(ldamodel, corpus):\n",
      "    sent_topics_df = []\n",
      "    for i, row_list in enumerate(ldamodel[corpus]):\n",
      "        row = row_list[0] if ldamodel.per_word_topics else row_list\n",
      "        row = sorted(row, key=lambda x: (x[1]), reverse=True)\n",
      "        for j, (topic_num, prop_topic) in enumerate(row):\n",
      "            if j == 0:\n",
      "                wp = ldamodel.show_topic(topic_num)\n",
      "                topic_keywords = \", \".join([word for word, prop in wp])\n",
      "                sent_topics_df.append(\n",
      "                    [i, int(topic_num), round(prop_topic, 4)*100, topic_keywords])\n",
      "            else:\n",
      "                break\n",
      "\n",
      "    return(sent_topics_df)\n",
      "\n",
      "\n",
      "################################# Topic Word Cloud Code #####################################\n",
      "\n",
      "st.markdown(\"## Topics and Topic Related Keywords \")\n",
      "st.markdown(\n",
      "    \"\"\"This Wordcloud representation shows the Topic Number and the Top Keywords that contstitute a Topic.\n",
      "    This further is used to cluster the resumes.      \"\"\")\n",
      "\n",
      "cols = [color for name, color in mcolors.TABLEAU_COLORS.items()]\n",
      "\n",
      "cloud = WordCloud(background_color='white',\n",
      "                  width=2500,\n",
      "                  height=1800,\n",
      "                  max_words=10,\n",
      "                  colormap='tab10',\n",
      "                  collocations=False,\n",
      "                  color_func=lambda *args, **kwargs: cols[i],\n",
      "                  prefer_horizontal=1.0)\n",
      "\n",
      "topics = lda_model.show_topics(formatted=False)\n",
      "\n",
      "fig, axes = plt.subplots(2, 2, figsize=(10, 10), sharex=True, sharey=True)\n",
      "\n",
      "for i, ax in enumerate(axes.flatten()):\n",
      "    fig.add_subplot(ax)\n",
      "    topic_words = dict(topics[i][1])\n",
      "    cloud.generate_from_frequencies(topic_words, max_font_size=300)\n",
      "    plt.gca().imshow(cloud)\n",
      "    plt.gca().set_title('Topic ' + str(i), fontdict=dict(size=16))\n",
      "    plt.gca().axis('off')\n",
      "\n",
      "\n",
      "plt.subplots_adjust(wspace=0, hspace=0)\n",
      "plt.axis('off')\n",
      "plt.margins(x=0, y=0)\n",
      "plt.tight_layout()\n",
      "st.pyplot()\n",
      "\n",
      "st.markdown(\"---\")\n",
      "####################### SETTING UP THE DATAFRAME FOR SUNBURST-GRAPH ############################\n",
      "df_topic_sents_keywords = format_topics_sentences(\n",
      "    ldamodel=lda_model, corpus=corpus)\n",
      "df_some = pd.DataFrame(df_topic_sents_keywords, columns=[\n",
      "                       'Document No', 'Dominant Topic', 'Topic % Contribution', 'Keywords'])\n",
      "df_some['Names'] = Resumes['Name']\n",
      "\n",
      "df = df_some\n",
      "\n",
      "st.markdown(\"## Topic Modelling of Resumes \")\n",
      "st.markdown(\n",
      "    \"Using LDA to divide the topics into a number of usefull topics and creating a Cluster of matching topic resumes.  \")\n",
      "fig3 = px.sunburst(df, path=['Dominant Topic', 'Names'], values='Topic % Contribution',\n",
      "                   color='Dominant Topic', color_continuous_scale='viridis', width=800, height=800, title=\"Topic Distribution Graph\")\n",
      "st.write(fig3)\n",
      "\n",
      "\n",
      "############################## RESUME PRINTING #############################\n",
      "\n",
      "# my_bar = st.progress(0)\n",
      "# for percent_complete in range(100):\n",
      "#     time.sleep(0.1)\n",
      "#     my_bar.progress(percent_complete + 1)\n",
      "\n",
      "option_2 = st.selectbox(\"Show the Best Matching Resumes?\", options=[\n",
      "    'NO', 'YES'])\n",
      "if option_2 == 'YES':\n",
      "    indx = st.slider(\"Which resume to display ?:\",\n",
      "                     1, Ranked_resumes.shape[0], 1)\n",
      "\n",
      "    st.write(\"Displaying Resume with Rank: \", indx)\n",
      "    st.markdown(\"---\")\n",
      "    st.markdown(\"## **Resume** \")\n",
      "    value = Ranked_resumes.iloc[indx-1, 2]\n",
      "    st.markdown(\"#### The Word Cloud For the Resume\")\n",
      "    wordcloud = WordCloud(width=800, height=800,\n",
      "                          background_color='white',\n",
      "                          colormap='viridis', collocations=False,\n",
      "                          min_font_size=10).generate(value)\n",
      "    plt.figure(figsize=(7, 7), facecolor=None)\n",
      "    plt.imshow(wordcloud)\n",
      "    plt.axis(\"off\")\n",
      "    plt.tight_layout(pad=0)\n",
      "    st.pyplot()\n",
      "\n",
      "    st.write(\"With a Match Score of :\", Ranked_resumes.iloc[indx-1, 6])\n",
      "    fig = go.Figure(data=[go.Table(\n",
      "        header=dict(values=[\"Resume\"],\n",
      "                    fill_color='#f0a500',\n",
      "                    align='center', font=dict(color='white', size=16)),\n",
      "        cells=dict(values=[str(value)],\n",
      "                   fill_color='#f4f4f4',\n",
      "                   align='left'))])\n",
      "\n",
      "    fig.update_layout(width=800, height=1200)\n",
      "    st.write(fig)\n",
      "    # st.text(df_sorted.iloc[indx-1, 1])\n",
      "    st.markdown(\"---\")\n",
      "\n",
      "File Cleaner.py has no source code\n",
      "\n",
      "File Distill.py has no source code\n",
      "\n",
      "File Similar.py has no source code\n",
      "\n",
      "File app2.py has no source code\n",
      "\n",
      "File fileReader.py has no source code\n",
      "\n",
      "File tf_idf.py has no source code\n",
      "\n",
      "File LOC changed from 4507 to 4507\n",
      "File LOC changed from 282 to 282\n",
      "import matplotlib.colors as mcolors\n",
      "import gensim\n",
      "import gensim.corpora as corpora\n",
      "from operator import index\n",
      "from wordcloud import WordCloud\n",
      "from pandas._config.config import options\n",
      "import pandas as pd\n",
      "import streamlit as st\n",
      "import plotly.express as px\n",
      "import plotly.graph_objects as go\n",
      "import matplotlib.pyplot as plt\n",
      "import Similar\n",
      "from PIL import Image\n",
      "import time\n",
      "\n",
      "\n",
      "image = Image.open('Images//NAIVE RESUME MATCHER.png')\n",
      "st.image(image, use_column_width=True)\n",
      "\n",
      "st.title(\"Naive Resume Matcher\")\n",
      "st.markdown(\"\"\"\n",
      "A Machine Learning Based Resume Matcher, to compare Resumes with Job Descriptions.\n",
      "Create a score based on how good/similar a resume is to the particular Job Description.\\n\n",
      "Documents are sorted based on Their TF-IDF Scores (Term Frequency-Inverse Document Frequency)\n",
      "\n",
      "Matching Algorihms used are :-\n",
      "- **String Matching**\n",
      "    - Monge Elkan\n",
      "\n",
      "- **Token Based**\n",
      "    - Jaccard\n",
      "    - Cosine\n",
      "    - Sorensen-Dice\n",
      "    - Overlap Coefficient\n",
      "\n",
      "Topic Modelling of Resumes is done to provide additional information about the resumes and what clusters/topics,\n",
      "the belong to. \n",
      "For this :-\n",
      "\n",
      "1. LSA (Latenet Semantic Analysis) (aka TruncatedSVD in sklearn) is done after TF-IDF.\n",
      "2. id2word, and doc2word algorithms are used on the Documents.\n",
      "3. LDA or Latent Dirichlet Allocation is done to extract the Topics from the Document set.(In this case Resumes)\n",
      "4. Additional Plots are done to gain more insights about the document.\n",
      "\n",
      "Total Score calculate is the overall average of the 4 mentioned token based algorithms and string based.\n",
      "\"\"\")\n",
      "\n",
      "\n",
      "# Reading the CSV files prepared by the fileReader.py\n",
      "Resumes = pd.read_csv('Resume_Data.csv')\n",
      "Jobs = pd.read_csv('Job_Data.csv')\n",
      "\n",
      "\n",
      "############################### JOB DESCRIPTION CODE ######################################\n",
      "# Checking for Multiple Job Descriptions\n",
      "# If more than one Job Descriptions are available, it asks user to select one as well.\n",
      "if len(Jobs['Name']) <= 1:\n",
      "    st.write(\n",
      "        \"There is only 1 Job Description present. It will be used to create scores.\")\n",
      "else:\n",
      "    st.write(\"There are \", len(Jobs['Name']),\n",
      "             \"Job Descriptions available. Please select one.\")\n",
      "\n",
      "\n",
      "# Asking to Print the Job Desciption Names\n",
      "if len(Jobs['Name']) > 1:\n",
      "    option_yn = st.selectbox(\n",
      "        \"Show the Job Description Names?\", options=['YES', 'NO'])\n",
      "    if option_yn == 'YES':\n",
      "        index = [a for a in range(len(Jobs['Name']))]\n",
      "        fig = go.Figure(data=[go.Table(header=dict(values=[\"Job No.\", \"Job Desc. Name\"], line_color='darkslategray',\n",
      "                                                   fill_color='lightskyblue'),\n",
      "                                       cells=dict(values=[index, Jobs['Name']], line_color='darkslategray',\n",
      "                                                  fill_color='cyan'))\n",
      "                              ])\n",
      "        fig.update_layout(width=700, height=400)\n",
      "        st.write(fig)\n",
      "\n",
      "\n",
      "# Asking to chose the Job Description\n",
      "index = st.slider(\"Which JD to select ? : \", 0,\n",
      "                  len(Jobs['Name'])-1, 1)\n",
      "\n",
      "\n",
      "option_yn = st.selectbox(\"Show the Job Description ?\", options=['YES', 'NO'])\n",
      "if option_yn == 'YES':\n",
      "    st.markdown(\"---\")\n",
      "    st.markdown(\"### Job Description :\")\n",
      "    fig = go.Figure(data=[go.Table(\n",
      "        header=dict(values=[\"Job Description\"],\n",
      "                    fill_color='#f0a500',\n",
      "                    align='center', font=dict(color='white', size=16)),\n",
      "        cells=dict(values=[Jobs['Context'][index]],\n",
      "                   fill_color='#f4f4f4',\n",
      "                   align='left'))])\n",
      "\n",
      "    fig.update_layout(width=800, height=500)\n",
      "    st.write(fig)\n",
      "    st.markdown(\"---\")\n",
      "\n",
      "\n",
      "#################################### SCORE CALCUATION ################################\n",
      "\n",
      "def calculate_scores(resumes, job_description):\n",
      "    scores = []\n",
      "    for x in range(resumes.shape[0]):\n",
      "        score = Similar.match(\n",
      "            resumes['TF_Based'][x], job_description['Cleaned'][index])\n",
      "        scores.append(score)\n",
      "    return scores\n",
      "\n",
      "\n",
      "Resumes['Scores'] = calculate_scores(Resumes, Jobs)\n",
      "\n",
      "Ranked_resumes = Resumes.sort_values(\n",
      "    by=['Scores'], ascending=False).reset_index(drop=True)\n",
      "\n",
      "Ranked_resumes['Rank'] = pd.DataFrame(\n",
      "    [i for i in range(1, len(Ranked_resumes['Scores'])+1)])\n",
      "\n",
      "########################### SCORE TABLE PLOT ##############################\n",
      "\n",
      "fig1 = go.Figure(data=[go.Table(\n",
      "    header=dict(values=[\"Rank\", \"Name\", \"Scores\"],\n",
      "                fill_color='#00416d',\n",
      "                align='center', font=dict(color='white', size=16)),\n",
      "    cells=dict(values=[Ranked_resumes.Rank, Ranked_resumes.Name, Ranked_resumes.Scores],\n",
      "               fill_color='#d6e0f0',\n",
      "               align='left'))])\n",
      "\n",
      "fig1.update_layout(title=\"Top Ranked Resumes\", width=700, height=1100)\n",
      "st.write(fig1)\n",
      "\n",
      "st.markdown(\"---\")\n",
      "\n",
      "fig2 = px.bar(Ranked_resumes,\n",
      "              x=Ranked_resumes['Name'], y=Ranked_resumes['Scores'], color='Scores',\n",
      "              color_continuous_scale='haline', title=\"Score and Rank Distribution\")\n",
      "# fig.update_layout(width=700, height=700)\n",
      "st.write(fig2)\n",
      "\n",
      "\n",
      "st.markdown(\"---\")\n",
      "\n",
      "############################################ TF-IDF Code ###################################\n",
      "\n",
      "\n",
      "def get_list_of_words(document):\n",
      "    Document = []\n",
      "\n",
      "    for a in document:\n",
      "        raw = a.split(\" \")\n",
      "        Document.append(raw)\n",
      "\n",
      "    return Document\n",
      "\n",
      "\n",
      "document = get_list_of_words(Resumes['Cleaned'])\n",
      "\n",
      "id2word = corpora.Dictionary(document)\n",
      "corpus = [id2word.doc2bow(text) for text in document]\n",
      "\n",
      "lda_model = gensim.models.ldamodel.LdaModel(corpus=corpus, id2word=id2word, num_topics=5, random_state=100,\n",
      "                                            update_every=1, chunksize=100, passes=50, alpha='auto', per_word_topics=True)\n",
      "\n",
      "################################### LDA CODE ##############################################\n",
      "\n",
      "\n",
      "def format_topics_sentences(ldamodel, corpus):\n",
      "    sent_topics_df = []\n",
      "    for i, row_list in enumerate(ldamodel[corpus]):\n",
      "        row = row_list[0] if ldamodel.per_word_topics else row_list\n",
      "        row = sorted(row, key=lambda x: (x[1]), reverse=True)\n",
      "        for j, (topic_num, prop_topic) in enumerate(row):\n",
      "            if j == 0:\n",
      "                wp = ldamodel.show_topic(topic_num)\n",
      "                topic_keywords = \", \".join([word for word, prop in wp])\n",
      "                sent_topics_df.append(\n",
      "                    [i, int(topic_num), round(prop_topic, 4)*100, topic_keywords])\n",
      "            else:\n",
      "                break\n",
      "\n",
      "    return(sent_topics_df)\n",
      "\n",
      "\n",
      "################################# Topic Word Cloud Code #####################################\n",
      "\n",
      "st.markdown(\"## Topics and Topic Related Keywords \")\n",
      "st.markdown(\n",
      "    \"\"\"This Wordcloud representation shows the Topic Number and the Top Keywords that contstitute a Topic.\n",
      "    This further is used to cluster the resumes.      \"\"\")\n",
      "\n",
      "cols = [color for name, color in mcolors.TABLEAU_COLORS.items()]\n",
      "\n",
      "cloud = WordCloud(background_color='white',\n",
      "                  width=2500,\n",
      "                  height=1800,\n",
      "                  max_words=10,\n",
      "                  colormap='tab10',\n",
      "                  collocations=False,\n",
      "                  color_func=lambda *args, **kwargs: cols[i],\n",
      "                  prefer_horizontal=1.0)\n",
      "\n",
      "topics = lda_model.show_topics(formatted=False)\n",
      "\n",
      "fig, axes = plt.subplots(2, 2, figsize=(10, 10), sharex=True, sharey=True)\n",
      "\n",
      "for i, ax in enumerate(axes.flatten()):\n",
      "    fig.add_subplot(ax)\n",
      "    topic_words = dict(topics[i][1])\n",
      "    cloud.generate_from_frequencies(topic_words, max_font_size=300)\n",
      "    plt.gca().imshow(cloud)\n",
      "    plt.gca().set_title('Topic ' + str(i), fontdict=dict(size=16))\n",
      "    plt.gca().axis('off')\n",
      "\n",
      "\n",
      "plt.subplots_adjust(wspace=0, hspace=0)\n",
      "plt.axis('off')\n",
      "plt.margins(x=0, y=0)\n",
      "plt.tight_layout()\n",
      "st.pyplot()\n",
      "\n",
      "st.markdown(\"---\")\n",
      "####################### SETTING UP THE DATAFRAME FOR SUNBURST-GRAPH ############################\n",
      "df_topic_sents_keywords = format_topics_sentences(\n",
      "    ldamodel=lda_model, corpus=corpus)\n",
      "df_some = pd.DataFrame(df_topic_sents_keywords, columns=[\n",
      "                       'Document No', 'Dominant Topic', 'Topic % Contribution', 'Keywords'])\n",
      "df_some['Names'] = Resumes['Name']\n",
      "\n",
      "df = df_some\n",
      "\n",
      "st.markdown(\"## Topic Modelling of Resumes \")\n",
      "st.markdown(\n",
      "    \"Using LDA to divide the topics into a number of usefull topics and creating a Cluster of matching topic resumes.  \")\n",
      "fig3 = px.sunburst(df, path=['Dominant Topic', 'Names'], values='Topic % Contribution',\n",
      "                   color='Dominant Topic', color_continuous_scale='viridis', width=800, height=800, title=\"Topic Distribution Graph\")\n",
      "st.write(fig3)\n",
      "\n",
      "\n",
      "############################## RESUME PRINTING #############################\n",
      "\n",
      "# my_bar = st.progress(0)\n",
      "# for percent_complete in range(100):\n",
      "#     time.sleep(0.1)\n",
      "#     my_bar.progress(percent_complete + 1)\n",
      "\n",
      "option_2 = st.selectbox(\"Show the Best Matching Resumes?\", options=[\n",
      "    'NO', 'YES'])\n",
      "if option_2 == 'YES':\n",
      "    indx = st.slider(\"Which resume to display ?:\",\n",
      "                     1, Ranked_resumes.shape[0], 1)\n",
      "\n",
      "    st.write(\"Displaying Resume with Rank: \", indx)\n",
      "    st.markdown(\"---\")\n",
      "    st.markdown(\"## **Resume** \")\n",
      "    value = Ranked_resumes.iloc[indx-1, 2]\n",
      "    st.markdown(\"#### The Word Cloud For the Resume\")\n",
      "    wordcloud = WordCloud(width=800, height=800,\n",
      "                          background_color='white',\n",
      "                          colormap='viridis', collocations=False,\n",
      "                          min_font_size=10).generate(value)\n",
      "    plt.figure(figsize=(7, 7), facecolor=None)\n",
      "    plt.imshow(wordcloud)\n",
      "    plt.axis(\"off\")\n",
      "    plt.tight_layout(pad=0)\n",
      "    st.pyplot()\n",
      "\n",
      "    st.write(\"With a Match Score of :\", Ranked_resumes.iloc[indx-1, 6])\n",
      "    fig = go.Figure(data=[go.Table(\n",
      "        header=dict(values=[\"Resume\"],\n",
      "                    fill_color='#f0a500',\n",
      "                    align='center', font=dict(color='white', size=16)),\n",
      "        cells=dict(values=[str(value)],\n",
      "                   fill_color='#f4f4f4',\n",
      "                   align='left'))])\n",
      "\n",
      "    fig.update_layout(width=800, height=1200)\n",
      "    st.write(fig)\n",
      "    # st.text(df_sorted.iloc[indx-1, 1])\n",
      "    st.markdown(\"---\")\n",
      "\n",
      "from operator import index\n",
      "from pandas._config.config import options\n",
      "import Cleaner\n",
      "import Similar\n",
      "import textract as tx\n",
      "import pandas as pd\n",
      "import os\n",
      "import tf_idf\n",
      "\n",
      "resume_dir = \"Data/Resumes/\"\n",
      "job_desc_dir = \"Data/JobDesc/\"\n",
      "resume_names = os.listdir(resume_dir)\n",
      "job_description_names = os.listdir(job_desc_dir)\n",
      "\n",
      "document = []\n",
      "\n",
      "\n",
      "def read_resumes(list_of_resumes, resume_directory):\n",
      "    placeholder = []\n",
      "    for res in list_of_resumes:\n",
      "        temp = []\n",
      "        temp.append(res)\n",
      "        text = tx.process(resume_directory+res, encoding='ascii')\n",
      "        text = str(text, 'utf-8')\n",
      "        temp.append(text)\n",
      "        placeholder.append(temp)\n",
      "    return placeholder\n",
      "\n",
      "\n",
      "document = read_resumes(resume_names, resume_dir)\n",
      "\n",
      "\n",
      "def get_cleaned_words(document):\n",
      "    for i in range(len(document)):\n",
      "        raw = Cleaner.Cleaner(document[i][1])\n",
      "        document[i].append(\" \".join(raw[0]))\n",
      "        document[i].append(\" \".join(raw[1]))\n",
      "        document[i].append(\" \".join(raw[2]))\n",
      "        sentence = tf_idf.do_tfidf(document[i][2].split(\" \"))\n",
      "        document[i].append(sentence)\n",
      "    return document\n",
      "\n",
      "\n",
      "Doc = get_cleaned_words(document)\n",
      "\n",
      "Database = pd.DataFrame(document, columns=[\n",
      "                        \"Name\", \"Context\", \"Cleaned\", \"Selective\", \"Selective_Reduced\", \"TF_Based\"])\n",
      "\n",
      "Database.to_csv(\"Resume_Data.csv\", index=False)\n",
      "\n",
      "\n",
      "def read_jobdescriptions(job_description_names, job_desc_dir):\n",
      "    placeholder = []\n",
      "    for tes in job_description_names:\n",
      "        temp = []\n",
      "        temp.append(tes)\n",
      "        text = tx.process(job_desc_dir+tes, encoding='ascii')\n",
      "        text = str(text, 'utf-8')\n",
      "        temp.append(text)\n",
      "        placeholder.append(temp)\n",
      "    return placeholder\n",
      "\n",
      "\n",
      "job_document = read_jobdescriptions(job_description_names, job_desc_dir)\n",
      "\n",
      "Jd = get_cleaned_words(job_document)\n",
      "\n",
      "jd_database = pd.DataFrame(Jd, columns=[\n",
      "                           \"Name\", \"Context\", \"Cleaned\", \"Selective\", \"Selective_Reduced\", \"TF_Based\"])\n",
      "\n",
      "jd_database.to_csv(\"Job_Data.csv\", index=False)\n",
      "\n",
      "import matplotlib.colors as mcolors\n",
      "import gensim\n",
      "import gensim.corpora as corpora\n",
      "from operator import index\n",
      "from wordcloud import WordCloud\n",
      "from pandas._config.config import options\n",
      "import pandas as pd\n",
      "import streamlit as st\n",
      "import plotly.express as px\n",
      "import plotly.graph_objects as go\n",
      "import matplotlib.pyplot as plt\n",
      "import Similar\n",
      "from PIL import Image\n",
      "import time\n",
      "\n",
      "\n",
      "image = Image.open('Images//NAIVE RESUME MATCHER.png')\n",
      "st.image(image, use_column_width=True)\n",
      "\n",
      "st.title(\"Naive Resume Matcher\")\n",
      "st.markdown(\"\"\"\n",
      "A Machine Learning Based Resume Matcher, to compare Resumes with Job Descriptions.\n",
      "Create a score based on how good/similar a resume is to the particular Job Description.\\n\n",
      "Documents are sorted based on Their TF-IDF Scores (Term Frequency-Inverse Document Frequency)\n",
      "\n",
      "Matching Algorihms used are :-\n",
      "- **String Matching**\n",
      "    - Monge Elkan\n",
      "\n",
      "- **Token Based**\n",
      "    - Jaccard\n",
      "    - Cosine\n",
      "    - Sorensen-Dice\n",
      "    - Overlap Coefficient\n",
      "\n",
      "Topic Modelling of Resumes is done to provide additional information about the resumes and what clusters/topics,\n",
      "the belong to. \n",
      "For this :-\n",
      "\n",
      "1. LSA (Latenet Semantic Analysis) (aka TruncatedSVD in sklearn) is done after TF-IDF.\n",
      "2. id2word, and doc2word algorithms are used on the Documents.\n",
      "3. LDA or Latent Dirichlet Allocation is done to extract the Topics from the Document set.(In this case Resumes)\n",
      "4. Additional Plots are done to gain more insights about the document.\n",
      "\n",
      "Total Score calculate is the overall average of the 4 mentioned token based algorithms and string based.\n",
      "\"\"\")\n",
      "\n",
      "\n",
      "# Reading the CSV files prepared by the fileReader.py\n",
      "Resumes = pd.read_csv('Resume_Data.csv')\n",
      "Jobs = pd.read_csv('Job_Data.csv')\n",
      "\n",
      "\n",
      "############################### JOB DESCRIPTION CODE ######################################\n",
      "# Checking for Multiple Job Descriptions\n",
      "# If more than one Job Descriptions are available, it asks user to select one as well.\n",
      "if len(Jobs['Name']) <= 1:\n",
      "    st.write(\n",
      "        \"There is only 1 Job Description present. It will be used to create scores.\")\n",
      "else:\n",
      "    st.write(\"There are \", len(Jobs['Name']),\n",
      "             \"Job Descriptions available. Please select one.\")\n",
      "\n",
      "\n",
      "# Asking to Print the Job Desciption Names\n",
      "if len(Jobs['Name']) > 1:\n",
      "    option_yn = st.selectbox(\n",
      "        \"Show the Job Description Names?\", options=['YES', 'NO'])\n",
      "    if option_yn == 'YES':\n",
      "        index = [a for a in range(len(Jobs['Name']))]\n",
      "        fig = go.Figure(data=[go.Table(header=dict(values=[\"Job No.\", \"Job Desc. Name\"], line_color='darkslategray',\n",
      "                                                   fill_color='lightskyblue'),\n",
      "                                       cells=dict(values=[index, Jobs['Name']], line_color='darkslategray',\n",
      "                                                  fill_color='cyan'))\n",
      "                              ])\n",
      "        fig.update_layout(width=700, height=400)\n",
      "        st.write(fig)\n",
      "\n",
      "\n",
      "# Asking to chose the Job Description\n",
      "index = st.slider(\"Which JD to select ? : \", 0,\n",
      "                  len(Jobs['Name'])-1, 1)\n",
      "\n",
      "\n",
      "option_yn = st.selectbox(\"Show the Job Description ?\", options=['YES', 'NO'])\n",
      "if option_yn == 'YES':\n",
      "    st.markdown(\"---\")\n",
      "    st.markdown(\"### Job Description :\")\n",
      "    fig = go.Figure(data=[go.Table(\n",
      "        header=dict(values=[\"Job Description\"],\n",
      "                    fill_color='#f0a500',\n",
      "                    align='center', font=dict(color='white', size=16)),\n",
      "        cells=dict(values=[Jobs['Context'][index]],\n",
      "                   fill_color='#f4f4f4',\n",
      "                   align='left'))])\n",
      "\n",
      "    fig.update_layout(width=800, height=500)\n",
      "    st.write(fig)\n",
      "    st.markdown(\"---\")\n",
      "\n",
      "\n",
      "#################################### SCORE CALCUATION ################################\n",
      "\n",
      "def calculate_scores(resumes, job_description):\n",
      "    scores = []\n",
      "    for x in range(resumes.shape[0]):\n",
      "        score = Similar.match(\n",
      "            resumes['TF_Based'][x], job_description['Cleaned'][index])\n",
      "        scores.append(score)\n",
      "    return scores\n",
      "\n",
      "\n",
      "Resumes['Scores'] = calculate_scores(Resumes, Jobs)\n",
      "\n",
      "Ranked_resumes = Resumes.sort_values(\n",
      "    by=['Scores'], ascending=False).reset_index(drop=True)\n",
      "\n",
      "Ranked_resumes['Rank'] = pd.DataFrame(\n",
      "    [i for i in range(1, len(Ranked_resumes['Scores'])+1)])\n",
      "\n",
      "###################################### SCORE TABLE PLOT ####################################\n",
      "\n",
      "fig1 = go.Figure(data=[go.Table(\n",
      "    header=dict(values=[\"Rank\", \"Name\", \"Scores\"],\n",
      "                fill_color='#00416d',\n",
      "                align='center', font=dict(color='white', size=16)),\n",
      "    cells=dict(values=[Ranked_resumes.Rank, Ranked_resumes.Name, Ranked_resumes.Scores],\n",
      "               fill_color='#d6e0f0',\n",
      "               align='left'))])\n",
      "\n",
      "fig1.update_layout(title=\"Top Ranked Resumes\", width=700, height=1100)\n",
      "st.write(fig1)\n",
      "\n",
      "st.markdown(\"---\")\n",
      "\n",
      "fig2 = px.bar(Ranked_resumes,\n",
      "              x=Ranked_resumes['Name'], y=Ranked_resumes['Scores'], color='Scores',\n",
      "              color_continuous_scale='haline', title=\"Score and Rank Distribution\")\n",
      "# fig.update_layout(width=700, height=700)\n",
      "st.write(fig2)\n",
      "\n",
      "\n",
      "st.markdown(\"---\")\n",
      "\n",
      "############################################ TF-IDF Code ###################################\n",
      "\n",
      "\n",
      "def get_list_of_words(document):\n",
      "    Document = []\n",
      "\n",
      "    for a in document:\n",
      "        raw = a.split(\" \")\n",
      "        Document.append(raw)\n",
      "\n",
      "    return Document\n",
      "\n",
      "\n",
      "document = get_list_of_words(Resumes['Cleaned'])\n",
      "\n",
      "id2word = corpora.Dictionary(document)\n",
      "corpus = [id2word.doc2bow(text) for text in document]\n",
      "\n",
      "lda_model = gensim.models.ldamodel.LdaModel(corpus=corpus, id2word=id2word, num_topics=5, random_state=100,\n",
      "                                            update_every=1, chunksize=100, passes=50, alpha='auto', per_word_topics=True)\n",
      "\n",
      "################################### LDA CODE ##############################################\n",
      "\n",
      "\n",
      "def format_topics_sentences(ldamodel, corpus):\n",
      "    sent_topics_df = []\n",
      "    for i, row_list in enumerate(ldamodel[corpus]):\n",
      "        row = row_list[0] if ldamodel.per_word_topics else row_list\n",
      "        row = sorted(row, key=lambda x: (x[1]), reverse=True)\n",
      "        for j, (topic_num, prop_topic) in enumerate(row):\n",
      "            if j == 0:\n",
      "                wp = ldamodel.show_topic(topic_num)\n",
      "                topic_keywords = \", \".join([word for word, prop in wp])\n",
      "                sent_topics_df.append(\n",
      "                    [i, int(topic_num), round(prop_topic, 4)*100, topic_keywords])\n",
      "            else:\n",
      "                break\n",
      "\n",
      "    return(sent_topics_df)\n",
      "\n",
      "\n",
      "################################# Topic Word Cloud Code #####################################\n",
      "\n",
      "st.markdown(\"## Topics and Topic Related Keywords \")\n",
      "st.markdown(\n",
      "    \"\"\"This Wordcloud representation shows the Topic Number and the Top Keywords that contstitute a Topic.\n",
      "    This further is used to cluster the resumes.      \"\"\")\n",
      "\n",
      "cols = [color for name, color in mcolors.TABLEAU_COLORS.items()]\n",
      "\n",
      "cloud = WordCloud(background_color='white',\n",
      "                  width=2500,\n",
      "                  height=1800,\n",
      "                  max_words=10,\n",
      "                  colormap='tab10',\n",
      "                  collocations=False,\n",
      "                  color_func=lambda *args, **kwargs: cols[i],\n",
      "                  prefer_horizontal=1.0)\n",
      "\n",
      "topics = lda_model.show_topics(formatted=False)\n",
      "\n",
      "fig, axes = plt.subplots(2, 2, figsize=(10, 10), sharex=True, sharey=True)\n",
      "\n",
      "for i, ax in enumerate(axes.flatten()):\n",
      "    fig.add_subplot(ax)\n",
      "    topic_words = dict(topics[i][1])\n",
      "    cloud.generate_from_frequencies(topic_words, max_font_size=300)\n",
      "    plt.gca().imshow(cloud)\n",
      "    plt.gca().set_title('Topic ' + str(i), fontdict=dict(size=16))\n",
      "    plt.gca().axis('off')\n",
      "\n",
      "\n",
      "plt.subplots_adjust(wspace=0, hspace=0)\n",
      "plt.axis('off')\n",
      "plt.margins(x=0, y=0)\n",
      "plt.tight_layout()\n",
      "st.pyplot()\n",
      "\n",
      "st.markdown(\"---\")\n",
      "\n",
      "####################### SETTING UP THE DATAFRAME FOR SUNBURST-GRAPH ############################\n",
      "\n",
      "df_topic_sents_keywords = format_topics_sentences(\n",
      "    ldamodel=lda_model, corpus=corpus)\n",
      "df_some = pd.DataFrame(df_topic_sents_keywords, columns=[\n",
      "                       'Document No', 'Dominant Topic', 'Topic % Contribution', 'Keywords'])\n",
      "df_some['Names'] = Resumes['Name']\n",
      "\n",
      "df = df_some\n",
      "\n",
      "st.markdown(\"## Topic Modelling of Resumes \")\n",
      "st.markdown(\n",
      "    \"Using LDA to divide the topics into a number of usefull topics and creating a Cluster of matching topic resumes.  \")\n",
      "fig3 = px.sunburst(df, path=['Dominant Topic', 'Names'], values='Topic % Contribution',\n",
      "                   color='Dominant Topic', color_continuous_scale='viridis', width=800, height=800, title=\"Topic Distribution Graph\")\n",
      "st.write(fig3)\n",
      "\n",
      "\n",
      "############################## RESUME PRINTING #############################\n",
      "\n",
      "option_2 = st.selectbox(\"Show the Best Matching Resumes?\", options=[\n",
      "    'NO', 'YES'])\n",
      "if option_2 == 'YES':\n",
      "    indx = st.slider(\"Which resume to display ?:\",\n",
      "                     1, Ranked_resumes.shape[0], 1)\n",
      "\n",
      "    st.write(\"Displaying Resume with Rank: \", indx)\n",
      "    st.markdown(\"---\")\n",
      "    st.markdown(\"## **Resume** \")\n",
      "    value = Ranked_resumes.iloc[indx-1, 2]\n",
      "    st.markdown(\"#### The Word Cloud For the Resume\")\n",
      "    wordcloud = WordCloud(width=800, height=800,\n",
      "                          background_color='white',\n",
      "                          colormap='viridis', collocations=False,\n",
      "                          min_font_size=10).generate(value)\n",
      "    plt.figure(figsize=(7, 7), facecolor=None)\n",
      "    plt.imshow(wordcloud)\n",
      "    plt.axis(\"off\")\n",
      "    plt.tight_layout(pad=0)\n",
      "    st.pyplot()\n",
      "\n",
      "    st.write(\"With a Match Score of :\", Ranked_resumes.iloc[indx-1, 6])\n",
      "    fig = go.Figure(data=[go.Table(\n",
      "        header=dict(values=[\"Resume\"],\n",
      "                    fill_color='#f0a500',\n",
      "                    align='center', font=dict(color='white', size=16)),\n",
      "        cells=dict(values=[str(value)],\n",
      "                   fill_color='#f4f4f4',\n",
      "                   align='left'))])\n",
      "\n",
      "    fig.update_layout(width=800, height=1200)\n",
      "    st.write(fig)\n",
      "    # st.text(df_sorted.iloc[indx-1, 1])\n",
      "    st.markdown(\"---\")\n",
      "\n",
      "from sklearn.feature_extraction.text import TfidfVectorizer\n",
      "\n",
      "\n",
      "def do_tfidf(token):\n",
      "    tfidf = TfidfVectorizer(max_df=0.07, min_df=0.001)\n",
      "    words = tfidf.fit_transform(token)\n",
      "    sentence = \" \".join(tfidf.get_feature_names())\n",
      "    return sentence\n",
      "\n",
      "import matplotlib.colors as mcolors\n",
      "import gensim\n",
      "import gensim.corpora as corpora\n",
      "from operator import index\n",
      "from wordcloud import WordCloud\n",
      "from pandas._config.config import options\n",
      "import pandas as pd\n",
      "import streamlit as st\n",
      "import plotly.express as px\n",
      "import plotly.graph_objects as go\n",
      "import matplotlib.pyplot as plt\n",
      "import Similar\n",
      "from PIL import Image\n",
      "import time\n",
      "\n",
      "\n",
      "image = Image.open('Images//logo.png')\n",
      "st.image(image, use_column_width=True)\n",
      "\n",
      "st.title(\"Naive Resume Matcher\")\n",
      "st.markdown(\"\"\"\n",
      "A Machine Learning Based Resume Matcher, to compare Resumes with Job Descriptions.\n",
      "Create a score based on how good/similar a resume is to the particular Job Description.\\n\n",
      "Documents are sorted based on Their TF-IDF Scores (Term Frequency-Inverse Document Frequency)\n",
      "\n",
      "Matching Algorihms used are :-\n",
      "- **String Matching**\n",
      "    - Monge Elkan\n",
      "\n",
      "- **Token Based**\n",
      "    - Jaccard\n",
      "    - Cosine\n",
      "    - Sorensen-Dice\n",
      "    - Overlap Coefficient\n",
      "\n",
      "Topic Modelling of Resumes is done to provide additional information about the resumes and what clusters/topics,\n",
      "the belong to. \n",
      "For this :-\n",
      "\n",
      "1. LSA (Latenet Semantic Analysis) (aka TruncatedSVD in sklearn) is done after TF-IDF.\n",
      "2. id2word, and doc2word algorithms are used on the Documents.\n",
      "3. LDA or Latent Dirichlet Allocation is done to extract the Topics from the Document set.(In this case Resumes)\n",
      "4. Additional Plots are done to gain more insights about the document.\n",
      "\n",
      "Total Score calculate is the overall average of the 4 mentioned token based algorithms and string based.\n",
      "\"\"\")\n",
      "\n",
      "\n",
      "# Reading the CSV files prepared by the fileReader.py\n",
      "Resumes = pd.read_csv('Resume_Data.csv')\n",
      "Jobs = pd.read_csv('Job_Data.csv')\n",
      "\n",
      "\n",
      "############################### JOB DESCRIPTION CODE ######################################\n",
      "# Checking for Multiple Job Descriptions\n",
      "# If more than one Job Descriptions are available, it asks user to select one as well.\n",
      "if len(Jobs['Name']) <= 1:\n",
      "    st.write(\n",
      "        \"There is only 1 Job Description present. It will be used to create scores.\")\n",
      "else:\n",
      "    st.write(\"There are \", len(Jobs['Name']),\n",
      "             \"Job Descriptions available. Please select one.\")\n",
      "\n",
      "\n",
      "# Asking to Print the Job Desciption Names\n",
      "if len(Jobs['Name']) > 1:\n",
      "    option_yn = st.selectbox(\n",
      "        \"Show the Job Description Names?\", options=['YES', 'NO'])\n",
      "    if option_yn == 'YES':\n",
      "        index = [a for a in range(len(Jobs['Name']))]\n",
      "        fig = go.Figure(data=[go.Table(header=dict(values=[\"Job No.\", \"Job Desc. Name\"], line_color='darkslategray',\n",
      "                                                   fill_color='lightskyblue'),\n",
      "                                       cells=dict(values=[index, Jobs['Name']], line_color='darkslategray',\n",
      "                                                  fill_color='cyan'))\n",
      "                              ])\n",
      "        fig.update_layout(width=700, height=400)\n",
      "        st.write(fig)\n",
      "\n",
      "\n",
      "# Asking to chose the Job Description\n",
      "index = st.slider(\"Which JD to select ? : \", 0,\n",
      "                  len(Jobs['Name'])-1, 1)\n",
      "\n",
      "\n",
      "option_yn = st.selectbox(\"Show the Job Description ?\", options=['YES', 'NO'])\n",
      "if option_yn == 'YES':\n",
      "    st.markdown(\"---\")\n",
      "    st.markdown(\"### Job Description :\")\n",
      "    fig = go.Figure(data=[go.Table(\n",
      "        header=dict(values=[\"Job Description\"],\n",
      "                    fill_color='#f0a500',\n",
      "                    align='center', font=dict(color='white', size=16)),\n",
      "        cells=dict(values=[Jobs['Context'][index]],\n",
      "                   fill_color='#f4f4f4',\n",
      "                   align='left'))])\n",
      "\n",
      "    fig.update_layout(width=800, height=500)\n",
      "    st.write(fig)\n",
      "    st.markdown(\"---\")\n",
      "\n",
      "\n",
      "#################################### SCORE CALCUATION ################################\n",
      "\n",
      "def calculate_scores(resumes, job_description):\n",
      "    scores = []\n",
      "    for x in range(resumes.shape[0]):\n",
      "        score = Similar.match(\n",
      "            resumes['TF_Based'][x], job_description['Cleaned'][index])\n",
      "        scores.append(score)\n",
      "    return scores\n",
      "\n",
      "\n",
      "Resumes['Scores'] = calculate_scores(Resumes, Jobs)\n",
      "\n",
      "Ranked_resumes = Resumes.sort_values(\n",
      "    by=['Scores'], ascending=False).reset_index(drop=True)\n",
      "\n",
      "Ranked_resumes['Rank'] = pd.DataFrame(\n",
      "    [i for i in range(1, len(Ranked_resumes['Scores'])+1)])\n",
      "\n",
      "###################################### SCORE TABLE PLOT ####################################\n",
      "\n",
      "fig1 = go.Figure(data=[go.Table(\n",
      "    header=dict(values=[\"Rank\", \"Name\", \"Scores\"],\n",
      "                fill_color='#00416d',\n",
      "                align='center', font=dict(color='white', size=16)),\n",
      "    cells=dict(values=[Ranked_resumes.Rank, Ranked_resumes.Name, Ranked_resumes.Scores],\n",
      "               fill_color='#d6e0f0',\n",
      "               align='left'))])\n",
      "\n",
      "fig1.update_layout(title=\"Top Ranked Resumes\", width=700, height=1100)\n",
      "st.write(fig1)\n",
      "\n",
      "st.markdown(\"---\")\n",
      "\n",
      "fig2 = px.bar(Ranked_resumes,\n",
      "              x=Ranked_resumes['Name'], y=Ranked_resumes['Scores'], color='Scores',\n",
      "              color_continuous_scale='haline', title=\"Score and Rank Distribution\")\n",
      "# fig.update_layout(width=700, height=700)\n",
      "st.write(fig2)\n",
      "\n",
      "\n",
      "st.markdown(\"---\")\n",
      "\n",
      "############################################ TF-IDF Code ###################################\n",
      "\n",
      "\n",
      "def get_list_of_words(document):\n",
      "    Document = []\n",
      "\n",
      "    for a in document:\n",
      "        raw = a.split(\" \")\n",
      "        Document.append(raw)\n",
      "\n",
      "    return Document\n",
      "\n",
      "\n",
      "document = get_list_of_words(Resumes['Cleaned'])\n",
      "\n",
      "id2word = corpora.Dictionary(document)\n",
      "corpus = [id2word.doc2bow(text) for text in document]\n",
      "\n",
      "lda_model = gensim.models.ldamodel.LdaModel(corpus=corpus, id2word=id2word, num_topics=5, random_state=100,\n",
      "                                            update_every=1, chunksize=100, passes=50, alpha='auto', per_word_topics=True)\n",
      "\n",
      "################################### LDA CODE ##############################################\n",
      "\n",
      "\n",
      "def format_topics_sentences(ldamodel, corpus):\n",
      "    sent_topics_df = []\n",
      "    for i, row_list in enumerate(ldamodel[corpus]):\n",
      "        row = row_list[0] if ldamodel.per_word_topics else row_list\n",
      "        row = sorted(row, key=lambda x: (x[1]), reverse=True)\n",
      "        for j, (topic_num, prop_topic) in enumerate(row):\n",
      "            if j == 0:\n",
      "                wp = ldamodel.show_topic(topic_num)\n",
      "                topic_keywords = \", \".join([word for word, prop in wp])\n",
      "                sent_topics_df.append(\n",
      "                    [i, int(topic_num), round(prop_topic, 4)*100, topic_keywords])\n",
      "            else:\n",
      "                break\n",
      "\n",
      "    return(sent_topics_df)\n",
      "\n",
      "\n",
      "################################# Topic Word Cloud Code #####################################\n",
      "\n",
      "st.markdown(\"## Topics and Topic Related Keywords \")\n",
      "st.markdown(\n",
      "    \"\"\"This Wordcloud representation shows the Topic Number and the Top Keywords that contstitute a Topic.\n",
      "    This further is used to cluster the resumes.      \"\"\")\n",
      "\n",
      "cols = [color for name, color in mcolors.TABLEAU_COLORS.items()]\n",
      "\n",
      "cloud = WordCloud(background_color='white',\n",
      "                  width=2500,\n",
      "                  height=1800,\n",
      "                  max_words=10,\n",
      "                  colormap='tab10',\n",
      "                  collocations=False,\n",
      "                  color_func=lambda *args, **kwargs: cols[i],\n",
      "                  prefer_horizontal=1.0)\n",
      "\n",
      "topics = lda_model.show_topics(formatted=False)\n",
      "\n",
      "fig, axes = plt.subplots(2, 2, figsize=(10, 10), sharex=True, sharey=True)\n",
      "\n",
      "for i, ax in enumerate(axes.flatten()):\n",
      "    fig.add_subplot(ax)\n",
      "    topic_words = dict(topics[i][1])\n",
      "    cloud.generate_from_frequencies(topic_words, max_font_size=300)\n",
      "    plt.gca().imshow(cloud)\n",
      "    plt.gca().set_title('Topic ' + str(i), fontdict=dict(size=16))\n",
      "    plt.gca().axis('off')\n",
      "\n",
      "\n",
      "plt.subplots_adjust(wspace=0, hspace=0)\n",
      "plt.axis('off')\n",
      "plt.margins(x=0, y=0)\n",
      "plt.tight_layout()\n",
      "st.pyplot()\n",
      "\n",
      "st.markdown(\"---\")\n",
      "\n",
      "####################### SETTING UP THE DATAFRAME FOR SUNBURST-GRAPH ############################\n",
      "\n",
      "df_topic_sents_keywords = format_topics_sentences(\n",
      "    ldamodel=lda_model, corpus=corpus)\n",
      "df_some = pd.DataFrame(df_topic_sents_keywords, columns=[\n",
      "                       'Document No', 'Dominant Topic', 'Topic % Contribution', 'Keywords'])\n",
      "df_some['Names'] = Resumes['Name']\n",
      "\n",
      "df = df_some\n",
      "\n",
      "st.markdown(\"## Topic Modelling of Resumes \")\n",
      "st.markdown(\n",
      "    \"Using LDA to divide the topics into a number of usefull topics and creating a Cluster of matching topic resumes.  \")\n",
      "fig3 = px.sunburst(df, path=['Dominant Topic', 'Names'], values='Topic % Contribution',\n",
      "                   color='Dominant Topic', color_continuous_scale='viridis', width=800, height=800, title=\"Topic Distribution Graph\")\n",
      "st.write(fig3)\n",
      "\n",
      "\n",
      "############################## RESUME PRINTING #############################\n",
      "\n",
      "option_2 = st.selectbox(\"Show the Best Matching Resumes?\", options=[\n",
      "    'NO', 'YES'])\n",
      "if option_2 == 'YES':\n",
      "    indx = st.slider(\"Which resume to display ?:\",\n",
      "                     1, Ranked_resumes.shape[0], 1)\n",
      "\n",
      "    st.write(\"Displaying Resume with Rank: \", indx)\n",
      "    st.markdown(\"---\")\n",
      "    st.markdown(\"## **Resume** \")\n",
      "    value = Ranked_resumes.iloc[indx-1, 2]\n",
      "    st.markdown(\"#### The Word Cloud For the Resume\")\n",
      "    wordcloud = WordCloud(width=800, height=800,\n",
      "                          background_color='white',\n",
      "                          colormap='viridis', collocations=False,\n",
      "                          min_font_size=10).generate(value)\n",
      "    plt.figure(figsize=(7, 7), facecolor=None)\n",
      "    plt.imshow(wordcloud)\n",
      "    plt.axis(\"off\")\n",
      "    plt.tight_layout(pad=0)\n",
      "    st.pyplot()\n",
      "\n",
      "    st.write(\"With a Match Score of :\", Ranked_resumes.iloc[indx-1, 6])\n",
      "    fig = go.Figure(data=[go.Table(\n",
      "        header=dict(values=[\"Resume\"],\n",
      "                    fill_color='#f0a500',\n",
      "                    align='center', font=dict(color='white', size=16)),\n",
      "        cells=dict(values=[str(value)],\n",
      "                   fill_color='#f4f4f4',\n",
      "                   align='left'))])\n",
      "\n",
      "    fig.update_layout(width=800, height=1200)\n",
      "    st.write(fig)\n",
      "    # st.text(df_sorted.iloc[indx-1, 1])\n",
      "    st.markdown(\"---\")\n",
      "\n",
      "import matplotlib.colors as mcolors\n",
      "import gensim\n",
      "import gensim.corpora as corpora\n",
      "from operator import index\n",
      "from wordcloud import WordCloud\n",
      "from pandas._config.config import options\n",
      "import pandas as pd\n",
      "import streamlit as st\n",
      "import plotly.express as px\n",
      "import plotly.graph_objects as go\n",
      "import matplotlib.pyplot as plt\n",
      "import Similar\n",
      "from PIL import Image\n",
      "import time\n",
      "\n",
      "\n",
      "image = Image.open('Images//logo.png')\n",
      "st.image(image, use_column_width=True)\n",
      "\n",
      "st.title(\"Naive Resume Matcher\")\n",
      "st.markdown(\"\"\"\n",
      "A Machine Learning Based Resume Matcher, to compare Resumes with Job Descriptions.\n",
      "Create a score based on how good/similar a resume is to the particular Job Description.\\n\n",
      "Documents are sorted based on Their TF-IDF Scores (Term Frequency-Inverse Document Frequency)\n",
      "\n",
      "Matching Algorihms used are :-\n",
      "- **String Matching**\n",
      "    - Monge Elkan\n",
      "\n",
      "- **Token Based**\n",
      "    - Jaccard\n",
      "    - Cosine\n",
      "    - Sorensen-Dice\n",
      "    - Overlap Coefficient\n",
      "\n",
      "Topic Modelling of Resumes is done to provide additional information about the resumes and what clusters/topics,\n",
      "the belong to. \n",
      "For this :-\n",
      "\n",
      "1. LSA (Latenet Semantic Analysis) (aka TruncatedSVD in sklearn) is done after TF-IDF.\n",
      "2. id2word, and doc2word algorithms are used on the Documents.\n",
      "3. LDA or Latent Dirichlet Allocation is done to extract the Topics from the Document set.(In this case Resumes)\n",
      "4. Additional Plots are done to gain more insights about the document.\n",
      "\n",
      "Total Score calculate is the overall average of the 4 mentioned token based algorithms and string based.\n",
      "\"\"\")\n",
      "\n",
      "\n",
      "# Reading the CSV files prepared by the fileReader.py\n",
      "Resumes = pd.read_csv('Resume_Data.csv')\n",
      "Jobs = pd.read_csv('Job_Data.csv')\n",
      "\n",
      "\n",
      "############################### JOB DESCRIPTION CODE ######################################\n",
      "# Checking for Multiple Job Descriptions\n",
      "# If more than one Job Descriptions are available, it asks user to select one as well.\n",
      "if len(Jobs['Name']) <= 1:\n",
      "    st.write(\n",
      "        \"There is only 1 Job Description present. It will be used to create scores.\")\n",
      "else:\n",
      "    st.write(\"There are \", len(Jobs['Name']),\n",
      "             \"Job Descriptions available. Please select one.\")\n",
      "\n",
      "\n",
      "# Asking to Print the Job Desciption Names\n",
      "if len(Jobs['Name']) > 1:\n",
      "    option_yn = st.selectbox(\n",
      "        \"Show the Job Description Names?\", options=['YES', 'NO'])\n",
      "    if option_yn == 'YES':\n",
      "        index = [a for a in range(len(Jobs['Name']))]\n",
      "        fig = go.Figure(data=[go.Table(header=dict(values=[\"Job No.\", \"Job Desc. Name\"], line_color='darkslategray',\n",
      "                                                   fill_color='lightskyblue'),\n",
      "                                       cells=dict(values=[index, Jobs['Name']], line_color='darkslategray',\n",
      "                                                  fill_color='cyan'))\n",
      "                              ])\n",
      "        fig.update_layout(width=700, height=400)\n",
      "        st.write(fig)\n",
      "\n",
      "\n",
      "# Asking to chose the Job Description\n",
      "index = st.slider(\"Which JD to select ? : \", 0,\n",
      "                  len(Jobs['Name'])-1, 1)\n",
      "\n",
      "\n",
      "option_yn = st.selectbox(\"Show the Job Description ?\", options=['YES', 'NO'])\n",
      "if option_yn == 'YES':\n",
      "    st.markdown(\"---\")\n",
      "    st.markdown(\"### Job Description :\")\n",
      "    fig = go.Figure(data=[go.Table(\n",
      "        header=dict(values=[\"Job Description\"],\n",
      "                    fill_color='#f0a500',\n",
      "                    align='center', font=dict(color='white', size=16)),\n",
      "        cells=dict(values=[Jobs['Context'][index]],\n",
      "                   fill_color='#f4f4f4',\n",
      "                   align='left'))])\n",
      "\n",
      "    fig.update_layout(width=800, height=500)\n",
      "    st.write(fig)\n",
      "    st.markdown(\"---\")\n",
      "\n",
      "\n",
      "#################################### SCORE CALCUATION ################################\n",
      "\n",
      "def calculate_scores(resumes, job_description):\n",
      "    scores = []\n",
      "    for x in range(resumes.shape[0]):\n",
      "        score = Similar.match(\n",
      "            resumes['TF_Based'][x], job_description['Selective_Reduced'][index])\n",
      "        scores.append(score)\n",
      "    return scores\n",
      "\n",
      "\n",
      "Resumes['Scores'] = calculate_scores(Resumes, Jobs)\n",
      "\n",
      "Ranked_resumes = Resumes.sort_values(\n",
      "    by=['Scores'], ascending=False).reset_index(drop=True)\n",
      "\n",
      "Ranked_resumes['Rank'] = pd.DataFrame(\n",
      "    [i for i in range(1, len(Ranked_resumes['Scores'])+1)])\n",
      "\n",
      "###################################### SCORE TABLE PLOT ####################################\n",
      "\n",
      "fig1 = go.Figure(data=[go.Table(\n",
      "    header=dict(values=[\"Rank\", \"Name\", \"Scores\"],\n",
      "                fill_color='#00416d',\n",
      "                align='center', font=dict(color='white', size=16)),\n",
      "    cells=dict(values=[Ranked_resumes.Rank, Ranked_resumes.Name, Ranked_resumes.Scores],\n",
      "               fill_color='#d6e0f0',\n",
      "               align='left'))])\n",
      "\n",
      "fig1.update_layout(title=\"Top Ranked Resumes\", width=700, height=1100)\n",
      "st.write(fig1)\n",
      "\n",
      "st.markdown(\"---\")\n",
      "\n",
      "fig2 = px.bar(Ranked_resumes,\n",
      "              x=Ranked_resumes['Name'], y=Ranked_resumes['Scores'], color='Scores',\n",
      "              color_continuous_scale='haline', title=\"Score and Rank Distribution\")\n",
      "# fig.update_layout(width=700, height=700)\n",
      "st.write(fig2)\n",
      "\n",
      "\n",
      "st.markdown(\"---\")\n",
      "\n",
      "############################################ TF-IDF Code ###################################\n",
      "\n",
      "\n",
      "def get_list_of_words(document):\n",
      "    Document = []\n",
      "\n",
      "    for a in document:\n",
      "        raw = a.split(\" \")\n",
      "        Document.append(raw)\n",
      "\n",
      "    return Document\n",
      "\n",
      "\n",
      "document = get_list_of_words(Resumes['Cleaned'])\n",
      "\n",
      "id2word = corpora.Dictionary(document)\n",
      "corpus = [id2word.doc2bow(text) for text in document]\n",
      "\n",
      "lda_model = gensim.models.ldamodel.LdaModel(corpus=corpus, id2word=id2word, num_topics=5, random_state=100,\n",
      "                                            update_every=1, chunksize=100, passes=50, alpha='auto', per_word_topics=True)\n",
      "\n",
      "################################### LDA CODE ##############################################\n",
      "\n",
      "\n",
      "def format_topics_sentences(ldamodel, corpus):\n",
      "    sent_topics_df = []\n",
      "    for i, row_list in enumerate(ldamodel[corpus]):\n",
      "        row = row_list[0] if ldamodel.per_word_topics else row_list\n",
      "        row = sorted(row, key=lambda x: (x[1]), reverse=True)\n",
      "        for j, (topic_num, prop_topic) in enumerate(row):\n",
      "            if j == 0:\n",
      "                wp = ldamodel.show_topic(topic_num)\n",
      "                topic_keywords = \", \".join([word for word, prop in wp])\n",
      "                sent_topics_df.append(\n",
      "                    [i, int(topic_num), round(prop_topic, 4)*100, topic_keywords])\n",
      "            else:\n",
      "                break\n",
      "\n",
      "    return(sent_topics_df)\n",
      "\n",
      "\n",
      "################################# Topic Word Cloud Code #####################################\n",
      "\n",
      "st.markdown(\"## Topics and Topic Related Keywords \")\n",
      "st.markdown(\n",
      "    \"\"\"This Wordcloud representation shows the Topic Number and the Top Keywords that contstitute a Topic.\n",
      "    This further is used to cluster the resumes.      \"\"\")\n",
      "\n",
      "cols = [color for name, color in mcolors.TABLEAU_COLORS.items()]\n",
      "\n",
      "cloud = WordCloud(background_color='white',\n",
      "                  width=2500,\n",
      "                  height=1800,\n",
      "                  max_words=10,\n",
      "                  colormap='tab10',\n",
      "                  collocations=False,\n",
      "                  color_func=lambda *args, **kwargs: cols[i],\n",
      "                  prefer_horizontal=1.0)\n",
      "\n",
      "topics = lda_model.show_topics(formatted=False)\n",
      "\n",
      "fig, axes = plt.subplots(2, 2, figsize=(10, 10), sharex=True, sharey=True)\n",
      "\n",
      "for i, ax in enumerate(axes.flatten()):\n",
      "    fig.add_subplot(ax)\n",
      "    topic_words = dict(topics[i][1])\n",
      "    cloud.generate_from_frequencies(topic_words, max_font_size=300)\n",
      "    plt.gca().imshow(cloud)\n",
      "    plt.gca().set_title('Topic ' + str(i), fontdict=dict(size=16))\n",
      "    plt.gca().axis('off')\n",
      "\n",
      "\n",
      "plt.subplots_adjust(wspace=0, hspace=0)\n",
      "plt.axis('off')\n",
      "plt.margins(x=0, y=0)\n",
      "plt.tight_layout()\n",
      "st.pyplot()\n",
      "\n",
      "st.markdown(\"---\")\n",
      "\n",
      "####################### SETTING UP THE DATAFRAME FOR SUNBURST-GRAPH ############################\n",
      "\n",
      "df_topic_sents_keywords = format_topics_sentences(\n",
      "    ldamodel=lda_model, corpus=corpus)\n",
      "df_some = pd.DataFrame(df_topic_sents_keywords, columns=[\n",
      "                       'Document No', 'Dominant Topic', 'Topic % Contribution', 'Keywords'])\n",
      "df_some['Names'] = Resumes['Name']\n",
      "\n",
      "df = df_some\n",
      "\n",
      "st.markdown(\"## Topic Modelling of Resumes \")\n",
      "st.markdown(\n",
      "    \"Using LDA to divide the topics into a number of usefull topics and creating a Cluster of matching topic resumes.  \")\n",
      "fig3 = px.sunburst(df, path=['Dominant Topic', 'Names'], values='Topic % Contribution',\n",
      "                   color='Dominant Topic', color_continuous_scale='viridis', width=800, height=800, title=\"Topic Distribution Graph\")\n",
      "st.write(fig3)\n",
      "\n",
      "\n",
      "############################## RESUME PRINTING #############################\n",
      "\n",
      "option_2 = st.selectbox(\"Show the Best Matching Resumes?\", options=[\n",
      "    'NO', 'YES'])\n",
      "if option_2 == 'YES':\n",
      "    indx = st.slider(\"Which resume to display ?:\",\n",
      "                     1, Ranked_resumes.shape[0], 1)\n",
      "\n",
      "    st.write(\"Displaying Resume with Rank: \", indx)\n",
      "    st.markdown(\"---\")\n",
      "    st.markdown(\"## **Resume** \")\n",
      "    value = Ranked_resumes.iloc[indx-1, 2]\n",
      "    st.markdown(\"#### The Word Cloud For the Resume\")\n",
      "    wordcloud = WordCloud(width=800, height=800,\n",
      "                          background_color='white',\n",
      "                          colormap='viridis', collocations=False,\n",
      "                          min_font_size=10).generate(value)\n",
      "    plt.figure(figsize=(7, 7), facecolor=None)\n",
      "    plt.imshow(wordcloud)\n",
      "    plt.axis(\"off\")\n",
      "    plt.tight_layout(pad=0)\n",
      "    st.pyplot()\n",
      "\n",
      "    st.write(\"With a Match Score of :\", Ranked_resumes.iloc[indx-1, 6])\n",
      "    fig = go.Figure(data=[go.Table(\n",
      "        header=dict(values=[\"Resume\"],\n",
      "                    fill_color='#f0a500',\n",
      "                    align='center', font=dict(color='white', size=16)),\n",
      "        cells=dict(values=[str(value)],\n",
      "                   fill_color='#f4f4f4',\n",
      "                   align='left'))])\n",
      "\n",
      "    fig.update_layout(width=800, height=1200)\n",
      "    st.write(fig)\n",
      "    # st.text(df_sorted.iloc[indx-1, 1])\n",
      "    st.markdown(\"---\")\n",
      "\n",
      "from operator import index\n",
      "from pandas._config.config import options\n",
      "import Cleaner\n",
      "import Similar\n",
      "import textract as tx\n",
      "import pandas as pd\n",
      "import os\n",
      "import tf_idf\n",
      "\n",
      "resume_dir = \"Data/Resumes/\"\n",
      "job_desc_dir = \"Data/JobDesc/\"\n",
      "resume_names = os.listdir(resume_dir)\n",
      "job_description_names = os.listdir(job_desc_dir)\n",
      "\n",
      "document = []\n",
      "\n",
      "\n",
      "def read_resumes(list_of_resumes, resume_directory):\n",
      "    placeholder = []\n",
      "    for res in list_of_resumes:\n",
      "        temp = []\n",
      "        temp.append(res)\n",
      "        text = tx.process(resume_directory+res, encoding='ascii')\n",
      "        text = str(text, 'utf-8')\n",
      "        temp.append(text)\n",
      "        placeholder.append(temp)\n",
      "    return placeholder\n",
      "\n",
      "\n",
      "document = read_resumes(resume_names, resume_dir)\n",
      "\n",
      "\n",
      "def get_cleaned_words(document):\n",
      "    for i in range(len(document)):\n",
      "        raw = Cleaner.Cleaner(document[i][1])\n",
      "        document[i].append(\" \".join(raw[0]))\n",
      "        document[i].append(\" \".join(raw[1]))\n",
      "        document[i].append(\" \".join(raw[2]))\n",
      "        sentence = tf_idf.do_tfidf(document[i][3].split(\" \"))\n",
      "        document[i].append(sentence)\n",
      "    return document\n",
      "\n",
      "\n",
      "Doc = get_cleaned_words(document)\n",
      "\n",
      "Database = pd.DataFrame(document, columns=[\n",
      "                        \"Name\", \"Context\", \"Cleaned\", \"Selective\", \"Selective_Reduced\", \"TF_Based\"])\n",
      "\n",
      "Database.to_csv(\"Resume_Data.csv\", index=False)\n",
      "\n",
      "\n",
      "def read_jobdescriptions(job_description_names, job_desc_dir):\n",
      "    placeholder = []\n",
      "    for tes in job_description_names:\n",
      "        temp = []\n",
      "        temp.append(tes)\n",
      "        text = tx.process(job_desc_dir+tes, encoding='ascii')\n",
      "        text = str(text, 'utf-8')\n",
      "        temp.append(text)\n",
      "        placeholder.append(temp)\n",
      "    return placeholder\n",
      "\n",
      "\n",
      "job_document = read_jobdescriptions(job_description_names, job_desc_dir)\n",
      "\n",
      "Jd = get_cleaned_words(job_document)\n",
      "\n",
      "jd_database = pd.DataFrame(Jd, columns=[\n",
      "                           \"Name\", \"Context\", \"Cleaned\", \"Selective\", \"Selective_Reduced\", \"TF_Based\"])\n",
      "\n",
      "jd_database.to_csv(\"Job_Data.csv\", index=False)\n",
      "\n",
      "import textdistance as td\n",
      "\n",
      "\n",
      "def match(resume, job_des):\n",
      "    j = td.jaccard.similarity(resume, job_des)\n",
      "    s = td.sorensen_dice.similarity(resume, job_des)\n",
      "    c = td.cosine.similarity(resume, job_des)\n",
      "    o = td.overlap.normalized_similarity(resume, job_des)\n",
      "    total = (j+s+c+o)/4\n",
      "    # total = (s+o)/2\n",
      "    return total*100\n",
      "\n",
      "import matplotlib.colors as mcolors\n",
      "import gensim\n",
      "import gensim.corpora as corpora\n",
      "from operator import index\n",
      "from wordcloud import WordCloud\n",
      "from pandas._config.config import options\n",
      "import pandas as pd\n",
      "import streamlit as st\n",
      "import plotly.express as px\n",
      "import plotly.graph_objects as go\n",
      "import matplotlib.pyplot as plt\n",
      "import Similar\n",
      "from PIL import Image\n",
      "import time\n",
      "\n",
      "\n",
      "image = Image.open('Images//logo.png')\n",
      "st.image(image, use_column_width=True)\n",
      "\n",
      "st.title(\"Naive Resume Matcher\")\n",
      "st.markdown(\"\"\"\n",
      "A Machine Learning Based Resume Matcher, to compare Resumes with Job Descriptions.\n",
      "Create a score based on how good/similar a resume is to the particular Job Description.\\n\n",
      "Documents are sorted based on Their TF-IDF Scores (Term Frequency-Inverse Document Frequency)\n",
      "\n",
      "Matching Algorihms used are :-\n",
      "- **String Matching**\n",
      "    - Monge Elkan\n",
      "\n",
      "- **Token Based**\n",
      "    - Jaccard\n",
      "    - Cosine\n",
      "    - Sorensen-Dice\n",
      "    - Overlap Coefficient\n",
      "\n",
      "Topic Modelling of Resumes is done to provide additional information about the resumes and what clusters/topics,\n",
      "the belong to. \n",
      "For this :-\n",
      "\n",
      "1. LSA (Latenet Semantic Analysis) (aka TruncatedSVD in sklearn) is done after TF-IDF.\n",
      "2. id2word, and doc2word algorithms are used on the Documents.\n",
      "3. LDA or Latent Dirichlet Allocation is done to extract the Topics from the Document set.(In this case Resumes)\n",
      "4. Additional Plots are done to gain more insights about the document.\n",
      "\n",
      "Total Score calculate is the overall average of the 4 mentioned token based algorithms and string based.\n",
      "\"\"\")\n",
      "\n",
      "\n",
      "# Reading the CSV files prepared by the fileReader.py\n",
      "Resumes = pd.read_csv('Resume_Data.csv')\n",
      "Jobs = pd.read_csv('Job_Data.csv')\n",
      "\n",
      "\n",
      "############################### JOB DESCRIPTION CODE ######################################\n",
      "# Checking for Multiple Job Descriptions\n",
      "# If more than one Job Descriptions are available, it asks user to select one as well.\n",
      "if len(Jobs['Name']) <= 1:\n",
      "    st.write(\n",
      "        \"There is only 1 Job Description present. It will be used to create scores.\")\n",
      "else:\n",
      "    st.write(\"There are \", len(Jobs['Name']),\n",
      "             \"Job Descriptions available. Please select one.\")\n",
      "\n",
      "\n",
      "# Asking to Print the Job Desciption Names\n",
      "if len(Jobs['Name']) > 1:\n",
      "    option_yn = st.selectbox(\n",
      "        \"Show the Job Description Names?\", options=['YES', 'NO'])\n",
      "    if option_yn == 'YES':\n",
      "        index = [a for a in range(len(Jobs['Name']))]\n",
      "        fig = go.Figure(data=[go.Table(header=dict(values=[\"Job No.\", \"Job Desc. Name\"], line_color='darkslategray',\n",
      "                                                   fill_color='lightskyblue'),\n",
      "                                       cells=dict(values=[index, Jobs['Name']], line_color='darkslategray',\n",
      "                                                  fill_color='cyan'))\n",
      "                              ])\n",
      "        fig.update_layout(width=700, height=400)\n",
      "        st.write(fig)\n",
      "\n",
      "\n",
      "# Asking to chose the Job Description\n",
      "index = st.slider(\"Which JD to select ? : \", 0,\n",
      "                  len(Jobs['Name'])-1, 1)\n",
      "\n",
      "\n",
      "option_yn = st.selectbox(\"Show the Job Description ?\", options=['YES', 'NO'])\n",
      "if option_yn == 'YES':\n",
      "    st.markdown(\"---\")\n",
      "    st.markdown(\"### Job Description :\")\n",
      "    fig = go.Figure(data=[go.Table(\n",
      "        header=dict(values=[\"Job Description\"],\n",
      "                    fill_color='#f0a500',\n",
      "                    align='center', font=dict(color='white', size=16)),\n",
      "        cells=dict(values=[Jobs['Context'][index]],\n",
      "                   fill_color='#f4f4f4',\n",
      "                   align='left'))])\n",
      "\n",
      "    fig.update_layout(width=800, height=500)\n",
      "    st.write(fig)\n",
      "    st.markdown(\"---\")\n",
      "\n",
      "\n",
      "#################################### SCORE CALCUATION ################################\n",
      "\n",
      "def calculate_scores(resumes, job_description):\n",
      "    scores = []\n",
      "    for x in range(resumes.shape[0]):\n",
      "        score = Similar.match(\n",
      "            resumes['TF_Based'][x], job_description['TF_Based'][index])\n",
      "        scores.append(score)\n",
      "    return scores\n",
      "\n",
      "\n",
      "Resumes['Scores'] = calculate_scores(Resumes, Jobs)\n",
      "\n",
      "Ranked_resumes = Resumes.sort_values(\n",
      "    by=['Scores'], ascending=False).reset_index(drop=True)\n",
      "\n",
      "Ranked_resumes['Rank'] = pd.DataFrame(\n",
      "    [i for i in range(1, len(Ranked_resumes['Scores'])+1)])\n",
      "\n",
      "###################################### SCORE TABLE PLOT ####################################\n",
      "\n",
      "fig1 = go.Figure(data=[go.Table(\n",
      "    header=dict(values=[\"Rank\", \"Name\", \"Scores\"],\n",
      "                fill_color='#00416d',\n",
      "                align='center', font=dict(color='white', size=16)),\n",
      "    cells=dict(values=[Ranked_resumes.Rank, Ranked_resumes.Name, Ranked_resumes.Scores],\n",
      "               fill_color='#d6e0f0',\n",
      "               align='left'))])\n",
      "\n",
      "fig1.update_layout(title=\"Top Ranked Resumes\", width=700, height=1100)\n",
      "st.write(fig1)\n",
      "\n",
      "st.markdown(\"---\")\n",
      "\n",
      "fig2 = px.bar(Ranked_resumes,\n",
      "              x=Ranked_resumes['Name'], y=Ranked_resumes['Scores'], color='Scores',\n",
      "              color_continuous_scale='haline', title=\"Score and Rank Distribution\")\n",
      "# fig.update_layout(width=700, height=700)\n",
      "st.write(fig2)\n",
      "\n",
      "\n",
      "st.markdown(\"---\")\n",
      "\n",
      "############################################ TF-IDF Code ###################################\n",
      "\n",
      "\n",
      "def get_list_of_words(document):\n",
      "    Document = []\n",
      "\n",
      "    for a in document:\n",
      "        raw = a.split(\" \")\n",
      "        Document.append(raw)\n",
      "\n",
      "    return Document\n",
      "\n",
      "\n",
      "document = get_list_of_words(Resumes['Cleaned'])\n",
      "\n",
      "id2word = corpora.Dictionary(document)\n",
      "corpus = [id2word.doc2bow(text) for text in document]\n",
      "\n",
      "lda_model = gensim.models.ldamodel.LdaModel(corpus=corpus, id2word=id2word, num_topics=6, random_state=100,\n",
      "                                            update_every=1, chunksize=100, passes=50, alpha='auto', per_word_topics=True)\n",
      "\n",
      "################################### LDA CODE ##############################################\n",
      "\n",
      "\n",
      "def format_topics_sentences(ldamodel, corpus):\n",
      "    sent_topics_df = []\n",
      "    for i, row_list in enumerate(ldamodel[corpus]):\n",
      "        row = row_list[0] if ldamodel.per_word_topics else row_list\n",
      "        row = sorted(row, key=lambda x: (x[1]), reverse=True)\n",
      "        for j, (topic_num, prop_topic) in enumerate(row):\n",
      "            if j == 0:\n",
      "                wp = ldamodel.show_topic(topic_num)\n",
      "                topic_keywords = \", \".join([word for word, prop in wp])\n",
      "                sent_topics_df.append(\n",
      "                    [i, int(topic_num), round(prop_topic, 4)*100, topic_keywords])\n",
      "            else:\n",
      "                break\n",
      "\n",
      "    return(sent_topics_df)\n",
      "\n",
      "\n",
      "################################# Topic Word Cloud Code #####################################\n",
      "\n",
      "st.markdown(\"## Topics and Topic Related Keywords \")\n",
      "st.markdown(\n",
      "    \"\"\"This Wordcloud representation shows the Topic Number and the Top Keywords that contstitute a Topic.\n",
      "    This further is used to cluster the resumes.      \"\"\")\n",
      "\n",
      "cols = [color for name, color in mcolors.TABLEAU_COLORS.items()]\n",
      "\n",
      "cloud = WordCloud(background_color='white',\n",
      "                  width=2500,\n",
      "                  height=1800,\n",
      "                  max_words=10,\n",
      "                  colormap='tab10',\n",
      "                  collocations=False,\n",
      "                  color_func=lambda *args, **kwargs: cols[i],\n",
      "                  prefer_horizontal=1.0)\n",
      "\n",
      "topics = lda_model.show_topics(formatted=False)\n",
      "\n",
      "fig, axes = plt.subplots(2, 3, figsize=(10, 10), sharex=True, sharey=True)\n",
      "\n",
      "for i, ax in enumerate(axes.flatten()):\n",
      "    fig.add_subplot(ax)\n",
      "    topic_words = dict(topics[i][1])\n",
      "    cloud.generate_from_frequencies(topic_words, max_font_size=300)\n",
      "    plt.gca().imshow(cloud)\n",
      "    plt.gca().set_title('Topic ' + str(i), fontdict=dict(size=16))\n",
      "    plt.gca().axis('off')\n",
      "\n",
      "\n",
      "plt.subplots_adjust(wspace=0, hspace=0)\n",
      "plt.axis('off')\n",
      "plt.margins(x=0, y=0)\n",
      "plt.tight_layout()\n",
      "st.pyplot()\n",
      "\n",
      "st.markdown(\"---\")\n",
      "\n",
      "####################### SETTING UP THE DATAFRAME FOR SUNBURST-GRAPH ############################\n",
      "\n",
      "df_topic_sents_keywords = format_topics_sentences(\n",
      "    ldamodel=lda_model, corpus=corpus)\n",
      "df_some = pd.DataFrame(df_topic_sents_keywords, columns=[\n",
      "                       'Document No', 'Dominant Topic', 'Topic % Contribution', 'Keywords'])\n",
      "df_some['Names'] = Resumes['Name']\n",
      "\n",
      "df = df_some\n",
      "\n",
      "st.markdown(\"## Topic Modelling of Resumes \")\n",
      "st.markdown(\n",
      "    \"Using LDA to divide the topics into a number of usefull topics and creating a Cluster of matching topic resumes.  \")\n",
      "fig3 = px.sunburst(df, path=['Dominant Topic', 'Names'], values='Topic % Contribution',\n",
      "                   color='Dominant Topic', color_continuous_scale='viridis', width=800, height=800, title=\"Topic Distribution Graph\")\n",
      "st.write(fig3)\n",
      "\n",
      "\n",
      "############################## RESUME PRINTING #############################\n",
      "\n",
      "option_2 = st.selectbox(\"Show the Best Matching Resumes?\", options=[\n",
      "    'YES', 'NO'])\n",
      "if option_2 == 'YES':\n",
      "    indx = st.slider(\"Which resume to display ?:\",\n",
      "                     1, Ranked_resumes.shape[0], 1)\n",
      "\n",
      "    st.write(\"Displaying Resume with Rank: \", indx)\n",
      "    st.markdown(\"---\")\n",
      "    st.markdown(\"## **Resume** \")\n",
      "    value = Ranked_resumes.iloc[indx-1, 2]\n",
      "    st.markdown(\"#### The Word Cloud For the Resume\")\n",
      "    wordcloud = WordCloud(width=800, height=800,\n",
      "                          background_color='white',\n",
      "                          colormap='viridis', collocations=False,\n",
      "                          min_font_size=10).generate(value)\n",
      "    plt.figure(figsize=(7, 7), facecolor=None)\n",
      "    plt.imshow(wordcloud)\n",
      "    plt.axis(\"off\")\n",
      "    plt.tight_layout(pad=0)\n",
      "    st.pyplot()\n",
      "\n",
      "    st.write(\"With a Match Score of :\", Ranked_resumes.iloc[indx-1, 6])\n",
      "    fig = go.Figure(data=[go.Table(\n",
      "        header=dict(values=[\"Resume\"],\n",
      "                    fill_color='#f0a500',\n",
      "                    align='center', font=dict(color='white', size=16)),\n",
      "        cells=dict(values=[str(value)],\n",
      "                   fill_color='#f4f4f4',\n",
      "                   align='left'))])\n",
      "\n",
      "    fig.update_layout(width=800, height=1200)\n",
      "    st.write(fig)\n",
      "    # st.text(df_sorted.iloc[indx-1, 1])\n",
      "    st.markdown(\"---\")\n",
      "\n",
      "from operator import index\n",
      "from pandas._config.config import options\n",
      "import Cleaner\n",
      "import textract as tx\n",
      "import pandas as pd\n",
      "import os\n",
      "import tf_idf\n",
      "\n",
      "resume_dir = \"Data/Resumes/\"\n",
      "job_desc_dir = \"Data/JobDesc/\"\n",
      "resume_names = os.listdir(resume_dir)\n",
      "job_description_names = os.listdir(job_desc_dir)\n",
      "\n",
      "document = []\n",
      "\n",
      "\n",
      "def read_resumes(list_of_resumes, resume_directory):\n",
      "    placeholder = []\n",
      "    for res in list_of_resumes:\n",
      "        temp = []\n",
      "        temp.append(res)\n",
      "        text = tx.process(resume_directory+res, encoding='ascii')\n",
      "        text = str(text, 'utf-8')\n",
      "        temp.append(text)\n",
      "        placeholder.append(temp)\n",
      "    return placeholder\n",
      "\n",
      "\n",
      "document = read_resumes(resume_names, resume_dir)\n",
      "\n",
      "\n",
      "def get_cleaned_words(document):\n",
      "    for i in range(len(document)):\n",
      "        raw = Cleaner.Cleaner(document[i][1])\n",
      "        document[i].append(\" \".join(raw[0]))\n",
      "        document[i].append(\" \".join(raw[1]))\n",
      "        document[i].append(\" \".join(raw[2]))\n",
      "        sentence = tf_idf.do_tfidf(document[i][3].split(\" \"))\n",
      "        document[i].append(sentence)\n",
      "    return document\n",
      "\n",
      "\n",
      "Doc = get_cleaned_words(document)\n",
      "\n",
      "Database = pd.DataFrame(document, columns=[\n",
      "                        \"Name\", \"Context\", \"Cleaned\", \"Selective\", \"Selective_Reduced\", \"TF_Based\"])\n",
      "\n",
      "Database.to_csv(\"Resume_Data.csv\", index=False)\n",
      "\n",
      "\n",
      "def read_jobdescriptions(job_description_names, job_desc_dir):\n",
      "    placeholder = []\n",
      "    for tes in job_description_names:\n",
      "        temp = []\n",
      "        temp.append(tes)\n",
      "        text = tx.process(job_desc_dir+tes, encoding='ascii')\n",
      "        text = str(text, 'utf-8')\n",
      "        temp.append(text)\n",
      "        placeholder.append(temp)\n",
      "    return placeholder\n",
      "\n",
      "\n",
      "job_document = read_jobdescriptions(job_description_names, job_desc_dir)\n",
      "\n",
      "Jd = get_cleaned_words(job_document)\n",
      "\n",
      "jd_database = pd.DataFrame(Jd, columns=[\n",
      "                           \"Name\", \"Context\", \"Cleaned\", \"Selective\", \"Selective_Reduced\", \"TF_Based\"])\n",
      "\n",
      "jd_database.to_csv(\"Job_Data.csv\", index=False)\n",
      "\n",
      "from sklearn.feature_extraction.text import TfidfVectorizer\n",
      "\n",
      "\n",
      "def do_tfidf(token):\n",
      "    tfidf = TfidfVectorizer(max_df=0.05, min_df=0.002)\n",
      "    words = tfidf.fit_transform(token)\n",
      "    sentence = \" \".join(tfidf.get_feature_names())\n",
      "    return sentence\n",
      "\n",
      "from operator import index\n",
      "from pandas._config.config import options\n",
      "import Cleaner\n",
      "import textract as tx\n",
      "import pandas as pd\n",
      "import os\n",
      "import tf_idf\n",
      "\n",
      "resume_dir = \"Data/Resumes/\"\n",
      "job_desc_dir = \"Data/JobDesc/\"\n",
      "resume_names = os.listdir(resume_dir)\n",
      "job_description_names = os.listdir(job_desc_dir)\n",
      "\n",
      "document = []\n",
      "\n",
      "\n",
      "def read_resumes(list_of_resumes, resume_directory):\n",
      "    placeholder = []\n",
      "    for res in list_of_resumes:\n",
      "        temp = []\n",
      "        temp.append(res)\n",
      "        text = tx.process(resume_directory+res, encoding='ascii')\n",
      "        text = str(text, 'utf-8')\n",
      "        temp.append(text)\n",
      "        placeholder.append(temp)\n",
      "    return placeholder\n",
      "\n",
      "\n",
      "document = read_resumes(resume_names, resume_dir)\n",
      "\n",
      "\n",
      "def get_cleaned_words(document):\n",
      "    for i in range(len(document)):\n",
      "        raw = Cleaner.Cleaner(document[i][1])\n",
      "        document[i].append(\" \".join(raw[0]))\n",
      "        document[i].append(\" \".join(raw[1]))\n",
      "        document[i].append(\" \".join(raw[2]))\n",
      "        sentence = tf_idf.do_tfidf(document[i][3].split(\" \"))\n",
      "        document[i].append(sentence)\n",
      "    return document\n",
      "\n",
      "\n",
      "Doc = get_cleaned_words(document)\n",
      "\n",
      "Database = pd.DataFrame(document, columns=[\n",
      "                        \"Name\", \"Context\", \"Cleaned\", \"Selective\", \"Selective_Reduced\", \"TF_Based\"])\n",
      "\n",
      "Database.to_csv(\"Resume_Data.csv\", index=False)\n",
      "\n",
      "# Database.to_json(\"Resume_Data.json\", index=False)\n",
      "\n",
      "\n",
      "def read_jobdescriptions(job_description_names, job_desc_dir):\n",
      "    placeholder = []\n",
      "    for tes in job_description_names:\n",
      "        temp = []\n",
      "        temp.append(tes)\n",
      "        text = tx.process(job_desc_dir+tes, encoding='ascii')\n",
      "        text = str(text, 'utf-8')\n",
      "        temp.append(text)\n",
      "        placeholder.append(temp)\n",
      "    return placeholder\n",
      "\n",
      "\n",
      "job_document = read_jobdescriptions(job_description_names, job_desc_dir)\n",
      "\n",
      "Jd = get_cleaned_words(job_document)\n",
      "\n",
      "jd_database = pd.DataFrame(Jd, columns=[\n",
      "                           \"Name\", \"Context\", \"Cleaned\", \"Selective\", \"Selective_Reduced\", \"TF_Based\"])\n",
      "\n",
      "jd_database.to_csv(\"Job_Data.csv\", index=False)\n",
      "\n",
      "import matplotlib.colors as mcolors\n",
      "import gensim\n",
      "import gensim.corpora as corpora\n",
      "from operator import index\n",
      "from wordcloud import WordCloud\n",
      "from pandas._config.config import options\n",
      "import pandas as pd\n",
      "import streamlit as st\n",
      "import plotly.express as px\n",
      "import plotly.graph_objects as go\n",
      "import matplotlib.pyplot as plt\n",
      "import Similar\n",
      "from PIL import Image\n",
      "import time\n",
      "\n",
      "\n",
      "image = Image.open('Images//logo.png')\n",
      "st.image(image, use_column_width=True)\n",
      "\n",
      "st.title(\"Resume Matcher\")\n",
      "\n",
      "\n",
      "# Reading the CSV files prepared by the fileReader.py\n",
      "Resumes = pd.read_csv('Resume_Data.csv')\n",
      "Jobs = pd.read_csv('Job_Data.csv')\n",
      "\n",
      "\n",
      "############################### JOB DESCRIPTION CODE ######################################\n",
      "# Checking for Multiple Job Descriptions\n",
      "# If more than one Job Descriptions are available, it asks user to select one as well.\n",
      "if len(Jobs['Name']) <= 1:\n",
      "    st.write(\n",
      "        \"There is only 1 Job Description present. It will be used to create scores.\")\n",
      "else:\n",
      "    st.write(\"There are \", len(Jobs['Name']),\n",
      "             \"Job Descriptions available. Please select one.\")\n",
      "\n",
      "\n",
      "# Asking to Print the Job Desciption Names\n",
      "if len(Jobs['Name']) > 1:\n",
      "    option_yn = st.selectbox(\n",
      "        \"Show the Job Description Names?\", options=['YES', 'NO'])\n",
      "    if option_yn == 'YES':\n",
      "        index = [a for a in range(len(Jobs['Name']))]\n",
      "        fig = go.Figure(data=[go.Table(header=dict(values=[\"Job No.\", \"Job Desc. Name\"], line_color='darkslategray',\n",
      "                                                   fill_color='lightskyblue'),\n",
      "                                       cells=dict(values=[index, Jobs['Name']], line_color='darkslategray',\n",
      "                                                  fill_color='cyan'))\n",
      "                              ])\n",
      "        fig.update_layout(width=700, height=400)\n",
      "        st.write(fig)\n",
      "\n",
      "\n",
      "# Asking to chose the Job Description\n",
      "index = st.slider(\"Which JD to select ? : \", 0,\n",
      "                  len(Jobs['Name'])-1, 1)\n",
      "\n",
      "\n",
      "option_yn = st.selectbox(\"Show the Job Description ?\", options=['YES', 'NO'])\n",
      "if option_yn == 'YES':\n",
      "    st.markdown(\"---\")\n",
      "    st.markdown(\"### Job Description :\")\n",
      "    fig = go.Figure(data=[go.Table(\n",
      "        header=dict(values=[\"Job Description\"],\n",
      "                    fill_color='#f0a500',\n",
      "                    align='center', font=dict(color='white', size=16)),\n",
      "        cells=dict(values=[Jobs['Context'][index]],\n",
      "                   fill_color='#f4f4f4',\n",
      "                   align='left'))])\n",
      "\n",
      "    fig.update_layout(width=800, height=500)\n",
      "    st.write(fig)\n",
      "    st.markdown(\"---\")\n",
      "\n",
      "\n",
      "#################################### SCORE CALCUATION ################################\n",
      "\n",
      "def calculate_scores(resumes, job_description):\n",
      "    scores = []\n",
      "    for x in range(resumes.shape[0]):\n",
      "        score = Similar.match(\n",
      "            resumes['TF_Based'][x], job_description['TF_Based'][index])\n",
      "        scores.append(score)\n",
      "    return scores\n",
      "\n",
      "\n",
      "Resumes['Scores'] = calculate_scores(Resumes, Jobs)\n",
      "\n",
      "Ranked_resumes = Resumes.sort_values(\n",
      "    by=['Scores'], ascending=False).reset_index(drop=True)\n",
      "\n",
      "Ranked_resumes['Rank'] = pd.DataFrame(\n",
      "    [i for i in range(1, len(Ranked_resumes['Scores'])+1)])\n",
      "\n",
      "###################################### SCORE TABLE PLOT ####################################\n",
      "\n",
      "fig1 = go.Figure(data=[go.Table(\n",
      "    header=dict(values=[\"Rank\", \"Name\", \"Scores\"],\n",
      "                fill_color='#00416d',\n",
      "                align='center', font=dict(color='white', size=16)),\n",
      "    cells=dict(values=[Ranked_resumes.Rank, Ranked_resumes.Name, Ranked_resumes.Scores],\n",
      "               fill_color='#d6e0f0',\n",
      "               align='left'))])\n",
      "\n",
      "fig1.update_layout(title=\"Top Ranked Resumes\", width=700, height=1100)\n",
      "st.write(fig1)\n",
      "\n",
      "st.markdown(\"---\")\n",
      "\n",
      "fig2 = px.bar(Ranked_resumes,\n",
      "              x=Ranked_resumes['Name'], y=Ranked_resumes['Scores'], color='Scores',\n",
      "              color_continuous_scale='haline', title=\"Score and Rank Distribution\")\n",
      "# fig.update_layout(width=700, height=700)\n",
      "st.write(fig2)\n",
      "\n",
      "\n",
      "st.markdown(\"---\")\n",
      "\n",
      "############################################ TF-IDF Code ###################################\n",
      "\n",
      "\n",
      "def get_list_of_words(document):\n",
      "    Document = []\n",
      "\n",
      "    for a in document:\n",
      "        raw = a.split(\" \")\n",
      "        Document.append(raw)\n",
      "\n",
      "    return Document\n",
      "\n",
      "\n",
      "document = get_list_of_words(Resumes['Cleaned'])\n",
      "\n",
      "id2word = corpora.Dictionary(document)\n",
      "corpus = [id2word.doc2bow(text) for text in document]\n",
      "\n",
      "lda_model = gensim.models.ldamodel.LdaModel(corpus=corpus, id2word=id2word, num_topics=6, random_state=100,\n",
      "                                            update_every=1, chunksize=100, passes=50, alpha='auto', per_word_topics=True)\n",
      "\n",
      "################################### LDA CODE ##############################################\n",
      "\n",
      "\n",
      "def format_topics_sentences(ldamodel, corpus):\n",
      "    sent_topics_df = []\n",
      "    for i, row_list in enumerate(ldamodel[corpus]):\n",
      "        row = row_list[0] if ldamodel.per_word_topics else row_list\n",
      "        row = sorted(row, key=lambda x: (x[1]), reverse=True)\n",
      "        for j, (topic_num, prop_topic) in enumerate(row):\n",
      "            if j == 0:\n",
      "                wp = ldamodel.show_topic(topic_num)\n",
      "                topic_keywords = \", \".join([word for word, prop in wp])\n",
      "                sent_topics_df.append(\n",
      "                    [i, int(topic_num), round(prop_topic, 4)*100, topic_keywords])\n",
      "            else:\n",
      "                break\n",
      "\n",
      "    return(sent_topics_df)\n",
      "\n",
      "\n",
      "################################# Topic Word Cloud Code #####################################\n",
      "\n",
      "st.markdown(\"## Topics and Topic Related Keywords \")\n",
      "st.markdown(\n",
      "    \"\"\"This Wordcloud representation shows the Topic Number and the Top Keywords that contstitute a Topic.\n",
      "    This further is used to cluster the resumes.      \"\"\")\n",
      "\n",
      "cols = [color for name, color in mcolors.TABLEAU_COLORS.items()]\n",
      "\n",
      "cloud = WordCloud(background_color='white',\n",
      "                  width=2500,\n",
      "                  height=1800,\n",
      "                  max_words=10,\n",
      "                  colormap='tab10',\n",
      "                  collocations=False,\n",
      "                  color_func=lambda *args, **kwargs: cols[i],\n",
      "                  prefer_horizontal=1.0)\n",
      "\n",
      "topics = lda_model.show_topics(formatted=False)\n",
      "\n",
      "fig, axes = plt.subplots(2, 3, figsize=(10, 10), sharex=True, sharey=True)\n",
      "\n",
      "for i, ax in enumerate(axes.flatten()):\n",
      "    fig.add_subplot(ax)\n",
      "    topic_words = dict(topics[i][1])\n",
      "    cloud.generate_from_frequencies(topic_words, max_font_size=300)\n",
      "    plt.gca().imshow(cloud)\n",
      "    plt.gca().set_title('Topic ' + str(i), fontdict=dict(size=16))\n",
      "    plt.gca().axis('off')\n",
      "\n",
      "\n",
      "plt.subplots_adjust(wspace=0, hspace=0)\n",
      "plt.axis('off')\n",
      "plt.margins(x=0, y=0)\n",
      "plt.tight_layout()\n",
      "st.pyplot()\n",
      "\n",
      "st.markdown(\"---\")\n",
      "\n",
      "####################### SETTING UP THE DATAFRAME FOR SUNBURST-GRAPH ############################\n",
      "\n",
      "df_topic_sents_keywords = format_topics_sentences(\n",
      "    ldamodel=lda_model, corpus=corpus)\n",
      "df_some = pd.DataFrame(df_topic_sents_keywords, columns=[\n",
      "                       'Document No', 'Dominant Topic', 'Topic % Contribution', 'Keywords'])\n",
      "df_some['Names'] = Resumes['Name']\n",
      "\n",
      "df = df_some\n",
      "\n",
      "st.markdown(\"## Topic Modelling of Resumes \")\n",
      "st.markdown(\n",
      "    \"Using LDA to divide the topics into a number of usefull topics and creating a Cluster of matching topic resumes.  \")\n",
      "fig3 = px.sunburst(df, path=['Dominant Topic', 'Names'], values='Topic % Contribution',\n",
      "                   color='Dominant Topic', color_continuous_scale='viridis', width=800, height=800, title=\"Topic Distribution Graph\")\n",
      "st.write(fig3)\n",
      "\n",
      "\n",
      "############################## RESUME PRINTING #############################\n",
      "\n",
      "option_2 = st.selectbox(\"Show the Best Matching Resumes?\", options=[\n",
      "    'YES', 'NO'])\n",
      "if option_2 == 'YES':\n",
      "    indx = st.slider(\"Which resume to display ?:\",\n",
      "                     1, Ranked_resumes.shape[0], 1)\n",
      "\n",
      "    st.write(\"Displaying Resume with Rank: \", indx)\n",
      "    st.markdown(\"---\")\n",
      "    st.markdown(\"## **Resume** \")\n",
      "    value = Ranked_resumes.iloc[indx-1, 2]\n",
      "    st.markdown(\"#### The Word Cloud For the Resume\")\n",
      "    wordcloud = WordCloud(width=800, height=800,\n",
      "                          background_color='white',\n",
      "                          colormap='viridis', collocations=False,\n",
      "                          min_font_size=10).generate(value)\n",
      "    plt.figure(figsize=(7, 7), facecolor=None)\n",
      "    plt.imshow(wordcloud)\n",
      "    plt.axis(\"off\")\n",
      "    plt.tight_layout(pad=0)\n",
      "    st.pyplot()\n",
      "\n",
      "    st.write(\"With a Match Score of :\", Ranked_resumes.iloc[indx-1, 6])\n",
      "    fig = go.Figure(data=[go.Table(\n",
      "        header=dict(values=[\"Resume\"],\n",
      "                    fill_color='#f0a500',\n",
      "                    align='center', font=dict(color='white', size=16)),\n",
      "        cells=dict(values=[str(value)],\n",
      "                   fill_color='#f4f4f4',\n",
      "                   align='left'))])\n",
      "\n",
      "    fig.update_layout(width=800, height=1200)\n",
      "    st.write(fig)\n",
      "    # st.text(df_sorted.iloc[indx-1, 1])\n",
      "    st.markdown(\"---\")\n",
      "\n",
      "import matplotlib.colors as mcolors\n",
      "import gensim\n",
      "import gensim.corpora as corpora\n",
      "from operator import index\n",
      "from wordcloud import WordCloud\n",
      "from pandas._config.config import options\n",
      "import pandas as pd\n",
      "import streamlit as st\n",
      "import plotly.express as px\n",
      "import plotly.graph_objects as go\n",
      "import matplotlib.pyplot as plt\n",
      "import Similar\n",
      "from PIL import Image\n",
      "import time\n",
      "\n",
      "\n",
      "image = Image.open('Images//logo.png')\n",
      "st.image(image, use_column_width=True)\n",
      "\n",
      "st.title(\"Resume Matcher\")\n",
      "\n",
      "\n",
      "# Reading the CSV files prepared by the fileReader.py\n",
      "Resumes = pd.read_csv('Resume_Data.csv')\n",
      "Jobs = pd.read_csv('Job_Data.csv')\n",
      "\n",
      "\n",
      "############################### JOB DESCRIPTION CODE ######################################\n",
      "# Checking for Multiple Job Descriptions\n",
      "# If more than one Job Descriptions are available, it asks user to select one as well.\n",
      "if len(Jobs['Name']) <= 1:\n",
      "    st.write(\n",
      "        \"There is only 1 Job Description present. It will be used to create scores.\")\n",
      "else:\n",
      "    st.write(\"There are \", len(Jobs['Name']),\n",
      "             \"Job Descriptions available. Please select one.\")\n",
      "\n",
      "\n",
      "# Asking to Print the Job Desciption Names\n",
      "if len(Jobs['Name']) > 1:\n",
      "    option_yn = st.selectbox(\n",
      "        \"Show the Job Description Names?\", options=['YES', 'NO'])\n",
      "    if option_yn == 'YES':\n",
      "        index = [a for a in range(len(Jobs['Name']))]\n",
      "        fig = go.Figure(data=[go.Table(header=dict(values=[\"Job No.\", \"Job Desc. Name\"], line_color='darkslategray',\n",
      "                                                   fill_color='lightskyblue'),\n",
      "                                       cells=dict(values=[index, Jobs['Name']], line_color='darkslategray',\n",
      "                                                  fill_color='cyan'))\n",
      "                              ])\n",
      "        fig.update_layout(width=700, height=400)\n",
      "        st.write(fig)\n",
      "\n",
      "\n",
      "# Asking to chose the Job Description\n",
      "index = st.slider(\"Which JD to select ? : \", 0,\n",
      "                  len(Jobs['Name'])-1, 1)\n",
      "\n",
      "\n",
      "option_yn = st.selectbox(\"Show the Job Description ?\", options=['YES', 'NO'])\n",
      "if option_yn == 'YES':\n",
      "    st.markdown(\"---\")\n",
      "    st.markdown(\"### Job Description :\")\n",
      "    fig = go.Figure(data=[go.Table(\n",
      "        header=dict(values=[\"Job Description\"],\n",
      "                    fill_color='#f0a500',\n",
      "                    align='center', font=dict(color='white', size=16)),\n",
      "        cells=dict(values=[Jobs['Context'][index]],\n",
      "                   fill_color='#f4f4f4',\n",
      "                   align='left'))])\n",
      "\n",
      "    fig.update_layout(width=800, height=500)\n",
      "    st.write(fig)\n",
      "    st.markdown(\"---\")\n",
      "\n",
      "\n",
      "#################################### SCORE CALCUATION ################################\n",
      "@st.cache()\n",
      "def calculate_scores(resumes, job_description):\n",
      "    scores = []\n",
      "    for x in range(resumes.shape[0]):\n",
      "        score = Similar.match(\n",
      "            resumes['TF_Based'][x], job_description['TF_Based'][index])\n",
      "        scores.append(score)\n",
      "    return scores\n",
      "\n",
      "\n",
      "Resumes['Scores'] = calculate_scores(Resumes, Jobs)\n",
      "\n",
      "Ranked_resumes = Resumes.sort_values(\n",
      "    by=['Scores'], ascending=False).reset_index(drop=True)\n",
      "\n",
      "Ranked_resumes['Rank'] = pd.DataFrame(\n",
      "    [i for i in range(1, len(Ranked_resumes['Scores'])+1)])\n",
      "\n",
      "###################################### SCORE TABLE PLOT ####################################\n",
      "\n",
      "fig1 = go.Figure(data=[go.Table(\n",
      "    header=dict(values=[\"Rank\", \"Name\", \"Scores\"],\n",
      "                fill_color='#00416d',\n",
      "                align='center', font=dict(color='white', size=16)),\n",
      "    cells=dict(values=[Ranked_resumes.Rank, Ranked_resumes.Name, Ranked_resumes.Scores],\n",
      "               fill_color='#d6e0f0',\n",
      "               align='left'))])\n",
      "\n",
      "fig1.update_layout(title=\"Top Ranked Resumes\", width=700, height=1100)\n",
      "st.write(fig1)\n",
      "\n",
      "st.markdown(\"---\")\n",
      "\n",
      "fig2 = px.bar(Ranked_resumes,\n",
      "              x=Ranked_resumes['Name'], y=Ranked_resumes['Scores'], color='Scores',\n",
      "              color_continuous_scale='haline', title=\"Score and Rank Distribution\")\n",
      "# fig.update_layout(width=700, height=700)\n",
      "st.write(fig2)\n",
      "\n",
      "\n",
      "st.markdown(\"---\")\n",
      "\n",
      "############################################ TF-IDF Code ###################################\n",
      "\n",
      "\n",
      "@st.cache()\n",
      "def get_list_of_words(document):\n",
      "    Document = []\n",
      "\n",
      "    for a in document:\n",
      "        raw = a.split(\" \")\n",
      "        Document.append(raw)\n",
      "\n",
      "    return Document\n",
      "\n",
      "\n",
      "document = get_list_of_words(Resumes['Cleaned'])\n",
      "\n",
      "id2word = corpora.Dictionary(document)\n",
      "corpus = [id2word.doc2bow(text) for text in document]\n",
      "\n",
      "\n",
      "lda_model = gensim.models.ldamodel.LdaModel(corpus=corpus, id2word=id2word, num_topics=6, random_state=100,\n",
      "                                            update_every=1, chunksize=100, passes=50, alpha='auto', per_word_topics=True)\n",
      "\n",
      "################################### LDA CODE ##############################################\n",
      "\n",
      "\n",
      "@st.cache  # Trying to improve performance by reducing the rerun computations\n",
      "def format_topics_sentences(ldamodel, corpus):\n",
      "    sent_topics_df = []\n",
      "    for i, row_list in enumerate(ldamodel[corpus]):\n",
      "        row = row_list[0] if ldamodel.per_word_topics else row_list\n",
      "        row = sorted(row, key=lambda x: (x[1]), reverse=True)\n",
      "        for j, (topic_num, prop_topic) in enumerate(row):\n",
      "            if j == 0:\n",
      "                wp = ldamodel.show_topic(topic_num)\n",
      "                topic_keywords = \", \".join([word for word, prop in wp])\n",
      "                sent_topics_df.append(\n",
      "                    [i, int(topic_num), round(prop_topic, 4)*100, topic_keywords])\n",
      "            else:\n",
      "                break\n",
      "\n",
      "    return(sent_topics_df)\n",
      "\n",
      "\n",
      "################################# Topic Word Cloud Code #####################################\n",
      "st.sidebar.button('Hit Me')\n",
      "st.markdown(\"## Topics and Topic Related Keywords \")\n",
      "st.markdown(\n",
      "    \"\"\"This Wordcloud representation shows the Topic Number and the Top Keywords that contstitute a Topic.\n",
      "    This further is used to cluster the resumes.      \"\"\")\n",
      "\n",
      "cols = [color for name, color in mcolors.TABLEAU_COLORS.items()]\n",
      "\n",
      "cloud = WordCloud(background_color='white',\n",
      "                  width=2500,\n",
      "                  height=1800,\n",
      "                  max_words=10,\n",
      "                  colormap='tab10',\n",
      "                  collocations=False,\n",
      "                  color_func=lambda *args, **kwargs: cols[i],\n",
      "                  prefer_horizontal=1.0)\n",
      "\n",
      "topics = lda_model.show_topics(formatted=False)\n",
      "\n",
      "fig, axes = plt.subplots(2, 3, figsize=(10, 10), sharex=True, sharey=True)\n",
      "\n",
      "for i, ax in enumerate(axes.flatten()):\n",
      "    fig.add_subplot(ax)\n",
      "    topic_words = dict(topics[i][1])\n",
      "    cloud.generate_from_frequencies(topic_words, max_font_size=300)\n",
      "    plt.gca().imshow(cloud)\n",
      "    plt.gca().set_title('Topic ' + str(i), fontdict=dict(size=16))\n",
      "    plt.gca().axis('off')\n",
      "\n",
      "\n",
      "plt.subplots_adjust(wspace=0, hspace=0)\n",
      "plt.axis('off')\n",
      "plt.margins(x=0, y=0)\n",
      "plt.tight_layout()\n",
      "st.pyplot()\n",
      "\n",
      "st.markdown(\"---\")\n",
      "\n",
      "####################### SETTING UP THE DATAFRAME FOR SUNBURST-GRAPH ############################\n",
      "\n",
      "df_topic_sents_keywords = format_topics_sentences(\n",
      "    ldamodel=lda_model, corpus=corpus)\n",
      "df_some = pd.DataFrame(df_topic_sents_keywords, columns=[\n",
      "                       'Document No', 'Dominant Topic', 'Topic % Contribution', 'Keywords'])\n",
      "df_some['Names'] = Resumes['Name']\n",
      "\n",
      "df = df_some\n",
      "\n",
      "st.markdown(\"## Topic Modelling of Resumes \")\n",
      "st.markdown(\n",
      "    \"Using LDA to divide the topics into a number of usefull topics and creating a Cluster of matching topic resumes.  \")\n",
      "fig3 = px.sunburst(df, path=['Dominant Topic', 'Names'], values='Topic % Contribution',\n",
      "                   color='Dominant Topic', color_continuous_scale='viridis', width=800, height=800, title=\"Topic Distribution Graph\")\n",
      "st.write(fig3)\n",
      "\n",
      "\n",
      "############################## RESUME PRINTING #############################\n",
      "\n",
      "option_2 = st.selectbox(\"Show the Best Matching Resumes?\", options=[\n",
      "    'YES', 'NO'])\n",
      "if option_2 == 'YES':\n",
      "    indx = st.slider(\"Which resume to display ?:\",\n",
      "                     1, Ranked_resumes.shape[0], 1)\n",
      "\n",
      "    st.write(\"Displaying Resume with Rank: \", indx)\n",
      "    st.markdown(\"---\")\n",
      "    st.markdown(\"## **Resume** \")\n",
      "    value = Ranked_resumes.iloc[indx-1, 2]\n",
      "    st.markdown(\"#### The Word Cloud For the Resume\")\n",
      "    wordcloud = WordCloud(width=800, height=800,\n",
      "                          background_color='white',\n",
      "                          colormap='viridis', collocations=False,\n",
      "                          min_font_size=10).generate(value)\n",
      "    plt.figure(figsize=(7, 7), facecolor=None)\n",
      "    plt.imshow(wordcloud)\n",
      "    plt.axis(\"off\")\n",
      "    plt.tight_layout(pad=0)\n",
      "    st.pyplot()\n",
      "\n",
      "    st.write(\"With a Match Score of :\", Ranked_resumes.iloc[indx-1, 6])\n",
      "    fig = go.Figure(data=[go.Table(\n",
      "        header=dict(values=[\"Resume\"],\n",
      "                    fill_color='#f0a500',\n",
      "                    align='center', font=dict(color='white', size=16)),\n",
      "        cells=dict(values=[str(value)],\n",
      "                   fill_color='#f4f4f4',\n",
      "                   align='left'))])\n",
      "\n",
      "    fig.update_layout(width=800, height=1200)\n",
      "    st.write(fig)\n",
      "    # st.text(df_sorted.iloc[indx-1, 1])\n",
      "    st.markdown(\"---\")\n",
      "\n",
      "import matplotlib.colors as mcolors\n",
      "import gensim\n",
      "import gensim.corpora as corpora\n",
      "from operator import index\n",
      "from wordcloud import WordCloud\n",
      "from pandas._config.config import options\n",
      "import pandas as pd\n",
      "import streamlit as st\n",
      "import plotly.express as px\n",
      "import plotly.graph_objects as go\n",
      "import matplotlib.pyplot as plt\n",
      "import Similar\n",
      "from PIL import Image\n",
      "import time\n",
      "\n",
      "\n",
      "image = Image.open('Images//logo.png')\n",
      "st.image(image, use_column_width=True)\n",
      "\n",
      "st.title(\"Resume Matcher\")\n",
      "\n",
      "\n",
      "# Reading the CSV files prepared by the fileReader.py\n",
      "Resumes = pd.read_csv('Resume_Data.csv')\n",
      "Jobs = pd.read_csv('Job_Data.csv')\n",
      "\n",
      "\n",
      "############################### JOB DESCRIPTION CODE ######################################\n",
      "# Checking for Multiple Job Descriptions\n",
      "# If more than one Job Descriptions are available, it asks user to select one as well.\n",
      "if len(Jobs['Name']) <= 1:\n",
      "    st.write(\n",
      "        \"There is only 1 Job Description present. It will be used to create scores.\")\n",
      "else:\n",
      "    st.write(\"There are \", len(Jobs['Name']),\n",
      "             \"Job Descriptions available. Please select one.\")\n",
      "\n",
      "\n",
      "# Asking to Print the Job Desciption Names\n",
      "if len(Jobs['Name']) > 1:\n",
      "    option_yn = st.selectbox(\n",
      "        \"Show the Job Description Names?\", options=['YES', 'NO'])\n",
      "    if option_yn == 'YES':\n",
      "        index = [a for a in range(len(Jobs['Name']))]\n",
      "        fig = go.Figure(data=[go.Table(header=dict(values=[\"Job No.\", \"Job Desc. Name\"], line_color='darkslategray',\n",
      "                                                   fill_color='lightskyblue'),\n",
      "                                       cells=dict(values=[index, Jobs['Name']], line_color='darkslategray',\n",
      "                                                  fill_color='cyan'))\n",
      "                              ])\n",
      "        fig.update_layout(width=700, height=400)\n",
      "        st.write(fig)\n",
      "\n",
      "\n",
      "# Asking to chose the Job Description\n",
      "index = st.slider(\"Which JD to select ? : \", 0,\n",
      "                  len(Jobs['Name'])-1, 1)\n",
      "\n",
      "\n",
      "option_yn = st.selectbox(\"Show the Job Description ?\", options=['YES', 'NO'])\n",
      "if option_yn == 'YES':\n",
      "    st.markdown(\"---\")\n",
      "    st.markdown(\"### Job Description :\")\n",
      "    fig = go.Figure(data=[go.Table(\n",
      "        header=dict(values=[\"Job Description\"],\n",
      "                    fill_color='#f0a500',\n",
      "                    align='center', font=dict(color='white', size=16)),\n",
      "        cells=dict(values=[Jobs['Context'][index]],\n",
      "                   fill_color='#f4f4f4',\n",
      "                   align='left'))])\n",
      "\n",
      "    fig.update_layout(width=800, height=500)\n",
      "    st.write(fig)\n",
      "    st.markdown(\"---\")\n",
      "\n",
      "\n",
      "#################################### SCORE CALCUATION ################################\n",
      "@st.cache()\n",
      "def calculate_scores(resumes, job_description):\n",
      "    scores = []\n",
      "    for x in range(resumes.shape[0]):\n",
      "        score = Similar.match(\n",
      "            resumes['TF_Based'][x], job_description['TF_Based'][index])\n",
      "        scores.append(score)\n",
      "    return scores\n",
      "\n",
      "\n",
      "Resumes['Scores'] = calculate_scores(Resumes, Jobs)\n",
      "\n",
      "Ranked_resumes = Resumes.sort_values(\n",
      "    by=['Scores'], ascending=False).reset_index(drop=True)\n",
      "\n",
      "Ranked_resumes['Rank'] = pd.DataFrame(\n",
      "    [i for i in range(1, len(Ranked_resumes['Scores'])+1)])\n",
      "\n",
      "###################################### SCORE TABLE PLOT ####################################\n",
      "\n",
      "fig1 = go.Figure(data=[go.Table(\n",
      "    header=dict(values=[\"Rank\", \"Name\", \"Scores\"],\n",
      "                fill_color='#00416d',\n",
      "                align='center', font=dict(color='white', size=16)),\n",
      "    cells=dict(values=[Ranked_resumes.Rank, Ranked_resumes.Name, Ranked_resumes.Scores],\n",
      "               fill_color='#d6e0f0',\n",
      "               align='left'))])\n",
      "\n",
      "fig1.update_layout(title=\"Top Ranked Resumes\", width=700, height=1100)\n",
      "st.write(fig1)\n",
      "\n",
      "st.markdown(\"---\")\n",
      "\n",
      "fig2 = px.bar(Ranked_resumes,\n",
      "              x=Ranked_resumes['Name'], y=Ranked_resumes['Scores'], color='Scores',\n",
      "              color_continuous_scale='haline', title=\"Score and Rank Distribution\")\n",
      "# fig.update_layout(width=700, height=700)\n",
      "st.write(fig2)\n",
      "\n",
      "\n",
      "st.markdown(\"---\")\n",
      "\n",
      "############################################ TF-IDF Code ###################################\n",
      "\n",
      "\n",
      "@st.cache()\n",
      "def get_list_of_words(document):\n",
      "    Document = []\n",
      "\n",
      "    for a in document:\n",
      "        raw = a.split(\" \")\n",
      "        Document.append(raw)\n",
      "\n",
      "    return Document\n",
      "\n",
      "\n",
      "document = get_list_of_words(Resumes['Cleaned'])\n",
      "\n",
      "id2word = corpora.Dictionary(document)\n",
      "corpus = [id2word.doc2bow(text) for text in document]\n",
      "\n",
      "\n",
      "lda_model = gensim.models.ldamodel.LdaModel(corpus=corpus, id2word=id2word, num_topics=6, random_state=100,\n",
      "                                            update_every=1, chunksize=100, passes=50, alpha='auto', per_word_topics=True)\n",
      "\n",
      "################################### LDA CODE ##############################################\n",
      "\n",
      "\n",
      "@st.cache  # Trying to improve performance by reducing the rerun computations\n",
      "def format_topics_sentences(ldamodel, corpus):\n",
      "    sent_topics_df = []\n",
      "    for i, row_list in enumerate(ldamodel[corpus]):\n",
      "        row = row_list[0] if ldamodel.per_word_topics else row_list\n",
      "        row = sorted(row, key=lambda x: (x[1]), reverse=True)\n",
      "        for j, (topic_num, prop_topic) in enumerate(row):\n",
      "            if j == 0:\n",
      "                wp = ldamodel.show_topic(topic_num)\n",
      "                topic_keywords = \", \".join([word for word, prop in wp])\n",
      "                sent_topics_df.append(\n",
      "                    [i, int(topic_num), round(prop_topic, 4)*100, topic_keywords])\n",
      "            else:\n",
      "                break\n",
      "\n",
      "    return(sent_topics_df)\n",
      "\n",
      "\n",
      "################################# Topic Word Cloud Code #####################################\n",
      "st.sidebar.button('Hit Me')\n",
      "st.markdown(\"## Topics and Topic Related Keywords \")\n",
      "st.markdown(\n",
      "    \"\"\"This Wordcloud representation shows the Topic Number and the Top Keywords that contstitute a Topic.\n",
      "    This further is used to cluster the resumes.      \"\"\")\n",
      "\n",
      "cols = [color for name, color in mcolors.TABLEAU_COLORS.items()]\n",
      "\n",
      "cloud = WordCloud(background_color='white',\n",
      "                  width=2500,\n",
      "                  height=1800,\n",
      "                  max_words=10,\n",
      "                  colormap='tab10',\n",
      "                  collocations=False,\n",
      "                  color_func=lambda *args, **kwargs: cols[i],\n",
      "                  prefer_horizontal=1.0)\n",
      "\n",
      "topics = lda_model.show_topics(formatted=False)\n",
      "\n",
      "fig, axes = plt.subplots(2, 3, figsize=(10, 10), sharex=True, sharey=True)\n",
      "\n",
      "for i, ax in enumerate(axes.flatten()):\n",
      "    fig.add_subplot(ax)\n",
      "    topic_words = dict(topics[i][1])\n",
      "    cloud.generate_from_frequencies(topic_words, max_font_size=300)\n",
      "    plt.gca().imshow(cloud)\n",
      "    plt.gca().set_title('Topic ' + str(i), fontdict=dict(size=16))\n",
      "    plt.gca().axis('off')\n",
      "\n",
      "\n",
      "plt.subplots_adjust(wspace=0, hspace=0)\n",
      "plt.axis('off')\n",
      "plt.margins(x=0, y=0)\n",
      "plt.tight_layout()\n",
      "st.pyplot(plt)\n",
      "\n",
      "st.markdown(\"---\")\n",
      "\n",
      "####################### SETTING UP THE DATAFRAME FOR SUNBURST-GRAPH ############################\n",
      "\n",
      "df_topic_sents_keywords = format_topics_sentences(\n",
      "    ldamodel=lda_model, corpus=corpus)\n",
      "df_some = pd.DataFrame(df_topic_sents_keywords, columns=[\n",
      "                       'Document No', 'Dominant Topic', 'Topic % Contribution', 'Keywords'])\n",
      "df_some['Names'] = Resumes['Name']\n",
      "\n",
      "df = df_some\n",
      "\n",
      "st.markdown(\"## Topic Modelling of Resumes \")\n",
      "st.markdown(\n",
      "    \"Using LDA to divide the topics into a number of usefull topics and creating a Cluster of matching topic resumes.  \")\n",
      "fig3 = px.sunburst(df, path=['Dominant Topic', 'Names'], values='Topic % Contribution',\n",
      "                   color='Dominant Topic', color_continuous_scale='viridis', width=800, height=800, title=\"Topic Distribution Graph\")\n",
      "st.write(fig3)\n",
      "\n",
      "\n",
      "############################## RESUME PRINTING #############################\n",
      "\n",
      "option_2 = st.selectbox(\"Show the Best Matching Resumes?\", options=[\n",
      "    'YES', 'NO'])\n",
      "if option_2 == 'YES':\n",
      "    indx = st.slider(\"Which resume to display ?:\",\n",
      "                     1, Ranked_resumes.shape[0], 1)\n",
      "\n",
      "    st.write(\"Displaying Resume with Rank: \", indx)\n",
      "    st.markdown(\"---\")\n",
      "    st.markdown(\"## **Resume** \")\n",
      "    value = Ranked_resumes.iloc[indx-1, 2]\n",
      "    st.markdown(\"#### The Word Cloud For the Resume\")\n",
      "    wordcloud = WordCloud(width=800, height=800,\n",
      "                          background_color='white',\n",
      "                          colormap='viridis', collocations=False,\n",
      "                          min_font_size=10).generate(value)\n",
      "    plt.figure(figsize=(7, 7), facecolor=None)\n",
      "    plt.imshow(wordcloud)\n",
      "    plt.axis(\"off\")\n",
      "    plt.tight_layout(pad=0)\n",
      "    st.pyplot()\n",
      "\n",
      "    st.write(\"With a Match Score of :\", Ranked_resumes.iloc[indx-1, 6])\n",
      "    fig = go.Figure(data=[go.Table(\n",
      "        header=dict(values=[\"Resume\"],\n",
      "                    fill_color='#f0a500',\n",
      "                    align='center', font=dict(color='white', size=16)),\n",
      "        cells=dict(values=[str(value)],\n",
      "                   fill_color='#f4f4f4',\n",
      "                   align='left'))])\n",
      "\n",
      "    fig.update_layout(width=800, height=1200)\n",
      "    st.write(fig)\n",
      "    # st.text(df_sorted.iloc[indx-1, 1])\n",
      "    st.markdown(\"---\")\n",
      "\n",
      "import matplotlib.colors as mcolors\n",
      "import gensim\n",
      "import gensim.corpora as corpora\n",
      "from operator import index\n",
      "from wordcloud import WordCloud\n",
      "from pandas._config.config import options\n",
      "import pandas as pd\n",
      "import streamlit as st\n",
      "import plotly.express as px\n",
      "import plotly.graph_objects as go\n",
      "import matplotlib.pyplot as plt\n",
      "import Similar\n",
      "from PIL import Image\n",
      "import time\n",
      "\n",
      "\n",
      "image = Image.open('Images//logo.png')\n",
      "st.image(image, use_column_width=True)\n",
      "\n",
      "st.title(\"Resume Matcher\")\n",
      "\n",
      "\n",
      "# Reading the CSV files prepared by the fileReader.py\n",
      "Resumes = pd.read_csv('Resume_Data.csv')\n",
      "Jobs = pd.read_csv('Job_Data.csv')\n",
      "\n",
      "\n",
      "############################### JOB DESCRIPTION CODE ######################################\n",
      "# Checking for Multiple Job Descriptions\n",
      "# If more than one Job Descriptions are available, it asks user to select one as well.\n",
      "if len(Jobs['Name']) <= 1:\n",
      "    st.write(\n",
      "        \"There is only 1 Job Description present. It will be used to create scores.\")\n",
      "else:\n",
      "    st.write(\"There are \", len(Jobs['Name']),\n",
      "             \"Job Descriptions available. Please select one.\")\n",
      "\n",
      "\n",
      "# Asking to Print the Job Desciption Names\n",
      "if len(Jobs['Name']) > 1:\n",
      "    option_yn = st.selectbox(\n",
      "        \"Show the Job Description Names?\", options=['YES', 'NO'])\n",
      "    if option_yn == 'YES':\n",
      "        index = [a for a in range(len(Jobs['Name']))]\n",
      "        fig = go.Figure(data=[go.Table(header=dict(values=[\"Job No.\", \"Job Desc. Name\"], line_color='darkslategray',\n",
      "                                                   fill_color='lightskyblue'),\n",
      "                                       cells=dict(values=[index, Jobs['Name']], line_color='darkslategray',\n",
      "                                                  fill_color='cyan'))\n",
      "                              ])\n",
      "        fig.update_layout(width=700, height=400)\n",
      "        st.write(fig)\n",
      "\n",
      "\n",
      "# Asking to chose the Job Description\n",
      "index = st.slider(\"Which JD to select ? : \", 0,\n",
      "                  len(Jobs['Name'])-1, 1)\n",
      "\n",
      "\n",
      "option_yn = st.selectbox(\"Show the Job Description ?\", options=['YES', 'NO'])\n",
      "if option_yn == 'YES':\n",
      "    st.markdown(\"---\")\n",
      "    st.markdown(\"### Job Description :\")\n",
      "    fig = go.Figure(data=[go.Table(\n",
      "        header=dict(values=[\"Job Description\"],\n",
      "                    fill_color='#f0a500',\n",
      "                    align='center', font=dict(color='white', size=16)),\n",
      "        cells=dict(values=[Jobs['Context'][index]],\n",
      "                   fill_color='#f4f4f4',\n",
      "                   align='left'))])\n",
      "\n",
      "    fig.update_layout(width=800, height=500)\n",
      "    st.write(fig)\n",
      "    st.markdown(\"---\")\n",
      "\n",
      "\n",
      "#################################### SCORE CALCUATION ################################\n",
      "@st.cache()\n",
      "def calculate_scores(resumes, job_description):\n",
      "    scores = []\n",
      "    for x in range(resumes.shape[0]):\n",
      "        score = Similar.match(\n",
      "            resumes['TF_Based'][x], job_description['TF_Based'][index])\n",
      "        scores.append(score)\n",
      "    return scores\n",
      "\n",
      "\n",
      "Resumes['Scores'] = calculate_scores(Resumes, Jobs)\n",
      "\n",
      "Ranked_resumes = Resumes.sort_values(\n",
      "    by=['Scores'], ascending=False).reset_index(drop=True)\n",
      "\n",
      "Ranked_resumes['Rank'] = pd.DataFrame(\n",
      "    [i for i in range(1, len(Ranked_resumes['Scores'])+1)])\n",
      "\n",
      "###################################### SCORE TABLE PLOT ####################################\n",
      "\n",
      "fig1 = go.Figure(data=[go.Table(\n",
      "    header=dict(values=[\"Rank\", \"Name\", \"Scores\"],\n",
      "                fill_color='#00416d',\n",
      "                align='center', font=dict(color='white', size=16)),\n",
      "    cells=dict(values=[Ranked_resumes.Rank, Ranked_resumes.Name, Ranked_resumes.Scores],\n",
      "               fill_color='#d6e0f0',\n",
      "               align='left'))])\n",
      "\n",
      "fig1.update_layout(title=\"Top Ranked Resumes\", width=700, height=1100)\n",
      "st.write(fig1)\n",
      "\n",
      "st.markdown(\"---\")\n",
      "\n",
      "fig2 = px.bar(Ranked_resumes,\n",
      "              x=Ranked_resumes['Name'], y=Ranked_resumes['Scores'], color='Scores',\n",
      "              color_continuous_scale='haline', title=\"Score and Rank Distribution\")\n",
      "# fig.update_layout(width=700, height=700)\n",
      "st.write(fig2)\n",
      "\n",
      "\n",
      "st.markdown(\"---\")\n",
      "\n",
      "############################################ TF-IDF Code ###################################\n",
      "\n",
      "\n",
      "@st.cache()\n",
      "def get_list_of_words(document):\n",
      "    Document = []\n",
      "\n",
      "    for a in document:\n",
      "        raw = a.split(\" \")\n",
      "        Document.append(raw)\n",
      "\n",
      "    return Document\n",
      "\n",
      "\n",
      "document = get_list_of_words(Resumes['Cleaned'])\n",
      "\n",
      "id2word = corpora.Dictionary(document)\n",
      "corpus = [id2word.doc2bow(text) for text in document]\n",
      "\n",
      "\n",
      "lda_model = gensim.models.ldamodel.LdaModel(corpus=corpus, id2word=id2word, num_topics=6, random_state=100,\n",
      "                                            update_every=3, chunksize=100, passes=50, alpha='auto', per_word_topics=True)\n",
      "\n",
      "################################### LDA CODE ##############################################\n",
      "\n",
      "\n",
      "@st.cache  # Trying to improve performance by reducing the rerun computations\n",
      "def format_topics_sentences(ldamodel, corpus):\n",
      "    sent_topics_df = []\n",
      "    for i, row_list in enumerate(ldamodel[corpus]):\n",
      "        row = row_list[0] if ldamodel.per_word_topics else row_list\n",
      "        row = sorted(row, key=lambda x: (x[1]), reverse=True)\n",
      "        for j, (topic_num, prop_topic) in enumerate(row):\n",
      "            if j == 0:\n",
      "                wp = ldamodel.show_topic(topic_num)\n",
      "                topic_keywords = \", \".join([word for word, prop in wp])\n",
      "                sent_topics_df.append(\n",
      "                    [i, int(topic_num), round(prop_topic, 4)*100, topic_keywords])\n",
      "            else:\n",
      "                break\n",
      "\n",
      "    return sent_topics_df\n",
      "\n",
      "\n",
      "################################# Topic Word Cloud Code #####################################\n",
      "st.sidebar.button('Hit Me')\n",
      "st.markdown(\"## Topics and Topic Related Keywords \")\n",
      "st.markdown(\n",
      "    \"\"\"This Wordcloud representation shows the Topic Number and the Top Keywords that contstitute a Topic.\n",
      "    This further is used to cluster the resumes.      \"\"\")\n",
      "\n",
      "cols = [color for name, color in mcolors.TABLEAU_COLORS.items()]\n",
      "\n",
      "cloud = WordCloud(background_color='white',\n",
      "                  width=2500,\n",
      "                  height=1800,\n",
      "                  max_words=10,\n",
      "                  colormap='tab10',\n",
      "                  collocations=False,\n",
      "                  color_func=lambda *args, **kwargs: cols[i],\n",
      "                  prefer_horizontal=1.0)\n",
      "\n",
      "topics = lda_model.show_topics(formatted=False)\n",
      "\n",
      "fig, axes = plt.subplots(2, 3, figsize=(10, 10), sharex=True, sharey=True)\n",
      "\n",
      "for i, ax in enumerate(axes.flatten()):\n",
      "    fig.add_subplot(ax)\n",
      "    topic_words = dict(topics[i][1])\n",
      "    cloud.generate_from_frequencies(topic_words, max_font_size=300)\n",
      "    plt.gca().imshow(cloud)\n",
      "    plt.gca().set_title('Topic ' + str(i), fontdict=dict(size=16))\n",
      "    plt.gca().axis('off')\n",
      "\n",
      "\n",
      "plt.subplots_adjust(wspace=0, hspace=0)\n",
      "plt.axis('off')\n",
      "plt.margins(x=0, y=0)\n",
      "plt.tight_layout()\n",
      "st.pyplot(plt)\n",
      "\n",
      "st.markdown(\"---\")\n",
      "\n",
      "####################### SETTING UP THE DATAFRAME FOR SUNBURST-GRAPH ############################\n",
      "\n",
      "df_topic_sents_keywords = format_topics_sentences(\n",
      "    ldamodel=lda_model, corpus=corpus)\n",
      "df_some = pd.DataFrame(df_topic_sents_keywords, columns=[\n",
      "                       'Document No', 'Dominant Topic', 'Topic % Contribution', 'Keywords'])\n",
      "df_some['Names'] = Resumes['Name']\n",
      "\n",
      "df = df_some\n",
      "\n",
      "st.markdown(\"## Topic Modelling of Resumes \")\n",
      "st.markdown(\n",
      "    \"Using LDA to divide the topics into a number of usefull topics and creating a Cluster of matching topic resumes.  \")\n",
      "fig3 = px.sunburst(df, path=['Dominant Topic', 'Names'], values='Topic % Contribution',\n",
      "                   color='Dominant Topic', color_continuous_scale='viridis', width=800, height=800, title=\"Topic Distribution Graph\")\n",
      "st.write(fig3)\n",
      "\n",
      "\n",
      "############################## RESUME PRINTING #############################\n",
      "\n",
      "option_2 = st.selectbox(\"Show the Best Matching Resumes?\", options=[\n",
      "    'YES', 'NO'])\n",
      "if option_2 == 'YES':\n",
      "    indx = st.slider(\"Which resume to display ?:\",\n",
      "                     1, Ranked_resumes.shape[0], 1)\n",
      "\n",
      "    st.write(\"Displaying Resume with Rank: \", indx)\n",
      "    st.markdown(\"---\")\n",
      "    st.markdown(\"## **Resume** \")\n",
      "    value = Ranked_resumes.iloc[indx-1, 2]\n",
      "    st.markdown(\"#### The Word Cloud For the Resume\")\n",
      "    wordcloud = WordCloud(width=800, height=800,\n",
      "                          background_color='white',\n",
      "                          colormap='viridis', collocations=False,\n",
      "                          min_font_size=10).generate(value)\n",
      "    plt.figure(figsize=(7, 7), facecolor=None)\n",
      "    plt.imshow(wordcloud)\n",
      "    plt.axis(\"off\")\n",
      "    plt.tight_layout(pad=0)\n",
      "    st.pyplot(plt)\n",
      "\n",
      "    st.write(\"With a Match Score of :\", Ranked_resumes.iloc[indx-1, 6])\n",
      "    fig = go.Figure(data=[go.Table(\n",
      "        header=dict(values=[\"Resume\"],\n",
      "                    fill_color='#f0a500',\n",
      "                    align='center', font=dict(color='white', size=16)),\n",
      "        cells=dict(values=[str(value)],\n",
      "                   fill_color='#f4f4f4',\n",
      "                   align='left'))])\n",
      "\n",
      "    fig.update_layout(width=800, height=1200)\n",
      "    st.write(fig)\n",
      "    # st.text(df_sorted.iloc[indx-1, 1])\n",
      "    st.markdown(\"---\")\n",
      "\n",
      "import matplotlib.colors as mcolors\n",
      "import gensim\n",
      "import gensim.corpora as corpora\n",
      "from operator import index\n",
      "from wordcloud import WordCloud\n",
      "from pandas._config.config import options\n",
      "import pandas as pd\n",
      "import streamlit as st\n",
      "import plotly.express as px\n",
      "import plotly.graph_objects as go\n",
      "import matplotlib.pyplot as plt\n",
      "import Similar\n",
      "from PIL import Image\n",
      "import time\n",
      "\n",
      "\n",
      "image = Image.open('Images//logo.png')\n",
      "st.image(image, use_column_width=True)\n",
      "\n",
      "st.title(\"Resume Matcher\")\n",
      "\n",
      "\n",
      "# Reading the CSV files prepared by the fileReader.py\n",
      "Resumes = pd.read_csv('Resume_Data.csv')\n",
      "Jobs = pd.read_csv('Job_Data.csv')\n",
      "\n",
      "\n",
      "############################### JOB DESCRIPTION CODE ######################################\n",
      "# Checking for Multiple Job Descriptions\n",
      "# If more than one Job Descriptions are available, it asks user to select one as well.\n",
      "if len(Jobs['Name']) <= 1:\n",
      "    st.write(\n",
      "        \"There is only 1 Job Description present. It will be used to create scores.\")\n",
      "else:\n",
      "    st.write(\"There are \", len(Jobs['Name']),\n",
      "             \"Job Descriptions available. Please select one.\")\n",
      "\n",
      "\n",
      "# Asking to Print the Job Desciption Names\n",
      "if len(Jobs['Name']) > 1:\n",
      "    option_yn = st.selectbox(\n",
      "        \"Show the Job Description Names?\", options=['YES', 'NO'])\n",
      "    if option_yn == 'YES':\n",
      "        index = [a for a in range(len(Jobs['Name']))]\n",
      "        fig = go.Figure(data=[go.Table(header=dict(values=[\"Job No.\", \"Job Desc. Name\"], line_color='darkslategray',\n",
      "                                                   fill_color='lightskyblue'),\n",
      "                                       cells=dict(values=[index, Jobs['Name']], line_color='darkslategray',\n",
      "                                                  fill_color='cyan'))\n",
      "                              ])\n",
      "        fig.update_layout(width=700, height=400)\n",
      "        st.write(fig)\n",
      "\n",
      "\n",
      "# Asking to chose the Job Description\n",
      "index = st.slider(\"Which JD to select ? : \", 0,\n",
      "                  len(Jobs['Name'])-1, 1)\n",
      "\n",
      "\n",
      "option_yn = st.selectbox(\"Show the Job Description ?\", options=['YES', 'NO'])\n",
      "if option_yn == 'YES':\n",
      "    st.markdown(\"---\")\n",
      "    st.markdown(\"### Job Description :\")\n",
      "    fig = go.Figure(data=[go.Table(\n",
      "        header=dict(values=[\"Job Description\"],\n",
      "                    fill_color='#f0a500',\n",
      "                    align='center', font=dict(color='white', size=16)),\n",
      "        cells=dict(values=[Jobs['Context'][index]],\n",
      "                   fill_color='#f4f4f4',\n",
      "                   align='left'))])\n",
      "\n",
      "    fig.update_layout(width=800, height=500)\n",
      "    st.write(fig)\n",
      "    st.markdown(\"---\")\n",
      "\n",
      "\n",
      "#################################### SCORE CALCUATION ################################\n",
      "@st.cache()\n",
      "def calculate_scores(resumes, job_description):\n",
      "    scores = []\n",
      "    for x in range(resumes.shape[0]):\n",
      "        score = Similar.match(\n",
      "            resumes['TF_Based'][x], job_description['TF_Based'][index])\n",
      "        scores.append(score)\n",
      "    return scores\n",
      "\n",
      "\n",
      "Resumes['Scores'] = calculate_scores(Resumes, Jobs)\n",
      "\n",
      "Ranked_resumes = Resumes.sort_values(\n",
      "    by=['Scores'], ascending=False).reset_index(drop=True)\n",
      "\n",
      "Ranked_resumes['Rank'] = pd.DataFrame(\n",
      "    [i for i in range(1, len(Ranked_resumes['Scores'])+1)])\n",
      "\n",
      "###################################### SCORE TABLE PLOT ####################################\n",
      "\n",
      "fig1 = go.Figure(data=[go.Table(\n",
      "    header=dict(values=[\"Rank\", \"Name\", \"Scores\"],\n",
      "                fill_color='#00416d',\n",
      "                align='center', font=dict(color='white', size=16)),\n",
      "    cells=dict(values=[Ranked_resumes.Rank, Ranked_resumes.Name, Ranked_resumes.Scores],\n",
      "               fill_color='#d6e0f0',\n",
      "               align='left'))])\n",
      "\n",
      "fig1.update_layout(title=\"Top Ranked Resumes\", width=700, height=1100)\n",
      "st.write(fig1)\n",
      "\n",
      "st.markdown(\"---\")\n",
      "\n",
      "fig2 = px.bar(Ranked_resumes,\n",
      "              x=Ranked_resumes['Name'], y=Ranked_resumes['Scores'], color='Scores',\n",
      "              color_continuous_scale='haline', title=\"Score and Rank Distribution\")\n",
      "# fig.update_layout(width=700, height=700)\n",
      "st.write(fig2)\n",
      "\n",
      "\n",
      "st.markdown(\"---\")\n",
      "\n",
      "############################################ TF-IDF Code ###################################\n",
      "\n",
      "\n",
      "@st.cache()\n",
      "def get_list_of_words(document):\n",
      "    Document = []\n",
      "\n",
      "    for a in document:\n",
      "        raw = a.split(\" \")\n",
      "        Document.append(raw)\n",
      "\n",
      "    return Document\n",
      "\n",
      "\n",
      "document = get_list_of_words(Resumes['Cleaned'])\n",
      "\n",
      "id2word = corpora.Dictionary(document)\n",
      "corpus = [id2word.doc2bow(text) for text in document]\n",
      "\n",
      "\n",
      "lda_model = gensim.models.ldamodel.LdaModel(corpus=corpus, id2word=id2word, num_topics=6, random_state=100,\n",
      "                                            update_every=3, chunksize=100, passes=50, alpha='auto', per_word_topics=True)\n",
      "\n",
      "################################### LDA CODE ##############################################\n",
      "\n",
      "\n",
      "@st.cache  # Trying to improve performance by reducing the rerun computations\n",
      "def format_topics_sentences(ldamodel, corpus):\n",
      "    sent_topics_df = []\n",
      "    for i, row_list in enumerate(ldamodel[corpus]):\n",
      "        row = row_list[0] if ldamodel.per_word_topics else row_list\n",
      "        row = sorted(row, key=lambda x: (x[1]), reverse=True)\n",
      "        for j, (topic_num, prop_topic) in enumerate(row):\n",
      "            if j == 0:\n",
      "                wp = ldamodel.show_topic(topic_num)\n",
      "                topic_keywords = \", \".join([word for word, prop in wp])\n",
      "                sent_topics_df.append(\n",
      "                    [i, int(topic_num), round(prop_topic, 4)*100, topic_keywords])\n",
      "            else:\n",
      "                break\n",
      "\n",
      "    return sent_topics_df\n",
      "\n",
      "\n",
      "################################# Topic Word Cloud Code #####################################\n",
      "# st.sidebar.button('Hit Me')\n",
      "st.markdown(\"## Topics and Topic Related Keywords \")\n",
      "st.markdown(\n",
      "    \"\"\"This Wordcloud representation shows the Topic Number and the Top Keywords that contstitute a Topic.\n",
      "    This further is used to cluster the resumes.      \"\"\")\n",
      "\n",
      "cols = [color for name, color in mcolors.TABLEAU_COLORS.items()]\n",
      "\n",
      "cloud = WordCloud(background_color='white',\n",
      "                  width=2500,\n",
      "                  height=1800,\n",
      "                  max_words=10,\n",
      "                  colormap='tab10',\n",
      "                  collocations=False,\n",
      "                  color_func=lambda *args, **kwargs: cols[i],\n",
      "                  prefer_horizontal=1.0)\n",
      "\n",
      "topics = lda_model.show_topics(formatted=False)\n",
      "\n",
      "fig, axes = plt.subplots(2, 3, figsize=(10, 10), sharex=True, sharey=True)\n",
      "\n",
      "for i, ax in enumerate(axes.flatten()):\n",
      "    fig.add_subplot(ax)\n",
      "    topic_words = dict(topics[i][1])\n",
      "    cloud.generate_from_frequencies(topic_words, max_font_size=300)\n",
      "    plt.gca().imshow(cloud)\n",
      "    plt.gca().set_title('Topic ' + str(i), fontdict=dict(size=16))\n",
      "    plt.gca().axis('off')\n",
      "\n",
      "\n",
      "plt.subplots_adjust(wspace=0, hspace=0)\n",
      "plt.axis('off')\n",
      "plt.margins(x=0, y=0)\n",
      "plt.tight_layout()\n",
      "st.pyplot(plt)\n",
      "\n",
      "st.markdown(\"---\")\n",
      "\n",
      "####################### SETTING UP THE DATAFRAME FOR SUNBURST-GRAPH ############################\n",
      "\n",
      "df_topic_sents_keywords = format_topics_sentences(\n",
      "    ldamodel=lda_model, corpus=corpus)\n",
      "df_some = pd.DataFrame(df_topic_sents_keywords, columns=[\n",
      "                       'Document No', 'Dominant Topic', 'Topic % Contribution', 'Keywords'])\n",
      "df_some['Names'] = Resumes['Name']\n",
      "\n",
      "df = df_some\n",
      "\n",
      "st.markdown(\"## Topic Modelling of Resumes \")\n",
      "st.markdown(\n",
      "    \"Using LDA to divide the topics into a number of usefull topics and creating a Cluster of matching topic resumes.  \")\n",
      "fig3 = px.sunburst(df, path=['Dominant Topic', 'Names'], values='Topic % Contribution',\n",
      "                   color='Dominant Topic', color_continuous_scale='viridis', width=800, height=800, title=\"Topic Distribution Graph\")\n",
      "st.write(fig3)\n",
      "\n",
      "\n",
      "############################## RESUME PRINTING #############################\n",
      "\n",
      "option_2 = st.selectbox(\"Show the Best Matching Resumes?\", options=[\n",
      "    'YES', 'NO'])\n",
      "if option_2 == 'YES':\n",
      "    indx = st.slider(\"Which resume to display ?:\",\n",
      "                     1, Ranked_resumes.shape[0], 1)\n",
      "\n",
      "    st.write(\"Displaying Resume with Rank: \", indx)\n",
      "    st.markdown(\"---\")\n",
      "    st.markdown(\"## **Resume** \")\n",
      "    value = Ranked_resumes.iloc[indx-1, 2]\n",
      "    st.markdown(\"#### The Word Cloud For the Resume\")\n",
      "    wordcloud = WordCloud(width=800, height=800,\n",
      "                          background_color='white',\n",
      "                          colormap='viridis', collocations=False,\n",
      "                          min_font_size=10).generate(value)\n",
      "    plt.figure(figsize=(7, 7), facecolor=None)\n",
      "    plt.imshow(wordcloud)\n",
      "    plt.axis(\"off\")\n",
      "    plt.tight_layout(pad=0)\n",
      "    st.pyplot(plt)\n",
      "\n",
      "    st.write(\"With a Match Score of :\", Ranked_resumes.iloc[indx-1, 6])\n",
      "    fig = go.Figure(data=[go.Table(\n",
      "        header=dict(values=[\"Resume\"],\n",
      "                    fill_color='#f0a500',\n",
      "                    align='center', font=dict(color='white', size=16)),\n",
      "        cells=dict(values=[str(value)],\n",
      "                   fill_color='#f4f4f4',\n",
      "                   align='left'))])\n",
      "\n",
      "    fig.update_layout(width=800, height=1200)\n",
      "    st.write(fig)\n",
      "    # st.text(df_sorted.iloc[indx-1, 1])\n",
      "    st.markdown(\"---\")\n",
      "\n",
      "from scripts.parsers.ParseResumeToJson import ParseResume\n",
      "from scripts.parsers.ParseJobDescToJson import ParseJobDesc\n",
      "from scripts.ReadPdf import read_single_pdf\n",
      "import os.path\n",
      "import pathlib\n",
      "import json\n",
      "\n",
      "\n",
      "READ_DATA_FROM = 'Data/Raw/'\n",
      "SAVE_DIRECTORY = 'Data/Processed/'\n",
      "\n",
      "\n",
      "def read_resumes(input_file: str) -> dict:\n",
      "    input_file_name = os.path.join(READ_DATA_FROM+input_file)\n",
      "    data = read_single_pdf(input_file_name)\n",
      "    output = ParseResume(data).get_JSON()\n",
      "    return output\n",
      "\n",
      "\n",
      "def read_job_desc(input_file: str) -> dict:\n",
      "    input_file_name = os.path.join(READ_DATA_FROM + input_file)\n",
      "    data = read_single_pdf(input_file_name)\n",
      "    output = ParseJobDesc(data).get_JSON()\n",
      "    return output\n",
      "\n",
      "\n",
      "def write_json_file(resume_dictionary: dict):\n",
      "    file_name = str(\"Resume-\" + resume_dictionary[\"unique_id\"] + \".json\")\n",
      "    save_directory_name = pathlib.Path(SAVE_DIRECTORY) / file_name\n",
      "    json_object = json.dumps(resume_dictionary, sort_keys=True, indent=14)\n",
      "    with open(save_directory_name, \"w+\") as outfile:\n",
      "        outfile.write(json_object)\n",
      "\n",
      "import re\n",
      "import urllib.request\n",
      "import spacy\n",
      "from scripts.utils.Utils import TextCleaner\n",
      "\n",
      "\n",
      "# Load the English model\n",
      "nlp = spacy.load('en_core_web_sm')\n",
      "\n",
      "\n",
      "RESUME_SECTIONS = [\n",
      "    'Contact Information',\n",
      "    'Objective',\n",
      "    'Summary',\n",
      "    'Education',\n",
      "    'Experience',\n",
      "    'Skills',\n",
      "    'Projects',\n",
      "    'Certifications',\n",
      "    'Licenses',\n",
      "    'Awards',\n",
      "    'Honors',\n",
      "    'Publications',\n",
      "    'References',\n",
      "    'Technical Skills',\n",
      "    'Computer Skills',\n",
      "    'Programming Languages',\n",
      "    'Software Skills',\n",
      "    'Soft Skills',\n",
      "    'Language Skills',\n",
      "    'Professional Skills',\n",
      "    'Transferable Skills',\n",
      "    'Work Experience',\n",
      "    'Professional Experience',\n",
      "    'Employment History',\n",
      "    'Internship Experience',\n",
      "    'Volunteer Experience',\n",
      "    'Leadership Experience',\n",
      "    'Research Experience',\n",
      "    'Teaching Experience'\n",
      "]\n",
      "\n",
      "\n",
      "class DataExtractor:\n",
      "    \"\"\"\n",
      "    A class for extracting various types of data from text.\n",
      "    \"\"\"\n",
      "\n",
      "    def __init__(self, raw_text: str):\n",
      "        \"\"\"\n",
      "        Initialize the DataExtractor object.\n",
      "\n",
      "        Args:\n",
      "            raw_text (str): The raw input text.\n",
      "        \"\"\"\n",
      "\n",
      "        self.text = raw_text\n",
      "        self.clean_text = TextCleaner.clean_text(self.text)\n",
      "        self.doc = nlp(self.clean_text)\n",
      "\n",
      "    def extract_links(self):\n",
      "        \"\"\"\n",
      "        Find links of any type in a given string.\n",
      "\n",
      "        Args:\n",
      "            text (str): The string to search for links.\n",
      "\n",
      "        Returns:\n",
      "            list: A list containing all the found links.\n",
      "        \"\"\"\n",
      "        link_pattern = r'\\b(?:https?://|www\\.)\\S+\\b'\n",
      "        links = re.findall(link_pattern, self.text)\n",
      "        return links\n",
      "\n",
      "    def extract_links_extended(self):\n",
      "        \"\"\"\n",
      "        Extract links of all kinds (HTTP, HTTPS, FTP, email, www.linkedin.com,\n",
      "          and github.com/user_name) from a webpage.\n",
      "\n",
      "        Args:\n",
      "            url (str): The URL of the webpage.\n",
      "\n",
      "        Returns:\n",
      "            list: A list containing all the extracted links.\n",
      "        \"\"\"\n",
      "        links = []\n",
      "        try:\n",
      "            response = urllib.request.urlopen(self.text)\n",
      "            html_content = response.read().decode('utf-8')\n",
      "            pattern = r'href=[\\'\"]?([^\\'\" >]+)'\n",
      "            raw_links = re.findall(pattern, html_content)\n",
      "            for link in raw_links:\n",
      "                if link.startswith(('http://', 'https://', 'ftp://', 'mailto:',\n",
      "                                    'www.linkedin.com', 'github.com/', 'twitter.com')):\n",
      "                    links.append(link)\n",
      "        except Exception as e:\n",
      "            print(f\"Error extracting links: {str(e)}\")\n",
      "        return links\n",
      "\n",
      "    def extract_names(self):\n",
      "        \"\"\"Extracts and returns a list of names from the given \n",
      "        text using spaCy's named entity recognition.\n",
      "\n",
      "        Args:\n",
      "            text (str): The text to extract names from.\n",
      "\n",
      "        Returns:\n",
      "            list: A list of strings representing the names extracted from the text.\n",
      "        \"\"\"\n",
      "        names = [ent.text for ent in self.doc.ents if ent.label_ == 'PERSON']\n",
      "        return names\n",
      "\n",
      "    def extract_emails(self):\n",
      "        \"\"\"\n",
      "        Extract email addresses from a given string.\n",
      "\n",
      "        Args:\n",
      "            text (str): The string from which to extract email addresses.\n",
      "\n",
      "        Returns:\n",
      "            list: A list containing all the extracted email addresses.\n",
      "        \"\"\"\n",
      "        email_pattern = r'\\b[A-Za-z0-9._%+-]+@[A-Za-z0-9.-]+\\.[A-Za-z]{2,}\\b'\n",
      "        emails = re.findall(email_pattern, self.text)\n",
      "        return emails\n",
      "\n",
      "    def extract_phone_numbers(self):\n",
      "        \"\"\"\n",
      "        Extract phone numbers from a given string.\n",
      "\n",
      "        Args:\n",
      "            text (str): The string from which to extract phone numbers.\n",
      "\n",
      "        Returns:\n",
      "            list: A list containing all the extracted phone numbers.\n",
      "        \"\"\"\n",
      "        phone_number_pattern = r'^(\\+\\d{1,3})?[-.\\s]?\\(?\\d{3}\\)?[-.\\s]?\\d{3}[-.\\s]?\\d{4}$'\n",
      "        phone_numbers = re.findall(phone_number_pattern, self.text)\n",
      "        return phone_numbers\n",
      "\n",
      "    def extract_experience(self):\n",
      "        \"\"\"\n",
      "        Extract experience from a given string. It does so by using the Spacy module.\n",
      "\n",
      "        Args:\n",
      "            text (str): The string from which to extract experience.\n",
      "\n",
      "        Returns:\n",
      "            str: A string containing all the extracted experience.\n",
      "        \"\"\"\n",
      "        experience_section = []\n",
      "        in_experience_section = False\n",
      "\n",
      "        for token in self.doc:\n",
      "            if token.text in RESUME_SECTIONS:\n",
      "                if token.text == 'Experience' or 'EXPERIENCE' or 'experience':\n",
      "                    in_experience_section = True\n",
      "                else:\n",
      "                    in_experience_section = False\n",
      "\n",
      "            if in_experience_section:\n",
      "                experience_section.append(token.text)\n",
      "\n",
      "        return ' '.join(experience_section)\n",
      "\n",
      "    def extract_position_year(self):\n",
      "        \"\"\"\n",
      "            Extract position and year from a given string.\n",
      "\n",
      "            Args:\n",
      "                text (str): The string from which to extract position and year.\n",
      "\n",
      "            Returns:\n",
      "                list: A list containing the extracted position and year.\n",
      "        \"\"\"\n",
      "        position_year_search_pattern = r\"(\\b\\w+\\b\\s+\\b\\w+\\b),\\s+(\\d{4})\\s*-\\s*(\\d{4}|\\bpresent\\b)\"\n",
      "        position_year = re.findall(\n",
      "            position_year_search_pattern, self.text)\n",
      "        return position_year\n",
      "\n",
      "    def extract_particular_words(self):\n",
      "        \"\"\"\n",
      "        Extract nouns and proper nouns from the given text.\n",
      "\n",
      "        Args:\n",
      "            text (str): The input text to extract nouns from.\n",
      "\n",
      "        Returns:\n",
      "            list: A list of extracted nouns.\n",
      "        \"\"\"\n",
      "        pos_tags = ['NOUN', 'PROPN', 'VERB', 'ADJ']\n",
      "        nouns = [token.text for token in self.doc if token.pos_ in pos_tags]\n",
      "        return nouns\n",
      "\n",
      "    def extract_entities(self):\n",
      "        \"\"\"\n",
      "        Extract named entities of types 'GPE' (geopolitical entity) and 'ORG' (organization) from the given text.\n",
      "\n",
      "        Args:\n",
      "            text (str): The input text to extract entities from.\n",
      "\n",
      "        Returns:\n",
      "            list: A list of extracted entities.\n",
      "        \"\"\"\n",
      "        entity_labels = ['GPE', 'ORG']\n",
      "        entities = [\n",
      "            token.text for token in self.doc.ents if token.label_ in entity_labels]\n",
      "        return entities\n",
      "\n",
      "import textacy\n",
      "from textacy import extract\n",
      "\n",
      "\n",
      "class KeytermExtractor:\n",
      "    \"\"\"\n",
      "    A class for extracting keyterms from a given text using various algorithms.\n",
      "    \"\"\"\n",
      "\n",
      "    def __init__(self, raw_text: str, top_n_values: int = 20):\n",
      "        \"\"\"\n",
      "        Initialize the KeytermExtractor object.\n",
      "\n",
      "        Args:\n",
      "            raw_text (str): The raw input text.\n",
      "            top_n_values (int): The number of top keyterms to extract.\n",
      "        \"\"\"\n",
      "        self.raw_text = raw_text\n",
      "        self.text_doc = textacy.make_spacy_doc(\n",
      "            self.raw_text, lang=\"en_core_web_md\")\n",
      "        self.top_n_values = top_n_values\n",
      "\n",
      "    def get_keyterms_based_on_textrank(self):\n",
      "        \"\"\"\n",
      "        Extract keyterms using the TextRank algorithm.\n",
      "\n",
      "        Returns:\n",
      "            List[str]: A list of top keyterms based on TextRank.\n",
      "        \"\"\"\n",
      "        return list(extract.keyterms.textrank(self.text_doc, normalize=\"lemma\",\n",
      "                                              topn=self.top_n_values))\n",
      "\n",
      "    def get_keyterms_based_on_sgrank(self):\n",
      "        \"\"\"\n",
      "        Extract keyterms using the SGRank algorithm.\n",
      "\n",
      "        Returns:\n",
      "            List[str]: A list of top keyterms based on SGRank.\n",
      "        \"\"\"\n",
      "        return list(extract.keyterms.sgrank(self.text_doc, normalize=\"lemma\",\n",
      "                                            topn=self.top_n_values))\n",
      "\n",
      "    def get_keyterms_based_on_scake(self):\n",
      "        \"\"\"\n",
      "        Extract keyterms using the sCAKE algorithm.\n",
      "\n",
      "        Returns:\n",
      "            List[str]: A list of top keyterms based on sCAKE.\n",
      "        \"\"\"\n",
      "        return list(extract.keyterms.scake(self.text_doc, normalize=\"lemma\",\n",
      "                                           topn=self.top_n_values))\n",
      "\n",
      "    def get_keyterms_based_on_yake(self):\n",
      "        \"\"\"\n",
      "        Extract keyterms using the YAKE algorithm.\n",
      "\n",
      "        Returns:\n",
      "            List[str]: A list of top keyterms based on YAKE.\n",
      "        \"\"\"\n",
      "        return list(extract.keyterms.yake(self.text_doc, normalize=\"lemma\",\n",
      "                                          topn=self.top_n_values))\n",
      "\n",
      "    def bi_gramchunker(self):\n",
      "        \"\"\"\n",
      "        Chunk the text into bigrams.\n",
      "\n",
      "        Returns:\n",
      "            List[str]: A list of bigrams.\n",
      "        \"\"\"\n",
      "        return list(textacy.extract.basics.ngrams(self.text_doc, n=2, filter_stops=True,\n",
      "                                                  filter_nums=True, filter_punct=True))\n",
      "\n",
      "    def tri_gramchunker(self):\n",
      "        \"\"\"\n",
      "        Chunk the text into trigrams.\n",
      "\n",
      "        Returns:\n",
      "            List[str]: A list of trigrams.\n",
      "        \"\"\"\n",
      "        return list(textacy.extract.basics.ngrams(self.text_doc, n=3, filter_stops=True,\n",
      "                                                  filter_nums=True, filter_punct=True))\n",
      "\n",
      "import os\n",
      "import glob\n",
      "from pypdf import PdfReader\n",
      "\n",
      "\n",
      "def read_multiple_pdf(file_path: str) -> list:\n",
      "    \"\"\"\n",
      "    Read multiple PDF files from the specified file path and extract the text from each page.\n",
      "\n",
      "    Args:\n",
      "        file_path (str): The directory path containing the PDF files.\n",
      "\n",
      "    Returns:\n",
      "        list: A list containing the extracted text from each page of the PDF files.\n",
      "    \"\"\"\n",
      "    pdf_files = get_pdf_files(file_path)\n",
      "    output = []\n",
      "    for file in pdf_files:\n",
      "        try:\n",
      "            with open(file, 'rb') as f:\n",
      "                pdf_reader = PdfReader(f)\n",
      "                count = len(pdf_reader.pages)\n",
      "                for i in range(count):\n",
      "                    page = pdf_reader.pages[i]\n",
      "                    output.append(page.extract_text())\n",
      "        except Exception as e:\n",
      "            print(f\"Error reading file '{file}': {str(e)}\")\n",
      "    return output\n",
      "\n",
      "\n",
      "def read_single_pdf(file_path: str) -> str:\n",
      "    \"\"\"\n",
      "    Read a single PDF file and extract the text from each page.\n",
      "\n",
      "    Args:\n",
      "        file_path (str): The path of the PDF file.\n",
      "\n",
      "    Returns:\n",
      "        list: A list containing the extracted text from each page of the PDF file.\n",
      "    \"\"\"\n",
      "    output = []\n",
      "    try:\n",
      "        with open(file_path, 'rb') as f:\n",
      "            pdf_reader = PdfReader(f)\n",
      "            count = len(pdf_reader.pages)\n",
      "            for i in range(count):\n",
      "                page = pdf_reader.pages[i]\n",
      "                output.append(page.extract_text())\n",
      "    except Exception as e:\n",
      "        print(f\"Error reading file '{file_path}': {str(e)}\")\n",
      "    return str(\" \".join(output))\n",
      "\n",
      "\n",
      "def get_pdf_files(file_path: str) -> list:\n",
      "    \"\"\"\n",
      "    Get a list of PDF files from the specified directory path.\n",
      "\n",
      "    Args:\n",
      "        file_path (str): The directory path containing the PDF files.\n",
      "\n",
      "    Returns:\n",
      "        list: A list of PDF file paths.\n",
      "    \"\"\"\n",
      "    pdf_files = []\n",
      "    try:\n",
      "        pdf_files = glob.glob(os.path.join(file_path, '*.pdf'))\n",
      "    except Exception as e:\n",
      "        print(f\"Error getting PDF files from '{file_path}': {str(e)}\")\n",
      "    return pdf_files\n",
      "\n",
      "import nltk\n",
      "from nltk.corpus import stopwords\n",
      "from nltk.tokenize import word_tokenize\n",
      "from nltk.stem import WordNetLemmatizer\n",
      "import string\n",
      "\n",
      "\n",
      "class TextCleaner:\n",
      "\n",
      "    def __init__(self, raw_text):\n",
      "        self.stopwords_set = set(stopwords.words('english') + list(string.punctuation))\n",
      "        self.lemmatizer = WordNetLemmatizer()\n",
      "        self.raw_input_text = raw_text\n",
      "\n",
      "    def clean_text(self) -> str:\n",
      "        tokens = word_tokenize(self.raw_input_text.lower())\n",
      "        tokens = [token for token in tokens if token not in self.stopwords_set]\n",
      "        tokens = [self.lemmatizer.lemmatize(token) for token in tokens]\n",
      "        cleaned_text = ' '.join(tokens)\n",
      "        return cleaned_text\n",
      "\n",
      "import json\n",
      "import pathlib\n",
      "from scripts.Extractor import DataExtractor\n",
      "from scripts.utils.Utils import TextCleaner, CountFrequency, generate_unique_id\n",
      "from scripts.KeytermsExtraction import KeytermExtractor\n",
      "import os\n",
      "\n",
      "SAVE_DIRECTORY = \"Data/Processed/\"\n",
      "\n",
      "\n",
      "class ParseJobDesc:\n",
      "\n",
      "    def __init__(self, job_desc: str):\n",
      "        self.job_desc_data = job_desc\n",
      "        self.clean_data = TextCleaner.clean_text(\n",
      "            self.job_desc_data)\n",
      "        self.entities = DataExtractor(self.clean_data).extract_entities()\n",
      "        self.key_words = DataExtractor(\n",
      "            self.clean_data).extract_particular_words()\n",
      "        self.pos_frequencies = CountFrequency(\n",
      "            self.clean_data).count_frequency()\n",
      "        self.keyterms = KeytermExtractor(\n",
      "            self.clean_data).get_keyterms_based_on_sgrank()\n",
      "        self.bi_grams = KeytermExtractor(self.clean_data).bi_gramchunker()\n",
      "        self.tri_grams = KeytermExtractor(self.clean_data).tri_gramchunker()\n",
      "\n",
      "    def get_JSON(self) -> dict:\n",
      "        \"\"\"\n",
      "        Returns a dictionary of job description data.\n",
      "        \"\"\"\n",
      "        job_desc_dictionary = {\n",
      "            \"unique_id\": generate_unique_id(),\n",
      "            \"job_desc_data\": self.job_desc_data,\n",
      "            \"clean_data\": self.clean_data,\n",
      "            \"entities\": self.entities,\n",
      "            \"extracted_keywords\": self.key_words,\n",
      "            \"keyterms\": self.keyterms,\n",
      "            \"bi_grams\": str(self.bi_grams),\n",
      "            \"tri_grams\": str(self.tri_grams),\n",
      "            \"pos_frequencies\": self.pos_frequencies\n",
      "        }\n",
      "\n",
      "        return job_desc_dictionary\n",
      "\n",
      "import json\n",
      "from scripts.Extractor import DataExtractor\n",
      "from scripts.utils.Utils import TextCleaner, CountFrequency, generate_unique_id\n",
      "from scripts.KeytermsExtraction import KeytermExtractor\n",
      "import os.path\n",
      "import os\n",
      "import pathlib\n",
      "\n",
      "SAVE_DIRECTORY = '../../Data/Processed/'\n",
      "\n",
      "\n",
      "class ParseResume:\n",
      "\n",
      "    def __init__(self, resume: str):\n",
      "        self.resume_data = resume\n",
      "        self.clean_data = TextCleaner.clean_text(\n",
      "            self.resume_data)\n",
      "        self.entities = DataExtractor(self.clean_data).extract_entities()\n",
      "        self.name = DataExtractor(self.clean_data[:30]).extract_names()\n",
      "        self.experience = DataExtractor(self.clean_data).extract_experience()\n",
      "        self.emails = DataExtractor(self.resume_data).extract_emails()\n",
      "        self.phones = DataExtractor(self.resume_data).extract_phone_numbers()\n",
      "        self.years = DataExtractor(self.clean_data).extract_position_year()\n",
      "        self.key_words = DataExtractor(\n",
      "            self.clean_data).extract_particular_words()\n",
      "        self.pos_frequencies = CountFrequency(\n",
      "            self.clean_data).count_frequency()\n",
      "        self.keyterms = KeytermExtractor(\n",
      "            self.clean_data).get_keyterms_based_on_sgrank()\n",
      "        self.bi_grams = KeytermExtractor(self.clean_data).bi_gramchunker()\n",
      "        self.tri_grams = KeytermExtractor(self.clean_data).tri_gramchunker()\n",
      "\n",
      "    def get_JSON(self) -> dict:\n",
      "        \"\"\"\n",
      "        Returns a dictionary of resume data.\n",
      "        \"\"\"\n",
      "        resume_dictionary = {\n",
      "            \"unique_id\": generate_unique_id(),\n",
      "            \"resume_data\": self.resume_data,\n",
      "            \"clean_data\": self.clean_data,\n",
      "            \"entities\": self.entities,\n",
      "            \"extracted_keywords\": self.key_words,\n",
      "            \"keyterms\": self.keyterms,\n",
      "            \"name\": self.name,\n",
      "            \"experience\": self.experience,\n",
      "            \"emails\": self.emails,\n",
      "            \"phones\": self.phones,\n",
      "            \"years\": self.years,\n",
      "            \"bi_grams\": str(self.bi_grams),\n",
      "            \"tri_grams\": str(self.tri_grams),\n",
      "            \"pos_frequencies\": self.pos_frequencies\n",
      "        }\n",
      "\n",
      "        return resume_dictionary\n",
      "\n",
      "File scripts/utils/Similar.py has no source code\n",
      "\n",
      "from uuid import uuid4\n",
      "import re\n",
      "import spacy\n",
      "\n",
      "# Load the English model\n",
      "nlp = spacy.load('en_core_web_md')\n",
      "\n",
      "REGEX_PATTERNS = {\n",
      "    'email_pattern': r'\\b[A-Za-z0-9._%+-]+@[A-Za-z0-9.-]+\\.[A-Za-z]{2,}\\b',\n",
      "    'phone_pattern': r\"\\(?\\d{3}\\)?[-.\\s]?\\d{3}[-.\\s]?\\d{4}\",\n",
      "    'link_pattern': r'\\b(?:https?://|www\\.)\\S+\\b'\n",
      "}\n",
      "\n",
      "\n",
      "def generate_unique_id():\n",
      "    \"\"\"\n",
      "    Generate a unique ID and return it as a string.\n",
      "\n",
      "    Returns:\n",
      "        str: A string with a unique ID.\n",
      "    \"\"\"\n",
      "    return str(uuid4())\n",
      "\n",
      "\n",
      "class TextCleaner:\n",
      "    \"\"\"\n",
      "    A class for cleaning a text by removing specific patterns.\n",
      "    \"\"\"\n",
      "\n",
      "    def remove_emails_links(text):\n",
      "        \"\"\"\n",
      "        Clean the input text by removing specific patterns.\n",
      "\n",
      "        Args:\n",
      "            text (str): The input text to clean.\n",
      "\n",
      "        Returns:\n",
      "            str: The cleaned text.\n",
      "        \"\"\"\n",
      "        for pattern in REGEX_PATTERNS:\n",
      "            text = re.sub(REGEX_PATTERNS[pattern], '', text)\n",
      "        return text\n",
      "\n",
      "    def clean_text(text):\n",
      "        \"\"\"\n",
      "        Clean the input text by removing specific patterns.\n",
      "\n",
      "        Args:\n",
      "            text (str): The input text to clean.\n",
      "\n",
      "        Returns:\n",
      "            str: The cleaned text.\n",
      "        \"\"\"\n",
      "        text = TextCleaner.remove_emails_links(text)\n",
      "        doc = nlp(text)\n",
      "        for token in doc:\n",
      "            if token.pos_ == 'PUNCT':\n",
      "                text = text.replace(token.text, '')\n",
      "        return str(text)\n",
      "\n",
      "    def remove_stopwords(text):\n",
      "        \"\"\"\n",
      "        Clean the input text by removing stopwords.\n",
      "\n",
      "        Args:\n",
      "            text (str): The input text to clean.\n",
      "\n",
      "        Returns:\n",
      "            str: The cleaned text.\n",
      "        \"\"\"\n",
      "        doc = nlp(text)\n",
      "        for token in doc:\n",
      "            if token.is_stop:\n",
      "                text = text.replace(token.text, '')\n",
      "        return text\n",
      "\n",
      "\n",
      "class CountFrequency:\n",
      "\n",
      "    def __init__(self, text):\n",
      "        self.text = text\n",
      "        self.doc = nlp(text)\n",
      "\n",
      "    def count_frequency(self):\n",
      "        \"\"\"\n",
      "        Count the frequency of words in the input text.\n",
      "\n",
      "        Returns:\n",
      "            dict: A dictionary with the words as keys and the frequency as values.\n",
      "        \"\"\"\n",
      "        pos_freq = {}\n",
      "        for token in self.doc:\n",
      "            if token.pos_ in pos_freq:\n",
      "                pos_freq[token.pos_] += 1\n",
      "            else:\n",
      "                pos_freq[token.pos_] = 1\n",
      "        return pos_freq\n",
      "\n",
      "File scripts/utils/tf_idf.py has no source code\n",
      "\n",
      "import string\n",
      "import spacy\n",
      "import pywaffle\n",
      "import streamlit as st\n",
      "import pandas as pd\n",
      "import json\n",
      "import plotly.express as px\n",
      "import plotly.graph_objects as go\n",
      "import matplotlib.pyplot as plt\n",
      "import squarify\n",
      "\n",
      "st.title('Resume :blue[Ranker]')\n",
      "st.subheader('_AI Based Resume Analyzer & Ranker_')\n",
      "\n",
      "\n",
      "def read_json(filename):\n",
      "    with open(filename) as f:\n",
      "        data = json.load(f)\n",
      "    return data\n",
      "\n",
      "\n",
      "def list_to_matrix(list_to_convert, num_columns):\n",
      "    \"\"\"Converts a list to a matrix of a suitable size.\n",
      "\n",
      "    Args:\n",
      "      list_to_convert: The list to convert.\n",
      "      num_columns: The number of columns in the matrix.\n",
      "\n",
      "    Returns:\n",
      "      A matrix of the specified size, with the contents of the list.\n",
      "    \"\"\"\n",
      "\n",
      "    matrix = []\n",
      "    for i in range(len(list_to_convert) // num_columns):\n",
      "        matrix.append(list_to_convert[i * num_columns:(i + 1) * num_columns])\n",
      "\n",
      "    if len(list_to_convert) % num_columns > 0:\n",
      "        matrix.append(list_to_convert[-(len(list_to_convert) % num_columns):])\n",
      "\n",
      "    for i in range(len(matrix)):\n",
      "        for j in range(len(matrix[i])):\n",
      "            if matrix[i][j] is None:\n",
      "                matrix[i][j] = \"\"\n",
      "\n",
      "    return matrix\n",
      "\n",
      "\n",
      "def split_list(list_to_split, chunk_size):\n",
      "    \"\"\"Splits a list into 3 equal lists.\n",
      "\n",
      "    Args:\n",
      "      list_to_split: The list to split.\n",
      "      chunk_size: The size of each chunk.\n",
      "\n",
      "    Returns:\n",
      "      A list of chunk_size (+1 if over) lists, each of which is a chunk of the original list.\n",
      "    \"\"\"\n",
      "\n",
      "    num_chunks = len(list_to_split) // chunk_size\n",
      "    remainder = len(list_to_split) % chunk_size\n",
      "\n",
      "    chunks = []\n",
      "    for i in range(num_chunks):\n",
      "        chunks.append(list_to_split[i * chunk_size:(i + 1) * chunk_size])\n",
      "\n",
      "    if remainder > 0:\n",
      "        chunks.append(list_to_split[num_chunks * chunk_size:])\n",
      "\n",
      "    return chunks\n",
      "\n",
      "\n",
      "def dirty_intersection(list1, list2):\n",
      "    intersection = list(set(list1) & set(list2))\n",
      "    remainder_1 = [x for x in list1 if x not in intersection]\n",
      "    remainder_2 = [x for x in list2 if x not in intersection]\n",
      "\n",
      "    output = pd.DataFrame({\n",
      "        'elements': [\"Common words\", \"Words unique to Resume\", \"Words unique to Job Description\"],\n",
      "        'values': [len(intersection), len(remainder_1), len(remainder_2)]\n",
      "    }, index=[1, 2, 3])\n",
      "\n",
      "    return output\n",
      "\n",
      "\n",
      "def find_intersection_of_lists(list1, list2):\n",
      "    \"\"\"Finds the intersection of two lists and returns the result as a Pandas dataframe.\n",
      "\n",
      "    Args:\n",
      "      list1: The first list.\n",
      "      list2: The second list.\n",
      "\n",
      "    Returns:\n",
      "      A Pandas dataframe with three columns: `intersection`, `remainder_1`, and `remainder_2`.\n",
      "    \"\"\"\n",
      "\n",
      "    def max_of_three(a, b, c):\n",
      "        max_value = a\n",
      "        if b > max_value:\n",
      "            max_value = b\n",
      "        if c > max_value:\n",
      "            max_value = c\n",
      "\n",
      "        return max_value\n",
      "\n",
      "    def fill_by_complements(num: int, list_to_fill: list):\n",
      "        if (num > len(list_to_fill)):\n",
      "            for i in range(num-len(list_to_fill)):\n",
      "                list_to_fill.append(\" \")\n",
      "\n",
      "    intersection = list(set(list1) & set(list2))\n",
      "    remainder_1 = [x for x in list1 if x not in intersection]\n",
      "    remainder_2 = [x for x in list2 if x not in intersection]\n",
      "\n",
      "    max_count = max_of_three(\n",
      "        len(intersection), len(remainder_1), len(remainder_2))\n",
      "\n",
      "    fill_by_complements(max_count, intersection)\n",
      "    fill_by_complements(max_count, remainder_1)\n",
      "    fill_by_complements(max_count, remainder_2)\n",
      "\n",
      "    df = pd.DataFrame({\n",
      "        'intersection': intersection,\n",
      "        'remainder_1': remainder_1,\n",
      "        'remainder_2': remainder_2\n",
      "    })\n",
      "\n",
      "    return df\n",
      "\n",
      "\n",
      "def preprocess_text(text):\n",
      "    \"\"\"Preprocesses text using spacy.\n",
      "\n",
      "    Args:\n",
      "      text: The text to preprocess.\n",
      "\n",
      "    Returns:\n",
      "      A list of string tokens.\n",
      "    \"\"\"\n",
      "\n",
      "    nlp = spacy.load(\"en_core_web_sm\")\n",
      "    doc = nlp(text)\n",
      "\n",
      "    # Lemmatize words.\n",
      "    tokens = [token.lemma_ for token in doc]\n",
      "\n",
      "    # Remove stopwords.\n",
      "    stopwords = spacy.lang.en.stop_words.STOP_WORDS\n",
      "    tokens = [token for token in tokens if token not in stopwords]\n",
      "\n",
      "    # Remove punctuation.\n",
      "    punctuation = set(string.punctuation)\n",
      "    tokens = [token for token in tokens if token not in punctuation]\n",
      "\n",
      "    return tokens\n",
      "\n",
      "\n",
      "# read the json file\n",
      "resume = read_json('resume.json')\n",
      "job_desc = read_json('Data/Processed/Job-Desc-a4f06ccb-8d5a-4d0b-9f02-3ba6d686472e.json')\n",
      "st.json(resume)\n",
      "\n",
      "st.json(job_desc)\n",
      "\n",
      "st.write(\"### Reading Resume's POS\")\n",
      "df = pd.DataFrame(resume['pos_frequencies'], index=[0])\n",
      "st.write(df)\n",
      "\n",
      "fig = go.Figure(data=go.Bar(y=list(resume['pos_frequencies'].values()), x=list(resume['pos_frequencies'].keys())),\n",
      "                layout_title_text=\"Resume's POS\")\n",
      "st.write(fig)\n",
      "\n",
      "df2 = pd.DataFrame(resume['keyterms'], columns=[\"keyword\", \"value\"])\n",
      "st.dataframe(df2)\n",
      "\n",
      "# Create the dictionary\n",
      "keyword_dict = {}\n",
      "for keyword, value in resume['keyterms']:\n",
      "    keyword_dict[keyword] = value\n",
      "\n",
      "fig = go.Figure(data=[go.Table(header=dict(values=[\"Keyword\", \"Value\"],\n",
      "                                           font=dict(size=12),\n",
      "                                           fill_color='#070A52'),\n",
      "                               cells=dict(values=[list(keyword_dict.keys()),\n",
      "                                                  list(keyword_dict.values())],\n",
      "                                          line_color='darkslategray',\n",
      "                                          fill_color='#6DA9E4'))\n",
      "                      ])\n",
      "st.plotly_chart(fig)\n",
      "\n",
      "st.divider()\n",
      "\n",
      "for keyword, value in resume['keyterms']:\n",
      "    pass\n",
      "\n",
      "\n",
      "# display the waffle chart\n",
      "figure = plt.figure(\n",
      "    FigureClass=pywaffle.Waffle,\n",
      "    rows=20,\n",
      "    columns=20,\n",
      "    values=keyword_dict,\n",
      "    legend={'loc': 'upper left', 'bbox_to_anchor': (1, 1)})\n",
      "\n",
      "\n",
      "# Display the dictionary\n",
      "\n",
      "st.pyplot(fig=figure)\n",
      "# st.write(dict)\n",
      "\n",
      "fig = px.treemap(df2, path=['keyword'], values='value',\n",
      "                 color_continuous_scale='RdBu',\n",
      "                 title='Resume POS')\n",
      "st.write(fig)\n",
      "\n",
      "st.text(resume['resume_data'])\n",
      "\n",
      "\n",
      "fig = go.Figure(data=[go.Table(\n",
      "    header=dict(values=[\"Extracted Keywords\"],\n",
      "                fill_color='#1D267D',\n",
      "                align='center', font=dict(color='white', size=16)),\n",
      "    cells=dict(values=[out for out in split_list(resume['extracted_keywords'], 25)],\n",
      "               fill_color='#19A7CE',\n",
      "               align='left'))])\n",
      "\n",
      "fig.update_layout(\n",
      "    uniformtext_minsize=13\n",
      ")\n",
      "\n",
      "st.plotly_chart(figure_or_data=fig)\n",
      "\n",
      "fig = go.Figure(data=[go.Table(\n",
      "    header=dict(values=[\"Tri Grams\"],\n",
      "                fill_color='#1D267D',\n",
      "                align='center', font=dict(color='white', size=16)),\n",
      "    cells=dict(values=[resume['tri_grams']],\n",
      "               fill_color='#19A7CE',\n",
      "               align='left'))])\n",
      "\n",
      "st.plotly_chart(figure_or_data=fig)\n",
      "\n",
      "fig = go.Figure(data=[go.Table(\n",
      "    header=dict(values=[\"Bi Grams\"],\n",
      "                fill_color='#1D267D',\n",
      "                align='center', font=dict(color='white', size=16)),\n",
      "    cells=dict(values=[resume['bi_grams']],\n",
      "               fill_color='#19A7CE',\n",
      "               align='left'))])\n",
      "\n",
      "st.plotly_chart(figure_or_data=fig)\n",
      "\n",
      "resume_list = preprocess_text(resume['resume_data'])\n",
      "\n",
      "job_desc_list = preprocess_text(job_desc['job_desc_data'])\n",
      "\n",
      "df_data = find_intersection_of_lists(resume_list, job_desc_list)\n",
      "\n",
      "data_length = dirty_intersection(resume_list, job_desc_list)\n",
      "\n",
      "# data_length = data_length\n",
      "\n",
      "st.write(df_data)\n",
      "\n",
      "st.write(data_length)\n",
      "\n",
      "st.write(px.data.tips())\n",
      "\n",
      "fig = px.pie(data_length, values='values', names='elements')\n",
      "st.write(fig)\n",
      "\n",
      "import os\n",
      "import glob\n",
      "from pypdf import PdfReader\n",
      "\n",
      "\n",
      "def get_pdf_files(file_path):\n",
      "    \"\"\"\n",
      "    Get all PDF files from the specified file path.\n",
      "\n",
      "    Args:\n",
      "        file_path (str): The directory path containing the PDF files.\n",
      "\n",
      "    Returns:\n",
      "        list: A list containing the paths of all the PDF files in the directory.\n",
      "    \"\"\"\n",
      "    if os.path.exists(file_path):\n",
      "        return glob.glob(os.path.join(file_path, '*.pdf'))\n",
      "    else:\n",
      "        return []\n",
      "\n",
      "\n",
      "def read_multiple_pdf(file_path: str) -> list:\n",
      "    \"\"\"\n",
      "    Read multiple PDF files from the specified file path and extract the text from each page.\n",
      "\n",
      "    Args:\n",
      "        file_path (str): The directory path containing the PDF files.\n",
      "\n",
      "    Returns:\n",
      "        list: A list containing the extracted text from each page of the PDF files.\n",
      "    \"\"\"\n",
      "    pdf_files = get_pdf_files(file_path)\n",
      "    output = []\n",
      "    for file in pdf_files:\n",
      "        try:\n",
      "            with open(file, 'rb') as f:\n",
      "                pdf_reader = PdfReader(f)\n",
      "                count = pdf_reader.getNumPages()\n",
      "                for i in range(count):\n",
      "                    page = pdf_reader.getPage(i)\n",
      "                    output.append(page.extractText())\n",
      "        except Exception as e:\n",
      "            print(f\"Error reading file '{file}': {str(e)}\")\n",
      "    return output\n",
      "\n",
      "\n",
      "def read_single_pdf(file_path: str) -> str:\n",
      "    \"\"\"\n",
      "    Read a single PDF file and extract the text from each page.\n",
      "\n",
      "    Args:\n",
      "        file_path (str): The path of the PDF file.\n",
      "\n",
      "    Returns:\n",
      "        list: A list containing the extracted text from each page of the PDF file.\n",
      "    \"\"\"\n",
      "    output = []\n",
      "    try:\n",
      "        with open(file_path, 'rb') as f:\n",
      "            pdf_reader = PdfReader(f)\n",
      "            count = len(pdf_reader.pages)\n",
      "            for i in range(count):\n",
      "                page = pdf_reader.pages[i]\n",
      "                output.append(page.extract_text())\n",
      "    except Exception as e:\n",
      "        print(f\"Error reading file '{file_path}': {str(e)}\")\n",
      "    return str(\" \".join(output))\n",
      "\n",
      "\n",
      "def get_pdf_files(file_path: str) -> list:\n",
      "    \"\"\"\n",
      "    Get a list of PDF files from the specified directory path.\n",
      "\n",
      "    Args:\n",
      "        file_path (str): The directory path containing the PDF files.\n",
      "\n",
      "    Returns:\n",
      "        list: A list of PDF file paths.\n",
      "    \"\"\"\n",
      "    pdf_files = []\n",
      "    try:\n",
      "        pdf_files = glob.glob(os.path.join(file_path, '*.pdf'))\n",
      "    except Exception as e:\n",
      "        print(f\"Error getting PDF files from '{file_path}': {str(e)}\")\n",
      "    return pdf_files\n",
      "\n",
      "import json\n",
      "import pathlib\n",
      "from scripts.Extractor import DataExtractor\n",
      "from scripts.utils.Utils import TextCleaner, CountFrequency, generate_unique_id\n",
      "from scripts.KeytermsExtraction import KeytermExtractor\n",
      "import os\n",
      "\n",
      "SAVE_DIRECTORY = \"../../Data/Processed/\"\n",
      "\n",
      "\n",
      "class ParseJobDesc:\n",
      "\n",
      "    def __init__(self, job_desc: str):\n",
      "        self.job_desc_data = job_desc\n",
      "        self.clean_data = TextCleaner.clean_text(\n",
      "            self.job_desc_data)\n",
      "        self.entities = DataExtractor(self.clean_data).extract_entities()\n",
      "        self.key_words = DataExtractor(\n",
      "            self.clean_data).extract_particular_words()\n",
      "        self.pos_frequencies = CountFrequency(\n",
      "            self.clean_data).count_frequency()\n",
      "        self.keyterms = KeytermExtractor(\n",
      "            self.clean_data).get_keyterms_based_on_sgrank()\n",
      "        self.bi_grams = KeytermExtractor(self.clean_data).bi_gramchunker()\n",
      "        self.tri_grams = KeytermExtractor(self.clean_data).tri_gramchunker()\n",
      "\n",
      "    def get_JSON(self) -> dict:\n",
      "        \"\"\"\n",
      "        Returns a dictionary of job description data.\n",
      "        \"\"\"\n",
      "        job_desc_dictionary = {\n",
      "            \"unique_id\": generate_unique_id(),\n",
      "            \"job_desc_data\": self.job_desc_data,\n",
      "            \"clean_data\": self.clean_data,\n",
      "            \"entities\": self.entities,\n",
      "            \"extracted_keywords\": self.key_words,\n",
      "            \"keyterms\": self.keyterms,\n",
      "            \"bi_grams\": str(self.bi_grams),\n",
      "            \"tri_grams\": str(self.tri_grams),\n",
      "            \"pos_frequencies\": self.pos_frequencies\n",
      "        }\n",
      "\n",
      "        return job_desc_dictionary\n",
      "\n",
      "File scripts/similarity/qdrant_search.py has no source code\n",
      "\n",
      "File scripts/utils/ExtraScripts.py has no source code\n",
      "\n",
      "import string\n",
      "import spacy\n",
      "import pywaffle\n",
      "import streamlit as st\n",
      "import pandas as pd\n",
      "import json\n",
      "import plotly.express as px\n",
      "import plotly.graph_objects as go\n",
      "import matplotlib.pyplot as plt\n",
      "import squarify\n",
      "\n",
      "st.title('Resume :blue[Matcher]')\n",
      "st.subheader('_AI Based Resume Analyzer & Ranker_')\n",
      "\n",
      "\n",
      "def read_json(filename):\n",
      "    with open(filename) as f:\n",
      "        data = json.load(f)\n",
      "    return data\n",
      "\n",
      "\n",
      "def list_to_matrix(list_to_convert, num_columns):\n",
      "    \"\"\"Converts a list to a matrix of a suitable size.\n",
      "\n",
      "    Args:\n",
      "      list_to_convert: The list to convert.\n",
      "      num_columns: The number of columns in the matrix.\n",
      "\n",
      "    Returns:\n",
      "      A matrix of the specified size, with the contents of the list.\n",
      "    \"\"\"\n",
      "\n",
      "    matrix = []\n",
      "    for i in range(len(list_to_convert) // num_columns):\n",
      "        matrix.append(list_to_convert[i * num_columns:(i + 1) * num_columns])\n",
      "\n",
      "    if len(list_to_convert) % num_columns > 0:\n",
      "        matrix.append(list_to_convert[-(len(list_to_convert) % num_columns):])\n",
      "\n",
      "    for i in range(len(matrix)):\n",
      "        for j in range(len(matrix[i])):\n",
      "            if matrix[i][j] is None:\n",
      "                matrix[i][j] = \"\"\n",
      "\n",
      "    return matrix\n",
      "\n",
      "\n",
      "def split_list(list_to_split, chunk_size):\n",
      "    \"\"\"Splits a list into 3 equal lists.\n",
      "\n",
      "    Args:\n",
      "      list_to_split: The list to split.\n",
      "      chunk_size: The size of each chunk.\n",
      "\n",
      "    Returns:\n",
      "      A list of chunk_size (+1 if over) lists, each of which is a chunk of the original list.\n",
      "    \"\"\"\n",
      "\n",
      "    num_chunks = len(list_to_split) // chunk_size\n",
      "    remainder = len(list_to_split) % chunk_size\n",
      "\n",
      "    chunks = []\n",
      "    for i in range(num_chunks):\n",
      "        chunks.append(list_to_split[i * chunk_size:(i + 1) * chunk_size])\n",
      "\n",
      "    if remainder > 0:\n",
      "        chunks.append(list_to_split[num_chunks * chunk_size:])\n",
      "\n",
      "    return chunks\n",
      "\n",
      "\n",
      "def dirty_intersection(list1, list2):\n",
      "    intersection = list(set(list1) & set(list2))\n",
      "    remainder_1 = [x for x in list1 if x not in intersection]\n",
      "    remainder_2 = [x for x in list2 if x not in intersection]\n",
      "\n",
      "    output = pd.DataFrame({\n",
      "        'elements': [\"Common words\", \"Words unique to Resume\", \"Words unique to Job Description\"],\n",
      "        'values': [len(intersection), len(remainder_1), len(remainder_2)]\n",
      "    }, index=[1, 2, 3])\n",
      "\n",
      "    return output\n",
      "\n",
      "\n",
      "def find_intersection_of_lists(list1, list2):\n",
      "    \"\"\"Finds the intersection of two lists and returns the result as a Pandas dataframe.\n",
      "\n",
      "    Args:\n",
      "      list1: The first list.\n",
      "      list2: The second list.\n",
      "\n",
      "    Returns:\n",
      "      A Pandas dataframe with three columns: `intersection`, `remainder_1`, and `remainder_2`.\n",
      "    \"\"\"\n",
      "\n",
      "    def max_of_three(a, b, c):\n",
      "        max_value = a\n",
      "        if b > max_value:\n",
      "            max_value = b\n",
      "        if c > max_value:\n",
      "            max_value = c\n",
      "\n",
      "        return max_value\n",
      "\n",
      "    def fill_by_complements(num: int, list_to_fill: list):\n",
      "        if (num > len(list_to_fill)):\n",
      "            for i in range(num-len(list_to_fill)):\n",
      "                list_to_fill.append(\" \")\n",
      "\n",
      "    intersection = list(set(list1) & set(list2))\n",
      "    remainder_1 = [x for x in list1 if x not in intersection]\n",
      "    remainder_2 = [x for x in list2 if x not in intersection]\n",
      "\n",
      "    max_count = max_of_three(\n",
      "        len(intersection), len(remainder_1), len(remainder_2))\n",
      "\n",
      "    fill_by_complements(max_count, intersection)\n",
      "    fill_by_complements(max_count, remainder_1)\n",
      "    fill_by_complements(max_count, remainder_2)\n",
      "\n",
      "    df = pd.DataFrame({\n",
      "        'intersection': intersection,\n",
      "        'remainder_1': remainder_1,\n",
      "        'remainder_2': remainder_2\n",
      "    })\n",
      "\n",
      "    return df\n",
      "\n",
      "\n",
      "def preprocess_text(text):\n",
      "    \"\"\"Preprocesses text using spacy.\n",
      "\n",
      "    Args:\n",
      "      text: The text to preprocess.\n",
      "\n",
      "    Returns:\n",
      "      A list of string tokens.\n",
      "    \"\"\"\n",
      "\n",
      "    nlp = spacy.load(\"en_core_web_sm\")\n",
      "    doc = nlp(text)\n",
      "\n",
      "    # Lemmatize words.\n",
      "    tokens = [token.lemma_ for token in doc]\n",
      "\n",
      "    # Remove stopwords.\n",
      "    stopwords = spacy.lang.en.stop_words.STOP_WORDS\n",
      "    tokens = [token for token in tokens if token not in stopwords]\n",
      "\n",
      "    # Remove punctuation.\n",
      "    punctuation = set(string.punctuation)\n",
      "    tokens = [token for token in tokens if token not in punctuation]\n",
      "\n",
      "    return tokens\n",
      "\n",
      "\n",
      "# read the json file\n",
      "resume = read_json(\n",
      "    'Data/Processed/Resume-d531571e-e4fa-45eb-ab6a-267cdeb6647e.json')\n",
      "job_desc = read_json(\n",
      "    'Data/Processed/Job-Desc-a4f06ccb-8d5a-4d0b-9f02-3ba6d686472e.json')\n",
      "st.json(resume)\n",
      "\n",
      "st.json(job_desc)\n",
      "\n",
      "st.write(\"### Reading Resume's POS\")\n",
      "df = pd.DataFrame(resume['pos_frequencies'], index=[0])\n",
      "st.write(df)\n",
      "\n",
      "fig = go.Figure(data=go.Bar(y=list(resume['pos_frequencies'].values()), x=list(resume['pos_frequencies'].keys())),\n",
      "                layout_title_text=\"Resume's POS\")\n",
      "st.write(fig)\n",
      "\n",
      "df2 = pd.DataFrame(resume['keyterms'], columns=[\"keyword\", \"value\"])\n",
      "st.dataframe(df2)\n",
      "\n",
      "# Create the dictionary\n",
      "keyword_dict = {}\n",
      "for keyword, value in resume['keyterms']:\n",
      "    keyword_dict[keyword] = value\n",
      "\n",
      "fig = go.Figure(data=[go.Table(header=dict(values=[\"Keyword\", \"Value\"],\n",
      "                                           font=dict(size=12),\n",
      "                                           fill_color='#070A52'),\n",
      "                               cells=dict(values=[list(keyword_dict.keys()),\n",
      "                                                  list(keyword_dict.values())],\n",
      "                                          line_color='darkslategray',\n",
      "                                          fill_color='#6DA9E4'))\n",
      "                      ])\n",
      "st.plotly_chart(fig)\n",
      "\n",
      "st.divider()\n",
      "\n",
      "for keyword, value in resume['keyterms']:\n",
      "    pass\n",
      "\n",
      "\n",
      "# display the waffle chart\n",
      "figure = plt.figure(\n",
      "    FigureClass=pywaffle.Waffle,\n",
      "    rows=20,\n",
      "    columns=20,\n",
      "    values=keyword_dict,\n",
      "    legend={'loc': 'upper left', 'bbox_to_anchor': (1, 1)})\n",
      "\n",
      "\n",
      "# Display the dictionary\n",
      "\n",
      "st.pyplot(fig=figure)\n",
      "# st.write(dict)\n",
      "\n",
      "fig = px.treemap(df2, path=['keyword'], values='value',\n",
      "                 color_continuous_scale='RdBu',\n",
      "                 title='Resume POS')\n",
      "st.write(fig)\n",
      "\n",
      "st.text(resume['clean_data'])\n",
      "\n",
      "\n",
      "fig = go.Figure(data=[go.Table(\n",
      "    header=dict(values=[\"Extracted Keywords\"],\n",
      "                fill_color='#1D267D',\n",
      "                align='center', font=dict(color='white', size=16)),\n",
      "    cells=dict(values=[out for out in split_list(resume['extracted_keywords'], 25)],\n",
      "               fill_color='#19A7CE',\n",
      "               align='left'))])\n",
      "\n",
      "fig.update_layout(\n",
      "    uniformtext_minsize=13\n",
      ")\n",
      "\n",
      "st.plotly_chart(figure_or_data=fig)\n",
      "\n",
      "fig = go.Figure(data=[go.Table(\n",
      "    header=dict(values=[\"Tri Grams\"],\n",
      "                fill_color='#1D267D',\n",
      "                align='center', font=dict(color='white', size=16)),\n",
      "    cells=dict(values=[resume['tri_grams']],\n",
      "               fill_color='#19A7CE',\n",
      "               align='left'))])\n",
      "\n",
      "st.plotly_chart(figure_or_data=fig)\n",
      "\n",
      "fig = go.Figure(data=[go.Table(\n",
      "    header=dict(values=[\"Bi Grams\"],\n",
      "                fill_color='#1D267D',\n",
      "                align='center', font=dict(color='white', size=16)),\n",
      "    cells=dict(values=[resume['bi_grams']],\n",
      "               fill_color='#19A7CE',\n",
      "               align='left'))])\n",
      "\n",
      "st.plotly_chart(figure_or_data=fig)\n",
      "\n",
      "resume_list = preprocess_text(resume['clean_data'])\n",
      "\n",
      "job_desc_list = preprocess_text(job_desc['clean_data'])\n",
      "\n",
      "df_data = find_intersection_of_lists(resume_list, job_desc_list)\n",
      "\n",
      "data_length = dirty_intersection(resume_list, job_desc_list)\n",
      "\n",
      "# data_length = data_length\n",
      "\n",
      "st.write(df_data)\n",
      "\n",
      "st.write(data_length)\n",
      "\n",
      "# st.write(px.data.tips())\n",
      "\n",
      "fig = px.pie(data_length, values='values', names='elements')\n",
      "st.write(fig)\n",
      "\n",
      "def list_to_matrix(list_to_convert, num_columns):\n",
      "    \"\"\"Converts a list to a matrix of a suitable size.\n",
      "\n",
      "    Args:\n",
      "      list_to_convert: The list to convert.\n",
      "      num_columns: The number of columns in the matrix.\n",
      "\n",
      "    Returns:\n",
      "      A matrix of the specified size, with the contents of the list.\n",
      "    \"\"\"\n",
      "\n",
      "    matrix = []\n",
      "    for i in range(len(list_to_convert) // num_columns):\n",
      "        matrix.append(list_to_convert[i * num_columns:(i + 1) * num_columns])\n",
      "\n",
      "    if len(list_to_convert) % num_columns > 0:\n",
      "        matrix.append(list_to_convert[-(len(list_to_convert) % num_columns):])\n",
      "\n",
      "    for i in range(len(matrix)):\n",
      "        for j in range(len(matrix[i])):\n",
      "            if matrix[i][j] is None:\n",
      "                matrix[i][j] = \"\"\n",
      "\n",
      "    return matrix\n",
      "\n",
      "\n",
      "def split_list(list_to_split, chunk_size):\n",
      "    \"\"\"Splits a list into 3 equal lists.\n",
      "\n",
      "    Args:\n",
      "      list_to_split: The list to split.\n",
      "      chunk_size: The size of each chunk.\n",
      "\n",
      "    Returns:\n",
      "      A list of chunk_size (+1 if over) lists, each of which is a chunk of the original list.\n",
      "    \"\"\"\n",
      "\n",
      "    num_chunks = len(list_to_split) // chunk_size\n",
      "    remainder = len(list_to_split) % chunk_size\n",
      "\n",
      "    chunks = []\n",
      "    for i in range(num_chunks):\n",
      "        chunks.append(list_to_split[i * chunk_size:(i + 1) * chunk_size])\n",
      "\n",
      "    if remainder > 0:\n",
      "        chunks.append(list_to_split[num_chunks * chunk_size:])\n",
      "\n",
      "    return chunks\n",
      "\n",
      "\n",
      "def dirty_intersection(list1, list2):\n",
      "    intersection = list(set(list1) & set(list2))\n",
      "    remainder_1 = [x for x in list1 if x not in intersection]\n",
      "    remainder_2 = [x for x in list2 if x not in intersection]\n",
      "\n",
      "    output = pd.DataFrame({\n",
      "        'elements': [\"Common words\", \"Words unique to Resume\", \"Words unique to Job Description\"],\n",
      "        'values': [len(intersection), len(remainder_1), len(remainder_2)]\n",
      "    }, index=[1, 2, 3])\n",
      "\n",
      "    return output\n",
      "\n",
      "\n",
      "def find_intersection_of_lists(list1, list2):\n",
      "    \"\"\"Finds the intersection of two lists and returns the result as a Pandas dataframe.\n",
      "\n",
      "    Args:\n",
      "      list1: The first list.\n",
      "      list2: The second list.\n",
      "\n",
      "    Returns:\n",
      "      A Pandas dataframe with three columns: `intersection`, `remainder_1`, and `remainder_2`.\n",
      "    \"\"\"\n",
      "\n",
      "    def max_of_three(a, b, c):\n",
      "        max_value = a\n",
      "        if b > max_value:\n",
      "            max_value = b\n",
      "        if c > max_value:\n",
      "            max_value = c\n",
      "\n",
      "        return max_value\n",
      "\n",
      "    def fill_by_complements(num: int, list_to_fill: list):\n",
      "        if (num > len(list_to_fill)):\n",
      "            for i in range(num-len(list_to_fill)):\n",
      "                list_to_fill.append(\" \")\n",
      "\n",
      "    intersection = list(set(list1) & set(list2))\n",
      "    remainder_1 = [x for x in list1 if x not in intersection]\n",
      "    remainder_2 = [x for x in list2 if x not in intersection]\n",
      "\n",
      "    max_count = max_of_three(\n",
      "        len(intersection), len(remainder_1), len(remainder_2))\n",
      "\n",
      "    fill_by_complements(max_count, intersection)\n",
      "    fill_by_complements(max_count, remainder_1)\n",
      "    fill_by_complements(max_count, remainder_2)\n",
      "\n",
      "    df = pd.DataFrame({\n",
      "        'intersection': intersection,\n",
      "        'remainder_1': remainder_1,\n",
      "        'remainder_2': remainder_2\n",
      "    })\n",
      "\n",
      "    return df\n",
      "\n",
      "\n",
      "def preprocess_text(text):\n",
      "    \"\"\"Preprocesses text using spacy.\n",
      "\n",
      "    Args:\n",
      "      text: The text to preprocess.\n",
      "\n",
      "    Returns:\n",
      "      A list of string tokens.\n",
      "    \"\"\"\n",
      "\n",
      "    nlp = spacy.load(\"en_core_web_sm\")\n",
      "    doc = nlp(text)\n",
      "\n",
      "    # Lemmatize words.\n",
      "    tokens = [token.lemma_ for token in doc]\n",
      "\n",
      "    # Remove stopwords.\n",
      "    stopwords = spacy.lang.en.stop_words.STOP_WORDS\n",
      "    tokens = [token for token in tokens if token not in stopwords]\n",
      "\n",
      "    # Remove punctuation.\n",
      "    punctuation = set(string.punctuation)\n",
      "    tokens = [token for token in tokens if token not in punctuation]\n",
      "\n",
      "    return tokens\n",
      "\n",
      "\n",
      "# Usage\n",
      "\n",
      "# resume_list = preprocess_text(resume['clean_data'])\n",
      "\n",
      "# job_desc_list = preprocess_text(job_desc['clean_data'])\n",
      "\n",
      "# df_data = find_intersection_of_lists(resume_list, job_desc_list)\n",
      "\n",
      "# data_length = dirty_intersection(resume_list, job_desc_list)\n",
      "\n",
      "# # data_length = data_length\n",
      "\n",
      "# st.write(df_data)\n",
      "\n",
      "# st.write(data_length)\n",
      "\n",
      "# # st.write(px.data.tips())\n",
      "\n",
      "# fig = px.pie(data_length, values='values', names='elements')\n",
      "# st.write(fig)\n",
      "\n",
      "# st.text(resume['clean_data'])\n",
      "\n",
      "\n",
      "# fig = go.Figure(data=[go.Table(\n",
      "#     header=dict(values=[\"Extracted Keywords\"],\n",
      "#                 fill_color='#1D267D',\n",
      "#                 align='center', font=dict(color='white', size=16)),\n",
      "#     cells=dict(values=[out for out in split_list(resume['extracted_keywords'], 25)],\n",
      "#                fill_color='#19A7CE',\n",
      "#                align='left'))])\n",
      "\n",
      "# fig.update_layout(\n",
      "#     uniformtext_minsize=13\n",
      "# )\n",
      "\n",
      "import string\n",
      "import spacy\n",
      "import pywaffle\n",
      "import streamlit as st\n",
      "import pandas as pd\n",
      "import json\n",
      "import plotly.express as px\n",
      "import plotly.graph_objects as go\n",
      "import matplotlib.pyplot as plt\n",
      "import squarify\n",
      "\n",
      "st.title('Resume :blue[Matcher]')\n",
      "st.subheader('_AI Based Resume Analyzer & Ranker_')\n",
      "\n",
      "\n",
      "def read_json(filename):\n",
      "    with open(filename) as f:\n",
      "        data = json.load(f)\n",
      "    return data\n",
      "\n",
      "\n",
      "# read the json file\n",
      "resume = read_json(\n",
      "    'Data/Processed/Resume-d531571e-e4fa-45eb-ab6a-267cdeb6647e.json')\n",
      "job_desc = read_json(\n",
      "    'Data/Processed/Job-Desc-a4f06ccb-8d5a-4d0b-9f02-3ba6d686472e.json')\n",
      "\n",
      "st.write(\"### Reading Resume's POS\")\n",
      "df = pd.DataFrame(resume['pos_frequencies'], index=[0])\n",
      "fig = go.Figure(data=go.Bar(y=list(resume['pos_frequencies'].values()), x=list(resume['pos_frequencies'].keys())),\n",
      "                layout_title_text=\"Resume's POS\")\n",
      "st.write(fig)\n",
      "\n",
      "df2 = pd.DataFrame(resume['keyterms'], columns=[\"keyword\", \"value\"])\n",
      "st.dataframe(df2)\n",
      "\n",
      "# Create the dictionary\n",
      "keyword_dict = {}\n",
      "for keyword, value in resume['keyterms']:\n",
      "    keyword_dict[keyword] = value\n",
      "\n",
      "fig = go.Figure(data=[go.Table(header=dict(values=[\"Keyword\", \"Value\"],\n",
      "                                           font=dict(size=12),\n",
      "                                           fill_color='#070A52'),\n",
      "                               cells=dict(values=[list(keyword_dict.keys()),\n",
      "                                                  list(keyword_dict.values())],\n",
      "                                          line_color='darkslategray',\n",
      "                                          fill_color='#6DA9E4'))\n",
      "                      ])\n",
      "st.plotly_chart(fig)\n",
      "\n",
      "st.divider()\n",
      "\n",
      "for keyword, value in resume['keyterms']:\n",
      "    pass\n",
      "\n",
      "\n",
      "# display the waffle chart\n",
      "figure = plt.figure(\n",
      "    FigureClass=pywaffle.Waffle,\n",
      "    rows=20,\n",
      "    columns=20,\n",
      "    values=keyword_dict,\n",
      "    legend={'loc': 'upper left', 'bbox_to_anchor': (1, 1)})\n",
      "\n",
      "\n",
      "# Display the dictionary\n",
      "\n",
      "st.pyplot(fig=figure)\n",
      "# st.write(dict)\n",
      "\n",
      "fig = px.treemap(df2, path=['keyword'], values='value',\n",
      "                 color_continuous_scale='RdBu',\n",
      "                 title='Resume POS')\n",
      "st.write(fig)\n",
      "\n",
      "\n",
      "st.plotly_chart(figure_or_data=fig)\n",
      "\n",
      "fig = go.Figure(data=[go.Table(\n",
      "    header=dict(values=[\"Tri Grams\"],\n",
      "                fill_color='#1D267D',\n",
      "                align='center', font=dict(color='white', size=16)),\n",
      "    cells=dict(values=[resume['tri_grams']],\n",
      "               fill_color='#19A7CE',\n",
      "               align='left'))])\n",
      "\n",
      "st.plotly_chart(figure_or_data=fig)\n",
      "\n",
      "fig = go.Figure(data=[go.Table(\n",
      "    header=dict(values=[\"Bi Grams\"],\n",
      "                fill_color='#1D267D',\n",
      "                align='center', font=dict(color='white', size=16)),\n",
      "    cells=dict(values=[resume['bi_grams']],\n",
      "               fill_color='#19A7CE',\n",
      "               align='left'))])\n",
      "\n",
      "st.plotly_chart(figure_or_data=fig)\n",
      "\n",
      "from scripts.parsers.ParseResumeToJson import ParseResume\n",
      "from scripts.parsers.ParseJobDescToJson import ParseJobDesc\n",
      "from scripts.ReadPdf import read_single_pdf\n",
      "import os.path\n",
      "import pathlib\n",
      "import json\n",
      "\n",
      "READ_JOB_DESCRIPTION_FROM = 'Data/JobDescription/'\n",
      "SAVE_DIRECTORY = 'Data/Processed/JobDescription'\n",
      "\n",
      "\n",
      "class JobDescriptionProcessor:\n",
      "    def __init__(self, input_file):\n",
      "        self.input_file = input_file\n",
      "        self.input_file_name = os.path.join(\n",
      "            READ_JOB_DESCRIPTION_FROM + self.input_file)\n",
      "\n",
      "    def process(self) -> bool:\n",
      "        try:\n",
      "            resume_dict = self._read_resumes()\n",
      "            self._write_json_file(resume_dict)\n",
      "            return True\n",
      "        except Exception as e:\n",
      "            print(f\"An error occurred: {str(e)}\")\n",
      "            return False\n",
      "\n",
      "    def _read_resumes(self) -> dict:\n",
      "        data = read_single_pdf(self.input_file_name)\n",
      "        output = ParseResume(data).get_JSON()\n",
      "        return output\n",
      "\n",
      "    def _read_job_desc(self) -> dict:\n",
      "        data = read_single_pdf(self.input_file_name)\n",
      "        output = ParseJobDesc(data).get_JSON()\n",
      "        return output\n",
      "\n",
      "    def _write_json_file(self, resume_dictionary: dict):\n",
      "        file_name = str(\"JobDescription-\" +\n",
      "                        resume_dictionary[\"unique_id\"] + \".json\")\n",
      "        save_directory_name = pathlib.Path(SAVE_DIRECTORY) / file_name\n",
      "        json_object = json.dumps(resume_dictionary, sort_keys=True, indent=14)\n",
      "        with open(save_directory_name, \"w+\") as outfile:\n",
      "            outfile.write(json_object)\n",
      "\n",
      "from scripts.parsers.ParseResumeToJson import ParseResume\n",
      "from scripts.parsers.ParseJobDescToJson import ParseJobDesc\n",
      "from scripts.ReadPdf import read_single_pdf\n",
      "import os.path\n",
      "import pathlib\n",
      "import json\n",
      "\n",
      "READ_RESUME_FROM = 'Data/Resumes/'\n",
      "SAVE_DIRECTORY = 'Data/Processed/Resumes'\n",
      "\n",
      "\n",
      "class ResumeProcessor:\n",
      "    def __init__(self, input_file):\n",
      "        self.input_file = input_file\n",
      "        self.input_file_name = os.path.join(READ_RESUME_FROM + self.input_file)\n",
      "\n",
      "    def process(self) -> bool:\n",
      "        try:\n",
      "            resume_dict = self._read_resumes()\n",
      "            self._write_json_file(resume_dict)\n",
      "            return True\n",
      "        except Exception as e:\n",
      "            print(f\"An error occurred: {str(e)}\")\n",
      "            return False\n",
      "\n",
      "    def _read_resumes(self) -> dict:\n",
      "        data = read_single_pdf(self.input_file_name)\n",
      "        output = ParseResume(data).get_JSON()\n",
      "        return output\n",
      "\n",
      "    def _read_job_desc(self) -> dict:\n",
      "        data = read_single_pdf(self.input_file_name)\n",
      "        output = ParseJobDesc(data).get_JSON()\n",
      "        return output\n",
      "\n",
      "    def _write_json_file(self, resume_dictionary: dict):\n",
      "        file_name = str(\"Resume-\" + resume_dictionary[\"unique_id\"] + \".json\")\n",
      "        save_directory_name = pathlib.Path(SAVE_DIRECTORY) / file_name\n",
      "        json_object = json.dumps(resume_dictionary, sort_keys=True, indent=14)\n",
      "        with open(save_directory_name, \"w+\") as outfile:\n",
      "            outfile.write(json_object)\n",
      "\n",
      "import json\n",
      "import pathlib\n",
      "from scripts.Extractor import DataExtractor\n",
      "from scripts.utils.Utils import TextCleaner, CountFrequency, generate_unique_id\n",
      "from scripts.KeytermsExtraction import KeytermExtractor\n",
      "import os\n",
      "\n",
      "SAVE_DIRECTORY = \"../../Data/Processed/JobDescription\"\n",
      "\n",
      "\n",
      "class ParseJobDesc:\n",
      "\n",
      "    def __init__(self, job_desc: str):\n",
      "        self.job_desc_data = job_desc\n",
      "        self.clean_data = TextCleaner.clean_text(\n",
      "            self.job_desc_data)\n",
      "        self.entities = DataExtractor(self.clean_data).extract_entities()\n",
      "        self.key_words = DataExtractor(\n",
      "            self.clean_data).extract_particular_words()\n",
      "        self.pos_frequencies = CountFrequency(\n",
      "            self.clean_data).count_frequency()\n",
      "        self.keyterms = KeytermExtractor(\n",
      "            self.clean_data).get_keyterms_based_on_sgrank()\n",
      "        self.bi_grams = KeytermExtractor(self.clean_data).bi_gramchunker()\n",
      "        self.tri_grams = KeytermExtractor(self.clean_data).tri_gramchunker()\n",
      "\n",
      "    def get_JSON(self) -> dict:\n",
      "        \"\"\"\n",
      "        Returns a dictionary of job description data.\n",
      "        \"\"\"\n",
      "        job_desc_dictionary = {\n",
      "            \"unique_id\": generate_unique_id(),\n",
      "            \"job_desc_data\": self.job_desc_data,\n",
      "            \"clean_data\": self.clean_data,\n",
      "            \"entities\": self.entities,\n",
      "            \"extracted_keywords\": self.key_words,\n",
      "            \"keyterms\": self.keyterms,\n",
      "            \"bi_grams\": str(self.bi_grams),\n",
      "            \"tri_grams\": str(self.tri_grams),\n",
      "            \"pos_frequencies\": self.pos_frequencies\n",
      "        }\n",
      "\n",
      "        return job_desc_dictionary\n",
      "\n",
      "import json\n",
      "from scripts.Extractor import DataExtractor\n",
      "from scripts.utils.Utils import TextCleaner, CountFrequency, generate_unique_id\n",
      "from scripts.KeytermsExtraction import KeytermExtractor\n",
      "import os.path\n",
      "import os\n",
      "import pathlib\n",
      "\n",
      "SAVE_DIRECTORY = '../../Data/Processed/Resumes'\n",
      "\n",
      "\n",
      "class ParseResume:\n",
      "\n",
      "    def __init__(self, resume: str):\n",
      "        self.resume_data = resume\n",
      "        self.clean_data = TextCleaner.clean_text(\n",
      "            self.resume_data)\n",
      "        self.entities = DataExtractor(self.clean_data).extract_entities()\n",
      "        self.name = DataExtractor(self.clean_data[:30]).extract_names()\n",
      "        self.experience = DataExtractor(self.clean_data).extract_experience()\n",
      "        self.emails = DataExtractor(self.resume_data).extract_emails()\n",
      "        self.phones = DataExtractor(self.resume_data).extract_phone_numbers()\n",
      "        self.years = DataExtractor(self.clean_data).extract_position_year()\n",
      "        self.key_words = DataExtractor(\n",
      "            self.clean_data).extract_particular_words()\n",
      "        self.pos_frequencies = CountFrequency(\n",
      "            self.clean_data).count_frequency()\n",
      "        self.keyterms = KeytermExtractor(\n",
      "            self.clean_data).get_keyterms_based_on_sgrank()\n",
      "        self.bi_grams = KeytermExtractor(self.clean_data).bi_gramchunker()\n",
      "        self.tri_grams = KeytermExtractor(self.clean_data).tri_gramchunker()\n",
      "\n",
      "    def get_JSON(self) -> dict:\n",
      "        \"\"\"\n",
      "        Returns a dictionary of resume data.\n",
      "        \"\"\"\n",
      "        resume_dictionary = {\n",
      "            \"unique_id\": generate_unique_id(),\n",
      "            \"resume_data\": self.resume_data,\n",
      "            \"clean_data\": self.clean_data,\n",
      "            \"entities\": self.entities,\n",
      "            \"extracted_keywords\": self.key_words,\n",
      "            \"keyterms\": self.keyterms,\n",
      "            \"name\": self.name,\n",
      "            \"experience\": self.experience,\n",
      "            \"emails\": self.emails,\n",
      "            \"phones\": self.phones,\n",
      "            \"years\": self.years,\n",
      "            \"bi_grams\": str(self.bi_grams),\n",
      "            \"tri_grams\": str(self.tri_grams),\n",
      "            \"pos_frequencies\": self.pos_frequencies\n",
      "        }\n",
      "\n",
      "        return resume_dictionary\n",
      "\n",
      "import os\n",
      "\n",
      "\n",
      "def get_filenames_from_dir(directory_path: str) -> list:\n",
      "    filenames = [f for f in os.listdir(directory_path) if os.path.isfile(\n",
      "        os.path.join(directory_path, f))]\n",
      "    return filenames\n",
      "\n",
      "import string\n",
      "import spacy\n",
      "import pywaffle\n",
      "import streamlit as st\n",
      "import pandas as pd\n",
      "import json\n",
      "import plotly.express as px\n",
      "import plotly.graph_objects as go\n",
      "import matplotlib.pyplot as plt\n",
      "import squarify\n",
      "\n",
      "st.title('Resume :blue[Matcher]')\n",
      "st.image('Assets/img/header_image.jpg')\n",
      "st.subheader('_AI Based Resume Analyzer & Ranker_')\n",
      "\n",
      "\n",
      "def read_json(filename):\n",
      "    with open(filename) as f:\n",
      "        data = json.load(f)\n",
      "    return data\n",
      "\n",
      "\n",
      "# read the json file\n",
      "resume = read_json(\n",
      "    'Data/Processed/Resume-d531571e-e4fa-45eb-ab6a-267cdeb6647e.json')\n",
      "job_desc = read_json(\n",
      "    'Data/Processed/Job-Desc-a4f06ccb-8d5a-4d0b-9f02-3ba6d686472e.json')\n",
      "\n",
      "st.write(\"### Reading Resume's POS\")\n",
      "df = pd.DataFrame(resume['pos_frequencies'], index=[0])\n",
      "fig = go.Figure(data=go.Bar(y=list(resume['pos_frequencies'].values()), x=list(resume['pos_frequencies'].keys())),\n",
      "                layout_title_text=\"Resume's POS\")\n",
      "st.write(fig)\n",
      "\n",
      "df2 = pd.DataFrame(resume['keyterms'], columns=[\"keyword\", \"value\"])\n",
      "st.dataframe(df2)\n",
      "\n",
      "# Create the dictionary\n",
      "keyword_dict = {}\n",
      "for keyword, value in resume['keyterms']:\n",
      "    keyword_dict[keyword] = value\n",
      "\n",
      "fig = go.Figure(data=[go.Table(header=dict(values=[\"Keyword\", \"Value\"],\n",
      "                                           font=dict(size=12),\n",
      "                                           fill_color='#070A52'),\n",
      "                               cells=dict(values=[list(keyword_dict.keys()),\n",
      "                                                  list(keyword_dict.values())],\n",
      "                                          line_color='darkslategray',\n",
      "                                          fill_color='#6DA9E4'))\n",
      "                      ])\n",
      "st.plotly_chart(fig)\n",
      "\n",
      "st.divider()\n",
      "\n",
      "for keyword, value in resume['keyterms']:\n",
      "    pass\n",
      "\n",
      "\n",
      "# display the waffle chart\n",
      "figure = plt.figure(\n",
      "    FigureClass=pywaffle.Waffle,\n",
      "    rows=20,\n",
      "    columns=20,\n",
      "    values=keyword_dict,\n",
      "    legend={'loc': 'upper left', 'bbox_to_anchor': (1, 1)})\n",
      "\n",
      "\n",
      "# Display the dictionary\n",
      "\n",
      "st.pyplot(fig=figure)\n",
      "# st.write(dict)\n",
      "\n",
      "fig = px.treemap(df2, path=['keyword'], values='value',\n",
      "                 color_continuous_scale='RdBu',\n",
      "                 title='Resume POS')\n",
      "st.write(fig)\n",
      "\n",
      "\n",
      "st.plotly_chart(figure_or_data=fig)\n",
      "\n",
      "fig = go.Figure(data=[go.Table(\n",
      "    header=dict(values=[\"Tri Grams\"],\n",
      "                fill_color='#1D267D',\n",
      "                align='center', font=dict(color='white', size=16)),\n",
      "    cells=dict(values=[resume['tri_grams']],\n",
      "               fill_color='#19A7CE',\n",
      "               align='left'))])\n",
      "\n",
      "st.plotly_chart(figure_or_data=fig)\n",
      "\n",
      "fig = go.Figure(data=[go.Table(\n",
      "    header=dict(values=[\"Bi Grams\"],\n",
      "                fill_color='#1D267D',\n",
      "                align='center', font=dict(color='white', size=16)),\n",
      "    cells=dict(values=[resume['bi_grams']],\n",
      "               fill_color='#19A7CE',\n",
      "               align='left'))])\n",
      "\n",
      "st.plotly_chart(figure_or_data=fig)\n",
      "\n",
      "import streamlit as st\n",
      "import pandas as pd\n",
      "import json\n",
      "import plotly.express as px\n",
      "import plotly.graph_objects as go\n",
      "import matplotlib.pyplot as plt\n",
      "from scripts.utils.ReadFiles import get_filenames_from_dir\n",
      "from scripts.ResumeProcessor import ResumeProcessor\n",
      "import time\n",
      "\n",
      "\n",
      "def read_json(filename):\n",
      "    with open(filename) as f:\n",
      "        data = json.load(f)\n",
      "    return data\n",
      "\n",
      "\n",
      "st.image('Assets/img/header_image.jpg')\n",
      "\n",
      "st.markdown(\"####  Please put your resumes in the `Data/Resumes` folder.\")\n",
      "st.markdown(\"##### There are some sample resumes in the `Data/Resumes` folder. The app automatically starts using the files present there.\")\n",
      "\n",
      "\n",
      "file_names = get_filenames_from_dir(\"Data/Resumes\")\n",
      "st.write(\"There are\", len(file_names), \" resumes present.\")\n",
      "\n",
      "st.info('Resumes are getting parsed, please wait.', icon=\"\")\n",
      "\n",
      "for file in file_names:\n",
      "    processor = ResumeProcessor(file)\n",
      "    success = processor.process()\n",
      "st.success('All the resumes have been parsed', icon=\"\")\n",
      "\n",
      "st.write(\"There are\", len(file_names),\n",
      "         \" resumes present. Please select one from the menu below:\")\n",
      "output = st.radio(\"Select from the resumes below:\",\n",
      "                  (file for file in file_names))\n",
      "st.write(\"You have selected \", output, \" printing the resume\")\n",
      "\n",
      "\n",
      "# # read the json file\n",
      "# resume = read_json(\n",
      "#     'Data/Processed/Resume-d531571e-e4fa-45eb-ab6a-267cdeb6647e.json')\n",
      "# job_desc = read_json(\n",
      "#     'Data/Processed/Job-Desc-a4f06ccb-8d5a-4d0b-9f02-3ba6d686472e.json')\n",
      "\n",
      "# st.write(\"#### Your Resume\")\n",
      "# st.caption(\"This is how your resume looks after PDF Parsing:\")\n",
      "\n",
      "# st.json(resume[\"clean_data\"])\n",
      "\n",
      "from scripts.parsers.ParseResumeToJson import ParseResume\n",
      "from scripts.parsers.ParseJobDescToJson import ParseJobDesc\n",
      "from scripts.ReadPdf import read_single_pdf\n",
      "import os.path\n",
      "import pathlib\n",
      "import json\n",
      "\n",
      "READ_JOB_DESCRIPTION_FROM = 'Data/JobDescription/'\n",
      "SAVE_DIRECTORY = 'Data/Processed/JobDescription'\n",
      "\n",
      "\n",
      "class JobDescriptionProcessor:\n",
      "    def __init__(self, input_file):\n",
      "        self.input_file = input_file\n",
      "        self.input_file_name = os.path.join(\n",
      "            READ_JOB_DESCRIPTION_FROM + self.input_file)\n",
      "\n",
      "    def process(self) -> bool:\n",
      "        try:\n",
      "            resume_dict = self._read_resumes()\n",
      "            self._write_json_file(resume_dict)\n",
      "            return True\n",
      "        except Exception as e:\n",
      "            print(f\"An error occurred: {str(e)}\")\n",
      "            return False\n",
      "\n",
      "    def _read_resumes(self) -> dict:\n",
      "        data = read_single_pdf(self.input_file_name)\n",
      "        output = ParseResume(data).get_JSON()\n",
      "        return output\n",
      "\n",
      "    def _read_job_desc(self) -> dict:\n",
      "        data = read_single_pdf(self.input_file_name)\n",
      "        output = ParseJobDesc(data).get_JSON()\n",
      "        return output\n",
      "\n",
      "    def _write_json_file(self, resume_dictionary: dict):\n",
      "        file_name = str(\"JobDescription-\" + self.input_file +\n",
      "                        resume_dictionary[\"unique_id\"] + \".json\")\n",
      "        save_directory_name = pathlib.Path(SAVE_DIRECTORY) / file_name\n",
      "        json_object = json.dumps(resume_dictionary, sort_keys=True, indent=14)\n",
      "        with open(save_directory_name, \"w+\") as outfile:\n",
      "            outfile.write(json_object)\n",
      "\n",
      "from scripts.parsers.ParseResumeToJson import ParseResume\n",
      "from scripts.parsers.ParseJobDescToJson import ParseJobDesc\n",
      "from scripts.ReadPdf import read_single_pdf\n",
      "import os.path\n",
      "import pathlib\n",
      "import json\n",
      "\n",
      "READ_RESUME_FROM = 'Data/Resumes/'\n",
      "SAVE_DIRECTORY = 'Data/Processed/Resumes'\n",
      "\n",
      "\n",
      "class ResumeProcessor:\n",
      "    def __init__(self, input_file):\n",
      "        self.input_file = input_file\n",
      "        self.input_file_name = os.path.join(READ_RESUME_FROM + self.input_file)\n",
      "\n",
      "    def process(self) -> bool:\n",
      "        try:\n",
      "            resume_dict = self._read_resumes()\n",
      "            self._write_json_file(resume_dict)\n",
      "            return True\n",
      "        except Exception as e:\n",
      "            print(f\"An error occurred: {str(e)}\")\n",
      "            return False\n",
      "\n",
      "    def _read_resumes(self) -> dict:\n",
      "        data = read_single_pdf(self.input_file_name)\n",
      "        output = ParseResume(data).get_JSON()\n",
      "        return output\n",
      "\n",
      "    def _read_job_desc(self) -> dict:\n",
      "        data = read_single_pdf(self.input_file_name)\n",
      "        output = ParseJobDesc(data).get_JSON()\n",
      "        return output\n",
      "\n",
      "    def _write_json_file(self, resume_dictionary: dict):\n",
      "        file_name = str(\"Resume-\" + self.input_file +\n",
      "                        resume_dictionary[\"unique_id\"] + \".json\")\n",
      "        save_directory_name = pathlib.Path(SAVE_DIRECTORY) / file_name\n",
      "        json_object = json.dumps(resume_dictionary, sort_keys=True, indent=14)\n",
      "        with open(save_directory_name, \"w+\") as outfile:\n",
      "            outfile.write(json_object)\n",
      "\n",
      "import streamlit as st\n",
      "import pandas as pd\n",
      "import json\n",
      "import plotly.express as px\n",
      "import plotly.graph_objects as go\n",
      "import matplotlib.pyplot as plt\n",
      "from scripts.utils.ReadFiles import get_filenames_from_dir\n",
      "from scripts.ResumeProcessor import ResumeProcessor\n",
      "import time\n",
      "\n",
      "\n",
      "def read_json(filename):\n",
      "    with open(filename) as f:\n",
      "        data = json.load(f)\n",
      "    return data\n",
      "\n",
      "\n",
      "st.image('Assets/img/header_image.jpg')\n",
      "\n",
      "st.markdown(\"####  Please put your resumes in the `Data/Resumes` folder.\")\n",
      "st.markdown(\"##### There are some sample resumes in the `Data/Resumes` folder. The app automatically starts using the files present there.\")\n",
      "\n",
      "\n",
      "file_names = get_filenames_from_dir(\"Data/Resumes\")\n",
      "st.write(\"There are\", len(file_names), \" resumes present.\")\n",
      "\n",
      "st.info('Resumes are getting parsed, please wait.', icon=\"\")\n",
      "\n",
      "for file in file_names:\n",
      "    processor = ResumeProcessor(file)\n",
      "    success = processor.process()\n",
      "st.success('All the resumes have been parsed', icon=\"\")\n",
      "\n",
      "resume_names = get_filenames_from_dir(\"Data/Processed/Resumes\")\n",
      "\n",
      "st.write(\"There are\", len(file_names),\n",
      "         \" resumes present. Please select one from the menu below:\")\n",
      "output = st.radio(\"Select from the resumes below:\",\n",
      "                  resume_names)\n",
      "st.write(\"You have selected \", output, \" printing the resume\")\n",
      "selected_file = read_json(\"Data/Processed/Resumes/\"+output)\n",
      "st.json(selected_file)\n",
      "\n",
      "\n",
      "# # read the json file\n",
      "# resume = read_json(\n",
      "#     'Data/Processed/Resume-d531571e-e4fa-45eb-ab6a-267cdeb6647e.json')\n",
      "# job_desc = read_json(\n",
      "#     'Data/Processed/Job-Desc-a4f06ccb-8d5a-4d0b-9f02-3ba6d686472e.json')\n",
      "\n",
      "# st.write(\"#### Your Resume\")\n",
      "# st.caption(\"This is how your resume looks after PDF Parsing:\")\n",
      "\n",
      "# st.json(resume[\"clean_data\"])\n",
      "\n",
      "import streamlit as st\n",
      "import pandas as pd\n",
      "import json\n",
      "import plotly.express as px\n",
      "import plotly.graph_objects as go\n",
      "import matplotlib.pyplot as plt\n",
      "from scripts.utils.ReadFiles import get_filenames_from_dir\n",
      "from scripts.ResumeProcessor import ResumeProcessor\n",
      "import time\n",
      "\n",
      "\n",
      "def read_json(filename):\n",
      "    with open(filename) as f:\n",
      "        data = json.load(f)\n",
      "    return data\n",
      "\n",
      "\n",
      "st.image('Assets/img/header_image.jpg')\n",
      "\n",
      "st.markdown(\"####  Please put your resumes in the `Data/Resumes` folder.\")\n",
      "st.markdown(\"##### There are some sample resumes in the `Data/Resumes` folder. The app automatically starts using the files present there.\")\n",
      "\n",
      "\n",
      "file_names = get_filenames_from_dir(\"Data/Resumes\")\n",
      "st.write(\"There are\", len(file_names), \" resumes present.\")\n",
      "\n",
      "st.info('Resumes are getting parsed, please wait.', icon=\"\")\n",
      "\n",
      "for file in file_names:\n",
      "    processor = ResumeProcessor(file)\n",
      "    success = processor.process()\n",
      "st.success('All the resumes have been parsed', icon=\"\")\n",
      "\n",
      "resume_names = get_filenames_from_dir(\"Data/Processed/Resumes\")\n",
      "\n",
      "st.write(\"There are\", len(file_names),\n",
      "         \" resumes present. Please select one from the menu below:\")\n",
      "output = st.slider('Select Resume Number', 0, 5, 2)\n",
      "st.write(\"You have selected \", output, \" printing the resume\")\n",
      "selected_file = read_json(\"Data/Processed/Resumes/\"+output)\n",
      "st.json(selected_file)\n",
      "\n",
      "\n",
      "# # read the json file\n",
      "# resume = read_json(\n",
      "#     'Data/Processed/Resume-d531571e-e4fa-45eb-ab6a-267cdeb6647e.json')\n",
      "# job_desc = read_json(\n",
      "#     'Data/Processed/Job-Desc-a4f06ccb-8d5a-4d0b-9f02-3ba6d686472e.json')\n",
      "\n",
      "# st.write(\"#### Your Resume\")\n",
      "# st.caption(\"This is how your resume looks after PDF Parsing:\")\n",
      "\n",
      "# st.json(resume[\"clean_data\"])\n",
      "\n",
      "File LOC changed from 373 to 325\n",
      "File LOC changed from 373 to 325\n",
      "File LOC changed from 293 to 260\n",
      "File LOC changed from 293 to 260\n",
      "File LOC changed from 404 to 346\n",
      "File LOC changed from 404 to 346\n",
      "File LOC changed from 342 to 305\n",
      "File LOC changed from 342 to 305\n",
      "File LOC changed from 349 to 293\n",
      "File LOC changed from 349 to 293\n",
      "import json\n",
      "from scripts.utils.ReadFiles import get_filenames_from_dir\n",
      "from scripts.ResumeProcessor import ResumeProcessor\n",
      "import logging\n",
      "\n",
      "logging.basicConfig(filename='app.log', filemode='w',\n",
      "                    level=logging.DEBUG,\n",
      "                    format='%(name)s - %(levelname)s - %(message)s')\n",
      "\n",
      "\n",
      "def read_json(filename):\n",
      "    with open(filename) as f:\n",
      "        data = json.load(f)\n",
      "    return data\n",
      "\n",
      "\n",
      "logging.info('Started to read from Data/Resumes')\n",
      "try:\n",
      "    # Check if there are resumes present or not.\n",
      "    # If present then parse it.\n",
      "    file_names = get_filenames_from_dir(\"Data/Resumes\")\n",
      "    logging.info('Reading from Data/Resumes is now complete.')\n",
      "except:\n",
      "    # Exit the program if there are no resumes.\n",
      "    logging.error('There are no resumes present in the specified folder.')\n",
      "    logging.error('Exiting from the program.')\n",
      "    logging.error(\n",
      "        'Please add resumes in the Data/Resumes folder and try again.')\n",
      "    exit(1)\n",
      "\n",
      "# Now after getting the file_names parse the resumes into a JSON Format.\n",
      "logging.info('Started parsing the resumes.')\n",
      "for file in file_names:\n",
      "    processor = ResumeProcessor(file)\n",
      "    success = processor.process()\n",
      "logging.info('Parsing of the resumes is now complete.')\n",
      "logging.info('Success now run `streamlit run streamlit_second.py`')\n",
      "\n",
      "import re\n",
      "import urllib.request\n",
      "import spacy\n",
      "from scripts.utils.Utils import TextCleaner\n",
      "\n",
      "\n",
      "# Load the English model\n",
      "nlp = spacy.load('en_core_web_sm')\n",
      "\n",
      "\n",
      "RESUME_SECTIONS = [\n",
      "    'Contact Information',\n",
      "    'Objective',\n",
      "    'Summary',\n",
      "    'Education',\n",
      "    'Experience',\n",
      "    'Skills',\n",
      "    'Projects',\n",
      "    'Certifications',\n",
      "    'Licenses',\n",
      "    'Awards',\n",
      "    'Honors',\n",
      "    'Publications',\n",
      "    'References',\n",
      "    'Technical Skills',\n",
      "    'Computer Skills',\n",
      "    'Programming Languages',\n",
      "    'Software Skills',\n",
      "    'Soft Skills',\n",
      "    'Language Skills',\n",
      "    'Professional Skills',\n",
      "    'Transferable Skills',\n",
      "    'Work Experience',\n",
      "    'Professional Experience',\n",
      "    'Employment History',\n",
      "    'Internship Experience',\n",
      "    'Volunteer Experience',\n",
      "    'Leadership Experience',\n",
      "    'Research Experience',\n",
      "    'Teaching Experience'\n",
      "]\n",
      "\n",
      "\n",
      "class DataExtractor:\n",
      "    \"\"\"\n",
      "    A class for extracting various types of data from text.\n",
      "    \"\"\"\n",
      "\n",
      "    def __init__(self, raw_text: str):\n",
      "        \"\"\"\n",
      "        Initialize the DataExtractor object.\n",
      "\n",
      "        Args:\n",
      "            raw_text (str): The raw input text.\n",
      "        \"\"\"\n",
      "\n",
      "        self.text = raw_text\n",
      "        self.clean_text = TextCleaner.clean_text(self.text)\n",
      "        self.doc = nlp(self.clean_text)\n",
      "\n",
      "    def extract_links(self):\n",
      "        \"\"\"\n",
      "        Find links of any type in a given string.\n",
      "\n",
      "        Args:\n",
      "            text (str): The string to search for links.\n",
      "\n",
      "        Returns:\n",
      "            list: A list containing all the found links.\n",
      "        \"\"\"\n",
      "        link_pattern = r'\\b(?:https?://|www\\.)\\S+\\b'\n",
      "        links = re.findall(link_pattern, self.text)\n",
      "        return links\n",
      "\n",
      "    def extract_links_extended(self):\n",
      "        \"\"\"\n",
      "        Extract links of all kinds (HTTP, HTTPS, FTP, email, www.linkedin.com,\n",
      "          and github.com/user_name) from a webpage.\n",
      "\n",
      "        Args:\n",
      "            url (str): The URL of the webpage.\n",
      "\n",
      "        Returns:\n",
      "            list: A list containing all the extracted links.\n",
      "        \"\"\"\n",
      "        links = []\n",
      "        try:\n",
      "            response = urllib.request.urlopen(self.text)\n",
      "            html_content = response.read().decode('utf-8')\n",
      "            pattern = r'href=[\\'\"]?([^\\'\" >]+)'\n",
      "            raw_links = re.findall(pattern, html_content)\n",
      "            for link in raw_links:\n",
      "                if link.startswith(('http://', 'https://', 'ftp://', 'mailto:',\n",
      "                                    'www.linkedin.com', 'github.com/', 'twitter.com')):\n",
      "                    links.append(link)\n",
      "        except Exception as e:\n",
      "            print(f\"Error extracting links: {str(e)}\")\n",
      "        return links\n",
      "\n",
      "    def extract_names(self):\n",
      "        \"\"\"Extracts and returns a list of names from the given \n",
      "        text using spaCy's named entity recognition.\n",
      "\n",
      "        Args:\n",
      "            text (str): The text to extract names from.\n",
      "\n",
      "        Returns:\n",
      "            list: A list of strings representing the names extracted from the text.\n",
      "        \"\"\"\n",
      "        names = [ent.text for ent in self.doc.ents if ent.label_ == 'PERSON']\n",
      "        return names\n",
      "\n",
      "    def extract_emails(self):\n",
      "        \"\"\"\n",
      "        Extract email addresses from a given string.\n",
      "\n",
      "        Args:\n",
      "            text (str): The string from which to extract email addresses.\n",
      "\n",
      "        Returns:\n",
      "            list: A list containing all the extracted email addresses.\n",
      "        \"\"\"\n",
      "        email_pattern = r'\\b[A-Za-z0-9._%+-]+@[A-Za-z0-9.-]+\\.[A-Za-z]{2,}\\b'\n",
      "        emails = re.findall(email_pattern, self.text)\n",
      "        return emails\n",
      "\n",
      "    def extract_phone_numbers(self):\n",
      "        \"\"\"\n",
      "        Extract phone numbers from a given string.\n",
      "\n",
      "        Args:\n",
      "            text (str): The string from which to extract phone numbers.\n",
      "\n",
      "        Returns:\n",
      "            list: A list containing all the extracted phone numbers.\n",
      "        \"\"\"\n",
      "        phone_number_pattern = r'^(\\+\\d{1,3})?[-.\\s]?\\(?\\d{3}\\)?[-.\\s]?\\d{3}[-.\\s]?\\d{4}$'\n",
      "        phone_numbers = re.findall(phone_number_pattern, self.text)\n",
      "        return phone_numbers\n",
      "\n",
      "    def extract_experience(self):\n",
      "        \"\"\"\n",
      "        Extract experience from a given string. It does so by using the Spacy module.\n",
      "\n",
      "        Args:\n",
      "            text (str): The string from which to extract experience.\n",
      "\n",
      "        Returns:\n",
      "            str: A string containing all the extracted experience.\n",
      "        \"\"\"\n",
      "        experience_section = []\n",
      "        in_experience_section = False\n",
      "\n",
      "        for token in self.doc:\n",
      "            if token.text in RESUME_SECTIONS:\n",
      "                if token.text == 'Experience' or 'EXPERIENCE' or 'experience':\n",
      "                    in_experience_section = True\n",
      "                else:\n",
      "                    in_experience_section = False\n",
      "\n",
      "            if in_experience_section:\n",
      "                experience_section.append(token.text)\n",
      "\n",
      "        return ' '.join(experience_section)\n",
      "\n",
      "    def extract_position_year(self):\n",
      "        \"\"\"\n",
      "            Extract position and year from a given string.\n",
      "\n",
      "            Args:\n",
      "                text (str): The string from which to extract position and year.\n",
      "\n",
      "            Returns:\n",
      "                list: A list containing the extracted position and year.\n",
      "        \"\"\"\n",
      "        position_year_search_pattern = r\"(\\b\\w+\\b\\s+\\b\\w+\\b),\\s+(\\d{4})\\s*-\\s*(\\d{4}|\\bpresent\\b)\"\n",
      "        position_year = re.findall(\n",
      "            position_year_search_pattern, self.text)\n",
      "        return position_year\n",
      "\n",
      "    def extract_particular_words(self):\n",
      "        \"\"\"\n",
      "        Extract nouns and proper nouns from the given text.\n",
      "\n",
      "        Args:\n",
      "            text (str): The input text to extract nouns from.\n",
      "\n",
      "        Returns:\n",
      "            list: A list of extracted nouns.\n",
      "        \"\"\"\n",
      "        pos_tags = ['NOUN', 'PROPN']\n",
      "        nouns = [token.text for token in self.doc if token.pos_ in pos_tags]\n",
      "        return nouns\n",
      "\n",
      "    def extract_entities(self):\n",
      "        \"\"\"\n",
      "        Extract named entities of types 'GPE' (geopolitical entity) and 'ORG' (organization) from the given text.\n",
      "\n",
      "        Args:\n",
      "            text (str): The input text to extract entities from.\n",
      "\n",
      "        Returns:\n",
      "            list: A list of extracted entities.\n",
      "        \"\"\"\n",
      "        entity_labels = ['GPE', 'ORG']\n",
      "        entities = [\n",
      "            token.text for token in self.doc.ents if token.label_ in entity_labels]\n",
      "        return entities\n",
      "\n",
      "import streamlit as st\n",
      "import pandas as pd\n",
      "import json\n",
      "import plotly.express as px\n",
      "import plotly.graph_objects as go\n",
      "import matplotlib.pyplot as plt\n",
      "from scripts.utils.ReadFiles import get_filenames_from_dir\n",
      "from scripts.ResumeProcessor import ResumeProcessor\n",
      "import time\n",
      "\n",
      "\n",
      "def read_json(filename):\n",
      "    with open(filename) as f:\n",
      "        data = json.load(f)\n",
      "    return data\n",
      "\n",
      "\n",
      "st.image('Assets/img/header_image.jpg')\n",
      "\n",
      "\n",
      "resume_names = get_filenames_from_dir(\"Data/Processed/Resumes\")\n",
      "\n",
      "st.write(\"There are\", len(resume_names),\n",
      "         \" resumes present. Please select one from the menu below:\")\n",
      "output = st.slider('Select Resume Number', 0, 5, 2)\n",
      "st.write(\"You have selected \", resume_names[output], \" printing the resume\")\n",
      "selected_file = read_json(\"Data/Processed/Resumes/\"+resume_names[output])\n",
      "st.json(selected_file)\n",
      "\n",
      "\n",
      "# # read the json file\n",
      "# resume = read_json(\n",
      "#     'Data/Processed/Resume-d531571e-e4fa-45eb-ab6a-267cdeb6647e.json')\n",
      "# job_desc = read_json(\n",
      "#     'Data/Processed/Job-Desc-a4f06ccb-8d5a-4d0b-9f02-3ba6d686472e.json')\n",
      "\n",
      "# st.write(\"#### Your Resume\")\n",
      "# st.caption(\"This is how your resume looks after PDF Parsing:\")\n",
      "\n",
      "# st.json(resume[\"clean_data\"])\n",
      "\n",
      "import streamlit as st\n",
      "import pandas as pd\n",
      "import json\n",
      "import plotly.express as px\n",
      "import plotly.graph_objects as go\n",
      "import matplotlib.pyplot as plt\n",
      "from scripts.utils.ReadFiles import get_filenames_from_dir\n",
      "from scripts.ResumeProcessor import ResumeProcessor\n",
      "import time\n",
      "from streamlit_extras import add_vertical_space as avs\n",
      "\n",
      "\n",
      "def read_json(filename):\n",
      "    with open(filename) as f:\n",
      "        data = json.load(f)\n",
      "    return data\n",
      "\n",
      "\n",
      "st.image('Assets/img/header_image.jpg')\n",
      "\n",
      "avs.add_vertical_space(5)\n",
      "\n",
      "resume_names = get_filenames_from_dir(\"Data/Processed/Resumes\")\n",
      "\n",
      "st.write(\"There are\", len(resume_names),\n",
      "         \" resumes present. Please select one from the menu below:\")\n",
      "output = st.slider('Select Resume Number', 0, 4, 2)\n",
      "\n",
      "avs.add_vertical_space(5)\n",
      "\n",
      "st.write(\"You have selected \", resume_names[output], \" printing the resume\")\n",
      "selected_file = read_json(\"Data/Processed/Resumes/\"+resume_names[output])\n",
      "\n",
      "avs.add_vertical_space(5)\n",
      "st.json(selected_file)\n",
      "\n",
      "\n",
      "# # read the json file\n",
      "# resume = read_json(\n",
      "#     'Data/Processed/Resume-d531571e-e4fa-45eb-ab6a-267cdeb6647e.json')\n",
      "# job_desc = read_json(\n",
      "#     'Data/Processed/Job-Desc-a4f06ccb-8d5a-4d0b-9f02-3ba6d686472e.json')\n",
      "\n",
      "# st.write(\"#### Your Resume\")\n",
      "# st.caption(\"This is how your resume looks after PDF Parsing:\")\n",
      "\n",
      "# st.json(resume[\"clean_data\"])\n",
      "\n",
      "import re\n",
      "import urllib.request\n",
      "import spacy\n",
      "from scripts.utils.Utils import TextCleaner\n",
      "\n",
      "\n",
      "# Load the English model\n",
      "nlp = spacy.load('en_core_web_sm')\n",
      "\n",
      "\n",
      "RESUME_SECTIONS = [\n",
      "    'Contact Information',\n",
      "    'Objective',\n",
      "    'Summary',\n",
      "    'Education',\n",
      "    'Experience',\n",
      "    'Skills',\n",
      "    'Projects',\n",
      "    'Certifications',\n",
      "    'Licenses',\n",
      "    'Awards',\n",
      "    'Honors',\n",
      "    'Publications',\n",
      "    'References',\n",
      "    'Technical Skills',\n",
      "    'Computer Skills',\n",
      "    'Programming Languages',\n",
      "    'Software Skills',\n",
      "    'Soft Skills',\n",
      "    'Language Skills',\n",
      "    'Professional Skills',\n",
      "    'Transferable Skills',\n",
      "    'Work Experience',\n",
      "    'Professional Experience',\n",
      "    'Employment History',\n",
      "    'Internship Experience',\n",
      "    'Volunteer Experience',\n",
      "    'Leadership Experience',\n",
      "    'Research Experience',\n",
      "    'Teaching Experience'\n",
      "]\n",
      "\n",
      "\n",
      "class DataExtractor:\n",
      "    \"\"\"\n",
      "    A class for extracting various types of data from text.\n",
      "    \"\"\"\n",
      "\n",
      "    def __init__(self, raw_text: str):\n",
      "        \"\"\"\n",
      "        Initialize the DataExtractor object.\n",
      "\n",
      "        Args:\n",
      "            raw_text (str): The raw input text.\n",
      "        \"\"\"\n",
      "\n",
      "        self.text = raw_text\n",
      "        self.clean_text = TextCleaner.clean_text(self.text)\n",
      "        self.doc = nlp(self.clean_text)\n",
      "\n",
      "    def extract_links(self):\n",
      "        \"\"\"\n",
      "        Find links of any type in a given string.\n",
      "\n",
      "        Args:\n",
      "            text (str): The string to search for links.\n",
      "\n",
      "        Returns:\n",
      "            list: A list containing all the found links.\n",
      "        \"\"\"\n",
      "        link_pattern = r'\\b(?:https?://|www\\.)\\S+\\b'\n",
      "        links = re.findall(link_pattern, self.text)\n",
      "        return links\n",
      "\n",
      "    def extract_links_extended(self):\n",
      "        \"\"\"\n",
      "        Extract links of all kinds (HTTP, HTTPS, FTP, email, www.linkedin.com,\n",
      "          and github.com/user_name) from a webpage.\n",
      "\n",
      "        Args:\n",
      "            url (str): The URL of the webpage.\n",
      "\n",
      "        Returns:\n",
      "            list: A list containing all the extracted links.\n",
      "        \"\"\"\n",
      "        links = []\n",
      "        try:\n",
      "            response = urllib.request.urlopen(self.text)\n",
      "            html_content = response.read().decode('utf-8')\n",
      "            pattern = r'href=[\\'\"]?([^\\'\" >]+)'\n",
      "            raw_links = re.findall(pattern, html_content)\n",
      "            for link in raw_links:\n",
      "                if link.startswith(('http://', 'https://', 'ftp://', 'mailto:',\n",
      "                                    'www.linkedin.com', 'github.com/', 'twitter.com')):\n",
      "                    links.append(link)\n",
      "        except Exception as e:\n",
      "            print(f\"Error extracting links: {str(e)}\")\n",
      "        return links\n",
      "\n",
      "    def extract_names(self):\n",
      "        \"\"\"Extracts and returns a list of names from the given \n",
      "        text using spaCy's named entity recognition.\n",
      "\n",
      "        Args:\n",
      "            text (str): The text to extract names from.\n",
      "\n",
      "        Returns:\n",
      "            list: A list of strings representing the names extracted from the text.\n",
      "        \"\"\"\n",
      "        names = [ent.text for ent in self.doc.ents if ent.label_ == 'PERSON']\n",
      "        return names\n",
      "\n",
      "    def extract_emails(self):\n",
      "        \"\"\"\n",
      "        Extract email addresses from a given string.\n",
      "\n",
      "        Args:\n",
      "            text (str): The string from which to extract email addresses.\n",
      "\n",
      "        Returns:\n",
      "            list: A list containing all the extracted email addresses.\n",
      "        \"\"\"\n",
      "        email_pattern = r'\\b[A-Za-z0-9._%+-]+@[A-Za-z0-9.-]+\\.[A-Za-z]{2,}\\b'\n",
      "        emails = re.findall(email_pattern, self.text)\n",
      "        return emails\n",
      "\n",
      "    def extract_phone_numbers(self):\n",
      "        \"\"\"\n",
      "        Extract phone numbers from a given string.\n",
      "\n",
      "        Args:\n",
      "            text (str): The string from which to extract phone numbers.\n",
      "\n",
      "        Returns:\n",
      "            list: A list containing all the extracted phone numbers.\n",
      "        \"\"\"\n",
      "        phone_number_pattern = r'^(\\+\\d{1,3})?[-.\\s]?\\(?\\d{3}\\)?[-.\\s]?\\d{3}[-.\\s]?\\d{4}$'\n",
      "        phone_numbers = re.findall(phone_number_pattern, self.text)\n",
      "        return phone_numbers\n",
      "\n",
      "    def extract_experience(self):\n",
      "        \"\"\"\n",
      "        Extract experience from a given string. It does so by using the Spacy module.\n",
      "\n",
      "        Args:\n",
      "            text (str): The string from which to extract experience.\n",
      "\n",
      "        Returns:\n",
      "            str: A string containing all the extracted experience.\n",
      "        \"\"\"\n",
      "        experience_section = []\n",
      "        in_experience_section = False\n",
      "\n",
      "        for token in self.doc:\n",
      "            if token.text in RESUME_SECTIONS:\n",
      "                if token.text == 'Experience' or 'EXPERIENCE' or 'experience':\n",
      "                    in_experience_section = True\n",
      "                else:\n",
      "                    in_experience_section = False\n",
      "\n",
      "            if in_experience_section:\n",
      "                experience_section.append(token.text)\n",
      "\n",
      "        return ' '.join(experience_section)\n",
      "\n",
      "    def extract_position_year(self):\n",
      "        \"\"\"\n",
      "            Extract position and year from a given string.\n",
      "\n",
      "            Args:\n",
      "                text (str): The string from which to extract position and year.\n",
      "\n",
      "            Returns:\n",
      "                list: A list containing the extracted position and year.\n",
      "        \"\"\"\n",
      "        position_year_search_pattern = r\"(\\b\\w+\\b\\s+\\b\\w+\\b),\\s+(\\d{4})\\s*-\\s*(\\d{4}|\\bpresent\\b)\"\n",
      "        position_year = re.findall(\n",
      "            position_year_search_pattern, self.text)\n",
      "        return position_year\n",
      "\n",
      "    def extract_particular_words(self):\n",
      "        \"\"\"\n",
      "        Extract nouns and proper nouns from the given text.\n",
      "\n",
      "        Args:\n",
      "            text (str): The input text to extract nouns from.\n",
      "\n",
      "        Returns:\n",
      "            list: A list of extracted nouns.\n",
      "        \"\"\"\n",
      "        pos_tags = ['NOUN', 'PROPN']\n",
      "        nouns = [token.text for token in self.doc if token.pos_ in pos_tags]\n",
      "        return nouns\n",
      "\n",
      "    def extract_entities(self):\n",
      "        \"\"\"\n",
      "        Extract named entities of types 'GPE' (geopolitical entity) and 'ORG' (organization) from the given text.\n",
      "\n",
      "        Args:\n",
      "            text (str): The input text to extract entities from.\n",
      "\n",
      "        Returns:\n",
      "            list: A list of extracted entities.\n",
      "        \"\"\"\n",
      "        entity_labels = ['GPE', 'ORG']\n",
      "        entities = [\n",
      "            token.text for token in self.doc.ents if token.label_ in entity_labels]\n",
      "        return list(set(entities))\n",
      "\n",
      "from plotly.offline import plot\n",
      "from pyvis.network import Network\n",
      "import networkx as nx\n",
      "from typing import List\n",
      "import streamlit as st\n",
      "import pandas as pd\n",
      "import json\n",
      "import plotly.express as px\n",
      "import plotly.graph_objects as go\n",
      "import matplotlib\n",
      "import matplotlib.pyplot as plt\n",
      "from scripts.utils.ReadFiles import get_filenames_from_dir\n",
      "from scripts.ResumeProcessor import ResumeProcessor\n",
      "import time\n",
      "from streamlit_extras import add_vertical_space as avs\n",
      "from annotated_text import annotated_text, parameters\n",
      "import nltk\n",
      "matplotlib.use('TkAgg')\n",
      "\n",
      "parameters.SHOW_LABEL_SEPARATOR = False\n",
      "parameters.BORDER_RADIUS = 3\n",
      "parameters.PADDING = \"0.5 0.25rem\"\n",
      "\n",
      "\n",
      "def create_annotated_text(input_string: str, word_list: List[str], annotation: str, color_code: str):\n",
      "    # Tokenize the input string\n",
      "    tokens = nltk.word_tokenize(input_string)\n",
      "\n",
      "    # Convert the list to a set for quick lookups\n",
      "    word_set = set(word_list)\n",
      "\n",
      "    # Initialize an empty list to hold the annotated text\n",
      "    annotated_text = []\n",
      "\n",
      "    for token in tokens:\n",
      "        # Check if the token is in the set\n",
      "        if token in word_set:\n",
      "            # If it is, append a tuple with the token, annotation, and color code\n",
      "            annotated_text.append((token, annotation, color_code))\n",
      "        else:\n",
      "            # If it's not, just append the token as a string\n",
      "            annotated_text.append(token)\n",
      "\n",
      "    return annotated_text\n",
      "\n",
      "\n",
      "# # Test the function\n",
      "# input_string = \"Alfred Pennyworth is a seasoned Product Manager in Silicon Valley, CA, USA.\"\n",
      "# word_list = [\"Alfred\", \"Pennyworth\", \"Product\",\n",
      "#              \"Manager\", \"Silicon\", \"Valley\", \"CA\", \"USA\"]\n",
      "# annotation = \"Adj\"\n",
      "# color_code = \"#faa\"\n",
      "\n",
      "# print(annotate_text(input_string, word_list, annotation, color_code))\n",
      "\n",
      "\n",
      "def read_json(filename):\n",
      "    with open(filename) as f:\n",
      "        data = json.load(f)\n",
      "    return data\n",
      "\n",
      "\n",
      "def tokenize_string(input_string):\n",
      "    tokens = nltk.word_tokenize(input_string)\n",
      "    return tokens\n",
      "\n",
      "\n",
      "st.image('Assets/img/header_image.jpg')\n",
      "\n",
      "avs.add_vertical_space(5)\n",
      "\n",
      "resume_names = get_filenames_from_dir(\"Data/Processed/Resumes\")\n",
      "\n",
      "st.write(\"There are\", len(resume_names),\n",
      "         \" resumes present. Please select one from the menu below:\")\n",
      "output = st.slider('Select Resume Number', 0, len(resume_names)-1, 2)\n",
      "\n",
      "avs.add_vertical_space(5)\n",
      "\n",
      "st.write(\"You have selected \", resume_names[output], \" printing the resume\")\n",
      "selected_file = read_json(\"Data/Processed/Resumes/\"+resume_names[output])\n",
      "\n",
      "avs.add_vertical_space(2)\n",
      "st.markdown(\"#### Parsed Resume Data\")\n",
      "st.caption(\n",
      "    \"This text is parsed from your resume. This is how it'll look like after getting parsed by an ATS.\")\n",
      "st.caption(\"Utilize this to understand how to make your resume ATS friendly.\")\n",
      "avs.add_vertical_space(3)\n",
      "# st.json(selected_file)\n",
      "st.write(selected_file[\"clean_data\"])\n",
      "\n",
      "avs.add_vertical_space(3)\n",
      "st.write(\"Now let's take a look at the extracted keywords from the resume.\")\n",
      "\n",
      "annotated_text(create_annotated_text(\n",
      "    selected_file[\"clean_data\"], selected_file[\"extracted_keywords\"],\n",
      "    \"KW\", \"#0B666A\"))\n",
      "\n",
      "avs.add_vertical_space(5)\n",
      "st.write(\"Now let's take a look at the extracted entities from the resume.\")\n",
      "\n",
      "\n",
      "def create_star_graph(nodes_and_weights):\n",
      "    # Create an empty graph\n",
      "    G = nx.Graph()\n",
      "\n",
      "    # Add the central node\n",
      "    central_node = \"resume\"\n",
      "    G.add_node(central_node)\n",
      "\n",
      "    # Add nodes and edges with weights to the graph\n",
      "    for node, weight in nodes_and_weights:\n",
      "        G.add_node(node)\n",
      "        G.add_edge(central_node, node, weight=weight*100)\n",
      "\n",
      "    # Get position layout for nodes\n",
      "    pos = nx.spring_layout(G)\n",
      "\n",
      "    # Create edge trace\n",
      "    edge_x = []\n",
      "    edge_y = []\n",
      "    for edge in G.edges():\n",
      "        x0, y0 = pos[edge[0]]\n",
      "        x1, y1 = pos[edge[1]]\n",
      "        edge_x.extend([x0, x1, None])\n",
      "        edge_y.extend([y0, y1, None])\n",
      "\n",
      "    edge_trace = go.Scatter(x=edge_x, y=edge_y, line=dict(\n",
      "        width=0.5, color='#888'), hoverinfo='none', mode='lines')\n",
      "\n",
      "    # Create node trace\n",
      "    node_x = []\n",
      "    node_y = []\n",
      "    for node in G.nodes():\n",
      "        x, y = pos[node]\n",
      "        node_x.append(x)\n",
      "        node_y.append(y)\n",
      "\n",
      "    node_trace = go.Scatter(x=node_x, y=node_y, mode='markers', hoverinfo='text',\n",
      "                            marker=dict(showscale=True, colorscale='Rainbow', reversescale=True, color=[], size=10,\n",
      "                                        colorbar=dict(thickness=15, title='Node Connections', xanchor='left',\n",
      "                                                      titleside='right'), line_width=2))\n",
      "\n",
      "    # Color node points by number of connections\n",
      "    node_adjacencies = []\n",
      "    node_text = []\n",
      "    for node in G.nodes():\n",
      "        adjacencies = list(G.adj[node])  # changes here\n",
      "        node_adjacencies.append(len(adjacencies))\n",
      "        node_text.append(f'{node}<br># of connections: {len(adjacencies)}')\n",
      "\n",
      "    node_trace.marker.color = node_adjacencies\n",
      "    node_trace.text = node_text\n",
      "\n",
      "    # Create the figure\n",
      "    fig = go.Figure(data=[edge_trace, node_trace],\n",
      "                    layout=go.Layout(title='Entities Linked to your Resume', titlefont_size=16, showlegend=False,\n",
      "                                     hovermode='closest', margin=dict(b=20, l=5, r=5, t=40),\n",
      "                                     xaxis=dict(\n",
      "                                         showgrid=False, zeroline=False, showticklabels=False),\n",
      "                                     yaxis=dict(showgrid=False, zeroline=False, showticklabels=False)))\n",
      "\n",
      "    # Show the figure\n",
      "    st.plotly_chart(fig)\n",
      "\n",
      "\n",
      "# Your list of nodes and weights\n",
      "nodes_and_weights = [\n",
      "    [\"Software Development Engineer\", 0.2532879474096427],\n",
      "    [\"USA\", 0.0908144765704578],\n",
      "    [\"product\", 0.06002611145320178],\n",
      "    [\"software\", 0.022530132055106372],\n",
      "    [\"experience\", 0.02143816867647363],\n",
      "    [\"management\", 0.02074385569284429],\n",
      "    [\"development\", 0.01766344418212818],\n",
      "    [\"user experience\", 0.017386580372690356],\n",
      "    [\"Manager\", 0.011915797594707535],\n",
      "    [\"Python\", 0.011856462684933523],\n",
      "    [\"SQL\", 0.011767339129786123],\n",
      "    [\"Project\", 0.011743798464230066],\n",
      "    [\"team\", 0.011515715597603877],\n",
      "    [\"application\", 0.0100341792709379],\n",
      "    [\"scalable\", 0.00737109583375892],\n",
      "    [\"Java\", 0.006971290228384053],\n",
      "    [\"AWS\", 0.006933520895706676],\n",
      "    [\"Apple\", 0.006866594203670648],\n",
      "    [\"leadership\", 0.006827491537536502],\n",
      "    [\"exceptional\", 0.0067843320435271096]\n",
      "]\n",
      "\n",
      "# Call the function with your data\n",
      "create_star_graph(nodes_and_weights)\n",
      "\n",
      "df2 = pd.DataFrame(selected_file['keyterms'], columns=[\"keyword\", \"value\"])\n",
      "\n",
      "# Create the dictionary\n",
      "keyword_dict = {}\n",
      "for keyword, value in selected_file['keyterms']:\n",
      "    keyword_dict[keyword] = value*100\n",
      "\n",
      "fig = go.Figure(data=[go.Table(header=dict(values=[\"Keyword\", \"Value\"],\n",
      "                                           font=dict(size=12),\n",
      "                                           fill_color='#070A52'),\n",
      "                               cells=dict(values=[list(keyword_dict.keys()),\n",
      "                                                  list(keyword_dict.values())],\n",
      "                                          line_color='darkslategray',\n",
      "                                          fill_color='#6DA9E4'))\n",
      "                      ])\n",
      "st.plotly_chart(fig)\n",
      "\n",
      "st.divider()\n",
      "\n",
      "fig = px.treemap(df2, path=['keyword'], values='value',\n",
      "                 color_continuous_scale='Rainbow',\n",
      "                 title='Key Terms/Topics Extracted from your Resume')\n",
      "st.write(fig)\n",
      "\n",
      "import json\n",
      "from scripts.utils.ReadFiles import get_filenames_from_dir\n",
      "from scripts.ResumeProcessor import ResumeProcessor\n",
      "from scripts.JobDescriptionProcessor import JobDescriptionProcessor\n",
      "import logging\n",
      "\n",
      "logging.basicConfig(filename='app.log', filemode='w',\n",
      "                    level=logging.DEBUG,\n",
      "                    format='%(name)s - %(levelname)s - %(message)s')\n",
      "\n",
      "\n",
      "def read_json(filename):\n",
      "    with open(filename) as f:\n",
      "        data = json.load(f)\n",
      "    return data\n",
      "\n",
      "\n",
      "logging.info('Started to read from Data/Resumes')\n",
      "try:\n",
      "    # Check if there are resumes present or not.\n",
      "    # If present then parse it.\n",
      "    file_names = get_filenames_from_dir(\"Data/Resumes\")\n",
      "    logging.info('Reading from Data/Resumes is now complete.')\n",
      "except:\n",
      "    # Exit the program if there are no resumes.\n",
      "    logging.error('There are no resumes present in the specified folder.')\n",
      "    logging.error('Exiting from the program.')\n",
      "    logging.error(\n",
      "        'Please add resumes in the Data/Resumes folder and try again.')\n",
      "    exit(1)\n",
      "\n",
      "# Now after getting the file_names parse the resumes into a JSON Format.\n",
      "logging.info('Started parsing the resumes.')\n",
      "for file in file_names:\n",
      "    processor = ResumeProcessor(file)\n",
      "    success = processor.process()\n",
      "logging.info('Parsing of the resumes is now complete.')\n",
      "\n",
      "logging.info('Started to read from Data/JobDescription')\n",
      "try:\n",
      "    # Check if there are resumes present or not.\n",
      "    # If present then parse it.\n",
      "    file_names = get_filenames_from_dir(\"Data/JobDescription\")\n",
      "    logging.info('Reading from Data/JobDescription is now complete.')\n",
      "except:\n",
      "    # Exit the program if there are no resumes.\n",
      "    logging.error(\n",
      "        'There are no job-description present in the specified folder.')\n",
      "    logging.error('Exiting from the program.')\n",
      "    logging.error(\n",
      "        'Please add resumes in the Data/JobDescription folder and try again.')\n",
      "    exit(1)\n",
      "\n",
      "# Now after getting the file_names parse the resumes into a JSON Format.\n",
      "logging.info('Started parsing the Job Descriptions.')\n",
      "for file in file_names:\n",
      "    processor = JobDescriptionProcessor(file)\n",
      "    success = processor.process()\n",
      "logging.info('Parsing of the Job Descriptions is now complete.')\n",
      "logging.info('Success now run `streamlit run streamlit_second.py`')\n",
      "\n",
      "from plotly.offline import plot\n",
      "from pyvis.network import Network\n",
      "import networkx as nx\n",
      "from typing import List\n",
      "import streamlit as st\n",
      "import pandas as pd\n",
      "import json\n",
      "import plotly.express as px\n",
      "import plotly.graph_objects as go\n",
      "import matplotlib\n",
      "import matplotlib.pyplot as plt\n",
      "from scripts.utils.ReadFiles import get_filenames_from_dir\n",
      "from scripts.ResumeProcessor import ResumeProcessor\n",
      "import time\n",
      "from streamlit_extras import add_vertical_space as avs\n",
      "from annotated_text import annotated_text, parameters\n",
      "import nltk\n",
      "matplotlib.use('TkAgg')\n",
      "\n",
      "parameters.SHOW_LABEL_SEPARATOR = False\n",
      "parameters.BORDER_RADIUS = 3\n",
      "parameters.PADDING = \"0.5 0.25rem\"\n",
      "\n",
      "\n",
      "def create_star_graph(nodes_and_weights, title):\n",
      "    # Create an empty graph\n",
      "    G = nx.Graph()\n",
      "\n",
      "    # Add the central node\n",
      "    central_node = \"resume\"\n",
      "    G.add_node(central_node)\n",
      "\n",
      "    # Add nodes and edges with weights to the graph\n",
      "    for node, weight in nodes_and_weights:\n",
      "        G.add_node(node)\n",
      "        G.add_edge(central_node, node, weight=weight*100)\n",
      "\n",
      "    # Get position layout for nodes\n",
      "    pos = nx.spring_layout(G)\n",
      "\n",
      "    # Create edge trace\n",
      "    edge_x = []\n",
      "    edge_y = []\n",
      "    for edge in G.edges():\n",
      "        x0, y0 = pos[edge[0]]\n",
      "        x1, y1 = pos[edge[1]]\n",
      "        edge_x.extend([x0, x1, None])\n",
      "        edge_y.extend([y0, y1, None])\n",
      "\n",
      "    edge_trace = go.Scatter(x=edge_x, y=edge_y, line=dict(\n",
      "        width=0.5, color='#888'), hoverinfo='none', mode='lines')\n",
      "\n",
      "    # Create node trace\n",
      "    node_x = []\n",
      "    node_y = []\n",
      "    for node in G.nodes():\n",
      "        x, y = pos[node]\n",
      "        node_x.append(x)\n",
      "        node_y.append(y)\n",
      "\n",
      "    node_trace = go.Scatter(x=node_x, y=node_y, mode='markers', hoverinfo='text',\n",
      "                            marker=dict(showscale=True, colorscale='Rainbow', reversescale=True, color=[], size=10,\n",
      "                                        colorbar=dict(thickness=15, title='Node Connections', xanchor='left',\n",
      "                                                      titleside='right'), line_width=2))\n",
      "\n",
      "    # Color node points by number of connections\n",
      "    node_adjacencies = []\n",
      "    node_text = []\n",
      "    for node in G.nodes():\n",
      "        adjacencies = list(G.adj[node])  # changes here\n",
      "        node_adjacencies.append(len(adjacencies))\n",
      "        node_text.append(f'{node}<br># of connections: {len(adjacencies)}')\n",
      "\n",
      "    node_trace.marker.color = node_adjacencies\n",
      "    node_trace.text = node_text\n",
      "\n",
      "    # Create the figure\n",
      "    fig = go.Figure(data=[edge_trace, node_trace],\n",
      "                    layout=go.Layout(title=title, titlefont_size=16, showlegend=False,\n",
      "                                     hovermode='closest', margin=dict(b=20, l=5, r=5, t=40),\n",
      "                                     xaxis=dict(\n",
      "                                         showgrid=False, zeroline=False, showticklabels=False),\n",
      "                                     yaxis=dict(showgrid=False, zeroline=False, showticklabels=False)))\n",
      "\n",
      "    # Show the figure\n",
      "    st.plotly_chart(fig)\n",
      "\n",
      "\n",
      "def create_annotated_text(input_string: str, word_list: List[str], annotation: str, color_code: str):\n",
      "    # Tokenize the input string\n",
      "    tokens = nltk.word_tokenize(input_string)\n",
      "\n",
      "    # Convert the list to a set for quick lookups\n",
      "    word_set = set(word_list)\n",
      "\n",
      "    # Initialize an empty list to hold the annotated text\n",
      "    annotated_text = []\n",
      "\n",
      "    for token in tokens:\n",
      "        # Check if the token is in the set\n",
      "        if token in word_set:\n",
      "            # If it is, append a tuple with the token, annotation, and color code\n",
      "            annotated_text.append((token, annotation, color_code))\n",
      "        else:\n",
      "            # If it's not, just append the token as a string\n",
      "            annotated_text.append(token)\n",
      "\n",
      "    return annotated_text\n",
      "\n",
      "\n",
      "def read_json(filename):\n",
      "    with open(filename) as f:\n",
      "        data = json.load(f)\n",
      "    return data\n",
      "\n",
      "\n",
      "def tokenize_string(input_string):\n",
      "    tokens = nltk.word_tokenize(input_string)\n",
      "    return tokens\n",
      "\n",
      "\n",
      "st.image('Assets/img/header_image.jpg')\n",
      "\n",
      "avs.add_vertical_space(5)\n",
      "\n",
      "resume_names = get_filenames_from_dir(\"Data/Processed/Resumes\")\n",
      "\n",
      "st.write(\"There are\", len(resume_names),\n",
      "         \" resumes present. Please select one from the menu below:\")\n",
      "output = st.slider('Select Resume Number', 0, len(resume_names)-1, 2)\n",
      "\n",
      "avs.add_vertical_space(5)\n",
      "\n",
      "st.write(\"You have selected \", resume_names[output], \" printing the resume\")\n",
      "selected_file = read_json(\"Data/Processed/Resumes/\"+resume_names[output])\n",
      "\n",
      "avs.add_vertical_space(2)\n",
      "st.markdown(\"#### Parsed Resume Data\")\n",
      "st.caption(\n",
      "    \"This text is parsed from your resume. This is how it'll look like after getting parsed by an ATS.\")\n",
      "st.caption(\"Utilize this to understand how to make your resume ATS friendly.\")\n",
      "avs.add_vertical_space(3)\n",
      "# st.json(selected_file)\n",
      "st.write(selected_file[\"clean_data\"])\n",
      "\n",
      "avs.add_vertical_space(3)\n",
      "st.write(\"Now let's take a look at the extracted keywords from the resume.\")\n",
      "\n",
      "annotated_text(create_annotated_text(\n",
      "    selected_file[\"clean_data\"], selected_file[\"extracted_keywords\"],\n",
      "    \"KW\", \"#0B666A\"))\n",
      "\n",
      "avs.add_vertical_space(5)\n",
      "st.write(\"Now let's take a look at the extracted entities from the resume.\")\n",
      "\n",
      "# Call the function with your data\n",
      "create_star_graph(selected_file['keyterms'], \"Entities from Resume\")\n",
      "\n",
      "df2 = pd.DataFrame(selected_file['keyterms'], columns=[\"keyword\", \"value\"])\n",
      "\n",
      "# Create the dictionary\n",
      "keyword_dict = {}\n",
      "for keyword, value in selected_file['keyterms']:\n",
      "    keyword_dict[keyword] = value*100\n",
      "\n",
      "fig = go.Figure(data=[go.Table(header=dict(values=[\"Keyword\", \"Value\"],\n",
      "                                           font=dict(size=12),\n",
      "                                           fill_color='#070A52'),\n",
      "                               cells=dict(values=[list(keyword_dict.keys()),\n",
      "                                                  list(keyword_dict.values())],\n",
      "                                          line_color='darkslategray',\n",
      "                                          fill_color='#6DA9E4'))\n",
      "                      ])\n",
      "st.plotly_chart(fig)\n",
      "\n",
      "st.divider()\n",
      "\n",
      "fig = px.treemap(df2, path=['keyword'], values='value',\n",
      "                 color_continuous_scale='Rainbow',\n",
      "                 title='Key Terms/Topics Extracted from your Resume')\n",
      "st.write(fig)\n",
      "\n",
      "avs.add_vertical_space(5)\n",
      "\n",
      "job_descriptions = get_filenames_from_dir(\"Data/Processed/JobDescription\")\n",
      "\n",
      "st.write(\"There are\", len(job_descriptions),\n",
      "         \" resumes present. Please select one from the menu below:\")\n",
      "output = st.slider('Select Job Description Number',\n",
      "                   0, len(job_descriptions)-1, 2)\n",
      "\n",
      "avs.add_vertical_space(5)\n",
      "\n",
      "st.write(\"You have selected \",\n",
      "         job_descriptions[output], \" printing the job description\")\n",
      "selected_jd = read_json(\n",
      "    \"Data/Processed/JobDescription/\"+job_descriptions[output])\n",
      "\n",
      "avs.add_vertical_space(2)\n",
      "st.markdown(\"#### Job Description\")\n",
      "st.caption(\n",
      "    \"Currently in the pipeline I'm parsing this from PDF but it'll be from txt or copy paste.\")\n",
      "avs.add_vertical_space(3)\n",
      "# st.json(selected_file)\n",
      "st.write(selected_jd[\"clean_data\"])\n",
      "\n",
      "st.markdown(\"#### Common Words between Job Description and Resumes Highlighted.\")\n",
      "\n",
      "annotated_text(create_annotated_text(\n",
      "    selected_file[\"clean_data\"], selected_jd[\"extracted_keywords\"],\n",
      "    \"JD\", \"#F24C3D\"))\n",
      "\n",
      "st.write(\"Now let's take a look at the extracted entities from the job description.\")\n",
      "\n",
      "# Call the function with your data\n",
      "create_star_graph(selected_jd['keyterms'], \"Entities from Job Description\")\n",
      "\n",
      "df2 = pd.DataFrame(selected_jd['keyterms'], columns=[\"keyword\", \"value\"])\n",
      "\n",
      "# Create the dictionary\n",
      "keyword_dict = {}\n",
      "for keyword, value in selected_jd['keyterms']:\n",
      "    keyword_dict[keyword] = value*100\n",
      "\n",
      "fig = go.Figure(data=[go.Table(header=dict(values=[\"Keyword\", \"Value\"],\n",
      "                                           font=dict(size=12),\n",
      "                                           fill_color='#070A52'),\n",
      "                               cells=dict(values=[list(keyword_dict.keys()),\n",
      "                                                  list(keyword_dict.values())],\n",
      "                                          line_color='darkslategray',\n",
      "                                          fill_color='#6DA9E4'))\n",
      "                      ])\n",
      "st.plotly_chart(fig)\n",
      "\n",
      "st.divider()\n",
      "\n",
      "fig = px.treemap(df2, path=['keyword'], values='value',\n",
      "                 color_continuous_scale='Rainbow',\n",
      "                 title='Key Terms/Topics Extracted from the selected Job Description')\n",
      "st.write(fig)\n",
      "\n",
      "import json\n",
      "from scripts.utils.ReadFiles import get_filenames_from_dir\n",
      "\n",
      "resume_path = \"Data/Processed/Resumes\"\n",
      "job_path = \"Data/Processed/JobDescription\"\n",
      "\n",
      "\n",
      "def read_json(filename):\n",
      "    with open(filename) as f:\n",
      "        data = json.load(f)\n",
      "    return data\n",
      "\n",
      "\n",
      "def build_resume_list(resume_names, path):\n",
      "    resumes = []\n",
      "    for resume in resume_names:\n",
      "        selected_file = read_json(path + '/' + resume)\n",
      "        resumes.append({\n",
      "            \"job_desc\": selected_file[\"clean_data\"]\n",
      "        })\n",
      "    return resumes\n",
      "\n",
      "\n",
      "resume_names = get_filenames_from_dir(job_path)\n",
      "resumes = build_resume_list(resume_names, job_path)\n",
      "\n",
      "print(resumes)  # To see the output.\n",
      "\n",
      "jobs = [{'job_desc': 'Job Description Product Manager 10+ Years of Experience\\nTech Solutions San Francisco CA USA\\nAbout Us\\nTech Solutions is a global leader in the technology industry specializing in the development of cuttingedge\\nsoftware products Were currently looking for a seasoned Product Manager with over 10 years of experience\\nto join our dynamic team\\nJob Description\\nThe Product Manager will be responsible for guiding the success of a product and leading the crossfunctional\\nteam that is responsible for improving it This is an important organizational role that sets the strategy\\nroadmap and feature definition for a product or product line\\nResponsibilities\\nDefine the product strategy and roadmap\\nDeliver MRDs and PRDs with prioritized features and corresponding justification\\nWork with external third parties to assess partnerships and licensing opportunities\\nRun beta and pilot programs with earlystage products and samples\\nBe an expert with respect to the competition\\nAct as a leader within the company\\nDevelop the core positioning and messaging for the product\\nPerform product demos to customers\\nSet pricing to meet revenue and profitability goals\\nRequirements\\n10+ years of experience in product management\\nDemonstrated success defining and launching excellent products\\nExcellent written and verbal communication skills\\nTechnical background with experience in software development\\nExcellent teamwork skills\\nProven ability to influence crossfunctional teams without formal authority\\nMust be able to travel 20\\nBachelors degree MBA preferred\\n1 Benefits\\nCompetitive salary package\\nHealth dental and vision insurance\\nRetirement savings plan\\nProfessional development opportunities\\nFlexible work hours\\nTech Solutions is an equal opportunity employer We celebrate diversity and are committed to creating\\nan inclusive environment for all employees\\nHow to Apply\\nTo apply please submit your resume and a brief explanation of your relevant experience to  \\n2'}, {'job_desc': 'Job Description Senior Full Stack Engineer 5+ Years of\\nExperience\\nTech Solutions San Francisco CA USA\\nAbout Us\\nTech Solutions is a leading technology company that creates innovative solutions across a variety of industries\\nOur mission is to improve lives through advanced technology Were currently seeking a Senior Full Stack\\nEngineer to join our dynamic team\\nJob Description\\nWere looking for a Senior Full Stack Engineer with 5+ years of experience in developing web applications\\nThe successful candidate will have experience working with both frontend and backend technologies and\\nwill be capable of overseeing projects from conception to production deployment\\nResponsibilities\\nDeveloping front end website architecture\\nDesigning user interactions on web pages\\nDeveloping back end website applications\\nCreating servers and databases for functionality\\nEnsuring crossplatform optimization for mobile phones\\nSeeing through a project from conception to finished product\\nDesigning and developing APIs\\nMeeting both technical and consumer needs\\nStaying abreast of developments in web applications and programming languages\\nRequirements\\nDegree in Computer Science or similar field\\n5+ years of experience in web development\\nStrong organizational and project management skills\\nProficiency with fundamental front end languages such as HTML CSS and JavaScript\\nProficiency with serverside languages such as Python Ruby Java PHP and Net\\nFamiliarity with database technology such as MySQL Oracle and MongoDB\\n1 Excellent verbal communication skills\\nGood problemsolving skills\\nAttention to detail\\nBenefits\\nCompetitive salary package\\nHealth dental and vision insurance\\nRetirement savings plan\\nProfessional development opportunities\\nFlexible work hours\\nTech Solutions is an equal opportunity employer and we value diversity at our company\\nHow to Apply\\nTo apply please submit your resume and a brief explanation of your relevant experience to  \\n2'},\n",
      "        {'job_desc': 'Job Description Front End Engineer 2 Years of Experience\\nTech Solutions San Francisco CA USA\\nAbout Us\\nAt Tech Solutions we are on a mission to build products that solve complex problems and improve peoples\\nlives We are seeking a talented Front End Engineer to join our dynamic team in San Francisco\\nJob Description\\nWe are looking for a Front End Engineer with at least 2 years of experience in developing scalable and\\nuserfriendly web applications The successful candidate will be proficient in modern JavaScript frameworks\\nand libraries HTML CSS and responsive design principles This role will contribute significantly to the\\ncreation and implementation of user interfaces for our web applications\\nResponsibilities\\nDevelop new userfacing features using modern JavaScript frameworks like Reactjs Vuejs or Angu\\nlarjs\\nBuild reusable code and libraries for future use\\nEnsure the technical feasibility of UI/UX designs\\nOptimize application for maximum speed and scalability\\nAssure that all user input is validated before submitting to backend services\\nCollaborate with other team members and stakeholders\\nRequirements\\n2 years of experience as a Front End Developer or similar role\\nProficiency in web markup including HTML5 CSS3\\nKnowledge of modern JavaScript programming and experience with libraries like jQuery\\nFamiliarity with modern frontend build pipelines and tools\\nExperience with popular frontend frameworks such as React Vue or Angular\\nFamiliarity with code versioning tools such as Git\\nDegree in Computer Science Engineering or a related field\\n1 Benefits\\nCompetitive salary package\\nHealth dental and vision insurance\\nRetirement savings plan\\nProfessional development opportunities\\nFlexible work hours\\nTech Solutions is proud to be an equal opportunity employer We celebrate diversity and are committed\\nto creating an inclusive environment for all employees\\nHow to Apply\\nTo apply please submit your resume and a brief explanation of your relevant experience to  \\n2'}, {'job_desc': 'Job Description Java Developer 3 Years of Experience\\nTech Solutions San Francisco CA USA\\nAbout Us\\nAt Tech Solutions we believe in the power of technology to solve complex problems We are a dynamic\\nforwardthinking tech company specializing in custom software solutions for various industries We are\\nseeking a talented and experienced Java Developer to join our team\\nJob Description\\nWe are seeking a skilled Java Developer with at least 3 years of experience in building highperforming scal\\nable enterprisegrade applications You will be part of a talented software team that works on missioncritical\\napplications Your roles and responsibilities will include managing Java/Java EE application development\\nwhile providing expertise in the full software development lifecycle\\nResponsibilities\\nDesigning implementing and maintaining Java applications that are often highvolume and low\\nlatency required for missioncritical systems\\nDelivering high availability and performance\\nContributing to all phases of the development lifecycle\\nWriting welldesigned efficient and testable code\\nConducting software analysis programming testing and debugging\\nEnsuring designs comply with specifications\\nPreparing and producing releases of software components\\nSupporting continuous improvement by investigating alternatives and technologies and presenting these\\nfor architectural review\\nRequirements\\nBS/MS degree in Computer Science Engineering or a related subject\\nProven handson Software Development experience\\nProven working experience in Java development\\nHandson experience in designing and developing applications using Java EE platforms\\nObjectOriented Analysis and design using common design patterns\\nProfound insight of Java and JEE internals Classloading Memory Management Transaction man\\nagement etc\\n1 Excellent knowledge of Relational Databases SQL and ORM technologies JPA2 Hibernate\\nExperience in developing web applications using at least one popular web framework JSF Wicket\\nGWT Spring MVC\\nExperience with testdriven development\\nBenefits\\nCompetitive salary package\\nHealth dental and vision insurance\\nRetirement savings plan\\nProfessional development opportunities\\nFlexible work hours\\nTech Solutions is proud to be an equal opportunity employer We celebrate diversity and are committed\\nto creating an inclusive environment for all employees\\nHow to Apply\\nTo apply please submit your resume and a brief explanation of your relevant experience to  \\n2'}]\n",
      "\n",
      "resume_data = [{'resume': 'Barry Allen\\nFrontEnd DeveloperGoogle HQ Mountain View CA USA\\nobilealt\\n/envel\\n/linkedininbwayne\\n\\nObjective\\nSeeking a challenging role as a FrontEnd Developer where I can leverage my knowledge of UI/UX\\ndesign and modern web technologies to create intuitive and engaging user interfaces\\nEducation\\n2018  2022 BTech Computer Science and Engineering Indian Institute of Technology\\nDelhi  New Delhi India\\nOverall GPA 95/10\\nSkills\\nJavaScript ES6+ TypeScript HTML5 CSS3 Python React Redux Angular\\nVuejs Nodejs Expressjs D3js Git Docker Webpack Babel Google Cloud\\nPlatform Firebase RESTful APIs GraphQL Agile Development TestDriven\\nDevelopment Responsive Design UI/UX\\nExperience\\nJune 2022 \\nPresentSoftware Engineer FrontEnd Google  Mountain View CA USA\\nDeveloping intuitive and engaging user interfaces using React and Redux Working closely\\nwith UX designers to implement responsive and accessible web design Participating in\\nagile development processes including sprint planning and code reviews Collaborating with\\nbackend developers to integrate RESTful APIs and ensure seamless data flow\\nProjects\\n2022 Personal Expense Tracker\\nDeveloped a personal expense tracker application using React Redux and Firebase Imple\\nmented user authentication using Firebase Auth and data storage using Firestore Utilized\\nD3js for data visualization to provide users with insights into their spending patterns'}, {'resume': 'Alfred Pennyworth\\nProduct ManagerSilicon Valley CA USA\\nobilealt\\n/envel\\n/linkedininapennyworth\\n\\nProfessional Summary\\nSeasoned Product Manager with over 20 years of experience in software development and product\\nmanagement having worked at all FAANG companies Exceptional leadership skills strategic\\nthinking and a track record of managing products from conception to market success\\nSkills\\nProduct management Agile methodologies Leadership Communication Project\\nmanagement User Experience Design Market Research Data Analysis Java\\nPython JavaScript HTML/CSS SQL AWS\\nExperience\\n2017 \\nPresentProduct Manager Google  Mountain View CA USA\\nLeading crossfunctional teams to design develop and launch innovative products Devel\\noping product strategies and making datadriven decisions to improve user experience and\\nmeet business goals\\n2012  2017 Software Development Engineer III Amazon  Seattle WA USA\\nLed a team of developers in building scalable and highperforming ecommerce applications\\nSuccessfully delivered multiple projects within the stipulated time and budget\\n2007  2012 Software Development Engineer II Apple  Cupertino CA USA\\nDesigned and implemented software components for various Apple services Optimized the\\nperformance of applications and improved code quality through thorough testing\\n2002  2007 Software Development Engineer I Netflix  Los Gatos CA USA\\nDeveloped and maintained the user interface for the Netflix web application Worked closely\\nwith product managers and designers to create an optimal user experience\\n1999  2002 Software Development Engineer I Facebook  Menlo Park CA USA\\nPlayed a key role in the development of early Facebook features Implemented scalable\\nbackend services using Java and SQL\\nEducation\\n2016  2018 Master of Business Administration Stanford University  Stanford CA USA\\n1997  1999 Master of Science in Computer Science Massachusetts Institute of Technology \\nCambridge MA USA\\n1994  1997 Bachelor of Science in Computer Science University of California Berkeley \\nBerkeley CA USA\\nProjects\\n1/2 2020 \\nPresentPersonal Project Home Automation System\\nDeveloped a smart home automation system using Raspberry Pi and Python The system\\nautomates various home appliances based on user behavior and preferences contributing to\\nenergy saving and improved user convenience\\n2/2'}, {'resume': 'Alfred Pennyworth\\nProduct ManagerSilicon Valley CA USA\\nobilealt\\n/envel\\n/linkedininapennyworth\\n\\nProfessional Summary\\nSeasoned Product Manager with over 20 years of experience in software development and product\\nmanagement having worked at all FAANG companies Exceptional leadership skills strategic\\nthinking and a track record of managing products from conception to market success\\nSkills\\nProduct management Agile methodologies Leadership Communication Project\\nmanagement User Experience Design Market Research Data Analysis Java\\nPython JavaScript HTML/CSS SQL AWS\\nExperience\\n2017 \\nPresentProduct Manager Google  Mountain View CA USA\\nLeading crossfunctional teams to design develop and launch innovative products Devel\\noping product strategies and making datadriven decisions to improve user experience and\\nmeet business goals\\n2012  2017 Software Development Engineer III Amazon  Seattle WA USA\\nLed a team of developers in building scalable and highperforming ecommerce applications\\nSuccessfully delivered multiple projects within the stipulated time and budget\\n2007  2012 Software Development Engineer II Apple  Cupertino CA USA\\nDesigned and implemented software components for various Apple services Optimized the\\nperformance of applications and improved code quality through thorough testing\\n2002  2007 Software Development Engineer I Netflix  Los Gatos CA USA\\nDeveloped and maintained the user interface for the Netflix web application Worked closely\\nwith product managers and designers to create an optimal user experience\\n1999  2002 Software Development Engineer I Facebook  Menlo Park CA USA\\nPlayed a key role in the development of early Facebook features Implemented scalable\\nbackend services using Java and SQL\\nEducation\\n2016  2018 Master of Business Administration Stanford University  Stanford CA USA\\n1997  1999 Master of Science in Computer Science Massachusetts Institute of Technology \\nCambridge MA USA\\n1994  1997 Bachelor of Science in Computer Science University of California Berkeley \\nBerkeley CA USA\\nProjects\\n1/2 2020 \\nPresentPersonal Project Home Automation System\\nDeveloped a smart home automation system using Raspberry Pi and Python The system\\nautomates various home appliances based on user behavior and preferences contributing to\\nenergy saving and improved user convenience\\n2/2'}, {'resume': 'JOHN DOE\\n123 Main St Anywhere USA    \\nLinkedIn linkedincom/in/johndoe  GitHub githubcom/johndoe\\nPROFESSIONAL SUMMARY\\nHighly skilled Full Stack Developer with over 5 years of experience in Java and Angular development\\nspecializing in designing building testing and maintaining web applications Proficient in an assortment\\nof technologies including Java Spring Boot Angular HTML5 CSS3 and SQL Exceptional ability to\\nwork in a team and selfdirect Committed to providing highquality results with little supervision\\nSKILLS\\nJava and J2EE\\nSpring Boot Spring MVC and Hibernate\\nAngular versions 2+\\nJavaScript TypeScript HTML5 CSS3 and Bootstrap\\nRESTful APIs\\nSQL and NoSQL databases MySQL MongoDB\\nAgile and Scrum\\nGit and GitHub\\nJunit and Mockito\\nDocker\\nWORK EXPERIENCE\\nFull Stack Java Developer  ABC Company Inc Anywhere USA June 2018  Present\\nDeveloped scalable robust and maintainable enterpriselevel applications using Java and Spring\\nBoot\\nUsed Angular for developing dynamic and responsive web frontends improving user experience\\nby 30\\nIntegrated applications with MySQL and MongoDB databases to store and retrieve data efficiently\\nCollaborated in an Agile development team to deliver highquality software every sprint\\nCreated RESTful services and APIs for frontend and thirdparty applications\\nWrote unit tests using Junit and Mockito for robust testing of application components\\nSoftware Developer  XYZ Solutions Anywhere USA July 2016  June 2018\\nParticipated in the complete software development life cycle from requirement analysis to deploy\\nment\\nImplemented business logic using Java and enhanced user interface using Angular\\nDeveloped and maintained SQL and NoSQL databases implementing complex queries for business\\nneeds\\nUtilized Git for version control and collaborated with team members via GitHub\\nAssisted in troubleshooting software debugging and system enhancements\\n1 EDUCATION\\nBachelor of Science in Computer Science  State University Anywhere USA May 2016\\nCERTIFICATIONS\\nOracle Certified Professional Java SE 8 Programmer\\nCertified Angular Developer\\n2'}, {'resume': 'Alfred Pennyworth\\nProduct ManagerSilicon Valley CA USA\\nobilealt\\n/envel\\n/linkedininapennyworth\\n\\nProfessional Summary\\nSeasoned Product Manager with over 20 years of experience in software development and product\\nmanagement having worked at all FAANG companies Exceptional leadership skills strategic\\nthinking and a track record of managing products from conception to market success\\nSkills\\nProduct management Agile methodologies Leadership Communication Project\\nmanagement User Experience Design Market Research Data Analysis Java\\nPython JavaScript HTML/CSS SQL AWS\\nExperience\\n2017 \\nPresentProduct Manager Google  Mountain View CA USA\\nLeading crossfunctional teams to design develop and launch innovative products Devel\\noping product strategies and making datadriven decisions to improve user experience and\\nmeet business goals\\n2012  2017 Software Development Engineer III Amazon  Seattle WA USA\\nLed a team of developers in building scalable and highperforming ecommerce applications\\nSuccessfully delivered multiple projects within the stipulated time and budget\\n2007  2012 Software Development Engineer II Apple  Cupertino CA USA\\nDesigned and implemented software components for various Apple services Optimized the\\nperformance of applications and improved code quality through thorough testing\\n2002  2007 Software Development Engineer I Netflix  Los Gatos CA USA\\nDeveloped and maintained the user interface for the Netflix web application Worked closely\\nwith product managers and designers to create an optimal user experience\\n1999  2002 Software Development Engineer I Facebook  Menlo Park CA USA\\nPlayed a key role in the development of early Facebook features Implemented scalable\\nbackend services using Java and SQL\\nEducation\\n2016  2018 Master of Business Administration Stanford University  Stanford CA USA\\n1997  1999 Master of Science in Computer Science Massachusetts Institute of Technology \\nCambridge MA USA\\n1994  1997 Bachelor of Science in Computer Science University of California Berkeley \\nBerkeley CA USA\\nProjects\\n1/2 2020 \\nPresentPersonal Project Home Automation System\\nDeveloped a smart home automation system using Raspberry Pi and Python The system\\nautomates various home appliances based on user behavior and preferences contributing to\\nenergy saving and improved user convenience\\n2/2'}, {'resume': 'Harvey Dent\\nMachine Learning Engineer321 Gotham St\\nGotham USA\\nobilealt\\n/envel\\n/linkedininhdent\\n/githubhdent\\nProfessional Summary\\nMachine Learning Engineer with 5 years of experience in designing building and deploying predictive\\nmodels and deep learning systems Proficient in Python TensorFlow PyTorch and Scikitlearn\\nCurrently leading a team of AI engineers at OpenAI\\nSkills\\nPython R TensorFlow PyTorch Scikitlearn Keras SQL NoSQL Git Docker\\nKubernetes Agile and Scrum Statistics Data visualization Deep Learning Natural\\nLanguage Processing\\nExperience\\n2021 \\nPresentMachine Learning Engineer OpenAI  San Francisco USA\\nLeading a team of AI engineers Designed and implemented deep learning models for natural\\nlanguage processing tasks Improved the efficiency of model training and data processing\\npipelines Published several research papers in toptier AI conferences\\n2018  2021 Data Scientist Uber  San Francisco USA\\nDeveloped and deployed machine learning models to improve the efficiency of ride allocation\\nalgorithms Utilized TensorFlow and PyTorch for developing predictive models Analyzed\\nand visualized large data sets to drive business strategies\\n2016  2018 Junior Data Analyst Facebook  Menlo Park USA\\nAnalyzed and visualized large datasets using Python and R Assisted in the development of\\nmachine learning models for user behavior prediction Conducted A/B testing and provided\\ninsights to the product team\\nEducation\\n2014  2016 Master of Science in Computer Science Specialization in AI MIT Cambridge\\nUSA\\n2010  2014 Bachelor of Science in Computer Science UC San Diego  San Diego USA\\nProjects\\n2021 \\nPresentPersonal Project Predictive Stock Trading Model\\nDeveloped a predictive stock trading model using deep learning and time series analysis\\nUsed PyTorch for model development and Docker for deployment The model predicts stock\\nprices with a high degree of accuracy and automates trading decisions'}, {'resume': 'JOHN DOE\\n123 Main St Anywhere USA    \\nLinkedIn linkedincom/in/johndoe  GitHub githubcom/johndoe\\nPROFESSIONAL SUMMARY\\nHighly skilled Full Stack Developer with over 5 years of experience in Java and Angular development\\nspecializing in designing building testing and maintaining web applications Proficient in an assortment\\nof technologies including Java Spring Boot Angular HTML5 CSS3 and SQL Exceptional ability to\\nwork in a team and selfdirect Committed to providing highquality results with little supervision\\nSKILLS\\nJava and J2EE\\nSpring Boot Spring MVC and Hibernate\\nAngular versions 2+\\nJavaScript TypeScript HTML5 CSS3 and Bootstrap\\nRESTful APIs\\nSQL and NoSQL databases MySQL MongoDB\\nAgile and Scrum\\nGit and GitHub\\nJunit and Mockito\\nDocker\\nWORK EXPERIENCE\\nFull Stack Java Developer  ABC Company Inc Anywhere USA June 2018  Present\\nDeveloped scalable robust and maintainable enterpriselevel applications using Java and Spring\\nBoot\\nUsed Angular for developing dynamic and responsive web frontends improving user experience\\nby 30\\nIntegrated applications with MySQL and MongoDB databases to store and retrieve data efficiently\\nCollaborated in an Agile development team to deliver highquality software every sprint\\nCreated RESTful services and APIs for frontend and thirdparty applications\\nWrote unit tests using Junit and Mockito for robust testing of application components\\nSoftware Developer  XYZ Solutions Anywhere USA July 2016  June 2018\\nParticipated in the complete software development life cycle from requirement analysis to deploy\\nment\\nImplemented business logic using Java and enhanced user interface using Angular\\nDeveloped and maintained SQL and NoSQL databases implementing complex queries for business\\nneeds\\nUtilized Git for version control and collaborated with team members via GitHub\\nAssisted in troubleshooting software debugging and system enhancements\\n1 EDUCATION\\nBachelor of Science in Computer Science  State University Anywhere USA May 2016\\nCERTIFICATIONS\\nOracle Certified Professional Java SE 8 Programmer\\nCertified Angular Developer\\n2'}, {'resume': 'Bruce Wayne\\nMERN Stack Developer123 Gotham St\\nGotham USA\\nobilealt\\n/envel\\n/linkedininbwayne\\n\\nProfessional Summary\\nHighly skilled MERN Stack Developer with over 10 years of experience specializing in designing\\nbuilding and maintaining complex web applications Proficient in MongoDB Expressjs React and\\nNodejs Currently contributing to the development of AI technologies at OpenAI with a primary\\nfocus on the ChatGPT project\\nSkills\\nJavaScript and TypeScript MongoDB Expressjs React Nodejs MERN stack\\nRESTful APIs Git and GitHub Docker and Kubernetes Agile and Scrum Python\\nand Machine Learning basics\\nExperience\\nJune 2020 \\nPresentMERN Stack Developer OpenAI  San Francisco USA\\nWorking on the development of the ChatGPT project using Nodejs Expressjs and React\\nImplementing RESTful services for communication between frontend and backend Utilizing\\nDocker and Kubernetes for deployment and management of applications Working in an\\nAgile environment delivering highquality software every sprint Contributing to the design\\nand implementation of machine learning algorithms for natural language processing tasks\\nJuly 2015 \\nMay 2020Full Stack Developer Uber  San Francisco USA\\nDeveloped and maintained scalable web applications using MERN stack Ensured the\\nperformance quality and responsiveness of applications Successfully deployed solutions\\nusing Docker and Kubernetes Collaborated with a team of engineers product managers\\nand UX designers Led a team of junior developers conducted code reviews and ensured\\nadherence to best coding practices Worked closely with the data science team to optimize\\nrecommendation algorithms and enhance user experience\\nJune 2012 \\nJune 2015Software Developer Facebook  Menlo Park USA\\nDeveloped features for the Facebook web application using React Ensured the performance\\nof the MongoDB databases Utilized RESTful APIs for communication between different\\nparts of the application Worked in a fastpaced testdriven development environment\\nAssisted in migrating the legacy system to a modern MERN stack architecture\\nEducation\\n2009  2012 PhD in Computer Science CalTech  Pasadena USA\\n2007  2009 Master of Science in Computer Science MIT Cambridge USA\\n2003  2007 Bachelor of Science in Computer Science UC San Diego  San Diego USA\\n1/2 Projects\\n2019 \\nPresentPersonal Project Gotham Event Planner\\nCreated a fullfeatured web application to plan and organize events in Gotham city Used\\nMERN stack for development and Docker for deployment The application allows users to\\ncreate manage and share events and integrates with Google Maps API to display event\\nlocations\\n2/2'}, {'resume': 'Harvey Dent\\nMachine Learning Engineer321 Gotham St\\nGotham USA\\nobilealt\\n/envel\\n/linkedininhdent\\n/githubhdent\\nProfessional Summary\\nMachine Learning Engineer with 5 years of experience in designing building and deploying predictive\\nmodels and deep learning systems Proficient in Python TensorFlow PyTorch and Scikitlearn\\nCurrently leading a team of AI engineers at OpenAI\\nSkills\\nPython R TensorFlow PyTorch Scikitlearn Keras SQL NoSQL Git Docker\\nKubernetes Agile and Scrum Statistics Data visualization Deep Learning Natural\\nLanguage Processing\\nExperience\\n2021 \\nPresentMachine Learning Engineer OpenAI  San Francisco USA\\nLeading a team of AI engineers Designed and implemented deep learning models for natural\\nlanguage processing tasks Improved the efficiency of model training and data processing\\npipelines Published several research papers in toptier AI conferences\\n2018  2021 Data Scientist Uber  San Francisco USA\\nDeveloped and deployed machine learning models to improve the efficiency of ride allocation\\nalgorithms Utilized TensorFlow and PyTorch for developing predictive models Analyzed\\nand visualized large data sets to drive business strategies\\n2016  2018 Junior Data Analyst Facebook  Menlo Park USA\\nAnalyzed and visualized large datasets using Python and R Assisted in the development of\\nmachine learning models for user behavior prediction Conducted A/B testing and provided\\ninsights to the product team\\nEducation\\n2014  2016 Master of Science in Computer Science Specialization in AI MIT Cambridge\\nUSA\\n2010  2014 Bachelor of Science in Computer Science UC San Diego  San Diego USA\\nProjects\\n2021 \\nPresentPersonal Project Predictive Stock Trading Model\\nDeveloped a predictive stock trading model using deep learning and time series analysis\\nUsed PyTorch for model development and Docker for deployment The model predicts stock\\nprices with a high degree of accuracy and automates trading decisions'}, {'resume': 'Bruce Wayne\\nMERN Stack Developer123 Gotham St\\nGotham USA\\nobilealt\\n/envel\\n/linkedininbwayne\\n\\nProfessional Summary\\nHighly skilled MERN Stack Developer with over 10 years of experience specializing in designing\\nbuilding and maintaining complex web applications Proficient in MongoDB Expressjs React and\\nNodejs Currently contributing to the development of AI technologies at OpenAI with a primary\\nfocus on the ChatGPT project\\nSkills\\nJavaScript and TypeScript MongoDB Expressjs React Nodejs MERN stack\\nRESTful APIs Git and GitHub Docker and Kubernetes Agile and Scrum Python\\nand Machine Learning basics\\nExperience\\nJune 2020 \\nPresentMERN Stack Developer OpenAI  San Francisco USA\\nWorking on the development of the ChatGPT project using Nodejs Expressjs and React\\nImplementing RESTful services for communication between frontend and backend Utilizing\\nDocker and Kubernetes for deployment and management of applications Working in an\\nAgile environment delivering highquality software every sprint Contributing to the design\\nand implementation of machine learning algorithms for natural language processing tasks\\nJuly 2015 \\nMay 2020Full Stack Developer Uber  San Francisco USA\\nDeveloped and maintained scalable web applications using MERN stack Ensured the\\nperformance quality and responsiveness of applications Successfully deployed solutions\\nusing Docker and Kubernetes Collaborated with a team of engineers product managers\\nand UX designers Led a team of junior developers conducted code reviews and ensured\\nadherence to best coding practices Worked closely with the data science team to optimize\\nrecommendation algorithms and enhance user experience\\nJune 2012 \\nJune 2015Software Developer Facebook  Menlo Park USA\\nDeveloped features for the Facebook web application using React Ensured the performance\\nof the MongoDB databases Utilized RESTful APIs for communication between different\\nparts of the application Worked in a fastpaced testdriven development environment\\nAssisted in migrating the legacy system to a modern MERN stack architecture\\nEducation\\n2009  2012 PhD in Computer Science CalTech  Pasadena USA\\n2007  2009 Master of Science in Computer Science MIT Cambridge USA\\n2003  2007 Bachelor of Science in Computer Science UC San Diego  San Diego USA\\n1/2 Projects\\n2019 \\nPresentPersonal Project Gotham Event Planner\\nCreated a fullfeatured web application to plan and organize events in Gotham city Used\\nMERN stack for development and Docker for deployment The application allows users to\\ncreate manage and share events and integrates with Google Maps API to display event\\nlocations\\n2/2'},\n",
      "               {'resume': 'Harvey Dent\\nMachine Learning Engineer321 Gotham St\\nGotham USA\\nobilealt\\n/envel\\n/linkedininhdent\\n/githubhdent\\nProfessional Summary\\nMachine Learning Engineer with 5 years of experience in designing building and deploying predictive\\nmodels and deep learning systems Proficient in Python TensorFlow PyTorch and Scikitlearn\\nCurrently leading a team of AI engineers at OpenAI\\nSkills\\nPython R TensorFlow PyTorch Scikitlearn Keras SQL NoSQL Git Docker\\nKubernetes Agile and Scrum Statistics Data visualization Deep Learning Natural\\nLanguage Processing\\nExperience\\n2021 \\nPresentMachine Learning Engineer OpenAI  San Francisco USA\\nLeading a team of AI engineers Designed and implemented deep learning models for natural\\nlanguage processing tasks Improved the efficiency of model training and data processing\\npipelines Published several research papers in toptier AI conferences\\n2018  2021 Data Scientist Uber  San Francisco USA\\nDeveloped and deployed machine learning models to improve the efficiency of ride allocation\\nalgorithms Utilized TensorFlow and PyTorch for developing predictive models Analyzed\\nand visualized large data sets to drive business strategies\\n2016  2018 Junior Data Analyst Facebook  Menlo Park USA\\nAnalyzed and visualized large datasets using Python and R Assisted in the development of\\nmachine learning models for user behavior prediction Conducted A/B testing and provided\\ninsights to the product team\\nEducation\\n2014  2016 Master of Science in Computer Science Specialization in AI MIT Cambridge\\nUSA\\n2010  2014 Bachelor of Science in Computer Science UC San Diego  San Diego USA\\nProjects\\n2021 \\nPresentPersonal Project Predictive Stock Trading Model\\nDeveloped a predictive stock trading model using deep learning and time series analysis\\nUsed PyTorch for model development and Docker for deployment The model predicts stock\\nprices with a high degree of accuracy and automates trading decisions'}, {'resume': 'Bruce Wayne\\nMERN Stack Developer123 Gotham St\\nGotham USA\\nobilealt\\n/envel\\n/linkedininbwayne\\n\\nProfessional Summary\\nHighly skilled MERN Stack Developer with over 10 years of experience specializing in designing\\nbuilding and maintaining complex web applications Proficient in MongoDB Expressjs React and\\nNodejs Currently contributing to the development of AI technologies at OpenAI with a primary\\nfocus on the ChatGPT project\\nSkills\\nJavaScript and TypeScript MongoDB Expressjs React Nodejs MERN stack\\nRESTful APIs Git and GitHub Docker and Kubernetes Agile and Scrum Python\\nand Machine Learning basics\\nExperience\\nJune 2020 \\nPresentMERN Stack Developer OpenAI  San Francisco USA\\nWorking on the development of the ChatGPT project using Nodejs Expressjs and React\\nImplementing RESTful services for communication between frontend and backend Utilizing\\nDocker and Kubernetes for deployment and management of applications Working in an\\nAgile environment delivering highquality software every sprint Contributing to the design\\nand implementation of machine learning algorithms for natural language processing tasks\\nJuly 2015 \\nMay 2020Full Stack Developer Uber  San Francisco USA\\nDeveloped and maintained scalable web applications using MERN stack Ensured the\\nperformance quality and responsiveness of applications Successfully deployed solutions\\nusing Docker and Kubernetes Collaborated with a team of engineers product managers\\nand UX designers Led a team of junior developers conducted code reviews and ensured\\nadherence to best coding practices Worked closely with the data science team to optimize\\nrecommendation algorithms and enhance user experience\\nJune 2012 \\nJune 2015Software Developer Facebook  Menlo Park USA\\nDeveloped features for the Facebook web application using React Ensured the performance\\nof the MongoDB databases Utilized RESTful APIs for communication between different\\nparts of the application Worked in a fastpaced testdriven development environment\\nAssisted in migrating the legacy system to a modern MERN stack architecture\\nEducation\\n2009  2012 PhD in Computer Science CalTech  Pasadena USA\\n2007  2009 Master of Science in Computer Science MIT Cambridge USA\\n2003  2007 Bachelor of Science in Computer Science UC San Diego  San Diego USA\\n1/2 Projects\\n2019 \\nPresentPersonal Project Gotham Event Planner\\nCreated a fullfeatured web application to plan and organize events in Gotham city Used\\nMERN stack for development and Docker for deployment The application allows users to\\ncreate manage and share events and integrates with Google Maps API to display event\\nlocations\\n2/2'}, {'resume': 'Barry Allen\\nFrontEnd DeveloperGoogle HQ Mountain View CA USA\\nobilealt\\n/envel\\n/linkedininbwayne\\n\\nObjective\\nSeeking a challenging role as a FrontEnd Developer where I can leverage my knowledge of UI/UX\\ndesign and modern web technologies to create intuitive and engaging user interfaces\\nEducation\\n2018  2022 BTech Computer Science and Engineering Indian Institute of Technology\\nDelhi  New Delhi India\\nOverall GPA 95/10\\nSkills\\nJavaScript ES6+ TypeScript HTML5 CSS3 Python React Redux Angular\\nVuejs Nodejs Expressjs D3js Git Docker Webpack Babel Google Cloud\\nPlatform Firebase RESTful APIs GraphQL Agile Development TestDriven\\nDevelopment Responsive Design UI/UX\\nExperience\\nJune 2022 \\nPresentSoftware Engineer FrontEnd Google  Mountain View CA USA\\nDeveloping intuitive and engaging user interfaces using React and Redux Working closely\\nwith UX designers to implement responsive and accessible web design Participating in\\nagile development processes including sprint planning and code reviews Collaborating with\\nbackend developers to integrate RESTful APIs and ensure seamless data flow\\nProjects\\n2022 Personal Expense Tracker\\nDeveloped a personal expense tracker application using React Redux and Firebase Imple\\nmented user authentication using Firebase Auth and data storage using Firestore Utilized\\nD3js for data visualization to provide users with insights into their spending patterns'}, {'resume': 'JOHN DOE\\n123 Main St Anywhere USA    \\nLinkedIn linkedincom/in/johndoe  GitHub githubcom/johndoe\\nPROFESSIONAL SUMMARY\\nHighly skilled Full Stack Developer with over 5 years of experience in Java and Angular development\\nspecializing in designing building testing and maintaining web applications Proficient in an assortment\\nof technologies including Java Spring Boot Angular HTML5 CSS3 and SQL Exceptional ability to\\nwork in a team and selfdirect Committed to providing highquality results with little supervision\\nSKILLS\\nJava and J2EE\\nSpring Boot Spring MVC and Hibernate\\nAngular versions 2+\\nJavaScript TypeScript HTML5 CSS3 and Bootstrap\\nRESTful APIs\\nSQL and NoSQL databases MySQL MongoDB\\nAgile and Scrum\\nGit and GitHub\\nJunit and Mockito\\nDocker\\nWORK EXPERIENCE\\nFull Stack Java Developer  ABC Company Inc Anywhere USA June 2018  Present\\nDeveloped scalable robust and maintainable enterpriselevel applications using Java and Spring\\nBoot\\nUsed Angular for developing dynamic and responsive web frontends improving user experience\\nby 30\\nIntegrated applications with MySQL and MongoDB databases to store and retrieve data efficiently\\nCollaborated in an Agile development team to deliver highquality software every sprint\\nCreated RESTful services and APIs for frontend and thirdparty applications\\nWrote unit tests using Junit and Mockito for robust testing of application components\\nSoftware Developer  XYZ Solutions Anywhere USA July 2016  June 2018\\nParticipated in the complete software development life cycle from requirement analysis to deploy\\nment\\nImplemented business logic using Java and enhanced user interface using Angular\\nDeveloped and maintained SQL and NoSQL databases implementing complex queries for business\\nneeds\\nUtilized Git for version control and collaborated with team members via GitHub\\nAssisted in troubleshooting software debugging and system enhancements\\n1 EDUCATION\\nBachelor of Science in Computer Science  State University Anywhere USA May 2016\\nCERTIFICATIONS\\nOracle Certified Professional Java SE 8 Programmer\\nCertified Angular Developer\\n2'}, {'resume': 'Harvey Dent\\nMachine Learning Engineer321 Gotham St\\nGotham USA\\nobilealt\\n/envel\\n/linkedininhdent\\n/githubhdent\\nProfessional Summary\\nMachine Learning Engineer with 5 years of experience in designing building and deploying predictive\\nmodels and deep learning systems Proficient in Python TensorFlow PyTorch and Scikitlearn\\nCurrently leading a team of AI engineers at OpenAI\\nSkills\\nPython R TensorFlow PyTorch Scikitlearn Keras SQL NoSQL Git Docker\\nKubernetes Agile and Scrum Statistics Data visualization Deep Learning Natural\\nLanguage Processing\\nExperience\\n2021 \\nPresentMachine Learning Engineer OpenAI  San Francisco USA\\nLeading a team of AI engineers Designed and implemented deep learning models for natural\\nlanguage processing tasks Improved the efficiency of model training and data processing\\npipelines Published several research papers in toptier AI conferences\\n2018  2021 Data Scientist Uber  San Francisco USA\\nDeveloped and deployed machine learning models to improve the efficiency of ride allocation\\nalgorithms Utilized TensorFlow and PyTorch for developing predictive models Analyzed\\nand visualized large data sets to drive business strategies\\n2016  2018 Junior Data Analyst Facebook  Menlo Park USA\\nAnalyzed and visualized large datasets using Python and R Assisted in the development of\\nmachine learning models for user behavior prediction Conducted A/B testing and provided\\ninsights to the product team\\nEducation\\n2014  2016 Master of Science in Computer Science Specialization in AI MIT Cambridge\\nUSA\\n2010  2014 Bachelor of Science in Computer Science UC San Diego  San Diego USA\\nProjects\\n2021 \\nPresentPersonal Project Predictive Stock Trading Model\\nDeveloped a predictive stock trading model using deep learning and time series analysis\\nUsed PyTorch for model development and Docker for deployment The model predicts stock\\nprices with a high degree of accuracy and automates trading decisions'}, {'resume': 'JOHN DOE\\n123 Main St Anywhere USA    \\nLinkedIn linkedincom/in/johndoe  GitHub githubcom/johndoe\\nPROFESSIONAL SUMMARY\\nHighly skilled Full Stack Developer with over 5 years of experience in Java and Angular development\\nspecializing in designing building testing and maintaining web applications Proficient in an assortment\\nof technologies including Java Spring Boot Angular HTML5 CSS3 and SQL Exceptional ability to\\nwork in a team and selfdirect Committed to providing highquality results with little supervision\\nSKILLS\\nJava and J2EE\\nSpring Boot Spring MVC and Hibernate\\nAngular versions 2+\\nJavaScript TypeScript HTML5 CSS3 and Bootstrap\\nRESTful APIs\\nSQL and NoSQL databases MySQL MongoDB\\nAgile and Scrum\\nGit and GitHub\\nJunit and Mockito\\nDocker\\nWORK EXPERIENCE\\nFull Stack Java Developer  ABC Company Inc Anywhere USA June 2018  Present\\nDeveloped scalable robust and maintainable enterpriselevel applications using Java and Spring\\nBoot\\nUsed Angular for developing dynamic and responsive web frontends improving user experience\\nby 30\\nIntegrated applications with MySQL and MongoDB databases to store and retrieve data efficiently\\nCollaborated in an Agile development team to deliver highquality software every sprint\\nCreated RESTful services and APIs for frontend and thirdparty applications\\nWrote unit tests using Junit and Mockito for robust testing of application components\\nSoftware Developer  XYZ Solutions Anywhere USA July 2016  June 2018\\nParticipated in the complete software development life cycle from requirement analysis to deploy\\nment\\nImplemented business logic using Java and enhanced user interface using Angular\\nDeveloped and maintained SQL and NoSQL databases implementing complex queries for business\\nneeds\\nUtilized Git for version control and collaborated with team members via GitHub\\nAssisted in troubleshooting software debugging and system enhancements\\n1 EDUCATION\\nBachelor of Science in Computer Science  State University Anywhere USA May 2016\\nCERTIFICATIONS\\nOracle Certified Professional Java SE 8 Programmer\\nCertified Angular Developer\\n2'}, {'resume': 'Alfred Pennyworth\\nProduct ManagerSilicon Valley CA USA\\nobilealt\\n/envel\\n/linkedininapennyworth\\n\\nProfessional Summary\\nSeasoned Product Manager with over 20 years of experience in software development and product\\nmanagement having worked at all FAANG companies Exceptional leadership skills strategic\\nthinking and a track record of managing products from conception to market success\\nSkills\\nProduct management Agile methodologies Leadership Communication Project\\nmanagement User Experience Design Market Research Data Analysis Java\\nPython JavaScript HTML/CSS SQL AWS\\nExperience\\n2017 \\nPresentProduct Manager Google  Mountain View CA USA\\nLeading crossfunctional teams to design develop and launch innovative products Devel\\noping product strategies and making datadriven decisions to improve user experience and\\nmeet business goals\\n2012  2017 Software Development Engineer III Amazon  Seattle WA USA\\nLed a team of developers in building scalable and highperforming ecommerce applications\\nSuccessfully delivered multiple projects within the stipulated time and budget\\n2007  2012 Software Development Engineer II Apple  Cupertino CA USA\\nDesigned and implemented software components for various Apple services Optimized the\\nperformance of applications and improved code quality through thorough testing\\n2002  2007 Software Development Engineer I Netflix  Los Gatos CA USA\\nDeveloped and maintained the user interface for the Netflix web application Worked closely\\nwith product managers and designers to create an optimal user experience\\n1999  2002 Software Development Engineer I Facebook  Menlo Park CA USA\\nPlayed a key role in the development of early Facebook features Implemented scalable\\nbackend services using Java and SQL\\nEducation\\n2016  2018 Master of Business Administration Stanford University  Stanford CA USA\\n1997  1999 Master of Science in Computer Science Massachusetts Institute of Technology \\nCambridge MA USA\\n1994  1997 Bachelor of Science in Computer Science University of California Berkeley \\nBerkeley CA USA\\nProjects\\n1/2 2020 \\nPresentPersonal Project Home Automation System\\nDeveloped a smart home automation system using Raspberry Pi and Python The system\\nautomates various home appliances based on user behavior and preferences contributing to\\nenergy saving and improved user convenience\\n2/2'}, {'resume': 'Bruce Wayne\\nMERN Stack Developer123 Gotham St\\nGotham USA\\nobilealt\\n/envel\\n/linkedininbwayne\\n\\nProfessional Summary\\nHighly skilled MERN Stack Developer with over 10 years of experience specializing in designing\\nbuilding and maintaining complex web applications Proficient in MongoDB Expressjs React and\\nNodejs Currently contributing to the development of AI technologies at OpenAI with a primary\\nfocus on the ChatGPT project\\nSkills\\nJavaScript and TypeScript MongoDB Expressjs React Nodejs MERN stack\\nRESTful APIs Git and GitHub Docker and Kubernetes Agile and Scrum Python\\nand Machine Learning basics\\nExperience\\nJune 2020 \\nPresentMERN Stack Developer OpenAI  San Francisco USA\\nWorking on the development of the ChatGPT project using Nodejs Expressjs and React\\nImplementing RESTful services for communication between frontend and backend Utilizing\\nDocker and Kubernetes for deployment and management of applications Working in an\\nAgile environment delivering highquality software every sprint Contributing to the design\\nand implementation of machine learning algorithms for natural language processing tasks\\nJuly 2015 \\nMay 2020Full Stack Developer Uber  San Francisco USA\\nDeveloped and maintained scalable web applications using MERN stack Ensured the\\nperformance quality and responsiveness of applications Successfully deployed solutions\\nusing Docker and Kubernetes Collaborated with a team of engineers product managers\\nand UX designers Led a team of junior developers conducted code reviews and ensured\\nadherence to best coding practices Worked closely with the data science team to optimize\\nrecommendation algorithms and enhance user experience\\nJune 2012 \\nJune 2015Software Developer Facebook  Menlo Park USA\\nDeveloped features for the Facebook web application using React Ensured the performance\\nof the MongoDB databases Utilized RESTful APIs for communication between different\\nparts of the application Worked in a fastpaced testdriven development environment\\nAssisted in migrating the legacy system to a modern MERN stack architecture\\nEducation\\n2009  2012 PhD in Computer Science CalTech  Pasadena USA\\n2007  2009 Master of Science in Computer Science MIT Cambridge USA\\n2003  2007 Bachelor of Science in Computer Science UC San Diego  San Diego USA\\n1/2 Projects\\n2019 \\nPresentPersonal Project Gotham Event Planner\\nCreated a fullfeatured web application to plan and organize events in Gotham city Used\\nMERN stack for development and Docker for deployment The application allows users to\\ncreate manage and share events and integrates with Google Maps API to display event\\nlocations\\n2/2'}, {'resume': 'Barry Allen\\nFrontEnd DeveloperGoogle HQ Mountain View CA USA\\nobilealt\\n/envel\\n/linkedininbwayne\\n\\nObjective\\nSeeking a challenging role as a FrontEnd Developer where I can leverage my knowledge of UI/UX\\ndesign and modern web technologies to create intuitive and engaging user interfaces\\nEducation\\n2018  2022 BTech Computer Science and Engineering Indian Institute of Technology\\nDelhi  New Delhi India\\nOverall GPA 95/10\\nSkills\\nJavaScript ES6+ TypeScript HTML5 CSS3 Python React Redux Angular\\nVuejs Nodejs Expressjs D3js Git Docker Webpack Babel Google Cloud\\nPlatform Firebase RESTful APIs GraphQL Agile Development TestDriven\\nDevelopment Responsive Design UI/UX\\nExperience\\nJune 2022 \\nPresentSoftware Engineer FrontEnd Google  Mountain View CA USA\\nDeveloping intuitive and engaging user interfaces using React and Redux Working closely\\nwith UX designers to implement responsive and accessible web design Participating in\\nagile development processes including sprint planning and code reviews Collaborating with\\nbackend developers to integrate RESTful APIs and ensure seamless data flow\\nProjects\\n2022 Personal Expense Tracker\\nDeveloped a personal expense tracker application using React Redux and Firebase Imple\\nmented user authentication using Firebase Auth and data storage using Firestore Utilized\\nD3js for data visualization to provide users with insights into their spending patterns'}, {'resume': 'Barry Allen\\nFrontEnd DeveloperGoogle HQ Mountain View CA USA\\nobilealt\\n/envel\\n/linkedininbwayne\\n\\nObjective\\nSeeking a challenging role as a FrontEnd Developer where I can leverage my knowledge of UI/UX\\ndesign and modern web technologies to create intuitive and engaging user interfaces\\nEducation\\n2018  2022 BTech Computer Science and Engineering Indian Institute of Technology\\nDelhi  New Delhi India\\nOverall GPA 95/10\\nSkills\\nJavaScript ES6+ TypeScript HTML5 CSS3 Python React Redux Angular\\nVuejs Nodejs Expressjs D3js Git Docker Webpack Babel Google Cloud\\nPlatform Firebase RESTful APIs GraphQL Agile Development TestDriven\\nDevelopment Responsive Design UI/UX\\nExperience\\nJune 2022 \\nPresentSoftware Engineer FrontEnd Google  Mountain View CA USA\\nDeveloping intuitive and engaging user interfaces using React and Redux Working closely\\nwith UX designers to implement responsive and accessible web design Participating in\\nagile development processes including sprint planning and code reviews Collaborating with\\nbackend developers to integrate RESTful APIs and ensure seamless data flow\\nProjects\\n2022 Personal Expense Tracker\\nDeveloped a personal expense tracker application using React Redux and Firebase Imple\\nmented user authentication using Firebase Auth and data storage using Firestore Utilized\\nD3js for data visualization to provide users with insights into their spending patterns'}]\n",
      "\n",
      "from qdrant_client import QdrantClient, models\n",
      "from sentence_transformers import SentenceTransformer\n",
      "\n",
      "\n",
      "class QdrantSearch:\n",
      "    def __init__(self, api_key, documents, query_string):\n",
      "        self.api_key = api_key\n",
      "        self.documents = documents\n",
      "        self.query_string = query_string\n",
      "        # This is take from the examples provided at Qdrant\n",
      "        self.url = \"https://4248f066-5345-4bec-b3c3-ca343a34747e.us-east-1-0.aws.cloud.qdrant.io:6333\"\n",
      "        # There is a disk option is also available.\n",
      "        self.qdrant = QdrantClient(\":memory:\")\n",
      "        # This needs to be downloaded.\n",
      "        self.encoder = SentenceTransformer('all-MiniLM-L6-v2')\n",
      "\n",
      "    def init_qdrant_client(self):\n",
      "        self.qdrant = QdrantClient(\n",
      "            url=self.url,\n",
      "            api_key=self.api_key,\n",
      "        )\n",
      "        self.qdrant.recreate_collection(\n",
      "            collection_name=\"resume_matcher\",\n",
      "            vectors_config=models.VectorParams(\n",
      "                size=self.encoder.get_sentence_embedding_dimension(),\n",
      "                distance=models.Distance.COSINE\n",
      "            )\n",
      "        )\n",
      "        self.qdrant.upload_records(\n",
      "            collection_name=\"resume_matcher\",\n",
      "            records=[\n",
      "                models.Record(\n",
      "                    id=idx,\n",
      "                    vector=self.encoder.encode(doc[\"resume\"]).tolist(),\n",
      "                    payload=doc\n",
      "                ) for idx, doc in enumerate(self.documents)\n",
      "            ]\n",
      "        )\n",
      "\n",
      "    def search_documents(self):\n",
      "        \"\"\"\n",
      "        Note the query string provided needs to be a job description.\n",
      "        \"\"\"\n",
      "\n",
      "        if not self.qdrant:\n",
      "            self.init_qdrant_client()\n",
      "\n",
      "        hits = self.qdrant.search(\n",
      "            collection_name=\"resume_matcher\",\n",
      "            query_vector=self.encoder.encode(self.query_string).tolist(),\n",
      "            limit=10\n",
      "        )\n",
      "\n",
      "        results = []\n",
      "        for hit in hits:\n",
      "            result = {\n",
      "                'text': hit.payload,\n",
      "                'query': self.query_string,\n",
      "                'score': hit.score,\n",
      "            }\n",
      "            results.append(result)\n",
      "\n",
      "        return results\n",
      "\n",
      "from plotly.offline import plot\n",
      "from pyvis.network import Network\n",
      "import networkx as nx\n",
      "from typing import List\n",
      "import streamlit as st\n",
      "import pandas as pd\n",
      "import json\n",
      "import plotly.express as px\n",
      "import plotly.graph_objects as go\n",
      "import matplotlib\n",
      "import matplotlib.pyplot as plt\n",
      "from scripts.utils.ReadFiles import get_filenames_from_dir\n",
      "from scripts.ResumeProcessor import ResumeProcessor\n",
      "import time\n",
      "from streamlit_extras import add_vertical_space as avs\n",
      "from annotated_text import annotated_text, parameters\n",
      "import nltk\n",
      "\n",
      "parameters.SHOW_LABEL_SEPARATOR = False\n",
      "parameters.BORDER_RADIUS = 3\n",
      "parameters.PADDING = \"0.5 0.25rem\"\n",
      "\n",
      "\n",
      "def create_star_graph(nodes_and_weights, title):\n",
      "    # Create an empty graph\n",
      "    G = nx.Graph()\n",
      "\n",
      "    # Add the central node\n",
      "    central_node = \"resume\"\n",
      "    G.add_node(central_node)\n",
      "\n",
      "    # Add nodes and edges with weights to the graph\n",
      "    for node, weight in nodes_and_weights:\n",
      "        G.add_node(node)\n",
      "        G.add_edge(central_node, node, weight=weight*100)\n",
      "\n",
      "    # Get position layout for nodes\n",
      "    pos = nx.spring_layout(G)\n",
      "\n",
      "    # Create edge trace\n",
      "    edge_x = []\n",
      "    edge_y = []\n",
      "    for edge in G.edges():\n",
      "        x0, y0 = pos[edge[0]]\n",
      "        x1, y1 = pos[edge[1]]\n",
      "        edge_x.extend([x0, x1, None])\n",
      "        edge_y.extend([y0, y1, None])\n",
      "\n",
      "    edge_trace = go.Scatter(x=edge_x, y=edge_y, line=dict(\n",
      "        width=0.5, color='#888'), hoverinfo='none', mode='lines')\n",
      "\n",
      "    # Create node trace\n",
      "    node_x = []\n",
      "    node_y = []\n",
      "    for node in G.nodes():\n",
      "        x, y = pos[node]\n",
      "        node_x.append(x)\n",
      "        node_y.append(y)\n",
      "\n",
      "    node_trace = go.Scatter(x=node_x, y=node_y, mode='markers', hoverinfo='text',\n",
      "                            marker=dict(showscale=True, colorscale='Rainbow', reversescale=True, color=[], size=10,\n",
      "                                        colorbar=dict(thickness=15, title='Node Connections', xanchor='left',\n",
      "                                                      titleside='right'), line_width=2))\n",
      "\n",
      "    # Color node points by number of connections\n",
      "    node_adjacencies = []\n",
      "    node_text = []\n",
      "    for node in G.nodes():\n",
      "        adjacencies = list(G.adj[node])  # changes here\n",
      "        node_adjacencies.append(len(adjacencies))\n",
      "        node_text.append(f'{node}<br># of connections: {len(adjacencies)}')\n",
      "\n",
      "    node_trace.marker.color = node_adjacencies\n",
      "    node_trace.text = node_text\n",
      "\n",
      "    # Create the figure\n",
      "    fig = go.Figure(data=[edge_trace, node_trace],\n",
      "                    layout=go.Layout(title=title, titlefont_size=16, showlegend=False,\n",
      "                                     hovermode='closest', margin=dict(b=20, l=5, r=5, t=40),\n",
      "                                     xaxis=dict(\n",
      "                                         showgrid=False, zeroline=False, showticklabels=False),\n",
      "                                     yaxis=dict(showgrid=False, zeroline=False, showticklabels=False)))\n",
      "\n",
      "    # Show the figure\n",
      "    st.plotly_chart(fig)\n",
      "\n",
      "\n",
      "def create_annotated_text(input_string: str, word_list: List[str], annotation: str, color_code: str):\n",
      "    # Tokenize the input string\n",
      "    tokens = nltk.word_tokenize(input_string)\n",
      "\n",
      "    # Convert the list to a set for quick lookups\n",
      "    word_set = set(word_list)\n",
      "\n",
      "    # Initialize an empty list to hold the annotated text\n",
      "    annotated_text = []\n",
      "\n",
      "    for token in tokens:\n",
      "        # Check if the token is in the set\n",
      "        if token in word_set:\n",
      "            # If it is, append a tuple with the token, annotation, and color code\n",
      "            annotated_text.append((token, annotation, color_code))\n",
      "        else:\n",
      "            # If it's not, just append the token as a string\n",
      "            annotated_text.append(token)\n",
      "\n",
      "    return annotated_text\n",
      "\n",
      "\n",
      "def read_json(filename):\n",
      "    with open(filename) as f:\n",
      "        data = json.load(f)\n",
      "    return data\n",
      "\n",
      "\n",
      "def tokenize_string(input_string):\n",
      "    tokens = nltk.word_tokenize(input_string)\n",
      "    return tokens\n",
      "\n",
      "\n",
      "st.image('Assets/img/header_image.jpg')\n",
      "\n",
      "avs.add_vertical_space(5)\n",
      "\n",
      "resume_names = get_filenames_from_dir(\"Data/Processed/Resumes\")\n",
      "\n",
      "st.write(\"There are\", len(resume_names),\n",
      "         \" resumes present. Please select one from the menu below:\")\n",
      "output = st.slider('Select Resume Number', 0, len(resume_names)-1, 2)\n",
      "\n",
      "avs.add_vertical_space(5)\n",
      "\n",
      "st.write(\"You have selected \", resume_names[output], \" printing the resume\")\n",
      "selected_file = read_json(\"Data/Processed/Resumes/\"+resume_names[output])\n",
      "\n",
      "avs.add_vertical_space(2)\n",
      "st.markdown(\"#### Parsed Resume Data\")\n",
      "st.caption(\n",
      "    \"This text is parsed from your resume. This is how it'll look like after getting parsed by an ATS.\")\n",
      "st.caption(\"Utilize this to understand how to make your resume ATS friendly.\")\n",
      "avs.add_vertical_space(3)\n",
      "# st.json(selected_file)\n",
      "st.write(selected_file[\"clean_data\"])\n",
      "\n",
      "avs.add_vertical_space(3)\n",
      "st.write(\"Now let's take a look at the extracted keywords from the resume.\")\n",
      "\n",
      "annotated_text(create_annotated_text(\n",
      "    selected_file[\"clean_data\"], selected_file[\"extracted_keywords\"],\n",
      "    \"KW\", \"#0B666A\"))\n",
      "\n",
      "avs.add_vertical_space(5)\n",
      "st.write(\"Now let's take a look at the extracted entities from the resume.\")\n",
      "\n",
      "# Call the function with your data\n",
      "create_star_graph(selected_file['keyterms'], \"Entities from Resume\")\n",
      "\n",
      "df2 = pd.DataFrame(selected_file['keyterms'], columns=[\"keyword\", \"value\"])\n",
      "\n",
      "# Create the dictionary\n",
      "keyword_dict = {}\n",
      "for keyword, value in selected_file['keyterms']:\n",
      "    keyword_dict[keyword] = value*100\n",
      "\n",
      "fig = go.Figure(data=[go.Table(header=dict(values=[\"Keyword\", \"Value\"],\n",
      "                                           font=dict(size=12),\n",
      "                                           fill_color='#070A52'),\n",
      "                               cells=dict(values=[list(keyword_dict.keys()),\n",
      "                                                  list(keyword_dict.values())],\n",
      "                                          line_color='darkslategray',\n",
      "                                          fill_color='#6DA9E4'))\n",
      "                      ])\n",
      "st.plotly_chart(fig)\n",
      "\n",
      "st.divider()\n",
      "\n",
      "fig = px.treemap(df2, path=['keyword'], values='value',\n",
      "                 color_continuous_scale='Rainbow',\n",
      "                 title='Key Terms/Topics Extracted from your Resume')\n",
      "st.write(fig)\n",
      "\n",
      "avs.add_vertical_space(5)\n",
      "\n",
      "job_descriptions = get_filenames_from_dir(\"Data/Processed/JobDescription\")\n",
      "\n",
      "st.write(\"There are\", len(job_descriptions),\n",
      "         \" resumes present. Please select one from the menu below:\")\n",
      "output = st.slider('Select Job Description Number',\n",
      "                   0, len(job_descriptions)-1, 2)\n",
      "\n",
      "avs.add_vertical_space(5)\n",
      "\n",
      "st.write(\"You have selected \",\n",
      "         job_descriptions[output], \" printing the job description\")\n",
      "selected_jd = read_json(\n",
      "    \"Data/Processed/JobDescription/\"+job_descriptions[output])\n",
      "\n",
      "avs.add_vertical_space(2)\n",
      "st.markdown(\"#### Job Description\")\n",
      "st.caption(\n",
      "    \"Currently in the pipeline I'm parsing this from PDF but it'll be from txt or copy paste.\")\n",
      "avs.add_vertical_space(3)\n",
      "# st.json(selected_file)\n",
      "st.write(selected_jd[\"clean_data\"])\n",
      "\n",
      "st.markdown(\"#### Common Words between Job Description and Resumes Highlighted.\")\n",
      "\n",
      "annotated_text(create_annotated_text(\n",
      "    selected_file[\"clean_data\"], selected_jd[\"extracted_keywords\"],\n",
      "    \"JD\", \"#F24C3D\"))\n",
      "\n",
      "st.write(\"Now let's take a look at the extracted entities from the job description.\")\n",
      "\n",
      "# Call the function with your data\n",
      "create_star_graph(selected_jd['keyterms'], \"Entities from Job Description\")\n",
      "\n",
      "df2 = pd.DataFrame(selected_jd['keyterms'], columns=[\"keyword\", \"value\"])\n",
      "\n",
      "# Create the dictionary\n",
      "keyword_dict = {}\n",
      "for keyword, value in selected_jd['keyterms']:\n",
      "    keyword_dict[keyword] = value*100\n",
      "\n",
      "fig = go.Figure(data=[go.Table(header=dict(values=[\"Keyword\", \"Value\"],\n",
      "                                           font=dict(size=12),\n",
      "                                           fill_color='#070A52'),\n",
      "                               cells=dict(values=[list(keyword_dict.keys()),\n",
      "                                                  list(keyword_dict.values())],\n",
      "                                          line_color='darkslategray',\n",
      "                                          fill_color='#6DA9E4'))\n",
      "                      ])\n",
      "st.plotly_chart(fig)\n",
      "\n",
      "st.divider()\n",
      "\n",
      "fig = px.treemap(df2, path=['keyword'], values='value',\n",
      "                 color_continuous_scale='Rainbow',\n",
      "                 title='Key Terms/Topics Extracted from the selected Job Description')\n",
      "st.write(fig)\n",
      "\n",
      "import json\n",
      "from scripts.utils.ReadFiles import get_filenames_from_dir\n",
      "\n",
      "resume_path = \"Data/Processed/Resumes\"\n",
      "job_path = \"Data/Processed/JobDescription\"\n",
      "\n",
      "\n",
      "def read_json(filename):\n",
      "    with open(filename) as f:\n",
      "        data = json.load(f)\n",
      "    return data\n",
      "\n",
      "\n",
      "def build_resume_list(resume_names, path):\n",
      "    resumes = []\n",
      "    for resume in resume_names:\n",
      "        selected_file = read_json(path + '/' + resume)\n",
      "        resumes.append({\n",
      "            \"resume\": selected_file[\"clean_data\"]\n",
      "        })\n",
      "    return resumes\n",
      "\n",
      "\n",
      "resume_names = get_filenames_from_dir(resume_path)\n",
      "resumes = build_resume_list(resume_names, resume_path)\n",
      "\n",
      "print(resumes)  # To see the output.\n",
      "\n",
      "jobs = [{'job_desc': 'Job Description Product Manager 10+ Years of Experience\\nTech Solutions San Francisco CA USA\\nAbout Us\\nTech Solutions is a global leader in the technology industry specializing in the development of cuttingedge\\nsoftware products Were currently looking for a seasoned Product Manager with over 10 years of experience\\nto join our dynamic team\\nJob Description\\nThe Product Manager will be responsible for guiding the success of a product and leading the crossfunctional\\nteam that is responsible for improving it This is an important organizational role that sets the strategy\\nroadmap and feature definition for a product or product line\\nResponsibilities\\nDefine the product strategy and roadmap\\nDeliver MRDs and PRDs with prioritized features and corresponding justification\\nWork with external third parties to assess partnerships and licensing opportunities\\nRun beta and pilot programs with earlystage products and samples\\nBe an expert with respect to the competition\\nAct as a leader within the company\\nDevelop the core positioning and messaging for the product\\nPerform product demos to customers\\nSet pricing to meet revenue and profitability goals\\nRequirements\\n10+ years of experience in product management\\nDemonstrated success defining and launching excellent products\\nExcellent written and verbal communication skills\\nTechnical background with experience in software development\\nExcellent teamwork skills\\nProven ability to influence crossfunctional teams without formal authority\\nMust be able to travel 20\\nBachelors degree MBA preferred\\n1 Benefits\\nCompetitive salary package\\nHealth dental and vision insurance\\nRetirement savings plan\\nProfessional development opportunities\\nFlexible work hours\\nTech Solutions is an equal opportunity employer We celebrate diversity and are committed to creating\\nan inclusive environment for all employees\\nHow to Apply\\nTo apply please submit your resume and a brief explanation of your relevant experience to  \\n2'}, {'job_desc': 'Job Description Senior Full Stack Engineer 5+ Years of\\nExperience\\nTech Solutions San Francisco CA USA\\nAbout Us\\nTech Solutions is a leading technology company that creates innovative solutions across a variety of industries\\nOur mission is to improve lives through advanced technology Were currently seeking a Senior Full Stack\\nEngineer to join our dynamic team\\nJob Description\\nWere looking for a Senior Full Stack Engineer with 5+ years of experience in developing web applications\\nThe successful candidate will have experience working with both frontend and backend technologies and\\nwill be capable of overseeing projects from conception to production deployment\\nResponsibilities\\nDeveloping front end website architecture\\nDesigning user interactions on web pages\\nDeveloping back end website applications\\nCreating servers and databases for functionality\\nEnsuring crossplatform optimization for mobile phones\\nSeeing through a project from conception to finished product\\nDesigning and developing APIs\\nMeeting both technical and consumer needs\\nStaying abreast of developments in web applications and programming languages\\nRequirements\\nDegree in Computer Science or similar field\\n5+ years of experience in web development\\nStrong organizational and project management skills\\nProficiency with fundamental front end languages such as HTML CSS and JavaScript\\nProficiency with serverside languages such as Python Ruby Java PHP and Net\\nFamiliarity with database technology such as MySQL Oracle and MongoDB\\n1 Excellent verbal communication skills\\nGood problemsolving skills\\nAttention to detail\\nBenefits\\nCompetitive salary package\\nHealth dental and vision insurance\\nRetirement savings plan\\nProfessional development opportunities\\nFlexible work hours\\nTech Solutions is an equal opportunity employer and we value diversity at our company\\nHow to Apply\\nTo apply please submit your resume and a brief explanation of your relevant experience to  \\n2'},\n",
      "        {'job_desc': 'Job Description Front End Engineer 2 Years of Experience\\nTech Solutions San Francisco CA USA\\nAbout Us\\nAt Tech Solutions we are on a mission to build products that solve complex problems and improve peoples\\nlives We are seeking a talented Front End Engineer to join our dynamic team in San Francisco\\nJob Description\\nWe are looking for a Front End Engineer with at least 2 years of experience in developing scalable and\\nuserfriendly web applications The successful candidate will be proficient in modern JavaScript frameworks\\nand libraries HTML CSS and responsive design principles This role will contribute significantly to the\\ncreation and implementation of user interfaces for our web applications\\nResponsibilities\\nDevelop new userfacing features using modern JavaScript frameworks like Reactjs Vuejs or Angu\\nlarjs\\nBuild reusable code and libraries for future use\\nEnsure the technical feasibility of UI/UX designs\\nOptimize application for maximum speed and scalability\\nAssure that all user input is validated before submitting to backend services\\nCollaborate with other team members and stakeholders\\nRequirements\\n2 years of experience as a Front End Developer or similar role\\nProficiency in web markup including HTML5 CSS3\\nKnowledge of modern JavaScript programming and experience with libraries like jQuery\\nFamiliarity with modern frontend build pipelines and tools\\nExperience with popular frontend frameworks such as React Vue or Angular\\nFamiliarity with code versioning tools such as Git\\nDegree in Computer Science Engineering or a related field\\n1 Benefits\\nCompetitive salary package\\nHealth dental and vision insurance\\nRetirement savings plan\\nProfessional development opportunities\\nFlexible work hours\\nTech Solutions is proud to be an equal opportunity employer We celebrate diversity and are committed\\nto creating an inclusive environment for all employees\\nHow to Apply\\nTo apply please submit your resume and a brief explanation of your relevant experience to  \\n2'}, {'job_desc': 'Job Description Java Developer 3 Years of Experience\\nTech Solutions San Francisco CA USA\\nAbout Us\\nAt Tech Solutions we believe in the power of technology to solve complex problems We are a dynamic\\nforwardthinking tech company specializing in custom software solutions for various industries We are\\nseeking a talented and experienced Java Developer to join our team\\nJob Description\\nWe are seeking a skilled Java Developer with at least 3 years of experience in building highperforming scal\\nable enterprisegrade applications You will be part of a talented software team that works on missioncritical\\napplications Your roles and responsibilities will include managing Java/Java EE application development\\nwhile providing expertise in the full software development lifecycle\\nResponsibilities\\nDesigning implementing and maintaining Java applications that are often highvolume and low\\nlatency required for missioncritical systems\\nDelivering high availability and performance\\nContributing to all phases of the development lifecycle\\nWriting welldesigned efficient and testable code\\nConducting software analysis programming testing and debugging\\nEnsuring designs comply with specifications\\nPreparing and producing releases of software components\\nSupporting continuous improvement by investigating alternatives and technologies and presenting these\\nfor architectural review\\nRequirements\\nBS/MS degree in Computer Science Engineering or a related subject\\nProven handson Software Development experience\\nProven working experience in Java development\\nHandson experience in designing and developing applications using Java EE platforms\\nObjectOriented Analysis and design using common design patterns\\nProfound insight of Java and JEE internals Classloading Memory Management Transaction man\\nagement etc\\n1 Excellent knowledge of Relational Databases SQL and ORM technologies JPA2 Hibernate\\nExperience in developing web applications using at least one popular web framework JSF Wicket\\nGWT Spring MVC\\nExperience with testdriven development\\nBenefits\\nCompetitive salary package\\nHealth dental and vision insurance\\nRetirement savings plan\\nProfessional development opportunities\\nFlexible work hours\\nTech Solutions is proud to be an equal opportunity employer We celebrate diversity and are committed\\nto creating an inclusive environment for all employees\\nHow to Apply\\nTo apply please submit your resume and a brief explanation of your relevant experience to  \\n2'}]\n",
      "\n",
      "\n",
      "resumes = [{'resume': 'JOHN DOE\\n123 Main St Anywhere USA    \\nLinkedIn linkedincom/in/johndoe  GitHub githubcom/johndoe\\nPROFESSIONAL SUMMARY\\nHighly skilled Full Stack Developer with over 5 years of experience in Java and Angular development\\nspecializing in designing building testing and maintaining web applications Proficient in an assortment\\nof technologies including Java Spring Boot Angular HTML5 CSS3 and SQL Exceptional ability to\\nwork in a team and selfdirect Committed to providing highquality results with little supervision\\nSKILLS\\nJava and J2EE\\nSpring Boot Spring MVC and Hibernate\\nAngular versions 2+\\nJavaScript TypeScript HTML5 CSS3 and Bootstrap\\nRESTful APIs\\nSQL and NoSQL databases MySQL MongoDB\\nAgile and Scrum\\nGit and GitHub\\nJunit and Mockito\\nDocker\\nWORK EXPERIENCE\\nFull Stack Java Developer  ABC Company Inc Anywhere USA June 2018  Present\\nDeveloped scalable robust and maintainable enterpriselevel applications using Java and Spring\\nBoot\\nUsed Angular for developing dynamic and responsive web frontends improving user experience\\nby 30\\nIntegrated applications with MySQL and MongoDB databases to store and retrieve data efficiently\\nCollaborated in an Agile development team to deliver highquality software every sprint\\nCreated RESTful services and APIs for frontend and thirdparty applications\\nWrote unit tests using Junit and Mockito for robust testing of application components\\nSoftware Developer  XYZ Solutions Anywhere USA July 2016  June 2018\\nParticipated in the complete software development life cycle from requirement analysis to deploy\\nment\\nImplemented business logic using Java and enhanced user interface using Angular\\nDeveloped and maintained SQL and NoSQL databases implementing complex queries for business\\nneeds\\nUtilized Git for version control and collaborated with team members via GitHub\\nAssisted in troubleshooting software debugging and system enhancements\\n1 EDUCATION\\nBachelor of Science in Computer Science  State University Anywhere USA May 2016\\nCERTIFICATIONS\\nOracle Certified Professional Java SE 8 Programmer\\nCertified Angular Developer\\n2'}, {'resume': 'Alfred Pennyworth\\nProduct ManagerSilicon Valley CA USA\\nobilealt\\n/envel\\n/linkedininapennyworth\\n\\nProfessional Summary\\nSeasoned Product Manager with over 20 years of experience in software development and product\\nmanagement having worked at all FAANG companies Exceptional leadership skills strategic\\nthinking and a track record of managing products from conception to market success\\nSkills\\nProduct management Agile methodologies Leadership Communication Project\\nmanagement User Experience Design Market Research Data Analysis Java\\nPython JavaScript HTML/CSS SQL AWS\\nExperience\\n2017 \\nPresentProduct Manager Google  Mountain View CA USA\\nLeading crossfunctional teams to design develop and launch innovative products Devel\\noping product strategies and making datadriven decisions to improve user experience and\\nmeet business goals\\n2012  2017 Software Development Engineer III Amazon  Seattle WA USA\\nLed a team of developers in building scalable and highperforming ecommerce applications\\nSuccessfully delivered multiple projects within the stipulated time and budget\\n2007  2012 Software Development Engineer II Apple  Cupertino CA USA\\nDesigned and implemented software components for various Apple services Optimized the\\nperformance of applications and improved code quality through thorough testing\\n2002  2007 Software Development Engineer I Netflix  Los Gatos CA USA\\nDeveloped and maintained the user interface for the Netflix web application Worked closely\\nwith product managers and designers to create an optimal user experience\\n1999  2002 Software Development Engineer I Facebook  Menlo Park CA USA\\nPlayed a key role in the development of early Facebook features Implemented scalable\\nbackend services using Java and SQL\\nEducation\\n2016  2018 Master of Business Administration Stanford University  Stanford CA USA\\n1997  1999 Master of Science in Computer Science Massachusetts Institute of Technology \\nCambridge MA USA\\n1994  1997 Bachelor of Science in Computer Science University of California Berkeley \\nBerkeley CA USA\\nProjects\\n1/2 2020 \\nPresentPersonal Project Home Automation System\\nDeveloped a smart home automation system using Raspberry Pi and Python The system\\nautomates various home appliances based on user behavior and preferences contributing to\\nenergy saving and improved user convenience\\n2/2'},\n",
      "           {'resume': 'Harvey Dent\\nMachine Learning Engineer321 Gotham St\\nGotham USA\\nobilealt\\n/envel\\n/linkedininhdent\\n/githubhdent\\nProfessional Summary\\nMachine Learning Engineer with 5 years of experience in designing building and deploying predictive\\nmodels and deep learning systems Proficient in Python TensorFlow PyTorch and Scikitlearn\\nCurrently leading a team of AI engineers at OpenAI\\nSkills\\nPython R TensorFlow PyTorch Scikitlearn Keras SQL NoSQL Git Docker\\nKubernetes Agile and Scrum Statistics Data visualization Deep Learning Natural\\nLanguage Processing\\nExperience\\n2021 \\nPresentMachine Learning Engineer OpenAI  San Francisco USA\\nLeading a team of AI engineers Designed and implemented deep learning models for natural\\nlanguage processing tasks Improved the efficiency of model training and data processing\\npipelines Published several research papers in toptier AI conferences\\n2018  2021 Data Scientist Uber  San Francisco USA\\nDeveloped and deployed machine learning models to improve the efficiency of ride allocation\\nalgorithms Utilized TensorFlow and PyTorch for developing predictive models Analyzed\\nand visualized large data sets to drive business strategies\\n2016  2018 Junior Data Analyst Facebook  Menlo Park USA\\nAnalyzed and visualized large datasets using Python and R Assisted in the development of\\nmachine learning models for user behavior prediction Conducted A/B testing and provided\\ninsights to the product team\\nEducation\\n2014  2016 Master of Science in Computer Science Specialization in AI MIT Cambridge\\nUSA\\n2010  2014 Bachelor of Science in Computer Science UC San Diego  San Diego USA\\nProjects\\n2021 \\nPresentPersonal Project Predictive Stock Trading Model\\nDeveloped a predictive stock trading model using deep learning and time series analysis\\nUsed PyTorch for model development and Docker for deployment The model predicts stock\\nprices with a high degree of accuracy and automates trading decisions'}, {'resume': 'Bruce Wayne\\nMERN Stack Developer123 Gotham St\\nGotham USA\\nobilealt\\n/envel\\n/linkedininbwayne\\n\\nProfessional Summary\\nHighly skilled MERN Stack Developer with over 10 years of experience specializing in designing\\nbuilding and maintaining complex web applications Proficient in MongoDB Expressjs React and\\nNodejs Currently contributing to the development of AI technologies at OpenAI with a primary\\nfocus on the ChatGPT project\\nSkills\\nJavaScript and TypeScript MongoDB Expressjs React Nodejs MERN stack\\nRESTful APIs Git and GitHub Docker and Kubernetes Agile and Scrum Python\\nand Machine Learning basics\\nExperience\\nJune 2020 \\nPresentMERN Stack Developer OpenAI  San Francisco USA\\nWorking on the development of the ChatGPT project using Nodejs Expressjs and React\\nImplementing RESTful services for communication between frontend and backend Utilizing\\nDocker and Kubernetes for deployment and management of applications Working in an\\nAgile environment delivering highquality software every sprint Contributing to the design\\nand implementation of machine learning algorithms for natural language processing tasks\\nJuly 2015 \\nMay 2020Full Stack Developer Uber  San Francisco USA\\nDeveloped and maintained scalable web applications using MERN stack Ensured the\\nperformance quality and responsiveness of applications Successfully deployed solutions\\nusing Docker and Kubernetes Collaborated with a team of engineers product managers\\nand UX designers Led a team of junior developers conducted code reviews and ensured\\nadherence to best coding practices Worked closely with the data science team to optimize\\nrecommendation algorithms and enhance user experience\\nJune 2012 \\nJune 2015Software Developer Facebook  Menlo Park USA\\nDeveloped features for the Facebook web application using React Ensured the performance\\nof the MongoDB databases Utilized RESTful APIs for communication between different\\nparts of the application Worked in a fastpaced testdriven development environment\\nAssisted in migrating the legacy system to a modern MERN stack architecture\\nEducation\\n2009  2012 PhD in Computer Science CalTech  Pasadena USA\\n2007  2009 Master of Science in Computer Science MIT Cambridge USA\\n2003  2007 Bachelor of Science in Computer Science UC San Diego  San Diego USA\\n1/2 Projects\\n2019 \\nPresentPersonal Project Gotham Event Planner\\nCreated a fullfeatured web application to plan and organize events in Gotham city Used\\nMERN stack for development and Docker for deployment The application allows users to\\ncreate manage and share events and integrates with Google Maps API to display event\\nlocations\\n2/2'}, {'resume': 'Barry Allen\\nFrontEnd DeveloperGoogle HQ Mountain View CA USA\\nobilealt\\n/envel\\n/linkedininbwayne\\n\\nObjective\\nSeeking a challenging role as a FrontEnd Developer where I can leverage my knowledge of UI/UX\\ndesign and modern web technologies to create intuitive and engaging user interfaces\\nEducation\\n2018  2022 BTech Computer Science and Engineering Indian Institute of Technology\\nDelhi  New Delhi India\\nOverall GPA 95/10\\nSkills\\nJavaScript ES6+ TypeScript HTML5 CSS3 Python React Redux Angular\\nVuejs Nodejs Expressjs D3js Git Docker Webpack Babel Google Cloud\\nPlatform Firebase RESTful APIs GraphQL Agile Development TestDriven\\nDevelopment Responsive Design UI/UX\\nExperience\\nJune 2022 \\nPresentSoftware Engineer FrontEnd Google  Mountain View CA USA\\nDeveloping intuitive and engaging user interfaces using React and Redux Working closely\\nwith UX designers to implement responsive and accessible web design Participating in\\nagile development processes including sprint planning and code reviews Collaborating with\\nbackend developers to integrate RESTful APIs and ensure seamless data flow\\nProjects\\n2022 Personal Expense Tracker\\nDeveloped a personal expense tracker application using React Redux and Firebase Imple\\nmented user authentication using Firebase Auth and data storage using Firestore Utilized\\nD3js for data visualization to provide users with insights into their spending patterns'}]\n",
      "\n",
      "import yaml\n",
      "from qdrant_client import QdrantClient, models\n",
      "from sentence_transformers import SentenceTransformer\n",
      "\n",
      "# NOTE: DO NOT RUN THIS FILE IN YOUR LOCAL SYSTEM UNLESS\n",
      "# YOU HAVE RAM > 16GB + NVIDIA GPU\n",
      "# SENTENCE_TRANSFORMERS HAVE DEPENDECY ON PYTORCH GPU\n",
      "# DON'T RUN UNLESS YOU HAVE THE CAPACITY\n",
      "# I'M WORKING ON A WAY TO USE THIS FILE IN COLAB\n",
      "\n",
      "\n",
      "class QdrantSearch:\n",
      "    def __init__(self, api_key, documents, query_string):\n",
      "        self.api_key = api_key\n",
      "        self.documents = documents\n",
      "        self.query_string = query_string\n",
      "        # This is take from the examples provided at Qdrant\n",
      "        self.url = \"https://4248f066-5345-4bec-b3c3-ca343a34747e.us-east-1-0.aws.cloud.qdrant.io:6333\"\n",
      "        # There is a disk option is also available.\n",
      "        self.qdrant = QdrantClient(\":memory:\")\n",
      "        # This needs to be downloaded.\n",
      "        self.encoder = SentenceTransformer('all-MiniLM-L6-v2')\n",
      "\n",
      "    def init_qdrant_client(self):\n",
      "        self.qdrant = QdrantClient(\n",
      "            url=self.url,\n",
      "            api_key=self.api_key,\n",
      "        )\n",
      "        self.qdrant.recreate_collection(\n",
      "            collection_name=\"resume_matcher\",\n",
      "            vectors_config=models.VectorParams(\n",
      "                size=self.encoder.get_sentence_embedding_dimension(),\n",
      "                distance=models.Distance.COSINE\n",
      "            )\n",
      "        )\n",
      "        self.qdrant.upload_records(\n",
      "            collection_name=\"resume_matcher\",\n",
      "            records=[\n",
      "                models.Record(\n",
      "                    id=idx,\n",
      "                    vector=self.encoder.encode(doc[\"resume\"]).tolist(),\n",
      "                    payload=doc\n",
      "                ) for idx, doc in enumerate(self.documents)\n",
      "            ]\n",
      "        )\n",
      "\n",
      "    def search_documents(self):\n",
      "        \"\"\"\n",
      "        Note the query string provided needs to be a job description.\n",
      "        \"\"\"\n",
      "\n",
      "        if not self.qdrant:\n",
      "            self.init_qdrant_client()\n",
      "\n",
      "        hits = self.qdrant.search(\n",
      "            collection_name=\"resume_matcher\",\n",
      "            query_vector=self.encoder.encode(self.query_string).tolist(),\n",
      "            limit=20\n",
      "        )\n",
      "\n",
      "        results = []\n",
      "        for hit in hits:\n",
      "            result = {\n",
      "                'text': str(hit.payload)[:30],\n",
      "                'query': self.query_string[:50],\n",
      "                'score': hit.score,\n",
      "            }\n",
      "            results.append(result)\n",
      "\n",
      "        return results\n",
      "\n",
      "\n",
      "# Get the API key stored in config.yml\n",
      "def read_api_key(filepath):\n",
      "    with open(filepath, 'r') as file:\n",
      "        try:\n",
      "            data = yaml.safe_load(file)\n",
      "            return data['api_keys']['qdrant']\n",
      "        except yaml.YAMLError as exc:\n",
      "            print(exc)\n",
      "\n",
      "\n",
      "api_key = read_api_key('qdrant-api-key.yml')\n",
      "\n",
      "\n",
      "# ### MAIN ###\n",
      "\n",
      "# output = QdrantSearch(api_key, resumes, job[\"job_desc\"])\n",
      "# output.init_qdrant_client()\n",
      "# results = output.search_documents()\n",
      "# for result in results:\n",
      "#     print(result)\n",
      "\n",
      "import string\n",
      "import spacy\n",
      "import pywaffle\n",
      "import streamlit as st\n",
      "import pandas as pd\n",
      "import json\n",
      "import plotly.express as px\n",
      "import plotly.graph_objects as go\n",
      "import matplotlib.pyplot as plt\n",
      "import squarify\n",
      "\n",
      "st.title('Resume :blue[Matcher]')\n",
      "st.image('Assets/img/header_image.jpg')\n",
      "st.subheader('_AI Based Resume Analyzer & Ranker_')\n",
      "\n",
      "\n",
      "def read_json(filename):\n",
      "    with open(filename) as f:\n",
      "        data = json.load(f)\n",
      "    return data\n",
      "\n",
      "\n",
      "# read the json file\n",
      "resume = read_json(\n",
      "    'Data/Processed/Resume-d531571e-e4fa-45eb-ab6a-267cdeb6647e.json')\n",
      "job_desc = read_json(\n",
      "    'Data/Processed/Job-Desc-a4f06ccb-8d5a-4d0b-9f02-3ba6d686472e.json')\n",
      "\n",
      "st.write(\"### Reading Resume's POS\")\n",
      "df = pd.DataFrame(resume['pos_frequencies'], index=[0])\n",
      "fig = go.Figure(data=go.Bar(y=list(resume['pos_frequencies'].values()), x=list(resume['pos_frequencies'].keys())),\n",
      "                layout_title_text=\"Resume's POS\")\n",
      "st.write(fig)\n",
      "\n",
      "df2 = pd.DataFrame(resume['keyterms'], columns=[\"keyword\", \"value\"])\n",
      "st.dataframe(df2)\n",
      "\n",
      "# Create the dictionary\n",
      "keyword_dict = {}\n",
      "for keyword, value in resume['keyterms']:\n",
      "    keyword_dict[keyword] = value\n",
      "\n",
      "fig = go.Figure(data=[go.Table(header=dict(values=[\"Keyword\", \"Value\"],\n",
      "                                           font=dict(size=12),\n",
      "                                           fill_color='#070A52'),\n",
      "                               cells=dict(values=[list(keyword_dict.keys()),\n",
      "                                                  list(keyword_dict.values())],\n",
      "                                          line_color='darkslategray',\n",
      "                                          fill_color='#6DA9E4'))\n",
      "                      ])\n",
      "st.plotly_chart(fig)\n",
      "\n",
      "st.divider()\n",
      "\n",
      "for keyword, value in resume['keyterms']:\n",
      "    pass\n",
      "\n",
      "\n",
      "# display the waffle chart\n",
      "figure = plt.figure(\n",
      "    FigureClass=pywaffle.Waffle,\n",
      "    rows=20,\n",
      "    columns=20,\n",
      "    values=keyword_dict,\n",
      "    legend={'loc': 'upper left', 'bbox_to_anchor': (1, 1)})\n",
      "\n",
      "\n",
      "# Display the dictionary\n",
      "\n",
      "st.pyplot(fig=figure)\n",
      "# st.write(dict)\n",
      "\n",
      "fig = px.treemap(df2, path=['keyword'], values='value',\n",
      "                 color_continuous_scale='RdBu',\n",
      "                 title='Resume POS')\n",
      "st.write(fig)\n",
      "\n",
      "\n",
      "st.plotly_chart(figure_or_data=fig)\n",
      "\n",
      "fig = go.Figure(data=[go.Table(\n",
      "    header=dict(values=[\"Tri Grams\"],\n",
      "                fill_color='#1D267D',\n",
      "                align='center', font=dict(color='white', size=16)),\n",
      "    cells=dict(values=[resume['tri_grams']],\n",
      "               fill_color='#19A7CE',\n",
      "               align='left'))])\n",
      "\n",
      "st.plotly_chart(figure_or_data=fig)\n",
      "\n",
      "fig = go.Figure(data=[go.Table(\n",
      "    header=dict(values=[\"Bi Grams\"],\n",
      "                fill_color='#1D267D',\n",
      "                align='center', font=dict(color='white', size=16)),\n",
      "    cells=dict(values=[resume['bi_grams']],\n",
      "               fill_color='#19A7CE',\n",
      "               align='left'))])\n",
      "\n",
      "st.plotly_chart(figure_or_data=fig)\n",
      "\n",
      "\n",
      "\n",
      "from plotly.offline import plot\n",
      "from pyvis.network import Network\n",
      "import networkx as nx\n",
      "from typing import List\n",
      "import streamlit as st\n",
      "import pandas as pd\n",
      "import json\n",
      "import plotly.express as px\n",
      "import plotly.graph_objects as go\n",
      "import matplotlib\n",
      "import matplotlib.pyplot as plt\n",
      "from scripts.utils.ReadFiles import get_filenames_from_dir\n",
      "from scripts.ResumeProcessor import ResumeProcessor\n",
      "import time\n",
      "from streamlit_extras import add_vertical_space as avs\n",
      "from annotated_text import annotated_text, parameters\n",
      "import nltk\n",
      "\n",
      "parameters.SHOW_LABEL_SEPARATOR = False\n",
      "parameters.BORDER_RADIUS = 3\n",
      "parameters.PADDING = \"0.5 0.25rem\"\n",
      "\n",
      "\n",
      "def create_star_graph(nodes_and_weights, title):\n",
      "    # Create an empty graph\n",
      "    G = nx.Graph()\n",
      "\n",
      "    # Add the central node\n",
      "    central_node = \"resume\"\n",
      "    G.add_node(central_node)\n",
      "\n",
      "    # Add nodes and edges with weights to the graph\n",
      "    for node, weight in nodes_and_weights:\n",
      "        G.add_node(node)\n",
      "        G.add_edge(central_node, node, weight=weight*100)\n",
      "\n",
      "    # Get position layout for nodes\n",
      "    pos = nx.spring_layout(G)\n",
      "\n",
      "    # Create edge trace\n",
      "    edge_x = []\n",
      "    edge_y = []\n",
      "    for edge in G.edges():\n",
      "        x0, y0 = pos[edge[0]]\n",
      "        x1, y1 = pos[edge[1]]\n",
      "        edge_x.extend([x0, x1, None])\n",
      "        edge_y.extend([y0, y1, None])\n",
      "\n",
      "    edge_trace = go.Scatter(x=edge_x, y=edge_y, line=dict(\n",
      "        width=0.5, color='#888'), hoverinfo='none', mode='lines')\n",
      "\n",
      "    # Create node trace\n",
      "    node_x = []\n",
      "    node_y = []\n",
      "    for node in G.nodes():\n",
      "        x, y = pos[node]\n",
      "        node_x.append(x)\n",
      "        node_y.append(y)\n",
      "\n",
      "    node_trace = go.Scatter(x=node_x, y=node_y, mode='markers', hoverinfo='text',\n",
      "                            marker=dict(showscale=True, colorscale='Rainbow', reversescale=True, color=[], size=10,\n",
      "                                        colorbar=dict(thickness=15, title='Node Connections', xanchor='left',\n",
      "                                                      titleside='right'), line_width=2))\n",
      "\n",
      "    # Color node points by number of connections\n",
      "    node_adjacencies = []\n",
      "    node_text = []\n",
      "    for node in G.nodes():\n",
      "        adjacencies = list(G.adj[node])  # changes here\n",
      "        node_adjacencies.append(len(adjacencies))\n",
      "        node_text.append(f'{node}<br># of connections: {len(adjacencies)}')\n",
      "\n",
      "    node_trace.marker.color = node_adjacencies\n",
      "    node_trace.text = node_text\n",
      "\n",
      "    # Create the figure\n",
      "    fig = go.Figure(data=[edge_trace, node_trace],\n",
      "                    layout=go.Layout(title=title, titlefont_size=16, showlegend=False,\n",
      "                                     hovermode='closest', margin=dict(b=20, l=5, r=5, t=40),\n",
      "                                     xaxis=dict(\n",
      "                                         showgrid=False, zeroline=False, showticklabels=False),\n",
      "                                     yaxis=dict(showgrid=False, zeroline=False, showticklabels=False)))\n",
      "\n",
      "    # Show the figure\n",
      "    st.plotly_chart(fig)\n",
      "\n",
      "\n",
      "def create_annotated_text(input_string: str, word_list: List[str], annotation: str, color_code: str):\n",
      "    # Tokenize the input string\n",
      "    tokens = nltk.word_tokenize(input_string)\n",
      "\n",
      "    # Convert the list to a set for quick lookups\n",
      "    word_set = set(word_list)\n",
      "\n",
      "    # Initialize an empty list to hold the annotated text\n",
      "    annotated_text = []\n",
      "\n",
      "    for token in tokens:\n",
      "        # Check if the token is in the set\n",
      "        if token in word_set:\n",
      "            # If it is, append a tuple with the token, annotation, and color code\n",
      "            annotated_text.append((token, annotation, color_code))\n",
      "        else:\n",
      "            # If it's not, just append the token as a string\n",
      "            annotated_text.append(token)\n",
      "\n",
      "    return annotated_text\n",
      "\n",
      "\n",
      "def read_json(filename):\n",
      "    with open(filename) as f:\n",
      "        data = json.load(f)\n",
      "    return data\n",
      "\n",
      "\n",
      "def tokenize_string(input_string):\n",
      "    tokens = nltk.word_tokenize(input_string)\n",
      "    return tokens\n",
      "\n",
      "\n",
      "st.image('Assets/img/header_image.jpg')\n",
      "\n",
      "avs.add_vertical_space(5)\n",
      "\n",
      "resume_names = get_filenames_from_dir(\"Data/Processed/Resumes\")\n",
      "\n",
      "st.write(\"There are\", len(resume_names),\n",
      "         \" resumes present. Please select one from the menu below:\")\n",
      "output = st.slider('Select Resume Number', 0, len(resume_names)-1, 2)\n",
      "\n",
      "avs.add_vertical_space(5)\n",
      "\n",
      "st.write(\"You have selected \", resume_names[output], \" printing the resume\")\n",
      "selected_file = read_json(\"Data/Processed/Resumes/\"+resume_names[output])\n",
      "\n",
      "avs.add_vertical_space(2)\n",
      "st.markdown(\"#### Parsed Resume Data\")\n",
      "st.caption(\n",
      "    \"This text is parsed from your resume. This is how it'll look like after getting parsed by an ATS.\")\n",
      "st.caption(\"Utilize this to understand how to make your resume ATS friendly.\")\n",
      "avs.add_vertical_space(3)\n",
      "# st.json(selected_file)\n",
      "st.write(selected_file[\"clean_data\"])\n",
      "\n",
      "avs.add_vertical_space(3)\n",
      "st.write(\"Now let's take a look at the extracted keywords from the resume.\")\n",
      "\n",
      "annotated_text(create_annotated_text(\n",
      "    selected_file[\"clean_data\"], selected_file[\"extracted_keywords\"],\n",
      "    \"KW\", \"#0B666A\"))\n",
      "\n",
      "avs.add_vertical_space(5)\n",
      "st.write(\"Now let's take a look at the extracted entities from the resume.\")\n",
      "\n",
      "# Call the function with your data\n",
      "create_star_graph(selected_file['keyterms'], \"Entities from Resume\")\n",
      "\n",
      "df2 = pd.DataFrame(selected_file['keyterms'], columns=[\"keyword\", \"value\"])\n",
      "\n",
      "# Create the dictionary\n",
      "keyword_dict = {}\n",
      "for keyword, value in selected_file['keyterms']:\n",
      "    keyword_dict[keyword] = value*100\n",
      "\n",
      "fig = go.Figure(data=[go.Table(header=dict(values=[\"Keyword\", \"Value\"],\n",
      "                                           font=dict(size=12),\n",
      "                                           fill_color='#070A52'),\n",
      "                               cells=dict(values=[list(keyword_dict.keys()),\n",
      "                                                  list(keyword_dict.values())],\n",
      "                                          line_color='darkslategray',\n",
      "                                          fill_color='#6DA9E4'))\n",
      "                      ])\n",
      "st.plotly_chart(fig)\n",
      "\n",
      "st.divider()\n",
      "\n",
      "fig = px.treemap(df2, path=['keyword'], values='value',\n",
      "                 color_continuous_scale='Rainbow',\n",
      "                 title='Key Terms/Topics Extracted from your Resume')\n",
      "st.write(fig)\n",
      "\n",
      "avs.add_vertical_space(5)\n",
      "\n",
      "job_descriptions = get_filenames_from_dir(\"Data/Processed/JobDescription\")\n",
      "\n",
      "st.write(\"There are\", len(job_descriptions),\n",
      "         \" resumes present. Please select one from the menu below:\")\n",
      "output = st.slider('Select Job Description Number',\n",
      "                   0, len(job_descriptions)-1, 2)\n",
      "\n",
      "avs.add_vertical_space(5)\n",
      "\n",
      "st.write(\"You have selected \",\n",
      "         job_descriptions[output], \" printing the job description\")\n",
      "selected_jd = read_json(\n",
      "    \"Data/Processed/JobDescription/\"+job_descriptions[output])\n",
      "\n",
      "avs.add_vertical_space(2)\n",
      "st.markdown(\"#### Job Description\")\n",
      "st.caption(\n",
      "    \"Currently in the pipeline I'm parsing this from PDF but it'll be from txt or copy paste.\")\n",
      "avs.add_vertical_space(3)\n",
      "# st.json(selected_file)\n",
      "st.write(selected_jd[\"clean_data\"])\n",
      "\n",
      "st.markdown(\"#### Common Words between Job Description and Resumes Highlighted.\")\n",
      "\n",
      "annotated_text(create_annotated_text(\n",
      "    selected_file[\"clean_data\"], selected_jd[\"extracted_keywords\"],\n",
      "    \"JD\", \"#F24C3D\"))\n",
      "\n",
      "st.write(\"Now let's take a look at the extracted entities from the job description.\")\n",
      "\n",
      "# Call the function with your data\n",
      "create_star_graph(selected_jd['keyterms'], \"Entities from Job Description\")\n",
      "\n",
      "df2 = pd.DataFrame(selected_jd['keyterms'], columns=[\"keyword\", \"value\"])\n",
      "\n",
      "# Create the dictionary\n",
      "keyword_dict = {}\n",
      "for keyword, value in selected_jd['keyterms']:\n",
      "    keyword_dict[keyword] = value*100\n",
      "\n",
      "fig = go.Figure(data=[go.Table(header=dict(values=[\"Keyword\", \"Value\"],\n",
      "                                           font=dict(size=12),\n",
      "                                           fill_color='#070A52'),\n",
      "                               cells=dict(values=[list(keyword_dict.keys()),\n",
      "                                                  list(keyword_dict.values())],\n",
      "                                          line_color='darkslategray',\n",
      "                                          fill_color='#6DA9E4'))\n",
      "                      ])\n",
      "st.plotly_chart(fig)\n",
      "\n",
      "st.divider()\n",
      "\n",
      "fig = px.treemap(df2, path=['keyword'], values='value',\n",
      "                 color_continuous_scale='Rainbow',\n",
      "                 title='Key Terms/Topics Extracted from the selected Job Description')\n",
      "st.write(fig)\n",
      "\n",
      "avs.add_vertical_space(5)\n",
      "\n",
      "st.divider()\n",
      "\n",
      "st.markdown(\"## Vector Similarity Scores\")\n",
      "st.caption(\"Powered by Qdrant Vector Search\")\n",
      "st.info(\"These are pre-computed queries\", icon=\"\")\n",
      "st.warning(\n",
      "    \"Running Qdrant or Sentence Transformers without having capacity is not recommended\", icon=\"\")\n",
      "\n",
      "\n",
      "# Your data\n",
      "data = [\n",
      "    {'text': \"{'resume': 'Alfred Pennyworth\",\n",
      "        'query': 'Job Description Product Manager', 'score': 0.62658},\n",
      "    {'text': \"{'resume': 'Barry Allen\",\n",
      "        'query': 'Job Description Product Manager', 'score': 0.43777737},\n",
      "    {'text': \"{'resume': 'Bruce Wayne \",\n",
      "        'query': 'Job Description Product Manager', 'score': 0.39835533},\n",
      "    {'text': \"{'resume': 'JOHN DOE\",\n",
      "        'query': 'Job Description Product Manager', 'score': 0.3915512},\n",
      "    {'text': \"{'resume': 'Harvey Dent\",\n",
      "        'query': 'Job Description Product Manager', 'score': 0.3519544},\n",
      "    {'text': \"{'resume': 'Barry Allen\",\n",
      "        'query': 'Job Description Senior Full Stack Engineer', 'score': 0.6541866},\n",
      "    {'text': \"{'resume': 'Alfred Pennyworth\",\n",
      "        'query': 'Job Description Senior Full Stack Engineer', 'score': 0.59806436},\n",
      "    {'text': \"{'resume': 'JOHN DOE\",\n",
      "        'query': 'Job Description Senior Full Stack Engineer', 'score': 0.5951386},\n",
      "    {'text': \"{'resume': 'Bruce Wayne \",\n",
      "        'query': 'Job Description Senior Full Stack Engineer', 'score': 0.57700855},\n",
      "    {'text': \"{'resume': 'Harvey Dent\",\n",
      "        'query': 'Job Description Senior Full Stack Engineer', 'score': 0.38489106},\n",
      "    {'text': \"{'resume': 'Barry Allen\",\n",
      "        'query': 'Job Description Front End Engineer', 'score': 0.76813436},\n",
      "    {'text': \"{'resume': 'Bruce Wayne'\",\n",
      "        'query': 'Job Description Front End Engineer', 'score': 0.60440844},\n",
      "    {'text': \"{'resume': 'JOHN DOE\",\n",
      "        'query': 'Job Description Front End Engineer', 'score': 0.56080043},\n",
      "    {'text': \"{'resume': 'Alfred Pennyworth\",\n",
      "        'query': 'Job Description Front End Engineer', 'score': 0.5395049},\n",
      "    {'text': \"{'resume': 'Harvey Dent\",\n",
      "        'query': 'Job Description Front End Engineer', 'score': 0.3859515},\n",
      "    {'text': \"{'resume': 'JOHN DOE\",\n",
      "        'query': 'Job Description Java Developer', 'score': 0.5449441},\n",
      "    {'text': \"{'resume': 'Alfred Pennyworth\",\n",
      "        'query': 'Job Description Java Developer', 'score': 0.53476423},\n",
      "    {'text': \"{'resume': 'Barry Allen\",\n",
      "        'query': 'Job Description Java Developer', 'score': 0.5313871},\n",
      "    {'text': \"{'resume': 'Bruce Wayne \",\n",
      "        'query': 'Job Description Java Developer', 'score': 0.44446343},\n",
      "    {'text': \"{'resume': 'Harvey Dent\",\n",
      "        'query': 'Job Description Java Developer', 'score': 0.3616274}\n",
      "]\n",
      "\n",
      "# Create a DataFrame\n",
      "df = pd.DataFrame(data)\n",
      "\n",
      "# Create different DataFrames based on the query and sort by score\n",
      "df1 = df[df['query'] ==\n",
      "         'Job Description Product Manager'].sort_values(by='score', ascending=False)\n",
      "df2 = df[df['query'] ==\n",
      "         'Job Description Senior Full Stack Engineer'].sort_values(by='score', ascending=False)\n",
      "df3 = df[df['query'] == 'Job Description Front End Engineer'].sort_values(\n",
      "    by='score', ascending=False)\n",
      "df4 = df[df['query'] == 'Job Description Java Developer'].sort_values(\n",
      "    by='score', ascending=False)\n",
      "\n",
      "\n",
      "def plot_df(df, title):\n",
      "    fig = px.bar(df, x='text', y=df['score']*100, title=title)\n",
      "    st.plotly_chart(fig)\n",
      "\n",
      "\n",
      "st.markdown(\"### Bar plots of scores based on similarity to Job Description.\")\n",
      "\n",
      "st.subheader(\":blue[Legend]\")\n",
      "st.text(\"Alfred Pennyworth :  Product Manager\")\n",
      "st.text(\"Barry Allen :  Front End Developer\")\n",
      "st.text(\"Harvey Dent :  Machine Learning Engineer\")\n",
      "st.text(\"Bruce Wayne :  Fullstack Developer (MERN)\")\n",
      "st.text(\"John Doe :  Fullstack Developer (Java)\")\n",
      "\n",
      "\n",
      "plot_df(df1, 'Job Description Product Manager 10+ Years of Exper')\n",
      "plot_df(df2, 'Job Description Senior Full Stack Engineer 5+ Year')\n",
      "plot_df(df3, 'Job Description Front End Engineer 2 Years of Expe')\n",
      "plot_df(df4, 'Job Description Java Developer 3 Years of Experien')\n",
      "\n",
      "import networkx as nx\n",
      "from typing import List\n",
      "import streamlit as st\n",
      "import pandas as pd\n",
      "import json\n",
      "import plotly.express as px\n",
      "import plotly.graph_objects as go\n",
      "from scripts.utils.ReadFiles import get_filenames_from_dir\n",
      "from streamlit_extras import add_vertical_space as avs\n",
      "from annotated_text import annotated_text, parameters\n",
      "import nltk\n",
      "\n",
      "parameters.SHOW_LABEL_SEPARATOR = False\n",
      "parameters.BORDER_RADIUS = 3\n",
      "parameters.PADDING = \"0.5 0.25rem\"\n",
      "\n",
      "\n",
      "def create_star_graph(nodes_and_weights, title):\n",
      "    # Create an empty graph\n",
      "    G = nx.Graph()\n",
      "\n",
      "    # Add the central node\n",
      "    central_node = \"resume\"\n",
      "    G.add_node(central_node)\n",
      "\n",
      "    # Add nodes and edges with weights to the graph\n",
      "    for node, weight in nodes_and_weights:\n",
      "        G.add_node(node)\n",
      "        G.add_edge(central_node, node, weight=weight*100)\n",
      "\n",
      "    # Get position layout for nodes\n",
      "    pos = nx.spring_layout(G)\n",
      "\n",
      "    # Create edge trace\n",
      "    edge_x = []\n",
      "    edge_y = []\n",
      "    for edge in G.edges():\n",
      "        x0, y0 = pos[edge[0]]\n",
      "        x1, y1 = pos[edge[1]]\n",
      "        edge_x.extend([x0, x1, None])\n",
      "        edge_y.extend([y0, y1, None])\n",
      "\n",
      "    edge_trace = go.Scatter(x=edge_x, y=edge_y, line=dict(\n",
      "        width=0.5, color='#888'), hoverinfo='none', mode='lines')\n",
      "\n",
      "    # Create node trace\n",
      "    node_x = []\n",
      "    node_y = []\n",
      "    for node in G.nodes():\n",
      "        x, y = pos[node]\n",
      "        node_x.append(x)\n",
      "        node_y.append(y)\n",
      "\n",
      "    node_trace = go.Scatter(x=node_x, y=node_y, mode='markers', hoverinfo='text',\n",
      "                            marker=dict(showscale=True, colorscale='Rainbow', reversescale=True, color=[], size=10,\n",
      "                                        colorbar=dict(thickness=15, title='Node Connections', xanchor='left',\n",
      "                                                      titleside='right'), line_width=2))\n",
      "\n",
      "    # Color node points by number of connections\n",
      "    node_adjacencies = []\n",
      "    node_text = []\n",
      "    for node in G.nodes():\n",
      "        adjacencies = list(G.adj[node])  # changes here\n",
      "        node_adjacencies.append(len(adjacencies))\n",
      "        node_text.append(f'{node}<br># of connections: {len(adjacencies)}')\n",
      "\n",
      "    node_trace.marker.color = node_adjacencies\n",
      "    node_trace.text = node_text\n",
      "\n",
      "    # Create the figure\n",
      "    fig = go.Figure(data=[edge_trace, node_trace],\n",
      "                    layout=go.Layout(title=title, titlefont_size=16, showlegend=False,\n",
      "                                     hovermode='closest', margin=dict(b=20, l=5, r=5, t=40),\n",
      "                                     xaxis=dict(\n",
      "                                         showgrid=False, zeroline=False, showticklabels=False),\n",
      "                                     yaxis=dict(showgrid=False, zeroline=False, showticklabels=False)))\n",
      "\n",
      "    # Show the figure\n",
      "    st.plotly_chart(fig)\n",
      "\n",
      "\n",
      "def create_annotated_text(input_string: str, word_list: List[str], annotation: str, color_code: str):\n",
      "    # Tokenize the input string\n",
      "    tokens = nltk.word_tokenize(input_string)\n",
      "\n",
      "    # Convert the list to a set for quick lookups\n",
      "    word_set = set(word_list)\n",
      "\n",
      "    # Initialize an empty list to hold the annotated text\n",
      "    annotated_text = []\n",
      "\n",
      "    for token in tokens:\n",
      "        # Check if the token is in the set\n",
      "        if token in word_set:\n",
      "            # If it is, append a tuple with the token, annotation, and color code\n",
      "            annotated_text.append((token, annotation, color_code))\n",
      "        else:\n",
      "            # If it's not, just append the token as a string\n",
      "            annotated_text.append(token)\n",
      "\n",
      "    return annotated_text\n",
      "\n",
      "\n",
      "def read_json(filename):\n",
      "    with open(filename) as f:\n",
      "        data = json.load(f)\n",
      "    return data\n",
      "\n",
      "\n",
      "def tokenize_string(input_string):\n",
      "    tokens = nltk.word_tokenize(input_string)\n",
      "    return tokens\n",
      "\n",
      "\n",
      "st.image('Assets/img/header_image.jpg')\n",
      "\n",
      "avs.add_vertical_space(5)\n",
      "\n",
      "resume_names = get_filenames_from_dir(\"Data/Processed/Resumes\")\n",
      "\n",
      "st.write(\"There are\", len(resume_names),\n",
      "         \" resumes present. Please select one from the menu below:\")\n",
      "output = st.slider('Select Resume Number', 0, len(resume_names)-1, 2)\n",
      "\n",
      "avs.add_vertical_space(5)\n",
      "\n",
      "st.write(\"You have selected \", resume_names[output], \" printing the resume\")\n",
      "selected_file = read_json(\"Data/Processed/Resumes/\"+resume_names[output])\n",
      "\n",
      "avs.add_vertical_space(2)\n",
      "st.markdown(\"#### Parsed Resume Data\")\n",
      "st.caption(\n",
      "    \"This text is parsed from your resume. This is how it'll look like after getting parsed by an ATS.\")\n",
      "st.caption(\"Utilize this to understand how to make your resume ATS friendly.\")\n",
      "avs.add_vertical_space(3)\n",
      "# st.json(selected_file)\n",
      "st.write(selected_file[\"clean_data\"])\n",
      "\n",
      "avs.add_vertical_space(3)\n",
      "st.write(\"Now let's take a look at the extracted keywords from the resume.\")\n",
      "\n",
      "annotated_text(create_annotated_text(\n",
      "    selected_file[\"clean_data\"], selected_file[\"extracted_keywords\"],\n",
      "    \"KW\", \"#0B666A\"))\n",
      "\n",
      "avs.add_vertical_space(5)\n",
      "st.write(\"Now let's take a look at the extracted entities from the resume.\")\n",
      "\n",
      "# Call the function with your data\n",
      "create_star_graph(selected_file['keyterms'], \"Entities from Resume\")\n",
      "\n",
      "df2 = pd.DataFrame(selected_file['keyterms'], columns=[\"keyword\", \"value\"])\n",
      "\n",
      "# Create the dictionary\n",
      "keyword_dict = {}\n",
      "for keyword, value in selected_file['keyterms']:\n",
      "    keyword_dict[keyword] = value*100\n",
      "\n",
      "fig = go.Figure(data=[go.Table(header=dict(values=[\"Keyword\", \"Value\"],\n",
      "                                           font=dict(size=12),\n",
      "                                           fill_color='#070A52'),\n",
      "                               cells=dict(values=[list(keyword_dict.keys()),\n",
      "                                                  list(keyword_dict.values())],\n",
      "                                          line_color='darkslategray',\n",
      "                                          fill_color='#6DA9E4'))\n",
      "                      ])\n",
      "st.plotly_chart(fig)\n",
      "\n",
      "st.divider()\n",
      "\n",
      "fig = px.treemap(df2, path=['keyword'], values='value',\n",
      "                 color_continuous_scale='Rainbow',\n",
      "                 title='Key Terms/Topics Extracted from your Resume')\n",
      "st.write(fig)\n",
      "\n",
      "avs.add_vertical_space(5)\n",
      "\n",
      "job_descriptions = get_filenames_from_dir(\"Data/Processed/JobDescription\")\n",
      "\n",
      "st.write(\"There are\", len(job_descriptions),\n",
      "         \" resumes present. Please select one from the menu below:\")\n",
      "output = st.slider('Select Job Description Number',\n",
      "                   0, len(job_descriptions)-1, 2)\n",
      "\n",
      "avs.add_vertical_space(5)\n",
      "\n",
      "st.write(\"You have selected \",\n",
      "         job_descriptions[output], \" printing the job description\")\n",
      "selected_jd = read_json(\n",
      "    \"Data/Processed/JobDescription/\"+job_descriptions[output])\n",
      "\n",
      "avs.add_vertical_space(2)\n",
      "st.markdown(\"#### Job Description\")\n",
      "st.caption(\n",
      "    \"Currently in the pipeline I'm parsing this from PDF but it'll be from txt or copy paste.\")\n",
      "avs.add_vertical_space(3)\n",
      "# st.json(selected_file)\n",
      "st.write(selected_jd[\"clean_data\"])\n",
      "\n",
      "st.markdown(\"#### Common Words between Job Description and Resumes Highlighted.\")\n",
      "\n",
      "annotated_text(create_annotated_text(\n",
      "    selected_file[\"clean_data\"], selected_jd[\"extracted_keywords\"],\n",
      "    \"JD\", \"#F24C3D\"))\n",
      "\n",
      "st.write(\"Now let's take a look at the extracted entities from the job description.\")\n",
      "\n",
      "# Call the function with your data\n",
      "create_star_graph(selected_jd['keyterms'], \"Entities from Job Description\")\n",
      "\n",
      "df2 = pd.DataFrame(selected_jd['keyterms'], columns=[\"keyword\", \"value\"])\n",
      "\n",
      "# Create the dictionary\n",
      "keyword_dict = {}\n",
      "for keyword, value in selected_jd['keyterms']:\n",
      "    keyword_dict[keyword] = value*100\n",
      "\n",
      "fig = go.Figure(data=[go.Table(header=dict(values=[\"Keyword\", \"Value\"],\n",
      "                                           font=dict(size=12),\n",
      "                                           fill_color='#070A52'),\n",
      "                               cells=dict(values=[list(keyword_dict.keys()),\n",
      "                                                  list(keyword_dict.values())],\n",
      "                                          line_color='darkslategray',\n",
      "                                          fill_color='#6DA9E4'))\n",
      "                      ])\n",
      "st.plotly_chart(fig)\n",
      "\n",
      "st.divider()\n",
      "\n",
      "fig = px.treemap(df2, path=['keyword'], values='value',\n",
      "                 color_continuous_scale='Rainbow',\n",
      "                 title='Key Terms/Topics Extracted from the selected Job Description')\n",
      "st.write(fig)\n",
      "\n",
      "avs.add_vertical_space(5)\n",
      "\n",
      "st.divider()\n",
      "\n",
      "st.markdown(\"## Vector Similarity Scores\")\n",
      "st.caption(\"Powered by Qdrant Vector Search\")\n",
      "st.info(\"These are pre-computed queries\", icon=\"\")\n",
      "st.warning(\n",
      "    \"Running Qdrant or Sentence Transformers without having capacity is not recommended\", icon=\"\")\n",
      "\n",
      "\n",
      "# Your data\n",
      "data = [\n",
      "    {'text': \"{'resume': 'Alfred Pennyworth\",\n",
      "        'query': 'Job Description Product Manager', 'score': 0.62658},\n",
      "    {'text': \"{'resume': 'Barry Allen\",\n",
      "        'query': 'Job Description Product Manager', 'score': 0.43777737},\n",
      "    {'text': \"{'resume': 'Bruce Wayne \",\n",
      "        'query': 'Job Description Product Manager', 'score': 0.39835533},\n",
      "    {'text': \"{'resume': 'JOHN DOE\",\n",
      "        'query': 'Job Description Product Manager', 'score': 0.3915512},\n",
      "    {'text': \"{'resume': 'Harvey Dent\",\n",
      "        'query': 'Job Description Product Manager', 'score': 0.3519544},\n",
      "    {'text': \"{'resume': 'Barry Allen\",\n",
      "        'query': 'Job Description Senior Full Stack Engineer', 'score': 0.6541866},\n",
      "    {'text': \"{'resume': 'Alfred Pennyworth\",\n",
      "        'query': 'Job Description Senior Full Stack Engineer', 'score': 0.59806436},\n",
      "    {'text': \"{'resume': 'JOHN DOE\",\n",
      "        'query': 'Job Description Senior Full Stack Engineer', 'score': 0.5951386},\n",
      "    {'text': \"{'resume': 'Bruce Wayne \",\n",
      "        'query': 'Job Description Senior Full Stack Engineer', 'score': 0.57700855},\n",
      "    {'text': \"{'resume': 'Harvey Dent\",\n",
      "        'query': 'Job Description Senior Full Stack Engineer', 'score': 0.38489106},\n",
      "    {'text': \"{'resume': 'Barry Allen\",\n",
      "        'query': 'Job Description Front End Engineer', 'score': 0.76813436},\n",
      "    {'text': \"{'resume': 'Bruce Wayne'\",\n",
      "        'query': 'Job Description Front End Engineer', 'score': 0.60440844},\n",
      "    {'text': \"{'resume': 'JOHN DOE\",\n",
      "        'query': 'Job Description Front End Engineer', 'score': 0.56080043},\n",
      "    {'text': \"{'resume': 'Alfred Pennyworth\",\n",
      "        'query': 'Job Description Front End Engineer', 'score': 0.5395049},\n",
      "    {'text': \"{'resume': 'Harvey Dent\",\n",
      "        'query': 'Job Description Front End Engineer', 'score': 0.3859515},\n",
      "    {'text': \"{'resume': 'JOHN DOE\",\n",
      "        'query': 'Job Description Java Developer', 'score': 0.5449441},\n",
      "    {'text': \"{'resume': 'Alfred Pennyworth\",\n",
      "        'query': 'Job Description Java Developer', 'score': 0.53476423},\n",
      "    {'text': \"{'resume': 'Barry Allen\",\n",
      "        'query': 'Job Description Java Developer', 'score': 0.5313871},\n",
      "    {'text': \"{'resume': 'Bruce Wayne \",\n",
      "        'query': 'Job Description Java Developer', 'score': 0.44446343},\n",
      "    {'text': \"{'resume': 'Harvey Dent\",\n",
      "        'query': 'Job Description Java Developer', 'score': 0.3616274}\n",
      "]\n",
      "\n",
      "# Create a DataFrame\n",
      "df = pd.DataFrame(data)\n",
      "\n",
      "# Create different DataFrames based on the query and sort by score\n",
      "df1 = df[df['query'] ==\n",
      "         'Job Description Product Manager'].sort_values(by='score', ascending=False)\n",
      "df2 = df[df['query'] ==\n",
      "         'Job Description Senior Full Stack Engineer'].sort_values(by='score', ascending=False)\n",
      "df3 = df[df['query'] == 'Job Description Front End Engineer'].sort_values(\n",
      "    by='score', ascending=False)\n",
      "df4 = df[df['query'] == 'Job Description Java Developer'].sort_values(\n",
      "    by='score', ascending=False)\n",
      "\n",
      "\n",
      "def plot_df(df, title):\n",
      "    fig = px.bar(df, x='text', y=df['score']*100, title=title)\n",
      "    st.plotly_chart(fig)\n",
      "\n",
      "\n",
      "st.markdown(\"### Bar plots of scores based on similarity to Job Description.\")\n",
      "\n",
      "st.subheader(\":blue[Legend]\")\n",
      "st.text(\"Alfred Pennyworth :  Product Manager\")\n",
      "st.text(\"Barry Allen :  Front End Developer\")\n",
      "st.text(\"Harvey Dent :  Machine Learning Engineer\")\n",
      "st.text(\"Bruce Wayne :  Fullstack Developer (MERN)\")\n",
      "st.text(\"John Doe :  Fullstack Developer (Java)\")\n",
      "\n",
      "\n",
      "plot_df(df1, 'Job Description Product Manager 10+ Years of Exper')\n",
      "plot_df(df2, 'Job Description Senior Full Stack Engineer 5+ Year')\n",
      "plot_df(df3, 'Job Description Front End Engineer 2 Years of Expe')\n",
      "plot_df(df4, 'Job Description Java Developer 3 Years of Experien')\n",
      "\n",
      "import nltk\n",
      "from nltk.corpus import stopwords\n",
      "from nltk.tokenize import word_tokenize\n",
      "from nltk.stem import WordNetLemmatizer\n",
      "import string\n",
      "nltk.download('punkt')\n",
      "\n",
      "\n",
      "class TextCleaner:\n",
      "\n",
      "    def __init__(self, raw_text):\n",
      "        self.stopwords_set = set(stopwords.words(\n",
      "            'english') + list(string.punctuation))\n",
      "        self.lemmatizer = WordNetLemmatizer()\n",
      "        self.raw_input_text = raw_text\n",
      "\n",
      "    def clean_text(self) -> str:\n",
      "        tokens = word_tokenize(self.raw_input_text.lower())\n",
      "        tokens = [token for token in tokens if token not in self.stopwords_set]\n",
      "        tokens = [self.lemmatizer.lemmatize(token) for token in tokens]\n",
      "        cleaned_text = ' '.join(tokens)\n",
      "        return cleaned_text\n",
      "\n",
      "import nltk\n",
      "from nltk.corpus import stopwords\n",
      "from nltk.tokenize import word_tokenize\n",
      "from nltk.stem import WordNetLemmatizer\n",
      "import string\n",
      "\n",
      "\n",
      "class TextCleaner:\n",
      "\n",
      "    def __init__(self, raw_text):\n",
      "        self.stopwords_set = set(stopwords.words(\n",
      "            'english') + list(string.punctuation))\n",
      "        self.lemmatizer = WordNetLemmatizer()\n",
      "        self.raw_input_text = raw_text\n",
      "\n",
      "    def clean_text(self) -> str:\n",
      "        tokens = word_tokenize(self.raw_input_text.lower())\n",
      "        tokens = [token for token in tokens if token not in self.stopwords_set]\n",
      "        tokens = [self.lemmatizer.lemmatize(token) for token in tokens]\n",
      "        cleaned_text = ' '.join(tokens)\n",
      "        return cleaned_text\n",
      "\n",
      "import networkx as nx\n",
      "from typing import List\n",
      "import streamlit as st\n",
      "import pandas as pd\n",
      "import json\n",
      "import plotly.express as px\n",
      "import plotly.graph_objects as go\n",
      "from scripts.utils.ReadFiles import get_filenames_from_dir\n",
      "from streamlit_extras import add_vertical_space as avs\n",
      "from annotated_text import annotated_text, parameters\n",
      "import nltk\n",
      "nltk.download('punkt')\n",
      "\n",
      "parameters.SHOW_LABEL_SEPARATOR = False\n",
      "parameters.BORDER_RADIUS = 3\n",
      "parameters.PADDING = \"0.5 0.25rem\"\n",
      "\n",
      "\n",
      "def create_star_graph(nodes_and_weights, title):\n",
      "    # Create an empty graph\n",
      "    G = nx.Graph()\n",
      "\n",
      "    # Add the central node\n",
      "    central_node = \"resume\"\n",
      "    G.add_node(central_node)\n",
      "\n",
      "    # Add nodes and edges with weights to the graph\n",
      "    for node, weight in nodes_and_weights:\n",
      "        G.add_node(node)\n",
      "        G.add_edge(central_node, node, weight=weight*100)\n",
      "\n",
      "    # Get position layout for nodes\n",
      "    pos = nx.spring_layout(G)\n",
      "\n",
      "    # Create edge trace\n",
      "    edge_x = []\n",
      "    edge_y = []\n",
      "    for edge in G.edges():\n",
      "        x0, y0 = pos[edge[0]]\n",
      "        x1, y1 = pos[edge[1]]\n",
      "        edge_x.extend([x0, x1, None])\n",
      "        edge_y.extend([y0, y1, None])\n",
      "\n",
      "    edge_trace = go.Scatter(x=edge_x, y=edge_y, line=dict(\n",
      "        width=0.5, color='#888'), hoverinfo='none', mode='lines')\n",
      "\n",
      "    # Create node trace\n",
      "    node_x = []\n",
      "    node_y = []\n",
      "    for node in G.nodes():\n",
      "        x, y = pos[node]\n",
      "        node_x.append(x)\n",
      "        node_y.append(y)\n",
      "\n",
      "    node_trace = go.Scatter(x=node_x, y=node_y, mode='markers', hoverinfo='text',\n",
      "                            marker=dict(showscale=True, colorscale='Rainbow', reversescale=True, color=[], size=10,\n",
      "                                        colorbar=dict(thickness=15, title='Node Connections', xanchor='left',\n",
      "                                                      titleside='right'), line_width=2))\n",
      "\n",
      "    # Color node points by number of connections\n",
      "    node_adjacencies = []\n",
      "    node_text = []\n",
      "    for node in G.nodes():\n",
      "        adjacencies = list(G.adj[node])  # changes here\n",
      "        node_adjacencies.append(len(adjacencies))\n",
      "        node_text.append(f'{node}<br># of connections: {len(adjacencies)}')\n",
      "\n",
      "    node_trace.marker.color = node_adjacencies\n",
      "    node_trace.text = node_text\n",
      "\n",
      "    # Create the figure\n",
      "    fig = go.Figure(data=[edge_trace, node_trace],\n",
      "                    layout=go.Layout(title=title, titlefont_size=16, showlegend=False,\n",
      "                                     hovermode='closest', margin=dict(b=20, l=5, r=5, t=40),\n",
      "                                     xaxis=dict(\n",
      "                                         showgrid=False, zeroline=False, showticklabels=False),\n",
      "                                     yaxis=dict(showgrid=False, zeroline=False, showticklabels=False)))\n",
      "\n",
      "    # Show the figure\n",
      "    st.plotly_chart(fig)\n",
      "\n",
      "\n",
      "def create_annotated_text(input_string: str, word_list: List[str], annotation: str, color_code: str):\n",
      "    # Tokenize the input string\n",
      "    tokens = nltk.word_tokenize(input_string)\n",
      "\n",
      "    # Convert the list to a set for quick lookups\n",
      "    word_set = set(word_list)\n",
      "\n",
      "    # Initialize an empty list to hold the annotated text\n",
      "    annotated_text = []\n",
      "\n",
      "    for token in tokens:\n",
      "        # Check if the token is in the set\n",
      "        if token in word_set:\n",
      "            # If it is, append a tuple with the token, annotation, and color code\n",
      "            annotated_text.append((token, annotation, color_code))\n",
      "        else:\n",
      "            # If it's not, just append the token as a string\n",
      "            annotated_text.append(token)\n",
      "\n",
      "    return annotated_text\n",
      "\n",
      "\n",
      "def read_json(filename):\n",
      "    with open(filename) as f:\n",
      "        data = json.load(f)\n",
      "    return data\n",
      "\n",
      "\n",
      "def tokenize_string(input_string):\n",
      "    tokens = nltk.word_tokenize(input_string)\n",
      "    return tokens\n",
      "\n",
      "\n",
      "st.image('Assets/img/header_image.jpg')\n",
      "\n",
      "avs.add_vertical_space(5)\n",
      "\n",
      "resume_names = get_filenames_from_dir(\"Data/Processed/Resumes\")\n",
      "\n",
      "st.write(\"There are\", len(resume_names),\n",
      "         \" resumes present. Please select one from the menu below:\")\n",
      "output = st.slider('Select Resume Number', 0, len(resume_names)-1, 2)\n",
      "\n",
      "avs.add_vertical_space(5)\n",
      "\n",
      "st.write(\"You have selected \", resume_names[output], \" printing the resume\")\n",
      "selected_file = read_json(\"Data/Processed/Resumes/\"+resume_names[output])\n",
      "\n",
      "avs.add_vertical_space(2)\n",
      "st.markdown(\"#### Parsed Resume Data\")\n",
      "st.caption(\n",
      "    \"This text is parsed from your resume. This is how it'll look like after getting parsed by an ATS.\")\n",
      "st.caption(\"Utilize this to understand how to make your resume ATS friendly.\")\n",
      "avs.add_vertical_space(3)\n",
      "# st.json(selected_file)\n",
      "st.write(selected_file[\"clean_data\"])\n",
      "\n",
      "avs.add_vertical_space(3)\n",
      "st.write(\"Now let's take a look at the extracted keywords from the resume.\")\n",
      "\n",
      "annotated_text(create_annotated_text(\n",
      "    selected_file[\"clean_data\"], selected_file[\"extracted_keywords\"],\n",
      "    \"KW\", \"#0B666A\"))\n",
      "\n",
      "avs.add_vertical_space(5)\n",
      "st.write(\"Now let's take a look at the extracted entities from the resume.\")\n",
      "\n",
      "# Call the function with your data\n",
      "create_star_graph(selected_file['keyterms'], \"Entities from Resume\")\n",
      "\n",
      "df2 = pd.DataFrame(selected_file['keyterms'], columns=[\"keyword\", \"value\"])\n",
      "\n",
      "# Create the dictionary\n",
      "keyword_dict = {}\n",
      "for keyword, value in selected_file['keyterms']:\n",
      "    keyword_dict[keyword] = value*100\n",
      "\n",
      "fig = go.Figure(data=[go.Table(header=dict(values=[\"Keyword\", \"Value\"],\n",
      "                                           font=dict(size=12),\n",
      "                                           fill_color='#070A52'),\n",
      "                               cells=dict(values=[list(keyword_dict.keys()),\n",
      "                                                  list(keyword_dict.values())],\n",
      "                                          line_color='darkslategray',\n",
      "                                          fill_color='#6DA9E4'))\n",
      "                      ])\n",
      "st.plotly_chart(fig)\n",
      "\n",
      "st.divider()\n",
      "\n",
      "fig = px.treemap(df2, path=['keyword'], values='value',\n",
      "                 color_continuous_scale='Rainbow',\n",
      "                 title='Key Terms/Topics Extracted from your Resume')\n",
      "st.write(fig)\n",
      "\n",
      "avs.add_vertical_space(5)\n",
      "\n",
      "job_descriptions = get_filenames_from_dir(\"Data/Processed/JobDescription\")\n",
      "\n",
      "st.write(\"There are\", len(job_descriptions),\n",
      "         \" resumes present. Please select one from the menu below:\")\n",
      "output = st.slider('Select Job Description Number',\n",
      "                   0, len(job_descriptions)-1, 2)\n",
      "\n",
      "avs.add_vertical_space(5)\n",
      "\n",
      "st.write(\"You have selected \",\n",
      "         job_descriptions[output], \" printing the job description\")\n",
      "selected_jd = read_json(\n",
      "    \"Data/Processed/JobDescription/\"+job_descriptions[output])\n",
      "\n",
      "avs.add_vertical_space(2)\n",
      "st.markdown(\"#### Job Description\")\n",
      "st.caption(\n",
      "    \"Currently in the pipeline I'm parsing this from PDF but it'll be from txt or copy paste.\")\n",
      "avs.add_vertical_space(3)\n",
      "# st.json(selected_file)\n",
      "st.write(selected_jd[\"clean_data\"])\n",
      "\n",
      "st.markdown(\"#### Common Words between Job Description and Resumes Highlighted.\")\n",
      "\n",
      "annotated_text(create_annotated_text(\n",
      "    selected_file[\"clean_data\"], selected_jd[\"extracted_keywords\"],\n",
      "    \"JD\", \"#F24C3D\"))\n",
      "\n",
      "st.write(\"Now let's take a look at the extracted entities from the job description.\")\n",
      "\n",
      "# Call the function with your data\n",
      "create_star_graph(selected_jd['keyterms'], \"Entities from Job Description\")\n",
      "\n",
      "df2 = pd.DataFrame(selected_jd['keyterms'], columns=[\"keyword\", \"value\"])\n",
      "\n",
      "# Create the dictionary\n",
      "keyword_dict = {}\n",
      "for keyword, value in selected_jd['keyterms']:\n",
      "    keyword_dict[keyword] = value*100\n",
      "\n",
      "fig = go.Figure(data=[go.Table(header=dict(values=[\"Keyword\", \"Value\"],\n",
      "                                           font=dict(size=12),\n",
      "                                           fill_color='#070A52'),\n",
      "                               cells=dict(values=[list(keyword_dict.keys()),\n",
      "                                                  list(keyword_dict.values())],\n",
      "                                          line_color='darkslategray',\n",
      "                                          fill_color='#6DA9E4'))\n",
      "                      ])\n",
      "st.plotly_chart(fig)\n",
      "\n",
      "st.divider()\n",
      "\n",
      "fig = px.treemap(df2, path=['keyword'], values='value',\n",
      "                 color_continuous_scale='Rainbow',\n",
      "                 title='Key Terms/Topics Extracted from the selected Job Description')\n",
      "st.write(fig)\n",
      "\n",
      "avs.add_vertical_space(5)\n",
      "\n",
      "st.divider()\n",
      "\n",
      "st.markdown(\"## Vector Similarity Scores\")\n",
      "st.caption(\"Powered by Qdrant Vector Search\")\n",
      "st.info(\"These are pre-computed queries\", icon=\"\")\n",
      "st.warning(\n",
      "    \"Running Qdrant or Sentence Transformers without having capacity is not recommended\", icon=\"\")\n",
      "\n",
      "\n",
      "# Your data\n",
      "data = [\n",
      "    {'text': \"{'resume': 'Alfred Pennyworth\",\n",
      "        'query': 'Job Description Product Manager', 'score': 0.62658},\n",
      "    {'text': \"{'resume': 'Barry Allen\",\n",
      "        'query': 'Job Description Product Manager', 'score': 0.43777737},\n",
      "    {'text': \"{'resume': 'Bruce Wayne \",\n",
      "        'query': 'Job Description Product Manager', 'score': 0.39835533},\n",
      "    {'text': \"{'resume': 'JOHN DOE\",\n",
      "        'query': 'Job Description Product Manager', 'score': 0.3915512},\n",
      "    {'text': \"{'resume': 'Harvey Dent\",\n",
      "        'query': 'Job Description Product Manager', 'score': 0.3519544},\n",
      "    {'text': \"{'resume': 'Barry Allen\",\n",
      "        'query': 'Job Description Senior Full Stack Engineer', 'score': 0.6541866},\n",
      "    {'text': \"{'resume': 'Alfred Pennyworth\",\n",
      "        'query': 'Job Description Senior Full Stack Engineer', 'score': 0.59806436},\n",
      "    {'text': \"{'resume': 'JOHN DOE\",\n",
      "        'query': 'Job Description Senior Full Stack Engineer', 'score': 0.5951386},\n",
      "    {'text': \"{'resume': 'Bruce Wayne \",\n",
      "        'query': 'Job Description Senior Full Stack Engineer', 'score': 0.57700855},\n",
      "    {'text': \"{'resume': 'Harvey Dent\",\n",
      "        'query': 'Job Description Senior Full Stack Engineer', 'score': 0.38489106},\n",
      "    {'text': \"{'resume': 'Barry Allen\",\n",
      "        'query': 'Job Description Front End Engineer', 'score': 0.76813436},\n",
      "    {'text': \"{'resume': 'Bruce Wayne'\",\n",
      "        'query': 'Job Description Front End Engineer', 'score': 0.60440844},\n",
      "    {'text': \"{'resume': 'JOHN DOE\",\n",
      "        'query': 'Job Description Front End Engineer', 'score': 0.56080043},\n",
      "    {'text': \"{'resume': 'Alfred Pennyworth\",\n",
      "        'query': 'Job Description Front End Engineer', 'score': 0.5395049},\n",
      "    {'text': \"{'resume': 'Harvey Dent\",\n",
      "        'query': 'Job Description Front End Engineer', 'score': 0.3859515},\n",
      "    {'text': \"{'resume': 'JOHN DOE\",\n",
      "        'query': 'Job Description Java Developer', 'score': 0.5449441},\n",
      "    {'text': \"{'resume': 'Alfred Pennyworth\",\n",
      "        'query': 'Job Description Java Developer', 'score': 0.53476423},\n",
      "    {'text': \"{'resume': 'Barry Allen\",\n",
      "        'query': 'Job Description Java Developer', 'score': 0.5313871},\n",
      "    {'text': \"{'resume': 'Bruce Wayne \",\n",
      "        'query': 'Job Description Java Developer', 'score': 0.44446343},\n",
      "    {'text': \"{'resume': 'Harvey Dent\",\n",
      "        'query': 'Job Description Java Developer', 'score': 0.3616274}\n",
      "]\n",
      "\n",
      "# Create a DataFrame\n",
      "df = pd.DataFrame(data)\n",
      "\n",
      "# Create different DataFrames based on the query and sort by score\n",
      "df1 = df[df['query'] ==\n",
      "         'Job Description Product Manager'].sort_values(by='score', ascending=False)\n",
      "df2 = df[df['query'] ==\n",
      "         'Job Description Senior Full Stack Engineer'].sort_values(by='score', ascending=False)\n",
      "df3 = df[df['query'] == 'Job Description Front End Engineer'].sort_values(\n",
      "    by='score', ascending=False)\n",
      "df4 = df[df['query'] == 'Job Description Java Developer'].sort_values(\n",
      "    by='score', ascending=False)\n",
      "\n",
      "\n",
      "def plot_df(df, title):\n",
      "    fig = px.bar(df, x='text', y=df['score']*100, title=title)\n",
      "    st.plotly_chart(fig)\n",
      "\n",
      "\n",
      "st.markdown(\"### Bar plots of scores based on similarity to Job Description.\")\n",
      "\n",
      "st.subheader(\":blue[Legend]\")\n",
      "st.text(\"Alfred Pennyworth :  Product Manager\")\n",
      "st.text(\"Barry Allen :  Front End Developer\")\n",
      "st.text(\"Harvey Dent :  Machine Learning Engineer\")\n",
      "st.text(\"Bruce Wayne :  Fullstack Developer (MERN)\")\n",
      "st.text(\"John Doe :  Fullstack Developer (Java)\")\n",
      "\n",
      "\n",
      "plot_df(df1, 'Job Description Product Manager 10+ Years of Exper')\n",
      "plot_df(df2, 'Job Description Senior Full Stack Engineer 5+ Year')\n",
      "plot_df(df3, 'Job Description Front End Engineer 2 Years of Expe')\n",
      "plot_df(df4, 'Job Description Java Developer 3 Years of Experien')\n",
      "\n",
      "File LOC changed from 28 to 42\n",
      "import json\n",
      "from scripts.utils.ReadFiles import get_filenames_from_dir\n",
      "\n",
      "resume_path = \"Data/Processed/Resumes\"\n",
      "job_path = \"Data/Processed/JobDescription\"\n",
      "\n",
      "\n",
      "def read_json(filename):\n",
      "    with open(filename) as f:\n",
      "        data = json.load(f)\n",
      "    return data\n",
      "\n",
      "\n",
      "def build_resume_list(resume_names, path):\n",
      "    resumes = []\n",
      "    for resume in resume_names:\n",
      "        selected_file = read_json(path + '/' + resume)\n",
      "        resumes.append({\n",
      "            \"resume\": selected_file[\"clean_data\"]\n",
      "        })\n",
      "    return resumes\n",
      "\n",
      "\n",
      "def build_jobdesc_list(jobdesc_names, path):\n",
      "    resumes = []\n",
      "    for resume in resume_names:\n",
      "        selected_file = read_json(path + '/' + resume)\n",
      "        resumes.append({\n",
      "            \"resume\": selected_file[\"clean_data\"]\n",
      "        })\n",
      "    return resumes\n",
      "\n",
      "\n",
      "resume_names = get_filenames_from_dir(resume_path)\n",
      "resumes = build_resume_list(resume_names, resume_path)\n",
      "\n",
      "jobdesc_names = get_filenames_from_dir(job_path)\n",
      "jobdescs = build_jobdesc_list(jobdesc_names, job_path)\n",
      "\n",
      "print(resumes)\n",
      "print(jobdescs)\n",
      "\n",
      "File archive/ExtraScripts.py has no source code\n",
      "\n",
      "File archive/run.py has no source code\n",
      "\n",
      "import string\n",
      "import spacy\n",
      "import pywaffle\n",
      "import streamlit as st\n",
      "import pandas as pd\n",
      "import json\n",
      "import plotly.express as px\n",
      "import plotly.graph_objects as go\n",
      "import matplotlib.pyplot as plt\n",
      "import squarify\n",
      "\n",
      "st.title('Resume :blue[Matcher]')\n",
      "st.image('Assets/img/header_image.jpg')\n",
      "st.subheader('_AI Based Resume Analyzer & Ranker_')\n",
      "\n",
      "\n",
      "def read_json(filename):\n",
      "    with open(filename) as f:\n",
      "        data = json.load(f)\n",
      "    return data\n",
      "\n",
      "\n",
      "# read the json file\n",
      "resume = read_json(\n",
      "    'Data/Processed/Resume-d531571e-e4fa-45eb-ab6a-267cdeb6647e.json')\n",
      "job_desc = read_json(\n",
      "    'Data/Processed/Job-Desc-a4f06ccb-8d5a-4d0b-9f02-3ba6d686472e.json')\n",
      "\n",
      "st.write(\"### Reading Resume's POS\")\n",
      "df = pd.DataFrame(resume['pos_frequencies'], index=[0])\n",
      "fig = go.Figure(data=go.Bar(y=list(resume['pos_frequencies'].values()), x=list(resume['pos_frequencies'].keys())),\n",
      "                layout_title_text=\"Resume's POS\")\n",
      "st.write(fig)\n",
      "\n",
      "df2 = pd.DataFrame(resume['keyterms'], columns=[\"keyword\", \"value\"])\n",
      "st.dataframe(df2)\n",
      "\n",
      "# Create the dictionary\n",
      "keyword_dict = {}\n",
      "for keyword, value in resume['keyterms']:\n",
      "    keyword_dict[keyword] = value\n",
      "\n",
      "fig = go.Figure(data=[go.Table(header=dict(values=[\"Keyword\", \"Value\"],\n",
      "                                           font=dict(size=12),\n",
      "                                           fill_color='#070A52'),\n",
      "                               cells=dict(values=[list(keyword_dict.keys()),\n",
      "                                                  list(keyword_dict.values())],\n",
      "                                          line_color='darkslategray',\n",
      "                                          fill_color='#6DA9E4'))\n",
      "                      ])\n",
      "st.plotly_chart(fig)\n",
      "\n",
      "st.divider()\n",
      "\n",
      "for keyword, value in resume['keyterms']:\n",
      "    pass\n",
      "\n",
      "\n",
      "# display the waffle chart\n",
      "figure = plt.figure(\n",
      "    FigureClass=pywaffle.Waffle,\n",
      "    rows=20,\n",
      "    columns=20,\n",
      "    values=keyword_dict,\n",
      "    legend={'loc': 'upper left', 'bbox_to_anchor': (1, 1)})\n",
      "\n",
      "\n",
      "# Display the dictionary\n",
      "\n",
      "st.pyplot(fig=figure)\n",
      "# st.write(dict)\n",
      "\n",
      "fig = px.treemap(df2, path=['keyword'], values='value',\n",
      "                 color_continuous_scale='RdBu',\n",
      "                 title='Resume POS')\n",
      "st.write(fig)\n",
      "\n",
      "\n",
      "st.plotly_chart(figure_or_data=fig)\n",
      "\n",
      "fig = go.Figure(data=[go.Table(\n",
      "    header=dict(values=[\"Tri Grams\"],\n",
      "                fill_color='#1D267D',\n",
      "                align='center', font=dict(color='white', size=16)),\n",
      "    cells=dict(values=[resume['tri_grams']],\n",
      "               fill_color='#19A7CE',\n",
      "               align='left'))])\n",
      "\n",
      "st.plotly_chart(figure_or_data=fig)\n",
      "\n",
      "fig = go.Figure(data=[go.Table(\n",
      "    header=dict(values=[\"Bi Grams\"],\n",
      "                fill_color='#1D267D',\n",
      "                align='center', font=dict(color='white', size=16)),\n",
      "    cells=dict(values=[resume['bi_grams']],\n",
      "               fill_color='#19A7CE',\n",
      "               align='left'))])\n",
      "\n",
      "st.plotly_chart(figure_or_data=fig)\n",
      "\n",
      "\n",
      "\n",
      "import networkx as nx\n",
      "from typing import List\n",
      "import streamlit as st\n",
      "import pandas as pd\n",
      "import json\n",
      "import plotly.express as px\n",
      "import plotly.graph_objects as go\n",
      "from scripts.utils.ReadFiles import get_filenames_from_dir\n",
      "from streamlit_extras import add_vertical_space as avs\n",
      "from annotated_text import annotated_text, parameters\n",
      "import nltk\n",
      "nltk.download('punkt')\n",
      "\n",
      "parameters.SHOW_LABEL_SEPARATOR = False\n",
      "parameters.BORDER_RADIUS = 3\n",
      "parameters.PADDING = \"0.5 0.25rem\"\n",
      "\n",
      "\n",
      "def create_star_graph(nodes_and_weights, title):\n",
      "    # Create an empty graph\n",
      "    G = nx.Graph()\n",
      "\n",
      "    # Add the central node\n",
      "    central_node = \"resume\"\n",
      "    G.add_node(central_node)\n",
      "\n",
      "    # Add nodes and edges with weights to the graph\n",
      "    for node, weight in nodes_and_weights:\n",
      "        G.add_node(node)\n",
      "        G.add_edge(central_node, node, weight=weight*100)\n",
      "\n",
      "    # Get position layout for nodes\n",
      "    pos = nx.spring_layout(G)\n",
      "\n",
      "    # Create edge trace\n",
      "    edge_x = []\n",
      "    edge_y = []\n",
      "    for edge in G.edges():\n",
      "        x0, y0 = pos[edge[0]]\n",
      "        x1, y1 = pos[edge[1]]\n",
      "        edge_x.extend([x0, x1, None])\n",
      "        edge_y.extend([y0, y1, None])\n",
      "\n",
      "    edge_trace = go.Scatter(x=edge_x, y=edge_y, line=dict(\n",
      "        width=0.5, color='#888'), hoverinfo='none', mode='lines')\n",
      "\n",
      "    # Create node trace\n",
      "    node_x = []\n",
      "    node_y = []\n",
      "    for node in G.nodes():\n",
      "        x, y = pos[node]\n",
      "        node_x.append(x)\n",
      "        node_y.append(y)\n",
      "\n",
      "    node_trace = go.Scatter(x=node_x, y=node_y, mode='markers', hoverinfo='text',\n",
      "                            marker=dict(showscale=True, colorscale='Rainbow', reversescale=True, color=[], size=10,\n",
      "                                        colorbar=dict(thickness=15, title='Node Connections', xanchor='left',\n",
      "                                                      titleside='right'), line_width=2))\n",
      "\n",
      "    # Color node points by number of connections\n",
      "    node_adjacencies = []\n",
      "    node_text = []\n",
      "    for node in G.nodes():\n",
      "        adjacencies = list(G.adj[node])  # changes here\n",
      "        node_adjacencies.append(len(adjacencies))\n",
      "        node_text.append(f'{node}<br># of connections: {len(adjacencies)}')\n",
      "\n",
      "    node_trace.marker.color = node_adjacencies\n",
      "    node_trace.text = node_text\n",
      "\n",
      "    # Create the figure\n",
      "    fig = go.Figure(data=[edge_trace, node_trace],\n",
      "                    layout=go.Layout(title=title, titlefont_size=16, showlegend=False,\n",
      "                                     hovermode='closest', margin=dict(b=20, l=5, r=5, t=40),\n",
      "                                     xaxis=dict(\n",
      "                                         showgrid=False, zeroline=False, showticklabels=False),\n",
      "                                     yaxis=dict(showgrid=False, zeroline=False, showticklabels=False)))\n",
      "\n",
      "    # Show the figure\n",
      "    st.plotly_chart(fig)\n",
      "\n",
      "\n",
      "def create_annotated_text(input_string: str, word_list: List[str], annotation: str, color_code: str):\n",
      "    # Tokenize the input string\n",
      "    tokens = nltk.word_tokenize(input_string)\n",
      "\n",
      "    # Convert the list to a set for quick lookups\n",
      "    word_set = set(word_list)\n",
      "\n",
      "    # Initialize an empty list to hold the annotated text\n",
      "    annotated_text = []\n",
      "\n",
      "    for token in tokens:\n",
      "        # Check if the token is in the set\n",
      "        if token in word_set:\n",
      "            # If it is, append a tuple with the token, annotation, and color code\n",
      "            annotated_text.append((token, annotation, color_code))\n",
      "        else:\n",
      "            # If it's not, just append the token as a string\n",
      "            annotated_text.append(token)\n",
      "\n",
      "    return annotated_text\n",
      "\n",
      "\n",
      "def read_json(filename):\n",
      "    with open(filename) as f:\n",
      "        data = json.load(f)\n",
      "    return data\n",
      "\n",
      "\n",
      "def tokenize_string(input_string):\n",
      "    tokens = nltk.word_tokenize(input_string)\n",
      "    return tokens\n",
      "\n",
      "\n",
      "st.image('Assets/img/header_image.jpg')\n",
      "\n",
      "avs.add_vertical_space(5)\n",
      "\n",
      "resume_names = get_filenames_from_dir(\"Data/Processed/Resumes\")\n",
      "\n",
      "st.write(\"There are\", len(resume_names),\n",
      "         \" resumes present. Please select one from the menu below:\")\n",
      "output = st.slider('Select Resume Number', 0, len(resume_names)-1, 2)\n",
      "\n",
      "avs.add_vertical_space(5)\n",
      "\n",
      "st.write(\"You have selected \", resume_names[output], \" printing the resume\")\n",
      "selected_file = read_json(\"Data/Processed/Resumes/\"+resume_names[output])\n",
      "\n",
      "avs.add_vertical_space(2)\n",
      "st.markdown(\"#### Parsed Resume Data\")\n",
      "st.caption(\n",
      "    \"This text is parsed from your resume. This is how it'll look like after getting parsed by an ATS.\")\n",
      "st.caption(\"Utilize this to understand how to make your resume ATS friendly.\")\n",
      "avs.add_vertical_space(3)\n",
      "# st.json(selected_file)\n",
      "st.write(selected_file[\"clean_data\"])\n",
      "\n",
      "avs.add_vertical_space(3)\n",
      "st.write(\"Now let's take a look at the extracted keywords from the resume.\")\n",
      "\n",
      "annotated_text(create_annotated_text(\n",
      "    selected_file[\"clean_data\"], selected_file[\"extracted_keywords\"],\n",
      "    \"KW\", \"#0B666A\"))\n",
      "\n",
      "avs.add_vertical_space(5)\n",
      "st.write(\"Now let's take a look at the extracted entities from the resume.\")\n",
      "\n",
      "# Call the function with your data\n",
      "create_star_graph(selected_file['keyterms'], \"Entities from Resume\")\n",
      "\n",
      "df2 = pd.DataFrame(selected_file['keyterms'], columns=[\"keyword\", \"value\"])\n",
      "\n",
      "# Create the dictionary\n",
      "keyword_dict = {}\n",
      "for keyword, value in selected_file['keyterms']:\n",
      "    keyword_dict[keyword] = value*100\n",
      "\n",
      "fig = go.Figure(data=[go.Table(header=dict(values=[\"Keyword\", \"Value\"],\n",
      "                                           font=dict(size=12),\n",
      "                                           fill_color='#070A52'),\n",
      "                               cells=dict(values=[list(keyword_dict.keys()),\n",
      "                                                  list(keyword_dict.values())],\n",
      "                                          line_color='darkslategray',\n",
      "                                          fill_color='#6DA9E4'))\n",
      "                      ])\n",
      "st.plotly_chart(fig)\n",
      "\n",
      "st.divider()\n",
      "\n",
      "fig = px.treemap(df2, path=['keyword'], values='value',\n",
      "                 color_continuous_scale='Rainbow',\n",
      "                 title='Key Terms/Topics Extracted from your Resume')\n",
      "st.write(fig)\n",
      "\n",
      "avs.add_vertical_space(5)\n",
      "\n",
      "job_descriptions = get_filenames_from_dir(\"Data/Processed/JobDescription\")\n",
      "\n",
      "st.write(\"There are\", len(job_descriptions),\n",
      "         \" resumes present. Please select one from the menu below:\")\n",
      "output = st.slider('Select Job Description Number',\n",
      "                   0, len(job_descriptions)-1, 2)\n",
      "\n",
      "avs.add_vertical_space(5)\n",
      "\n",
      "st.write(\"You have selected \",\n",
      "         job_descriptions[output], \" printing the job description\")\n",
      "selected_jd = read_json(\n",
      "    \"Data/Processed/JobDescription/\"+job_descriptions[output])\n",
      "\n",
      "avs.add_vertical_space(2)\n",
      "st.markdown(\"#### Job Description\")\n",
      "st.caption(\n",
      "    \"Currently in the pipeline I'm parsing this from PDF but it'll be from txt or copy paste.\")\n",
      "avs.add_vertical_space(3)\n",
      "# st.json(selected_file)\n",
      "st.write(selected_jd[\"clean_data\"])\n",
      "\n",
      "st.markdown(\"#### Common Words between Job Description and Resumes Highlighted.\")\n",
      "\n",
      "annotated_text(create_annotated_text(\n",
      "    selected_file[\"clean_data\"], selected_jd[\"extracted_keywords\"],\n",
      "    \"JD\", \"#F24C3D\"))\n",
      "\n",
      "st.write(\"Now let's take a look at the extracted entities from the job description.\")\n",
      "\n",
      "# Call the function with your data\n",
      "create_star_graph(selected_jd['keyterms'], \"Entities from Job Description\")\n",
      "\n",
      "df2 = pd.DataFrame(selected_jd['keyterms'], columns=[\"keyword\", \"value\"])\n",
      "\n",
      "# Create the dictionary\n",
      "keyword_dict = {}\n",
      "for keyword, value in selected_jd['keyterms']:\n",
      "    keyword_dict[keyword] = value*100\n",
      "\n",
      "fig = go.Figure(data=[go.Table(header=dict(values=[\"Keyword\", \"Value\"],\n",
      "                                           font=dict(size=12),\n",
      "                                           fill_color='#070A52'),\n",
      "                               cells=dict(values=[list(keyword_dict.keys()),\n",
      "                                                  list(keyword_dict.values())],\n",
      "                                          line_color='darkslategray',\n",
      "                                          fill_color='#6DA9E4'))\n",
      "                      ])\n",
      "st.plotly_chart(fig)\n",
      "\n",
      "st.divider()\n",
      "\n",
      "fig = px.treemap(df2, path=['keyword'], values='value',\n",
      "                 color_continuous_scale='Rainbow',\n",
      "                 title='Key Terms/Topics Extracted from the selected Job Description')\n",
      "st.write(fig)\n",
      "\n",
      "avs.add_vertical_space(5)\n",
      "\n",
      "st.divider()\n",
      "\n",
      "st.markdown(\"## Vector Similarity Scores\")\n",
      "st.caption(\"Powered by Qdrant Vector Search\")\n",
      "st.info(\"These are pre-computed queries\", icon=\"\")\n",
      "st.warning(\n",
      "    \"Running Qdrant or Sentence Transformers without having capacity is not recommended\", icon=\"\")\n",
      "\n",
      "\n",
      "# Your data\n",
      "data = [\n",
      "    {'text': \"{'resume': 'Alfred Pennyworth\",\n",
      "        'query': 'Job Description Product Manager', 'score': 0.62658},\n",
      "    {'text': \"{'resume': 'Barry Allen\",\n",
      "        'query': 'Job Description Product Manager', 'score': 0.43777737},\n",
      "    {'text': \"{'resume': 'Bruce Wayne \",\n",
      "        'query': 'Job Description Product Manager', 'score': 0.39835533},\n",
      "    {'text': \"{'resume': 'JOHN DOE\",\n",
      "        'query': 'Job Description Product Manager', 'score': 0.3915512},\n",
      "    {'text': \"{'resume': 'Harvey Dent\",\n",
      "        'query': 'Job Description Product Manager', 'score': 0.3519544},\n",
      "    {'text': \"{'resume': 'Barry Allen\",\n",
      "        'query': 'Job Description Senior Full Stack Engineer', 'score': 0.6541866},\n",
      "    {'text': \"{'resume': 'Alfred Pennyworth\",\n",
      "        'query': 'Job Description Senior Full Stack Engineer', 'score': 0.59806436},\n",
      "    {'text': \"{'resume': 'JOHN DOE\",\n",
      "        'query': 'Job Description Senior Full Stack Engineer', 'score': 0.5951386},\n",
      "    {'text': \"{'resume': 'Bruce Wayne \",\n",
      "        'query': 'Job Description Senior Full Stack Engineer', 'score': 0.57700855},\n",
      "    {'text': \"{'resume': 'Harvey Dent\",\n",
      "        'query': 'Job Description Senior Full Stack Engineer', 'score': 0.38489106},\n",
      "    {'text': \"{'resume': 'Barry Allen\",\n",
      "        'query': 'Job Description Front End Engineer', 'score': 0.76813436},\n",
      "    {'text': \"{'resume': 'Bruce Wayne'\",\n",
      "        'query': 'Job Description Front End Engineer', 'score': 0.60440844},\n",
      "    {'text': \"{'resume': 'JOHN DOE\",\n",
      "        'query': 'Job Description Front End Engineer', 'score': 0.56080043},\n",
      "    {'text': \"{'resume': 'Alfred Pennyworth\",\n",
      "        'query': 'Job Description Front End Engineer', 'score': 0.5395049},\n",
      "    {'text': \"{'resume': 'Harvey Dent\",\n",
      "        'query': 'Job Description Front End Engineer', 'score': 0.3859515},\n",
      "    {'text': \"{'resume': 'JOHN DOE\",\n",
      "        'query': 'Job Description Java Developer', 'score': 0.5449441},\n",
      "    {'text': \"{'resume': 'Alfred Pennyworth\",\n",
      "        'query': 'Job Description Java Developer', 'score': 0.53476423},\n",
      "    {'text': \"{'resume': 'Barry Allen\",\n",
      "        'query': 'Job Description Java Developer', 'score': 0.5313871},\n",
      "    {'text': \"{'resume': 'Bruce Wayne \",\n",
      "        'query': 'Job Description Java Developer', 'score': 0.44446343},\n",
      "    {'text': \"{'resume': 'Harvey Dent\",\n",
      "        'query': 'Job Description Java Developer', 'score': 0.3616274}\n",
      "]\n",
      "\n",
      "# Create a DataFrame\n",
      "df = pd.DataFrame(data)\n",
      "\n",
      "# Create different DataFrames based on the query and sort by score\n",
      "df1 = df[df['query'] ==\n",
      "         'Job Description Product Manager'].sort_values(by='score', ascending=False)\n",
      "df2 = df[df['query'] ==\n",
      "         'Job Description Senior Full Stack Engineer'].sort_values(by='score', ascending=False)\n",
      "df3 = df[df['query'] == 'Job Description Front End Engineer'].sort_values(\n",
      "    by='score', ascending=False)\n",
      "df4 = df[df['query'] == 'Job Description Java Developer'].sort_values(\n",
      "    by='score', ascending=False)\n",
      "\n",
      "\n",
      "def plot_df(df, title):\n",
      "    fig = px.bar(df, x='text', y=df['score']*100, title=title)\n",
      "    st.plotly_chart(fig)\n",
      "\n",
      "\n",
      "st.markdown(\"### Bar plots of scores based on similarity to Job Description.\")\n",
      "\n",
      "st.subheader(\":blue[Legend]\")\n",
      "st.text(\"Alfred Pennyworth :  Product Manager\")\n",
      "st.text(\"Barry Allen :  Front End Developer\")\n",
      "st.text(\"Harvey Dent :  Machine Learning Engineer\")\n",
      "st.text(\"Bruce Wayne :  Fullstack Developer (MERN)\")\n",
      "st.text(\"John Doe :  Fullstack Developer (Java)\")\n",
      "\n",
      "\n",
      "plot_df(df1, 'Job Description Product Manager 10+ Years of Exper')\n",
      "plot_df(df2, 'Job Description Senior Full Stack Engineer 5+ Year')\n",
      "plot_df(df3, 'Job Description Front End Engineer 2 Years of Expe')\n",
      "plot_df(df4, 'Job Description Java Developer 3 Years of Experien')\n",
      "\n",
      "import networkx as nx\n",
      "from typing import List\n",
      "import streamlit as st\n",
      "import pandas as pd\n",
      "import json\n",
      "import plotly.express as px\n",
      "import plotly.graph_objects as go\n",
      "from scripts.utils.ReadFiles import get_filenames_from_dir\n",
      "from streamlit_extras import add_vertical_space as avs\n",
      "from annotated_text import annotated_text, parameters\n",
      "from streamlit_extras.badges import badge\n",
      "import nltk\n",
      "nltk.download('punkt')\n",
      "\n",
      "parameters.SHOW_LABEL_SEPARATOR = False\n",
      "parameters.BORDER_RADIUS = 3\n",
      "parameters.PADDING = \"0.5 0.25rem\"\n",
      "\n",
      "\n",
      "def create_star_graph(nodes_and_weights, title):\n",
      "    # Create an empty graph\n",
      "    G = nx.Graph()\n",
      "\n",
      "    # Add the central node\n",
      "    central_node = \"resume\"\n",
      "    G.add_node(central_node)\n",
      "\n",
      "    # Add nodes and edges with weights to the graph\n",
      "    for node, weight in nodes_and_weights:\n",
      "        G.add_node(node)\n",
      "        G.add_edge(central_node, node, weight=weight*100)\n",
      "\n",
      "    # Get position layout for nodes\n",
      "    pos = nx.spring_layout(G)\n",
      "\n",
      "    # Create edge trace\n",
      "    edge_x = []\n",
      "    edge_y = []\n",
      "    for edge in G.edges():\n",
      "        x0, y0 = pos[edge[0]]\n",
      "        x1, y1 = pos[edge[1]]\n",
      "        edge_x.extend([x0, x1, None])\n",
      "        edge_y.extend([y0, y1, None])\n",
      "\n",
      "    edge_trace = go.Scatter(x=edge_x, y=edge_y, line=dict(\n",
      "        width=0.5, color='#888'), hoverinfo='none', mode='lines')\n",
      "\n",
      "    # Create node trace\n",
      "    node_x = []\n",
      "    node_y = []\n",
      "    for node in G.nodes():\n",
      "        x, y = pos[node]\n",
      "        node_x.append(x)\n",
      "        node_y.append(y)\n",
      "\n",
      "    node_trace = go.Scatter(x=node_x, y=node_y, mode='markers', hoverinfo='text',\n",
      "                            marker=dict(showscale=True, colorscale='Rainbow', reversescale=True, color=[], size=10,\n",
      "                                        colorbar=dict(thickness=15, title='Node Connections', xanchor='left',\n",
      "                                                      titleside='right'), line_width=2))\n",
      "\n",
      "    # Color node points by number of connections\n",
      "    node_adjacencies = []\n",
      "    node_text = []\n",
      "    for node in G.nodes():\n",
      "        adjacencies = list(G.adj[node])  # changes here\n",
      "        node_adjacencies.append(len(adjacencies))\n",
      "        node_text.append(f'{node}<br># of connections: {len(adjacencies)}')\n",
      "\n",
      "    node_trace.marker.color = node_adjacencies\n",
      "    node_trace.text = node_text\n",
      "\n",
      "    # Create the figure\n",
      "    fig = go.Figure(data=[edge_trace, node_trace],\n",
      "                    layout=go.Layout(title=title, titlefont_size=16, showlegend=False,\n",
      "                                     hovermode='closest', margin=dict(b=20, l=5, r=5, t=40),\n",
      "                                     xaxis=dict(\n",
      "                                         showgrid=False, zeroline=False, showticklabels=False),\n",
      "                                     yaxis=dict(showgrid=False, zeroline=False, showticklabels=False)))\n",
      "\n",
      "    # Show the figure\n",
      "    st.plotly_chart(fig)\n",
      "\n",
      "\n",
      "def create_annotated_text(input_string: str, word_list: List[str], annotation: str, color_code: str):\n",
      "    # Tokenize the input string\n",
      "    tokens = nltk.word_tokenize(input_string)\n",
      "\n",
      "    # Convert the list to a set for quick lookups\n",
      "    word_set = set(word_list)\n",
      "\n",
      "    # Initialize an empty list to hold the annotated text\n",
      "    annotated_text = []\n",
      "\n",
      "    for token in tokens:\n",
      "        # Check if the token is in the set\n",
      "        if token in word_set:\n",
      "            # If it is, append a tuple with the token, annotation, and color code\n",
      "            annotated_text.append((token, annotation, color_code))\n",
      "        else:\n",
      "            # If it's not, just append the token as a string\n",
      "            annotated_text.append(token)\n",
      "\n",
      "    return annotated_text\n",
      "\n",
      "\n",
      "def read_json(filename):\n",
      "    with open(filename) as f:\n",
      "        data = json.load(f)\n",
      "    return data\n",
      "\n",
      "\n",
      "def tokenize_string(input_string):\n",
      "    tokens = nltk.word_tokenize(input_string)\n",
      "    return tokens\n",
      "\n",
      "\n",
      "st.image('Assets/img/header_image.jpg')\n",
      "\n",
      "st.title(':blue[Resume Matcher]')\n",
      "st.subheader(\n",
      "    'Free and Open Source ATS to help your resume pass the screening stage.')\n",
      "st.markdown(\n",
      "    \"Check the website [www.resumematcher.fyi](https://www.resumematcher.fyi/)\")\n",
      "st.markdown(\n",
      "    ' Give Resume Matcher a Star on [GitHub](https://github.com/srbhr/Naive-Resume-Matching/)')\n",
      "badge(type=\"github\", name=\"srbhr/Naive-Resume-Matching\")\n",
      "\n",
      "st.text('For updates follow me on Twitter.')\n",
      "badge(type=\"twitter\", name=\"_srbhr_\")\n",
      "\n",
      "st.markdown(\n",
      "    'If you like the project and would like to further help in development please consider ')\n",
      "badge(type=\"buymeacoffee\", name=\"srbhr\")\n",
      "\n",
      "avs.add_vertical_space(5)\n",
      "\n",
      "resume_names = get_filenames_from_dir(\"Data/Processed/Resumes\")\n",
      "\n",
      "st.write(\"There are\", len(resume_names),\n",
      "         \" resumes present. Please select one from the menu below:\")\n",
      "output = st.slider('Select Resume Number', 0, len(resume_names)-1, 2)\n",
      "\n",
      "avs.add_vertical_space(5)\n",
      "\n",
      "st.write(\"You have selected \", resume_names[output], \" printing the resume\")\n",
      "selected_file = read_json(\"Data/Processed/Resumes/\"+resume_names[output])\n",
      "\n",
      "avs.add_vertical_space(2)\n",
      "st.markdown(\"#### Parsed Resume Data\")\n",
      "st.caption(\n",
      "    \"This text is parsed from your resume. This is how it'll look like after getting parsed by an ATS.\")\n",
      "st.caption(\"Utilize this to understand how to make your resume ATS friendly.\")\n",
      "avs.add_vertical_space(3)\n",
      "# st.json(selected_file)\n",
      "st.write(selected_file[\"clean_data\"])\n",
      "\n",
      "avs.add_vertical_space(3)\n",
      "st.write(\"Now let's take a look at the extracted keywords from the resume.\")\n",
      "\n",
      "annotated_text(create_annotated_text(\n",
      "    selected_file[\"clean_data\"], selected_file[\"extracted_keywords\"],\n",
      "    \"KW\", \"#0B666A\"))\n",
      "\n",
      "avs.add_vertical_space(5)\n",
      "st.write(\"Now let's take a look at the extracted entities from the resume.\")\n",
      "\n",
      "# Call the function with your data\n",
      "create_star_graph(selected_file['keyterms'], \"Entities from Resume\")\n",
      "\n",
      "df2 = pd.DataFrame(selected_file['keyterms'], columns=[\"keyword\", \"value\"])\n",
      "\n",
      "# Create the dictionary\n",
      "keyword_dict = {}\n",
      "for keyword, value in selected_file['keyterms']:\n",
      "    keyword_dict[keyword] = value*100\n",
      "\n",
      "fig = go.Figure(data=[go.Table(header=dict(values=[\"Keyword\", \"Value\"],\n",
      "                                           font=dict(size=12),\n",
      "                                           fill_color='#070A52'),\n",
      "                               cells=dict(values=[list(keyword_dict.keys()),\n",
      "                                                  list(keyword_dict.values())],\n",
      "                                          line_color='darkslategray',\n",
      "                                          fill_color='#6DA9E4'))\n",
      "                      ])\n",
      "st.plotly_chart(fig)\n",
      "\n",
      "st.divider()\n",
      "\n",
      "fig = px.treemap(df2, path=['keyword'], values='value',\n",
      "                 color_continuous_scale='Rainbow',\n",
      "                 title='Key Terms/Topics Extracted from your Resume')\n",
      "st.write(fig)\n",
      "\n",
      "avs.add_vertical_space(5)\n",
      "\n",
      "job_descriptions = get_filenames_from_dir(\"Data/Processed/JobDescription\")\n",
      "\n",
      "st.write(\"There are\", len(job_descriptions),\n",
      "         \" resumes present. Please select one from the menu below:\")\n",
      "output = st.slider('Select Job Description Number',\n",
      "                   0, len(job_descriptions)-1, 2)\n",
      "\n",
      "avs.add_vertical_space(5)\n",
      "\n",
      "st.write(\"You have selected \",\n",
      "         job_descriptions[output], \" printing the job description\")\n",
      "selected_jd = read_json(\n",
      "    \"Data/Processed/JobDescription/\"+job_descriptions[output])\n",
      "\n",
      "avs.add_vertical_space(2)\n",
      "st.markdown(\"#### Job Description\")\n",
      "st.caption(\n",
      "    \"Currently in the pipeline I'm parsing this from PDF but it'll be from txt or copy paste.\")\n",
      "avs.add_vertical_space(3)\n",
      "# st.json(selected_file)\n",
      "st.write(selected_jd[\"clean_data\"])\n",
      "\n",
      "st.markdown(\"#### Common Words between Job Description and Resumes Highlighted.\")\n",
      "\n",
      "annotated_text(create_annotated_text(\n",
      "    selected_file[\"clean_data\"], selected_jd[\"extracted_keywords\"],\n",
      "    \"JD\", \"#F24C3D\"))\n",
      "\n",
      "st.write(\"Now let's take a look at the extracted entities from the job description.\")\n",
      "\n",
      "# Call the function with your data\n",
      "create_star_graph(selected_jd['keyterms'], \"Entities from Job Description\")\n",
      "\n",
      "df2 = pd.DataFrame(selected_jd['keyterms'], columns=[\"keyword\", \"value\"])\n",
      "\n",
      "# Create the dictionary\n",
      "keyword_dict = {}\n",
      "for keyword, value in selected_jd['keyterms']:\n",
      "    keyword_dict[keyword] = value*100\n",
      "\n",
      "fig = go.Figure(data=[go.Table(header=dict(values=[\"Keyword\", \"Value\"],\n",
      "                                           font=dict(size=12),\n",
      "                                           fill_color='#070A52'),\n",
      "                               cells=dict(values=[list(keyword_dict.keys()),\n",
      "                                                  list(keyword_dict.values())],\n",
      "                                          line_color='darkslategray',\n",
      "                                          fill_color='#6DA9E4'))\n",
      "                      ])\n",
      "st.plotly_chart(fig)\n",
      "\n",
      "st.divider()\n",
      "\n",
      "fig = px.treemap(df2, path=['keyword'], values='value',\n",
      "                 color_continuous_scale='Rainbow',\n",
      "                 title='Key Terms/Topics Extracted from the selected Job Description')\n",
      "st.write(fig)\n",
      "\n",
      "avs.add_vertical_space(3)\n",
      "\n",
      "st.title(':blue[Resume Matcher]')\n",
      "st.subheader(\n",
      "    'Free and Open Source ATS to help your resume pass the screening stage.')\n",
      "st.markdown(\n",
      "    ' Give Resume Matcher a Star on [GitHub](https://github.com/srbhr/Naive-Resume-Matching/)')\n",
      "badge(type=\"github\", name=\"srbhr/Naive-Resume-Matching\")\n",
      "\n",
      "st.text('For updates follow me on Twitter.')\n",
      "badge(type=\"twitter\", name=\"_srbhr_\")\n",
      "\n",
      "st.markdown(\n",
      "    'If you like the project and would like to further help in development please consider ')\n",
      "badge(type=\"buymeacoffee\", name=\"srbhr\")\n",
      "\n",
      "import networkx as nx\n",
      "from typing import List\n",
      "import streamlit as st\n",
      "import pandas as pd\n",
      "import json\n",
      "import plotly.express as px\n",
      "import plotly.graph_objects as go\n",
      "from scripts.utils.ReadFiles import get_filenames_from_dir\n",
      "from streamlit_extras import add_vertical_space as avs\n",
      "from annotated_text import annotated_text, parameters\n",
      "from streamlit_extras.badges import badge\n",
      "import nltk\n",
      "nltk.download('punkt')\n",
      "\n",
      "parameters.SHOW_LABEL_SEPARATOR = False\n",
      "parameters.BORDER_RADIUS = 3\n",
      "parameters.PADDING = \"0.5 0.25rem\"\n",
      "\n",
      "\n",
      "def create_star_graph(nodes_and_weights, title):\n",
      "    # Create an empty graph\n",
      "    G = nx.Graph()\n",
      "\n",
      "    # Add the central node\n",
      "    central_node = \"resume\"\n",
      "    G.add_node(central_node)\n",
      "\n",
      "    # Add nodes and edges with weights to the graph\n",
      "    for node, weight in nodes_and_weights:\n",
      "        G.add_node(node)\n",
      "        G.add_edge(central_node, node, weight=weight*100)\n",
      "\n",
      "    # Get position layout for nodes\n",
      "    pos = nx.spring_layout(G)\n",
      "\n",
      "    # Create edge trace\n",
      "    edge_x = []\n",
      "    edge_y = []\n",
      "    for edge in G.edges():\n",
      "        x0, y0 = pos[edge[0]]\n",
      "        x1, y1 = pos[edge[1]]\n",
      "        edge_x.extend([x0, x1, None])\n",
      "        edge_y.extend([y0, y1, None])\n",
      "\n",
      "    edge_trace = go.Scatter(x=edge_x, y=edge_y, line=dict(\n",
      "        width=0.5, color='#888'), hoverinfo='none', mode='lines')\n",
      "\n",
      "    # Create node trace\n",
      "    node_x = []\n",
      "    node_y = []\n",
      "    for node in G.nodes():\n",
      "        x, y = pos[node]\n",
      "        node_x.append(x)\n",
      "        node_y.append(y)\n",
      "\n",
      "    node_trace = go.Scatter(x=node_x, y=node_y, mode='markers', hoverinfo='text',\n",
      "                            marker=dict(showscale=True, colorscale='Rainbow', reversescale=True, color=[], size=10,\n",
      "                                        colorbar=dict(thickness=15, title='Node Connections', xanchor='left',\n",
      "                                                      titleside='right'), line_width=2))\n",
      "\n",
      "    # Color node points by number of connections\n",
      "    node_adjacencies = []\n",
      "    node_text = []\n",
      "    for node in G.nodes():\n",
      "        adjacencies = list(G.adj[node])  # changes here\n",
      "        node_adjacencies.append(len(adjacencies))\n",
      "        node_text.append(f'{node}<br># of connections: {len(adjacencies)}')\n",
      "\n",
      "    node_trace.marker.color = node_adjacencies\n",
      "    node_trace.text = node_text\n",
      "\n",
      "    # Create the figure\n",
      "    fig = go.Figure(data=[edge_trace, node_trace],\n",
      "                    layout=go.Layout(title=title, titlefont_size=16, showlegend=False,\n",
      "                                     hovermode='closest', margin=dict(b=20, l=5, r=5, t=40),\n",
      "                                     xaxis=dict(\n",
      "                                         showgrid=False, zeroline=False, showticklabels=False),\n",
      "                                     yaxis=dict(showgrid=False, zeroline=False, showticklabels=False)))\n",
      "\n",
      "    # Show the figure\n",
      "    st.plotly_chart(fig)\n",
      "\n",
      "\n",
      "def create_annotated_text(input_string: str, word_list: List[str], annotation: str, color_code: str):\n",
      "    # Tokenize the input string\n",
      "    tokens = nltk.word_tokenize(input_string)\n",
      "\n",
      "    # Convert the list to a set for quick lookups\n",
      "    word_set = set(word_list)\n",
      "\n",
      "    # Initialize an empty list to hold the annotated text\n",
      "    annotated_text = []\n",
      "\n",
      "    for token in tokens:\n",
      "        # Check if the token is in the set\n",
      "        if token in word_set:\n",
      "            # If it is, append a tuple with the token, annotation, and color code\n",
      "            annotated_text.append((token, annotation, color_code))\n",
      "        else:\n",
      "            # If it's not, just append the token as a string\n",
      "            annotated_text.append(token)\n",
      "\n",
      "    return annotated_text\n",
      "\n",
      "\n",
      "def read_json(filename):\n",
      "    with open(filename) as f:\n",
      "        data = json.load(f)\n",
      "    return data\n",
      "\n",
      "\n",
      "def tokenize_string(input_string):\n",
      "    tokens = nltk.word_tokenize(input_string)\n",
      "    return tokens\n",
      "\n",
      "\n",
      "st.image('Assets/img/header_image.jpg')\n",
      "\n",
      "st.title(':blue[Resume Matcher]')\n",
      "st.subheader(\n",
      "    'Free and Open Source ATS to help your resume pass the screening stage.')\n",
      "st.markdown(\n",
      "    \"Check the website [www.resumematcher.fyi](https://www.resumematcher.fyi/)\")\n",
      "st.markdown(\n",
      "    ' Give Resume Matcher a Star on [GitHub](https://github.com/srbhr/Naive-Resume-Matching/)')\n",
      "badge(type=\"github\", name=\"srbhr/Naive-Resume-Matching\")\n",
      "\n",
      "st.text('For updates follow me on Twitter.')\n",
      "badge(type=\"twitter\", name=\"_srbhr_\")\n",
      "\n",
      "st.markdown(\n",
      "    'If you like the project and would like to further help in development please consider ')\n",
      "badge(type=\"buymeacoffee\", name=\"srbhr\")\n",
      "\n",
      "avs.add_vertical_space(5)\n",
      "\n",
      "resume_names = get_filenames_from_dir(\"Data/Processed/Resumes\")\n",
      "\n",
      "st.write(\"There are\", len(resume_names),\n",
      "         \" resumes present. Please select one from the menu below:\")\n",
      "output = st.slider('Select Resume Number', 0, len(resume_names)-1, 2)\n",
      "\n",
      "avs.add_vertical_space(5)\n",
      "\n",
      "st.write(\"You have selected \", resume_names[output], \" printing the resume\")\n",
      "selected_file = read_json(\"Data/Processed/Resumes/\"+resume_names[output])\n",
      "\n",
      "avs.add_vertical_space(2)\n",
      "st.markdown(\"#### Parsed Resume Data\")\n",
      "st.caption(\n",
      "    \"This text is parsed from your resume. This is how it'll look like after getting parsed by an ATS.\")\n",
      "st.caption(\"Utilize this to understand how to make your resume ATS friendly.\")\n",
      "avs.add_vertical_space(3)\n",
      "# st.json(selected_file)\n",
      "st.write(selected_file[\"clean_data\"])\n",
      "\n",
      "avs.add_vertical_space(3)\n",
      "st.write(\"Now let's take a look at the extracted keywords from the resume.\")\n",
      "\n",
      "annotated_text(create_annotated_text(\n",
      "    selected_file[\"clean_data\"], selected_file[\"extracted_keywords\"],\n",
      "    \"KW\", \"#0B666A\"))\n",
      "\n",
      "avs.add_vertical_space(5)\n",
      "st.write(\"Now let's take a look at the extracted entities from the resume.\")\n",
      "\n",
      "# Call the function with your data\n",
      "create_star_graph(selected_file['keyterms'], \"Entities from Resume\")\n",
      "\n",
      "df2 = pd.DataFrame(selected_file['keyterms'], columns=[\"keyword\", \"value\"])\n",
      "\n",
      "# Create the dictionary\n",
      "keyword_dict = {}\n",
      "for keyword, value in selected_file['keyterms']:\n",
      "    keyword_dict[keyword] = value*100\n",
      "\n",
      "fig = go.Figure(data=[go.Table(header=dict(values=[\"Keyword\", \"Value\"],\n",
      "                                           font=dict(size=12),\n",
      "                                           fill_color='#070A52'),\n",
      "                               cells=dict(values=[list(keyword_dict.keys()),\n",
      "                                                  list(keyword_dict.values())],\n",
      "                                          line_color='darkslategray',\n",
      "                                          fill_color='#6DA9E4'))\n",
      "                      ])\n",
      "st.plotly_chart(fig)\n",
      "\n",
      "st.divider()\n",
      "\n",
      "fig = px.treemap(df2, path=['keyword'], values='value',\n",
      "                 color_continuous_scale='Rainbow',\n",
      "                 title='Key Terms/Topics Extracted from your Resume')\n",
      "st.write(fig)\n",
      "\n",
      "avs.add_vertical_space(5)\n",
      "\n",
      "job_descriptions = get_filenames_from_dir(\"Data/Processed/JobDescription\")\n",
      "\n",
      "st.write(\"There are\", len(job_descriptions),\n",
      "         \" resumes present. Please select one from the menu below:\")\n",
      "output = st.slider('Select Job Description Number',\n",
      "                   0, len(job_descriptions)-1, 2)\n",
      "\n",
      "avs.add_vertical_space(5)\n",
      "\n",
      "st.write(\"You have selected \",\n",
      "         job_descriptions[output], \" printing the job description\")\n",
      "selected_jd = read_json(\n",
      "    \"Data/Processed/JobDescription/\"+job_descriptions[output])\n",
      "\n",
      "avs.add_vertical_space(2)\n",
      "st.markdown(\"#### Job Description\")\n",
      "st.caption(\n",
      "    \"Currently in the pipeline I'm parsing this from PDF but it'll be from txt or copy paste.\")\n",
      "avs.add_vertical_space(3)\n",
      "# st.json(selected_file)\n",
      "st.write(selected_jd[\"clean_data\"])\n",
      "\n",
      "st.markdown(\"#### Common Words between Job Description and Resumes Highlighted.\")\n",
      "\n",
      "annotated_text(create_annotated_text(\n",
      "    selected_file[\"clean_data\"], selected_jd[\"extracted_keywords\"],\n",
      "    \"JD\", \"#F24C3D\"))\n",
      "\n",
      "st.write(\"Now let's take a look at the extracted entities from the job description.\")\n",
      "\n",
      "# Call the function with your data\n",
      "create_star_graph(selected_jd['keyterms'], \"Entities from Job Description\")\n",
      "\n",
      "df2 = pd.DataFrame(selected_jd['keyterms'], columns=[\"keyword\", \"value\"])\n",
      "\n",
      "# Create the dictionary\n",
      "keyword_dict = {}\n",
      "for keyword, value in selected_jd['keyterms']:\n",
      "    keyword_dict[keyword] = value*100\n",
      "\n",
      "fig = go.Figure(data=[go.Table(header=dict(values=[\"Keyword\", \"Value\"],\n",
      "                                           font=dict(size=12),\n",
      "                                           fill_color='#070A52'),\n",
      "                               cells=dict(values=[list(keyword_dict.keys()),\n",
      "                                                  list(keyword_dict.values())],\n",
      "                                          line_color='darkslategray',\n",
      "                                          fill_color='#6DA9E4'))\n",
      "                      ])\n",
      "st.plotly_chart(fig)\n",
      "\n",
      "st.divider()\n",
      "\n",
      "fig = px.treemap(df2, path=['keyword'], values='value',\n",
      "                 color_continuous_scale='Rainbow',\n",
      "                 title='Key Terms/Topics Extracted from the selected Job Description')\n",
      "st.write(fig)\n",
      "\n",
      "avs.add_vertical_space(5)\n",
      "\n",
      "st.divider()\n",
      "\n",
      "st.markdown(\"## Vector Similarity Scores\")\n",
      "st.caption(\"Powered by Qdrant Vector Search\")\n",
      "st.info(\"These are pre-computed queries\", icon=\"\")\n",
      "st.warning(\n",
      "    \"Running Qdrant or Sentence Transformers without having capacity is not recommended\", icon=\"\")\n",
      "\n",
      "\n",
      "# Your data\n",
      "data = [\n",
      "    {'text': \"{'resume': 'Alfred Pennyworth\",\n",
      "        'query': 'Job Description Product Manager', 'score': 0.62658},\n",
      "    {'text': \"{'resume': 'Barry Allen\",\n",
      "        'query': 'Job Description Product Manager', 'score': 0.43777737},\n",
      "    {'text': \"{'resume': 'Bruce Wayne \",\n",
      "        'query': 'Job Description Product Manager', 'score': 0.39835533},\n",
      "    {'text': \"{'resume': 'JOHN DOE\",\n",
      "        'query': 'Job Description Product Manager', 'score': 0.3915512},\n",
      "    {'text': \"{'resume': 'Harvey Dent\",\n",
      "        'query': 'Job Description Product Manager', 'score': 0.3519544},\n",
      "    {'text': \"{'resume': 'Barry Allen\",\n",
      "        'query': 'Job Description Senior Full Stack Engineer', 'score': 0.6541866},\n",
      "    {'text': \"{'resume': 'Alfred Pennyworth\",\n",
      "        'query': 'Job Description Senior Full Stack Engineer', 'score': 0.59806436},\n",
      "    {'text': \"{'resume': 'JOHN DOE\",\n",
      "        'query': 'Job Description Senior Full Stack Engineer', 'score': 0.5951386},\n",
      "    {'text': \"{'resume': 'Bruce Wayne \",\n",
      "        'query': 'Job Description Senior Full Stack Engineer', 'score': 0.57700855},\n",
      "    {'text': \"{'resume': 'Harvey Dent\",\n",
      "        'query': 'Job Description Senior Full Stack Engineer', 'score': 0.38489106},\n",
      "    {'text': \"{'resume': 'Barry Allen\",\n",
      "        'query': 'Job Description Front End Engineer', 'score': 0.76813436},\n",
      "    {'text': \"{'resume': 'Bruce Wayne'\",\n",
      "        'query': 'Job Description Front End Engineer', 'score': 0.60440844},\n",
      "    {'text': \"{'resume': 'JOHN DOE\",\n",
      "        'query': 'Job Description Front End Engineer', 'score': 0.56080043},\n",
      "    {'text': \"{'resume': 'Alfred Pennyworth\",\n",
      "        'query': 'Job Description Front End Engineer', 'score': 0.5395049},\n",
      "    {'text': \"{'resume': 'Harvey Dent\",\n",
      "        'query': 'Job Description Front End Engineer', 'score': 0.3859515},\n",
      "    {'text': \"{'resume': 'JOHN DOE\",\n",
      "        'query': 'Job Description Java Developer', 'score': 0.5449441},\n",
      "    {'text': \"{'resume': 'Alfred Pennyworth\",\n",
      "        'query': 'Job Description Java Developer', 'score': 0.53476423},\n",
      "    {'text': \"{'resume': 'Barry Allen\",\n",
      "        'query': 'Job Description Java Developer', 'score': 0.5313871},\n",
      "    {'text': \"{'resume': 'Bruce Wayne \",\n",
      "        'query': 'Job Description Java Developer', 'score': 0.44446343},\n",
      "    {'text': \"{'resume': 'Harvey Dent\",\n",
      "        'query': 'Job Description Java Developer', 'score': 0.3616274}\n",
      "]\n",
      "\n",
      "# Create a DataFrame\n",
      "df = pd.DataFrame(data)\n",
      "\n",
      "# Create different DataFrames based on the query and sort by score\n",
      "df1 = df[df['query'] ==\n",
      "         'Job Description Product Manager'].sort_values(by='score', ascending=False)\n",
      "df2 = df[df['query'] ==\n",
      "         'Job Description Senior Full Stack Engineer'].sort_values(by='score', ascending=False)\n",
      "df3 = df[df['query'] == 'Job Description Front End Engineer'].sort_values(\n",
      "    by='score', ascending=False)\n",
      "df4 = df[df['query'] == 'Job Description Java Developer'].sort_values(\n",
      "    by='score', ascending=False)\n",
      "\n",
      "\n",
      "def plot_df(df, title):\n",
      "    fig = px.bar(df, x='text', y=df['score']*100, title=title)\n",
      "    st.plotly_chart(fig)\n",
      "\n",
      "\n",
      "st.markdown(\"### Bar plots of scores based on similarity to Job Description.\")\n",
      "\n",
      "st.subheader(\":blue[Legend]\")\n",
      "st.text(\"Alfred Pennyworth :  Product Manager\")\n",
      "st.text(\"Barry Allen :  Front End Developer\")\n",
      "st.text(\"Harvey Dent :  Machine Learning Engineer\")\n",
      "st.text(\"Bruce Wayne :  Fullstack Developer (MERN)\")\n",
      "st.text(\"John Doe :  Fullstack Developer (Java)\")\n",
      "\n",
      "\n",
      "plot_df(df1, 'Job Description Product Manager 10+ Years of Exper')\n",
      "plot_df(df2, 'Job Description Senior Full Stack Engineer 5+ Year')\n",
      "plot_df(df3, 'Job Description Front End Engineer 2 Years of Expe')\n",
      "plot_df(df4, 'Job Description Java Developer 3 Years of Experien')\n",
      "\n",
      "\n",
      "avs.add_vertical_space(3)\n",
      "\n",
      "st.markdown(\n",
      "    ' Give Resume Matcher a Star on [GitHub](https://github.com/srbhr/Naive-Resume-Matching/)')\n",
      "badge(type=\"github\", name=\"srbhr/Naive-Resume-Matching\")\n",
      "\n",
      "st.text('For updates follow me on Twitter.')\n",
      "badge(type=\"twitter\", name=\"_srbhr_\")\n",
      "\n",
      "st.markdown(\n",
      "    'If you like the project and would like to further help in development please consider ')\n",
      "badge(type=\"buymeacoffee\", name=\"srbhr\")\n",
      "\n",
      "import networkx as nx\n",
      "from typing import List\n",
      "import streamlit as st\n",
      "import pandas as pd\n",
      "import json\n",
      "import plotly.express as px\n",
      "import plotly.graph_objects as go\n",
      "from scripts.utils.ReadFiles import get_filenames_from_dir\n",
      "from streamlit_extras import add_vertical_space as avs\n",
      "from annotated_text import annotated_text, parameters\n",
      "from streamlit_extras.badges import badge\n",
      "import nltk\n",
      "nltk.download('punkt')\n",
      "\n",
      "parameters.SHOW_LABEL_SEPARATOR = False\n",
      "parameters.BORDER_RADIUS = 3\n",
      "parameters.PADDING = \"0.5 0.25rem\"\n",
      "\n",
      "\n",
      "def create_star_graph(nodes_and_weights, title):\n",
      "    # Create an empty graph\n",
      "    G = nx.Graph()\n",
      "\n",
      "    # Add the central node\n",
      "    central_node = \"resume\"\n",
      "    G.add_node(central_node)\n",
      "\n",
      "    # Add nodes and edges with weights to the graph\n",
      "    for node, weight in nodes_and_weights:\n",
      "        G.add_node(node)\n",
      "        G.add_edge(central_node, node, weight=weight*100)\n",
      "\n",
      "    # Get position layout for nodes\n",
      "    pos = nx.spring_layout(G)\n",
      "\n",
      "    # Create edge trace\n",
      "    edge_x = []\n",
      "    edge_y = []\n",
      "    for edge in G.edges():\n",
      "        x0, y0 = pos[edge[0]]\n",
      "        x1, y1 = pos[edge[1]]\n",
      "        edge_x.extend([x0, x1, None])\n",
      "        edge_y.extend([y0, y1, None])\n",
      "\n",
      "    edge_trace = go.Scatter(x=edge_x, y=edge_y, line=dict(\n",
      "        width=0.5, color='#888'), hoverinfo='none', mode='lines')\n",
      "\n",
      "    # Create node trace\n",
      "    node_x = []\n",
      "    node_y = []\n",
      "    for node in G.nodes():\n",
      "        x, y = pos[node]\n",
      "        node_x.append(x)\n",
      "        node_y.append(y)\n",
      "\n",
      "    node_trace = go.Scatter(x=node_x, y=node_y, mode='markers', hoverinfo='text',\n",
      "                            marker=dict(showscale=True, colorscale='Rainbow', reversescale=True, color=[], size=10,\n",
      "                                        colorbar=dict(thickness=15, title='Node Connections', xanchor='left',\n",
      "                                                      titleside='right'), line_width=2))\n",
      "\n",
      "    # Color node points by number of connections\n",
      "    node_adjacencies = []\n",
      "    node_text = []\n",
      "    for node in G.nodes():\n",
      "        adjacencies = list(G.adj[node])  # changes here\n",
      "        node_adjacencies.append(len(adjacencies))\n",
      "        node_text.append(f'{node}<br># of connections: {len(adjacencies)}')\n",
      "\n",
      "    node_trace.marker.color = node_adjacencies\n",
      "    node_trace.text = node_text\n",
      "\n",
      "    # Create the figure\n",
      "    fig = go.Figure(data=[edge_trace, node_trace],\n",
      "                    layout=go.Layout(title=title, titlefont_size=16, showlegend=False,\n",
      "                                     hovermode='closest', margin=dict(b=20, l=5, r=5, t=40),\n",
      "                                     xaxis=dict(\n",
      "                                         showgrid=False, zeroline=False, showticklabels=False),\n",
      "                                     yaxis=dict(showgrid=False, zeroline=False, showticklabels=False)))\n",
      "\n",
      "    # Show the figure\n",
      "    st.plotly_chart(fig)\n",
      "\n",
      "\n",
      "def create_annotated_text(input_string: str, word_list: List[str], annotation: str, color_code: str):\n",
      "    # Tokenize the input string\n",
      "    tokens = nltk.word_tokenize(input_string)\n",
      "\n",
      "    # Convert the list to a set for quick lookups\n",
      "    word_set = set(word_list)\n",
      "\n",
      "    # Initialize an empty list to hold the annotated text\n",
      "    annotated_text = []\n",
      "\n",
      "    for token in tokens:\n",
      "        # Check if the token is in the set\n",
      "        if token in word_set:\n",
      "            # If it is, append a tuple with the token, annotation, and color code\n",
      "            annotated_text.append((token, annotation, color_code))\n",
      "        else:\n",
      "            # If it's not, just append the token as a string\n",
      "            annotated_text.append(token)\n",
      "\n",
      "    return annotated_text\n",
      "\n",
      "\n",
      "def read_json(filename):\n",
      "    with open(filename) as f:\n",
      "        data = json.load(f)\n",
      "    return data\n",
      "\n",
      "\n",
      "def tokenize_string(input_string):\n",
      "    tokens = nltk.word_tokenize(input_string)\n",
      "    return tokens\n",
      "\n",
      "\n",
      "st.image('Assets/img/header_image.jpg')\n",
      "\n",
      "st.title(':blue[Resume Matcher]')\n",
      "st.subheader(\n",
      "    'Free and Open Source ATS to help your resume pass the screening stage.')\n",
      "st.markdown(\n",
      "    \"Check the website [www.resumematcher.fyi](https://www.resumematcher.fyi/)\")\n",
      "st.markdown(\n",
      "    ' Give Resume Matcher a Star on [GitHub](https://github.com/srbhr/resume-matcher)')\n",
      "badge(type=\"github\", name=\"srbhr/Resume-Matcher\")\n",
      "\n",
      "st.text('For updates follow me on Twitter.')\n",
      "badge(type=\"twitter\", name=\"_srbhr_\")\n",
      "\n",
      "st.markdown(\n",
      "    'If you like the project and would like to further help in development please consider ')\n",
      "badge(type=\"buymeacoffee\", name=\"srbhr\")\n",
      "\n",
      "avs.add_vertical_space(5)\n",
      "\n",
      "resume_names = get_filenames_from_dir(\"Data/Processed/Resumes\")\n",
      "\n",
      "st.write(\"There are\", len(resume_names),\n",
      "         \" resumes present. Please select one from the menu below:\")\n",
      "output = st.slider('Select Resume Number', 0, len(resume_names)-1, 2)\n",
      "\n",
      "avs.add_vertical_space(5)\n",
      "\n",
      "st.write(\"You have selected \", resume_names[output], \" printing the resume\")\n",
      "selected_file = read_json(\"Data/Processed/Resumes/\"+resume_names[output])\n",
      "\n",
      "avs.add_vertical_space(2)\n",
      "st.markdown(\"#### Parsed Resume Data\")\n",
      "st.caption(\n",
      "    \"This text is parsed from your resume. This is how it'll look like after getting parsed by an ATS.\")\n",
      "st.caption(\"Utilize this to understand how to make your resume ATS friendly.\")\n",
      "avs.add_vertical_space(3)\n",
      "# st.json(selected_file)\n",
      "st.write(selected_file[\"clean_data\"])\n",
      "\n",
      "avs.add_vertical_space(3)\n",
      "st.write(\"Now let's take a look at the extracted keywords from the resume.\")\n",
      "\n",
      "annotated_text(create_annotated_text(\n",
      "    selected_file[\"clean_data\"], selected_file[\"extracted_keywords\"],\n",
      "    \"KW\", \"#0B666A\"))\n",
      "\n",
      "avs.add_vertical_space(5)\n",
      "st.write(\"Now let's take a look at the extracted entities from the resume.\")\n",
      "\n",
      "# Call the function with your data\n",
      "create_star_graph(selected_file['keyterms'], \"Entities from Resume\")\n",
      "\n",
      "df2 = pd.DataFrame(selected_file['keyterms'], columns=[\"keyword\", \"value\"])\n",
      "\n",
      "# Create the dictionary\n",
      "keyword_dict = {}\n",
      "for keyword, value in selected_file['keyterms']:\n",
      "    keyword_dict[keyword] = value*100\n",
      "\n",
      "fig = go.Figure(data=[go.Table(header=dict(values=[\"Keyword\", \"Value\"],\n",
      "                                           font=dict(size=12),\n",
      "                                           fill_color='#070A52'),\n",
      "                               cells=dict(values=[list(keyword_dict.keys()),\n",
      "                                                  list(keyword_dict.values())],\n",
      "                                          line_color='darkslategray',\n",
      "                                          fill_color='#6DA9E4'))\n",
      "                      ])\n",
      "st.plotly_chart(fig)\n",
      "\n",
      "st.divider()\n",
      "\n",
      "fig = px.treemap(df2, path=['keyword'], values='value',\n",
      "                 color_continuous_scale='Rainbow',\n",
      "                 title='Key Terms/Topics Extracted from your Resume')\n",
      "st.write(fig)\n",
      "\n",
      "avs.add_vertical_space(5)\n",
      "\n",
      "job_descriptions = get_filenames_from_dir(\"Data/Processed/JobDescription\")\n",
      "\n",
      "st.write(\"There are\", len(job_descriptions),\n",
      "         \" resumes present. Please select one from the menu below:\")\n",
      "output = st.slider('Select Job Description Number',\n",
      "                   0, len(job_descriptions)-1, 2)\n",
      "\n",
      "avs.add_vertical_space(5)\n",
      "\n",
      "st.write(\"You have selected \",\n",
      "         job_descriptions[output], \" printing the job description\")\n",
      "selected_jd = read_json(\n",
      "    \"Data/Processed/JobDescription/\"+job_descriptions[output])\n",
      "\n",
      "avs.add_vertical_space(2)\n",
      "st.markdown(\"#### Job Description\")\n",
      "st.caption(\n",
      "    \"Currently in the pipeline I'm parsing this from PDF but it'll be from txt or copy paste.\")\n",
      "avs.add_vertical_space(3)\n",
      "# st.json(selected_file)\n",
      "st.write(selected_jd[\"clean_data\"])\n",
      "\n",
      "st.markdown(\"#### Common Words between Job Description and Resumes Highlighted.\")\n",
      "\n",
      "annotated_text(create_annotated_text(\n",
      "    selected_file[\"clean_data\"], selected_jd[\"extracted_keywords\"],\n",
      "    \"JD\", \"#F24C3D\"))\n",
      "\n",
      "st.write(\"Now let's take a look at the extracted entities from the job description.\")\n",
      "\n",
      "# Call the function with your data\n",
      "create_star_graph(selected_jd['keyterms'], \"Entities from Job Description\")\n",
      "\n",
      "df2 = pd.DataFrame(selected_jd['keyterms'], columns=[\"keyword\", \"value\"])\n",
      "\n",
      "# Create the dictionary\n",
      "keyword_dict = {}\n",
      "for keyword, value in selected_jd['keyterms']:\n",
      "    keyword_dict[keyword] = value*100\n",
      "\n",
      "fig = go.Figure(data=[go.Table(header=dict(values=[\"Keyword\", \"Value\"],\n",
      "                                           font=dict(size=12),\n",
      "                                           fill_color='#070A52'),\n",
      "                               cells=dict(values=[list(keyword_dict.keys()),\n",
      "                                                  list(keyword_dict.values())],\n",
      "                                          line_color='darkslategray',\n",
      "                                          fill_color='#6DA9E4'))\n",
      "                      ])\n",
      "st.plotly_chart(fig)\n",
      "\n",
      "st.divider()\n",
      "\n",
      "fig = px.treemap(df2, path=['keyword'], values='value',\n",
      "                 color_continuous_scale='Rainbow',\n",
      "                 title='Key Terms/Topics Extracted from the selected Job Description')\n",
      "st.write(fig)\n",
      "\n",
      "avs.add_vertical_space(3)\n",
      "\n",
      "st.title(':blue[Resume Matcher]')\n",
      "st.subheader(\n",
      "    'Free and Open Source ATS to help your resume pass the screening stage.')\n",
      "st.markdown(\n",
      "    ' Give Resume Matcher a Star on [GitHub](https://github.com/srbhr/Resume-Matcher/)')\n",
      "badge(type=\"github\", name=\"srbhr/Resume-Matcher\")\n",
      "\n",
      "st.text('For updates follow me on Twitter.')\n",
      "badge(type=\"twitter\", name=\"_srbhr_\")\n",
      "\n",
      "st.markdown(\n",
      "    'If you like the project and would like to further help in development please consider ')\n",
      "badge(type=\"buymeacoffee\", name=\"srbhr\")\n",
      "\n",
      "import networkx as nx\n",
      "from typing import List\n",
      "import streamlit as st\n",
      "import pandas as pd\n",
      "import json\n",
      "import plotly.express as px\n",
      "import plotly.graph_objects as go\n",
      "from scripts.utils.ReadFiles import get_filenames_from_dir\n",
      "from streamlit_extras import add_vertical_space as avs\n",
      "from annotated_text import annotated_text, parameters\n",
      "from streamlit_extras.badges import badge\n",
      "import nltk\n",
      "nltk.download('punkt')\n",
      "\n",
      "parameters.SHOW_LABEL_SEPARATOR = False\n",
      "parameters.BORDER_RADIUS = 3\n",
      "parameters.PADDING = \"0.5 0.25rem\"\n",
      "\n",
      "\n",
      "def create_star_graph(nodes_and_weights, title):\n",
      "    # Create an empty graph\n",
      "    G = nx.Graph()\n",
      "\n",
      "    # Add the central node\n",
      "    central_node = \"resume\"\n",
      "    G.add_node(central_node)\n",
      "\n",
      "    # Add nodes and edges with weights to the graph\n",
      "    for node, weight in nodes_and_weights:\n",
      "        G.add_node(node)\n",
      "        G.add_edge(central_node, node, weight=weight*100)\n",
      "\n",
      "    # Get position layout for nodes\n",
      "    pos = nx.spring_layout(G)\n",
      "\n",
      "    # Create edge trace\n",
      "    edge_x = []\n",
      "    edge_y = []\n",
      "    for edge in G.edges():\n",
      "        x0, y0 = pos[edge[0]]\n",
      "        x1, y1 = pos[edge[1]]\n",
      "        edge_x.extend([x0, x1, None])\n",
      "        edge_y.extend([y0, y1, None])\n",
      "\n",
      "    edge_trace = go.Scatter(x=edge_x, y=edge_y, line=dict(\n",
      "        width=0.5, color='#888'), hoverinfo='none', mode='lines')\n",
      "\n",
      "    # Create node trace\n",
      "    node_x = []\n",
      "    node_y = []\n",
      "    for node in G.nodes():\n",
      "        x, y = pos[node]\n",
      "        node_x.append(x)\n",
      "        node_y.append(y)\n",
      "\n",
      "    node_trace = go.Scatter(x=node_x, y=node_y, mode='markers', hoverinfo='text',\n",
      "                            marker=dict(showscale=True, colorscale='Rainbow', reversescale=True, color=[], size=10,\n",
      "                                        colorbar=dict(thickness=15, title='Node Connections', xanchor='left',\n",
      "                                                      titleside='right'), line_width=2))\n",
      "\n",
      "    # Color node points by number of connections\n",
      "    node_adjacencies = []\n",
      "    node_text = []\n",
      "    for node in G.nodes():\n",
      "        adjacencies = list(G.adj[node])  # changes here\n",
      "        node_adjacencies.append(len(adjacencies))\n",
      "        node_text.append(f'{node}<br># of connections: {len(adjacencies)}')\n",
      "\n",
      "    node_trace.marker.color = node_adjacencies\n",
      "    node_trace.text = node_text\n",
      "\n",
      "    # Create the figure\n",
      "    fig = go.Figure(data=[edge_trace, node_trace],\n",
      "                    layout=go.Layout(title=title, titlefont_size=16, showlegend=False,\n",
      "                                     hovermode='closest', margin=dict(b=20, l=5, r=5, t=40),\n",
      "                                     xaxis=dict(\n",
      "                                         showgrid=False, zeroline=False, showticklabels=False),\n",
      "                                     yaxis=dict(showgrid=False, zeroline=False, showticklabels=False)))\n",
      "\n",
      "    # Show the figure\n",
      "    st.plotly_chart(fig)\n",
      "\n",
      "\n",
      "def create_annotated_text(input_string: str, word_list: List[str], annotation: str, color_code: str):\n",
      "    # Tokenize the input string\n",
      "    tokens = nltk.word_tokenize(input_string)\n",
      "\n",
      "    # Convert the list to a set for quick lookups\n",
      "    word_set = set(word_list)\n",
      "\n",
      "    # Initialize an empty list to hold the annotated text\n",
      "    annotated_text = []\n",
      "\n",
      "    for token in tokens:\n",
      "        # Check if the token is in the set\n",
      "        if token in word_set:\n",
      "            # If it is, append a tuple with the token, annotation, and color code\n",
      "            annotated_text.append((token, annotation, color_code))\n",
      "        else:\n",
      "            # If it's not, just append the token as a string\n",
      "            annotated_text.append(token)\n",
      "\n",
      "    return annotated_text\n",
      "\n",
      "\n",
      "def read_json(filename):\n",
      "    with open(filename) as f:\n",
      "        data = json.load(f)\n",
      "    return data\n",
      "\n",
      "\n",
      "def tokenize_string(input_string):\n",
      "    tokens = nltk.word_tokenize(input_string)\n",
      "    return tokens\n",
      "\n",
      "\n",
      "st.image('Assets/img/header_image.jpg')\n",
      "\n",
      "st.title(':blue[Resume Matcher]')\n",
      "st.subheader(\n",
      "    'Free and Open Source ATS to help your resume pass the screening stage.')\n",
      "st.markdown(\n",
      "    \"Check the website [www.resumematcher.fyi](https://www.resumematcher.fyi/)\")\n",
      "st.markdown(\n",
      "    ' Give Resume Matcher a Star on [GitHub](https://github.com/srbhr/Resume-Matcher/)')\n",
      "badge(type=\"github\", name=\"srbhr/Resume-Matcher\")\n",
      "\n",
      "st.text('For updates follow me on Twitter.')\n",
      "badge(type=\"twitter\", name=\"_srbhr_\")\n",
      "\n",
      "st.markdown(\n",
      "    'If you like the project and would like to further help in development please consider ')\n",
      "badge(type=\"buymeacoffee\", name=\"srbhr\")\n",
      "\n",
      "avs.add_vertical_space(5)\n",
      "\n",
      "resume_names = get_filenames_from_dir(\"Data/Processed/Resumes\")\n",
      "\n",
      "st.write(\"There are\", len(resume_names),\n",
      "         \" resumes present. Please select one from the menu below:\")\n",
      "output = st.slider('Select Resume Number', 0, len(resume_names)-1, 2)\n",
      "\n",
      "avs.add_vertical_space(5)\n",
      "\n",
      "st.write(\"You have selected \", resume_names[output], \" printing the resume\")\n",
      "selected_file = read_json(\"Data/Processed/Resumes/\"+resume_names[output])\n",
      "\n",
      "avs.add_vertical_space(2)\n",
      "st.markdown(\"#### Parsed Resume Data\")\n",
      "st.caption(\n",
      "    \"This text is parsed from your resume. This is how it'll look like after getting parsed by an ATS.\")\n",
      "st.caption(\"Utilize this to understand how to make your resume ATS friendly.\")\n",
      "avs.add_vertical_space(3)\n",
      "# st.json(selected_file)\n",
      "st.write(selected_file[\"clean_data\"])\n",
      "\n",
      "avs.add_vertical_space(3)\n",
      "st.write(\"Now let's take a look at the extracted keywords from the resume.\")\n",
      "\n",
      "annotated_text(create_annotated_text(\n",
      "    selected_file[\"clean_data\"], selected_file[\"extracted_keywords\"],\n",
      "    \"KW\", \"#0B666A\"))\n",
      "\n",
      "avs.add_vertical_space(5)\n",
      "st.write(\"Now let's take a look at the extracted entities from the resume.\")\n",
      "\n",
      "# Call the function with your data\n",
      "create_star_graph(selected_file['keyterms'], \"Entities from Resume\")\n",
      "\n",
      "df2 = pd.DataFrame(selected_file['keyterms'], columns=[\"keyword\", \"value\"])\n",
      "\n",
      "# Create the dictionary\n",
      "keyword_dict = {}\n",
      "for keyword, value in selected_file['keyterms']:\n",
      "    keyword_dict[keyword] = value*100\n",
      "\n",
      "fig = go.Figure(data=[go.Table(header=dict(values=[\"Keyword\", \"Value\"],\n",
      "                                           font=dict(size=12),\n",
      "                                           fill_color='#070A52'),\n",
      "                               cells=dict(values=[list(keyword_dict.keys()),\n",
      "                                                  list(keyword_dict.values())],\n",
      "                                          line_color='darkslategray',\n",
      "                                          fill_color='#6DA9E4'))\n",
      "                      ])\n",
      "st.plotly_chart(fig)\n",
      "\n",
      "st.divider()\n",
      "\n",
      "fig = px.treemap(df2, path=['keyword'], values='value',\n",
      "                 color_continuous_scale='Rainbow',\n",
      "                 title='Key Terms/Topics Extracted from your Resume')\n",
      "st.write(fig)\n",
      "\n",
      "avs.add_vertical_space(5)\n",
      "\n",
      "job_descriptions = get_filenames_from_dir(\"Data/Processed/JobDescription\")\n",
      "\n",
      "st.write(\"There are\", len(job_descriptions),\n",
      "         \" resumes present. Please select one from the menu below:\")\n",
      "output = st.slider('Select Job Description Number',\n",
      "                   0, len(job_descriptions)-1, 2)\n",
      "\n",
      "avs.add_vertical_space(5)\n",
      "\n",
      "st.write(\"You have selected \",\n",
      "         job_descriptions[output], \" printing the job description\")\n",
      "selected_jd = read_json(\n",
      "    \"Data/Processed/JobDescription/\"+job_descriptions[output])\n",
      "\n",
      "avs.add_vertical_space(2)\n",
      "st.markdown(\"#### Job Description\")\n",
      "st.caption(\n",
      "    \"Currently in the pipeline I'm parsing this from PDF but it'll be from txt or copy paste.\")\n",
      "avs.add_vertical_space(3)\n",
      "# st.json(selected_file)\n",
      "st.write(selected_jd[\"clean_data\"])\n",
      "\n",
      "st.markdown(\"#### Common Words between Job Description and Resumes Highlighted.\")\n",
      "\n",
      "annotated_text(create_annotated_text(\n",
      "    selected_file[\"clean_data\"], selected_jd[\"extracted_keywords\"],\n",
      "    \"JD\", \"#F24C3D\"))\n",
      "\n",
      "st.write(\"Now let's take a look at the extracted entities from the job description.\")\n",
      "\n",
      "# Call the function with your data\n",
      "create_star_graph(selected_jd['keyterms'], \"Entities from Job Description\")\n",
      "\n",
      "df2 = pd.DataFrame(selected_jd['keyterms'], columns=[\"keyword\", \"value\"])\n",
      "\n",
      "# Create the dictionary\n",
      "keyword_dict = {}\n",
      "for keyword, value in selected_jd['keyterms']:\n",
      "    keyword_dict[keyword] = value*100\n",
      "\n",
      "fig = go.Figure(data=[go.Table(header=dict(values=[\"Keyword\", \"Value\"],\n",
      "                                           font=dict(size=12),\n",
      "                                           fill_color='#070A52'),\n",
      "                               cells=dict(values=[list(keyword_dict.keys()),\n",
      "                                                  list(keyword_dict.values())],\n",
      "                                          line_color='darkslategray',\n",
      "                                          fill_color='#6DA9E4'))\n",
      "                      ])\n",
      "st.plotly_chart(fig)\n",
      "\n",
      "st.divider()\n",
      "\n",
      "fig = px.treemap(df2, path=['keyword'], values='value',\n",
      "                 color_continuous_scale='Rainbow',\n",
      "                 title='Key Terms/Topics Extracted from the selected Job Description')\n",
      "st.write(fig)\n",
      "\n",
      "avs.add_vertical_space(5)\n",
      "\n",
      "st.divider()\n",
      "\n",
      "st.markdown(\"## Vector Similarity Scores\")\n",
      "st.caption(\"Powered by Qdrant Vector Search\")\n",
      "st.info(\"These are pre-computed queries\", icon=\"\")\n",
      "st.warning(\n",
      "    \"Running Qdrant or Sentence Transformers without having capacity is not recommended\", icon=\"\")\n",
      "\n",
      "\n",
      "# Your data\n",
      "data = [\n",
      "    {'text': \"{'resume': 'Alfred Pennyworth\",\n",
      "        'query': 'Job Description Product Manager', 'score': 0.62658},\n",
      "    {'text': \"{'resume': 'Barry Allen\",\n",
      "        'query': 'Job Description Product Manager', 'score': 0.43777737},\n",
      "    {'text': \"{'resume': 'Bruce Wayne \",\n",
      "        'query': 'Job Description Product Manager', 'score': 0.39835533},\n",
      "    {'text': \"{'resume': 'JOHN DOE\",\n",
      "        'query': 'Job Description Product Manager', 'score': 0.3915512},\n",
      "    {'text': \"{'resume': 'Harvey Dent\",\n",
      "        'query': 'Job Description Product Manager', 'score': 0.3519544},\n",
      "    {'text': \"{'resume': 'Barry Allen\",\n",
      "        'query': 'Job Description Senior Full Stack Engineer', 'score': 0.6541866},\n",
      "    {'text': \"{'resume': 'Alfred Pennyworth\",\n",
      "        'query': 'Job Description Senior Full Stack Engineer', 'score': 0.59806436},\n",
      "    {'text': \"{'resume': 'JOHN DOE\",\n",
      "        'query': 'Job Description Senior Full Stack Engineer', 'score': 0.5951386},\n",
      "    {'text': \"{'resume': 'Bruce Wayne \",\n",
      "        'query': 'Job Description Senior Full Stack Engineer', 'score': 0.57700855},\n",
      "    {'text': \"{'resume': 'Harvey Dent\",\n",
      "        'query': 'Job Description Senior Full Stack Engineer', 'score': 0.38489106},\n",
      "    {'text': \"{'resume': 'Barry Allen\",\n",
      "        'query': 'Job Description Front End Engineer', 'score': 0.76813436},\n",
      "    {'text': \"{'resume': 'Bruce Wayne'\",\n",
      "        'query': 'Job Description Front End Engineer', 'score': 0.60440844},\n",
      "    {'text': \"{'resume': 'JOHN DOE\",\n",
      "        'query': 'Job Description Front End Engineer', 'score': 0.56080043},\n",
      "    {'text': \"{'resume': 'Alfred Pennyworth\",\n",
      "        'query': 'Job Description Front End Engineer', 'score': 0.5395049},\n",
      "    {'text': \"{'resume': 'Harvey Dent\",\n",
      "        'query': 'Job Description Front End Engineer', 'score': 0.3859515},\n",
      "    {'text': \"{'resume': 'JOHN DOE\",\n",
      "        'query': 'Job Description Java Developer', 'score': 0.5449441},\n",
      "    {'text': \"{'resume': 'Alfred Pennyworth\",\n",
      "        'query': 'Job Description Java Developer', 'score': 0.53476423},\n",
      "    {'text': \"{'resume': 'Barry Allen\",\n",
      "        'query': 'Job Description Java Developer', 'score': 0.5313871},\n",
      "    {'text': \"{'resume': 'Bruce Wayne \",\n",
      "        'query': 'Job Description Java Developer', 'score': 0.44446343},\n",
      "    {'text': \"{'resume': 'Harvey Dent\",\n",
      "        'query': 'Job Description Java Developer', 'score': 0.3616274}\n",
      "]\n",
      "\n",
      "# Create a DataFrame\n",
      "df = pd.DataFrame(data)\n",
      "\n",
      "# Create different DataFrames based on the query and sort by score\n",
      "df1 = df[df['query'] ==\n",
      "         'Job Description Product Manager'].sort_values(by='score', ascending=False)\n",
      "df2 = df[df['query'] ==\n",
      "         'Job Description Senior Full Stack Engineer'].sort_values(by='score', ascending=False)\n",
      "df3 = df[df['query'] == 'Job Description Front End Engineer'].sort_values(\n",
      "    by='score', ascending=False)\n",
      "df4 = df[df['query'] == 'Job Description Java Developer'].sort_values(\n",
      "    by='score', ascending=False)\n",
      "\n",
      "\n",
      "def plot_df(df, title):\n",
      "    fig = px.bar(df, x='text', y=df['score']*100, title=title)\n",
      "    st.plotly_chart(fig)\n",
      "\n",
      "\n",
      "st.markdown(\"### Bar plots of scores based on similarity to Job Description.\")\n",
      "\n",
      "st.subheader(\":blue[Legend]\")\n",
      "st.text(\"Alfred Pennyworth :  Product Manager\")\n",
      "st.text(\"Barry Allen :  Front End Developer\")\n",
      "st.text(\"Harvey Dent :  Machine Learning Engineer\")\n",
      "st.text(\"Bruce Wayne :  Fullstack Developer (MERN)\")\n",
      "st.text(\"John Doe :  Fullstack Developer (Java)\")\n",
      "\n",
      "\n",
      "plot_df(df1, 'Job Description Product Manager 10+ Years of Exper')\n",
      "plot_df(df2, 'Job Description Senior Full Stack Engineer 5+ Year')\n",
      "plot_df(df3, 'Job Description Front End Engineer 2 Years of Expe')\n",
      "plot_df(df4, 'Job Description Java Developer 3 Years of Experien')\n",
      "\n",
      "\n",
      "avs.add_vertical_space(3)\n",
      "\n",
      "st.markdown(\n",
      "    ' Give Resume Matcher a Star on [GitHub](https://github.com/srbhr/Resume-Matcher/)')\n",
      "badge(type=\"github\", name=\"srbhr/Resume-Matcher\")\n",
      "\n",
      "st.text('For updates follow me on Twitter.')\n",
      "badge(type=\"twitter\", name=\"_srbhr_\")\n",
      "\n",
      "st.markdown(\n",
      "    'If you like the project and would like to further help in development please consider ')\n",
      "badge(type=\"buymeacoffee\", name=\"srbhr\")\n",
      "\n",
      "import os\n",
      "\n",
      "def get_filenames_from_dir(directory_path: str) -> list:\n",
      "    filenames = [f for f in os.listdir(directory_path) if os.path.isfile(os.path.join(directory_path, f)) and f != '.DS_Store']\n",
      "    return filenames\n",
      "\n",
      "import networkx as nx\n",
      "from typing import List\n",
      "import streamlit as st\n",
      "import pandas as pd\n",
      "import json\n",
      "import plotly.express as px\n",
      "import plotly.graph_objects as go\n",
      "from scripts.utils.ReadFiles import get_filenames_from_dir\n",
      "from streamlit_extras import add_vertical_space as avs\n",
      "from annotated_text import annotated_text, parameters\n",
      "from streamlit_extras.badges import badge\n",
      "import nltk\n",
      "\n",
      "try:\n",
      "    nltk.data.find('tokenizers/punkt')\n",
      "except LookupError:\n",
      "    nltk.download('punkt')\n",
      "\n",
      "parameters.SHOW_LABEL_SEPARATOR = False\n",
      "parameters.BORDER_RADIUS = 3\n",
      "parameters.PADDING = \"0.5 0.25rem\"\n",
      "\n",
      "\n",
      "def create_star_graph(nodes_and_weights, title):\n",
      "    # Create an empty graph\n",
      "    G = nx.Graph()\n",
      "\n",
      "    # Add the central node\n",
      "    central_node = \"resume\"\n",
      "    G.add_node(central_node)\n",
      "\n",
      "    # Add nodes and edges with weights to the graph\n",
      "    for node, weight in nodes_and_weights:\n",
      "        G.add_node(node)\n",
      "        G.add_edge(central_node, node, weight=weight*100)\n",
      "\n",
      "    # Get position layout for nodes\n",
      "    pos = nx.spring_layout(G)\n",
      "\n",
      "    # Create edge trace\n",
      "    edge_x = []\n",
      "    edge_y = []\n",
      "    for edge in G.edges():\n",
      "        x0, y0 = pos[edge[0]]\n",
      "        x1, y1 = pos[edge[1]]\n",
      "        edge_x.extend([x0, x1, None])\n",
      "        edge_y.extend([y0, y1, None])\n",
      "\n",
      "    edge_trace = go.Scatter(x=edge_x, y=edge_y, line=dict(\n",
      "        width=0.5, color='#888'), hoverinfo='none', mode='lines')\n",
      "\n",
      "    # Create node trace\n",
      "    node_x = []\n",
      "    node_y = []\n",
      "    for node in G.nodes():\n",
      "        x, y = pos[node]\n",
      "        node_x.append(x)\n",
      "        node_y.append(y)\n",
      "\n",
      "    node_trace = go.Scatter(x=node_x, y=node_y, mode='markers', hoverinfo='text',\n",
      "                            marker=dict(showscale=True, colorscale='Rainbow', reversescale=True, color=[], size=10,\n",
      "                                        colorbar=dict(thickness=15, title='Node Connections', xanchor='left',\n",
      "                                                      titleside='right'), line_width=2))\n",
      "\n",
      "    # Color node points by number of connections\n",
      "    node_adjacencies = []\n",
      "    node_text = []\n",
      "    for node in G.nodes():\n",
      "        adjacencies = list(G.adj[node])  # changes here\n",
      "        node_adjacencies.append(len(adjacencies))\n",
      "        node_text.append(f'{node}<br># of connections: {len(adjacencies)}')\n",
      "\n",
      "    node_trace.marker.color = node_adjacencies\n",
      "    node_trace.text = node_text\n",
      "\n",
      "    # Create the figure\n",
      "    fig = go.Figure(data=[edge_trace, node_trace],\n",
      "                    layout=go.Layout(title=title, titlefont_size=16, showlegend=False,\n",
      "                                     hovermode='closest', margin=dict(b=20, l=5, r=5, t=40),\n",
      "                                     xaxis=dict(\n",
      "                                         showgrid=False, zeroline=False, showticklabels=False),\n",
      "                                     yaxis=dict(showgrid=False, zeroline=False, showticklabels=False)))\n",
      "\n",
      "    # Show the figure\n",
      "    st.plotly_chart(fig)\n",
      "\n",
      "\n",
      "def create_annotated_text(input_string: str, word_list: List[str], annotation: str, color_code: str):\n",
      "    # Tokenize the input string\n",
      "    tokens = nltk.word_tokenize(input_string)\n",
      "\n",
      "    # Convert the list to a set for quick lookups\n",
      "    word_set = set(word_list)\n",
      "\n",
      "    # Initialize an empty list to hold the annotated text\n",
      "    annotated_text = []\n",
      "\n",
      "    for token in tokens:\n",
      "        # Check if the token is in the set\n",
      "        if token in word_set:\n",
      "            # If it is, append a tuple with the token, annotation, and color code\n",
      "            annotated_text.append((token, annotation, color_code))\n",
      "        else:\n",
      "            # If it's not, just append the token as a string\n",
      "            annotated_text.append(token)\n",
      "\n",
      "    return annotated_text\n",
      "\n",
      "\n",
      "def read_json(filename):\n",
      "    with open(filename) as f:\n",
      "        data = json.load(f)\n",
      "    return data\n",
      "\n",
      "\n",
      "def tokenize_string(input_string):\n",
      "    tokens = nltk.word_tokenize(input_string)\n",
      "    return tokens\n",
      "\n",
      "\n",
      "st.image('Assets/img/header_image.jpg')\n",
      "\n",
      "st.title(':blue[Resume Matcher]')\n",
      "st.subheader(\n",
      "    'Free and Open Source ATS to help your resume pass the screening stage.')\n",
      "st.markdown(\n",
      "    \"Check the website [www.resumematcher.fyi](https://www.resumematcher.fyi/)\")\n",
      "st.markdown(\n",
      "    ' Give Resume Matcher a Star on [GitHub](https://github.com/srbhr/resume-matcher)')\n",
      "badge(type=\"github\", name=\"srbhr/Resume-Matcher\")\n",
      "\n",
      "st.text('For updates follow me on Twitter.')\n",
      "badge(type=\"twitter\", name=\"_srbhr_\")\n",
      "\n",
      "st.markdown(\n",
      "    'If you like the project and would like to further help in development please consider ')\n",
      "badge(type=\"buymeacoffee\", name=\"srbhr\")\n",
      "\n",
      "avs.add_vertical_space(5)\n",
      "\n",
      "resume_names = get_filenames_from_dir(\"Data/Processed/Resumes\")\n",
      "\n",
      "st.write(\"There are\", len(resume_names),\n",
      "         \" resumes present. Please select one from the menu below:\")\n",
      "output = st.slider('Select Resume Number', 0, len(resume_names)-1, 2)\n",
      "\n",
      "avs.add_vertical_space(5)\n",
      "\n",
      "st.write(\"You have selected \", resume_names[output], \" printing the resume\")\n",
      "selected_file = read_json(\"Data/Processed/Resumes/\"+resume_names[output])\n",
      "\n",
      "avs.add_vertical_space(2)\n",
      "st.markdown(\"#### Parsed Resume Data\")\n",
      "st.caption(\n",
      "    \"This text is parsed from your resume. This is how it'll look like after getting parsed by an ATS.\")\n",
      "st.caption(\"Utilize this to understand how to make your resume ATS friendly.\")\n",
      "avs.add_vertical_space(3)\n",
      "# st.json(selected_file)\n",
      "st.write(selected_file[\"clean_data\"])\n",
      "\n",
      "avs.add_vertical_space(3)\n",
      "st.write(\"Now let's take a look at the extracted keywords from the resume.\")\n",
      "\n",
      "annotated_text(create_annotated_text(\n",
      "    selected_file[\"clean_data\"], selected_file[\"extracted_keywords\"],\n",
      "    \"KW\", \"#0B666A\"))\n",
      "\n",
      "avs.add_vertical_space(5)\n",
      "st.write(\"Now let's take a look at the extracted entities from the resume.\")\n",
      "\n",
      "# Call the function with your data\n",
      "create_star_graph(selected_file['keyterms'], \"Entities from Resume\")\n",
      "\n",
      "df2 = pd.DataFrame(selected_file['keyterms'], columns=[\"keyword\", \"value\"])\n",
      "\n",
      "# Create the dictionary\n",
      "keyword_dict = {}\n",
      "for keyword, value in selected_file['keyterms']:\n",
      "    keyword_dict[keyword] = value*100\n",
      "\n",
      "fig = go.Figure(data=[go.Table(header=dict(values=[\"Keyword\", \"Value\"],\n",
      "                                           font=dict(size=12),\n",
      "                                           fill_color='#070A52'),\n",
      "                               cells=dict(values=[list(keyword_dict.keys()),\n",
      "                                                  list(keyword_dict.values())],\n",
      "                                          line_color='darkslategray',\n",
      "                                          fill_color='#6DA9E4'))\n",
      "                      ])\n",
      "st.plotly_chart(fig)\n",
      "\n",
      "st.divider()\n",
      "\n",
      "fig = px.treemap(df2, path=['keyword'], values='value',\n",
      "                 color_continuous_scale='Rainbow',\n",
      "                 title='Key Terms/Topics Extracted from your Resume')\n",
      "st.write(fig)\n",
      "\n",
      "avs.add_vertical_space(5)\n",
      "\n",
      "job_descriptions = get_filenames_from_dir(\"Data/Processed/JobDescription\")\n",
      "\n",
      "st.write(\"There are\", len(job_descriptions),\n",
      "         \" resumes present. Please select one from the menu below:\")\n",
      "output = st.slider('Select Job Description Number',\n",
      "                   0, len(job_descriptions)-1, 2)\n",
      "\n",
      "avs.add_vertical_space(5)\n",
      "\n",
      "st.write(\"You have selected \",\n",
      "         job_descriptions[output], \" printing the job description\")\n",
      "selected_jd = read_json(\n",
      "    \"Data/Processed/JobDescription/\"+job_descriptions[output])\n",
      "\n",
      "avs.add_vertical_space(2)\n",
      "st.markdown(\"#### Job Description\")\n",
      "st.caption(\n",
      "    \"Currently in the pipeline I'm parsing this from PDF but it'll be from txt or copy paste.\")\n",
      "avs.add_vertical_space(3)\n",
      "# st.json(selected_file)\n",
      "st.write(selected_jd[\"clean_data\"])\n",
      "\n",
      "st.markdown(\"#### Common Words between Job Description and Resumes Highlighted.\")\n",
      "\n",
      "annotated_text(create_annotated_text(\n",
      "    selected_file[\"clean_data\"], selected_jd[\"extracted_keywords\"],\n",
      "    \"JD\", \"#F24C3D\"))\n",
      "\n",
      "st.write(\"Now let's take a look at the extracted entities from the job description.\")\n",
      "\n",
      "# Call the function with your data\n",
      "create_star_graph(selected_jd['keyterms'], \"Entities from Job Description\")\n",
      "\n",
      "df2 = pd.DataFrame(selected_jd['keyterms'], columns=[\"keyword\", \"value\"])\n",
      "\n",
      "# Create the dictionary\n",
      "keyword_dict = {}\n",
      "for keyword, value in selected_jd['keyterms']:\n",
      "    keyword_dict[keyword] = value*100\n",
      "\n",
      "fig = go.Figure(data=[go.Table(header=dict(values=[\"Keyword\", \"Value\"],\n",
      "                                           font=dict(size=12),\n",
      "                                           fill_color='#070A52'),\n",
      "                               cells=dict(values=[list(keyword_dict.keys()),\n",
      "                                                  list(keyword_dict.values())],\n",
      "                                          line_color='darkslategray',\n",
      "                                          fill_color='#6DA9E4'))\n",
      "                      ])\n",
      "st.plotly_chart(fig)\n",
      "\n",
      "st.divider()\n",
      "\n",
      "fig = px.treemap(df2, path=['keyword'], values='value',\n",
      "                 color_continuous_scale='Rainbow',\n",
      "                 title='Key Terms/Topics Extracted from the selected Job Description')\n",
      "st.write(fig)\n",
      "\n",
      "avs.add_vertical_space(3)\n",
      "\n",
      "st.title(':blue[Resume Matcher]')\n",
      "st.subheader(\n",
      "    'Free and Open Source ATS to help your resume pass the screening stage.')\n",
      "st.markdown(\n",
      "    ' Give Resume Matcher a Star on [GitHub](https://github.com/srbhr/Resume-Matcher/)')\n",
      "badge(type=\"github\", name=\"srbhr/Resume-Matcher\")\n",
      "\n",
      "st.text('For updates follow me on Twitter.')\n",
      "badge(type=\"twitter\", name=\"_srbhr_\")\n",
      "\n",
      "st.markdown(\n",
      "    'If you like the project and would like to further help in development please consider ')\n",
      "badge(type=\"buymeacoffee\", name=\"srbhr\")\n",
      "\n",
      "import json\n",
      "from scripts.utils.ReadFiles import get_filenames_from_dir\n",
      "from scripts.ResumeProcessor import ResumeProcessor\n",
      "from scripts.JobDescriptionProcessor import JobDescriptionProcessor\n",
      "import logging\n",
      "import os\n",
      "logging.basicConfig(filename='app.log', filemode='w',\n",
      "                    level=logging.DEBUG,\n",
      "                    format='%(name)s - %(levelname)s - %(message)s')\n",
      "\n",
      "PROCESSED_RESUMES_PATH = \"Data/Processed/Resumes\"\n",
      "PROCESSED_JOB_DESCRIPTIONS_PATH = \"Data/Processed/JobDescription\"\n",
      "\n",
      "def read_json(filename):\n",
      "    with open(filename) as f:\n",
      "        data = json.load(f)\n",
      "    return data\n",
      "\n",
      "def remove_old_files(files_path):\n",
      "\n",
      "    for filename in os.listdir(files_path):\n",
      "        try:\n",
      "            file_path = os.path.join(files_path, filename)\n",
      "\n",
      "            if os.path.isfile(file_path):\n",
      "                os.remove(file_path)\n",
      "        except Exception as e:  \n",
      "            logging.error(f\"Error deleting {file_path}:\\n{e}\")\n",
      "\n",
      "    logging.info(\"Deleted old files from \"+files_path)\n",
      "\n",
      "\n",
      "logging.info('Started to read from Data/Resumes')\n",
      "try:\n",
      "    # Check if there are resumes present or not.\n",
      "    # If present then parse it.\n",
      "    remove_old_files(PROCESSED_RESUMES_PATH)\n",
      "\n",
      "    file_names = get_filenames_from_dir(\"Data/Resumes\")\n",
      "    logging.info('Reading from Data/Resumes is now complete.')\n",
      "except:\n",
      "    # Exit the program if there are no resumes.\n",
      "    logging.error('There are no resumes present in the specified folder.')\n",
      "    logging.error('Exiting from the program.')\n",
      "    logging.error(\n",
      "        'Please add resumes in the Data/Resumes folder and try again.')\n",
      "    exit(1)\n",
      "\n",
      "# Now after getting the file_names parse the resumes into a JSON Format.\n",
      "logging.info('Started parsing the resumes.')\n",
      "for file in file_names:\n",
      "    processor = ResumeProcessor(file)\n",
      "    success = processor.process()\n",
      "logging.info('Parsing of the resumes is now complete.')\n",
      "\n",
      "logging.info('Started to read from Data/JobDescription')\n",
      "try:\n",
      "    # Check if there are resumes present or not.\n",
      "    # If present then parse it.\n",
      "    remove_old_files(PROCESSED_JOB_DESCRIPTIONS_PATH)\n",
      "\n",
      "    file_names = get_filenames_from_dir(\"Data/JobDescription\")\n",
      "    logging.info('Reading from Data/JobDescription is now complete.')\n",
      "except:\n",
      "    # Exit the program if there are no resumes.\n",
      "    logging.error(\n",
      "        'There are no job-description present in the specified folder.')\n",
      "    logging.error('Exiting from the program.')\n",
      "    logging.error(\n",
      "        'Please add resumes in the Data/JobDescription folder and try again.')\n",
      "    exit(1)\n",
      "\n",
      "# Now after getting the file_names parse the resumes into a JSON Format.\n",
      "logging.info('Started parsing the Job Descriptions.')\n",
      "for file in file_names:\n",
      "    processor = JobDescriptionProcessor(file)\n",
      "    success = processor.process()\n",
      "logging.info('Parsing of the Job Descriptions is now complete.')\n",
      "logging.info('Success now run `streamlit run streamlit_second.py`')\n",
      "\n",
      "import networkx as nx\n",
      "from typing import List\n",
      "import streamlit as st\n",
      "import pandas as pd\n",
      "import json\n",
      "import plotly.express as px\n",
      "import plotly.graph_objects as go\n",
      "from scripts.utils.ReadFiles import get_filenames_from_dir\n",
      "from streamlit_extras import add_vertical_space as avs\n",
      "from annotated_text import annotated_text, parameters\n",
      "from streamlit_extras.badges import badge\n",
      "import nltk\n",
      "nltk.download('punkt')\n",
      "\n",
      "parameters.SHOW_LABEL_SEPARATOR = False\n",
      "parameters.BORDER_RADIUS = 3\n",
      "parameters.PADDING = \"0.5 0.25rem\"\n",
      "\n",
      "\n",
      "def create_star_graph(nodes_and_weights, title):\n",
      "    # Create an empty graph\n",
      "    G = nx.Graph()\n",
      "\n",
      "    # Add the central node\n",
      "    central_node = \"resume\"\n",
      "    G.add_node(central_node)\n",
      "\n",
      "    # Add nodes and edges with weights to the graph\n",
      "    for node, weight in nodes_and_weights:\n",
      "        G.add_node(node)\n",
      "        G.add_edge(central_node, node, weight=weight*100)\n",
      "\n",
      "    # Get position layout for nodes\n",
      "    pos = nx.spring_layout(G)\n",
      "\n",
      "    # Create edge trace\n",
      "    edge_x = []\n",
      "    edge_y = []\n",
      "    for edge in G.edges():\n",
      "        x0, y0 = pos[edge[0]]\n",
      "        x1, y1 = pos[edge[1]]\n",
      "        edge_x.extend([x0, x1, None])\n",
      "        edge_y.extend([y0, y1, None])\n",
      "\n",
      "    edge_trace = go.Scatter(x=edge_x, y=edge_y, line=dict(\n",
      "        width=0.5, color='#888'), hoverinfo='none', mode='lines')\n",
      "\n",
      "    # Create node trace\n",
      "    node_x = []\n",
      "    node_y = []\n",
      "    for node in G.nodes():\n",
      "        x, y = pos[node]\n",
      "        node_x.append(x)\n",
      "        node_y.append(y)\n",
      "\n",
      "    node_trace = go.Scatter(x=node_x, y=node_y, mode='markers', hoverinfo='text',\n",
      "                            marker=dict(showscale=True, colorscale='Rainbow', reversescale=True, color=[], size=10,\n",
      "                                        colorbar=dict(thickness=15, title='Node Connections', xanchor='left',\n",
      "                                                      titleside='right'), line_width=2))\n",
      "\n",
      "    # Color node points by number of connections\n",
      "    node_adjacencies = []\n",
      "    node_text = []\n",
      "    for node in G.nodes():\n",
      "        adjacencies = list(G.adj[node])  # changes here\n",
      "        node_adjacencies.append(len(adjacencies))\n",
      "        node_text.append(f'{node}<br># of connections: {len(adjacencies)}')\n",
      "\n",
      "    node_trace.marker.color = node_adjacencies\n",
      "    node_trace.text = node_text\n",
      "\n",
      "    # Create the figure\n",
      "    fig = go.Figure(data=[edge_trace, node_trace],\n",
      "                    layout=go.Layout(title=title, titlefont_size=16, showlegend=False,\n",
      "                                     hovermode='closest', margin=dict(b=20, l=5, r=5, t=40),\n",
      "                                     xaxis=dict(\n",
      "                                         showgrid=False, zeroline=False, showticklabels=False),\n",
      "                                     yaxis=dict(showgrid=False, zeroline=False, showticklabels=False)))\n",
      "\n",
      "    # Show the figure\n",
      "    st.plotly_chart(fig)\n",
      "\n",
      "\n",
      "def create_annotated_text(input_string: str, word_list: List[str], annotation: str, color_code: str):\n",
      "    # Tokenize the input string\n",
      "    tokens = nltk.word_tokenize(input_string)\n",
      "\n",
      "    # Convert the list to a set for quick lookups\n",
      "    word_set = set(word_list)\n",
      "\n",
      "    # Initialize an empty list to hold the annotated text\n",
      "    annotated_text = []\n",
      "\n",
      "    for token in tokens:\n",
      "        # Check if the token is in the set\n",
      "        if token in word_set:\n",
      "            # If it is, append a tuple with the token, annotation, and color code\n",
      "            annotated_text.append((token, annotation, color_code))\n",
      "        else:\n",
      "            # If it's not, just append the token as a string\n",
      "            annotated_text.append(token)\n",
      "\n",
      "    return annotated_text\n",
      "\n",
      "\n",
      "def read_json(filename):\n",
      "    with open(filename) as f:\n",
      "        data = json.load(f)\n",
      "    return data\n",
      "\n",
      "\n",
      "def tokenize_string(input_string):\n",
      "    tokens = nltk.word_tokenize(input_string)\n",
      "    return tokens\n",
      "\n",
      "\n",
      "st.image('Assets/img/header_image.jpg')\n",
      "\n",
      "st.title(':blue[Resume Matcher]')\n",
      "st.subheader(\n",
      "    'Free and Open Source ATS to help your resume pass the screening stage.')\n",
      "st.markdown(\n",
      "    \"Check the website [www.resumematcher.fyi](https://www.resumematcher.fyi/)\")\n",
      "st.markdown(\n",
      "    ' Give Resume Matcher a Star on [GitHub](https://github.com/srbhr/resume-matcher)')\n",
      "badge(type=\"github\", name=\"srbhr/Resume-Matcher\")\n",
      "\n",
      "st.text('For updates follow me on Twitter.')\n",
      "badge(type=\"twitter\", name=\"_srbhr_\")\n",
      "\n",
      "st.markdown(\n",
      "    'If you like the project and would like to further help in development please consider ')\n",
      "badge(type=\"buymeacoffee\", name=\"srbhr\")\n",
      "\n",
      "avs.add_vertical_space(5)\n",
      "\n",
      "resume_names = get_filenames_from_dir(\"Data/Processed/Resumes\")\n",
      "\n",
      "output = 0\n",
      "\n",
      "if len(resume_names) > 1:\n",
      "    st.write(\"There are\", len(resume_names),\n",
      "         \" resumes present. Please select one from the menu below:\")\n",
      "    output = st.slider('Select Resume Number', 0, len(resume_names)-1, 0)\n",
      "else:\n",
      "    st.write(\"There is 1 resume present\")\n",
      "\n",
      "avs.add_vertical_space(5)\n",
      "\n",
      "st.write(\"You have selected \", resume_names[output], \" printing the resume\")\n",
      "selected_file = read_json(\"Data/Processed/Resumes/\"+resume_names[output])\n",
      "\n",
      "avs.add_vertical_space(2)\n",
      "st.markdown(\"#### Parsed Resume Data\")\n",
      "st.caption(\n",
      "    \"This text is parsed from your resume. This is how it'll look like after getting parsed by an ATS.\")\n",
      "st.caption(\"Utilize this to understand how to make your resume ATS friendly.\")\n",
      "avs.add_vertical_space(3)\n",
      "# st.json(selected_file)\n",
      "st.write(selected_file[\"clean_data\"])\n",
      "\n",
      "avs.add_vertical_space(3)\n",
      "st.write(\"Now let's take a look at the extracted keywords from the resume.\")\n",
      "\n",
      "annotated_text(create_annotated_text(\n",
      "    selected_file[\"clean_data\"], selected_file[\"extracted_keywords\"],\n",
      "    \"KW\", \"#0B666A\"))\n",
      "\n",
      "avs.add_vertical_space(5)\n",
      "st.write(\"Now let's take a look at the extracted entities from the resume.\")\n",
      "\n",
      "# Call the function with your data\n",
      "create_star_graph(selected_file['keyterms'], \"Entities from Resume\")\n",
      "\n",
      "df2 = pd.DataFrame(selected_file['keyterms'], columns=[\"keyword\", \"value\"])\n",
      "\n",
      "# Create the dictionary\n",
      "keyword_dict = {}\n",
      "for keyword, value in selected_file['keyterms']:\n",
      "    keyword_dict[keyword] = value*100\n",
      "\n",
      "fig = go.Figure(data=[go.Table(header=dict(values=[\"Keyword\", \"Value\"],\n",
      "                                           font=dict(size=12),\n",
      "                                           fill_color='#070A52'),\n",
      "                               cells=dict(values=[list(keyword_dict.keys()),\n",
      "                                                  list(keyword_dict.values())],\n",
      "                                          line_color='darkslategray',\n",
      "                                          fill_color='#6DA9E4'))\n",
      "                      ])\n",
      "st.plotly_chart(fig)\n",
      "\n",
      "st.divider()\n",
      "\n",
      "fig = px.treemap(df2, path=['keyword'], values='value',\n",
      "                 color_continuous_scale='Rainbow',\n",
      "                 title='Key Terms/Topics Extracted from your Resume')\n",
      "st.write(fig)\n",
      "\n",
      "avs.add_vertical_space(5)\n",
      "\n",
      "job_descriptions = get_filenames_from_dir(\"Data/Processed/JobDescription\")\n",
      "\n",
      "output = 0\n",
      "if len(job_descriptions) > 1:\n",
      "    st.write(\"There are\", len(job_descriptions),\n",
      "         \" resumes present. Please select one from the menu below:\")\n",
      "    output = st.slider('Select Job Description Number',\n",
      "                    0, len(job_descriptions)-1, 0)\n",
      "else:\n",
      "    st.write(\"There is 1 job description present\")\n",
      "\n",
      "avs.add_vertical_space(5)\n",
      "\n",
      "st.write(\"You have selected \",\n",
      "         job_descriptions[output], \" printing the job description\")\n",
      "selected_jd = read_json(\n",
      "    \"Data/Processed/JobDescription/\"+job_descriptions[output])\n",
      "\n",
      "avs.add_vertical_space(2)\n",
      "st.markdown(\"#### Job Description\")\n",
      "st.caption(\n",
      "    \"Currently in the pipeline I'm parsing this from PDF but it'll be from txt or copy paste.\")\n",
      "avs.add_vertical_space(3)\n",
      "# st.json(selected_file)\n",
      "st.write(selected_jd[\"clean_data\"])\n",
      "\n",
      "st.markdown(\"#### Common Words between Job Description and Resumes Highlighted.\")\n",
      "\n",
      "annotated_text(create_annotated_text(\n",
      "    selected_file[\"clean_data\"], selected_jd[\"extracted_keywords\"],\n",
      "    \"JD\", \"#F24C3D\"))\n",
      "\n",
      "st.write(\"Now let's take a look at the extracted entities from the job description.\")\n",
      "\n",
      "# Call the function with your data\n",
      "create_star_graph(selected_jd['keyterms'], \"Entities from Job Description\")\n",
      "\n",
      "df2 = pd.DataFrame(selected_jd['keyterms'], columns=[\"keyword\", \"value\"])\n",
      "\n",
      "# Create the dictionary\n",
      "keyword_dict = {}\n",
      "for keyword, value in selected_jd['keyterms']:\n",
      "    keyword_dict[keyword] = value*100\n",
      "\n",
      "fig = go.Figure(data=[go.Table(header=dict(values=[\"Keyword\", \"Value\"],\n",
      "                                           font=dict(size=12),\n",
      "                                           fill_color='#070A52'),\n",
      "                               cells=dict(values=[list(keyword_dict.keys()),\n",
      "                                                  list(keyword_dict.values())],\n",
      "                                          line_color='darkslategray',\n",
      "                                          fill_color='#6DA9E4'))\n",
      "                      ])\n",
      "st.plotly_chart(fig)\n",
      "\n",
      "st.divider()\n",
      "\n",
      "fig = px.treemap(df2, path=['keyword'], values='value',\n",
      "                 color_continuous_scale='Rainbow',\n",
      "                 title='Key Terms/Topics Extracted from the selected Job Description')\n",
      "st.write(fig)\n",
      "\n",
      "avs.add_vertical_space(3)\n",
      "\n",
      "st.title(':blue[Resume Matcher]')\n",
      "st.subheader(\n",
      "    'Free and Open Source ATS to help your resume pass the screening stage.')\n",
      "st.markdown(\n",
      "    ' Give Resume Matcher a Star on [GitHub](https://github.com/srbhr/Resume-Matcher/)')\n",
      "badge(type=\"github\", name=\"srbhr/Resume-Matcher\")\n",
      "\n",
      "st.text('For updates follow me on Twitter.')\n",
      "badge(type=\"twitter\", name=\"_srbhr_\")\n",
      "\n",
      "st.markdown(\n",
      "    'If you like the project and would like to further help in development please consider ')\n",
      "badge(type=\"buymeacoffee\", name=\"srbhr\")\n",
      "\n",
      "import json\n",
      "import os\n",
      "\n",
      "import cohere\n",
      "import yaml\n",
      "from qdrant_client import QdrantClient, models\n",
      "from qdrant_client.http.models import Batch\n",
      "\n",
      "cwd = os.path.join(\"/home\", \"subramanyam24\", \"projects\", \"Resume-Matcher\")\n",
      "READ_RESUME_FROM = os.path.join(cwd, 'Data', 'Processed', 'Resumes')\n",
      "READ_JOB_DESCRIPTION_FROM = os.path.join(cwd, 'Data', 'Processed', 'JobDescription')\n",
      "config_path = os.path.join(cwd, \"scripts\", \"similarity\")\n",
      "\n",
      "\n",
      "def read_config(filepath):\n",
      "    with open(filepath) as f:\n",
      "        config = yaml.safe_load(f)\n",
      "    return config\n",
      "\n",
      "\n",
      "def read_doc(path):\n",
      "    with open(path) as f:\n",
      "        try:\n",
      "            data = json.load(f)\n",
      "        except Exception as e:\n",
      "            print(f'Error reading JSON file: {e}')\n",
      "            data = {}\n",
      "    return data\n",
      "\n",
      "\n",
      "class QdrantSearch:\n",
      "    def __init__(self, resumes, jd):\n",
      "        config = read_config(config_path + \"/config.yml\")\n",
      "        self.cohere_key = config['cohere']['api_key']\n",
      "        self.qdrant_key = config['qdrant']['api_key']\n",
      "        self.qdrant_url = config['qdrant']['url']\n",
      "        self.resumes = resumes\n",
      "        self.jd = jd\n",
      "\n",
      "        self.cohere = cohere.Client(self.cohere_key)\n",
      "\n",
      "        self.qdrant = QdrantClient(\n",
      "            url=self.qdrant_url,\n",
      "            api_key=self.qdrant_key,\n",
      "        )\n",
      "\n",
      "        vector_size = 4096\n",
      "        self.qdrant.recreate_collection(\n",
      "            collection_name=\"collection_resume_matcher\",\n",
      "            vectors_config=models.VectorParams(\n",
      "                size=vector_size,\n",
      "                distance=models.Distance.COSINE\n",
      "            )\n",
      "        )\n",
      "\n",
      "    def get_embedding(self, text):\n",
      "        embeddings = self.cohere.embed([text], \"large\").embeddings\n",
      "        return list(map(float, embeddings[0])), len(embeddings[0])\n",
      "\n",
      "    def update_qdrant(self):\n",
      "        vectors = []\n",
      "        ids = []\n",
      "        for i, resume in enumerate(self.resumes):\n",
      "            vector, size = self.get_embedding(resume)\n",
      "            vectors.append(vector)\n",
      "            ids.append(i)\n",
      "\n",
      "        self.qdrant.upsert(\n",
      "            collection_name=\"collection_resume_matcher\",\n",
      "            points=Batch(\n",
      "                ids=ids,\n",
      "                vectors=vectors,\n",
      "                payloads=[{\"text\": resume} for resume in self.resumes]\n",
      "\n",
      "            )\n",
      "        )\n",
      "\n",
      "    def search(self):\n",
      "        vector, _ = self.get_embedding(self.jd)\n",
      "\n",
      "        hits = self.qdrant.search(\n",
      "            collection_name=\"collection_resume_matcher\",\n",
      "            query_vector=vector,\n",
      "            limit=30\n",
      "        )\n",
      "        results = []\n",
      "        for hit in hits:\n",
      "            result = {\n",
      "                'text': str(hit.payload)[:30],\n",
      "                'score': hit.score\n",
      "            }\n",
      "            results.append(result)\n",
      "\n",
      "        return results\n",
      "\n",
      "\n",
      "def get_similarity_score(resume_string, jd_string):\n",
      "    qdrant_search = QdrantSearch([resume_string], jd_string)\n",
      "    qdrant_search.update_qdrant()\n",
      "    results = qdrant_search.search()\n",
      "    return results\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "import json\n",
      "import os\n",
      "\n",
      "import cohere\n",
      "import yaml\n",
      "from qdrant_client import QdrantClient, models\n",
      "from qdrant_client.http.models import Batch\n",
      "\n",
      "\n",
      "def find_path(folder_name):\n",
      "    curr_dir = os.getcwd()\n",
      "    while True:\n",
      "        if folder_name in os.listdir(curr_dir):\n",
      "            return os.path.join(curr_dir, folder_name)\n",
      "        else:\n",
      "            parent_dir = os.path.dirname(curr_dir)\n",
      "            if parent_dir == '/':\n",
      "                break\n",
      "            curr_dir = parent_dir\n",
      "    raise ValueError(f\"Folder '{folder_name}' not found.\")\n",
      "\n",
      "\n",
      "cwd = find_path('Resume-Matcher')\n",
      "READ_RESUME_FROM = os.path.join(cwd, 'Data', 'Processed', 'Resumes')\n",
      "READ_JOB_DESCRIPTION_FROM = os.path.join(cwd, 'Data', 'Processed', 'JobDescription')\n",
      "config_path = os.path.join(cwd, \"scripts\", \"similarity\")\n",
      "\n",
      "\n",
      "def read_config(filepath):\n",
      "    with open(filepath) as f:\n",
      "        config = yaml.safe_load(f)\n",
      "    return config\n",
      "\n",
      "\n",
      "def read_doc(path):\n",
      "    with open(path) as f:\n",
      "        try:\n",
      "            data = json.load(f)\n",
      "        except Exception as e:\n",
      "            print(f'Error reading JSON file: {e}')\n",
      "            data = {}\n",
      "    return data\n",
      "\n",
      "\n",
      "class QdrantSearch:\n",
      "    def __init__(self, resumes, jd):\n",
      "        config = read_config(config_path + \"/config.yml\")\n",
      "        self.cohere_key = config['cohere']['api_key']\n",
      "        self.qdrant_key = config['qdrant']['api_key']\n",
      "        self.qdrant_url = config['qdrant']['url']\n",
      "        self.resumes = resumes\n",
      "        self.jd = jd\n",
      "\n",
      "        self.cohere = cohere.Client(self.cohere_key)\n",
      "\n",
      "        self.qdrant = QdrantClient(\n",
      "            url=self.qdrant_url,\n",
      "            api_key=self.qdrant_key,\n",
      "        )\n",
      "\n",
      "        vector_size = 4096\n",
      "        self.qdrant.recreate_collection(\n",
      "            collection_name=\"collection_resume_matcher\",\n",
      "            vectors_config=models.VectorParams(\n",
      "                size=vector_size,\n",
      "                distance=models.Distance.COSINE\n",
      "            )\n",
      "        )\n",
      "\n",
      "    def get_embedding(self, text):\n",
      "        embeddings = self.cohere.embed([text], \"large\").embeddings\n",
      "        return list(map(float, embeddings[0])), len(embeddings[0])\n",
      "\n",
      "    def update_qdrant(self):\n",
      "        vectors = []\n",
      "        ids = []\n",
      "        for i, resume in enumerate(self.resumes):\n",
      "            vector, size = self.get_embedding(resume)\n",
      "            vectors.append(vector)\n",
      "            ids.append(i)\n",
      "\n",
      "        self.qdrant.upsert(\n",
      "            collection_name=\"collection_resume_matcher\",\n",
      "            points=Batch(\n",
      "                ids=ids,\n",
      "                vectors=vectors,\n",
      "                payloads=[{\"text\": resume} for resume in self.resumes]\n",
      "\n",
      "            )\n",
      "        )\n",
      "\n",
      "    def search(self):\n",
      "        vector, _ = self.get_embedding(self.jd)\n",
      "\n",
      "        hits = self.qdrant.search(\n",
      "            collection_name=\"collection_resume_matcher\",\n",
      "            query_vector=vector,\n",
      "            limit=30\n",
      "        )\n",
      "        results = []\n",
      "        for hit in hits:\n",
      "            result = {\n",
      "                'text': str(hit.payload)[:30],\n",
      "                'score': hit.score\n",
      "            }\n",
      "            results.append(result)\n",
      "\n",
      "        return results\n",
      "\n",
      "\n",
      "def get_similarity_score(resume_string, jd_string):\n",
      "    qdrant_search = QdrantSearch([resume_string], jd_string)\n",
      "    qdrant_search.update_qdrant()\n",
      "    results = qdrant_search.search()\n",
      "    return results\n",
      "\n",
      "\n",
      "\n",
      "import json\n",
      "from typing import List\n",
      "\n",
      "import networkx as nx\n",
      "import nltk\n",
      "import pandas as pd\n",
      "import plotly.express as px\n",
      "import plotly.graph_objects as go\n",
      "import streamlit as st\n",
      "from annotated_text import annotated_text, parameters\n",
      "from streamlit_extras import add_vertical_space as avs\n",
      "from streamlit_extras.badges import badge\n",
      "\n",
      "from scripts.similarity.get_similarity_score import get_similarity_score\n",
      "from scripts.utils.ReadFiles import get_filenames_from_dir\n",
      "\n",
      "\n",
      "try:\n",
      "    nltk.data.find('tokenizers/punkt')\n",
      "except LookupError:\n",
      "    nltk.download('punkt')\n",
      "\n",
      "parameters.SHOW_LABEL_SEPARATOR = False\n",
      "parameters.BORDER_RADIUS = 3\n",
      "parameters.PADDING = \"0.5 0.25rem\"\n",
      "\n",
      "\n",
      "def create_star_graph(nodes_and_weights, title):\n",
      "    # Create an empty graph\n",
      "    G = nx.Graph()\n",
      "\n",
      "    # Add the central node\n",
      "    central_node = \"resume\"\n",
      "    G.add_node(central_node)\n",
      "\n",
      "    # Add nodes and edges with weights to the graph\n",
      "    for node, weight in nodes_and_weights:\n",
      "        G.add_node(node)\n",
      "        G.add_edge(central_node, node, weight=weight * 100)\n",
      "\n",
      "    # Get position layout for nodes\n",
      "    pos = nx.spring_layout(G)\n",
      "\n",
      "    # Create edge trace\n",
      "    edge_x = []\n",
      "    edge_y = []\n",
      "    for edge in G.edges():\n",
      "        x0, y0 = pos[edge[0]]\n",
      "        x1, y1 = pos[edge[1]]\n",
      "        edge_x.extend([x0, x1, None])\n",
      "        edge_y.extend([y0, y1, None])\n",
      "\n",
      "    edge_trace = go.Scatter(x=edge_x, y=edge_y, line=dict(\n",
      "        width=0.5, color='#888'), hoverinfo='none', mode='lines')\n",
      "\n",
      "    # Create node trace\n",
      "    node_x = []\n",
      "    node_y = []\n",
      "    for node in G.nodes():\n",
      "        x, y = pos[node]\n",
      "        node_x.append(x)\n",
      "        node_y.append(y)\n",
      "\n",
      "    node_trace = go.Scatter(x=node_x, y=node_y, mode='markers', hoverinfo='text',\n",
      "                            marker=dict(showscale=True, colorscale='Rainbow', reversescale=True, color=[], size=10,\n",
      "                                        colorbar=dict(thickness=15, title='Node Connections', xanchor='left',\n",
      "                                                      titleside='right'), line_width=2))\n",
      "\n",
      "    # Color node points by number of connections\n",
      "    node_adjacencies = []\n",
      "    node_text = []\n",
      "    for node in G.nodes():\n",
      "        adjacencies = list(G.adj[node])  # changes here\n",
      "        node_adjacencies.append(len(adjacencies))\n",
      "        node_text.append(f'{node}<br># of connections: {len(adjacencies)}')\n",
      "\n",
      "    node_trace.marker.color = node_adjacencies\n",
      "    node_trace.text = node_text\n",
      "\n",
      "    # Create the figure\n",
      "    fig = go.Figure(data=[edge_trace, node_trace],\n",
      "                    layout=go.Layout(title=title, titlefont_size=16, showlegend=False,\n",
      "                                     hovermode='closest', margin=dict(b=20, l=5, r=5, t=40),\n",
      "                                     xaxis=dict(\n",
      "                                         showgrid=False, zeroline=False, showticklabels=False),\n",
      "                                     yaxis=dict(showgrid=False, zeroline=False, showticklabels=False)))\n",
      "\n",
      "    # Show the figure\n",
      "    st.plotly_chart(fig)\n",
      "\n",
      "\n",
      "def create_annotated_text(input_string: str, word_list: List[str], annotation: str, color_code: str):\n",
      "    # Tokenize the input string\n",
      "    tokens = nltk.word_tokenize(input_string)\n",
      "\n",
      "    # Convert the list to a set for quick lookups\n",
      "    word_set = set(word_list)\n",
      "\n",
      "    # Initialize an empty list to hold the annotated text\n",
      "    annotated_text = []\n",
      "\n",
      "    for token in tokens:\n",
      "        # Check if the token is in the set\n",
      "        if token in word_set:\n",
      "            # If it is, append a tuple with the token, annotation, and color code\n",
      "            annotated_text.append((token, annotation, color_code))\n",
      "        else:\n",
      "            # If it's not, just append the token as a string\n",
      "            annotated_text.append(token)\n",
      "\n",
      "    return annotated_text\n",
      "\n",
      "\n",
      "def read_json(filename):\n",
      "    with open(filename) as f:\n",
      "        data = json.load(f)\n",
      "    return data\n",
      "\n",
      "\n",
      "def tokenize_string(input_string):\n",
      "    tokens = nltk.word_tokenize(input_string)\n",
      "    return tokens\n",
      "\n",
      "\n",
      "st.image('Assets/img/header_image.jpg')\n",
      "\n",
      "st.title(':blue[Resume Matcher]')\n",
      "st.subheader(\n",
      "    'Free and Open Source ATS to help your resume pass the screening stage.')\n",
      "st.markdown(\n",
      "    \"Check the website [www.resumematcher.fyi](https://www.resumematcher.fyi/)\")\n",
      "st.markdown(\n",
      "    ' Give Resume Matcher a Star on [GitHub](https://github.com/srbhr/resume-matcher)')\n",
      "badge(type=\"github\", name=\"srbhr/Resume-Matcher\")\n",
      "\n",
      "st.text('For updates follow me on Twitter.')\n",
      "badge(type=\"twitter\", name=\"_srbhr_\")\n",
      "\n",
      "st.markdown(\n",
      "    'If you like the project and would like to further help in development please consider ')\n",
      "badge(type=\"buymeacoffee\", name=\"srbhr\")\n",
      "\n",
      "avs.add_vertical_space(5)\n",
      "\n",
      "resume_names = get_filenames_from_dir(\"Data/Processed/Resumes\")\n",
      "\n",
      "st.write(\"There are\", len(resume_names),\n",
      "         \" resumes present. Please select one from the menu below:\")\n",
      "output = st.slider('Select Resume Number', 0, len(resume_names) - 1, 2)\n",
      "\n",
      "avs.add_vertical_space(5)\n",
      "\n",
      "st.write(\"You have selected \", resume_names[output], \" printing the resume\")\n",
      "selected_file = read_json(\"Data/Processed/Resumes/\" + resume_names[output])\n",
      "\n",
      "avs.add_vertical_space(2)\n",
      "st.markdown(\"#### Parsed Resume Data\")\n",
      "st.caption(\n",
      "    \"This text is parsed from your resume. This is how it'll look like after getting parsed by an ATS.\")\n",
      "st.caption(\"Utilize this to understand how to make your resume ATS friendly.\")\n",
      "avs.add_vertical_space(3)\n",
      "# st.json(selected_file)\n",
      "st.write(selected_file[\"clean_data\"])\n",
      "\n",
      "avs.add_vertical_space(3)\n",
      "st.write(\"Now let's take a look at the extracted keywords from the resume.\")\n",
      "\n",
      "annotated_text(create_annotated_text(\n",
      "    selected_file[\"clean_data\"], selected_file[\"extracted_keywords\"],\n",
      "    \"KW\", \"#0B666A\"))\n",
      "\n",
      "resume_string = ' '.join(selected_file[\"extracted_keywords\"])\n",
      "\n",
      "avs.add_vertical_space(5)\n",
      "st.write(\"Now let's take a look at the extracted entities from the resume.\")\n",
      "\n",
      "# Call the function with your data\n",
      "create_star_graph(selected_file['keyterms'], \"Entities from Resume\")\n",
      "\n",
      "df2 = pd.DataFrame(selected_file['keyterms'], columns=[\"keyword\", \"value\"])\n",
      "\n",
      "# Create the dictionary\n",
      "keyword_dict = {}\n",
      "for keyword, value in selected_file['keyterms']:\n",
      "    keyword_dict[keyword] = value * 100\n",
      "\n",
      "fig = go.Figure(data=[go.Table(header=dict(values=[\"Keyword\", \"Value\"],\n",
      "                                           font=dict(size=12),\n",
      "                                           fill_color='#070A52'),\n",
      "                               cells=dict(values=[list(keyword_dict.keys()),\n",
      "                                                  list(keyword_dict.values())],\n",
      "                                          line_color='darkslategray',\n",
      "                                          fill_color='#6DA9E4'))\n",
      "                      ])\n",
      "st.plotly_chart(fig)\n",
      "\n",
      "st.divider()\n",
      "\n",
      "fig = px.treemap(df2, path=['keyword'], values='value',\n",
      "                 color_continuous_scale='Rainbow',\n",
      "                 title='Key Terms/Topics Extracted from your Resume')\n",
      "st.write(fig)\n",
      "\n",
      "avs.add_vertical_space(5)\n",
      "\n",
      "job_descriptions = get_filenames_from_dir(\"Data/Processed/JobDescription\")\n",
      "\n",
      "st.write(\"There are\", len(job_descriptions),\n",
      "         \" resumes present. Please select one from the menu below:\")\n",
      "output = st.slider('Select Job Description Number',\n",
      "                   0, len(job_descriptions) - 1, 2)\n",
      "\n",
      "avs.add_vertical_space(5)\n",
      "\n",
      "st.write(\"You have selected \",\n",
      "         job_descriptions[output], \" printing the job description\")\n",
      "selected_jd = read_json(\n",
      "    \"Data/Processed/JobDescription/\" + job_descriptions[output])\n",
      "\n",
      "avs.add_vertical_space(2)\n",
      "st.markdown(\"#### Job Description\")\n",
      "st.caption(\n",
      "    \"Currently in the pipeline I'm parsing this from PDF but it'll be from txt or copy paste.\")\n",
      "avs.add_vertical_space(3)\n",
      "# st.json(selected_file)\n",
      "st.write(selected_jd[\"clean_data\"])\n",
      "\n",
      "st.markdown(\"#### Common Words between Job Description and Resumes Highlighted.\")\n",
      "\n",
      "annotated_text(create_annotated_text(\n",
      "    selected_file[\"clean_data\"], selected_jd[\"extracted_keywords\"],\n",
      "    \"JD\", \"#F24C3D\"))\n",
      "jd_string = ' '.join(selected_jd[\"extracted_keywords\"])\n",
      "\n",
      "st.write(\"Now let's take a look at the extracted entities from the job description.\")\n",
      "\n",
      "# Call the function with your data\n",
      "create_star_graph(selected_jd['keyterms'], \"Entities from Job Description\")\n",
      "\n",
      "df2 = pd.DataFrame(selected_jd['keyterms'], columns=[\"keyword\", \"value\"])\n",
      "\n",
      "# Create the dictionary\n",
      "keyword_dict = {}\n",
      "for keyword, value in selected_jd['keyterms']:\n",
      "    keyword_dict[keyword] = value * 100\n",
      "\n",
      "fig = go.Figure(data=[go.Table(header=dict(values=[\"Keyword\", \"Value\"],\n",
      "                                           font=dict(size=12),\n",
      "                                           fill_color='#070A52'),\n",
      "                               cells=dict(values=[list(keyword_dict.keys()),\n",
      "                                                  list(keyword_dict.values())],\n",
      "                                          line_color='darkslategray',\n",
      "                                          fill_color='#6DA9E4'))\n",
      "                      ])\n",
      "st.plotly_chart(fig)\n",
      "\n",
      "st.divider()\n",
      "\n",
      "fig = px.treemap(df2, path=['keyword'], values='value',\n",
      "                 color_continuous_scale='Rainbow',\n",
      "                 title='Key Terms/Topics Extracted from the selected Job Description')\n",
      "st.write(fig)\n",
      "\n",
      "avs.add_vertical_space(3)\n",
      "\n",
      "result = get_similarity_score(resume_string, jd_string)\n",
      "similarity_score=result[0][\"score\"]\n",
      "st.write(\"Similarity Score obtained for the resume and job description is:\", similarity_score)\n",
      "\n",
      "st.title(':blue[Resume Matcher]')\n",
      "st.subheader(\n",
      "    'Free and Open Source ATS to help your resume pass the screening stage.')\n",
      "st.markdown(\n",
      "    ' Give Resume Matcher a Star on [GitHub](https://github.com/srbhr/Resume-Matcher/)')\n",
      "badge(type=\"github\", name=\"srbhr/Resume-Matcher\")\n",
      "\n",
      "st.text('For updates follow me on Twitter.')\n",
      "badge(type=\"twitter\", name=\"_srbhr_\")\n",
      "\n",
      "st.markdown(\n",
      "    'If you like the project and would like to further help in development please consider ')\n",
      "badge(type=\"buymeacoffee\", name=\"srbhr\")\n",
      "\n",
      "import json\n",
      "import logging\n",
      "import os\n",
      "\n",
      "import cohere\n",
      "import yaml\n",
      "from qdrant_client import QdrantClient, models\n",
      "from qdrant_client.http.models import Batch\n",
      "\n",
      "logging.basicConfig(\n",
      "    filename='app_similarity_score.log',\n",
      "    filemode='w',\n",
      "    level=logging.INFO,\n",
      "    format=\"%(asctime)s - %(name)s - %(levelname)s - %(message)s\"\n",
      ")\n",
      "\n",
      "logger = logging.getLogger(__name__)\n",
      "logger.setLevel(logging.DEBUG)\n",
      "\n",
      "console_handler = logging.StreamHandler()\n",
      "formatter = logging.Formatter(\"%(asctime)s - %(name)s - %(levelname)s - %(message)s\")\n",
      "console_handler.setFormatter(formatter)\n",
      "console_handler.setLevel(logging.DEBUG)\n",
      "\n",
      "file_handler = logging.FileHandler(\"app_similarity_score.log\")\n",
      "file_handler.setLevel(logging.DEBUG)\n",
      "file_handler.setFormatter(formatter)\n",
      "\n",
      "logger.addHandler(file_handler)\n",
      "logger.addHandler(console_handler)\n",
      "\n",
      "\n",
      "def find_path(folder_name):\n",
      "    curr_dir = os.getcwd()\n",
      "    while True:\n",
      "        if folder_name in os.listdir(curr_dir):\n",
      "            return os.path.join(curr_dir, folder_name)\n",
      "        else:\n",
      "            parent_dir = os.path.dirname(curr_dir)\n",
      "            if parent_dir == '/':\n",
      "                break\n",
      "            curr_dir = parent_dir\n",
      "    raise ValueError(f\"Folder '{folder_name}' not found.\")\n",
      "\n",
      "\n",
      "cwd = find_path('Resume-Matcher')\n",
      "READ_RESUME_FROM = os.path.join(cwd, 'Data', 'Processed', 'Resumes')\n",
      "READ_JOB_DESCRIPTION_FROM = os.path.join(cwd, 'Data', 'Processed', 'JobDescription')\n",
      "config_path = os.path.join(cwd, \"scripts\", \"similarity\")\n",
      "\n",
      "\n",
      "def read_config(filepath):\n",
      "    try:\n",
      "        with open(filepath) as f:\n",
      "            config = yaml.safe_load(f)\n",
      "        return config\n",
      "    except FileNotFoundError as e:\n",
      "        logger.error(f\"Configuration file {filepath} not found: {e}\")\n",
      "    except yaml.YAMLError as e:\n",
      "        logger.error(f\"Error parsing YAML in configuration file {filepath}: {e}\", exc_info=True)\n",
      "    except Exception as e:\n",
      "        logger.error(f\"Error reading configuration file {filepath}: {e}\")\n",
      "    return None\n",
      "\n",
      "\n",
      "def read_doc(path):\n",
      "    with open(path) as f:\n",
      "        try:\n",
      "            data = json.load(f)\n",
      "        except Exception as e:\n",
      "            logger.error(f'Error reading JSON file: {e}')\n",
      "            data = {}\n",
      "    return data\n",
      "\n",
      "\n",
      "class QdrantSearch:\n",
      "    def __init__(self, resumes, jd):\n",
      "        config = read_config(config_path + \"/config.yml\")\n",
      "        self.cohere_key = config['cohere']['api_key']\n",
      "        self.qdrant_key = config['qdrant']['api_key']\n",
      "        self.qdrant_url = config['qdrant']['url']\n",
      "        self.resumes = resumes\n",
      "        self.jd = jd\n",
      "        self.cohere = cohere.Client(self.cohere_key)\n",
      "\n",
      "        self.qdrant = QdrantClient(\n",
      "            url=self.qdrant_url,\n",
      "            api_key=self.qdrant_key,\n",
      "        )\n",
      "\n",
      "        vector_size = 4096\n",
      "        self.qdrant.recreate_collection(\n",
      "            collection_name=\"collection_resume_matcher\",\n",
      "            vectors_config=models.VectorParams(\n",
      "                size=vector_size,\n",
      "                distance=models.Distance.COSINE\n",
      "            )\n",
      "        )\n",
      "\n",
      "        self.logger = logging.getLogger(self.__class__.__name__)\n",
      "\n",
      "        self.logger.addHandler(console_handler)\n",
      "        self.logger.addHandler(file_handler)\n",
      "\n",
      "    def get_embedding(self, text):\n",
      "        try:\n",
      "            embeddings = self.cohere.embed([text], \"large\").embeddings\n",
      "            return list(map(float, embeddings[0])), len(embeddings[0])\n",
      "        except Exception as e:\n",
      "            self.logger.error(f\"Error getting embeddings: {e}\", exc_info=True)\n",
      "\n",
      "    def update_qdrant(self):\n",
      "        vectors = []\n",
      "        ids = []\n",
      "        for i, resume in enumerate(self.resumes):\n",
      "            vector, size = self.get_embedding(resume)\n",
      "            vectors.append(vector)\n",
      "            ids.append(i)\n",
      "        try:\n",
      "            self.qdrant.upsert(\n",
      "                collection_name=\"collection_resume_matcher\",\n",
      "                points=Batch(\n",
      "                    ids=ids,\n",
      "                    vectors=vectors,\n",
      "                    payloads=[{\"text\": resume} for resume in self.resumes]\n",
      "\n",
      "                )\n",
      "            )\n",
      "        except Exception as e:\n",
      "            self.logger.error(f\"Error upserting the vectors to the qdrant collection: {e}\", exc_info=True)\n",
      "\n",
      "    def search(self):\n",
      "        vector, _ = self.get_embedding(self.jd)\n",
      "\n",
      "        hits = self.qdrant.search(\n",
      "            collection_name=\"collection_resume_matcher\",\n",
      "            query_vector=vector,\n",
      "            limit=30\n",
      "        )\n",
      "        results = []\n",
      "        for hit in hits:\n",
      "            result = {\n",
      "                'text': str(hit.payload)[:30],\n",
      "                'score': hit.score\n",
      "            }\n",
      "            results.append(result)\n",
      "\n",
      "        return results\n",
      "\n",
      "\n",
      "def get_similarity_score(resume_string, job_description_string):\n",
      "    logger.info(\"Started getting similarity score\")\n",
      "    qdrant_search = QdrantSearch([resume_string], job_description_string)\n",
      "    qdrant_search.update_qdrant()\n",
      "    search_result = qdrant_search.search()\n",
      "    logger.info(\"Finished getting similarity score\")\n",
      "    return search_result\n",
      "\n",
      "\n",
      "if __name__ == \"__main__\":\n",
      "    # To give your custom resume use this code\n",
      "    resume_dict = read_config(\n",
      "        READ_RESUME_FROM + \"/Resume-bruce_wayne_fullstack.pdf4783d115-e6fc-462e-ae4d-479152884b28.json\")\n",
      "    job_dict = read_config(\n",
      "        READ_JOB_DESCRIPTION_FROM + \"/JobDescription-job_desc_full_stack_engineer_pdf4de00846-a4fe-4fe5-a4d7\"\n",
      "                                    \"-2a8a1b9ad020.json\")\n",
      "    resume_keywords = resume_dict[\"extracted_keywords\"]\n",
      "    job_description_keywords = job_dict[\"extracted_keywords\"]\n",
      "\n",
      "    resume_string = ' '.join(resume_keywords)\n",
      "    jd_string = ' '.join(job_description_keywords)\n",
      "    final_result = get_similarity_score(resume_string, jd_string)\n",
      "    for r in final_result:\n",
      "        print(r)\n",
      "\n",
      "import json\n",
      "import os\n",
      "from typing import List\n",
      "\n",
      "import networkx as nx\n",
      "import nltk\n",
      "import pandas as pd\n",
      "import plotly.express as px\n",
      "import plotly.graph_objects as go\n",
      "import streamlit as st\n",
      "from annotated_text import annotated_text, parameters\n",
      "from streamlit_extras import add_vertical_space as avs\n",
      "from streamlit_extras.badges import badge\n",
      "\n",
      "from scripts.similarity.get_similarity_score import get_similarity_score, find_path, read_config\n",
      "from scripts.utils.ReadFiles import get_filenames_from_dir\n",
      "\n",
      "cwd = find_path('Resume-Matcher')\n",
      "config_path = os.path.join(cwd, \"scripts\", \"similarity\")\n",
      "\n",
      "try:\n",
      "    nltk.data.find('tokenizers/punkt')\n",
      "except LookupError:\n",
      "    nltk.download('punkt')\n",
      "\n",
      "parameters.SHOW_LABEL_SEPARATOR = False\n",
      "parameters.BORDER_RADIUS = 3\n",
      "parameters.PADDING = \"0.5 0.25rem\"\n",
      "\n",
      "\n",
      "def create_star_graph(nodes_and_weights, title):\n",
      "    # Create an empty graph\n",
      "    G = nx.Graph()\n",
      "\n",
      "    # Add the central node\n",
      "    central_node = \"resume\"\n",
      "    G.add_node(central_node)\n",
      "\n",
      "    # Add nodes and edges with weights to the graph\n",
      "    for node, weight in nodes_and_weights:\n",
      "        G.add_node(node)\n",
      "        G.add_edge(central_node, node, weight=weight * 100)\n",
      "\n",
      "    # Get position layout for nodes\n",
      "    pos = nx.spring_layout(G)\n",
      "\n",
      "    # Create edge trace\n",
      "    edge_x = []\n",
      "    edge_y = []\n",
      "    for edge in G.edges():\n",
      "        x0, y0 = pos[edge[0]]\n",
      "        x1, y1 = pos[edge[1]]\n",
      "        edge_x.extend([x0, x1, None])\n",
      "        edge_y.extend([y0, y1, None])\n",
      "\n",
      "    edge_trace = go.Scatter(x=edge_x, y=edge_y, line=dict(\n",
      "        width=0.5, color='#888'), hoverinfo='none', mode='lines')\n",
      "\n",
      "    # Create node trace\n",
      "    node_x = []\n",
      "    node_y = []\n",
      "    for node in G.nodes():\n",
      "        x, y = pos[node]\n",
      "        node_x.append(x)\n",
      "        node_y.append(y)\n",
      "\n",
      "    node_trace = go.Scatter(x=node_x, y=node_y, mode='markers', hoverinfo='text',\n",
      "                            marker=dict(showscale=True, colorscale='Rainbow', reversescale=True, color=[], size=10,\n",
      "                                        colorbar=dict(thickness=15, title='Node Connections', xanchor='left',\n",
      "                                                      titleside='right'), line_width=2))\n",
      "\n",
      "    # Color node points by number of connections\n",
      "    node_adjacencies = []\n",
      "    node_text = []\n",
      "    for node in G.nodes():\n",
      "        adjacencies = list(G.adj[node])  # changes here\n",
      "        node_adjacencies.append(len(adjacencies))\n",
      "        node_text.append(f'{node}<br># of connections: {len(adjacencies)}')\n",
      "\n",
      "    node_trace.marker.color = node_adjacencies\n",
      "    node_trace.text = node_text\n",
      "\n",
      "    # Create the figure\n",
      "    fig = go.Figure(data=[edge_trace, node_trace],\n",
      "                    layout=go.Layout(title=title, titlefont_size=16, showlegend=False,\n",
      "                                     hovermode='closest', margin=dict(b=20, l=5, r=5, t=40),\n",
      "                                     xaxis=dict(\n",
      "                                         showgrid=False, zeroline=False, showticklabels=False),\n",
      "                                     yaxis=dict(showgrid=False, zeroline=False, showticklabels=False)))\n",
      "\n",
      "    # Show the figure\n",
      "    st.plotly_chart(fig)\n",
      "\n",
      "\n",
      "def create_annotated_text(input_string: str, word_list: List[str], annotation: str, color_code: str):\n",
      "    # Tokenize the input string\n",
      "    tokens = nltk.word_tokenize(input_string)\n",
      "\n",
      "    # Convert the list to a set for quick lookups\n",
      "    word_set = set(word_list)\n",
      "\n",
      "    # Initialize an empty list to hold the annotated text\n",
      "    annotated_text = []\n",
      "\n",
      "    for token in tokens:\n",
      "        # Check if the token is in the set\n",
      "        if token in word_set:\n",
      "            # If it is, append a tuple with the token, annotation, and color code\n",
      "            annotated_text.append((token, annotation, color_code))\n",
      "        else:\n",
      "            # If it's not, just append the token as a string\n",
      "            annotated_text.append(token)\n",
      "\n",
      "    return annotated_text\n",
      "\n",
      "\n",
      "def read_json(filename):\n",
      "    with open(filename) as f:\n",
      "        data = json.load(f)\n",
      "    return data\n",
      "\n",
      "\n",
      "def tokenize_string(input_string):\n",
      "    tokens = nltk.word_tokenize(input_string)\n",
      "    return tokens\n",
      "\n",
      "\n",
      "st.image('Assets/img/header_image.jpg')\n",
      "\n",
      "st.title(':blue[Resume Matcher]')\n",
      "st.subheader(\n",
      "    'Free and Open Source ATS to help your resume pass the screening stage.')\n",
      "st.markdown(\n",
      "    \"Check the website [www.resumematcher.fyi](https://www.resumematcher.fyi/)\")\n",
      "st.markdown(\n",
      "    ' Give Resume Matcher a Star on [GitHub](https://github.com/srbhr/resume-matcher)')\n",
      "badge(type=\"github\", name=\"srbhr/Resume-Matcher\")\n",
      "\n",
      "st.text('For updates follow me on Twitter.')\n",
      "badge(type=\"twitter\", name=\"_srbhr_\")\n",
      "\n",
      "st.markdown(\n",
      "    'If you like the project and would like to further help in development please consider ')\n",
      "badge(type=\"buymeacoffee\", name=\"srbhr\")\n",
      "\n",
      "avs.add_vertical_space(5)\n",
      "\n",
      "resume_names = get_filenames_from_dir(\"Data/Processed/Resumes\")\n",
      "\n",
      "output = 0\n",
      "\n",
      "if len(resume_names) > 1:\n",
      "    st.write(\"There are\", len(resume_names),\n",
      "             \" resumes present. Please select one from the menu below:\")\n",
      "    output = st.slider('Select Resume Number', 0, len(resume_names) - 1, 0)\n",
      "else:\n",
      "    st.write(\"There is 1 resume present\")\n",
      "\n",
      "avs.add_vertical_space(5)\n",
      "\n",
      "st.write(\"You have selected \", resume_names[output], \" printing the resume\")\n",
      "selected_file = read_json(\"Data/Processed/Resumes/\" + resume_names[output])\n",
      "\n",
      "avs.add_vertical_space(2)\n",
      "st.markdown(\"#### Parsed Resume Data\")\n",
      "st.caption(\n",
      "    \"This text is parsed from your resume. This is how it'll look like after getting parsed by an ATS.\")\n",
      "st.caption(\"Utilize this to understand how to make your resume ATS friendly.\")\n",
      "avs.add_vertical_space(3)\n",
      "# st.json(selected_file)\n",
      "st.write(selected_file[\"clean_data\"])\n",
      "\n",
      "avs.add_vertical_space(3)\n",
      "st.write(\"Now let's take a look at the extracted keywords from the resume.\")\n",
      "\n",
      "annotated_text(create_annotated_text(\n",
      "    selected_file[\"clean_data\"], selected_file[\"extracted_keywords\"],\n",
      "    \"KW\", \"#0B666A\"))\n",
      "\n",
      "avs.add_vertical_space(5)\n",
      "st.write(\"Now let's take a look at the extracted entities from the resume.\")\n",
      "\n",
      "# Call the function with your data\n",
      "create_star_graph(selected_file['keyterms'], \"Entities from Resume\")\n",
      "\n",
      "df2 = pd.DataFrame(selected_file['keyterms'], columns=[\"keyword\", \"value\"])\n",
      "\n",
      "# Create the dictionary\n",
      "keyword_dict = {}\n",
      "for keyword, value in selected_file['keyterms']:\n",
      "    keyword_dict[keyword] = value * 100\n",
      "\n",
      "fig = go.Figure(data=[go.Table(header=dict(values=[\"Keyword\", \"Value\"],\n",
      "                                           font=dict(size=12),\n",
      "                                           fill_color='#070A52'),\n",
      "                               cells=dict(values=[list(keyword_dict.keys()),\n",
      "                                                  list(keyword_dict.values())],\n",
      "                                          line_color='darkslategray',\n",
      "                                          fill_color='#6DA9E4'))\n",
      "                      ])\n",
      "st.plotly_chart(fig)\n",
      "\n",
      "st.divider()\n",
      "\n",
      "fig = px.treemap(df2, path=['keyword'], values='value',\n",
      "                 color_continuous_scale='Rainbow',\n",
      "                 title='Key Terms/Topics Extracted from your Resume')\n",
      "st.write(fig)\n",
      "\n",
      "avs.add_vertical_space(5)\n",
      "\n",
      "job_descriptions = get_filenames_from_dir(\"Data/Processed/JobDescription\")\n",
      "\n",
      "output = 0\n",
      "if len(job_descriptions) > 1:\n",
      "    st.write(\"There are\", len(job_descriptions),\n",
      "             \" resumes present. Please select one from the menu below:\")\n",
      "    output = st.slider('Select Job Description Number',\n",
      "                       0, len(job_descriptions) - 1, 0)\n",
      "else:\n",
      "    st.write(\"There is 1 job description present\")\n",
      "\n",
      "avs.add_vertical_space(5)\n",
      "\n",
      "st.write(\"You have selected \",\n",
      "         job_descriptions[output], \" printing the job description\")\n",
      "selected_jd = read_json(\n",
      "    \"Data/Processed/JobDescription/\" + job_descriptions[output])\n",
      "\n",
      "avs.add_vertical_space(2)\n",
      "st.markdown(\"#### Job Description\")\n",
      "st.caption(\n",
      "    \"Currently in the pipeline I'm parsing this from PDF but it'll be from txt or copy paste.\")\n",
      "avs.add_vertical_space(3)\n",
      "# st.json(selected_file)\n",
      "st.write(selected_jd[\"clean_data\"])\n",
      "\n",
      "st.markdown(\"#### Common Words between Job Description and Resumes Highlighted.\")\n",
      "\n",
      "annotated_text(create_annotated_text(\n",
      "    selected_file[\"clean_data\"], selected_jd[\"extracted_keywords\"],\n",
      "    \"JD\", \"#F24C3D\"))\n",
      "\n",
      "st.write(\"Now let's take a look at the extracted entities from the job description.\")\n",
      "\n",
      "# Call the function with your data\n",
      "create_star_graph(selected_jd['keyterms'], \"Entities from Job Description\")\n",
      "\n",
      "df2 = pd.DataFrame(selected_jd['keyterms'], columns=[\"keyword\", \"value\"])\n",
      "\n",
      "# Create the dictionary\n",
      "keyword_dict = {}\n",
      "for keyword, value in selected_jd['keyterms']:\n",
      "    keyword_dict[keyword] = value * 100\n",
      "\n",
      "fig = go.Figure(data=[go.Table(header=dict(values=[\"Keyword\", \"Value\"],\n",
      "                                           font=dict(size=12),\n",
      "                                           fill_color='#070A52'),\n",
      "                               cells=dict(values=[list(keyword_dict.keys()),\n",
      "                                                  list(keyword_dict.values())],\n",
      "                                          line_color='darkslategray',\n",
      "                                          fill_color='#6DA9E4'))\n",
      "                      ])\n",
      "st.plotly_chart(fig)\n",
      "\n",
      "st.divider()\n",
      "\n",
      "fig = px.treemap(df2, path=['keyword'], values='value',\n",
      "                 color_continuous_scale='Rainbow',\n",
      "                 title='Key Terms/Topics Extracted from the selected Job Description')\n",
      "st.write(fig)\n",
      "\n",
      "avs.add_vertical_space(3)\n",
      "\n",
      "config_file_path = config_path + \"/config.yml\"\n",
      "if os.path.exists(config_file_path):\n",
      "    config_data = read_config(config_file_path)\n",
      "    if config_data:\n",
      "        print(\"Config file parsed successfully:\")\n",
      "        resume_string = ' '.join(selected_file[\"extracted_keywords\"])\n",
      "        jd_string = ' '.join(selected_jd[\"extracted_keywords\"])\n",
      "        result = get_similarity_score(resume_string, jd_string)\n",
      "        similarity_score = result[0][\"score\"]\n",
      "        st.write(\"Similarity Score obtained for the resume and job description is:\", similarity_score)\n",
      "else:\n",
      "    print(\"Config file does not exist.\")\n",
      "\n",
      "\n",
      "st.title(':blue[Resume Matcher]')\n",
      "st.subheader(\n",
      "    'Free and Open Source ATS to help your resume pass the screening stage.')\n",
      "st.markdown(\n",
      "    ' Give Resume Matcher a Star on [GitHub](https://github.com/srbhr/Resume-Matcher/)')\n",
      "badge(type=\"github\", name=\"srbhr/Resume-Matcher\")\n",
      "\n",
      "st.text('For updates follow me on Twitter.')\n",
      "badge(type=\"twitter\", name=\"_srbhr_\")\n",
      "\n",
      "st.markdown(\n",
      "    'If you like the project and would like to further help in development please consider ')\n",
      "badge(type=\"buymeacoffee\", name=\"srbhr\")\n",
      "\n",
      "import json\n",
      "from scripts.utils.ReadFiles import get_filenames_from_dir\n",
      "from scripts.ResumeProcessor import ResumeProcessor\n",
      "from scripts.JobDescriptionProcessor import JobDescriptionProcessor\n",
      "from scripts.utils.logger import init_logging_config\n",
      "import logging\n",
      "import os\n",
      "\n",
      "init_logging_config()\n",
      "\n",
      "PROCESSED_RESUMES_PATH = \"Data/Processed/Resumes\"\n",
      "PROCESSED_JOB_DESCRIPTIONS_PATH = \"Data/Processed/JobDescription\"\n",
      "\n",
      "def read_json(filename):\n",
      "    with open(filename) as f:\n",
      "        data = json.load(f)\n",
      "    return data\n",
      "\n",
      "def remove_old_files(files_path):\n",
      "\n",
      "    for filename in os.listdir(files_path):\n",
      "        try:\n",
      "            file_path = os.path.join(files_path, filename)\n",
      "\n",
      "            if os.path.isfile(file_path):\n",
      "                os.remove(file_path)\n",
      "        except Exception as e:  \n",
      "            logging.error(f\"Error deleting {file_path}:\\n{e}\")\n",
      "\n",
      "    logging.info(\"Deleted old files from \"+files_path)\n",
      "\n",
      "\n",
      "logging.info('Started to read from Data/Resumes')\n",
      "try:\n",
      "    # Check if there are resumes present or not.\n",
      "    # If present then parse it.\n",
      "    remove_old_files(PROCESSED_RESUMES_PATH)\n",
      "\n",
      "    file_names = get_filenames_from_dir(\"Data/Resumes\")\n",
      "    logging.info('Reading from Data/Resumes is now complete.')\n",
      "except:\n",
      "    # Exit the program if there are no resumes.\n",
      "    logging.error('There are no resumes present in the specified folder.')\n",
      "    logging.error('Exiting from the program.')\n",
      "    logging.error(\n",
      "        'Please add resumes in the Data/Resumes folder and try again.')\n",
      "    exit(1)\n",
      "\n",
      "# Now after getting the file_names parse the resumes into a JSON Format.\n",
      "logging.info('Started parsing the resumes.')\n",
      "for file in file_names:\n",
      "    processor = ResumeProcessor(file)\n",
      "    success = processor.process()\n",
      "logging.info('Parsing of the resumes is now complete.')\n",
      "\n",
      "logging.info('Started to read from Data/JobDescription')\n",
      "try:\n",
      "    # Check if there are resumes present or not.\n",
      "    # If present then parse it.\n",
      "    remove_old_files(PROCESSED_JOB_DESCRIPTIONS_PATH)\n",
      "\n",
      "    file_names = get_filenames_from_dir(\"Data/JobDescription\")\n",
      "    logging.info('Reading from Data/JobDescription is now complete.')\n",
      "except:\n",
      "    # Exit the program if there are no resumes.\n",
      "    logging.error(\n",
      "        'There are no job-description present in the specified folder.')\n",
      "    logging.error('Exiting from the program.')\n",
      "    logging.error(\n",
      "        'Please add resumes in the Data/JobDescription folder and try again.')\n",
      "    exit(1)\n",
      "\n",
      "# Now after getting the file_names parse the resumes into a JSON Format.\n",
      "logging.info('Started parsing the Job Descriptions.')\n",
      "for file in file_names:\n",
      "    processor = JobDescriptionProcessor(file)\n",
      "    success = processor.process()\n",
      "logging.info('Parsing of the Job Descriptions is now complete.')\n",
      "logging.info('Success now run `streamlit run streamlit_second.py`')\n",
      "\n",
      "import logging\n",
      "\n",
      "\n",
      "def init_logging_config():\n",
      "    class CustomFormatter(logging.Formatter):\n",
      "        def __init__(self, file=False):\n",
      "            super().__init__()\n",
      "            yellow = \"\\x1b[36;10m\" if not file else \"\"\n",
      "            blue = \"\\x1b[35;10m\" if not file else \"\"\n",
      "            green = \"\\x1b[32;10m\" if not file else \"\"\n",
      "            red = \"\\x1b[31;10m\" if not file else \"\"\n",
      "            bold_red = \"\\x1b[31;1m\" if not file else \"\"\n",
      "            reset = \"\\x1b[0m\" if not file else \"\"\n",
      "            log = \"%(asctime)s (%(filename)s:%(lineno)d) - %(levelname)s: \"\n",
      "            msg = reset + \"%(message)s\"\n",
      "\n",
      "            self.FORMATS = {\n",
      "                logging.DEBUG: blue + log + msg,\n",
      "                logging.INFO: green + log + msg,\n",
      "                logging.WARNING: yellow + log + msg,\n",
      "                logging.ERROR: red + log + msg,\n",
      "                logging.CRITICAL: bold_red + log + msg,\n",
      "            }\n",
      "\n",
      "        def format(self, record):\n",
      "            log_fmt = self.FORMATS.get(record.levelno)\n",
      "            formatter = logging.Formatter(log_fmt)\n",
      "            return formatter.format(record)\n",
      "\n",
      "    logger = logging.getLogger()\n",
      "    logger.setLevel(logging.DEBUG)\n",
      "\n",
      "    stderr_handler = logging.StreamHandler()\n",
      "    stderr_handler.setLevel(logging.DEBUG)\n",
      "    stderr_handler.setFormatter(CustomFormatter())\n",
      "    logger.addHandler(stderr_handler)\n",
      "\n",
      "    file_handler = logging.FileHandler(\"app.log\",  mode=\"w\")\n",
      "    file_handler.setLevel(logging.DEBUG)\n",
      "    file_handler.setFormatter(CustomFormatter(True))\n",
      "    logger.addHandler(file_handler)\n",
      "\n",
      "import json\n",
      "import os\n",
      "from typing import List\n",
      "\n",
      "import networkx as nx\n",
      "import nltk\n",
      "import pandas as pd\n",
      "import plotly.express as px\n",
      "import plotly.graph_objects as go\n",
      "import streamlit as st\n",
      "from annotated_text import annotated_text, parameters\n",
      "from streamlit_extras import add_vertical_space as avs\n",
      "from streamlit_extras.badges import badge\n",
      "\n",
      "from scripts.similarity.get_similarity_score import get_similarity_score, find_path, read_config\n",
      "from scripts.utils.ReadFiles import get_filenames_from_dir\n",
      "\n",
      "cwd = find_path('Resume-Matcher')\n",
      "config_path = os.path.join(cwd, \"scripts\", \"similarity\")\n",
      "\n",
      "try:\n",
      "    nltk.data.find('tokenizers/punkt')\n",
      "except LookupError:\n",
      "    nltk.download('punkt')\n",
      "\n",
      "parameters.SHOW_LABEL_SEPARATOR = False\n",
      "parameters.BORDER_RADIUS = 3\n",
      "parameters.PADDING = \"0.5 0.25rem\"\n",
      "\n",
      "\n",
      "def create_star_graph(nodes_and_weights, title):\n",
      "    # Create an empty graph\n",
      "    G = nx.Graph()\n",
      "\n",
      "    # Add the central node\n",
      "    central_node = \"resume\"\n",
      "    G.add_node(central_node)\n",
      "\n",
      "    # Add nodes and edges with weights to the graph\n",
      "    for node, weight in nodes_and_weights:\n",
      "        G.add_node(node)\n",
      "        G.add_edge(central_node, node, weight=weight * 100)\n",
      "\n",
      "    # Get position layout for nodes\n",
      "    pos = nx.spring_layout(G)\n",
      "\n",
      "    # Create edge trace\n",
      "    edge_x = []\n",
      "    edge_y = []\n",
      "    for edge in G.edges():\n",
      "        x0, y0 = pos[edge[0]]\n",
      "        x1, y1 = pos[edge[1]]\n",
      "        edge_x.extend([x0, x1, None])\n",
      "        edge_y.extend([y0, y1, None])\n",
      "\n",
      "    edge_trace = go.Scatter(x=edge_x, y=edge_y, line=dict(\n",
      "        width=0.5, color='#888'), hoverinfo='none', mode='lines')\n",
      "\n",
      "    # Create node trace\n",
      "    node_x = []\n",
      "    node_y = []\n",
      "    for node in G.nodes():\n",
      "        x, y = pos[node]\n",
      "        node_x.append(x)\n",
      "        node_y.append(y)\n",
      "\n",
      "    node_trace = go.Scatter(x=node_x, y=node_y, mode='markers', hoverinfo='text',\n",
      "                            marker=dict(showscale=True, colorscale='Rainbow', reversescale=True, color=[], size=10,\n",
      "                                        colorbar=dict(thickness=15, title='Node Connections', xanchor='left',\n",
      "                                                      titleside='right'), line_width=2))\n",
      "\n",
      "    # Color node points by number of connections\n",
      "    node_adjacencies = []\n",
      "    node_text = []\n",
      "    for node in G.nodes():\n",
      "        adjacencies = list(G.adj[node])  # changes here\n",
      "        node_adjacencies.append(len(adjacencies))\n",
      "        node_text.append(f'{node}<br># of connections: {len(adjacencies)}')\n",
      "\n",
      "    node_trace.marker.color = node_adjacencies\n",
      "    node_trace.text = node_text\n",
      "\n",
      "    # Create the figure\n",
      "    fig = go.Figure(data=[edge_trace, node_trace],\n",
      "                    layout=go.Layout(title=title, titlefont_size=16, showlegend=False,\n",
      "                                     hovermode='closest', margin=dict(b=20, l=5, r=5, t=40),\n",
      "                                     xaxis=dict(\n",
      "                                         showgrid=False, zeroline=False, showticklabels=False),\n",
      "                                     yaxis=dict(showgrid=False, zeroline=False, showticklabels=False)))\n",
      "\n",
      "    # Show the figure\n",
      "    st.plotly_chart(fig)\n",
      "\n",
      "\n",
      "def create_annotated_text(input_string: str, word_list: List[str], annotation: str, color_code: str):\n",
      "    # Tokenize the input string\n",
      "    tokens = nltk.word_tokenize(input_string)\n",
      "\n",
      "    # Convert the list to a set for quick lookups\n",
      "    word_set = set(word_list)\n",
      "\n",
      "    # Initialize an empty list to hold the annotated text\n",
      "    annotated_text = []\n",
      "\n",
      "    for token in tokens:\n",
      "        # Check if the token is in the set\n",
      "        if token in word_set:\n",
      "            # If it is, append a tuple with the token, annotation, and color code\n",
      "            annotated_text.append((token, annotation, color_code))\n",
      "        else:\n",
      "            # If it's not, just append the token as a string\n",
      "            annotated_text.append(token)\n",
      "\n",
      "    return annotated_text\n",
      "\n",
      "\n",
      "def read_json(filename):\n",
      "    with open(filename) as f:\n",
      "        data = json.load(f)\n",
      "    return data\n",
      "\n",
      "\n",
      "def tokenize_string(input_string):\n",
      "    tokens = nltk.word_tokenize(input_string)\n",
      "    return tokens\n",
      "\n",
      "\n",
      "st.image('Assets/img/header_image.jpg')\n",
      "\n",
      "st.title(':blue[Resume Matcher]')\n",
      "st.subheader(\n",
      "    'Free and Open Source ATS to help your resume pass the screening stage.')\n",
      "st.markdown(\n",
      "    \"Check the website [www.resumematcher.fyi](https://www.resumematcher.fyi/)\")\n",
      "st.markdown(\n",
      "    ' Give Resume Matcher a Star on [GitHub](https://github.com/srbhr/resume-matcher)')\n",
      "badge(type=\"github\", name=\"srbhr/Resume-Matcher\")\n",
      "\n",
      "st.text('For updates follow me on Twitter.')\n",
      "badge(type=\"twitter\", name=\"_srbhr_\")\n",
      "\n",
      "st.markdown(\n",
      "    'If you like the project and would like to further help in development please consider ')\n",
      "badge(type=\"buymeacoffee\", name=\"srbhr\")\n",
      "\n",
      "avs.add_vertical_space(5)\n",
      "\n",
      "resume_names = get_filenames_from_dir(\"Data/Processed/Resumes\")\n",
      "\n",
      "output = 0\n",
      "\n",
      "if len(resume_names) > 1:\n",
      "    st.write(\"There are\", len(resume_names),\n",
      "             \" resumes present. Please select one from the menu below:\")\n",
      "    output = st.slider('Select Resume Number', 0, len(resume_names) - 1, 0)\n",
      "else:\n",
      "    st.write(\"There is 1 resume present\")\n",
      "\n",
      "avs.add_vertical_space(5)\n",
      "\n",
      "st.write(\"You have selected \", resume_names[output], \" printing the resume\")\n",
      "selected_file = read_json(\"Data/Processed/Resumes/\" + resume_names[output])\n",
      "\n",
      "avs.add_vertical_space(2)\n",
      "st.markdown(\"#### Parsed Resume Data\")\n",
      "st.caption(\n",
      "    \"This text is parsed from your resume. This is how it'll look like after getting parsed by an ATS.\")\n",
      "st.caption(\"Utilize this to understand how to make your resume ATS friendly.\")\n",
      "avs.add_vertical_space(3)\n",
      "# st.json(selected_file)\n",
      "st.write(selected_file[\"clean_data\"])\n",
      "\n",
      "avs.add_vertical_space(3)\n",
      "st.write(\"Now let's take a look at the extracted keywords from the resume.\")\n",
      "\n",
      "annotated_text(create_annotated_text(\n",
      "    selected_file[\"clean_data\"], selected_file[\"extracted_keywords\"],\n",
      "    \"KW\", \"#0B666A\"))\n",
      "\n",
      "avs.add_vertical_space(5)\n",
      "st.write(\"Now let's take a look at the extracted entities from the resume.\")\n",
      "\n",
      "# Call the function with your data\n",
      "create_star_graph(selected_file['keyterms'], \"Entities from Resume\")\n",
      "\n",
      "df2 = pd.DataFrame(selected_file['keyterms'], columns=[\"keyword\", \"value\"])\n",
      "\n",
      "# Create the dictionary\n",
      "keyword_dict = {}\n",
      "for keyword, value in selected_file['keyterms']:\n",
      "    keyword_dict[keyword] = value * 100\n",
      "\n",
      "fig = go.Figure(data=[go.Table(header=dict(values=[\"Keyword\", \"Value\"],\n",
      "                                           font=dict(size=12),\n",
      "                                           fill_color='#070A52'),\n",
      "                               cells=dict(values=[list(keyword_dict.keys()),\n",
      "                                                  list(keyword_dict.values())],\n",
      "                                          line_color='darkslategray',\n",
      "                                          fill_color='#6DA9E4'))\n",
      "                      ])\n",
      "st.plotly_chart(fig)\n",
      "\n",
      "st.divider()\n",
      "\n",
      "fig = px.treemap(df2, path=['keyword'], values='value',\n",
      "                 color_continuous_scale='Rainbow',\n",
      "                 title='Key Terms/Topics Extracted from your Resume')\n",
      "st.write(fig)\n",
      "\n",
      "avs.add_vertical_space(5)\n",
      "\n",
      "job_descriptions = get_filenames_from_dir(\"Data/Processed/JobDescription\")\n",
      "\n",
      "output = 0\n",
      "if len(job_descriptions) > 1:\n",
      "    st.write(\"There are\", len(job_descriptions),\n",
      "             \" job descriptions present. Please select one from the menu below:\")\n",
      "    output = st.slider('Select Job Description Number',\n",
      "                       0, len(job_descriptions) - 1, 0)\n",
      "else:\n",
      "    st.write(\"There is 1 job description present\")\n",
      "\n",
      "avs.add_vertical_space(5)\n",
      "\n",
      "st.write(\"You have selected \",\n",
      "         job_descriptions[output], \" printing the job description\")\n",
      "selected_jd = read_json(\n",
      "    \"Data/Processed/JobDescription/\" + job_descriptions[output])\n",
      "\n",
      "avs.add_vertical_space(2)\n",
      "st.markdown(\"#### Job Description\")\n",
      "st.caption(\n",
      "    \"Currently in the pipeline I'm parsing this from PDF but it'll be from txt or copy paste.\")\n",
      "avs.add_vertical_space(3)\n",
      "# st.json(selected_file)\n",
      "st.write(selected_jd[\"clean_data\"])\n",
      "\n",
      "st.markdown(\"#### Common Words between Job Description and Resumes Highlighted.\")\n",
      "\n",
      "annotated_text(create_annotated_text(\n",
      "    selected_file[\"clean_data\"], selected_jd[\"extracted_keywords\"],\n",
      "    \"JD\", \"#F24C3D\"))\n",
      "\n",
      "st.write(\"Now let's take a look at the extracted entities from the job description.\")\n",
      "\n",
      "# Call the function with your data\n",
      "create_star_graph(selected_jd['keyterms'], \"Entities from Job Description\")\n",
      "\n",
      "df2 = pd.DataFrame(selected_jd['keyterms'], columns=[\"keyword\", \"value\"])\n",
      "\n",
      "# Create the dictionary\n",
      "keyword_dict = {}\n",
      "for keyword, value in selected_jd['keyterms']:\n",
      "    keyword_dict[keyword] = value * 100\n",
      "\n",
      "fig = go.Figure(data=[go.Table(header=dict(values=[\"Keyword\", \"Value\"],\n",
      "                                           font=dict(size=12),\n",
      "                                           fill_color='#070A52'),\n",
      "                               cells=dict(values=[list(keyword_dict.keys()),\n",
      "                                                  list(keyword_dict.values())],\n",
      "                                          line_color='darkslategray',\n",
      "                                          fill_color='#6DA9E4'))\n",
      "                      ])\n",
      "st.plotly_chart(fig)\n",
      "\n",
      "st.divider()\n",
      "\n",
      "fig = px.treemap(df2, path=['keyword'], values='value',\n",
      "                 color_continuous_scale='Rainbow',\n",
      "                 title='Key Terms/Topics Extracted from the selected Job Description')\n",
      "st.write(fig)\n",
      "\n",
      "avs.add_vertical_space(3)\n",
      "\n",
      "config_file_path = config_path + \"/config.yml\"\n",
      "if os.path.exists(config_file_path):\n",
      "    config_data = read_config(config_file_path)\n",
      "    if config_data:\n",
      "        print(\"Config file parsed successfully:\")\n",
      "        resume_string = ' '.join(selected_file[\"extracted_keywords\"])\n",
      "        jd_string = ' '.join(selected_jd[\"extracted_keywords\"])\n",
      "        result = get_similarity_score(resume_string, jd_string)\n",
      "        similarity_score = result[0][\"score\"]\n",
      "        st.write(\"Similarity Score obtained for the resume and job description is:\", similarity_score)\n",
      "else:\n",
      "    print(\"Config file does not exist.\")\n",
      "\n",
      "\n",
      "st.title(':blue[Resume Matcher]')\n",
      "st.subheader(\n",
      "    'Free and Open Source ATS to help your resume pass the screening stage.')\n",
      "st.markdown(\n",
      "    ' Give Resume Matcher a Star on [GitHub](https://github.com/srbhr/Resume-Matcher/)')\n",
      "badge(type=\"github\", name=\"srbhr/Resume-Matcher\")\n",
      "\n",
      "st.text('For updates follow me on Twitter.')\n",
      "badge(type=\"twitter\", name=\"_srbhr_\")\n",
      "\n",
      "st.markdown(\n",
      "    'If you like the project and would like to further help in development please consider ')\n",
      "badge(type=\"buymeacoffee\", name=\"srbhr\")\n",
      "\n",
      "from bs4 import BeautifulSoup\n",
      "import requests\n",
      "import easygui\n",
      "from reportlab.pdfgen import canvas\n",
      "from reportlab.lib.pagesizes import letter\n",
      "from os import listdir\n",
      "from os.path import isfile, join\n",
      "import logging\n",
      "\n",
      "\n",
      "'''\n",
      "This script takes a LinkedIn job posting URL\n",
      "and converts the description to a PDF file.\n",
      "The PDF file is saved in the Data/JobDescription folder.\n",
      "The name will be outputX.pdf, where X is the number of files in the folder.\n",
      "\n",
      "IMPORTANT: Make sure the URL is to the actuall job description,\n",
      "and not the job search page.\n",
      "'''\n",
      "\n",
      "\n",
      "def split_string(s: str, max_len: int = 82) -> list[str]:\n",
      "    words = s.split()\n",
      "    lines = []\n",
      "    current_line = \"\"\n",
      "\n",
      "    for word in words:\n",
      "        if len(current_line) + len(word) + 1 > max_len:\n",
      "            lines.append(current_line.strip())\n",
      "            current_line = \"\"\n",
      "        current_line += word + \" \"\n",
      "\n",
      "    if current_line:\n",
      "        lines.append(current_line.strip())\n",
      "\n",
      "    return lines\n",
      "\n",
      "\n",
      "def linkedin_to_pdf():\n",
      "    url = easygui.enterbox(\"Enter the URL of the LinkedIn Job Posting:\")\n",
      "    try:\n",
      "        page = requests.get(url)\n",
      "        content = page.text\n",
      "\n",
      "        soup = BeautifulSoup(content, \"lxml\")\n",
      "\n",
      "        description = (\n",
      "            soup.find(\"div\", class_=\"show-more-less-html__markup\")\n",
      "            .get_text(strip=True, separator=\"\\n\")\n",
      "            .split(\"Primary Location\")[0]\n",
      "            .strip()\n",
      "        )\n",
      "        logging.info(\"Description: \\n\" + description)\n",
      "\n",
      "        return save_to_pdf(description)\n",
      "    except Exception as e:\n",
      "        logging.error(f\"Could not get the description from the URL:\\n{url}\")\n",
      "        logging.error(e)\n",
      "        exit()\n",
      "\n",
      "\n",
      "def save_to_pdf(description: str):\n",
      "    job_path = \"Data/JobDescription/\"\n",
      "    description = description.split(\"\\n\")\n",
      "    files_number = len([f for f in listdir(job_path) if isfile(join(job_path, f))])\n",
      "    file_name = f\"output{files_number}.pdf\"\n",
      "\n",
      "    c = canvas.Canvas(job_path+file_name, pagesize=letter)\n",
      "\n",
      "    y = 780\n",
      "    for value in description:\n",
      "        value = split_string(value)\n",
      "        for v in value:\n",
      "            if y < 20:\n",
      "                c.showPage()\n",
      "                y = 780\n",
      "            c.drawString(72, y, v)\n",
      "            y -= 20\n",
      "\n",
      "    c.save()\n",
      "    logging.info(\"PDF saved to Data/JobDescription/\"+file_name)\n",
      "\n",
      "    return file_name[:-4]\n",
      "\n",
      "\n",
      "linkedin_to_pdf()\n",
      "\n",
      "File LOC changed from 265 to 265\n",
      "File LOC changed from 261 to 261\n",
      "File LOC changed from 283 to 283\n",
      "File LOC changed from 247 to 247\n",
      "File LOC changed from 324 to 324\n",
      "File LOC changed from 260 to 260\n",
      "File LOC changed from 345 to 345\n",
      "File LOC changed from 304 to 304\n",
      "File LOC changed from 289 to 289\n",
      "import json\n",
      "import logging\n",
      "import os\n",
      "\n",
      "import cohere\n",
      "import yaml\n",
      "from qdrant_client import QdrantClient, models\n",
      "from qdrant_client.http.models import Batch\n",
      "\n",
      "logging.basicConfig(\n",
      "    filename='app_similarity_score.log',\n",
      "    filemode='w',\n",
      "    level=logging.INFO,\n",
      "    format=\"%(asctime)s - %(name)s - %(levelname)s - %(message)s\"\n",
      ")\n",
      "\n",
      "logger = logging.getLogger(__name__)\n",
      "logger.setLevel(logging.DEBUG)\n",
      "\n",
      "console_handler = logging.StreamHandler()\n",
      "formatter = logging.Formatter(\"%(asctime)s - %(name)s - %(levelname)s - %(message)s\")\n",
      "console_handler.setFormatter(formatter)\n",
      "console_handler.setLevel(logging.DEBUG)\n",
      "\n",
      "file_handler = logging.FileHandler(\"app_similarity_score.log\")\n",
      "file_handler.setLevel(logging.DEBUG)\n",
      "file_handler.setFormatter(formatter)\n",
      "\n",
      "logger.addHandler(file_handler)\n",
      "logger.addHandler(console_handler)\n",
      "\n",
      "\n",
      "def find_path(folder_name):\n",
      "    curr_dir = os.getcwd()\n",
      "    while True:\n",
      "        if folder_name in os.listdir(curr_dir):\n",
      "            return os.path.join(curr_dir, folder_name)\n",
      "        else:\n",
      "            parent_dir = os.path.dirname(curr_dir)\n",
      "            if parent_dir == '/':\n",
      "                break\n",
      "            curr_dir = parent_dir\n",
      "    raise ValueError(f\"Folder '{folder_name}' not found.\")\n",
      "\n",
      "\n",
      "cwd = find_path('Resume-Matcher')\n",
      "READ_RESUME_FROM = os.path.join(cwd, 'Data', 'Processed', 'Resumes')\n",
      "READ_JOB_DESCRIPTION_FROM = os.path.join(cwd, 'Data', 'Processed', 'JobDescription')\n",
      "config_path = os.path.join(cwd, \"scripts\", \"similarity\")\n",
      "\n",
      "\n",
      "def read_config(filepath):\n",
      "    try:\n",
      "        with open(filepath) as f:\n",
      "            config = yaml.safe_load(f)\n",
      "        return config\n",
      "    except FileNotFoundError as e:\n",
      "        logger.error(f\"Configuration file {filepath} not found: {e}\")\n",
      "    except yaml.YAMLError as e:\n",
      "        logger.error(f\"Error parsing YAML in configuration file {filepath}: {e}\", exc_info=True)\n",
      "    except Exception as e:\n",
      "        logger.error(f\"Error reading configuration file {filepath}: {e}\")\n",
      "    return None\n",
      "\n",
      "\n",
      "def read_doc(path):\n",
      "    with open(path) as f:\n",
      "        try:\n",
      "            data = json.load(f)\n",
      "        except Exception as e:\n",
      "            logger.error(f'Error reading JSON file: {e}')\n",
      "            data = {}\n",
      "    return data\n",
      "\n",
      "\n",
      "class QdrantSearch:\n",
      "    def __init__(self, resumes, jd):\n",
      "        config = read_config(config_path + \"/config.yml\")\n",
      "        self.cohere_key = config['cohere']['api_key']\n",
      "        self.qdrant_key = config['qdrant']['api_key']\n",
      "        self.qdrant_url = config['qdrant']['url']\n",
      "        self.resumes = resumes\n",
      "        self.jd = jd\n",
      "        self.cohere = cohere.Client(self.cohere_key)\n",
      "        self.collection_name = \"resume_collection_name\"\n",
      "        self.qdrant = QdrantClient(\n",
      "            url=self.qdrant_url,\n",
      "            api_key=self.qdrant_key,\n",
      "        )\n",
      "\n",
      "        vector_size = 4096\n",
      "        print(f\"collection name={self.collection_name}\")\n",
      "        self.qdrant.recreate_collection(\n",
      "            collection_name=self.collection_name,\n",
      "            vectors_config=models.VectorParams(\n",
      "                size=vector_size,\n",
      "                distance=models.Distance.COSINE\n",
      "            )\n",
      "        )\n",
      "\n",
      "        self.logger = logging.getLogger(self.__class__.__name__)\n",
      "\n",
      "        self.logger.addHandler(console_handler)\n",
      "        self.logger.addHandler(file_handler)\n",
      "\n",
      "    def get_embedding(self, text):\n",
      "        try:\n",
      "            embeddings = self.cohere.embed([text], \"large\").embeddings\n",
      "            return list(map(float, embeddings[0])), len(embeddings[0])\n",
      "        except Exception as e:\n",
      "            self.logger.error(f\"Error getting embeddings: {e}\", exc_info=True)\n",
      "\n",
      "    def update_qdrant(self):\n",
      "        vectors = []\n",
      "        ids = []\n",
      "        for i, resume in enumerate(self.resumes):\n",
      "            vector, size = self.get_embedding(resume)\n",
      "            vectors.append(vector)\n",
      "            ids.append(i)\n",
      "        try:\n",
      "            self.qdrant.upsert(\n",
      "                collection_name=self.collection_name,\n",
      "                points=Batch(\n",
      "                    ids=ids,\n",
      "                    vectors=vectors,\n",
      "                    payloads=[{\"text\": resume} for resume in self.resumes]\n",
      "\n",
      "                )\n",
      "            )\n",
      "        except Exception as e:\n",
      "            self.logger.error(f\"Error upserting the vectors to the qdrant collection: {e}\", exc_info=True)\n",
      "\n",
      "    def search(self):\n",
      "        vector, _ = self.get_embedding(self.jd)\n",
      "\n",
      "        hits = self.qdrant.search(\n",
      "            collection_name=self.collection_name,\n",
      "            query_vector=vector,\n",
      "            limit=30\n",
      "        )\n",
      "        results = []\n",
      "        for hit in hits:\n",
      "            result = {\n",
      "                'text': str(hit.payload)[:30],\n",
      "                'score': hit.score\n",
      "            }\n",
      "            results.append(result)\n",
      "\n",
      "        return results\n",
      "\n",
      "\n",
      "def get_similarity_score(resume_string, job_description_string):\n",
      "    logger.info(\"Started getting similarity score\")\n",
      "    qdrant_search = QdrantSearch([resume_string], job_description_string)\n",
      "    qdrant_search.update_qdrant()\n",
      "    search_result = qdrant_search.search()\n",
      "    logger.info(\"Finished getting similarity score\")\n",
      "    return search_result\n",
      "\n",
      "\n",
      "if __name__ == \"__main__\":\n",
      "    # To give your custom resume use this code\n",
      "    resume_dict = read_config(\n",
      "        READ_RESUME_FROM + \"/Resume-bruce_wayne_fullstack.pdf4783d115-e6fc-462e-ae4d-479152884b28.json\")\n",
      "    job_dict = read_config(\n",
      "        READ_JOB_DESCRIPTION_FROM + \"/JobDescription-job_desc_full_stack_engineer_pdf4de00846-a4fe-4fe5-a4d7\"\n",
      "                                    \"-2a8a1b9ad020.json\")\n",
      "    resume_keywords = resume_dict[\"extracted_keywords\"]\n",
      "    job_description_keywords = job_dict[\"extracted_keywords\"]\n",
      "\n",
      "    resume_string = ' '.join(resume_keywords)\n",
      "    jd_string = ' '.join(job_description_keywords)\n",
      "    final_result = get_similarity_score(resume_string, jd_string)\n",
      "    for r in final_result:\n",
      "        print(r)\n",
      "\n",
      "File __init__.py has no source code\n",
      "\n",
      "import json\n",
      "from scripts import ResumeProcessor, JobDescriptionProcessor\n",
      "from scripts.utils import init_logging_config, get_filenames_from_dir\n",
      "import logging\n",
      "import os\n",
      "\n",
      "init_logging_config()\n",
      "\n",
      "PROCESSED_RESUMES_PATH = \"Data/Processed/Resumes\"\n",
      "PROCESSED_JOB_DESCRIPTIONS_PATH = \"Data/Processed/JobDescription\"\n",
      "\n",
      "def read_json(filename):\n",
      "    with open(filename) as f:\n",
      "        data = json.load(f)\n",
      "    return data\n",
      "\n",
      "def remove_old_files(files_path):\n",
      "\n",
      "    for filename in os.listdir(files_path):\n",
      "        try:\n",
      "            file_path = os.path.join(files_path, filename)\n",
      "\n",
      "            if os.path.isfile(file_path):\n",
      "                os.remove(file_path)\n",
      "        except Exception as e:  \n",
      "            logging.error(f\"Error deleting {file_path}:\\n{e}\")\n",
      "\n",
      "    logging.info(\"Deleted old files from \"+files_path)\n",
      "\n",
      "\n",
      "logging.info('Started to read from Data/Resumes')\n",
      "try:\n",
      "    # Check if there are resumes present or not.\n",
      "    # If present then parse it.\n",
      "    remove_old_files(PROCESSED_RESUMES_PATH)\n",
      "\n",
      "    file_names = get_filenames_from_dir(\"Data/Resumes\")\n",
      "    logging.info('Reading from Data/Resumes is now complete.')\n",
      "except:\n",
      "    # Exit the program if there are no resumes.\n",
      "    logging.error('There are no resumes present in the specified folder.')\n",
      "    logging.error('Exiting from the program.')\n",
      "    logging.error(\n",
      "        'Please add resumes in the Data/Resumes folder and try again.')\n",
      "    exit(1)\n",
      "\n",
      "# Now after getting the file_names parse the resumes into a JSON Format.\n",
      "logging.info('Started parsing the resumes.')\n",
      "for file in file_names:\n",
      "    processor = ResumeProcessor(file)\n",
      "    success = processor.process()\n",
      "logging.info('Parsing of the resumes is now complete.')\n",
      "\n",
      "logging.info('Started to read from Data/JobDescription')\n",
      "try:\n",
      "    # Check if there are resumes present or not.\n",
      "    # If present then parse it.\n",
      "    remove_old_files(PROCESSED_JOB_DESCRIPTIONS_PATH)\n",
      "\n",
      "    file_names = get_filenames_from_dir(\"Data/JobDescription\")\n",
      "    logging.info('Reading from Data/JobDescription is now complete.')\n",
      "except:\n",
      "    # Exit the program if there are no resumes.\n",
      "    logging.error(\n",
      "        'There are no job-description present in the specified folder.')\n",
      "    logging.error('Exiting from the program.')\n",
      "    logging.error(\n",
      "        'Please add resumes in the Data/JobDescription folder and try again.')\n",
      "    exit(1)\n",
      "\n",
      "# Now after getting the file_names parse the resumes into a JSON Format.\n",
      "logging.info('Started parsing the Job Descriptions.')\n",
      "for file in file_names:\n",
      "    processor = JobDescriptionProcessor(file)\n",
      "    success = processor.process()\n",
      "logging.info('Parsing of the Job Descriptions is now complete.')\n",
      "logging.info('Success now run `streamlit run streamlit_second.py`')\n",
      "\n",
      "import re\n",
      "import urllib.request\n",
      "import spacy\n",
      "from .utils import TextCleaner\n",
      "\n",
      "\n",
      "# Load the English model\n",
      "nlp = spacy.load('en_core_web_sm')\n",
      "\n",
      "\n",
      "RESUME_SECTIONS = [\n",
      "    'Contact Information',\n",
      "    'Objective',\n",
      "    'Summary',\n",
      "    'Education',\n",
      "    'Experience',\n",
      "    'Skills',\n",
      "    'Projects',\n",
      "    'Certifications',\n",
      "    'Licenses',\n",
      "    'Awards',\n",
      "    'Honors',\n",
      "    'Publications',\n",
      "    'References',\n",
      "    'Technical Skills',\n",
      "    'Computer Skills',\n",
      "    'Programming Languages',\n",
      "    'Software Skills',\n",
      "    'Soft Skills',\n",
      "    'Language Skills',\n",
      "    'Professional Skills',\n",
      "    'Transferable Skills',\n",
      "    'Work Experience',\n",
      "    'Professional Experience',\n",
      "    'Employment History',\n",
      "    'Internship Experience',\n",
      "    'Volunteer Experience',\n",
      "    'Leadership Experience',\n",
      "    'Research Experience',\n",
      "    'Teaching Experience'\n",
      "]\n",
      "\n",
      "\n",
      "class DataExtractor:\n",
      "    \"\"\"\n",
      "    A class for extracting various types of data from text.\n",
      "    \"\"\"\n",
      "\n",
      "    def __init__(self, raw_text: str):\n",
      "        \"\"\"\n",
      "        Initialize the DataExtractor object.\n",
      "\n",
      "        Args:\n",
      "            raw_text (str): The raw input text.\n",
      "        \"\"\"\n",
      "\n",
      "        self.text = raw_text\n",
      "        self.clean_text = TextCleaner.clean_text(self.text)\n",
      "        self.doc = nlp(self.clean_text)\n",
      "\n",
      "    def extract_links(self):\n",
      "        \"\"\"\n",
      "        Find links of any type in a given string.\n",
      "\n",
      "        Args:\n",
      "            text (str): The string to search for links.\n",
      "\n",
      "        Returns:\n",
      "            list: A list containing all the found links.\n",
      "        \"\"\"\n",
      "        link_pattern = r'\\b(?:https?://|www\\.)\\S+\\b'\n",
      "        links = re.findall(link_pattern, self.text)\n",
      "        return links\n",
      "\n",
      "    def extract_links_extended(self):\n",
      "        \"\"\"\n",
      "        Extract links of all kinds (HTTP, HTTPS, FTP, email, www.linkedin.com,\n",
      "          and github.com/user_name) from a webpage.\n",
      "\n",
      "        Args:\n",
      "            url (str): The URL of the webpage.\n",
      "\n",
      "        Returns:\n",
      "            list: A list containing all the extracted links.\n",
      "        \"\"\"\n",
      "        links = []\n",
      "        try:\n",
      "            response = urllib.request.urlopen(self.text)\n",
      "            html_content = response.read().decode('utf-8')\n",
      "            pattern = r'href=[\\'\"]?([^\\'\" >]+)'\n",
      "            raw_links = re.findall(pattern, html_content)\n",
      "            for link in raw_links:\n",
      "                if link.startswith(('http://', 'https://', 'ftp://', 'mailto:',\n",
      "                                    'www.linkedin.com', 'github.com/', 'twitter.com')):\n",
      "                    links.append(link)\n",
      "        except Exception as e:\n",
      "            print(f\"Error extracting links: {str(e)}\")\n",
      "        return links\n",
      "\n",
      "    def extract_names(self):\n",
      "        \"\"\"Extracts and returns a list of names from the given \n",
      "        text using spaCy's named entity recognition.\n",
      "\n",
      "        Args:\n",
      "            text (str): The text to extract names from.\n",
      "\n",
      "        Returns:\n",
      "            list: A list of strings representing the names extracted from the text.\n",
      "        \"\"\"\n",
      "        names = [ent.text for ent in self.doc.ents if ent.label_ == 'PERSON']\n",
      "        return names\n",
      "\n",
      "    def extract_emails(self):\n",
      "        \"\"\"\n",
      "        Extract email addresses from a given string.\n",
      "\n",
      "        Args:\n",
      "            text (str): The string from which to extract email addresses.\n",
      "\n",
      "        Returns:\n",
      "            list: A list containing all the extracted email addresses.\n",
      "        \"\"\"\n",
      "        email_pattern = r'\\b[A-Za-z0-9._%+-]+@[A-Za-z0-9.-]+\\.[A-Za-z]{2,}\\b'\n",
      "        emails = re.findall(email_pattern, self.text)\n",
      "        return emails\n",
      "\n",
      "    def extract_phone_numbers(self):\n",
      "        \"\"\"\n",
      "        Extract phone numbers from a given string.\n",
      "\n",
      "        Args:\n",
      "            text (str): The string from which to extract phone numbers.\n",
      "\n",
      "        Returns:\n",
      "            list: A list containing all the extracted phone numbers.\n",
      "        \"\"\"\n",
      "        phone_number_pattern = r'^(\\+\\d{1,3})?[-.\\s]?\\(?\\d{3}\\)?[-.\\s]?\\d{3}[-.\\s]?\\d{4}$'\n",
      "        phone_numbers = re.findall(phone_number_pattern, self.text)\n",
      "        return phone_numbers\n",
      "\n",
      "    def extract_experience(self):\n",
      "        \"\"\"\n",
      "        Extract experience from a given string. It does so by using the Spacy module.\n",
      "\n",
      "        Args:\n",
      "            text (str): The string from which to extract experience.\n",
      "\n",
      "        Returns:\n",
      "            str: A string containing all the extracted experience.\n",
      "        \"\"\"\n",
      "        experience_section = []\n",
      "        in_experience_section = False\n",
      "\n",
      "        for token in self.doc:\n",
      "            if token.text in RESUME_SECTIONS:\n",
      "                if token.text == 'Experience' or 'EXPERIENCE' or 'experience':\n",
      "                    in_experience_section = True\n",
      "                else:\n",
      "                    in_experience_section = False\n",
      "\n",
      "            if in_experience_section:\n",
      "                experience_section.append(token.text)\n",
      "\n",
      "        return ' '.join(experience_section)\n",
      "\n",
      "    def extract_position_year(self):\n",
      "        \"\"\"\n",
      "            Extract position and year from a given string.\n",
      "\n",
      "            Args:\n",
      "                text (str): The string from which to extract position and year.\n",
      "\n",
      "            Returns:\n",
      "                list: A list containing the extracted position and year.\n",
      "        \"\"\"\n",
      "        position_year_search_pattern = r\"(\\b\\w+\\b\\s+\\b\\w+\\b),\\s+(\\d{4})\\s*-\\s*(\\d{4}|\\bpresent\\b)\"\n",
      "        position_year = re.findall(\n",
      "            position_year_search_pattern, self.text)\n",
      "        return position_year\n",
      "\n",
      "    def extract_particular_words(self):\n",
      "        \"\"\"\n",
      "        Extract nouns and proper nouns from the given text.\n",
      "\n",
      "        Args:\n",
      "            text (str): The input text to extract nouns from.\n",
      "\n",
      "        Returns:\n",
      "            list: A list of extracted nouns.\n",
      "        \"\"\"\n",
      "        pos_tags = ['NOUN', 'PROPN']\n",
      "        nouns = [token.text for token in self.doc if token.pos_ in pos_tags]\n",
      "        return nouns\n",
      "\n",
      "    def extract_entities(self):\n",
      "        \"\"\"\n",
      "        Extract named entities of types 'GPE' (geopolitical entity) and 'ORG' (organization) from the given text.\n",
      "\n",
      "        Args:\n",
      "            text (str): The input text to extract entities from.\n",
      "\n",
      "        Returns:\n",
      "            list: A list of extracted entities.\n",
      "        \"\"\"\n",
      "        entity_labels = ['GPE', 'ORG']\n",
      "        entities = [\n",
      "            token.text for token in self.doc.ents if token.label_ in entity_labels]\n",
      "        return list(set(entities))\n",
      "\n",
      "from .parsers import ParseResume\n",
      "from .parsers import ParseJobDesc\n",
      "from .ReadPdf import read_single_pdf\n",
      "import os.path\n",
      "import pathlib\n",
      "import json\n",
      "\n",
      "READ_JOB_DESCRIPTION_FROM = 'Data/JobDescription/'\n",
      "SAVE_DIRECTORY = 'Data/Processed/JobDescription'\n",
      "\n",
      "\n",
      "class JobDescriptionProcessor:\n",
      "    def __init__(self, input_file):\n",
      "        self.input_file = input_file\n",
      "        self.input_file_name = os.path.join(\n",
      "            READ_JOB_DESCRIPTION_FROM + self.input_file)\n",
      "\n",
      "    def process(self) -> bool:\n",
      "        try:\n",
      "            resume_dict = self._read_resumes()\n",
      "            self._write_json_file(resume_dict)\n",
      "            return True\n",
      "        except Exception as e:\n",
      "            print(f\"An error occurred: {str(e)}\")\n",
      "            return False\n",
      "\n",
      "    def _read_resumes(self) -> dict:\n",
      "        data = read_single_pdf(self.input_file_name)\n",
      "        output = ParseResume(data).get_JSON()\n",
      "        return output\n",
      "\n",
      "    def _read_job_desc(self) -> dict:\n",
      "        data = read_single_pdf(self.input_file_name)\n",
      "        output = ParseJobDesc(data).get_JSON()\n",
      "        return output\n",
      "\n",
      "    def _write_json_file(self, resume_dictionary: dict):\n",
      "        file_name = str(\"JobDescription-\" + self.input_file +\n",
      "                        resume_dictionary[\"unique_id\"] + \".json\")\n",
      "        save_directory_name = pathlib.Path(SAVE_DIRECTORY) / file_name\n",
      "        json_object = json.dumps(resume_dictionary, sort_keys=True, indent=14)\n",
      "        with open(save_directory_name, \"w+\") as outfile:\n",
      "            outfile.write(json_object)\n",
      "\n",
      "from .parsers import ParseResume\n",
      "from .parsers import ParseJobDesc\n",
      "from .ReadPdf import read_single_pdf\n",
      "import os.path\n",
      "import pathlib\n",
      "import json\n",
      "\n",
      "READ_RESUME_FROM = 'Data/Resumes/'\n",
      "SAVE_DIRECTORY = 'Data/Processed/Resumes'\n",
      "\n",
      "\n",
      "class ResumeProcessor:\n",
      "    def __init__(self, input_file):\n",
      "        self.input_file = input_file\n",
      "        self.input_file_name = os.path.join(READ_RESUME_FROM + self.input_file)\n",
      "\n",
      "    def process(self) -> bool:\n",
      "        try:\n",
      "            resume_dict = self._read_resumes()\n",
      "            self._write_json_file(resume_dict)\n",
      "            return True\n",
      "        except Exception as e:\n",
      "            print(f\"An error occurred: {str(e)}\")\n",
      "            return False\n",
      "\n",
      "    def _read_resumes(self) -> dict:\n",
      "        data = read_single_pdf(self.input_file_name)\n",
      "        output = ParseResume(data).get_JSON()\n",
      "        return output\n",
      "\n",
      "    def _read_job_desc(self) -> dict:\n",
      "        data = read_single_pdf(self.input_file_name)\n",
      "        output = ParseJobDesc(data).get_JSON()\n",
      "        return output\n",
      "\n",
      "    def _write_json_file(self, resume_dictionary: dict):\n",
      "        file_name = str(\"Resume-\" + self.input_file +\n",
      "                        resume_dictionary[\"unique_id\"] + \".json\")\n",
      "        save_directory_name = pathlib.Path(SAVE_DIRECTORY) / file_name\n",
      "        json_object = json.dumps(resume_dictionary, sort_keys=True, indent=14)\n",
      "        with open(save_directory_name, \"w+\") as outfile:\n",
      "            outfile.write(json_object)\n",
      "\n",
      "from . import ReadPdf\n",
      "from .JobDescriptionProcessor import JobDescriptionProcessor\n",
      "from .ResumeProcessor import ResumeProcessor\n",
      "from .ParseJobDescToJson import ParseJobDesc\n",
      "from .ParseResumeToJson import ParseResume\n",
      "from .get_similarity_score import get_similarity_score, find_path, read_config\n",
      "from .logger import init_logging_config\n",
      "from .Utils import TextCleaner\n",
      "from .ReadFiles import get_filenames_from_dir\n",
      "import json\n",
      "import os\n",
      "from typing import List\n",
      "\n",
      "import networkx as nx\n",
      "import nltk\n",
      "import pandas as pd\n",
      "import plotly.express as px\n",
      "import plotly.graph_objects as go\n",
      "import streamlit as st\n",
      "from annotated_text import annotated_text, parameters\n",
      "from streamlit_extras import add_vertical_space as avs\n",
      "from streamlit_extras.badges import badge\n",
      "\n",
      "from scripts.similarity import get_similarity_score, find_path, read_config\n",
      "from scripts.utils import get_filenames_from_dir\n",
      "\n",
      "cwd = find_path('Resume-Matcher')\n",
      "config_path = os.path.join(cwd, \"scripts\", \"similarity\")\n",
      "\n",
      "try:\n",
      "    nltk.data.find('tokenizers/punkt')\n",
      "except LookupError:\n",
      "    nltk.download('punkt')\n",
      "\n",
      "parameters.SHOW_LABEL_SEPARATOR = False\n",
      "parameters.BORDER_RADIUS = 3\n",
      "parameters.PADDING = \"0.5 0.25rem\"\n",
      "\n",
      "\n",
      "def create_star_graph(nodes_and_weights, title):\n",
      "    # Create an empty graph\n",
      "    G = nx.Graph()\n",
      "\n",
      "    # Add the central node\n",
      "    central_node = \"resume\"\n",
      "    G.add_node(central_node)\n",
      "\n",
      "    # Add nodes and edges with weights to the graph\n",
      "    for node, weight in nodes_and_weights:\n",
      "        G.add_node(node)\n",
      "        G.add_edge(central_node, node, weight=weight * 100)\n",
      "\n",
      "    # Get position layout for nodes\n",
      "    pos = nx.spring_layout(G)\n",
      "\n",
      "    # Create edge trace\n",
      "    edge_x = []\n",
      "    edge_y = []\n",
      "    for edge in G.edges():\n",
      "        x0, y0 = pos[edge[0]]\n",
      "        x1, y1 = pos[edge[1]]\n",
      "        edge_x.extend([x0, x1, None])\n",
      "        edge_y.extend([y0, y1, None])\n",
      "\n",
      "    edge_trace = go.Scatter(x=edge_x, y=edge_y, line=dict(\n",
      "        width=0.5, color='#888'), hoverinfo='none', mode='lines')\n",
      "\n",
      "    # Create node trace\n",
      "    node_x = []\n",
      "    node_y = []\n",
      "    for node in G.nodes():\n",
      "        x, y = pos[node]\n",
      "        node_x.append(x)\n",
      "        node_y.append(y)\n",
      "\n",
      "    node_trace = go.Scatter(x=node_x, y=node_y, mode='markers', hoverinfo='text',\n",
      "                            marker=dict(showscale=True, colorscale='Rainbow', reversescale=True, color=[], size=10,\n",
      "                                        colorbar=dict(thickness=15, title='Node Connections', xanchor='left',\n",
      "                                                      titleside='right'), line_width=2))\n",
      "\n",
      "    # Color node points by number of connections\n",
      "    node_adjacencies = []\n",
      "    node_text = []\n",
      "    for node in G.nodes():\n",
      "        adjacencies = list(G.adj[node])  # changes here\n",
      "        node_adjacencies.append(len(adjacencies))\n",
      "        node_text.append(f'{node}<br># of connections: {len(adjacencies)}')\n",
      "\n",
      "    node_trace.marker.color = node_adjacencies\n",
      "    node_trace.text = node_text\n",
      "\n",
      "    # Create the figure\n",
      "    fig = go.Figure(data=[edge_trace, node_trace],\n",
      "                    layout=go.Layout(title=title, titlefont_size=16, showlegend=False,\n",
      "                                     hovermode='closest', margin=dict(b=20, l=5, r=5, t=40),\n",
      "                                     xaxis=dict(\n",
      "                                         showgrid=False, zeroline=False, showticklabels=False),\n",
      "                                     yaxis=dict(showgrid=False, zeroline=False, showticklabels=False)))\n",
      "\n",
      "    # Show the figure\n",
      "    st.plotly_chart(fig)\n",
      "\n",
      "\n",
      "def create_annotated_text(input_string: str, word_list: List[str], annotation: str, color_code: str):\n",
      "    # Tokenize the input string\n",
      "    tokens = nltk.word_tokenize(input_string)\n",
      "\n",
      "    # Convert the list to a set for quick lookups\n",
      "    word_set = set(word_list)\n",
      "\n",
      "    # Initialize an empty list to hold the annotated text\n",
      "    annotated_text = []\n",
      "\n",
      "    for token in tokens:\n",
      "        # Check if the token is in the set\n",
      "        if token in word_set:\n",
      "            # If it is, append a tuple with the token, annotation, and color code\n",
      "            annotated_text.append((token, annotation, color_code))\n",
      "        else:\n",
      "            # If it's not, just append the token as a string\n",
      "            annotated_text.append(token)\n",
      "\n",
      "    return annotated_text\n",
      "\n",
      "\n",
      "def read_json(filename):\n",
      "    with open(filename) as f:\n",
      "        data = json.load(f)\n",
      "    return data\n",
      "\n",
      "\n",
      "def tokenize_string(input_string):\n",
      "    tokens = nltk.word_tokenize(input_string)\n",
      "    return tokens\n",
      "\n",
      "\n",
      "st.image('Assets/img/header_image.jpg')\n",
      "\n",
      "st.title(':blue[Resume Matcher]')\n",
      "st.subheader(\n",
      "    'Free and Open Source ATS to help your resume pass the screening stage.')\n",
      "st.markdown(\n",
      "    \"Check the website [www.resumematcher.fyi](https://www.resumematcher.fyi/)\")\n",
      "st.markdown(\n",
      "    ' Give Resume Matcher a Star on [GitHub](https://github.com/srbhr/resume-matcher)')\n",
      "badge(type=\"github\", name=\"srbhr/Resume-Matcher\")\n",
      "\n",
      "st.text('For updates follow me on Twitter.')\n",
      "badge(type=\"twitter\", name=\"_srbhr_\")\n",
      "\n",
      "st.markdown(\n",
      "    'If you like the project and would like to further help in development please consider ')\n",
      "badge(type=\"buymeacoffee\", name=\"srbhr\")\n",
      "\n",
      "avs.add_vertical_space(5)\n",
      "\n",
      "resume_names = get_filenames_from_dir(\"Data/Processed/Resumes\")\n",
      "\n",
      "output = 0\n",
      "\n",
      "if len(resume_names) > 1:\n",
      "    st.write(\"There are\", len(resume_names),\n",
      "             \" resumes present. Please select one from the menu below:\")\n",
      "    output = st.slider('Select Resume Number', 0, len(resume_names) - 1, 0)\n",
      "else:\n",
      "    st.write(\"There is 1 resume present\")\n",
      "\n",
      "avs.add_vertical_space(5)\n",
      "\n",
      "st.write(\"You have selected \", resume_names[output], \" printing the resume\")\n",
      "selected_file = read_json(\"Data/Processed/Resumes/\" + resume_names[output])\n",
      "\n",
      "avs.add_vertical_space(2)\n",
      "st.markdown(\"#### Parsed Resume Data\")\n",
      "st.caption(\n",
      "    \"This text is parsed from your resume. This is how it'll look like after getting parsed by an ATS.\")\n",
      "st.caption(\"Utilize this to understand how to make your resume ATS friendly.\")\n",
      "avs.add_vertical_space(3)\n",
      "# st.json(selected_file)\n",
      "st.write(selected_file[\"clean_data\"])\n",
      "\n",
      "avs.add_vertical_space(3)\n",
      "st.write(\"Now let's take a look at the extracted keywords from the resume.\")\n",
      "\n",
      "annotated_text(create_annotated_text(\n",
      "    selected_file[\"clean_data\"], selected_file[\"extracted_keywords\"],\n",
      "    \"KW\", \"#0B666A\"))\n",
      "\n",
      "avs.add_vertical_space(5)\n",
      "st.write(\"Now let's take a look at the extracted entities from the resume.\")\n",
      "\n",
      "# Call the function with your data\n",
      "create_star_graph(selected_file['keyterms'], \"Entities from Resume\")\n",
      "\n",
      "df2 = pd.DataFrame(selected_file['keyterms'], columns=[\"keyword\", \"value\"])\n",
      "\n",
      "# Create the dictionary\n",
      "keyword_dict = {}\n",
      "for keyword, value in selected_file['keyterms']:\n",
      "    keyword_dict[keyword] = value * 100\n",
      "\n",
      "fig = go.Figure(data=[go.Table(header=dict(values=[\"Keyword\", \"Value\"],\n",
      "                                           font=dict(size=12),\n",
      "                                           fill_color='#070A52'),\n",
      "                               cells=dict(values=[list(keyword_dict.keys()),\n",
      "                                                  list(keyword_dict.values())],\n",
      "                                          line_color='darkslategray',\n",
      "                                          fill_color='#6DA9E4'))\n",
      "                      ])\n",
      "st.plotly_chart(fig)\n",
      "\n",
      "st.divider()\n",
      "\n",
      "fig = px.treemap(df2, path=['keyword'], values='value',\n",
      "                 color_continuous_scale='Rainbow',\n",
      "                 title='Key Terms/Topics Extracted from your Resume')\n",
      "st.write(fig)\n",
      "\n",
      "avs.add_vertical_space(5)\n",
      "\n",
      "job_descriptions = get_filenames_from_dir(\"Data/Processed/JobDescription\")\n",
      "\n",
      "output = 0\n",
      "if len(job_descriptions) > 1:\n",
      "    st.write(\"There are\", len(job_descriptions),\n",
      "             \" job descriptions present. Please select one from the menu below:\")\n",
      "    output = st.slider('Select Job Description Number',\n",
      "                       0, len(job_descriptions) - 1, 0)\n",
      "else:\n",
      "    st.write(\"There is 1 job description present\")\n",
      "\n",
      "avs.add_vertical_space(5)\n",
      "\n",
      "st.write(\"You have selected \",\n",
      "         job_descriptions[output], \" printing the job description\")\n",
      "selected_jd = read_json(\n",
      "    \"Data/Processed/JobDescription/\" + job_descriptions[output])\n",
      "\n",
      "avs.add_vertical_space(2)\n",
      "st.markdown(\"#### Job Description\")\n",
      "st.caption(\n",
      "    \"Currently in the pipeline I'm parsing this from PDF but it'll be from txt or copy paste.\")\n",
      "avs.add_vertical_space(3)\n",
      "# st.json(selected_file)\n",
      "st.write(selected_jd[\"clean_data\"])\n",
      "\n",
      "st.markdown(\"#### Common Words between Job Description and Resumes Highlighted.\")\n",
      "\n",
      "annotated_text(create_annotated_text(\n",
      "    selected_file[\"clean_data\"], selected_jd[\"extracted_keywords\"],\n",
      "    \"JD\", \"#F24C3D\"))\n",
      "\n",
      "st.write(\"Now let's take a look at the extracted entities from the job description.\")\n",
      "\n",
      "# Call the function with your data\n",
      "create_star_graph(selected_jd['keyterms'], \"Entities from Job Description\")\n",
      "\n",
      "df2 = pd.DataFrame(selected_jd['keyterms'], columns=[\"keyword\", \"value\"])\n",
      "\n",
      "# Create the dictionary\n",
      "keyword_dict = {}\n",
      "for keyword, value in selected_jd['keyterms']:\n",
      "    keyword_dict[keyword] = value * 100\n",
      "\n",
      "fig = go.Figure(data=[go.Table(header=dict(values=[\"Keyword\", \"Value\"],\n",
      "                                           font=dict(size=12),\n",
      "                                           fill_color='#070A52'),\n",
      "                               cells=dict(values=[list(keyword_dict.keys()),\n",
      "                                                  list(keyword_dict.values())],\n",
      "                                          line_color='darkslategray',\n",
      "                                          fill_color='#6DA9E4'))\n",
      "                      ])\n",
      "st.plotly_chart(fig)\n",
      "\n",
      "st.divider()\n",
      "\n",
      "fig = px.treemap(df2, path=['keyword'], values='value',\n",
      "                 color_continuous_scale='Rainbow',\n",
      "                 title='Key Terms/Topics Extracted from the selected Job Description')\n",
      "st.write(fig)\n",
      "\n",
      "avs.add_vertical_space(3)\n",
      "\n",
      "config_file_path = config_path + \"/config.yml\"\n",
      "if os.path.exists(config_file_path):\n",
      "    config_data = read_config(config_file_path)\n",
      "    if config_data:\n",
      "        print(\"Config file parsed successfully:\")\n",
      "        resume_string = ' '.join(selected_file[\"extracted_keywords\"])\n",
      "        jd_string = ' '.join(selected_jd[\"extracted_keywords\"])\n",
      "        result = get_similarity_score(resume_string, jd_string)\n",
      "        similarity_score = result[0][\"score\"]\n",
      "        st.write(\"Similarity Score obtained for the resume and job description is:\", similarity_score)\n",
      "else:\n",
      "    print(\"Config file does not exist.\")\n",
      "\n",
      "\n",
      "st.title(':blue[Resume Matcher]')\n",
      "st.subheader(\n",
      "    'Free and Open Source ATS to help your resume pass the screening stage.')\n",
      "st.markdown(\n",
      "    ' Give Resume Matcher a Star on [GitHub](https://github.com/srbhr/Resume-Matcher/)')\n",
      "badge(type=\"github\", name=\"srbhr/Resume-Matcher\")\n",
      "\n",
      "st.text('For updates follow me on Twitter.')\n",
      "badge(type=\"twitter\", name=\"_srbhr_\")\n",
      "\n",
      "st.markdown(\n",
      "    'If you like the project and would like to further help in development please consider ')\n",
      "badge(type=\"buymeacoffee\", name=\"srbhr\")\n",
      "\n",
      "import networkx as nx\n",
      "from typing import List\n",
      "import streamlit as st\n",
      "import pandas as pd\n",
      "import json\n",
      "import plotly.express as px\n",
      "import plotly.graph_objects as go\n",
      "from scripts.utils import get_filenames_from_dir\n",
      "from streamlit_extras import add_vertical_space as avs\n",
      "from annotated_text import annotated_text, parameters\n",
      "from streamlit_extras.badges import badge\n",
      "import nltk\n",
      "nltk.download('punkt')\n",
      "\n",
      "parameters.SHOW_LABEL_SEPARATOR = False\n",
      "parameters.BORDER_RADIUS = 3\n",
      "parameters.PADDING = \"0.5 0.25rem\"\n",
      "\n",
      "\n",
      "def create_star_graph(nodes_and_weights, title):\n",
      "    # Create an empty graph\n",
      "    G = nx.Graph()\n",
      "\n",
      "    # Add the central node\n",
      "    central_node = \"resume\"\n",
      "    G.add_node(central_node)\n",
      "\n",
      "    # Add nodes and edges with weights to the graph\n",
      "    for node, weight in nodes_and_weights:\n",
      "        G.add_node(node)\n",
      "        G.add_edge(central_node, node, weight=weight*100)\n",
      "\n",
      "    # Get position layout for nodes\n",
      "    pos = nx.spring_layout(G)\n",
      "\n",
      "    # Create edge trace\n",
      "    edge_x = []\n",
      "    edge_y = []\n",
      "    for edge in G.edges():\n",
      "        x0, y0 = pos[edge[0]]\n",
      "        x1, y1 = pos[edge[1]]\n",
      "        edge_x.extend([x0, x1, None])\n",
      "        edge_y.extend([y0, y1, None])\n",
      "\n",
      "    edge_trace = go.Scatter(x=edge_x, y=edge_y, line=dict(\n",
      "        width=0.5, color='#888'), hoverinfo='none', mode='lines')\n",
      "\n",
      "    # Create node trace\n",
      "    node_x = []\n",
      "    node_y = []\n",
      "    for node in G.nodes():\n",
      "        x, y = pos[node]\n",
      "        node_x.append(x)\n",
      "        node_y.append(y)\n",
      "\n",
      "    node_trace = go.Scatter(x=node_x, y=node_y, mode='markers', hoverinfo='text',\n",
      "                            marker=dict(showscale=True, colorscale='Rainbow', reversescale=True, color=[], size=10,\n",
      "                                        colorbar=dict(thickness=15, title='Node Connections', xanchor='left',\n",
      "                                                      titleside='right'), line_width=2))\n",
      "\n",
      "    # Color node points by number of connections\n",
      "    node_adjacencies = []\n",
      "    node_text = []\n",
      "    for node in G.nodes():\n",
      "        adjacencies = list(G.adj[node])  # changes here\n",
      "        node_adjacencies.append(len(adjacencies))\n",
      "        node_text.append(f'{node}<br># of connections: {len(adjacencies)}')\n",
      "\n",
      "    node_trace.marker.color = node_adjacencies\n",
      "    node_trace.text = node_text\n",
      "\n",
      "    # Create the figure\n",
      "    fig = go.Figure(data=[edge_trace, node_trace],\n",
      "                    layout=go.Layout(title=title, titlefont_size=16, showlegend=False,\n",
      "                                     hovermode='closest', margin=dict(b=20, l=5, r=5, t=40),\n",
      "                                     xaxis=dict(\n",
      "                                         showgrid=False, zeroline=False, showticklabels=False),\n",
      "                                     yaxis=dict(showgrid=False, zeroline=False, showticklabels=False)))\n",
      "\n",
      "    # Show the figure\n",
      "    st.plotly_chart(fig)\n",
      "\n",
      "\n",
      "def create_annotated_text(input_string: str, word_list: List[str], annotation: str, color_code: str):\n",
      "    # Tokenize the input string\n",
      "    tokens = nltk.word_tokenize(input_string)\n",
      "\n",
      "    # Convert the list to a set for quick lookups\n",
      "    word_set = set(word_list)\n",
      "\n",
      "    # Initialize an empty list to hold the annotated text\n",
      "    annotated_text = []\n",
      "\n",
      "    for token in tokens:\n",
      "        # Check if the token is in the set\n",
      "        if token in word_set:\n",
      "            # If it is, append a tuple with the token, annotation, and color code\n",
      "            annotated_text.append((token, annotation, color_code))\n",
      "        else:\n",
      "            # If it's not, just append the token as a string\n",
      "            annotated_text.append(token)\n",
      "\n",
      "    return annotated_text\n",
      "\n",
      "\n",
      "def read_json(filename):\n",
      "    with open(filename) as f:\n",
      "        data = json.load(f)\n",
      "    return data\n",
      "\n",
      "\n",
      "def tokenize_string(input_string):\n",
      "    tokens = nltk.word_tokenize(input_string)\n",
      "    return tokens\n",
      "\n",
      "\n",
      "st.image('Assets/img/header_image.jpg')\n",
      "\n",
      "st.title(':blue[Resume Matcher]')\n",
      "st.subheader(\n",
      "    'Free and Open Source ATS to help your resume pass the screening stage.')\n",
      "st.markdown(\n",
      "    \"Check the website [www.resumematcher.fyi](https://www.resumematcher.fyi/)\")\n",
      "st.markdown(\n",
      "    ' Give Resume Matcher a Star on [GitHub](https://github.com/srbhr/Resume-Matcher/)')\n",
      "badge(type=\"github\", name=\"srbhr/Resume-Matcher\")\n",
      "\n",
      "st.text('For updates follow me on Twitter.')\n",
      "badge(type=\"twitter\", name=\"_srbhr_\")\n",
      "\n",
      "st.markdown(\n",
      "    'If you like the project and would like to further help in development please consider ')\n",
      "badge(type=\"buymeacoffee\", name=\"srbhr\")\n",
      "\n",
      "avs.add_vertical_space(5)\n",
      "\n",
      "resume_names = get_filenames_from_dir(\"Data/Processed/Resumes\")\n",
      "\n",
      "st.write(\"There are\", len(resume_names),\n",
      "         \" resumes present. Please select one from the menu below:\")\n",
      "output = st.slider('Select Resume Number', 0, len(resume_names)-1, 2)\n",
      "\n",
      "avs.add_vertical_space(5)\n",
      "\n",
      "st.write(\"You have selected \", resume_names[output], \" printing the resume\")\n",
      "selected_file = read_json(\"Data/Processed/Resumes/\"+resume_names[output])\n",
      "\n",
      "avs.add_vertical_space(2)\n",
      "st.markdown(\"#### Parsed Resume Data\")\n",
      "st.caption(\n",
      "    \"This text is parsed from your resume. This is how it'll look like after getting parsed by an ATS.\")\n",
      "st.caption(\"Utilize this to understand how to make your resume ATS friendly.\")\n",
      "avs.add_vertical_space(3)\n",
      "# st.json(selected_file)\n",
      "st.write(selected_file[\"clean_data\"])\n",
      "\n",
      "avs.add_vertical_space(3)\n",
      "st.write(\"Now let's take a look at the extracted keywords from the resume.\")\n",
      "\n",
      "annotated_text(create_annotated_text(\n",
      "    selected_file[\"clean_data\"], selected_file[\"extracted_keywords\"],\n",
      "    \"KW\", \"#0B666A\"))\n",
      "\n",
      "avs.add_vertical_space(5)\n",
      "st.write(\"Now let's take a look at the extracted entities from the resume.\")\n",
      "\n",
      "# Call the function with your data\n",
      "create_star_graph(selected_file['keyterms'], \"Entities from Resume\")\n",
      "\n",
      "df2 = pd.DataFrame(selected_file['keyterms'], columns=[\"keyword\", \"value\"])\n",
      "\n",
      "# Create the dictionary\n",
      "keyword_dict = {}\n",
      "for keyword, value in selected_file['keyterms']:\n",
      "    keyword_dict[keyword] = value*100\n",
      "\n",
      "fig = go.Figure(data=[go.Table(header=dict(values=[\"Keyword\", \"Value\"],\n",
      "                                           font=dict(size=12),\n",
      "                                           fill_color='#070A52'),\n",
      "                               cells=dict(values=[list(keyword_dict.keys()),\n",
      "                                                  list(keyword_dict.values())],\n",
      "                                          line_color='darkslategray',\n",
      "                                          fill_color='#6DA9E4'))\n",
      "                      ])\n",
      "st.plotly_chart(fig)\n",
      "\n",
      "st.divider()\n",
      "\n",
      "fig = px.treemap(df2, path=['keyword'], values='value',\n",
      "                 color_continuous_scale='Rainbow',\n",
      "                 title='Key Terms/Topics Extracted from your Resume')\n",
      "st.write(fig)\n",
      "\n",
      "avs.add_vertical_space(5)\n",
      "\n",
      "job_descriptions = get_filenames_from_dir(\"Data/Processed/JobDescription\")\n",
      "\n",
      "st.write(\"There are\", len(job_descriptions),\n",
      "         \" resumes present. Please select one from the menu below:\")\n",
      "output = st.slider('Select Job Description Number',\n",
      "                   0, len(job_descriptions)-1, 2)\n",
      "\n",
      "avs.add_vertical_space(5)\n",
      "\n",
      "st.write(\"You have selected \",\n",
      "         job_descriptions[output], \" printing the job description\")\n",
      "selected_jd = read_json(\n",
      "    \"Data/Processed/JobDescription/\"+job_descriptions[output])\n",
      "\n",
      "avs.add_vertical_space(2)\n",
      "st.markdown(\"#### Job Description\")\n",
      "st.caption(\n",
      "    \"Currently in the pipeline I'm parsing this from PDF but it'll be from txt or copy paste.\")\n",
      "avs.add_vertical_space(3)\n",
      "# st.json(selected_file)\n",
      "st.write(selected_jd[\"clean_data\"])\n",
      "\n",
      "st.markdown(\"#### Common Words between Job Description and Resumes Highlighted.\")\n",
      "\n",
      "annotated_text(create_annotated_text(\n",
      "    selected_file[\"clean_data\"], selected_jd[\"extracted_keywords\"],\n",
      "    \"JD\", \"#F24C3D\"))\n",
      "\n",
      "st.write(\"Now let's take a look at the extracted entities from the job description.\")\n",
      "\n",
      "# Call the function with your data\n",
      "create_star_graph(selected_jd['keyterms'], \"Entities from Job Description\")\n",
      "\n",
      "df2 = pd.DataFrame(selected_jd['keyterms'], columns=[\"keyword\", \"value\"])\n",
      "\n",
      "# Create the dictionary\n",
      "keyword_dict = {}\n",
      "for keyword, value in selected_jd['keyterms']:\n",
      "    keyword_dict[keyword] = value*100\n",
      "\n",
      "fig = go.Figure(data=[go.Table(header=dict(values=[\"Keyword\", \"Value\"],\n",
      "                                           font=dict(size=12),\n",
      "                                           fill_color='#070A52'),\n",
      "                               cells=dict(values=[list(keyword_dict.keys()),\n",
      "                                                  list(keyword_dict.values())],\n",
      "                                          line_color='darkslategray',\n",
      "                                          fill_color='#6DA9E4'))\n",
      "                      ])\n",
      "st.plotly_chart(fig)\n",
      "\n",
      "st.divider()\n",
      "\n",
      "fig = px.treemap(df2, path=['keyword'], values='value',\n",
      "                 color_continuous_scale='Rainbow',\n",
      "                 title='Key Terms/Topics Extracted from the selected Job Description')\n",
      "st.write(fig)\n",
      "\n",
      "avs.add_vertical_space(5)\n",
      "\n",
      "st.divider()\n",
      "\n",
      "st.markdown(\"## Vector Similarity Scores\")\n",
      "st.caption(\"Powered by Qdrant Vector Search\")\n",
      "st.info(\"These are pre-computed queries\", icon=\"\")\n",
      "st.warning(\n",
      "    \"Running Qdrant or Sentence Transformers without having capacity is not recommended\", icon=\"\")\n",
      "\n",
      "\n",
      "# Your data\n",
      "data = [\n",
      "    {'text': \"{'resume': 'Alfred Pennyworth\",\n",
      "        'query': 'Job Description Product Manager', 'score': 0.62658},\n",
      "    {'text': \"{'resume': 'Barry Allen\",\n",
      "        'query': 'Job Description Product Manager', 'score': 0.43777737},\n",
      "    {'text': \"{'resume': 'Bruce Wayne \",\n",
      "        'query': 'Job Description Product Manager', 'score': 0.39835533},\n",
      "    {'text': \"{'resume': 'JOHN DOE\",\n",
      "        'query': 'Job Description Product Manager', 'score': 0.3915512},\n",
      "    {'text': \"{'resume': 'Harvey Dent\",\n",
      "        'query': 'Job Description Product Manager', 'score': 0.3519544},\n",
      "    {'text': \"{'resume': 'Barry Allen\",\n",
      "        'query': 'Job Description Senior Full Stack Engineer', 'score': 0.6541866},\n",
      "    {'text': \"{'resume': 'Alfred Pennyworth\",\n",
      "        'query': 'Job Description Senior Full Stack Engineer', 'score': 0.59806436},\n",
      "    {'text': \"{'resume': 'JOHN DOE\",\n",
      "        'query': 'Job Description Senior Full Stack Engineer', 'score': 0.5951386},\n",
      "    {'text': \"{'resume': 'Bruce Wayne \",\n",
      "        'query': 'Job Description Senior Full Stack Engineer', 'score': 0.57700855},\n",
      "    {'text': \"{'resume': 'Harvey Dent\",\n",
      "        'query': 'Job Description Senior Full Stack Engineer', 'score': 0.38489106},\n",
      "    {'text': \"{'resume': 'Barry Allen\",\n",
      "        'query': 'Job Description Front End Engineer', 'score': 0.76813436},\n",
      "    {'text': \"{'resume': 'Bruce Wayne'\",\n",
      "        'query': 'Job Description Front End Engineer', 'score': 0.60440844},\n",
      "    {'text': \"{'resume': 'JOHN DOE\",\n",
      "        'query': 'Job Description Front End Engineer', 'score': 0.56080043},\n",
      "    {'text': \"{'resume': 'Alfred Pennyworth\",\n",
      "        'query': 'Job Description Front End Engineer', 'score': 0.5395049},\n",
      "    {'text': \"{'resume': 'Harvey Dent\",\n",
      "        'query': 'Job Description Front End Engineer', 'score': 0.3859515},\n",
      "    {'text': \"{'resume': 'JOHN DOE\",\n",
      "        'query': 'Job Description Java Developer', 'score': 0.5449441},\n",
      "    {'text': \"{'resume': 'Alfred Pennyworth\",\n",
      "        'query': 'Job Description Java Developer', 'score': 0.53476423},\n",
      "    {'text': \"{'resume': 'Barry Allen\",\n",
      "        'query': 'Job Description Java Developer', 'score': 0.5313871},\n",
      "    {'text': \"{'resume': 'Bruce Wayne \",\n",
      "        'query': 'Job Description Java Developer', 'score': 0.44446343},\n",
      "    {'text': \"{'resume': 'Harvey Dent\",\n",
      "        'query': 'Job Description Java Developer', 'score': 0.3616274}\n",
      "]\n",
      "\n",
      "# Create a DataFrame\n",
      "df = pd.DataFrame(data)\n",
      "\n",
      "# Create different DataFrames based on the query and sort by score\n",
      "df1 = df[df['query'] ==\n",
      "         'Job Description Product Manager'].sort_values(by='score', ascending=False)\n",
      "df2 = df[df['query'] ==\n",
      "         'Job Description Senior Full Stack Engineer'].sort_values(by='score', ascending=False)\n",
      "df3 = df[df['query'] == 'Job Description Front End Engineer'].sort_values(\n",
      "    by='score', ascending=False)\n",
      "df4 = df[df['query'] == 'Job Description Java Developer'].sort_values(\n",
      "    by='score', ascending=False)\n",
      "\n",
      "\n",
      "def plot_df(df, title):\n",
      "    fig = px.bar(df, x='text', y=df['score']*100, title=title)\n",
      "    st.plotly_chart(fig)\n",
      "\n",
      "\n",
      "st.markdown(\"### Bar plots of scores based on similarity to Job Description.\")\n",
      "\n",
      "st.subheader(\":blue[Legend]\")\n",
      "st.text(\"Alfred Pennyworth :  Product Manager\")\n",
      "st.text(\"Barry Allen :  Front End Developer\")\n",
      "st.text(\"Harvey Dent :  Machine Learning Engineer\")\n",
      "st.text(\"Bruce Wayne :  Fullstack Developer (MERN)\")\n",
      "st.text(\"John Doe :  Fullstack Developer (Java)\")\n",
      "\n",
      "\n",
      "plot_df(df1, 'Job Description Product Manager 10+ Years of Exper')\n",
      "plot_df(df2, 'Job Description Senior Full Stack Engineer 5+ Year')\n",
      "plot_df(df3, 'Job Description Front End Engineer 2 Years of Expe')\n",
      "plot_df(df4, 'Job Description Java Developer 3 Years of Experien')\n",
      "\n",
      "\n",
      "avs.add_vertical_space(3)\n",
      "\n",
      "st.markdown(\n",
      "    ' Give Resume Matcher a Star on [GitHub](https://github.com/srbhr/Resume-Matcher/)')\n",
      "badge(type=\"github\", name=\"srbhr/Resume-Matcher\")\n",
      "\n",
      "st.text('For updates follow me on Twitter.')\n",
      "badge(type=\"twitter\", name=\"_srbhr_\")\n",
      "\n",
      "st.markdown(\n",
      "    'If you like the project and would like to further help in development please consider ')\n",
      "badge(type=\"buymeacoffee\", name=\"srbhr\")\n",
      "\n",
      "from fastapi import FastAPI, Depends, HTTPException\n",
      "import json\n",
      "from os.path import join, exists, abspath, dirname\n",
      "import yaml\n",
      "from ..scripts.service_keys import get_similarity_config_keys_values, update_yaml_config\n",
      "\n",
      "from ..scripts.resume_processor import build_response\n",
      "from ..schemas.resume_processor import (\n",
      "    ResumeProcessorResponse,\n",
      "    Job,\n",
      "    ResumeProcessorRequest,\n",
      ")\n",
      "\n",
      "app = FastAPI(\n",
      "    title=\"Resume Matcher\",\n",
      "    description=\"APIs for Resume Matcher\",\n",
      "    version=\"0.1.0\",\n",
      ")\n",
      "\n",
      "\n",
      "@app.post(\"/api/resume-processor\", tags=[\"resume-processor\"])\n",
      "async def resume_processor(\n",
      "    form_data: ResumeProcessorRequest = Depends(ResumeProcessorRequest.as_form),\n",
      ") -> ResumeProcessorResponse:\n",
      "    \"\"\"\n",
      "    Process a resume file and match it against a list of job descriptions.\n",
      "\n",
      "    Args:\n",
      "        form_data (ResumeProcessorRequest): The request data containing the resume file and list of job descriptions.\n",
      "\n",
      "    Returns:\n",
      "        ResumeProcessorResponse: The response containing the results (e.g. vector scores, common words, and suggested word edits) against each job description.\n",
      "    \"\"\"\n",
      "    print(f\"resume_processor() API request > form_data: {form_data}\", \"\\n\")\n",
      "\n",
      "    # Get the file object\n",
      "    resume_file = form_data.resume\n",
      "\n",
      "    # Validate file type as PDF\n",
      "    if resume_file.content_type != \"application/pdf\":\n",
      "        raise HTTPException(status_code=400, detail=\"File must be a PDF document\")\n",
      "\n",
      "    # Parse the jobs data as a JSON string\n",
      "    jobs_data = json.loads(form_data.jobs)\n",
      "\n",
      "    # Convert the jobs data to a list of Job objects\n",
      "    jobs_list = [Job(**job) for job in jobs_data]\n",
      "\n",
      "    # Build the response\n",
      "    response = build_response(resume_file, jobs_list)\n",
      "\n",
      "    return response\n",
      "\n",
      "\n",
      "@app.get(\"/api/service-keys\", tags=[\"get-service-keys\"])\n",
      "async def get_service_keys() -> dict[str, dict[str, str]]:\n",
      "    \"\"\"\n",
      "    Returns the configuration keys and secrets required for the similarity script.\n",
      "\n",
      "    Reads the configuration files and returns the keys and values as a dictionary.\n",
      "    If saved secrets are available, they are returned for each configuration key, otherwise fallback to default placeholder value defined from master config.\n",
      "\n",
      "    Returns:\n",
      "        A flattened dictionary containing the configuration keys and values.\n",
      "    \"\"\"\n",
      "\n",
      "    # Get the absolute path of the project's root directory\n",
      "    ROOT_DIR = abspath(join(dirname(__file__), \"..\", \"..\", \"..\"))\n",
      "\n",
      "    print(\"DEBUG\", \"ROOT_DIR:\", ROOT_DIR)\n",
      "\n",
      "    # Construct the paths to the config files using the root directory path\n",
      "    file_path_config_definition = join(\n",
      "        ROOT_DIR, \"scripts\", \"similarity\", \"config.yml\"\n",
      "    )  # master config - definition of required keys - config should not be edited\n",
      "    file_path_saved_secrets = join(\n",
      "        ROOT_DIR, \"scripts\", \"similarity\", \"config.local.yml\"\n",
      "    )  # saved secrets - config can be programmatically created / updated with actual secrets. Ignored from version control (.gitignore)\n",
      "\n",
      "    try:\n",
      "        # if master config definition file does not exist, raise a 404 not found error\n",
      "        if not exists(file_path_config_definition):\n",
      "            raise HTTPException(\n",
      "                status_code=404,\n",
      "                detail=f\"Config definition file not found at path: {file_path_config_definition}\",\n",
      "            )\n",
      "\n",
      "        # variable to hold an updated reference of stored (git ignored) secrets\n",
      "        config_secrets_keys = {}\n",
      "\n",
      "        # if saved secrets config file exists, read the file and update the config_secrets_keys variable with the saved secrets\n",
      "        if exists(file_path_saved_secrets):\n",
      "            with open(file_path_saved_secrets) as file:\n",
      "                config_secrets = yaml.load(file, Loader=yaml.FullLoader)\n",
      "                config_secrets_keys = get_similarity_config_keys_values(config_secrets)\n",
      "\n",
      "        # read the master config file and update the config_keys vaiable with the master config keys\n",
      "        with open(file_path_config_definition) as file:\n",
      "            config = yaml.load(file, Loader=yaml.FullLoader)\n",
      "            config_keys = get_similarity_config_keys_values(config)\n",
      "\n",
      "            # and update any keys with saved secrets from the config_secrets_keys variable, otherwise fallback to default placeholder value defined from master config\n",
      "            for key, value in config_secrets_keys.items():\n",
      "                if key in config_keys:\n",
      "                    config_keys[key] = value\n",
      "\n",
      "            print(\n",
      "                f\"get_service_keys() API request > config_keys: {config_keys}\",\n",
      "                \"\\n\",\n",
      "            )\n",
      "\n",
      "            # respond with the available required config keys and its saved secrets\n",
      "            return {\"config_keys\": config_keys}\n",
      "    except Exception as e:\n",
      "        print(\"DEBUG\", \"Error retreiving service keys:\", e)\n",
      "        raise HTTPException(\n",
      "            status_code=500, detail=f\"Error retreiving service keys: {e}\"\n",
      "        )\n",
      "\n",
      "\n",
      "@app.put(\"/api/service-keys\", tags=[\"update-service-keys\"])\n",
      "async def update_service_keys(keys: dict[str, str]):\n",
      "    \"\"\"\n",
      "    Update the service keys in the secret (git ignored) config file.\n",
      "\n",
      "    Args:\n",
      "        keys (dict[str, str]): A dictionary containing the service keys to be updated.\n",
      "\n",
      "    Returns:\n",
      "        dict: A dictionary containing a success message and the updated keys.\n",
      "    \"\"\"\n",
      "    try:\n",
      "        # Get the absolute path of the project's root directory\n",
      "        ROOT_DIR = abspath(join(dirname(__file__), \"..\", \"..\", \"..\"))\n",
      "        print(\"DEBUG\", \"ROOT_DIR:\", ROOT_DIR)\n",
      "\n",
      "        # define the path to the secret (git ignored) config file, which will hold the saved secrets (or placeholder values)\n",
      "        file_path_writeable = join(\n",
      "            ROOT_DIR, \"scripts\", \"similarity\", \"config.local.yml\"\n",
      "        )\n",
      "\n",
      "        # build a config file structured as a YAML dictionary, with the updated keys and value (secrets)\n",
      "        config_local_secrets = update_yaml_config(keys)\n",
      "\n",
      "        with open(file_path_writeable, \"w\") as file_write:\n",
      "            # write the config key with secrets to (git ignored) secret config file\n",
      "            yaml.dump(config_local_secrets, file_write)\n",
      "\n",
      "            # respond with a success message and the updated keys\n",
      "            return {\"message\": \"Config file updated successfully\", \"keys\": keys}\n",
      "    except Exception as e:\n",
      "        print(\"DEBUG\", \"Error updating service keys:\", e)\n",
      "        raise HTTPException(status_code=500, detail=f\"Error updating service keys: {e}\")\n",
      "\n",
      "File webapp/backend/schemas/__init__.py has no source code\n",
      "\n",
      "from typing import List, Optional\n",
      "from fastapi import File, Form, UploadFile\n",
      "from pydantic import BaseModel\n",
      "\n",
      "\n",
      "class Job(BaseModel):\n",
      "    id: str\n",
      "    link: Optional[str] = None\n",
      "    description: Optional[str] = None\n",
      "\n",
      "\n",
      "class ResumeProcessorRequest(BaseModel):\n",
      "    resume: UploadFile\n",
      "    jobs: str\n",
      "\n",
      "    @classmethod\n",
      "    def as_form(cls, resume: UploadFile = File(...), jobs: str = Form(...)):\n",
      "        return cls(resume=resume, jobs=jobs)\n",
      "\n",
      "\n",
      "class VectorScore(BaseModel):\n",
      "    jobId: str\n",
      "    score: int\n",
      "\n",
      "\n",
      "class CommonWord(BaseModel):\n",
      "    jobId: str\n",
      "    text: str\n",
      "\n",
      "\n",
      "class Changes(BaseModel):\n",
      "    changeFrom: str\n",
      "    changeTo: str\n",
      "\n",
      "\n",
      "class Suggestion(BaseModel):\n",
      "    jobId: str\n",
      "    changes: List[Changes]\n",
      "\n",
      "\n",
      "class ResumeProcessorResponse(BaseModel):\n",
      "    vectorScoresSet: List[VectorScore]\n",
      "    commonWordsSet: List[CommonWord]\n",
      "    suggestionsSet: List[Suggestion]\n",
      "\n",
      "File webapp/backend/scripts/__init__.py has no source code\n",
      "\n",
      "import random\n",
      "from fastapi import UploadFile\n",
      "from typing import List\n",
      "from ..schemas.resume_processor import (\n",
      "    ResumeProcessorResponse,\n",
      "    Job,\n",
      "    Suggestion,\n",
      "    VectorScore,\n",
      "    CommonWord,\n",
      "    Changes,\n",
      ")\n",
      "\n",
      "\n",
      "def build_response(file: UploadFile, jobs: List[Job]) -> ResumeProcessorResponse:\n",
      "    # Print out the input data (from API request body) for debugging purposes\n",
      "    print(f\"build_response() input > resume: {file.filename}\", \"\\n\")\n",
      "    print(f\"build_response() input > jobs: {jobs}\", \"\\n\")\n",
      "\n",
      "    # TEMPORARY: Dynamically (partially) mock the mostly fixed data based on number of jobs data submitted by the user\n",
      "    # Currently mocked for now, to visualise potential response data model schema for the frontend client UI to process / handle.\n",
      "    # Will need to be implemented with actual results generated from other scripts... (TBD) \n",
      "    vector_scores_set = [\n",
      "        VectorScore(jobId=job.id, score=random.randint(1, 100)) for job in jobs\n",
      "    ]\n",
      "\n",
      "    common_words_set = [\n",
      "        CommonWord(\n",
      "            jobId=job.id,\n",
      "            text=\"<p>Job Description Senior <span data-highlight>Full Stack Engineer</span> 5+ Years of Experience Tech Solutions San Francisco CA USA. ABout Us Tech Solutions is a ...</p>\",\n",
      "        )\n",
      "        for job in jobs\n",
      "    ]\n",
      "\n",
      "    suggestions_set = [\n",
      "        Suggestion(\n",
      "            jobId=job.id,\n",
      "            changes=[\n",
      "                Changes(changeFrom=\"Web Engineer\", changeTo=\"Frontend Developer\"),\n",
      "                Changes(\n",
      "                    changeFrom=\"5+ years of experience\",\n",
      "                    changeTo=\"5+ years of experience with React\",\n",
      "                ),\n",
      "                Changes(changeFrom=\"Tech ideas\", changeTo=\"Tech solutions\"),\n",
      "                Changes(\n",
      "                    changeFrom=\"unit tested\", changeTo=\"comprehensively unit tested\"\n",
      "                ),\n",
      "                Changes(\n",
      "                    changeFrom=\"worked closely with design team\",\n",
      "                    changeTo=\"collaborated with design team\",\n",
      "                ),\n",
      "                Changes(\n",
      "                    changeFrom=\"completed solution.\",\n",
      "                    changeTo=\"successfully delivered solution.\",\n",
      "                ),\n",
      "            ],\n",
      "        )\n",
      "        for job in jobs\n",
      "    ]\n",
      "\n",
      "    # Return the response (to be sent back to the API caller)\n",
      "    return ResumeProcessorResponse(\n",
      "        vectorScoresSet=vector_scores_set,\n",
      "        commonWordsSet=common_words_set,\n",
      "        suggestionsSet=suggestions_set,\n",
      "    )\n",
      "\n",
      "from typing import Any, Dict\n",
      "\n",
      "\n",
      "from typing import Dict\n",
      "\n",
      "\n",
      "def get_similarity_config_keys_values(\n",
      "    config: Dict[str, str | Dict[str, str]]\n",
      ") -> Dict[str, str]:\n",
      "    \"\"\"\n",
      "    Recursively flattens a nested dictionary into a single-level dictionary with keys\n",
      "    in the format \"parent_key__child_key\" for all nested keys. Returns the resulting\n",
      "    flattened dictionary.\n",
      "\n",
      "    Args:\n",
      "        config (Dict[str, str | Dict[str, str]]): The nested dictionary to flatten.\n",
      "\n",
      "    Returns:\n",
      "        Dict[str, str]: The resulting flattened dictionary.\n",
      "    \"\"\"\n",
      "    result: Dict[str, str] = {}\n",
      "    for key, value in config.items():\n",
      "        if isinstance(value, dict):\n",
      "            # Recursively call this function to flatten the nested dictionary\n",
      "            subkeys: Dict[str, str] = get_similarity_config_keys_values(value)  # type: ignore - use of isinstance() check strictly infers that value is a dict[str, str], but it could contain nested dicts as its values. Need to figure out how to correctly type this \n",
      "\n",
      "            # Update the result dictionary with the flattened keys and values\n",
      "            result.update(\n",
      "                {\n",
      "                    f\"{key}__{subkey_key}\": subkey_value\n",
      "                    for subkey_key, subkey_value in subkeys.items()\n",
      "                }\n",
      "            )\n",
      "        else:\n",
      "            # If the value is not a dictionary (it is a string), add value to the resulting key in the dictionary\n",
      "            result[key] = value\n",
      "\n",
      "    # Return the resulting flattened dictionary\n",
      "    return result\n",
      "\n",
      "\n",
      "from typing import Dict, Any\n",
      "\n",
      "\n",
      "def update_yaml_config(keys: Dict[str, str]) -> Dict[str, Any]:\n",
      "    \"\"\"\n",
      "    Recursively updates a dictionary with keys that contain double underscores ('__') in their names.\n",
      "    The double underscores are used to indicate nested keys in a YAML file.\n",
      "\n",
      "    Args:\n",
      "        keys (Dict[str, str]): A dictionary containing keys and values to update.\n",
      "\n",
      "    Returns:\n",
      "        Dict[str, Any]: The updated dictionary. Note: the type should be Dict[str, str | Dict[str, str]], but the type checker doesn't like that. Need to figure out how to correctly type this \n",
      "    \"\"\"\n",
      "\n",
      "    # Initialize the config dictionary which will be used to write to the YAML file\n",
      "    config: Dict[str, Any] = {}\n",
      "\n",
      "    # Iterate through the keys and values from the input flattened dictionary\n",
      "    for key, value in keys.items():\n",
      "        if \"__\" in key:\n",
      "            # If the key contains double underscores, it is a nested key\n",
      "            parent_key, child_key = key.split(\"__\", 1)\n",
      "\n",
      "            # Recursively call this function to update the nested key\n",
      "            if parent_key not in config:\n",
      "                config[parent_key] = {}\n",
      "            config[parent_key].update(update_yaml_config({child_key: value}))\n",
      "        else:\n",
      "            # If the key does not contain double underscores, it is not a nested key, and the (string) value can be added to the config dictionary\n",
      "            config[key] = value\n",
      "\n",
      "    # Return the updated config dictionary to be written to the YAML file\n",
      "    return config\n",
      "\n",
      "import json\n",
      "import os\n",
      "from typing import List\n",
      "\n",
      "import networkx as nx\n",
      "import nltk\n",
      "import pandas as pd\n",
      "import plotly.express as px\n",
      "import plotly.graph_objects as go\n",
      "import streamlit as st\n",
      "from annotated_text import annotated_text, parameters\n",
      "from streamlit_extras import add_vertical_space as avs\n",
      "from streamlit_extras.badges import badge\n",
      "\n",
      "from scripts.similarity import get_similarity_score, find_path, read_config\n",
      "from scripts.utils import get_filenames_from_dir\n",
      "\n",
      "cwd = find_path('Resume-Matcher')\n",
      "config_path = os.path.join(cwd, \"scripts\", \"similarity\")\n",
      "\n",
      "try:\n",
      "    nltk.data.find('tokenizers/punkt')\n",
      "except LookupError:\n",
      "    nltk.download('punkt')\n",
      "\n",
      "parameters.SHOW_LABEL_SEPARATOR = False\n",
      "parameters.BORDER_RADIUS = 3\n",
      "parameters.PADDING = \"0.5 0.25rem\"\n",
      "\n",
      "\n",
      "def create_star_graph(nodes_and_weights, title):\n",
      "    # Create an empty graph\n",
      "    G = nx.Graph()\n",
      "\n",
      "    # Add the central node\n",
      "    central_node = \"resume\"\n",
      "    G.add_node(central_node)\n",
      "\n",
      "    # Add nodes and edges with weights to the graph\n",
      "    for node, weight in nodes_and_weights:\n",
      "        G.add_node(node)\n",
      "        G.add_edge(central_node, node, weight=weight * 100)\n",
      "\n",
      "    # Get position layout for nodes\n",
      "    pos = nx.spring_layout(G)\n",
      "\n",
      "    # Create edge trace\n",
      "    edge_x = []\n",
      "    edge_y = []\n",
      "    for edge in G.edges():\n",
      "        x0, y0 = pos[edge[0]]\n",
      "        x1, y1 = pos[edge[1]]\n",
      "        edge_x.extend([x0, x1, None])\n",
      "        edge_y.extend([y0, y1, None])\n",
      "\n",
      "    edge_trace = go.Scatter(x=edge_x, y=edge_y, line=dict(\n",
      "        width=0.5, color='#888'), hoverinfo='none', mode='lines')\n",
      "\n",
      "    # Create node trace\n",
      "    node_x = []\n",
      "    node_y = []\n",
      "    for node in G.nodes():\n",
      "        x, y = pos[node]\n",
      "        node_x.append(x)\n",
      "        node_y.append(y)\n",
      "\n",
      "    node_trace = go.Scatter(x=node_x, y=node_y, mode='markers', hoverinfo='text',\n",
      "                            marker=dict(showscale=True, colorscale='Rainbow', reversescale=True, color=[], size=10,\n",
      "                                        colorbar=dict(thickness=15, title='Node Connections', xanchor='left',\n",
      "                                                      titleside='right'), line_width=2))\n",
      "\n",
      "    # Color node points by number of connections\n",
      "    node_adjacencies = []\n",
      "    node_text = []\n",
      "    for node in G.nodes():\n",
      "        adjacencies = list(G.adj[node])  # changes here\n",
      "        node_adjacencies.append(len(adjacencies))\n",
      "        node_text.append(f'{node}<br># of connections: {len(adjacencies)}')\n",
      "\n",
      "    node_trace.marker.color = node_adjacencies\n",
      "    node_trace.text = node_text\n",
      "\n",
      "    # Create the figure\n",
      "    fig = go.Figure(data=[edge_trace, node_trace],\n",
      "                    layout=go.Layout(title=title, titlefont_size=16, showlegend=False,\n",
      "                                     hovermode='closest', margin=dict(b=20, l=5, r=5, t=40),\n",
      "                                     xaxis=dict(\n",
      "                                         showgrid=False, zeroline=False, showticklabels=False),\n",
      "                                     yaxis=dict(showgrid=False, zeroline=False, showticklabels=False)))\n",
      "\n",
      "    # Show the figure\n",
      "    st.plotly_chart(fig)\n",
      "\n",
      "\n",
      "def create_annotated_text(input_string: str, word_list: List[str], annotation: str, color_code: str):\n",
      "    # Tokenize the input string\n",
      "    tokens = nltk.word_tokenize(input_string)\n",
      "\n",
      "    # Convert the list to a set for quick lookups\n",
      "    word_set = set(word_list)\n",
      "\n",
      "    # Initialize an empty list to hold the annotated text\n",
      "    annotated_text = []\n",
      "\n",
      "    for token in tokens:\n",
      "        # Check if the token is in the set\n",
      "        if token in word_set:\n",
      "            # If it is, append a tuple with the token, annotation, and color code\n",
      "            annotated_text.append((token, annotation, color_code))\n",
      "        else:\n",
      "            # If it's not, just append the token as a string\n",
      "            annotated_text.append(token)\n",
      "\n",
      "    return annotated_text\n",
      "\n",
      "\n",
      "def read_json(filename):\n",
      "    with open(filename) as f:\n",
      "        data = json.load(f)\n",
      "    return data\n",
      "\n",
      "\n",
      "def tokenize_string(input_string):\n",
      "    tokens = nltk.word_tokenize(input_string)\n",
      "    return tokens\n",
      "\n",
      "\n",
      "st.image('Assets/img/header_image.png')\n",
      "\n",
      "st.title(':blue[Resume Matcher]')\n",
      "st.subheader(\n",
      "    'Free and Open Source ATS to help your resume pass the screening stage.')\n",
      "st.markdown(\n",
      "    \"Check the website [www.resumematcher.fyi](https://www.resumematcher.fyi/)\")\n",
      "st.markdown(\n",
      "    ' Give Resume Matcher a Star on [GitHub](https://github.com/srbhr/resume-matcher)')\n",
      "badge(type=\"github\", name=\"srbhr/Resume-Matcher\")\n",
      "\n",
      "st.text('For updates follow me on Twitter.')\n",
      "badge(type=\"twitter\", name=\"_srbhr_\")\n",
      "\n",
      "st.markdown(\n",
      "    'If you like the project and would like to further help in development please consider ')\n",
      "badge(type=\"buymeacoffee\", name=\"srbhr\")\n",
      "\n",
      "avs.add_vertical_space(5)\n",
      "\n",
      "resume_names = get_filenames_from_dir(\"Data/Processed/Resumes\")\n",
      "\n",
      "output = 0\n",
      "\n",
      "if len(resume_names) > 1:\n",
      "    st.write(\"There are\", len(resume_names),\n",
      "             \" resumes present. Please select one from the menu below:\")\n",
      "    output = st.slider('Select Resume Number', 0, len(resume_names) - 1, 0)\n",
      "else:\n",
      "    st.write(\"There is 1 resume present\")\n",
      "\n",
      "avs.add_vertical_space(5)\n",
      "\n",
      "st.write(\"You have selected \", resume_names[output], \" printing the resume\")\n",
      "selected_file = read_json(\"Data/Processed/Resumes/\" + resume_names[output])\n",
      "\n",
      "avs.add_vertical_space(2)\n",
      "st.markdown(\"#### Parsed Resume Data\")\n",
      "st.caption(\n",
      "    \"This text is parsed from your resume. This is how it'll look like after getting parsed by an ATS.\")\n",
      "st.caption(\"Utilize this to understand how to make your resume ATS friendly.\")\n",
      "avs.add_vertical_space(3)\n",
      "# st.json(selected_file)\n",
      "st.write(selected_file[\"clean_data\"])\n",
      "\n",
      "avs.add_vertical_space(3)\n",
      "st.write(\"Now let's take a look at the extracted keywords from the resume.\")\n",
      "\n",
      "annotated_text(create_annotated_text(\n",
      "    selected_file[\"clean_data\"], selected_file[\"extracted_keywords\"],\n",
      "    \"KW\", \"#0B666A\"))\n",
      "\n",
      "avs.add_vertical_space(5)\n",
      "st.write(\"Now let's take a look at the extracted entities from the resume.\")\n",
      "\n",
      "# Call the function with your data\n",
      "create_star_graph(selected_file['keyterms'], \"Entities from Resume\")\n",
      "\n",
      "df2 = pd.DataFrame(selected_file['keyterms'], columns=[\"keyword\", \"value\"])\n",
      "\n",
      "# Create the dictionary\n",
      "keyword_dict = {}\n",
      "for keyword, value in selected_file['keyterms']:\n",
      "    keyword_dict[keyword] = value * 100\n",
      "\n",
      "fig = go.Figure(data=[go.Table(header=dict(values=[\"Keyword\", \"Value\"],\n",
      "                                           font=dict(size=12),\n",
      "                                           fill_color='#070A52'),\n",
      "                               cells=dict(values=[list(keyword_dict.keys()),\n",
      "                                                  list(keyword_dict.values())],\n",
      "                                          line_color='darkslategray',\n",
      "                                          fill_color='#6DA9E4'))\n",
      "                      ])\n",
      "st.plotly_chart(fig)\n",
      "\n",
      "st.divider()\n",
      "\n",
      "fig = px.treemap(df2, path=['keyword'], values='value',\n",
      "                 color_continuous_scale='Rainbow',\n",
      "                 title='Key Terms/Topics Extracted from your Resume')\n",
      "st.write(fig)\n",
      "\n",
      "avs.add_vertical_space(5)\n",
      "\n",
      "job_descriptions = get_filenames_from_dir(\"Data/Processed/JobDescription\")\n",
      "\n",
      "output = 0\n",
      "if len(job_descriptions) > 1:\n",
      "    st.write(\"There are\", len(job_descriptions),\n",
      "             \" job descriptions present. Please select one from the menu below:\")\n",
      "    output = st.slider('Select Job Description Number',\n",
      "                       0, len(job_descriptions) - 1, 0)\n",
      "else:\n",
      "    st.write(\"There is 1 job description present\")\n",
      "\n",
      "avs.add_vertical_space(5)\n",
      "\n",
      "st.write(\"You have selected \",\n",
      "         job_descriptions[output], \" printing the job description\")\n",
      "selected_jd = read_json(\n",
      "    \"Data/Processed/JobDescription/\" + job_descriptions[output])\n",
      "\n",
      "avs.add_vertical_space(2)\n",
      "st.markdown(\"#### Job Description\")\n",
      "st.caption(\n",
      "    \"Currently in the pipeline I'm parsing this from PDF but it'll be from txt or copy paste.\")\n",
      "avs.add_vertical_space(3)\n",
      "# st.json(selected_file)\n",
      "st.write(selected_jd[\"clean_data\"])\n",
      "\n",
      "st.markdown(\"#### Common Words between Job Description and Resumes Highlighted.\")\n",
      "\n",
      "annotated_text(create_annotated_text(\n",
      "    selected_file[\"clean_data\"], selected_jd[\"extracted_keywords\"],\n",
      "    \"JD\", \"#F24C3D\"))\n",
      "\n",
      "st.write(\"Now let's take a look at the extracted entities from the job description.\")\n",
      "\n",
      "# Call the function with your data\n",
      "create_star_graph(selected_jd['keyterms'], \"Entities from Job Description\")\n",
      "\n",
      "df2 = pd.DataFrame(selected_jd['keyterms'], columns=[\"keyword\", \"value\"])\n",
      "\n",
      "# Create the dictionary\n",
      "keyword_dict = {}\n",
      "for keyword, value in selected_jd['keyterms']:\n",
      "    keyword_dict[keyword] = value * 100\n",
      "\n",
      "fig = go.Figure(data=[go.Table(header=dict(values=[\"Keyword\", \"Value\"],\n",
      "                                           font=dict(size=12),\n",
      "                                           fill_color='#070A52'),\n",
      "                               cells=dict(values=[list(keyword_dict.keys()),\n",
      "                                                  list(keyword_dict.values())],\n",
      "                                          line_color='darkslategray',\n",
      "                                          fill_color='#6DA9E4'))\n",
      "                      ])\n",
      "st.plotly_chart(fig)\n",
      "\n",
      "st.divider()\n",
      "\n",
      "fig = px.treemap(df2, path=['keyword'], values='value',\n",
      "                 color_continuous_scale='Rainbow',\n",
      "                 title='Key Terms/Topics Extracted from the selected Job Description')\n",
      "st.write(fig)\n",
      "\n",
      "avs.add_vertical_space(3)\n",
      "\n",
      "config_file_path = config_path + \"/config.yml\"\n",
      "if os.path.exists(config_file_path):\n",
      "    config_data = read_config(config_file_path)\n",
      "    if config_data:\n",
      "        print(\"Config file parsed successfully:\")\n",
      "        resume_string = ' '.join(selected_file[\"extracted_keywords\"])\n",
      "        jd_string = ' '.join(selected_jd[\"extracted_keywords\"])\n",
      "        result = get_similarity_score(resume_string, jd_string)\n",
      "        similarity_score = result[0][\"score\"]\n",
      "        st.write(\"Similarity Score obtained for the resume and job description is:\", similarity_score)\n",
      "else:\n",
      "    print(\"Config file does not exist.\")\n",
      "\n",
      "\n",
      "st.title(':blue[Resume Matcher]')\n",
      "st.subheader(\n",
      "    'Free and Open Source ATS to help your resume pass the screening stage.')\n",
      "st.markdown(\n",
      "    ' Give Resume Matcher a Star on [GitHub](https://github.com/srbhr/Resume-Matcher/)')\n",
      "badge(type=\"github\", name=\"srbhr/Resume-Matcher\")\n",
      "\n",
      "st.text('For updates follow me on Twitter.')\n",
      "badge(type=\"twitter\", name=\"_srbhr_\")\n",
      "\n",
      "st.markdown(\n",
      "    'If you like the project and would like to further help in development please consider ')\n",
      "badge(type=\"buymeacoffee\", name=\"srbhr\")\n",
      "\n",
      "import networkx as nx\n",
      "from typing import List\n",
      "import streamlit as st\n",
      "import pandas as pd\n",
      "import json\n",
      "import plotly.express as px\n",
      "import plotly.graph_objects as go\n",
      "from scripts.utils import get_filenames_from_dir\n",
      "from streamlit_extras import add_vertical_space as avs\n",
      "from annotated_text import annotated_text, parameters\n",
      "from streamlit_extras.badges import badge\n",
      "import nltk\n",
      "nltk.download('punkt')\n",
      "\n",
      "parameters.SHOW_LABEL_SEPARATOR = False\n",
      "parameters.BORDER_RADIUS = 3\n",
      "parameters.PADDING = \"0.5 0.25rem\"\n",
      "\n",
      "\n",
      "def create_star_graph(nodes_and_weights, title):\n",
      "    # Create an empty graph\n",
      "    G = nx.Graph()\n",
      "\n",
      "    # Add the central node\n",
      "    central_node = \"resume\"\n",
      "    G.add_node(central_node)\n",
      "\n",
      "    # Add nodes and edges with weights to the graph\n",
      "    for node, weight in nodes_and_weights:\n",
      "        G.add_node(node)\n",
      "        G.add_edge(central_node, node, weight=weight*100)\n",
      "\n",
      "    # Get position layout for nodes\n",
      "    pos = nx.spring_layout(G)\n",
      "\n",
      "    # Create edge trace\n",
      "    edge_x = []\n",
      "    edge_y = []\n",
      "    for edge in G.edges():\n",
      "        x0, y0 = pos[edge[0]]\n",
      "        x1, y1 = pos[edge[1]]\n",
      "        edge_x.extend([x0, x1, None])\n",
      "        edge_y.extend([y0, y1, None])\n",
      "\n",
      "    edge_trace = go.Scatter(x=edge_x, y=edge_y, line=dict(\n",
      "        width=0.5, color='#888'), hoverinfo='none', mode='lines')\n",
      "\n",
      "    # Create node trace\n",
      "    node_x = []\n",
      "    node_y = []\n",
      "    for node in G.nodes():\n",
      "        x, y = pos[node]\n",
      "        node_x.append(x)\n",
      "        node_y.append(y)\n",
      "\n",
      "    node_trace = go.Scatter(x=node_x, y=node_y, mode='markers', hoverinfo='text',\n",
      "                            marker=dict(showscale=True, colorscale='Rainbow', reversescale=True, color=[], size=10,\n",
      "                                        colorbar=dict(thickness=15, title='Node Connections', xanchor='left',\n",
      "                                                      titleside='right'), line_width=2))\n",
      "\n",
      "    # Color node points by number of connections\n",
      "    node_adjacencies = []\n",
      "    node_text = []\n",
      "    for node in G.nodes():\n",
      "        adjacencies = list(G.adj[node])  # changes here\n",
      "        node_adjacencies.append(len(adjacencies))\n",
      "        node_text.append(f'{node}<br># of connections: {len(adjacencies)}')\n",
      "\n",
      "    node_trace.marker.color = node_adjacencies\n",
      "    node_trace.text = node_text\n",
      "\n",
      "    # Create the figure\n",
      "    fig = go.Figure(data=[edge_trace, node_trace],\n",
      "                    layout=go.Layout(title=title, titlefont_size=16, showlegend=False,\n",
      "                                     hovermode='closest', margin=dict(b=20, l=5, r=5, t=40),\n",
      "                                     xaxis=dict(\n",
      "                                         showgrid=False, zeroline=False, showticklabels=False),\n",
      "                                     yaxis=dict(showgrid=False, zeroline=False, showticklabels=False)))\n",
      "\n",
      "    # Show the figure\n",
      "    st.plotly_chart(fig)\n",
      "\n",
      "\n",
      "def create_annotated_text(input_string: str, word_list: List[str], annotation: str, color_code: str):\n",
      "    # Tokenize the input string\n",
      "    tokens = nltk.word_tokenize(input_string)\n",
      "\n",
      "    # Convert the list to a set for quick lookups\n",
      "    word_set = set(word_list)\n",
      "\n",
      "    # Initialize an empty list to hold the annotated text\n",
      "    annotated_text = []\n",
      "\n",
      "    for token in tokens:\n",
      "        # Check if the token is in the set\n",
      "        if token in word_set:\n",
      "            # If it is, append a tuple with the token, annotation, and color code\n",
      "            annotated_text.append((token, annotation, color_code))\n",
      "        else:\n",
      "            # If it's not, just append the token as a string\n",
      "            annotated_text.append(token)\n",
      "\n",
      "    return annotated_text\n",
      "\n",
      "\n",
      "def read_json(filename):\n",
      "    with open(filename) as f:\n",
      "        data = json.load(f)\n",
      "    return data\n",
      "\n",
      "\n",
      "def tokenize_string(input_string):\n",
      "    tokens = nltk.word_tokenize(input_string)\n",
      "    return tokens\n",
      "\n",
      "\n",
      "st.image('Assets/img/header_image.png')\n",
      "\n",
      "st.title(':blue[Resume Matcher]')\n",
      "st.subheader(\n",
      "    'Free and Open Source ATS to help your resume pass the screening stage.')\n",
      "st.markdown(\n",
      "    \"Check the website [www.resumematcher.fyi](https://www.resumematcher.fyi/)\")\n",
      "st.markdown(\n",
      "    ' Give Resume Matcher a Star on [GitHub](https://github.com/srbhr/Resume-Matcher/)')\n",
      "badge(type=\"github\", name=\"srbhr/Resume-Matcher\")\n",
      "\n",
      "st.text('For updates follow me on Twitter.')\n",
      "badge(type=\"twitter\", name=\"_srbhr_\")\n",
      "\n",
      "st.markdown(\n",
      "    'If you like the project and would like to further help in development please consider ')\n",
      "badge(type=\"buymeacoffee\", name=\"srbhr\")\n",
      "\n",
      "avs.add_vertical_space(5)\n",
      "\n",
      "resume_names = get_filenames_from_dir(\"Data/Processed/Resumes\")\n",
      "\n",
      "st.write(\"There are\", len(resume_names),\n",
      "         \" resumes present. Please select one from the menu below:\")\n",
      "output = st.slider('Select Resume Number', 0, len(resume_names)-1, 2)\n",
      "\n",
      "avs.add_vertical_space(5)\n",
      "\n",
      "st.write(\"You have selected \", resume_names[output], \" printing the resume\")\n",
      "selected_file = read_json(\"Data/Processed/Resumes/\"+resume_names[output])\n",
      "\n",
      "avs.add_vertical_space(2)\n",
      "st.markdown(\"#### Parsed Resume Data\")\n",
      "st.caption(\n",
      "    \"This text is parsed from your resume. This is how it'll look like after getting parsed by an ATS.\")\n",
      "st.caption(\"Utilize this to understand how to make your resume ATS friendly.\")\n",
      "avs.add_vertical_space(3)\n",
      "# st.json(selected_file)\n",
      "st.write(selected_file[\"clean_data\"])\n",
      "\n",
      "avs.add_vertical_space(3)\n",
      "st.write(\"Now let's take a look at the extracted keywords from the resume.\")\n",
      "\n",
      "annotated_text(create_annotated_text(\n",
      "    selected_file[\"clean_data\"], selected_file[\"extracted_keywords\"],\n",
      "    \"KW\", \"#0B666A\"))\n",
      "\n",
      "avs.add_vertical_space(5)\n",
      "st.write(\"Now let's take a look at the extracted entities from the resume.\")\n",
      "\n",
      "# Call the function with your data\n",
      "create_star_graph(selected_file['keyterms'], \"Entities from Resume\")\n",
      "\n",
      "df2 = pd.DataFrame(selected_file['keyterms'], columns=[\"keyword\", \"value\"])\n",
      "\n",
      "# Create the dictionary\n",
      "keyword_dict = {}\n",
      "for keyword, value in selected_file['keyterms']:\n",
      "    keyword_dict[keyword] = value*100\n",
      "\n",
      "fig = go.Figure(data=[go.Table(header=dict(values=[\"Keyword\", \"Value\"],\n",
      "                                           font=dict(size=12),\n",
      "                                           fill_color='#070A52'),\n",
      "                               cells=dict(values=[list(keyword_dict.keys()),\n",
      "                                                  list(keyword_dict.values())],\n",
      "                                          line_color='darkslategray',\n",
      "                                          fill_color='#6DA9E4'))\n",
      "                      ])\n",
      "st.plotly_chart(fig)\n",
      "\n",
      "st.divider()\n",
      "\n",
      "fig = px.treemap(df2, path=['keyword'], values='value',\n",
      "                 color_continuous_scale='Rainbow',\n",
      "                 title='Key Terms/Topics Extracted from your Resume')\n",
      "st.write(fig)\n",
      "\n",
      "avs.add_vertical_space(5)\n",
      "\n",
      "job_descriptions = get_filenames_from_dir(\"Data/Processed/JobDescription\")\n",
      "\n",
      "st.write(\"There are\", len(job_descriptions),\n",
      "         \" resumes present. Please select one from the menu below:\")\n",
      "output = st.slider('Select Job Description Number',\n",
      "                   0, len(job_descriptions)-1, 2)\n",
      "\n",
      "avs.add_vertical_space(5)\n",
      "\n",
      "st.write(\"You have selected \",\n",
      "         job_descriptions[output], \" printing the job description\")\n",
      "selected_jd = read_json(\n",
      "    \"Data/Processed/JobDescription/\"+job_descriptions[output])\n",
      "\n",
      "avs.add_vertical_space(2)\n",
      "st.markdown(\"#### Job Description\")\n",
      "st.caption(\n",
      "    \"Currently in the pipeline I'm parsing this from PDF but it'll be from txt or copy paste.\")\n",
      "avs.add_vertical_space(3)\n",
      "# st.json(selected_file)\n",
      "st.write(selected_jd[\"clean_data\"])\n",
      "\n",
      "st.markdown(\"#### Common Words between Job Description and Resumes Highlighted.\")\n",
      "\n",
      "annotated_text(create_annotated_text(\n",
      "    selected_file[\"clean_data\"], selected_jd[\"extracted_keywords\"],\n",
      "    \"JD\", \"#F24C3D\"))\n",
      "\n",
      "st.write(\"Now let's take a look at the extracted entities from the job description.\")\n",
      "\n",
      "# Call the function with your data\n",
      "create_star_graph(selected_jd['keyterms'], \"Entities from Job Description\")\n",
      "\n",
      "df2 = pd.DataFrame(selected_jd['keyterms'], columns=[\"keyword\", \"value\"])\n",
      "\n",
      "# Create the dictionary\n",
      "keyword_dict = {}\n",
      "for keyword, value in selected_jd['keyterms']:\n",
      "    keyword_dict[keyword] = value*100\n",
      "\n",
      "fig = go.Figure(data=[go.Table(header=dict(values=[\"Keyword\", \"Value\"],\n",
      "                                           font=dict(size=12),\n",
      "                                           fill_color='#070A52'),\n",
      "                               cells=dict(values=[list(keyword_dict.keys()),\n",
      "                                                  list(keyword_dict.values())],\n",
      "                                          line_color='darkslategray',\n",
      "                                          fill_color='#6DA9E4'))\n",
      "                      ])\n",
      "st.plotly_chart(fig)\n",
      "\n",
      "st.divider()\n",
      "\n",
      "fig = px.treemap(df2, path=['keyword'], values='value',\n",
      "                 color_continuous_scale='Rainbow',\n",
      "                 title='Key Terms/Topics Extracted from the selected Job Description')\n",
      "st.write(fig)\n",
      "\n",
      "avs.add_vertical_space(5)\n",
      "\n",
      "st.divider()\n",
      "\n",
      "st.markdown(\"## Vector Similarity Scores\")\n",
      "st.caption(\"Powered by Qdrant Vector Search\")\n",
      "st.info(\"These are pre-computed queries\", icon=\"\")\n",
      "st.warning(\n",
      "    \"Running Qdrant or Sentence Transformers without having capacity is not recommended\", icon=\"\")\n",
      "\n",
      "\n",
      "# Your data\n",
      "data = [\n",
      "    {'text': \"{'resume': 'Alfred Pennyworth\",\n",
      "        'query': 'Job Description Product Manager', 'score': 0.62658},\n",
      "    {'text': \"{'resume': 'Barry Allen\",\n",
      "        'query': 'Job Description Product Manager', 'score': 0.43777737},\n",
      "    {'text': \"{'resume': 'Bruce Wayne \",\n",
      "        'query': 'Job Description Product Manager', 'score': 0.39835533},\n",
      "    {'text': \"{'resume': 'JOHN DOE\",\n",
      "        'query': 'Job Description Product Manager', 'score': 0.3915512},\n",
      "    {'text': \"{'resume': 'Harvey Dent\",\n",
      "        'query': 'Job Description Product Manager', 'score': 0.3519544},\n",
      "    {'text': \"{'resume': 'Barry Allen\",\n",
      "        'query': 'Job Description Senior Full Stack Engineer', 'score': 0.6541866},\n",
      "    {'text': \"{'resume': 'Alfred Pennyworth\",\n",
      "        'query': 'Job Description Senior Full Stack Engineer', 'score': 0.59806436},\n",
      "    {'text': \"{'resume': 'JOHN DOE\",\n",
      "        'query': 'Job Description Senior Full Stack Engineer', 'score': 0.5951386},\n",
      "    {'text': \"{'resume': 'Bruce Wayne \",\n",
      "        'query': 'Job Description Senior Full Stack Engineer', 'score': 0.57700855},\n",
      "    {'text': \"{'resume': 'Harvey Dent\",\n",
      "        'query': 'Job Description Senior Full Stack Engineer', 'score': 0.38489106},\n",
      "    {'text': \"{'resume': 'Barry Allen\",\n",
      "        'query': 'Job Description Front End Engineer', 'score': 0.76813436},\n",
      "    {'text': \"{'resume': 'Bruce Wayne'\",\n",
      "        'query': 'Job Description Front End Engineer', 'score': 0.60440844},\n",
      "    {'text': \"{'resume': 'JOHN DOE\",\n",
      "        'query': 'Job Description Front End Engineer', 'score': 0.56080043},\n",
      "    {'text': \"{'resume': 'Alfred Pennyworth\",\n",
      "        'query': 'Job Description Front End Engineer', 'score': 0.5395049},\n",
      "    {'text': \"{'resume': 'Harvey Dent\",\n",
      "        'query': 'Job Description Front End Engineer', 'score': 0.3859515},\n",
      "    {'text': \"{'resume': 'JOHN DOE\",\n",
      "        'query': 'Job Description Java Developer', 'score': 0.5449441},\n",
      "    {'text': \"{'resume': 'Alfred Pennyworth\",\n",
      "        'query': 'Job Description Java Developer', 'score': 0.53476423},\n",
      "    {'text': \"{'resume': 'Barry Allen\",\n",
      "        'query': 'Job Description Java Developer', 'score': 0.5313871},\n",
      "    {'text': \"{'resume': 'Bruce Wayne \",\n",
      "        'query': 'Job Description Java Developer', 'score': 0.44446343},\n",
      "    {'text': \"{'resume': 'Harvey Dent\",\n",
      "        'query': 'Job Description Java Developer', 'score': 0.3616274}\n",
      "]\n",
      "\n",
      "# Create a DataFrame\n",
      "df = pd.DataFrame(data)\n",
      "\n",
      "# Create different DataFrames based on the query and sort by score\n",
      "df1 = df[df['query'] ==\n",
      "         'Job Description Product Manager'].sort_values(by='score', ascending=False)\n",
      "df2 = df[df['query'] ==\n",
      "         'Job Description Senior Full Stack Engineer'].sort_values(by='score', ascending=False)\n",
      "df3 = df[df['query'] == 'Job Description Front End Engineer'].sort_values(\n",
      "    by='score', ascending=False)\n",
      "df4 = df[df['query'] == 'Job Description Java Developer'].sort_values(\n",
      "    by='score', ascending=False)\n",
      "\n",
      "\n",
      "def plot_df(df, title):\n",
      "    fig = px.bar(df, x='text', y=df['score']*100, title=title)\n",
      "    st.plotly_chart(fig)\n",
      "\n",
      "\n",
      "st.markdown(\"### Bar plots of scores based on similarity to Job Description.\")\n",
      "\n",
      "st.subheader(\":blue[Legend]\")\n",
      "st.text(\"Alfred Pennyworth :  Product Manager\")\n",
      "st.text(\"Barry Allen :  Front End Developer\")\n",
      "st.text(\"Harvey Dent :  Machine Learning Engineer\")\n",
      "st.text(\"Bruce Wayne :  Fullstack Developer (MERN)\")\n",
      "st.text(\"John Doe :  Fullstack Developer (Java)\")\n",
      "\n",
      "\n",
      "plot_df(df1, 'Job Description Product Manager 10+ Years of Exper')\n",
      "plot_df(df2, 'Job Description Senior Full Stack Engineer 5+ Year')\n",
      "plot_df(df3, 'Job Description Front End Engineer 2 Years of Expe')\n",
      "plot_df(df4, 'Job Description Java Developer 3 Years of Experien')\n",
      "\n",
      "\n",
      "avs.add_vertical_space(3)\n",
      "\n",
      "st.markdown(\n",
      "    ' Give Resume Matcher a Star on [GitHub](https://github.com/srbhr/Resume-Matcher/)')\n",
      "badge(type=\"github\", name=\"srbhr/Resume-Matcher\")\n",
      "\n",
      "st.text('For updates follow me on Twitter.')\n",
      "badge(type=\"twitter\", name=\"_srbhr_\")\n",
      "\n",
      "st.markdown(\n",
      "    'If you like the project and would like to further help in development please consider ')\n",
      "badge(type=\"buymeacoffee\", name=\"srbhr\")\n",
      "\n",
      "from fastapi import FastAPI, Depends, HTTPException\n",
      "import json\n",
      "from os.path import join, exists, abspath, dirname\n",
      "import yaml\n",
      "from ..scripts.service_keys import get_similarity_config_keys_values, update_yaml_config\n",
      "\n",
      "from ..scripts.resume_processor import build_response\n",
      "from ..schemas.resume_processor import (\n",
      "    ResumeProcessorResponse,\n",
      "    Job,\n",
      "    ResumeProcessorRequest,\n",
      ")\n",
      "\n",
      "from ..scripts.files import save_file_upload, save_job_uploads_to_pdfs\n",
      "\n",
      "app = FastAPI(\n",
      "    title=\"Resume Matcher\",\n",
      "    description=\"APIs for Resume Matcher\",\n",
      "    version=\"0.1.0\",\n",
      ")\n",
      "\n",
      "\n",
      "@app.post(\"/api/resume-processor\", tags=[\"resume-processor\"])\n",
      "async def resume_processor(\n",
      "    form_data: ResumeProcessorRequest = Depends(ResumeProcessorRequest.as_form),\n",
      ") -> ResumeProcessorResponse:\n",
      "    \"\"\"\n",
      "    Process a resume file and match it against a list of job descriptions.\n",
      "\n",
      "    Args:\n",
      "        form_data (ResumeProcessorRequest): The request data containing the resume file and list of job descriptions.\n",
      "\n",
      "    Returns:\n",
      "        ResumeProcessorResponse: The response containing the results (e.g. vector scores, common words, and suggested word edits) against each job description.\n",
      "    \"\"\"\n",
      "    print(f\"resume_processor() API request > form_data: {form_data}\", \"\\n\")\n",
      "\n",
      "    # Get the file object\n",
      "    resume_file = form_data.resume\n",
      "\n",
      "    # Validate file type as PDF\n",
      "    if resume_file.content_type != \"application/pdf\":\n",
      "        raise HTTPException(status_code=400, detail=\"File must be a PDF document\")\n",
      "\n",
      "    # Parse the jobs data as a JSON string\n",
      "    jobs_data = json.loads(form_data.jobs)\n",
      "\n",
      "    # Convert the jobs data to a list of Job objects\n",
      "    jobs_list = [Job(**job) for job in jobs_data]\n",
      "\n",
      "    # Save the resume file (PDF) to local file system\n",
      "    save_file_upload(resume_file)\n",
      "\n",
      "    # Save the job descriptions (PDFs) to local file system\n",
      "    save_job_uploads_to_pdfs(jobs_list)\n",
      "\n",
      "    # Build the response\n",
      "    response = build_response(resume_file, jobs_list)\n",
      "\n",
      "    return response\n",
      "\n",
      "\n",
      "@app.get(\"/api/service-keys\", tags=[\"get-service-keys\"])\n",
      "async def get_service_keys() -> dict[str, dict[str, str]]:\n",
      "    \"\"\"\n",
      "    Returns the configuration keys and secrets required for the similarity script.\n",
      "\n",
      "    Reads the configuration files and returns the keys and values as a dictionary.\n",
      "    If saved secrets are available, they are returned for each configuration key, otherwise fallback to default placeholder value defined from master config.\n",
      "\n",
      "    Returns:\n",
      "        A flattened dictionary containing the configuration keys and values.\n",
      "    \"\"\"\n",
      "\n",
      "    # Get the absolute path of the project's root directory\n",
      "    ROOT_DIR = abspath(join(dirname(__file__), \"..\", \"..\", \"..\"))\n",
      "\n",
      "    print(\"DEBUG\", \"ROOT_DIR:\", ROOT_DIR)\n",
      "\n",
      "    # Construct the paths to the config files using the root directory path\n",
      "    file_path_config_definition = join(\n",
      "        ROOT_DIR, \"scripts\", \"similarity\", \"config.yml\"\n",
      "    )  # master config - definition of required keys - config should not be edited\n",
      "    file_path_saved_secrets = join(\n",
      "        ROOT_DIR, \"scripts\", \"similarity\", \"config.local.yml\"\n",
      "    )  # saved secrets - config can be programmatically created / updated with actual secrets. Ignored from version control (.gitignore)\n",
      "\n",
      "    try:\n",
      "        # if master config definition file does not exist, raise a 404 not found error\n",
      "        if not exists(file_path_config_definition):\n",
      "            raise HTTPException(\n",
      "                status_code=404,\n",
      "                detail=f\"Config definition file not found at path: {file_path_config_definition}\",\n",
      "            )\n",
      "\n",
      "        # variable to hold an updated reference of stored (git ignored) secrets\n",
      "        config_secrets_keys = {}\n",
      "\n",
      "        # if saved secrets config file exists, read the file and update the config_secrets_keys variable with the saved secrets\n",
      "        if exists(file_path_saved_secrets):\n",
      "            with open(file_path_saved_secrets) as file:\n",
      "                config_secrets = yaml.load(file, Loader=yaml.FullLoader)\n",
      "                config_secrets_keys = get_similarity_config_keys_values(config_secrets)\n",
      "\n",
      "        # read the master config file and update the config_keys vaiable with the master config keys\n",
      "        with open(file_path_config_definition) as file:\n",
      "            config = yaml.load(file, Loader=yaml.FullLoader)\n",
      "            config_keys = get_similarity_config_keys_values(config)\n",
      "\n",
      "            # and update any keys with saved secrets from the config_secrets_keys variable, otherwise fallback to default placeholder value defined from master config\n",
      "            for key, value in config_secrets_keys.items():\n",
      "                if key in config_keys:\n",
      "                    config_keys[key] = value\n",
      "\n",
      "            print(\n",
      "                f\"get_service_keys() API request > config_keys: {config_keys}\",\n",
      "                \"\\n\",\n",
      "            )\n",
      "\n",
      "            # respond with the available required config keys and its saved secrets\n",
      "            return {\"config_keys\": config_keys}\n",
      "    except Exception as e:\n",
      "        print(\"DEBUG\", \"Error retreiving service keys:\", e)\n",
      "        raise HTTPException(\n",
      "            status_code=500, detail=f\"Error retreiving service keys: {e}\"\n",
      "        )\n",
      "\n",
      "\n",
      "@app.put(\"/api/service-keys\", tags=[\"update-service-keys\"])\n",
      "async def update_service_keys(keys: dict[str, str]):\n",
      "    \"\"\"\n",
      "    Update the service keys in the secret (git ignored) config file.\n",
      "\n",
      "    Args:\n",
      "        keys (dict[str, str]): A dictionary containing the service keys to be updated.\n",
      "\n",
      "    Returns:\n",
      "        dict: A dictionary containing a success message and the updated keys.\n",
      "    \"\"\"\n",
      "    try:\n",
      "        # Get the absolute path of the project's root directory\n",
      "        ROOT_DIR = abspath(join(dirname(__file__), \"..\", \"..\", \"..\"))\n",
      "        print(\"DEBUG\", \"ROOT_DIR:\", ROOT_DIR)\n",
      "\n",
      "        # define the path to the secret (git ignored) config file, which will hold the saved secrets (or placeholder values)\n",
      "        file_path_writeable = join(\n",
      "            ROOT_DIR, \"scripts\", \"similarity\", \"config.local.yml\"\n",
      "        )\n",
      "\n",
      "        # build a config file structured as a YAML dictionary, with the updated keys and value (secrets)\n",
      "        config_local_secrets = update_yaml_config(keys)\n",
      "\n",
      "        with open(file_path_writeable, \"w\") as file_write:\n",
      "            # write the config key with secrets to (git ignored) secret config file\n",
      "            yaml.dump(config_local_secrets, file_write)\n",
      "\n",
      "            # respond with a success message and the updated keys\n",
      "            return {\"message\": \"Config file updated successfully\", \"keys\": keys}\n",
      "    except Exception as e:\n",
      "        print(\"DEBUG\", \"Error updating service keys:\", e)\n",
      "        raise HTTPException(status_code=500, detail=f\"Error updating service keys: {e}\")\n",
      "\n",
      "from fastapi import UploadFile\n",
      "from os import listdir, unlink\n",
      "from os.path import join, isfile, abspath\n",
      "from typing import List\n",
      "from reportlab.pdfgen import canvas  # type: ignore\n",
      "from reportlab.lib.pagesizes import A4  # type: ignore\n",
      "from reportlab.lib.styles import getSampleStyleSheet  # type: ignore\n",
      "from reportlab.platypus import Paragraph  # type: ignore\n",
      "from ..schemas.resume_processor import Job\n",
      "\n",
      "\n",
      "def clear_directory_of_local_files(*path: str) -> None:\n",
      "    \"\"\"\n",
      "    Clears the directory at the specified path.\n",
      "\n",
      "    Args:\n",
      "      *path (str): The path to the directory to clear.\n",
      "    \"\"\"\n",
      "\n",
      "    full_path_to_dir = build_full_file_path(*path)\n",
      "\n",
      "    # Print the path to the directory to clear\n",
      "    print(f\"clear_directory() path > {full_path_to_dir}\", \"\\n\")\n",
      "\n",
      "    # Clear the directory at the specified path\n",
      "    for file in listdir(full_path_to_dir):\n",
      "        file_path = join(full_path_to_dir, file)\n",
      "        try:\n",
      "            # Check if the file exists, and contains .local., then delete it\n",
      "            if isfile(file_path) and \".local.\" in file_path:\n",
      "                unlink(file_path)\n",
      "                print(f\"clear_directory() deleted > {file_path}\", \"\\n\")\n",
      "        except Exception as e:\n",
      "            print(f\"clear_directory() error > {e}\", \"\\n\")\n",
      "\n",
      "\n",
      "def build_full_file_path(*path: str) -> str:\n",
      "    \"\"\"\n",
      "    Returns the full file path from the list of path segments.\n",
      "\n",
      "    Args:\n",
      "      *path (str): The path segments.\n",
      "\n",
      "    Returns:\n",
      "      str: The full file path.\n",
      "    \"\"\"\n",
      "\n",
      "    # Get the absolute path to the project base directory (step back 1 directory from the webapp/ directory)\n",
      "    project_absolute_path = abspath(\"./..\")\n",
      "    sub_path = join(*path)\n",
      "\n",
      "    # Print the path to the directory to clear\n",
      "    print(f\"build_full_file_path() path > {project_absolute_path=}\", \"\\n\")\n",
      "    print(f\"build_full_file_path() path > {sub_path=}\", \"\\n\")\n",
      "\n",
      "    # Return the full file path\n",
      "    return join(project_absolute_path, sub_path)\n",
      "\n",
      "\n",
      "def save_file_upload(file: UploadFile) -> str:\n",
      "    \"\"\"\n",
      "    Saves the uploaded file to the local filesystem.\n",
      "\n",
      "    Args:\n",
      "      file (UploadFile): The file to be saved.\n",
      "\n",
      "    Returns:\n",
      "      str: The path where the file is saved.\n",
      "    \"\"\"\n",
      "\n",
      "\n",
      "    # Clear the directory of local files\n",
      "    clear_directory_of_local_files(\"Data\", \"Resumes\")\n",
      "\n",
      "    # Get the file name and extension from the file name\n",
      "    file_name_parts = file.filename.split(\".\") if file.filename else []\n",
      "    file_name = file_name_parts[0] if len(file_name_parts) > 1 else \"resume\"\n",
      "    file_extension = f\".{file_name_parts[-1]}\" if len(file_name_parts) > 1 else \"\"\n",
      "\n",
      "    # Combine the file name and extension to create the full file name\n",
      "    full_file_name = f\"{file_name}.local{file_extension}\"\n",
      "\n",
      "    # Set the path where the file will be saved (from project base directory)\n",
      "    save_file_path = build_full_file_path(\"Data\", \"Resumes\", full_file_name)\n",
      "\n",
      "    # Save the file to the local filesystem\n",
      "    with open(save_file_path, \"wb\") as buffer:\n",
      "        buffer.write(file.file.read())\n",
      "\n",
      "    # Print the path where the file is saved\n",
      "    print(f\"save_file() file saved to > {save_file_path=}\", \"\\n\")\n",
      "\n",
      "    # Return the path where the file is saved\n",
      "    return save_file_path\n",
      "\n",
      "\n",
      "def save_job_uploads_to_pdfs(jobs: List[Job]):\n",
      "    # Clear the directory of local files\n",
      "    clear_directory_of_local_files(\"Data\", \"JobDescription\")\n",
      "\n",
      "    for job in jobs:\n",
      "        job_id = job.id\n",
      "        job_description = job.description or \"No description provided.\"\n",
      "\n",
      "        print(f\"save_job_uploads_to_pdfs() job_id > {job_id}\", \"\\n\")\n",
      "        print(f\"save_job_uploads_to_pdfs() job_description > {job_description}\", \"\\n\")\n",
      "\n",
      "        # blank_pdf_file_path = build_full_file_path(\n",
      "        #     \"webapp\", \"backend\", \"data\", \"blank.pdf\"\n",
      "        # )\n",
      "\n",
      "        job_desc_pdf_file_path = build_full_file_path(\n",
      "            \"Data\", \"JobDescription\", f\"{job_id}.local.pdf\"\n",
      "        )\n",
      "\n",
      "        # solution  to wrap text in a PDF sourced from: https://stackoverflow.com/a/74243699/5033801\n",
      "        # there may be a better solution to this, but this works for now\n",
      "        text_width = A4[0] / 1.25\n",
      "        text_height = A4[1] / 4\n",
      "        x = A4[0] / 10\n",
      "        y = A4[1] / 2.25\n",
      "        styles = getSampleStyleSheet()\n",
      "\n",
      "        pdf_canvas = canvas.Canvas(job_desc_pdf_file_path, pagesize=A4)\n",
      "\n",
      "        p = Paragraph(job_description, styles[\"Normal\"])  # type: ignore\n",
      "        p.wrapOn(pdf_canvas, text_width, text_height)  # type: ignore\n",
      "        p.drawOn(pdf_canvas, x, y)  # type: ignore\n",
      "\n",
      "        print(f\"save_job_uploads_to_pdfs() job_desc_pdf_file_path > {job_desc_pdf_file_path}\", \"\\n\")\n",
      "\n",
      "        pdf_canvas.save()\n",
      "\n",
      "import json\n",
      "import os\n",
      "from typing import List\n",
      "\n",
      "import networkx as nx\n",
      "import nltk\n",
      "import pandas as pd\n",
      "import plotly.express as px\n",
      "import plotly.graph_objects as go\n",
      "import streamlit as st\n",
      "from annotated_text import annotated_text, parameters\n",
      "from streamlit_extras import add_vertical_space as avs\n",
      "from streamlit_extras.badges import badge\n",
      "\n",
      "from scripts.similarity import get_similarity_score, find_path, read_config\n",
      "from scripts.utils import get_filenames_from_dir\n",
      "\n",
      "cwd = find_path('Resume-Matcher')\n",
      "config_path = os.path.join(cwd, \"scripts\", \"similarity\")\n",
      "\n",
      "try:\n",
      "    nltk.data.find('tokenizers/punkt')\n",
      "except LookupError:\n",
      "    nltk.download('punkt')\n",
      "\n",
      "parameters.SHOW_LABEL_SEPARATOR = False\n",
      "parameters.BORDER_RADIUS = 3\n",
      "parameters.PADDING = \"0.5 0.25rem\"\n",
      "\n",
      "\n",
      "def create_star_graph(nodes_and_weights, title):\n",
      "    # Create an empty graph\n",
      "    G = nx.Graph()\n",
      "\n",
      "    # Add the central node\n",
      "    central_node = \"resume\"\n",
      "    G.add_node(central_node)\n",
      "\n",
      "    # Add nodes and edges with weights to the graph\n",
      "    for node, weight in nodes_and_weights:\n",
      "        G.add_node(node)\n",
      "        G.add_edge(central_node, node, weight=weight * 100)\n",
      "\n",
      "    # Get position layout for nodes\n",
      "    pos = nx.spring_layout(G)\n",
      "\n",
      "    # Create edge trace\n",
      "    edge_x = []\n",
      "    edge_y = []\n",
      "    for edge in G.edges():\n",
      "        x0, y0 = pos[edge[0]]\n",
      "        x1, y1 = pos[edge[1]]\n",
      "        edge_x.extend([x0, x1, None])\n",
      "        edge_y.extend([y0, y1, None])\n",
      "\n",
      "    edge_trace = go.Scatter(x=edge_x, y=edge_y, line=dict(\n",
      "        width=0.5, color='#888'), hoverinfo='none', mode='lines')\n",
      "\n",
      "    # Create node trace\n",
      "    node_x = []\n",
      "    node_y = []\n",
      "    for node in G.nodes():\n",
      "        x, y = pos[node]\n",
      "        node_x.append(x)\n",
      "        node_y.append(y)\n",
      "\n",
      "    node_trace = go.Scatter(x=node_x, y=node_y, mode='markers', hoverinfo='text',\n",
      "                            marker=dict(showscale=True, colorscale='Rainbow', reversescale=True, color=[], size=10,\n",
      "                                        colorbar=dict(thickness=15, title='Node Connections', xanchor='left',\n",
      "                                                      titleside='right'), line_width=2))\n",
      "\n",
      "    # Color node points by number of connections\n",
      "    node_adjacencies = []\n",
      "    node_text = []\n",
      "    for node in G.nodes():\n",
      "        adjacencies = list(G.adj[node])  # changes here\n",
      "        node_adjacencies.append(len(adjacencies))\n",
      "        node_text.append(f'{node}<br># of connections: {len(adjacencies)}')\n",
      "\n",
      "    node_trace.marker.color = node_adjacencies\n",
      "    node_trace.text = node_text\n",
      "\n",
      "    # Create the figure\n",
      "    fig = go.Figure(data=[edge_trace, node_trace],\n",
      "                    layout=go.Layout(title=title, titlefont_size=16, showlegend=False,\n",
      "                                     hovermode='closest', margin=dict(b=20, l=5, r=5, t=40),\n",
      "                                     xaxis=dict(\n",
      "                                         showgrid=False, zeroline=False, showticklabels=False),\n",
      "                                     yaxis=dict(showgrid=False, zeroline=False, showticklabels=False)))\n",
      "\n",
      "    # Show the figure\n",
      "    st.plotly_chart(fig)\n",
      "\n",
      "\n",
      "def create_annotated_text(input_string: str, word_list: List[str], annotation: str, color_code: str):\n",
      "    # Tokenize the input string\n",
      "    tokens = nltk.word_tokenize(input_string)\n",
      "\n",
      "    # Convert the list to a set for quick lookups\n",
      "    word_set = set(word_list)\n",
      "\n",
      "    # Initialize an empty list to hold the annotated text\n",
      "    annotated_text = []\n",
      "\n",
      "    for token in tokens:\n",
      "        # Check if the token is in the set\n",
      "        if token in word_set:\n",
      "            # If it is, append a tuple with the token, annotation, and color code\n",
      "            annotated_text.append((token, annotation, color_code))\n",
      "        else:\n",
      "            # If it's not, just append the token as a string\n",
      "            annotated_text.append(token)\n",
      "\n",
      "    return annotated_text\n",
      "\n",
      "\n",
      "def read_json(filename):\n",
      "    with open(filename) as f:\n",
      "        data = json.load(f)\n",
      "    return data\n",
      "\n",
      "\n",
      "def tokenize_string(input_string):\n",
      "    tokens = nltk.word_tokenize(input_string)\n",
      "    return tokens\n",
      "\n",
      "\n",
      "st.image('Assets/img/header_image.png')\n",
      "\n",
      "st.title(':blue[Resume Matcher]')\n",
      "st.subheader(\n",
      "    'Free and Open Source ATS to help your resume pass the screening stage.')\n",
      "st.markdown(\n",
      "    \"Check the website [www.resumematcher.fyi](https://www.resumematcher.fyi/)\")\n",
      "st.markdown(\n",
      "    ' Give Resume Matcher a Star on [GitHub](https://github.com/srbhr/resume-matcher)')\n",
      "badge(type=\"github\", name=\"srbhr/Resume-Matcher\")\n",
      "\n",
      "st.text('For updates follow me on Twitter.')\n",
      "badge(type=\"twitter\", name=\"_srbhr_\")\n",
      "\n",
      "st.markdown(\n",
      "    'If you like the project and would like to further help in development please consider ')\n",
      "badge(type=\"buymeacoffee\", name=\"srbhr\")\n",
      "\n",
      "avs.add_vertical_space(5)\n",
      "\n",
      "resume_names = get_filenames_from_dir(\"Data/Processed/Resumes\")\n",
      "\n",
      "\n",
      "st.markdown(f\"##### There are {len(resume_names)} resumes present. Please select one from the menu below:\")\n",
      "output  = st.selectbox(f\"\", resume_names)\n",
      "    \n",
      "\n",
      "\n",
      "avs.add_vertical_space(5)\n",
      "\n",
      "# st.write(\"You have selected \", output, \" printing the resume\")\n",
      "selected_file = read_json(\"Data/Processed/Resumes/\" + output)\n",
      "\n",
      "avs.add_vertical_space(2)\n",
      "st.markdown(\"#### Parsed Resume Data\")\n",
      "st.caption(\n",
      "    \"This text is parsed from your resume. This is how it'll look like after getting parsed by an ATS.\")\n",
      "st.caption(\"Utilize this to understand how to make your resume ATS friendly.\")\n",
      "avs.add_vertical_space(3)\n",
      "# st.json(selected_file)\n",
      "st.write(selected_file[\"clean_data\"])\n",
      "\n",
      "avs.add_vertical_space(3)\n",
      "st.write(\"Now let's take a look at the extracted keywords from the resume.\")\n",
      "\n",
      "annotated_text(create_annotated_text(\n",
      "    selected_file[\"clean_data\"], selected_file[\"extracted_keywords\"],\n",
      "    \"KW\", \"#0B666A\"))\n",
      "\n",
      "avs.add_vertical_space(5)\n",
      "st.write(\"Now let's take a look at the extracted entities from the resume.\")\n",
      "\n",
      "# Call the function with your data\n",
      "create_star_graph(selected_file['keyterms'], \"Entities from Resume\")\n",
      "\n",
      "df2 = pd.DataFrame(selected_file['keyterms'], columns=[\"keyword\", \"value\"])\n",
      "\n",
      "# Create the dictionary\n",
      "keyword_dict = {}\n",
      "for keyword, value in selected_file['keyterms']:\n",
      "    keyword_dict[keyword] = value * 100\n",
      "\n",
      "fig = go.Figure(data=[go.Table(header=dict(values=[\"Keyword\", \"Value\"],\n",
      "                                           font=dict(size=12),\n",
      "                                           fill_color='#070A52'),\n",
      "                               cells=dict(values=[list(keyword_dict.keys()),\n",
      "                                                  list(keyword_dict.values())],\n",
      "                                          line_color='darkslategray',\n",
      "                                          fill_color='#6DA9E4'))\n",
      "                      ])\n",
      "st.plotly_chart(fig)\n",
      "\n",
      "st.divider()\n",
      "\n",
      "fig = px.treemap(df2, path=['keyword'], values='value',\n",
      "                 color_continuous_scale='Rainbow',\n",
      "                 title='Key Terms/Topics Extracted from your Resume')\n",
      "st.write(fig)\n",
      "\n",
      "avs.add_vertical_space(5)\n",
      "\n",
      "job_descriptions = get_filenames_from_dir(\"Data/Processed/JobDescription\")\n",
      "\n",
      "\n",
      "st.markdown(f\"##### There are {len(job_descriptions)} job descriptions present. Please select one from the menu below:\")\n",
      "output = st.selectbox(\"\", job_descriptions)\n",
      "\n",
      "\n",
      "avs.add_vertical_space(5)\n",
      "\n",
      "selected_jd = read_json(\n",
      "    \"Data/Processed/JobDescription/\" + output)\n",
      "\n",
      "avs.add_vertical_space(2)\n",
      "st.markdown(\"#### Job Description\")\n",
      "st.caption(\n",
      "    \"Currently in the pipeline I'm parsing this from PDF but it'll be from txt or copy paste.\")\n",
      "avs.add_vertical_space(3)\n",
      "# st.json(selected_file)\n",
      "st.write(selected_jd[\"clean_data\"])\n",
      "\n",
      "st.markdown(\"#### Common Words between Job Description and Resumes Highlighted.\")\n",
      "\n",
      "annotated_text(create_annotated_text(\n",
      "    selected_file[\"clean_data\"], selected_jd[\"extracted_keywords\"],\n",
      "    \"JD\", \"#F24C3D\"))\n",
      "\n",
      "st.write(\"Now let's take a look at the extracted entities from the job description.\")\n",
      "\n",
      "# Call the function with your data\n",
      "create_star_graph(selected_jd['keyterms'], \"Entities from Job Description\")\n",
      "\n",
      "df2 = pd.DataFrame(selected_jd['keyterms'], columns=[\"keyword\", \"value\"])\n",
      "\n",
      "# Create the dictionary\n",
      "keyword_dict = {}\n",
      "for keyword, value in selected_jd['keyterms']:\n",
      "    keyword_dict[keyword] = value * 100\n",
      "\n",
      "fig = go.Figure(data=[go.Table(header=dict(values=[\"Keyword\", \"Value\"],\n",
      "                                           font=dict(size=12),\n",
      "                                           fill_color='#070A52'),\n",
      "                               cells=dict(values=[list(keyword_dict.keys()),\n",
      "                                                  list(keyword_dict.values())],\n",
      "                                          line_color='darkslategray',\n",
      "                                          fill_color='#6DA9E4'))\n",
      "                      ])\n",
      "st.plotly_chart(fig)\n",
      "\n",
      "st.divider()\n",
      "\n",
      "fig = px.treemap(df2, path=['keyword'], values='value',\n",
      "                 color_continuous_scale='Rainbow',\n",
      "                 title='Key Terms/Topics Extracted from the selected Job Description')\n",
      "st.write(fig)\n",
      "\n",
      "avs.add_vertical_space(3)\n",
      "\n",
      "config_file_path = config_path + \"/config.yml\"\n",
      "if os.path.exists(config_file_path):\n",
      "    config_data = read_config(config_file_path)\n",
      "    if config_data:\n",
      "        print(\"Config file parsed successfully:\")\n",
      "        resume_string = ' '.join(selected_file[\"extracted_keywords\"])\n",
      "        jd_string = ' '.join(selected_jd[\"extracted_keywords\"])\n",
      "        result = get_similarity_score(resume_string, jd_string)\n",
      "        similarity_score = result[0][\"score\"]\n",
      "        st.write(\"Similarity Score obtained for the resume and job description is:\", similarity_score)\n",
      "else:\n",
      "    print(\"Config file does not exist.\")\n",
      "\n",
      "\n",
      "st.title(':blue[Resume Matcher]')\n",
      "st.subheader(\n",
      "    'Free and Open Source ATS to help your resume pass the screening stage.')\n",
      "st.markdown(\n",
      "    ' Give Resume Matcher a Star on [GitHub](https://github.com/srbhr/Resume-Matcher/)')\n",
      "badge(type=\"github\", name=\"srbhr/Resume-Matcher\")\n",
      "\n",
      "st.text('For updates follow me on Twitter.')\n",
      "badge(type=\"twitter\", name=\"_srbhr_\")\n",
      "\n",
      "st.markdown(\n",
      "    'If you like the project and would like to further help in development please consider ')\n",
      "badge(type=\"buymeacoffee\", name=\"srbhr\")\n",
      "\n",
      "import networkx as nx\n",
      "from typing import List\n",
      "import streamlit as st\n",
      "import pandas as pd\n",
      "import json\n",
      "import plotly.express as px\n",
      "import plotly.graph_objects as go\n",
      "from scripts.utils import get_filenames_from_dir\n",
      "from streamlit_extras import add_vertical_space as avs\n",
      "from annotated_text import annotated_text, parameters\n",
      "from streamlit_extras.badges import badge\n",
      "import nltk\n",
      "nltk.download('punkt')\n",
      "\n",
      "parameters.SHOW_LABEL_SEPARATOR = False\n",
      "parameters.BORDER_RADIUS = 3\n",
      "parameters.PADDING = \"0.5 0.25rem\"\n",
      "\n",
      "\n",
      "def create_star_graph(nodes_and_weights, title):\n",
      "    # Create an empty graph\n",
      "    G = nx.Graph()\n",
      "\n",
      "    # Add the central node\n",
      "    central_node = \"resume\"\n",
      "    G.add_node(central_node)\n",
      "\n",
      "    # Add nodes and edges with weights to the graph\n",
      "    for node, weight in nodes_and_weights:\n",
      "        G.add_node(node)\n",
      "        G.add_edge(central_node, node, weight=weight*100)\n",
      "\n",
      "    # Get position layout for nodes\n",
      "    pos = nx.spring_layout(G)\n",
      "\n",
      "    # Create edge trace\n",
      "    edge_x = []\n",
      "    edge_y = []\n",
      "    for edge in G.edges():\n",
      "        x0, y0 = pos[edge[0]]\n",
      "        x1, y1 = pos[edge[1]]\n",
      "        edge_x.extend([x0, x1, None])\n",
      "        edge_y.extend([y0, y1, None])\n",
      "\n",
      "    edge_trace = go.Scatter(x=edge_x, y=edge_y, line=dict(\n",
      "        width=0.5, color='#888'), hoverinfo='none', mode='lines')\n",
      "\n",
      "    # Create node trace\n",
      "    node_x = []\n",
      "    node_y = []\n",
      "    for node in G.nodes():\n",
      "        x, y = pos[node]\n",
      "        node_x.append(x)\n",
      "        node_y.append(y)\n",
      "\n",
      "    node_trace = go.Scatter(x=node_x, y=node_y, mode='markers', hoverinfo='text',\n",
      "                            marker=dict(showscale=True, colorscale='Rainbow', reversescale=True, color=[], size=10,\n",
      "                                        colorbar=dict(thickness=15, title='Node Connections', xanchor='left',\n",
      "                                                      titleside='right'), line_width=2))\n",
      "\n",
      "    # Color node points by number of connections\n",
      "    node_adjacencies = []\n",
      "    node_text = []\n",
      "    for node in G.nodes():\n",
      "        adjacencies = list(G.adj[node])  # changes here\n",
      "        node_adjacencies.append(len(adjacencies))\n",
      "        node_text.append(f'{node}<br># of connections: {len(adjacencies)}')\n",
      "\n",
      "    node_trace.marker.color = node_adjacencies\n",
      "    node_trace.text = node_text\n",
      "\n",
      "    # Create the figure\n",
      "    fig = go.Figure(data=[edge_trace, node_trace],\n",
      "                    layout=go.Layout(title=title, titlefont_size=16, showlegend=False,\n",
      "                                     hovermode='closest', margin=dict(b=20, l=5, r=5, t=40),\n",
      "                                     xaxis=dict(\n",
      "                                         showgrid=False, zeroline=False, showticklabels=False),\n",
      "                                     yaxis=dict(showgrid=False, zeroline=False, showticklabels=False)))\n",
      "\n",
      "    # Show the figure\n",
      "    st.plotly_chart(fig)\n",
      "\n",
      "\n",
      "def create_annotated_text(input_string: str, word_list: List[str], annotation: str, color_code: str):\n",
      "    # Tokenize the input string\n",
      "    tokens = nltk.word_tokenize(input_string)\n",
      "\n",
      "    # Convert the list to a set for quick lookups\n",
      "    word_set = set(word_list)\n",
      "\n",
      "    # Initialize an empty list to hold the annotated text\n",
      "    annotated_text = []\n",
      "\n",
      "    for token in tokens:\n",
      "        # Check if the token is in the set\n",
      "        if token in word_set:\n",
      "            # If it is, append a tuple with the token, annotation, and color code\n",
      "            annotated_text.append((token, annotation, color_code))\n",
      "        else:\n",
      "            # If it's not, just append the token as a string\n",
      "            annotated_text.append(token)\n",
      "\n",
      "    return annotated_text\n",
      "\n",
      "\n",
      "def read_json(filename):\n",
      "    with open(filename) as f:\n",
      "        data = json.load(f)\n",
      "    return data\n",
      "\n",
      "\n",
      "def tokenize_string(input_string):\n",
      "    tokens = nltk.word_tokenize(input_string)\n",
      "    return tokens\n",
      "\n",
      "\n",
      "st.image('Assets/img/header_image.png')\n",
      "\n",
      "st.title(':blue[Resume Matcher]')\n",
      "st.subheader(\n",
      "    'Free and Open Source ATS to help your resume pass the screening stage.')\n",
      "st.markdown(\n",
      "    \"Check the website [www.resumematcher.fyi](https://www.resumematcher.fyi/)\")\n",
      "st.markdown(\n",
      "    ' Give Resume Matcher a Star on [GitHub](https://github.com/srbhr/Resume-Matcher/)')\n",
      "badge(type=\"github\", name=\"srbhr/Resume-Matcher\")\n",
      "\n",
      "st.text('For updates follow me on Twitter.')\n",
      "badge(type=\"twitter\", name=\"_srbhr_\")\n",
      "\n",
      "st.markdown(\n",
      "    'If you like the project and would like to further help in development please consider ')\n",
      "badge(type=\"buymeacoffee\", name=\"srbhr\")\n",
      "\n",
      "avs.add_vertical_space(5)\n",
      "\n",
      "resume_names = get_filenames_from_dir(\"Data/Processed/Resumes\")\n",
      "\n",
      "output = st.selectbox(f\"There are {len(resume_names)} resumes present. Please select one from the menu below:\", resume_names)\n",
      "\n",
      "avs.add_vertical_space(5)\n",
      "\n",
      "selected_file = read_json(\"Data/Processed/Resumes/\"+output)\n",
      "\n",
      "avs.add_vertical_space(2)\n",
      "st.markdown(\"#### Parsed Resume Data\")\n",
      "st.caption(\n",
      "    \"This text is parsed from your resume. This is how it'll look like after getting parsed by an ATS.\")\n",
      "st.caption(\"Utilize this to understand how to make your resume ATS friendly.\")\n",
      "avs.add_vertical_space(3)\n",
      "# st.json(selected_file)\n",
      "st.write(selected_file[\"clean_data\"])\n",
      "\n",
      "avs.add_vertical_space(3)\n",
      "st.write(\"Now let's take a look at the extracted keywords from the resume.\")\n",
      "\n",
      "annotated_text(create_annotated_text(\n",
      "    selected_file[\"clean_data\"], selected_file[\"extracted_keywords\"],\n",
      "    \"KW\", \"#0B666A\"))\n",
      "\n",
      "avs.add_vertical_space(5)\n",
      "st.write(\"Now let's take a look at the extracted entities from the resume.\")\n",
      "\n",
      "# Call the function with your data\n",
      "create_star_graph(selected_file['keyterms'], \"Entities from Resume\")\n",
      "\n",
      "df2 = pd.DataFrame(selected_file['keyterms'], columns=[\"keyword\", \"value\"])\n",
      "\n",
      "# Create the dictionary\n",
      "keyword_dict = {}\n",
      "for keyword, value in selected_file['keyterms']:\n",
      "    keyword_dict[keyword] = value*100\n",
      "\n",
      "fig = go.Figure(data=[go.Table(header=dict(values=[\"Keyword\", \"Value\"],\n",
      "                                           font=dict(size=12),\n",
      "                                           fill_color='#070A52'),\n",
      "                               cells=dict(values=[list(keyword_dict.keys()),\n",
      "                                                  list(keyword_dict.values())],\n",
      "                                          line_color='darkslategray',\n",
      "                                          fill_color='#6DA9E4'))\n",
      "                      ])\n",
      "st.plotly_chart(fig)\n",
      "\n",
      "st.divider()\n",
      "\n",
      "fig = px.treemap(df2, path=['keyword'], values='value',\n",
      "                 color_continuous_scale='Rainbow',\n",
      "                 title='Key Terms/Topics Extracted from your Resume')\n",
      "st.write(fig)\n",
      "\n",
      "avs.add_vertical_space(5)\n",
      "\n",
      "job_descriptions = get_filenames_from_dir(\"Data/Processed/JobDescription\")\n",
      "\n",
      "output = st.selectbox(f\"There are {len(job_descriptions)} job descriptions present. Please select one from the menu below:\", job_descriptions)\n",
      "\n",
      "avs.add_vertical_space(5)\n",
      "\n",
      "selected_jd = read_json(\n",
      "    \"Data/Processed/JobDescription/\"+output)\n",
      "\n",
      "avs.add_vertical_space(2)\n",
      "st.markdown(\"#### Job Description\")\n",
      "st.caption(\n",
      "    \"Currently in the pipeline I'm parsing this from PDF but it'll be from txt or copy paste.\")\n",
      "avs.add_vertical_space(3)\n",
      "# st.json(selected_file)\n",
      "st.write(selected_jd[\"clean_data\"])\n",
      "\n",
      "st.markdown(\"#### Common Words between Job Description and Resumes Highlighted.\")\n",
      "\n",
      "annotated_text(create_annotated_text(\n",
      "    selected_file[\"clean_data\"], selected_jd[\"extracted_keywords\"],\n",
      "    \"JD\", \"#F24C3D\"))\n",
      "\n",
      "st.write(\"Now let's take a look at the extracted entities from the job description.\")\n",
      "\n",
      "# Call the function with your data\n",
      "create_star_graph(selected_jd['keyterms'], \"Entities from Job Description\")\n",
      "\n",
      "df2 = pd.DataFrame(selected_jd['keyterms'], columns=[\"keyword\", \"value\"])\n",
      "\n",
      "# Create the dictionary\n",
      "keyword_dict = {}\n",
      "for keyword, value in selected_jd['keyterms']:\n",
      "    keyword_dict[keyword] = value*100\n",
      "\n",
      "fig = go.Figure(data=[go.Table(header=dict(values=[\"Keyword\", \"Value\"],\n",
      "                                           font=dict(size=12),\n",
      "                                           fill_color='#070A52'),\n",
      "                               cells=dict(values=[list(keyword_dict.keys()),\n",
      "                                                  list(keyword_dict.values())],\n",
      "                                          line_color='darkslategray',\n",
      "                                          fill_color='#6DA9E4'))\n",
      "                      ])\n",
      "st.plotly_chart(fig)\n",
      "\n",
      "st.divider()\n",
      "\n",
      "fig = px.treemap(df2, path=['keyword'], values='value',\n",
      "                 color_continuous_scale='Rainbow',\n",
      "                 title='Key Terms/Topics Extracted from the selected Job Description')\n",
      "st.write(fig)\n",
      "\n",
      "avs.add_vertical_space(5)\n",
      "\n",
      "st.divider()\n",
      "\n",
      "st.markdown(\"## Vector Similarity Scores\")\n",
      "st.caption(\"Powered by Qdrant Vector Search\")\n",
      "st.info(\"These are pre-computed queries\", icon=\"\")\n",
      "st.warning(\n",
      "    \"Running Qdrant or Sentence Transformers without having capacity is not recommended\", icon=\"\")\n",
      "\n",
      "\n",
      "# Your data\n",
      "data = [\n",
      "    {'text': \"{'resume': 'Alfred Pennyworth\",\n",
      "        'query': 'Job Description Product Manager', 'score': 0.62658},\n",
      "    {'text': \"{'resume': 'Barry Allen\",\n",
      "        'query': 'Job Description Product Manager', 'score': 0.43777737},\n",
      "    {'text': \"{'resume': 'Bruce Wayne \",\n",
      "        'query': 'Job Description Product Manager', 'score': 0.39835533},\n",
      "    {'text': \"{'resume': 'JOHN DOE\",\n",
      "        'query': 'Job Description Product Manager', 'score': 0.3915512},\n",
      "    {'text': \"{'resume': 'Harvey Dent\",\n",
      "        'query': 'Job Description Product Manager', 'score': 0.3519544},\n",
      "    {'text': \"{'resume': 'Barry Allen\",\n",
      "        'query': 'Job Description Senior Full Stack Engineer', 'score': 0.6541866},\n",
      "    {'text': \"{'resume': 'Alfred Pennyworth\",\n",
      "        'query': 'Job Description Senior Full Stack Engineer', 'score': 0.59806436},\n",
      "    {'text': \"{'resume': 'JOHN DOE\",\n",
      "        'query': 'Job Description Senior Full Stack Engineer', 'score': 0.5951386},\n",
      "    {'text': \"{'resume': 'Bruce Wayne \",\n",
      "        'query': 'Job Description Senior Full Stack Engineer', 'score': 0.57700855},\n",
      "    {'text': \"{'resume': 'Harvey Dent\",\n",
      "        'query': 'Job Description Senior Full Stack Engineer', 'score': 0.38489106},\n",
      "    {'text': \"{'resume': 'Barry Allen\",\n",
      "        'query': 'Job Description Front End Engineer', 'score': 0.76813436},\n",
      "    {'text': \"{'resume': 'Bruce Wayne'\",\n",
      "        'query': 'Job Description Front End Engineer', 'score': 0.60440844},\n",
      "    {'text': \"{'resume': 'JOHN DOE\",\n",
      "        'query': 'Job Description Front End Engineer', 'score': 0.56080043},\n",
      "    {'text': \"{'resume': 'Alfred Pennyworth\",\n",
      "        'query': 'Job Description Front End Engineer', 'score': 0.5395049},\n",
      "    {'text': \"{'resume': 'Harvey Dent\",\n",
      "        'query': 'Job Description Front End Engineer', 'score': 0.3859515},\n",
      "    {'text': \"{'resume': 'JOHN DOE\",\n",
      "        'query': 'Job Description Java Developer', 'score': 0.5449441},\n",
      "    {'text': \"{'resume': 'Alfred Pennyworth\",\n",
      "        'query': 'Job Description Java Developer', 'score': 0.53476423},\n",
      "    {'text': \"{'resume': 'Barry Allen\",\n",
      "        'query': 'Job Description Java Developer', 'score': 0.5313871},\n",
      "    {'text': \"{'resume': 'Bruce Wayne \",\n",
      "        'query': 'Job Description Java Developer', 'score': 0.44446343},\n",
      "    {'text': \"{'resume': 'Harvey Dent\",\n",
      "        'query': 'Job Description Java Developer', 'score': 0.3616274}\n",
      "]\n",
      "\n",
      "# Create a DataFrame\n",
      "df = pd.DataFrame(data)\n",
      "\n",
      "# Create different DataFrames based on the query and sort by score\n",
      "df1 = df[df['query'] ==\n",
      "         'Job Description Product Manager'].sort_values(by='score', ascending=False)\n",
      "df2 = df[df['query'] ==\n",
      "         'Job Description Senior Full Stack Engineer'].sort_values(by='score', ascending=False)\n",
      "df3 = df[df['query'] == 'Job Description Front End Engineer'].sort_values(\n",
      "    by='score', ascending=False)\n",
      "df4 = df[df['query'] == 'Job Description Java Developer'].sort_values(\n",
      "    by='score', ascending=False)\n",
      "\n",
      "\n",
      "def plot_df(df, title):\n",
      "    fig = px.bar(df, x='text', y=df['score']*100, title=title)\n",
      "    st.plotly_chart(fig)\n",
      "\n",
      "\n",
      "st.markdown(\"### Bar plots of scores based on similarity to Job Description.\")\n",
      "\n",
      "st.subheader(\":blue[Legend]\")\n",
      "st.text(\"Alfred Pennyworth :  Product Manager\")\n",
      "st.text(\"Barry Allen :  Front End Developer\")\n",
      "st.text(\"Harvey Dent :  Machine Learning Engineer\")\n",
      "st.text(\"Bruce Wayne :  Fullstack Developer (MERN)\")\n",
      "st.text(\"John Doe :  Fullstack Developer (Java)\")\n",
      "\n",
      "\n",
      "plot_df(df1, 'Job Description Product Manager 10+ Years of Exper')\n",
      "plot_df(df2, 'Job Description Senior Full Stack Engineer 5+ Year')\n",
      "plot_df(df3, 'Job Description Front End Engineer 2 Years of Expe')\n",
      "plot_df(df4, 'Job Description Java Developer 3 Years of Experien')\n",
      "\n",
      "\n",
      "avs.add_vertical_space(3)\n",
      "\n",
      "st.markdown(\n",
      "    ' Give Resume Matcher a Star on [GitHub](https://github.com/srbhr/Resume-Matcher/)')\n",
      "badge(type=\"github\", name=\"srbhr/Resume-Matcher\")\n",
      "\n",
      "st.text('For updates follow me on Twitter.')\n",
      "badge(type=\"twitter\", name=\"_srbhr_\")\n",
      "\n",
      "st.markdown(\n",
      "    'If you like the project and would like to further help in development please consider ')\n",
      "badge(type=\"buymeacoffee\", name=\"srbhr\")\n",
      "\n",
      "import logging\n",
      "from os import listdir\n",
      "from os.path import isfile, join\n",
      "\n",
      "import easygui\n",
      "import requests\n",
      "from bs4 import BeautifulSoup\n",
      "from pathvalidate import sanitize_filename\n",
      "from xhtml2pdf import pisa\n",
      "\n",
      "'''\n",
      "This script takes a LinkedIn job posting URL\n",
      "and converts the description to a PDF file.\n",
      "The PDF file is saved in the Data/JobDescription folder.\n",
      "The name will be OrgName__Job Title_X.pdf, where X is the number of files in the folder.\n",
      "\n",
      "IMPORTANT: Make sure the URL is to the actual job description,\n",
      "and not the job search page.\n",
      "'''\n",
      "\n",
      "\n",
      "def linkedin_to_pdf(job_url: str):\n",
      "\n",
      "    job_path = \"Data/JobDescription/\"\n",
      "    job_description = \"\"\n",
      "    files_number = len([f for f in listdir(job_path) if isfile(join(job_path, f))])\n",
      "\n",
      "    try:\n",
      "        page = requests.get(job_url)\n",
      "\n",
      "        if page.status_code != 200:\n",
      "            print(f\"Failed to retrieve the job posting at {job_url}. Status code: {page.status_code}\")\n",
      "            return\n",
      "\n",
      "        # Parse the HTML content of the job posting using BeautifulSoup\n",
      "        soup = BeautifulSoup(page.text, 'html.parser')\n",
      "\n",
      "        # Find the job title element and get the text\n",
      "        job_title = soup.find('h1', {'class': 'topcard__title'}).text.strip()\n",
      "\n",
      "        # Find the organization name element (try both selectors)\n",
      "        organization_element = soup.find('span', {'class': 'topcard__flavor'})\n",
      "\n",
      "        if not organization_element:\n",
      "            organization_element = soup.find('a', {'class': 'topcard__org-name-link'})\n",
      "\n",
      "        # Extract the organization name\n",
      "        organization = organization_element.text.strip()\n",
      "\n",
      "        # Find the job description element\n",
      "        job_description_element = soup.find('div', {'class': 'show-more-less-html__markup'})\n",
      "\n",
      "        # Extract the job description and concatenate its elements\n",
      "        if job_description_element:\n",
      "            for element in job_description_element.contents:\n",
      "                job_description += str(element)\n",
      "\n",
      "        # Set file_path and sanitize organization name and job title\n",
      "        file_path = f\"{job_path}{sanitize_filename(organization + '__' + job_title)}_{files_number}.pdf\"\n",
      "\n",
      "        # Create a PDF file and write the job description to it\n",
      "        with open(file_path, 'wb') as pdf_file:\n",
      "            pisa.CreatePDF(job_description, dest=pdf_file, encoding='utf-8')\n",
      "\n",
      "        logging.info(\"PDF saved to \" + file_path)\n",
      "\n",
      "    except Exception as e:\n",
      "        logging.error(f\"Could not get the description from the URL: {job_url}\")\n",
      "        logging.error(e)\n",
      "        exit()\n",
      "\n",
      "\n",
      "if __name__ == \"__main__\":\n",
      "    url = easygui.enterbox(\"Enter the URL of the LinkedIn Job Posting:\").strip()\n",
      "    linkedin_to_pdf(url)\n",
      "# Import necessary libraries\n",
      "import json\n",
      "import os\n",
      "from typing import List\n",
      "\n",
      "import networkx as nx\n",
      "import nltk\n",
      "import pandas as pd\n",
      "import plotly.express as px\n",
      "import plotly.graph_objects as go\n",
      "import streamlit as st\n",
      "from annotated_text import annotated_text, parameters\n",
      "from streamlit_extras import add_vertical_space as avs\n",
      "from streamlit_extras.badges import badge\n",
      "\n",
      "from scripts import ResumeProcessor, JobDescriptionProcessor\n",
      "from scripts.ReadPdf import read_single_pdf\n",
      "from scripts.similarity import get_similarity_score, find_path, read_config\n",
      "from scripts.parsers import ParseResume\n",
      "from scripts.parsers import ParseJobDesc\n",
      "from scripts.utils import get_filenames_from_dir\n",
      "\n",
      "# Set page configuration\n",
      "st.set_page_config(page_title='Resume Matcher', page_icon=\"Assets/img/favicon.ico\", initial_sidebar_state='auto', layout='wide')\n",
      "\n",
      "# Find the current working directory and configuration path\n",
      "cwd = find_path('Resume-Matcher')\n",
      "config_path = os.path.join(cwd, \"scripts\", \"similarity\")\n",
      "\n",
      "# Check if NLTK punkt data is available, if not, download it\n",
      "try:\n",
      "    nltk.data.find('tokenizers/punkt')\n",
      "except LookupError:\n",
      "    nltk.download('punkt')\n",
      "\n",
      "# Set some visualization parameters using the annotated_text library\n",
      "parameters.SHOW_LABEL_SEPARATOR = False\n",
      "parameters.BORDER_RADIUS = 3\n",
      "parameters.PADDING = \"0.5 0.25rem\"\n",
      "\n",
      "\n",
      "# Function to set session state variables\n",
      "def update_session_state(key, val):\n",
      "    st.session_state[key] = val\n",
      "\n",
      "\n",
      "# Function to delete all files in a directory\n",
      "def delete_from_dir(filepath: str) -> bool:\n",
      "    try:\n",
      "        for file in os.scandir(filepath):\n",
      "            os.remove(file.path)\n",
      "\n",
      "        return True\n",
      "    except OSError as error:\n",
      "        print(f\"Exception: {error}\")\n",
      "        return False\n",
      "\n",
      "\n",
      "# Function to create a star-shaped graph visualization\n",
      "def create_star_graph(nodes_and_weights, title):\n",
      "    \"\"\"\n",
      "    Create a star-shaped graph visualization.\n",
      "\n",
      "    Args:\n",
      "        nodes_and_weights (list): List of tuples containing nodes and their weights.\n",
      "        title (str): Title for the graph.\n",
      "\n",
      "    Returns:\n",
      "        None\n",
      "    \"\"\"\n",
      "    # Create an empty graph\n",
      "    graph = nx.Graph()\n",
      "\n",
      "    # Add the central node\n",
      "    central_node = \"resume\"\n",
      "    graph.add_node(central_node)\n",
      "\n",
      "    # Add nodes and edges with weights to the graph\n",
      "    for node, weight in nodes_and_weights:\n",
      "        graph.add_node(node)\n",
      "        graph.add_edge(central_node, node, weight=weight * 100)\n",
      "\n",
      "    # Get position layout for nodes\n",
      "    pos = nx.spring_layout(graph)\n",
      "\n",
      "    # Create edge trace\n",
      "    edge_x = []\n",
      "    edge_y = []\n",
      "    for edge in graph.edges():\n",
      "        x0, y0 = pos[edge[0]]\n",
      "        x1, y1 = pos[edge[1]]\n",
      "        edge_x.extend([x0, x1, None])\n",
      "        edge_y.extend([y0, y1, None])\n",
      "\n",
      "    edge_trace = go.Scatter(x=edge_x, y=edge_y, line=dict(\n",
      "        width=0.5, color='#888'), hoverinfo='none', mode='lines')\n",
      "\n",
      "    # Create node trace\n",
      "    node_x = []\n",
      "    node_y = []\n",
      "    for node in graph.nodes():\n",
      "        x, y = pos[node]\n",
      "        node_x.append(x)\n",
      "        node_y.append(y)\n",
      "\n",
      "    node_trace = go.Scatter(x=node_x, y=node_y, mode='markers', hoverinfo='text',\n",
      "                            marker=dict(showscale=True, colorscale='Rainbow', reversescale=True, color=[], size=10,\n",
      "                                        colorbar=dict(thickness=15, title='Node Connections', xanchor='left',\n",
      "                                                      titleside='right'), line_width=2))\n",
      "\n",
      "    # Color node points by number of connections\n",
      "    node_adjacencies = []\n",
      "    node_text = []\n",
      "    for node in graph.nodes():\n",
      "        adjacencies = list(graph.adj[node])  # Changes here\n",
      "        node_adjacencies.append(len(adjacencies))\n",
      "        node_text.append(f'{node}<br># of connections: {len(adjacencies)}')\n",
      "\n",
      "    node_trace.marker.color = node_adjacencies\n",
      "    node_trace.text = node_text\n",
      "\n",
      "    # Create the figure\n",
      "    figure = go.Figure(data=[edge_trace, node_trace],\n",
      "                       layout=go.Layout(title=title, titlefont=dict(size=16), showlegend=False,\n",
      "                                        hovermode='closest', margin=dict(b=20, l=5, r=5, t=40),\n",
      "                                        xaxis=dict(\n",
      "                                            showgrid=False, zeroline=False, showticklabels=False),\n",
      "                                        yaxis=dict(showgrid=False, zeroline=False, showticklabels=False)))\n",
      "\n",
      "    # Show the figure\n",
      "    st.plotly_chart(figure, use_container_width=True)\n",
      "\n",
      "\n",
      "# Function to create annotated text with highlighting\n",
      "def create_annotated_text(input_string: str, word_list: List[str], annotation: str, color_code: str):\n",
      "    \"\"\"\n",
      "    Create annotated text with highlighted keywords.\n",
      "\n",
      "    Args:\n",
      "        input_string (str): The input text.\n",
      "        word_list (List[str]): List of keywords to be highlighted.\n",
      "        annotation (str): Annotation label for highlighted keywords.\n",
      "        color_code (str): Color code for highlighting.\n",
      "\n",
      "    Returns:\n",
      "        List: Annotated text with highlighted keywords.\n",
      "    \"\"\"\n",
      "    # Tokenize the input string\n",
      "    tokens = nltk.word_tokenize(input_string)\n",
      "\n",
      "    # Convert the list to a set for quick lookups\n",
      "    word_set = set(word_list)\n",
      "\n",
      "    # Initialize an empty list to hold the annotated text\n",
      "    ret_annotated_text = []\n",
      "\n",
      "    for token in tokens:\n",
      "        # Check if the token is in the set\n",
      "        if token in word_set:\n",
      "            # If it is, append a tuple with the token, annotation, and color code\n",
      "            ret_annotated_text.append((token, annotation, color_code))\n",
      "        else:\n",
      "            # If it's not, just append the token as a string\n",
      "            ret_annotated_text.append(token)\n",
      "\n",
      "    return ret_annotated_text\n",
      "\n",
      "\n",
      "# Function to read JSON data from a file\n",
      "def read_json(filename):\n",
      "    \"\"\"\n",
      "    Read JSON data from a file.\n",
      "\n",
      "    Args:\n",
      "        filename (str): The path to the JSON file.\n",
      "\n",
      "    Returns:\n",
      "        dict: The JSON data.\n",
      "    \"\"\"\n",
      "    with open(filename) as f:\n",
      "        data = json.load(f)\n",
      "    return data\n",
      "\n",
      "\n",
      "# Function to tokenize a string\n",
      "def tokenize_string(input_string):\n",
      "    \"\"\"\n",
      "    Tokenize a string into words.\n",
      "\n",
      "    Args:\n",
      "        input_string (str): The input string.\n",
      "\n",
      "    Returns:\n",
      "        List[str]: List of tokens.\n",
      "    \"\"\"\n",
      "    tokens = nltk.word_tokenize(input_string)\n",
      "    return tokens\n",
      "\n",
      "\n",
      "# Cleanup processed resume / job descriptions\n",
      "delete_from_dir(os.path.join(cwd, \"Data\", \"Processed\", \"Resumes\"))\n",
      "delete_from_dir(os.path.join(cwd, \"Data\", \"Processed\", \"JobDescription\"))\n",
      "\n",
      "# Set default session states for first run\n",
      "if \"resumeUploaded\" not in st.session_state.keys():\n",
      "    update_session_state(\"resumeUploaded\", \"Pending\")\n",
      "    update_session_state(\"resumePath\", \"\")\n",
      "if \"jobDescriptionUploaded\" not in st.session_state.keys():\n",
      "    update_session_state(\"jobDescriptionUploaded\", \"Pending\")\n",
      "    update_session_state(\"jobDescriptionPath\", \"\")\n",
      "\n",
      "# Display the main title and sub-headers\n",
      "st.title(':blue[Resume Matcher]')\n",
      "with st.sidebar:\n",
      "    st.image('Assets/img/header_image.png')\n",
      "    st.subheader('Free and Open Source ATS to help your resume pass the screening stage.')\n",
      "    st.markdown('Check the website [www.resumematcher.fyi](https://www.resumematcher.fyi/)')\n",
      "    st.markdown('Give Resume Matcher a  on [GitHub](https://github.com/srbhr/resume-matcher)')\n",
      "    badge(type=\"github\", name=\"srbhr/Resume-Matcher\")\n",
      "    st.markdown('For updates follow me on Twitter.')\n",
      "    badge(type=\"twitter\", name=\"_srbhr_\")\n",
      "    st.markdown('If you like the project and would like to further help in development please consider ')\n",
      "    badge(type=\"buymeacoffee\", name=\"srbhr\")\n",
      "\n",
      "st.divider()\n",
      "avs.add_vertical_space(1)\n",
      "\n",
      "with st.container():\n",
      "    resumeCol, jobDescriptionCol = st.columns(2)\n",
      "    with resumeCol:\n",
      "        uploaded_Resume = st.file_uploader(\"Choose a Resume\", type=\"pdf\")\n",
      "        if uploaded_Resume is not None:\n",
      "            if st.session_state[\"resumeUploaded\"] == \"Pending\":\n",
      "                save_path_resume = os.path.join(cwd, \"Data\", \"Resumes\", uploaded_Resume.name)\n",
      "\n",
      "                with open(save_path_resume, mode='wb') as w:\n",
      "                    w.write(uploaded_Resume.getvalue())\n",
      "\n",
      "                if os.path.exists(save_path_resume):\n",
      "                    st.toast(f'File {uploaded_Resume.name} is successfully saved!', icon=\"\")\n",
      "                    update_session_state(\"resumeUploaded\", \"Uploaded\")\n",
      "                    update_session_state(\"resumePath\", save_path_resume)\n",
      "        else:\n",
      "            update_session_state(\"resumeUploaded\", \"Pending\")\n",
      "            update_session_state(\"resumePath\", \"\")\n",
      "\n",
      "    with jobDescriptionCol:\n",
      "        uploaded_JobDescription = st.file_uploader(\"Choose a Job Description\", type=\"pdf\")\n",
      "        if uploaded_JobDescription is not None:\n",
      "            if st.session_state[\"jobDescriptionUploaded\"] == \"Pending\":\n",
      "                save_path_jobDescription = os.path.join(cwd, \"Data\", \"JobDescription\", uploaded_JobDescription.name)\n",
      "\n",
      "                with open(save_path_jobDescription, mode='wb') as w:\n",
      "                    w.write(uploaded_JobDescription.getvalue())\n",
      "\n",
      "                if os.path.exists(save_path_jobDescription):\n",
      "                    st.toast(f'File {uploaded_JobDescription.name} is successfully saved!', icon=\"\")\n",
      "                    update_session_state(\"jobDescriptionUploaded\", \"Uploaded\")\n",
      "                    update_session_state(\"jobDescriptionPath\", save_path_jobDescription)\n",
      "        else:\n",
      "            update_session_state(\"jobDescriptionUploaded\", \"Pending\")\n",
      "            update_session_state(\"jobDescriptionPath\", \"\")\n",
      "\n",
      "with st.spinner('Please wait...'):\n",
      "    if (uploaded_Resume is not None and\n",
      "            st.session_state[\"jobDescriptionUploaded\"] == \"Uploaded\" and\n",
      "            uploaded_JobDescription is not None and\n",
      "            st.session_state[\"jobDescriptionUploaded\"] == \"Uploaded\"):\n",
      "\n",
      "        resumeProcessor = ParseResume(read_single_pdf(st.session_state[\"resumePath\"]))\n",
      "        jobDescriptionProcessor = ParseJobDesc(read_single_pdf(st.session_state[\"jobDescriptionPath\"]))\n",
      "\n",
      "        # Resume / JD output\n",
      "        selected_file = resumeProcessor.get_JSON()\n",
      "        selected_jd = jobDescriptionProcessor.get_JSON()\n",
      "\n",
      "        # Add containers for each row to avoid overlap\n",
      "        with st.container():\n",
      "            resumeCol, jobDescriptionCol = st.columns(2)\n",
      "            with resumeCol:\n",
      "                with st.expander(\"Parsed Resume Data\"):\n",
      "                    st.caption(\n",
      "                        \"This text is parsed from your resume. This is how it'll look like after getting parsed by an \"\n",
      "                        \"ATS.\")\n",
      "                    st.caption(\"Utilize this to understand how to make your resume ATS friendly.\")\n",
      "                    avs.add_vertical_space(3)\n",
      "                    st.write(selected_file[\"clean_data\"])\n",
      "\n",
      "            with jobDescriptionCol:\n",
      "                with st.expander(\"Parsed Job Description\"):\n",
      "                    st.caption(\n",
      "                        \"Currently in the pipeline I'm parsing this from PDF but it'll be from txt or copy paste.\")\n",
      "                    avs.add_vertical_space(3)\n",
      "                    st.write(selected_jd[\"clean_data\"])\n",
      "\n",
      "        with st.container():\n",
      "            resumeCol, jobDescriptionCol = st.columns(2)\n",
      "            with resumeCol:\n",
      "                with st.expander(\"Extracted Keywords\"):\n",
      "                    st.write(\"Now let's take a look at the extracted keywords from the resume.\")\n",
      "                    annotated_text(create_annotated_text(\n",
      "                        selected_file[\"clean_data\"], selected_file[\"extracted_keywords\"],\n",
      "                        \"KW\", \"#0B666A\"))\n",
      "            with jobDescriptionCol:\n",
      "                with st.expander(\"Extracted Keywords\"):\n",
      "                    st.write(\"Now let's take a look at the extracted keywords from the job description.\")\n",
      "                    annotated_text(create_annotated_text(\n",
      "                        selected_jd[\"clean_data\"], selected_jd[\"extracted_keywords\"],\n",
      "                        \"KW\", \"#0B666A\"))\n",
      "\n",
      "        with st.container():\n",
      "            resumeCol, jobDescriptionCol = st.columns(2)\n",
      "            with resumeCol:\n",
      "                with st.expander(\"Extracted Entities\"):\n",
      "                    st.write(\"Now let's take a look at the extracted entities from the resume.\")\n",
      "\n",
      "                    # Call the function with your data\n",
      "                    create_star_graph(selected_file['keyterms'], \"Entities from Resume\")\n",
      "            with jobDescriptionCol:\n",
      "                with st.expander(\"Extracted Entities\"):\n",
      "                    st.write(\"Now let's take a look at the extracted entities from the job description.\")\n",
      "\n",
      "                    # Call the function with your data\n",
      "                    create_star_graph(selected_jd['keyterms'], \"Entities from Job Description\")\n",
      "\n",
      "        with st.container():\n",
      "            resumeCol, jobDescriptionCol = st.columns(2)\n",
      "            with resumeCol:\n",
      "                with st.expander(\"Keywords & Values\"):\n",
      "                    df1 = pd.DataFrame(selected_file['keyterms'], columns=[\"keyword\", \"value\"])\n",
      "\n",
      "                    # Create the dictionary\n",
      "                    keyword_dict = {}\n",
      "                    for keyword, value in selected_file['keyterms']:\n",
      "                        keyword_dict[keyword] = value * 100\n",
      "\n",
      "                    fig = go.Figure(data=[go.Table(header=dict(values=[\"Keyword\", \"Value\"],\n",
      "                                                               font=dict(size=12, color=\"white\"),\n",
      "                                                               fill_color='#1d2078'),\n",
      "                                                   cells=dict(values=[list(keyword_dict.keys()),\n",
      "                                                                      list(keyword_dict.values())],\n",
      "                                                              line_color='darkslategray',\n",
      "                                                              fill_color='#6DA9E4'))\n",
      "                                          ])\n",
      "                    st.plotly_chart(fig, use_container_width=True)\n",
      "            with jobDescriptionCol:\n",
      "                with st.expander(\"Keywords & Values\"):\n",
      "                    df2 = pd.DataFrame(selected_jd['keyterms'], columns=[\"keyword\", \"value\"])\n",
      "\n",
      "                    # Create the dictionary\n",
      "                    keyword_dict = {}\n",
      "                    for keyword, value in selected_jd['keyterms']:\n",
      "                        keyword_dict[keyword] = value * 100\n",
      "\n",
      "                    fig = go.Figure(data=[go.Table(header=dict(values=[\"Keyword\", \"Value\"],\n",
      "                                                               font=dict(size=12, color=\"white\"),\n",
      "                                                               fill_color='#1d2078'),\n",
      "                                                   cells=dict(values=[list(keyword_dict.keys()),\n",
      "                                                                      list(keyword_dict.values())],\n",
      "                                                              line_color='darkslategray',\n",
      "                                                              fill_color='#6DA9E4'))\n",
      "                                          ])\n",
      "                    st.plotly_chart(fig, use_container_width=True)\n",
      "\n",
      "        with st.container():\n",
      "            resumeCol, jobDescriptionCol = st.columns(2)\n",
      "            with resumeCol:\n",
      "                with st.expander(\"Key Topics\"):\n",
      "                    fig = px.treemap(df1, path=['keyword'], values='value',\n",
      "                                     color_continuous_scale='Rainbow',\n",
      "                                     title='Key Terms/Topics Extracted from your Resume')\n",
      "                    st.plotly_chart(fig, use_container_width=True)\n",
      "\n",
      "            with jobDescriptionCol:\n",
      "                with st.expander(\"Key Topics\"):\n",
      "                    fig = px.treemap(df2, path=['keyword'], values='value',\n",
      "                                     color_continuous_scale='Rainbow',\n",
      "                                     title='Key Terms/Topics Extracted from Job Description')\n",
      "                    st.plotly_chart(fig, use_container_width=True)\n",
      "\n",
      "        avs.add_vertical_space(2)\n",
      "        config_file_path = config_path + \"/config.yml\"\n",
      "        if os.path.exists(config_file_path):\n",
      "            config_data = read_config(config_file_path)\n",
      "            if config_data:\n",
      "                print(\"Config file parsed successfully:\")\n",
      "                resume_string = ' '.join(selected_file[\"extracted_keywords\"])\n",
      "                jd_string = ' '.join(selected_jd[\"extracted_keywords\"])\n",
      "                result = get_similarity_score(resume_string, jd_string)\n",
      "                similarity_score = round(result[0][\"score\"] * 100, 2)\n",
      "\n",
      "                # Default color to green\n",
      "                score_color = \"green\"\n",
      "                if similarity_score < 60:\n",
      "                    score_color = \"red\"\n",
      "                elif 60 <= similarity_score < 75:\n",
      "                    score_color = \"orange\"\n",
      "\n",
      "                st.markdown(f'Similarity Score obtained for the resume and job description is '\n",
      "                            f'<span style=\"color:{score_color};font-size:24px; font-weight:Bold\">{similarity_score}</span>',\n",
      "                            unsafe_allow_html=True)\n",
      "        else:\n",
      "            print(\"Config file does not exist.\")\n",
      "\n",
      "        avs.add_vertical_space(2)\n",
      "        with st.expander(\"Common words between Resume and Job Description:\"):\n",
      "            annotated_text(create_annotated_text(\n",
      "                selected_file[\"clean_data\"], selected_jd[\"extracted_keywords\"],\n",
      "                \"JD\", \"#F24C3D\"))\n",
      "\n",
      "st.divider()\n",
      "\n",
      "# Go back to top\n",
      "st.markdown('[:arrow_up: Back to Top](#resume-matcher)')\n",
      "\n",
      "import json\n",
      "import os\n",
      "from typing import List\n",
      "\n",
      "import networkx as nx\n",
      "import nltk\n",
      "import pandas as pd\n",
      "import plotly.express as px\n",
      "import plotly.graph_objects as go\n",
      "import streamlit as st\n",
      "from annotated_text import annotated_text, parameters\n",
      "from streamlit_extras import add_vertical_space as avs\n",
      "from streamlit_extras.badges import badge\n",
      "\n",
      "from scripts.similarity import get_similarity_score, find_path, read_config\n",
      "from scripts.utils import get_filenames_from_dir\n",
      "\n",
      "# Set page configuration\n",
      "st.set_page_config(page_title='Resume Matcher', page_icon=\"Assets/img/favicon.ico\", initial_sidebar_state='auto')\n",
      "\n",
      "cwd = find_path('Resume-Matcher')\n",
      "config_path = os.path.join(cwd, \"scripts\", \"similarity\")\n",
      "\n",
      "try:\n",
      "    nltk.data.find('tokenizers/punkt')\n",
      "except LookupError:\n",
      "    nltk.download('punkt')\n",
      "\n",
      "parameters.SHOW_LABEL_SEPARATOR = False\n",
      "parameters.BORDER_RADIUS = 3\n",
      "parameters.PADDING = \"0.5 0.25rem\"\n",
      "\n",
      "\n",
      "def create_star_graph(nodes_and_weights, title):\n",
      "    # Create an empty graph\n",
      "    G = nx.Graph()\n",
      "\n",
      "    # Add the central node\n",
      "    central_node = \"resume\"\n",
      "    G.add_node(central_node)\n",
      "\n",
      "    # Add nodes and edges with weights to the graph\n",
      "    for node, weight in nodes_and_weights:\n",
      "        G.add_node(node)\n",
      "        G.add_edge(central_node, node, weight=weight * 100)\n",
      "\n",
      "    # Get position layout for nodes\n",
      "    pos = nx.spring_layout(G)\n",
      "\n",
      "    # Create edge trace\n",
      "    edge_x = []\n",
      "    edge_y = []\n",
      "    for edge in G.edges():\n",
      "        x0, y0 = pos[edge[0]]\n",
      "        x1, y1 = pos[edge[1]]\n",
      "        edge_x.extend([x0, x1, None])\n",
      "        edge_y.extend([y0, y1, None])\n",
      "\n",
      "    edge_trace = go.Scatter(x=edge_x, y=edge_y, line=dict(\n",
      "        width=0.5, color='#888'), hoverinfo='none', mode='lines')\n",
      "\n",
      "    # Create node trace\n",
      "    node_x = []\n",
      "    node_y = []\n",
      "    for node in G.nodes():\n",
      "        x, y = pos[node]\n",
      "        node_x.append(x)\n",
      "        node_y.append(y)\n",
      "\n",
      "    node_trace = go.Scatter(x=node_x, y=node_y, mode='markers', hoverinfo='text',\n",
      "                            marker=dict(showscale=True, colorscale='Rainbow', reversescale=True, color=[], size=10,\n",
      "                                        colorbar=dict(thickness=15, title='Node Connections', xanchor='left',\n",
      "                                                      titleside='right'), line_width=2))\n",
      "\n",
      "    # Color node points by number of connections\n",
      "    node_adjacencies = []\n",
      "    node_text = []\n",
      "    for node in G.nodes():\n",
      "        adjacencies = list(G.adj[node])  # changes here\n",
      "        node_adjacencies.append(len(adjacencies))\n",
      "        node_text.append(f'{node}<br># of connections: {len(adjacencies)}')\n",
      "\n",
      "    node_trace.marker.color = node_adjacencies\n",
      "    node_trace.text = node_text\n",
      "\n",
      "    # Create the figure\n",
      "    fig = go.Figure(data=[edge_trace, node_trace],\n",
      "                    layout=go.Layout(title=title, titlefont_size=16, showlegend=False,\n",
      "                                     hovermode='closest', margin=dict(b=20, l=5, r=5, t=40),\n",
      "                                     xaxis=dict(\n",
      "                                         showgrid=False, zeroline=False, showticklabels=False),\n",
      "                                     yaxis=dict(showgrid=False, zeroline=False, showticklabels=False)))\n",
      "\n",
      "    # Show the figure\n",
      "    st.plotly_chart(fig)\n",
      "\n",
      "\n",
      "def create_annotated_text(input_string: str, word_list: List[str], annotation: str, color_code: str):\n",
      "    # Tokenize the input string\n",
      "    tokens = nltk.word_tokenize(input_string)\n",
      "\n",
      "    # Convert the list to a set for quick lookups\n",
      "    word_set = set(word_list)\n",
      "\n",
      "    # Initialize an empty list to hold the annotated text\n",
      "    annotated_text = []\n",
      "\n",
      "    for token in tokens:\n",
      "        # Check if the token is in the set\n",
      "        if token in word_set:\n",
      "            # If it is, append a tuple with the token, annotation, and color code\n",
      "            annotated_text.append((token, annotation, color_code))\n",
      "        else:\n",
      "            # If it's not, just append the token as a string\n",
      "            annotated_text.append(token)\n",
      "\n",
      "    return annotated_text\n",
      "\n",
      "\n",
      "def read_json(filename):\n",
      "    with open(filename) as f:\n",
      "        data = json.load(f)\n",
      "    return data\n",
      "\n",
      "\n",
      "def tokenize_string(input_string):\n",
      "    tokens = nltk.word_tokenize(input_string)\n",
      "    return tokens\n",
      "\n",
      "\n",
      "# Display the main title and subheaders\n",
      "st.title(':blue[Resume Matcher]')\n",
      "with st.sidebar:\n",
      "    st.image('Assets/img/header_image.png')\n",
      "    st.subheader('Free and Open Source ATS to help your resume pass the screening stage.')\n",
      "    st.markdown('Check the website [www.resumematcher.fyi](https://www.resumematcher.fyi/)')\n",
      "\n",
      "    st.markdown('Give Resume Matcher a  on [GitHub](https://github.com/srbhr/resume-matcher)')\n",
      "\n",
      "    badge(type=\"github\", name=\"srbhr/Resume-Matcher\")\n",
      "    st.markdown('For updates follow me on Twitter.')\n",
      "    badge(type=\"twitter\", name=\"_srbhr_\")\n",
      "    st.markdown('If you like the project and would like to further help in development please consider ')\n",
      "    badge(type=\"buymeacoffee\", name=\"srbhr\")\n",
      "\n",
      "st.divider()\n",
      "avs.add_vertical_space(1)\n",
      "\n",
      "resume_names = get_filenames_from_dir(\"Data/Processed/Resumes\")\n",
      "\n",
      "\n",
      "st.markdown(f\"##### There are {len(resume_names)} resumes present. Please select one from the menu below:\")\n",
      "output  = st.selectbox(f\"\", resume_names)\n",
      "    \n",
      "\n",
      "\n",
      "avs.add_vertical_space(5)\n",
      "\n",
      "# st.write(\"You have selected \", output, \" printing the resume\")\n",
      "selected_file = read_json(\"Data/Processed/Resumes/\" + output)\n",
      "\n",
      "avs.add_vertical_space(2)\n",
      "st.markdown(\"#### Parsed Resume Data\")\n",
      "st.caption(\n",
      "    \"This text is parsed from your resume. This is how it'll look like after getting parsed by an ATS.\")\n",
      "st.caption(\"Utilize this to understand how to make your resume ATS friendly.\")\n",
      "avs.add_vertical_space(3)\n",
      "# st.json(selected_file)\n",
      "st.write(selected_file[\"clean_data\"])\n",
      "\n",
      "avs.add_vertical_space(3)\n",
      "st.write(\"Now let's take a look at the extracted keywords from the resume.\")\n",
      "\n",
      "annotated_text(create_annotated_text(\n",
      "    selected_file[\"clean_data\"], selected_file[\"extracted_keywords\"],\n",
      "    \"KW\", \"#0B666A\"))\n",
      "\n",
      "avs.add_vertical_space(5)\n",
      "st.write(\"Now let's take a look at the extracted entities from the resume.\")\n",
      "\n",
      "# Call the function with your data\n",
      "create_star_graph(selected_file['keyterms'], \"Entities from Resume\")\n",
      "\n",
      "df2 = pd.DataFrame(selected_file['keyterms'], columns=[\"keyword\", \"value\"])\n",
      "\n",
      "# Create the dictionary\n",
      "keyword_dict = {}\n",
      "for keyword, value in selected_file['keyterms']:\n",
      "    keyword_dict[keyword] = value * 100\n",
      "\n",
      "fig = go.Figure(data=[go.Table(header=dict(values=[\"Keyword\", \"Value\"],\n",
      "                                           font=dict(size=12),\n",
      "                                           fill_color='#070A52'),\n",
      "                               cells=dict(values=[list(keyword_dict.keys()),\n",
      "                                                  list(keyword_dict.values())],\n",
      "                                          line_color='darkslategray',\n",
      "                                          fill_color='#6DA9E4'))\n",
      "                      ])\n",
      "st.plotly_chart(fig)\n",
      "\n",
      "st.divider()\n",
      "\n",
      "fig = px.treemap(df2, path=['keyword'], values='value',\n",
      "                 color_continuous_scale='Rainbow',\n",
      "                 title='Key Terms/Topics Extracted from your Resume')\n",
      "st.write(fig)\n",
      "\n",
      "avs.add_vertical_space(5)\n",
      "\n",
      "job_descriptions = get_filenames_from_dir(\"Data/Processed/JobDescription\")\n",
      "\n",
      "\n",
      "st.markdown(f\"##### There are {len(job_descriptions)} job descriptions present. Please select one from the menu below:\")\n",
      "output = st.selectbox(\"\", job_descriptions)\n",
      "\n",
      "\n",
      "avs.add_vertical_space(5)\n",
      "\n",
      "selected_jd = read_json(\n",
      "    \"Data/Processed/JobDescription/\" + output)\n",
      "\n",
      "avs.add_vertical_space(2)\n",
      "st.markdown(\"#### Job Description\")\n",
      "st.caption(\n",
      "    \"Currently in the pipeline I'm parsing this from PDF but it'll be from txt or copy paste.\")\n",
      "avs.add_vertical_space(3)\n",
      "# st.json(selected_file)\n",
      "st.write(selected_jd[\"clean_data\"])\n",
      "\n",
      "st.markdown(\"#### Common Words between Job Description and Resumes Highlighted.\")\n",
      "\n",
      "annotated_text(create_annotated_text(\n",
      "    selected_file[\"clean_data\"], selected_jd[\"extracted_keywords\"],\n",
      "    \"JD\", \"#F24C3D\"))\n",
      "\n",
      "st.write(\"Now let's take a look at the extracted entities from the job description.\")\n",
      "\n",
      "# Call the function with your data\n",
      "create_star_graph(selected_jd['keyterms'], \"Entities from Job Description\")\n",
      "\n",
      "df2 = pd.DataFrame(selected_jd['keyterms'], columns=[\"keyword\", \"value\"])\n",
      "\n",
      "# Create the dictionary\n",
      "keyword_dict = {}\n",
      "for keyword, value in selected_jd['keyterms']:\n",
      "    keyword_dict[keyword] = value * 100\n",
      "\n",
      "fig = go.Figure(data=[go.Table(header=dict(values=[\"Keyword\", \"Value\"],\n",
      "                                           font=dict(size=12),\n",
      "                                           fill_color='#070A52'),\n",
      "                               cells=dict(values=[list(keyword_dict.keys()),\n",
      "                                                  list(keyword_dict.values())],\n",
      "                                          line_color='darkslategray',\n",
      "                                          fill_color='#6DA9E4'))\n",
      "                      ])\n",
      "st.plotly_chart(fig)\n",
      "\n",
      "st.divider()\n",
      "\n",
      "fig = px.treemap(df2, path=['keyword'], values='value',\n",
      "                 color_continuous_scale='Rainbow',\n",
      "                 title='Key Terms/Topics Extracted from the selected Job Description')\n",
      "st.write(fig)\n",
      "\n",
      "avs.add_vertical_space(3)\n",
      "\n",
      "config_file_path = config_path + \"/config.yml\"\n",
      "if os.path.exists(config_file_path):\n",
      "    config_data = read_config(config_file_path)\n",
      "    if config_data:\n",
      "        print(\"Config file parsed successfully:\")\n",
      "        resume_string = ' '.join(selected_file[\"extracted_keywords\"])\n",
      "        jd_string = ' '.join(selected_jd[\"extracted_keywords\"])\n",
      "        result = get_similarity_score(resume_string, jd_string)\n",
      "        similarity_score = result[0][\"score\"]\n",
      "        st.write(\"Similarity Score obtained for the resume and job description is:\", similarity_score)\n",
      "else:\n",
      "    print(\"Config file does not exist.\")\n",
      "\n",
      "\n",
      "# Go back to top\n",
      "st.markdown('[:arrow_up: Back to Top](#resume-matcher)')\n",
      "\n",
      "import networkx as nx\n",
      "from typing import List\n",
      "import streamlit as st\n",
      "import pandas as pd\n",
      "import json\n",
      "import plotly.express as px\n",
      "import plotly.graph_objects as go\n",
      "from scripts.utils import get_filenames_from_dir\n",
      "from streamlit_extras import add_vertical_space as avs\n",
      "from annotated_text import annotated_text, parameters\n",
      "from streamlit_extras.badges import badge\n",
      "import nltk\n",
      "\n",
      "# Set page configuration\n",
      "st.set_page_config(page_title='Resume Matcher', page_icon=\"Assets/img/favicon.ico\", initial_sidebar_state='auto')\n",
      "\n",
      "nltk.download('punkt')\n",
      "\n",
      "parameters.SHOW_LABEL_SEPARATOR = False\n",
      "parameters.BORDER_RADIUS = 3\n",
      "parameters.PADDING = \"0.5 0.25rem\"\n",
      "\n",
      "\n",
      "def create_star_graph(nodes_and_weights, title):\n",
      "    # Create an empty graph\n",
      "    G = nx.Graph()\n",
      "\n",
      "    # Add the central node\n",
      "    central_node = \"resume\"\n",
      "    G.add_node(central_node)\n",
      "\n",
      "    # Add nodes and edges with weights to the graph\n",
      "    for node, weight in nodes_and_weights:\n",
      "        G.add_node(node)\n",
      "        G.add_edge(central_node, node, weight=weight*100)\n",
      "\n",
      "    # Get position layout for nodes\n",
      "    pos = nx.spring_layout(G)\n",
      "\n",
      "    # Create edge trace\n",
      "    edge_x = []\n",
      "    edge_y = []\n",
      "    for edge in G.edges():\n",
      "        x0, y0 = pos[edge[0]]\n",
      "        x1, y1 = pos[edge[1]]\n",
      "        edge_x.extend([x0, x1, None])\n",
      "        edge_y.extend([y0, y1, None])\n",
      "\n",
      "    edge_trace = go.Scatter(x=edge_x, y=edge_y, line=dict(\n",
      "        width=0.5, color='#888'), hoverinfo='none', mode='lines')\n",
      "\n",
      "    # Create node trace\n",
      "    node_x = []\n",
      "    node_y = []\n",
      "    for node in G.nodes():\n",
      "        x, y = pos[node]\n",
      "        node_x.append(x)\n",
      "        node_y.append(y)\n",
      "\n",
      "    node_trace = go.Scatter(x=node_x, y=node_y, mode='markers', hoverinfo='text',\n",
      "                            marker=dict(showscale=True, colorscale='Rainbow', reversescale=True, color=[], size=10,\n",
      "                                        colorbar=dict(thickness=15, title='Node Connections', xanchor='left',\n",
      "                                                      titleside='right'), line_width=2))\n",
      "\n",
      "    # Color node points by number of connections\n",
      "    node_adjacencies = []\n",
      "    node_text = []\n",
      "    for node in G.nodes():\n",
      "        adjacencies = list(G.adj[node])  # changes here\n",
      "        node_adjacencies.append(len(adjacencies))\n",
      "        node_text.append(f'{node}<br># of connections: {len(adjacencies)}')\n",
      "\n",
      "    node_trace.marker.color = node_adjacencies\n",
      "    node_trace.text = node_text\n",
      "\n",
      "    # Create the figure\n",
      "    fig = go.Figure(data=[edge_trace, node_trace],\n",
      "                    layout=go.Layout(title=title, titlefont_size=16, showlegend=False,\n",
      "                                     hovermode='closest', margin=dict(b=20, l=5, r=5, t=40),\n",
      "                                     xaxis=dict(\n",
      "                                         showgrid=False, zeroline=False, showticklabels=False),\n",
      "                                     yaxis=dict(showgrid=False, zeroline=False, showticklabels=False)))\n",
      "\n",
      "    # Show the figure\n",
      "    st.plotly_chart(fig)\n",
      "\n",
      "\n",
      "def create_annotated_text(input_string: str, word_list: List[str], annotation: str, color_code: str):\n",
      "    # Tokenize the input string\n",
      "    tokens = nltk.word_tokenize(input_string)\n",
      "\n",
      "    # Convert the list to a set for quick lookups\n",
      "    word_set = set(word_list)\n",
      "\n",
      "    # Initialize an empty list to hold the annotated text\n",
      "    annotated_text = []\n",
      "\n",
      "    for token in tokens:\n",
      "        # Check if the token is in the set\n",
      "        if token in word_set:\n",
      "            # If it is, append a tuple with the token, annotation, and color code\n",
      "            annotated_text.append((token, annotation, color_code))\n",
      "        else:\n",
      "            # If it's not, just append the token as a string\n",
      "            annotated_text.append(token)\n",
      "\n",
      "    return annotated_text\n",
      "\n",
      "\n",
      "def read_json(filename):\n",
      "    with open(filename) as f:\n",
      "        data = json.load(f)\n",
      "    return data\n",
      "\n",
      "\n",
      "def tokenize_string(input_string):\n",
      "    tokens = nltk.word_tokenize(input_string)\n",
      "    return tokens\n",
      "\n",
      "\n",
      "# Display the main title and subheaders\n",
      "st.title(':blue[Resume Matcher]')\n",
      "with st.sidebar:\n",
      "    st.image('Assets/img/header_image.png')\n",
      "    st.subheader('Free and Open Source ATS to help your resume pass the screening stage.')\n",
      "    st.markdown('Check the website [www.resumematcher.fyi](https://www.resumematcher.fyi/)')\n",
      "\n",
      "    st.markdown('Give Resume Matcher a  on [GitHub](https://github.com/srbhr/resume-matcher)')\n",
      "\n",
      "    badge(type=\"github\", name=\"srbhr/Resume-Matcher\")\n",
      "    st.markdown('For updates follow me on Twitter.')\n",
      "    badge(type=\"twitter\", name=\"_srbhr_\")\n",
      "    st.markdown('If you like the project and would like to further help in development please consider ')\n",
      "    badge(type=\"buymeacoffee\", name=\"srbhr\")\n",
      "\n",
      "st.divider()\n",
      "avs.add_vertical_space(1)\n",
      "\n",
      "resume_names = get_filenames_from_dir(\"Data/Processed/Resumes\")\n",
      "\n",
      "output = st.selectbox(f\"There are {len(resume_names)} resumes present. Please select one from the menu below:\", resume_names)\n",
      "\n",
      "avs.add_vertical_space(5)\n",
      "\n",
      "selected_file = read_json(\"Data/Processed/Resumes/\"+output)\n",
      "\n",
      "avs.add_vertical_space(2)\n",
      "st.markdown(\"#### Parsed Resume Data\")\n",
      "st.caption(\n",
      "    \"This text is parsed from your resume. This is how it'll look like after getting parsed by an ATS.\")\n",
      "st.caption(\"Utilize this to understand how to make your resume ATS friendly.\")\n",
      "avs.add_vertical_space(3)\n",
      "# st.json(selected_file)\n",
      "st.write(selected_file[\"clean_data\"])\n",
      "\n",
      "avs.add_vertical_space(3)\n",
      "st.write(\"Now let's take a look at the extracted keywords from the resume.\")\n",
      "\n",
      "annotated_text(create_annotated_text(\n",
      "    selected_file[\"clean_data\"], selected_file[\"extracted_keywords\"],\n",
      "    \"KW\", \"#0B666A\"))\n",
      "\n",
      "avs.add_vertical_space(5)\n",
      "st.write(\"Now let's take a look at the extracted entities from the resume.\")\n",
      "\n",
      "# Call the function with your data\n",
      "create_star_graph(selected_file['keyterms'], \"Entities from Resume\")\n",
      "\n",
      "df2 = pd.DataFrame(selected_file['keyterms'], columns=[\"keyword\", \"value\"])\n",
      "\n",
      "# Create the dictionary\n",
      "keyword_dict = {}\n",
      "for keyword, value in selected_file['keyterms']:\n",
      "    keyword_dict[keyword] = value*100\n",
      "\n",
      "fig = go.Figure(data=[go.Table(header=dict(values=[\"Keyword\", \"Value\"],\n",
      "                                           font=dict(size=12),\n",
      "                                           fill_color='#070A52'),\n",
      "                               cells=dict(values=[list(keyword_dict.keys()),\n",
      "                                                  list(keyword_dict.values())],\n",
      "                                          line_color='darkslategray',\n",
      "                                          fill_color='#6DA9E4'))\n",
      "                      ])\n",
      "st.plotly_chart(fig)\n",
      "\n",
      "st.divider()\n",
      "\n",
      "fig = px.treemap(df2, path=['keyword'], values='value',\n",
      "                 color_continuous_scale='Rainbow',\n",
      "                 title='Key Terms/Topics Extracted from your Resume')\n",
      "st.write(fig)\n",
      "\n",
      "avs.add_vertical_space(5)\n",
      "\n",
      "job_descriptions = get_filenames_from_dir(\"Data/Processed/JobDescription\")\n",
      "\n",
      "output = st.selectbox(f\"There are {len(job_descriptions)} job descriptions present. Please select one from the menu below:\", job_descriptions)\n",
      "\n",
      "avs.add_vertical_space(5)\n",
      "\n",
      "selected_jd = read_json(\n",
      "    \"Data/Processed/JobDescription/\"+output)\n",
      "\n",
      "avs.add_vertical_space(2)\n",
      "st.markdown(\"#### Job Description\")\n",
      "st.caption(\n",
      "    \"Currently in the pipeline I'm parsing this from PDF but it'll be from txt or copy paste.\")\n",
      "avs.add_vertical_space(3)\n",
      "# st.json(selected_file)\n",
      "st.write(selected_jd[\"clean_data\"])\n",
      "\n",
      "st.markdown(\"#### Common Words between Job Description and Resumes Highlighted.\")\n",
      "\n",
      "annotated_text(create_annotated_text(\n",
      "    selected_file[\"clean_data\"], selected_jd[\"extracted_keywords\"],\n",
      "    \"JD\", \"#F24C3D\"))\n",
      "\n",
      "st.write(\"Now let's take a look at the extracted entities from the job description.\")\n",
      "\n",
      "# Call the function with your data\n",
      "create_star_graph(selected_jd['keyterms'], \"Entities from Job Description\")\n",
      "\n",
      "df2 = pd.DataFrame(selected_jd['keyterms'], columns=[\"keyword\", \"value\"])\n",
      "\n",
      "# Create the dictionary\n",
      "keyword_dict = {}\n",
      "for keyword, value in selected_jd['keyterms']:\n",
      "    keyword_dict[keyword] = value*100\n",
      "\n",
      "fig = go.Figure(data=[go.Table(header=dict(values=[\"Keyword\", \"Value\"],\n",
      "                                           font=dict(size=12),\n",
      "                                           fill_color='#070A52'),\n",
      "                               cells=dict(values=[list(keyword_dict.keys()),\n",
      "                                                  list(keyword_dict.values())],\n",
      "                                          line_color='darkslategray',\n",
      "                                          fill_color='#6DA9E4'))\n",
      "                      ])\n",
      "st.plotly_chart(fig)\n",
      "\n",
      "st.divider()\n",
      "\n",
      "fig = px.treemap(df2, path=['keyword'], values='value',\n",
      "                 color_continuous_scale='Rainbow',\n",
      "                 title='Key Terms/Topics Extracted from the selected Job Description')\n",
      "st.write(fig)\n",
      "\n",
      "avs.add_vertical_space(5)\n",
      "\n",
      "st.divider()\n",
      "\n",
      "st.markdown(\"## Vector Similarity Scores\")\n",
      "st.caption(\"Powered by Qdrant Vector Search\")\n",
      "st.info(\"These are pre-computed queries\", icon=\"\")\n",
      "st.warning(\n",
      "    \"Running Qdrant or Sentence Transformers without having capacity is not recommended\", icon=\"\")\n",
      "\n",
      "\n",
      "# Your data\n",
      "data = [\n",
      "    {'text': \"{'resume': 'Alfred Pennyworth\",\n",
      "        'query': 'Job Description Product Manager', 'score': 0.62658},\n",
      "    {'text': \"{'resume': 'Barry Allen\",\n",
      "        'query': 'Job Description Product Manager', 'score': 0.43777737},\n",
      "    {'text': \"{'resume': 'Bruce Wayne \",\n",
      "        'query': 'Job Description Product Manager', 'score': 0.39835533},\n",
      "    {'text': \"{'resume': 'JOHN DOE\",\n",
      "        'query': 'Job Description Product Manager', 'score': 0.3915512},\n",
      "    {'text': \"{'resume': 'Harvey Dent\",\n",
      "        'query': 'Job Description Product Manager', 'score': 0.3519544},\n",
      "    {'text': \"{'resume': 'Barry Allen\",\n",
      "        'query': 'Job Description Senior Full Stack Engineer', 'score': 0.6541866},\n",
      "    {'text': \"{'resume': 'Alfred Pennyworth\",\n",
      "        'query': 'Job Description Senior Full Stack Engineer', 'score': 0.59806436},\n",
      "    {'text': \"{'resume': 'JOHN DOE\",\n",
      "        'query': 'Job Description Senior Full Stack Engineer', 'score': 0.5951386},\n",
      "    {'text': \"{'resume': 'Bruce Wayne \",\n",
      "        'query': 'Job Description Senior Full Stack Engineer', 'score': 0.57700855},\n",
      "    {'text': \"{'resume': 'Harvey Dent\",\n",
      "        'query': 'Job Description Senior Full Stack Engineer', 'score': 0.38489106},\n",
      "    {'text': \"{'resume': 'Barry Allen\",\n",
      "        'query': 'Job Description Front End Engineer', 'score': 0.76813436},\n",
      "    {'text': \"{'resume': 'Bruce Wayne'\",\n",
      "        'query': 'Job Description Front End Engineer', 'score': 0.60440844},\n",
      "    {'text': \"{'resume': 'JOHN DOE\",\n",
      "        'query': 'Job Description Front End Engineer', 'score': 0.56080043},\n",
      "    {'text': \"{'resume': 'Alfred Pennyworth\",\n",
      "        'query': 'Job Description Front End Engineer', 'score': 0.5395049},\n",
      "    {'text': \"{'resume': 'Harvey Dent\",\n",
      "        'query': 'Job Description Front End Engineer', 'score': 0.3859515},\n",
      "    {'text': \"{'resume': 'JOHN DOE\",\n",
      "        'query': 'Job Description Java Developer', 'score': 0.5449441},\n",
      "    {'text': \"{'resume': 'Alfred Pennyworth\",\n",
      "        'query': 'Job Description Java Developer', 'score': 0.53476423},\n",
      "    {'text': \"{'resume': 'Barry Allen\",\n",
      "        'query': 'Job Description Java Developer', 'score': 0.5313871},\n",
      "    {'text': \"{'resume': 'Bruce Wayne \",\n",
      "        'query': 'Job Description Java Developer', 'score': 0.44446343},\n",
      "    {'text': \"{'resume': 'Harvey Dent\",\n",
      "        'query': 'Job Description Java Developer', 'score': 0.3616274}\n",
      "]\n",
      "\n",
      "# Create a DataFrame\n",
      "df = pd.DataFrame(data)\n",
      "\n",
      "# Create different DataFrames based on the query and sort by score\n",
      "df1 = df[df['query'] ==\n",
      "         'Job Description Product Manager'].sort_values(by='score', ascending=False)\n",
      "df2 = df[df['query'] ==\n",
      "         'Job Description Senior Full Stack Engineer'].sort_values(by='score', ascending=False)\n",
      "df3 = df[df['query'] == 'Job Description Front End Engineer'].sort_values(\n",
      "    by='score', ascending=False)\n",
      "df4 = df[df['query'] == 'Job Description Java Developer'].sort_values(\n",
      "    by='score', ascending=False)\n",
      "\n",
      "\n",
      "def plot_df(df, title):\n",
      "    fig = px.bar(df, x='text', y=df['score']*100, title=title)\n",
      "    st.plotly_chart(fig)\n",
      "\n",
      "\n",
      "st.markdown(\"### Bar plots of scores based on similarity to Job Description.\")\n",
      "\n",
      "st.subheader(\":blue[Legend]\")\n",
      "st.text(\"Alfred Pennyworth :  Product Manager\")\n",
      "st.text(\"Barry Allen :  Front End Developer\")\n",
      "st.text(\"Harvey Dent :  Machine Learning Engineer\")\n",
      "st.text(\"Bruce Wayne :  Fullstack Developer (MERN)\")\n",
      "st.text(\"John Doe :  Fullstack Developer (Java)\")\n",
      "\n",
      "\n",
      "plot_df(df1, 'Job Description Product Manager 10+ Years of Exper')\n",
      "plot_df(df2, 'Job Description Senior Full Stack Engineer 5+ Year')\n",
      "plot_df(df3, 'Job Description Front End Engineer 2 Years of Expe')\n",
      "plot_df(df4, 'Job Description Java Developer 3 Years of Experien')\n",
      "\n",
      "\n",
      "avs.add_vertical_space(3)\n",
      "\n",
      "# Go back to top\n",
      "st.markdown('[:arrow_up: Back to Top](#resume-matcher)')\n",
      "import json\n",
      "import logging\n",
      "import os\n",
      "from typing import List\n",
      "\n",
      "import yaml\n",
      "from qdrant_client import QdrantClient\n",
      "\n",
      "logging.basicConfig(\n",
      "    filename='app_similarity_score.log',\n",
      "    filemode='w',\n",
      "    level=logging.INFO,\n",
      "    format=\"%(asctime)s - %(name)s - %(levelname)s - %(message)s\"\n",
      ")\n",
      "\n",
      "logger = logging.getLogger(__name__)\n",
      "logger.setLevel(logging.DEBUG)\n",
      "\n",
      "console_handler = logging.StreamHandler()\n",
      "formatter = logging.Formatter(\"%(asctime)s - %(name)s - %(levelname)s - %(message)s\")\n",
      "console_handler.setFormatter(formatter)\n",
      "console_handler.setLevel(logging.DEBUG)\n",
      "\n",
      "file_handler = logging.FileHandler(\"app_similarity_score.log\")\n",
      "file_handler.setLevel(logging.DEBUG)\n",
      "file_handler.setFormatter(formatter)\n",
      "\n",
      "logger.addHandler(file_handler)\n",
      "logger.addHandler(console_handler)\n",
      "\n",
      "\n",
      "def find_path(folder_name):\n",
      "    curr_dir = os.getcwd()\n",
      "    while True:\n",
      "        if folder_name in os.listdir(curr_dir):\n",
      "            return os.path.join(curr_dir, folder_name)\n",
      "        else:\n",
      "            parent_dir = os.path.dirname(curr_dir)\n",
      "            if parent_dir == '/':\n",
      "                break\n",
      "            curr_dir = parent_dir\n",
      "    raise ValueError(f\"Folder '{folder_name}' not found.\")\n",
      "\n",
      "\n",
      "cwd = find_path('Resume-Matcher')\n",
      "READ_RESUME_FROM = os.path.join(cwd, 'Data', 'Processed', 'Resumes')\n",
      "READ_JOB_DESCRIPTION_FROM = os.path.join(cwd, 'Data', 'Processed', 'JobDescription')\n",
      "config_path = os.path.join(cwd, \"scripts\", \"similarity\")\n",
      "\n",
      "\n",
      "def read_config(filepath):\n",
      "    try:\n",
      "        with open(filepath) as f:\n",
      "            config = yaml.safe_load(f)\n",
      "        return config\n",
      "    except FileNotFoundError as e:\n",
      "        logger.error(f\"Configuration file {filepath} not found: {e}\")\n",
      "    except yaml.YAMLError as e:\n",
      "        logger.error(f\"Error parsing YAML in configuration file {filepath}: {e}\", exc_info=True)\n",
      "    except Exception as e:\n",
      "        logger.error(f\"Error reading configuration file {filepath}: {e}\")\n",
      "    return None\n",
      "\n",
      "\n",
      "def read_doc(path):\n",
      "    with open(path) as f:\n",
      "        try:\n",
      "            data = json.load(f)\n",
      "        except Exception as e:\n",
      "            logger.error(f'Error reading JSON file: {e}')\n",
      "            data = {}\n",
      "    return data\n",
      "\n",
      "\n",
      "def get_score(resume_string, job_description_string):\n",
      "    logger.info(\"Started getting similarity score\")\n",
      "\n",
      "    documents: List[str] = [resume_string]\n",
      "    client = QdrantClient(\":memory:\")\n",
      "    client.set_model(\"BAAI/bge-base-en\")\n",
      "\n",
      "    client.add(\n",
      "        collection_name=\"demo_collection\",\n",
      "        documents=documents,\n",
      "    )\n",
      "\n",
      "    search_result = client.query(\n",
      "        collection_name=\"demo_collection\",\n",
      "        query_text=job_description_string\n",
      "    )\n",
      "    logger.info(\"Finished getting similarity score\")\n",
      "    return search_result\n",
      "\n",
      "\n",
      "if __name__ == \"__main__\":\n",
      "    # To give your custom resume use this code\n",
      "    resume_dict = read_config(\n",
      "        READ_RESUME_FROM + \"/Resume-alfred_pennyworth_pm.pdf83632b66-5cce-4322-a3c6-895ff7e3dd96.json\")\n",
      "    job_dict = read_config(\n",
      "        READ_JOB_DESCRIPTION_FROM + \"/JobDescription-job_desc_product_manager.pdf6763dc68-12ff-4b32-b652-ccee195de071.json\")\n",
      "    resume_keywords = resume_dict[\"extracted_keywords\"]\n",
      "    job_description_keywords = job_dict[\"extracted_keywords\"]\n",
      "\n",
      "    resume_string = ' '.join(resume_keywords)\n",
      "    jd_string = ' '.join(job_description_keywords)\n",
      "    final_result = get_score(resume_string, jd_string)\n",
      "    for r in final_result:\n",
      "        print(r.score)\n",
      "\n",
      "import json\n",
      "import os\n",
      "from typing import List\n",
      "\n",
      "import networkx as nx\n",
      "import nltk\n",
      "import pandas as pd\n",
      "import plotly.express as px\n",
      "import plotly.graph_objects as go\n",
      "import streamlit as st\n",
      "from annotated_text import annotated_text, parameters\n",
      "from streamlit_extras import add_vertical_space as avs\n",
      "from streamlit_extras.badges import badge\n",
      "\n",
      "from scripts.utils import get_filenames_from_dir\n",
      "from scripts.similarity.get_score import *\n",
      "# Set page configuration\n",
      "st.set_page_config(page_title='Resume Matcher', page_icon=\"Assets/img/favicon.ico\", initial_sidebar_state='auto')\n",
      "\n",
      "cwd = find_path('Resume-Matcher')\n",
      "config_path = os.path.join(cwd, \"scripts\", \"similarity\")\n",
      "\n",
      "try:\n",
      "    nltk.data.find('tokenizers/punkt')\n",
      "except LookupError:\n",
      "    nltk.download('punkt')\n",
      "\n",
      "parameters.SHOW_LABEL_SEPARATOR = False\n",
      "parameters.BORDER_RADIUS = 3\n",
      "parameters.PADDING = \"0.5 0.25rem\"\n",
      "\n",
      "\n",
      "def create_star_graph(nodes_and_weights, title):\n",
      "    # Create an empty graph\n",
      "    G = nx.Graph()\n",
      "\n",
      "    # Add the central node\n",
      "    central_node = \"resume\"\n",
      "    G.add_node(central_node)\n",
      "\n",
      "    # Add nodes and edges with weights to the graph\n",
      "    for node, weight in nodes_and_weights:\n",
      "        G.add_node(node)\n",
      "        G.add_edge(central_node, node, weight=weight * 100)\n",
      "\n",
      "    # Get position layout for nodes\n",
      "    pos = nx.spring_layout(G)\n",
      "\n",
      "    # Create edge trace\n",
      "    edge_x = []\n",
      "    edge_y = []\n",
      "    for edge in G.edges():\n",
      "        x0, y0 = pos[edge[0]]\n",
      "        x1, y1 = pos[edge[1]]\n",
      "        edge_x.extend([x0, x1, None])\n",
      "        edge_y.extend([y0, y1, None])\n",
      "\n",
      "    edge_trace = go.Scatter(x=edge_x, y=edge_y, line=dict(\n",
      "        width=0.5, color='#888'), hoverinfo='none', mode='lines')\n",
      "\n",
      "    # Create node trace\n",
      "    node_x = []\n",
      "    node_y = []\n",
      "    for node in G.nodes():\n",
      "        x, y = pos[node]\n",
      "        node_x.append(x)\n",
      "        node_y.append(y)\n",
      "\n",
      "    node_trace = go.Scatter(x=node_x, y=node_y, mode='markers', hoverinfo='text',\n",
      "                            marker=dict(showscale=True, colorscale='Rainbow', reversescale=True, color=[], size=10,\n",
      "                                        colorbar=dict(thickness=15, title='Node Connections', xanchor='left',\n",
      "                                                      titleside='right'), line_width=2))\n",
      "\n",
      "    # Color node points by number of connections\n",
      "    node_adjacencies = []\n",
      "    node_text = []\n",
      "    for node in G.nodes():\n",
      "        adjacencies = list(G.adj[node])  # changes here\n",
      "        node_adjacencies.append(len(adjacencies))\n",
      "        node_text.append(f'{node}<br># of connections: {len(adjacencies)}')\n",
      "\n",
      "    node_trace.marker.color = node_adjacencies\n",
      "    node_trace.text = node_text\n",
      "\n",
      "    # Create the figure\n",
      "    fig = go.Figure(data=[edge_trace, node_trace],\n",
      "                    layout=go.Layout(title=title, titlefont_size=16, showlegend=False,\n",
      "                                     hovermode='closest', margin=dict(b=20, l=5, r=5, t=40),\n",
      "                                     xaxis=dict(\n",
      "                                         showgrid=False, zeroline=False, showticklabels=False),\n",
      "                                     yaxis=dict(showgrid=False, zeroline=False, showticklabels=False)))\n",
      "\n",
      "    # Show the figure\n",
      "    st.plotly_chart(fig)\n",
      "\n",
      "\n",
      "def create_annotated_text(input_string: str, word_list: List[str], annotation: str, color_code: str):\n",
      "    # Tokenize the input string\n",
      "    tokens = nltk.word_tokenize(input_string)\n",
      "\n",
      "    # Convert the list to a set for quick lookups\n",
      "    word_set = set(word_list)\n",
      "\n",
      "    # Initialize an empty list to hold the annotated text\n",
      "    annotated_text = []\n",
      "\n",
      "    for token in tokens:\n",
      "        # Check if the token is in the set\n",
      "        if token in word_set:\n",
      "            # If it is, append a tuple with the token, annotation, and color code\n",
      "            annotated_text.append((token, annotation, color_code))\n",
      "        else:\n",
      "            # If it's not, just append the token as a string\n",
      "            annotated_text.append(token)\n",
      "\n",
      "    return annotated_text\n",
      "\n",
      "\n",
      "def read_json(filename):\n",
      "    with open(filename) as f:\n",
      "        data = json.load(f)\n",
      "    return data\n",
      "\n",
      "\n",
      "def tokenize_string(input_string):\n",
      "    tokens = nltk.word_tokenize(input_string)\n",
      "    return tokens\n",
      "\n",
      "\n",
      "# Display the main title and subheaders\n",
      "st.title(':blue[Resume Matcher]')\n",
      "with st.sidebar:\n",
      "    st.image('Assets/img/header_image.png')\n",
      "    st.subheader('Free and Open Source ATS to help your resume pass the screening stage.')\n",
      "    st.markdown('Check the website [www.resumematcher.fyi](https://www.resumematcher.fyi/)')\n",
      "\n",
      "    st.markdown('Give Resume Matcher a  on [GitHub](https://github.com/srbhr/resume-matcher)')\n",
      "\n",
      "    badge(type=\"github\", name=\"srbhr/Resume-Matcher\")\n",
      "    st.markdown('For updates follow me on Twitter.')\n",
      "    badge(type=\"twitter\", name=\"_srbhr_\")\n",
      "    st.markdown('If you like the project and would like to further help in development please consider ')\n",
      "    badge(type=\"buymeacoffee\", name=\"srbhr\")\n",
      "\n",
      "st.divider()\n",
      "avs.add_vertical_space(1)\n",
      "\n",
      "resume_names = get_filenames_from_dir(\"Data/Processed/Resumes\")\n",
      "\n",
      "\n",
      "st.markdown(f\"##### There are {len(resume_names)} resumes present. Please select one from the menu below:\")\n",
      "output  = st.selectbox(f\"\", resume_names)\n",
      "    \n",
      "\n",
      "\n",
      "avs.add_vertical_space(5)\n",
      "\n",
      "# st.write(\"You have selected \", output, \" printing the resume\")\n",
      "selected_file = read_json(\"Data/Processed/Resumes/\" + output)\n",
      "\n",
      "avs.add_vertical_space(2)\n",
      "st.markdown(\"#### Parsed Resume Data\")\n",
      "st.caption(\n",
      "    \"This text is parsed from your resume. This is how it'll look like after getting parsed by an ATS.\")\n",
      "st.caption(\"Utilize this to understand how to make your resume ATS friendly.\")\n",
      "avs.add_vertical_space(3)\n",
      "# st.json(selected_file)\n",
      "st.write(selected_file[\"clean_data\"])\n",
      "\n",
      "avs.add_vertical_space(3)\n",
      "st.write(\"Now let's take a look at the extracted keywords from the resume.\")\n",
      "\n",
      "annotated_text(create_annotated_text(\n",
      "    selected_file[\"clean_data\"], selected_file[\"extracted_keywords\"],\n",
      "    \"KW\", \"#0B666A\"))\n",
      "\n",
      "avs.add_vertical_space(5)\n",
      "st.write(\"Now let's take a look at the extracted entities from the resume.\")\n",
      "\n",
      "# Call the function with your data\n",
      "create_star_graph(selected_file['keyterms'], \"Entities from Resume\")\n",
      "\n",
      "df2 = pd.DataFrame(selected_file['keyterms'], columns=[\"keyword\", \"value\"])\n",
      "\n",
      "# Create the dictionary\n",
      "keyword_dict = {}\n",
      "for keyword, value in selected_file['keyterms']:\n",
      "    keyword_dict[keyword] = value * 100\n",
      "\n",
      "fig = go.Figure(data=[go.Table(header=dict(values=[\"Keyword\", \"Value\"],\n",
      "                                           font=dict(size=12),\n",
      "                                           fill_color='#070A52'),\n",
      "                               cells=dict(values=[list(keyword_dict.keys()),\n",
      "                                                  list(keyword_dict.values())],\n",
      "                                          line_color='darkslategray',\n",
      "                                          fill_color='#6DA9E4'))\n",
      "                      ])\n",
      "st.plotly_chart(fig)\n",
      "\n",
      "st.divider()\n",
      "\n",
      "fig = px.treemap(df2, path=['keyword'], values='value',\n",
      "                 color_continuous_scale='Rainbow',\n",
      "                 title='Key Terms/Topics Extracted from your Resume')\n",
      "st.write(fig)\n",
      "\n",
      "avs.add_vertical_space(5)\n",
      "\n",
      "job_descriptions = get_filenames_from_dir(\"Data/Processed/JobDescription\")\n",
      "\n",
      "\n",
      "st.markdown(f\"##### There are {len(job_descriptions)} job descriptions present. Please select one from the menu below:\")\n",
      "output = st.selectbox(\"\", job_descriptions)\n",
      "\n",
      "\n",
      "avs.add_vertical_space(5)\n",
      "\n",
      "selected_jd = read_json(\n",
      "    \"Data/Processed/JobDescription/\" + output)\n",
      "\n",
      "avs.add_vertical_space(2)\n",
      "st.markdown(\"#### Job Description\")\n",
      "st.caption(\n",
      "    \"Currently in the pipeline I'm parsing this from PDF but it'll be from txt or copy paste.\")\n",
      "avs.add_vertical_space(3)\n",
      "# st.json(selected_file)\n",
      "st.write(selected_jd[\"clean_data\"])\n",
      "\n",
      "st.markdown(\"#### Common Words between Job Description and Resumes Highlighted.\")\n",
      "\n",
      "annotated_text(create_annotated_text(\n",
      "    selected_file[\"clean_data\"], selected_jd[\"extracted_keywords\"],\n",
      "    \"JD\", \"#F24C3D\"))\n",
      "\n",
      "st.write(\"Now let's take a look at the extracted entities from the job description.\")\n",
      "\n",
      "# Call the function with your data\n",
      "create_star_graph(selected_jd['keyterms'], \"Entities from Job Description\")\n",
      "\n",
      "df2 = pd.DataFrame(selected_jd['keyterms'], columns=[\"keyword\", \"value\"])\n",
      "\n",
      "# Create the dictionary\n",
      "keyword_dict = {}\n",
      "for keyword, value in selected_jd['keyterms']:\n",
      "    keyword_dict[keyword] = value * 100\n",
      "\n",
      "fig = go.Figure(data=[go.Table(header=dict(values=[\"Keyword\", \"Value\"],\n",
      "                                           font=dict(size=12),\n",
      "                                           fill_color='#070A52'),\n",
      "                               cells=dict(values=[list(keyword_dict.keys()),\n",
      "                                                  list(keyword_dict.values())],\n",
      "                                          line_color='darkslategray',\n",
      "                                          fill_color='#6DA9E4'))\n",
      "                      ])\n",
      "st.plotly_chart(fig)\n",
      "\n",
      "st.divider()\n",
      "\n",
      "fig = px.treemap(df2, path=['keyword'], values='value',\n",
      "                 color_continuous_scale='Rainbow',\n",
      "                 title='Key Terms/Topics Extracted from the selected Job Description')\n",
      "st.write(fig)\n",
      "\n",
      "avs.add_vertical_space(3)\n",
      "\n",
      "resume_string = ' '.join(selected_file[\"extracted_keywords\"])\n",
      "jd_string = ' '.join(selected_jd[\"extracted_keywords\"])\n",
      "result = get_score(resume_string, jd_string)\n",
      "similarity_score = result[0].score\n",
      "st.write(\"Similarity Score obtained for the resume and job description is:\", similarity_score)\n",
      "\n",
      "# Go back to top\n",
      "st.markdown('[:arrow_up: Back to Top](#resume-matcher)')\n",
      "\n",
      "jobs = [\n",
      "    {\n",
      "        \"job_desc\": \"Job Description Product Manager 10+ Years of Experience\\nTech Solutions San Francisco CA USA\\nAbout Us\\nTech Solutions is a global leader in the technology industry specializing in the development of cuttingedge\\nsoftware products Were currently looking for a seasoned Product Manager with over 10 years of experience\\nto join our dynamic team\\nJob Description\\nThe Product Manager will be responsible for guiding the success of a product and leading the crossfunctional\\nteam that is responsible for improving it This is an important organizational role that sets the strategy\\nroadmap and feature definition for a product or product line\\nResponsibilities\\nDefine the product strategy and roadmap\\nDeliver MRDs and PRDs with prioritized features and corresponding justification\\nWork with external third parties to assess partnerships and licensing opportunities\\nRun beta and pilot programs with earlystage products and samples\\nBe an expert with respect to the competition\\nAct as a leader within the company\\nDevelop the core positioning and messaging for the product\\nPerform product demos to customers\\nSet pricing to meet revenue and profitability goals\\nRequirements\\n10+ years of experience in product management\\nDemonstrated success defining and launching excellent products\\nExcellent written and verbal communication skills\\nTechnical background with experience in software development\\nExcellent teamwork skills\\nProven ability to influence crossfunctional teams without formal authority\\nMust be able to travel 20\\nBachelors degree MBA preferred\\n1 Benefits\\nCompetitive salary package\\nHealth dental and vision insurance\\nRetirement savings plan\\nProfessional development opportunities\\nFlexible work hours\\nTech Solutions is an equal opportunity employer We celebrate diversity and are committed to creating\\nan inclusive environment for all employees\\nHow to Apply\\nTo apply please submit your resume and a brief explanation of your relevant experience to  \\n2\"\n",
      "    },\n",
      "    {\n",
      "        \"job_desc\": \"Job Description Senior Full Stack Engineer 5+ Years of\\nExperience\\nTech Solutions San Francisco CA USA\\nAbout Us\\nTech Solutions is a leading technology company that creates innovative solutions across a variety of industries\\nOur mission is to improve lives through advanced technology Were currently seeking a Senior Full Stack\\nEngineer to join our dynamic team\\nJob Description\\nWere looking for a Senior Full Stack Engineer with 5+ years of experience in developing web applications\\nThe successful candidate will have experience working with both frontend and backend technologies and\\nwill be capable of overseeing projects from conception to production deployment\\nResponsibilities\\nDeveloping front end website architecture\\nDesigning user interactions on web pages\\nDeveloping back end website applications\\nCreating servers and databases for functionality\\nEnsuring crossplatform optimization for mobile phones\\nSeeing through a project from conception to finished product\\nDesigning and developing APIs\\nMeeting both technical and consumer needs\\nStaying abreast of developments in web applications and programming languages\\nRequirements\\nDegree in Computer Science or similar field\\n5+ years of experience in web development\\nStrong organizational and project management skills\\nProficiency with fundamental front end languages such as HTML CSS and JavaScript\\nProficiency with serverside languages such as Python Ruby Java PHP and Net\\nFamiliarity with database technology such as MySQL Oracle and MongoDB\\n1 Excellent verbal communication skills\\nGood problemsolving skills\\nAttention to detail\\nBenefits\\nCompetitive salary package\\nHealth dental and vision insurance\\nRetirement savings plan\\nProfessional development opportunities\\nFlexible work hours\\nTech Solutions is an equal opportunity employer and we value diversity at our company\\nHow to Apply\\nTo apply please submit your resume and a brief explanation of your relevant experience to  \\n2\"\n",
      "    },\n",
      "    {\n",
      "        \"job_desc\": \"Job Description Front End Engineer 2 Years of Experience\\nTech Solutions San Francisco CA USA\\nAbout Us\\nAt Tech Solutions we are on a mission to build products that solve complex problems and improve peoples\\nlives We are seeking a talented Front End Engineer to join our dynamic team in San Francisco\\nJob Description\\nWe are looking for a Front End Engineer with at least 2 years of experience in developing scalable and\\nuserfriendly web applications The successful candidate will be proficient in modern JavaScript frameworks\\nand libraries HTML CSS and responsive design principles This role will contribute significantly to the\\ncreation and implementation of user interfaces for our web applications\\nResponsibilities\\nDevelop new userfacing features using modern JavaScript frameworks like Reactjs Vuejs or Angu\\nlarjs\\nBuild reusable code and libraries for future use\\nEnsure the technical feasibility of UI/UX designs\\nOptimize application for maximum speed and scalability\\nAssure that all user input is validated before submitting to backend services\\nCollaborate with other team members and stakeholders\\nRequirements\\n2 years of experience as a Front End Developer or similar role\\nProficiency in web markup including HTML5 CSS3\\nKnowledge of modern JavaScript programming and experience with libraries like jQuery\\nFamiliarity with modern frontend build pipelines and tools\\nExperience with popular frontend frameworks such as React Vue or Angular\\nFamiliarity with code versioning tools such as Git\\nDegree in Computer Science Engineering or a related field\\n1 Benefits\\nCompetitive salary package\\nHealth dental and vision insurance\\nRetirement savings plan\\nProfessional development opportunities\\nFlexible work hours\\nTech Solutions is proud to be an equal opportunity employer We celebrate diversity and are committed\\nto creating an inclusive environment for all employees\\nHow to Apply\\nTo apply please submit your resume and a brief explanation of your relevant experience to  \\n2\"\n",
      "    },\n",
      "    {\n",
      "        \"job_desc\": \"Job Description Java Developer 3 Years of Experience\\nTech Solutions San Francisco CA USA\\nAbout Us\\nAt Tech Solutions we believe in the power of technology to solve complex problems We are a dynamic\\nforwardthinking tech company specializing in custom software solutions for various industries We are\\nseeking a talented and experienced Java Developer to join our team\\nJob Description\\nWe are seeking a skilled Java Developer with at least 3 years of experience in building highperforming scal\\nable enterprisegrade applications You will be part of a talented software team that works on missioncritical\\napplications Your roles and responsibilities will include managing Java/Java EE application development\\nwhile providing expertise in the full software development lifecycle\\nResponsibilities\\nDesigning implementing and maintaining Java applications that are often highvolume and low\\nlatency required for missioncritical systems\\nDelivering high availability and performance\\nContributing to all phases of the development lifecycle\\nWriting welldesigned efficient and testable code\\nConducting software analysis programming testing and debugging\\nEnsuring designs comply with specifications\\nPreparing and producing releases of software components\\nSupporting continuous improvement by investigating alternatives and technologies and presenting these\\nfor architectural review\\nRequirements\\nBS/MS degree in Computer Science Engineering or a related subject\\nProven handson Software Development experience\\nProven working experience in Java development\\nHandson experience in designing and developing applications using Java EE platforms\\nObjectOriented Analysis and design using common design patterns\\nProfound insight of Java and JEE internals Classloading Memory Management Transaction man\\nagement etc\\n1 Excellent knowledge of Relational Databases SQL and ORM technologies JPA2 Hibernate\\nExperience in developing web applications using at least one popular web framework JSF Wicket\\nGWT Spring MVC\\nExperience with testdriven development\\nBenefits\\nCompetitive salary package\\nHealth dental and vision insurance\\nRetirement savings plan\\nProfessional development opportunities\\nFlexible work hours\\nTech Solutions is proud to be an equal opportunity employer We celebrate diversity and are committed\\nto creating an inclusive environment for all employees\\nHow to Apply\\nTo apply please submit your resume and a brief explanation of your relevant experience to  \\n2\"\n",
      "    },\n",
      "]\n",
      "\n",
      "\n",
      "resumes = [\n",
      "    {\n",
      "        \"resume\": \"JOHN DOE\\n123 Main St Anywhere USA    \\nLinkedIn linkedincom/in/johndoe  GitHub githubcom/johndoe\\nPROFESSIONAL SUMMARY\\nHighly skilled Full Stack Developer with over 5 years of experience in Java and Angular development\\nspecializing in designing building testing and maintaining web applications Proficient in an assortment\\nof technologies including Java Spring Boot Angular HTML5 CSS3 and SQL Exceptional ability to\\nwork in a team and selfdirect Committed to providing highquality results with little supervision\\nSKILLS\\nJava and J2EE\\nSpring Boot Spring MVC and Hibernate\\nAngular versions 2+\\nJavaScript TypeScript HTML5 CSS3 and Bootstrap\\nRESTful APIs\\nSQL and NoSQL databases MySQL MongoDB\\nAgile and Scrum\\nGit and GitHub\\nJunit and Mockito\\nDocker\\nWORK EXPERIENCE\\nFull Stack Java Developer  ABC Company Inc Anywhere USA June 2018  Present\\nDeveloped scalable robust and maintainable enterpriselevel applications using Java and Spring\\nBoot\\nUsed Angular for developing dynamic and responsive web frontends improving user experience\\nby 30\\nIntegrated applications with MySQL and MongoDB databases to store and retrieve data efficiently\\nCollaborated in an Agile development team to deliver highquality software every sprint\\nCreated RESTful services and APIs for frontend and thirdparty applications\\nWrote unit tests using Junit and Mockito for robust testing of application components\\nSoftware Developer  XYZ Solutions Anywhere USA July 2016  June 2018\\nParticipated in the complete software development life cycle from requirement analysis to deploy\\nment\\nImplemented business logic using Java and enhanced user interface using Angular\\nDeveloped and maintained SQL and NoSQL databases implementing complex queries for business\\nneeds\\nUtilized Git for version control and collaborated with team members via GitHub\\nAssisted in troubleshooting software debugging and system enhancements\\n1 EDUCATION\\nBachelor of Science in Computer Science  State University Anywhere USA May 2016\\nCERTIFICATIONS\\nOracle Certified Professional Java SE 8 Programmer\\nCertified Angular Developer\\n2\"\n",
      "    },\n",
      "    {\n",
      "        \"resume\": \"Alfred Pennyworth\\nProduct ManagerSilicon Valley CA USA\\nobilealt\\n/envel\\n/linkedininapennyworth\\n\\nProfessional Summary\\nSeasoned Product Manager with over 20 years of experience in software development and product\\nmanagement having worked at all FAANG companies Exceptional leadership skills strategic\\nthinking and a track record of managing products from conception to market success\\nSkills\\nProduct management Agile methodologies Leadership Communication Project\\nmanagement User Experience Design Market Research Data Analysis Java\\nPython JavaScript HTML/CSS SQL AWS\\nExperience\\n2017 \\nPresentProduct Manager Google  Mountain View CA USA\\nLeading crossfunctional teams to design develop and launch innovative products Devel\\noping product strategies and making datadriven decisions to improve user experience and\\nmeet business goals\\n2012  2017 Software Development Engineer III Amazon  Seattle WA USA\\nLed a team of developers in building scalable and highperforming ecommerce applications\\nSuccessfully delivered multiple projects within the stipulated time and budget\\n2007  2012 Software Development Engineer II Apple  Cupertino CA USA\\nDesigned and implemented software components for various Apple services Optimized the\\nperformance of applications and improved code quality through thorough testing\\n2002  2007 Software Development Engineer I Netflix  Los Gatos CA USA\\nDeveloped and maintained the user interface for the Netflix web application Worked closely\\nwith product managers and designers to create an optimal user experience\\n1999  2002 Software Development Engineer I Facebook  Menlo Park CA USA\\nPlayed a key role in the development of early Facebook features Implemented scalable\\nbackend services using Java and SQL\\nEducation\\n2016  2018 Master of Business Administration Stanford University  Stanford CA USA\\n1997  1999 Master of Science in Computer Science Massachusetts Institute of Technology \\nCambridge MA USA\\n1994  1997 Bachelor of Science in Computer Science University of California Berkeley \\nBerkeley CA USA\\nProjects\\n1/2 2020 \\nPresentPersonal Project Home Automation System\\nDeveloped a smart home automation system using Raspberry Pi and Python The system\\nautomates various home appliances based on user behavior and preferences contributing to\\nenergy saving and improved user convenience\\n2/2\"\n",
      "    },\n",
      "    {\n",
      "        \"resume\": \"Harvey Dent\\nMachine Learning Engineer321 Gotham St\\nGotham USA\\nobilealt\\n/envel\\n/linkedininhdent\\n/githubhdent\\nProfessional Summary\\nMachine Learning Engineer with 5 years of experience in designing building and deploying predictive\\nmodels and deep learning systems Proficient in Python TensorFlow PyTorch and Scikitlearn\\nCurrently leading a team of AI engineers at OpenAI\\nSkills\\nPython R TensorFlow PyTorch Scikitlearn Keras SQL NoSQL Git Docker\\nKubernetes Agile and Scrum Statistics Data visualization Deep Learning Natural\\nLanguage Processing\\nExperience\\n2021 \\nPresentMachine Learning Engineer OpenAI  San Francisco USA\\nLeading a team of AI engineers Designed and implemented deep learning models for natural\\nlanguage processing tasks Improved the efficiency of model training and data processing\\npipelines Published several research papers in toptier AI conferences\\n2018  2021 Data Scientist Uber  San Francisco USA\\nDeveloped and deployed machine learning models to improve the efficiency of ride allocation\\nalgorithms Utilized TensorFlow and PyTorch for developing predictive models Analyzed\\nand visualized large data sets to drive business strategies\\n2016  2018 Junior Data Analyst Facebook  Menlo Park USA\\nAnalyzed and visualized large datasets using Python and R Assisted in the development of\\nmachine learning models for user behavior prediction Conducted A/B testing and provided\\ninsights to the product team\\nEducation\\n2014  2016 Master of Science in Computer Science Specialization in AI MIT Cambridge\\nUSA\\n2010  2014 Bachelor of Science in Computer Science UC San Diego  San Diego USA\\nProjects\\n2021 \\nPresentPersonal Project Predictive Stock Trading Model\\nDeveloped a predictive stock trading model using deep learning and time series analysis\\nUsed PyTorch for model development and Docker for deployment The model predicts stock\\nprices with a high degree of accuracy and automates trading decisions\"\n",
      "    },\n",
      "    {\n",
      "        \"resume\": \"Bruce Wayne\\nMERN Stack Developer123 Gotham St\\nGotham USA\\nobilealt\\n/envel\\n/linkedininbwayne\\n\\nProfessional Summary\\nHighly skilled MERN Stack Developer with over 10 years of experience specializing in designing\\nbuilding and maintaining complex web applications Proficient in MongoDB Expressjs React and\\nNodejs Currently contributing to the development of AI technologies at OpenAI with a primary\\nfocus on the ChatGPT project\\nSkills\\nJavaScript and TypeScript MongoDB Expressjs React Nodejs MERN stack\\nRESTful APIs Git and GitHub Docker and Kubernetes Agile and Scrum Python\\nand Machine Learning basics\\nExperience\\nJune 2020 \\nPresentMERN Stack Developer OpenAI  San Francisco USA\\nWorking on the development of the ChatGPT project using Nodejs Expressjs and React\\nImplementing RESTful services for communication between frontend and backend Utilizing\\nDocker and Kubernetes for deployment and management of applications Working in an\\nAgile environment delivering highquality software every sprint Contributing to the design\\nand implementation of machine learning algorithms for natural language processing tasks\\nJuly 2015 \\nMay 2020Full Stack Developer Uber  San Francisco USA\\nDeveloped and maintained scalable web applications using MERN stack Ensured the\\nperformance quality and responsiveness of applications Successfully deployed solutions\\nusing Docker and Kubernetes Collaborated with a team of engineers product managers\\nand UX designers Led a team of junior developers conducted code reviews and ensured\\nadherence to best coding practices Worked closely with the data science team to optimize\\nrecommendation algorithms and enhance user experience\\nJune 2012 \\nJune 2015Software Developer Facebook  Menlo Park USA\\nDeveloped features for the Facebook web application using React Ensured the performance\\nof the MongoDB databases Utilized RESTful APIs for communication between different\\nparts of the application Worked in a fastpaced testdriven development environment\\nAssisted in migrating the legacy system to a modern MERN stack architecture\\nEducation\\n2009  2012 PhD in Computer Science CalTech  Pasadena USA\\n2007  2009 Master of Science in Computer Science MIT Cambridge USA\\n2003  2007 Bachelor of Science in Computer Science UC San Diego  San Diego USA\\n1/2 Projects\\n2019 \\nPresentPersonal Project Gotham Event Planner\\nCreated a fullfeatured web application to plan and organize events in Gotham city Used\\nMERN stack for development and Docker for deployment The application allows users to\\ncreate manage and share events and integrates with Google Maps API to display event\\nlocations\\n2/2\"\n",
      "    },\n",
      "    {\n",
      "        \"resume\": \"Barry Allen\\nFrontEnd DeveloperGoogle HQ Mountain View CA USA\\nobilealt\\n/envel\\n/linkedininbwayne\\n\\nObjective\\nSeeking a challenging role as a FrontEnd Developer where I can leverage my knowledge of UI/UX\\ndesign and modern web technologies to create intuitive and engaging user interfaces\\nEducation\\n2018  2022 BTech Computer Science and Engineering Indian Institute of Technology\\nDelhi  New Delhi India\\nOverall GPA 95/10\\nSkills\\nJavaScript ES6+ TypeScript HTML5 CSS3 Python React Redux Angular\\nVuejs Nodejs Expressjs D3js Git Docker Webpack Babel Google Cloud\\nPlatform Firebase RESTful APIs GraphQL Agile Development TestDriven\\nDevelopment Responsive Design UI/UX\\nExperience\\nJune 2022 \\nPresentSoftware Engineer FrontEnd Google  Mountain View CA USA\\nDeveloping intuitive and engaging user interfaces using React and Redux Working closely\\nwith UX designers to implement responsive and accessible web design Participating in\\nagile development processes including sprint planning and code reviews Collaborating with\\nbackend developers to integrate RESTful APIs and ensure seamless data flow\\nProjects\\n2022 Personal Expense Tracker\\nDeveloped a personal expense tracker application using React Redux and Firebase Imple\\nmented user authentication using Firebase Auth and data storage using Firestore Utilized\\nD3js for data visualization to provide users with insights into their spending patterns\"\n",
      "    },\n",
      "]\n",
      "\n",
      "import json\n",
      "\n",
      "from scripts.utils.ReadFiles import get_filenames_from_dir\n",
      "\n",
      "resume_path = \"Data/Processed/Resumes\"\n",
      "job_path = \"Data/Processed/JobDescription\"\n",
      "\n",
      "\n",
      "def read_json(filename):\n",
      "    with open(filename) as f:\n",
      "        data = json.load(f)\n",
      "    return data\n",
      "\n",
      "\n",
      "def build_resume_list(resume_names, path):\n",
      "    resumes = []\n",
      "    for resume in resume_names:\n",
      "        selected_file = read_json(path + \"/\" + resume)\n",
      "        resumes.append({\"resume\": selected_file[\"clean_data\"]})\n",
      "    return resumes\n",
      "\n",
      "\n",
      "def build_jobdesc_list(jobdesc_names, path):\n",
      "    resumes = []\n",
      "    for resume in resume_names:\n",
      "        selected_file = read_json(path + \"/\" + resume)\n",
      "        resumes.append({\"resume\": selected_file[\"clean_data\"]})\n",
      "    return resumes\n",
      "\n",
      "\n",
      "resume_names = get_filenames_from_dir(resume_path)\n",
      "resumes = build_resume_list(resume_names, resume_path)\n",
      "\n",
      "jobdesc_names = get_filenames_from_dir(job_path)\n",
      "jobdescs = build_jobdesc_list(jobdesc_names, job_path)\n",
      "\n",
      "print(resumes)\n",
      "print(jobdescs)\n",
      "\n",
      "def list_to_matrix(list_to_convert, num_columns):\n",
      "    \"\"\"Converts a list to a matrix of a suitable size.\n",
      "\n",
      "    Args:\n",
      "      list_to_convert: The list to convert.\n",
      "      num_columns: The number of columns in the matrix.\n",
      "\n",
      "    Returns:\n",
      "      A matrix of the specified size, with the contents of the list.\n",
      "    \"\"\"\n",
      "\n",
      "    matrix = []\n",
      "    for i in range(len(list_to_convert) // num_columns):\n",
      "        matrix.append(list_to_convert[i * num_columns : (i + 1) * num_columns])\n",
      "\n",
      "    if len(list_to_convert) % num_columns > 0:\n",
      "        matrix.append(list_to_convert[-(len(list_to_convert) % num_columns) :])\n",
      "\n",
      "    for i in range(len(matrix)):\n",
      "        for j in range(len(matrix[i])):\n",
      "            if matrix[i][j] is None:\n",
      "                matrix[i][j] = \"\"\n",
      "\n",
      "    return matrix\n",
      "\n",
      "\n",
      "def split_list(list_to_split, chunk_size):\n",
      "    \"\"\"Splits a list into 3 equal lists.\n",
      "\n",
      "    Args:\n",
      "      list_to_split: The list to split.\n",
      "      chunk_size: The size of each chunk.\n",
      "\n",
      "    Returns:\n",
      "      A list of chunk_size (+1 if over) lists, each of which is a chunk of the original list.\n",
      "    \"\"\"\n",
      "\n",
      "    num_chunks = len(list_to_split) // chunk_size\n",
      "    remainder = len(list_to_split) % chunk_size\n",
      "\n",
      "    chunks = []\n",
      "    for i in range(num_chunks):\n",
      "        chunks.append(list_to_split[i * chunk_size : (i + 1) * chunk_size])\n",
      "\n",
      "    if remainder > 0:\n",
      "        chunks.append(list_to_split[num_chunks * chunk_size :])\n",
      "\n",
      "    return chunks\n",
      "\n",
      "\n",
      "def dirty_intersection(list1, list2):\n",
      "    intersection = list(set(list1) & set(list2))\n",
      "    remainder_1 = [x for x in list1 if x not in intersection]\n",
      "    remainder_2 = [x for x in list2 if x not in intersection]\n",
      "\n",
      "    output = pd.DataFrame(\n",
      "        {\n",
      "            \"elements\": [\n",
      "                \"Common words\",\n",
      "                \"Words unique to Resume\",\n",
      "                \"Words unique to Job Description\",\n",
      "            ],\n",
      "            \"values\": [len(intersection), len(remainder_1), len(remainder_2)],\n",
      "        },\n",
      "        index=[1, 2, 3],\n",
      "    )\n",
      "\n",
      "    return output\n",
      "\n",
      "\n",
      "def find_intersection_of_lists(list1, list2):\n",
      "    \"\"\"Finds the intersection of two lists and returns the result as a Pandas dataframe.\n",
      "\n",
      "    Args:\n",
      "      list1: The first list.\n",
      "      list2: The second list.\n",
      "\n",
      "    Returns:\n",
      "      A Pandas dataframe with three columns: `intersection`, `remainder_1`, and `remainder_2`.\n",
      "    \"\"\"\n",
      "\n",
      "    def max_of_three(a, b, c):\n",
      "        max_value = a\n",
      "        if b > max_value:\n",
      "            max_value = b\n",
      "        if c > max_value:\n",
      "            max_value = c\n",
      "\n",
      "        return max_value\n",
      "\n",
      "    def fill_by_complements(num: int, list_to_fill: list):\n",
      "        if num > len(list_to_fill):\n",
      "            for i in range(num - len(list_to_fill)):\n",
      "                list_to_fill.append(\" \")\n",
      "\n",
      "    intersection = list(set(list1) & set(list2))\n",
      "    remainder_1 = [x for x in list1 if x not in intersection]\n",
      "    remainder_2 = [x for x in list2 if x not in intersection]\n",
      "\n",
      "    max_count = max_of_three(len(intersection), len(remainder_1), len(remainder_2))\n",
      "\n",
      "    fill_by_complements(max_count, intersection)\n",
      "    fill_by_complements(max_count, remainder_1)\n",
      "    fill_by_complements(max_count, remainder_2)\n",
      "\n",
      "    df = pd.DataFrame(\n",
      "        {\n",
      "            \"intersection\": intersection,\n",
      "            \"remainder_1\": remainder_1,\n",
      "            \"remainder_2\": remainder_2,\n",
      "        }\n",
      "    )\n",
      "\n",
      "    return df\n",
      "\n",
      "\n",
      "def preprocess_text(text):\n",
      "    \"\"\"Preprocesses text using spacy.\n",
      "\n",
      "    Args:\n",
      "      text: The text to preprocess.\n",
      "\n",
      "    Returns:\n",
      "      A list of string tokens.\n",
      "    \"\"\"\n",
      "\n",
      "    nlp = spacy.load(\"en_core_web_sm\")\n",
      "    doc = nlp(text)\n",
      "\n",
      "    # Lemmatize words.\n",
      "    tokens = [token.lemma_ for token in doc]\n",
      "\n",
      "    # Remove stopwords.\n",
      "    stopwords = spacy.lang.en.stop_words.STOP_WORDS\n",
      "    tokens = [token for token in tokens if token not in stopwords]\n",
      "\n",
      "    # Remove punctuation.\n",
      "    punctuation = set(string.punctuation)\n",
      "    tokens = [token for token in tokens if token not in punctuation]\n",
      "\n",
      "    return tokens\n",
      "\n",
      "\n",
      "# Usage\n",
      "\n",
      "# resume_list = preprocess_text(resume['clean_data'])\n",
      "\n",
      "# job_desc_list = preprocess_text(job_desc['clean_data'])\n",
      "\n",
      "# df_data = find_intersection_of_lists(resume_list, job_desc_list)\n",
      "\n",
      "# data_length = dirty_intersection(resume_list, job_desc_list)\n",
      "\n",
      "# # data_length = data_length\n",
      "\n",
      "# st.write(df_data)\n",
      "\n",
      "# st.write(data_length)\n",
      "\n",
      "# # st.write(px.data.tips())\n",
      "\n",
      "# fig = px.pie(data_length, values='values', names='elements')\n",
      "# st.write(fig)\n",
      "\n",
      "# st.text(resume['clean_data'])\n",
      "\n",
      "\n",
      "# fig = go.Figure(data=[go.Table(\n",
      "#     header=dict(values=[\"Extracted Keywords\"],\n",
      "#                 fill_color='#1D267D',\n",
      "#                 align='center', font=dict(color='white', size=16)),\n",
      "#     cells=dict(values=[out for out in split_list(resume['extracted_keywords'], 25)],\n",
      "#                fill_color='#19A7CE',\n",
      "#                align='left'))])\n",
      "\n",
      "# fig.update_layout(\n",
      "#     uniformtext_minsize=13\n",
      "# )\n",
      "\n",
      "import json\n",
      "import os.path\n",
      "import pathlib\n",
      "\n",
      "from scripts.parsers.ParseJobDescToJson import ParseJobDesc\n",
      "from scripts.parsers.ParseResumeToJson import ParseResume\n",
      "from scripts.ReadPdf import read_single_pdf\n",
      "\n",
      "READ_DATA_FROM = \"Data/Raw/\"\n",
      "SAVE_DIRECTORY = \"Data/Processed/\"\n",
      "\n",
      "\n",
      "def read_resumes(input_file: str) -> dict:\n",
      "    input_file_name = os.path.join(READ_DATA_FROM + input_file)\n",
      "    data = read_single_pdf(input_file_name)\n",
      "    output = ParseResume(data).get_JSON()\n",
      "    return output\n",
      "\n",
      "\n",
      "def read_job_desc(input_file: str) -> dict:\n",
      "    input_file_name = os.path.join(READ_DATA_FROM + input_file)\n",
      "    data = read_single_pdf(input_file_name)\n",
      "    output = ParseJobDesc(data).get_JSON()\n",
      "    return output\n",
      "\n",
      "\n",
      "def write_json_file(resume_dictionary: dict):\n",
      "    file_name = str(\"Resume-\" + resume_dictionary[\"unique_id\"] + \".json\")\n",
      "    save_directory_name = pathlib.Path(SAVE_DIRECTORY) / file_name\n",
      "    json_object = json.dumps(resume_dictionary, sort_keys=True, indent=14)\n",
      "    with open(save_directory_name, \"w+\") as outfile:\n",
      "        outfile.write(json_object)\n",
      "\n",
      "import json\n",
      "import string\n",
      "\n",
      "import matplotlib.pyplot as plt\n",
      "import pandas as pd\n",
      "import plotly.express as px\n",
      "import plotly.graph_objects as go\n",
      "import pywaffle\n",
      "import spacy\n",
      "import squarify\n",
      "import streamlit as st\n",
      "\n",
      "st.title(\"Resume :blue[Matcher]\")\n",
      "st.image(\"Assets/img/header_image.jpg\")\n",
      "st.subheader(\"_AI Based Resume Analyzer & Ranker_\")\n",
      "\n",
      "\n",
      "def read_json(filename):\n",
      "    with open(filename) as f:\n",
      "        data = json.load(f)\n",
      "    return data\n",
      "\n",
      "\n",
      "# read the json file\n",
      "resume = read_json(\"Data/Processed/Resume-d531571e-e4fa-45eb-ab6a-267cdeb6647e.json\")\n",
      "job_desc = read_json(\n",
      "    \"Data/Processed/Job-Desc-a4f06ccb-8d5a-4d0b-9f02-3ba6d686472e.json\"\n",
      ")\n",
      "\n",
      "st.write(\"### Reading Resume's POS\")\n",
      "df = pd.DataFrame(resume[\"pos_frequencies\"], index=[0])\n",
      "fig = go.Figure(\n",
      "    data=go.Bar(\n",
      "        y=list(resume[\"pos_frequencies\"].values()),\n",
      "        x=list(resume[\"pos_frequencies\"].keys()),\n",
      "    ),\n",
      "    layout_title_text=\"Resume's POS\",\n",
      ")\n",
      "st.write(fig)\n",
      "\n",
      "df2 = pd.DataFrame(resume[\"keyterms\"], columns=[\"keyword\", \"value\"])\n",
      "st.dataframe(df2)\n",
      "\n",
      "# Create the dictionary\n",
      "keyword_dict = {}\n",
      "for keyword, value in resume[\"keyterms\"]:\n",
      "    keyword_dict[keyword] = value\n",
      "\n",
      "fig = go.Figure(\n",
      "    data=[\n",
      "        go.Table(\n",
      "            header=dict(\n",
      "                values=[\"Keyword\", \"Value\"], font=dict(size=12), fill_color=\"#070A52\"\n",
      "            ),\n",
      "            cells=dict(\n",
      "                values=[list(keyword_dict.keys()), list(keyword_dict.values())],\n",
      "                line_color=\"darkslategray\",\n",
      "                fill_color=\"#6DA9E4\",\n",
      "            ),\n",
      "        )\n",
      "    ]\n",
      ")\n",
      "st.plotly_chart(fig)\n",
      "\n",
      "st.divider()\n",
      "\n",
      "for keyword, value in resume[\"keyterms\"]:\n",
      "    pass\n",
      "\n",
      "\n",
      "# display the waffle chart\n",
      "figure = plt.figure(\n",
      "    FigureClass=pywaffle.Waffle,\n",
      "    rows=20,\n",
      "    columns=20,\n",
      "    values=keyword_dict,\n",
      "    legend={\"loc\": \"upper left\", \"bbox_to_anchor\": (1, 1)},\n",
      ")\n",
      "\n",
      "\n",
      "# Display the dictionary\n",
      "\n",
      "st.pyplot(fig=figure)\n",
      "# st.write(dict)\n",
      "\n",
      "fig = px.treemap(\n",
      "    df2,\n",
      "    path=[\"keyword\"],\n",
      "    values=\"value\",\n",
      "    color_continuous_scale=\"RdBu\",\n",
      "    title=\"Resume POS\",\n",
      ")\n",
      "st.write(fig)\n",
      "\n",
      "\n",
      "st.plotly_chart(figure_or_data=fig)\n",
      "\n",
      "fig = go.Figure(\n",
      "    data=[\n",
      "        go.Table(\n",
      "            header=dict(\n",
      "                values=[\"Tri Grams\"],\n",
      "                fill_color=\"#1D267D\",\n",
      "                align=\"center\",\n",
      "                font=dict(color=\"white\", size=16),\n",
      "            ),\n",
      "            cells=dict(\n",
      "                values=[resume[\"tri_grams\"]], fill_color=\"#19A7CE\", align=\"left\"\n",
      "            ),\n",
      "        )\n",
      "    ]\n",
      ")\n",
      "\n",
      "st.plotly_chart(figure_or_data=fig)\n",
      "\n",
      "fig = go.Figure(\n",
      "    data=[\n",
      "        go.Table(\n",
      "            header=dict(\n",
      "                values=[\"Bi Grams\"],\n",
      "                fill_color=\"#1D267D\",\n",
      "                align=\"center\",\n",
      "                font=dict(color=\"white\", size=16),\n",
      "            ),\n",
      "            cells=dict(values=[resume[\"bi_grams\"]], fill_color=\"#19A7CE\", align=\"left\"),\n",
      "        )\n",
      "    ]\n",
      ")\n",
      "\n",
      "st.plotly_chart(figure_or_data=fig)\n",
      "\n",
      "import json\n",
      "import logging\n",
      "import os\n",
      "\n",
      "from scripts import JobDescriptionProcessor, ResumeProcessor\n",
      "from scripts.utils import get_filenames_from_dir, init_logging_config\n",
      "\n",
      "init_logging_config()\n",
      "\n",
      "PROCESSED_RESUMES_PATH = \"Data/Processed/Resumes\"\n",
      "PROCESSED_JOB_DESCRIPTIONS_PATH = \"Data/Processed/JobDescription\"\n",
      "\n",
      "\n",
      "def read_json(filename):\n",
      "    with open(filename) as f:\n",
      "        data = json.load(f)\n",
      "    return data\n",
      "\n",
      "\n",
      "def remove_old_files(files_path):\n",
      "\n",
      "    for filename in os.listdir(files_path):\n",
      "        try:\n",
      "            file_path = os.path.join(files_path, filename)\n",
      "\n",
      "            if os.path.isfile(file_path):\n",
      "                os.remove(file_path)\n",
      "        except Exception as e:\n",
      "            logging.error(f\"Error deleting {file_path}:\\n{e}\")\n",
      "\n",
      "    logging.info(\"Deleted old files from \" + files_path)\n",
      "\n",
      "\n",
      "logging.info(\"Started to read from Data/Resumes\")\n",
      "try:\n",
      "    # Check if there are resumes present or not.\n",
      "    # If present then parse it.\n",
      "    remove_old_files(PROCESSED_RESUMES_PATH)\n",
      "\n",
      "    file_names = get_filenames_from_dir(\"Data/Resumes\")\n",
      "    logging.info(\"Reading from Data/Resumes is now complete.\")\n",
      "except:\n",
      "    # Exit the program if there are no resumes.\n",
      "    logging.error(\"There are no resumes present in the specified folder.\")\n",
      "    logging.error(\"Exiting from the program.\")\n",
      "    logging.error(\"Please add resumes in the Data/Resumes folder and try again.\")\n",
      "    exit(1)\n",
      "\n",
      "# Now after getting the file_names parse the resumes into a JSON Format.\n",
      "logging.info(\"Started parsing the resumes.\")\n",
      "for file in file_names:\n",
      "    processor = ResumeProcessor(file)\n",
      "    success = processor.process()\n",
      "logging.info(\"Parsing of the resumes is now complete.\")\n",
      "\n",
      "logging.info(\"Started to read from Data/JobDescription\")\n",
      "try:\n",
      "    # Check if there are resumes present or not.\n",
      "    # If present then parse it.\n",
      "    remove_old_files(PROCESSED_JOB_DESCRIPTIONS_PATH)\n",
      "\n",
      "    file_names = get_filenames_from_dir(\"Data/JobDescription\")\n",
      "    logging.info(\"Reading from Data/JobDescription is now complete.\")\n",
      "except:\n",
      "    # Exit the program if there are no resumes.\n",
      "    logging.error(\"There are no job-description present in the specified folder.\")\n",
      "    logging.error(\"Exiting from the program.\")\n",
      "    logging.error(\"Please add resumes in the Data/JobDescription folder and try again.\")\n",
      "    exit(1)\n",
      "\n",
      "# Now after getting the file_names parse the resumes into a JSON Format.\n",
      "logging.info(\"Started parsing the Job Descriptions.\")\n",
      "for file in file_names:\n",
      "    processor = JobDescriptionProcessor(file)\n",
      "    success = processor.process()\n",
      "logging.info(\"Parsing of the Job Descriptions is now complete.\")\n",
      "logging.info(\"Success now run `streamlit run streamlit_second.py`\")\n",
      "\n",
      "import re\n",
      "import urllib.request\n",
      "\n",
      "import spacy\n",
      "\n",
      "from .utils import TextCleaner\n",
      "\n",
      "# Load the English model\n",
      "nlp = spacy.load(\"en_core_web_sm\")\n",
      "\n",
      "\n",
      "RESUME_SECTIONS = [\n",
      "    \"Contact Information\",\n",
      "    \"Objective\",\n",
      "    \"Summary\",\n",
      "    \"Education\",\n",
      "    \"Experience\",\n",
      "    \"Skills\",\n",
      "    \"Projects\",\n",
      "    \"Certifications\",\n",
      "    \"Licenses\",\n",
      "    \"Awards\",\n",
      "    \"Honors\",\n",
      "    \"Publications\",\n",
      "    \"References\",\n",
      "    \"Technical Skills\",\n",
      "    \"Computer Skills\",\n",
      "    \"Programming Languages\",\n",
      "    \"Software Skills\",\n",
      "    \"Soft Skills\",\n",
      "    \"Language Skills\",\n",
      "    \"Professional Skills\",\n",
      "    \"Transferable Skills\",\n",
      "    \"Work Experience\",\n",
      "    \"Professional Experience\",\n",
      "    \"Employment History\",\n",
      "    \"Internship Experience\",\n",
      "    \"Volunteer Experience\",\n",
      "    \"Leadership Experience\",\n",
      "    \"Research Experience\",\n",
      "    \"Teaching Experience\",\n",
      "]\n",
      "\n",
      "\n",
      "class DataExtractor:\n",
      "    \"\"\"\n",
      "    A class for extracting various types of data from text.\n",
      "    \"\"\"\n",
      "\n",
      "    def __init__(self, raw_text: str):\n",
      "        \"\"\"\n",
      "        Initialize the DataExtractor object.\n",
      "\n",
      "        Args:\n",
      "            raw_text (str): The raw input text.\n",
      "        \"\"\"\n",
      "\n",
      "        self.text = raw_text\n",
      "        self.clean_text = TextCleaner.clean_text(self.text)\n",
      "        self.doc = nlp(self.clean_text)\n",
      "\n",
      "    def extract_links(self):\n",
      "        \"\"\"\n",
      "        Find links of any type in a given string.\n",
      "\n",
      "        Args:\n",
      "            text (str): The string to search for links.\n",
      "\n",
      "        Returns:\n",
      "            list: A list containing all the found links.\n",
      "        \"\"\"\n",
      "        link_pattern = r\"\\b(?:https?://|www\\.)\\S+\\b\"\n",
      "        links = re.findall(link_pattern, self.text)\n",
      "        return links\n",
      "\n",
      "    def extract_links_extended(self):\n",
      "        \"\"\"\n",
      "        Extract links of all kinds (HTTP, HTTPS, FTP, email, www.linkedin.com,\n",
      "          and github.com/user_name) from a webpage.\n",
      "\n",
      "        Args:\n",
      "            url (str): The URL of the webpage.\n",
      "\n",
      "        Returns:\n",
      "            list: A list containing all the extracted links.\n",
      "        \"\"\"\n",
      "        links = []\n",
      "        try:\n",
      "            response = urllib.request.urlopen(self.text)\n",
      "            html_content = response.read().decode(\"utf-8\")\n",
      "            pattern = r'href=[\\'\"]?([^\\'\" >]+)'\n",
      "            raw_links = re.findall(pattern, html_content)\n",
      "            for link in raw_links:\n",
      "                if link.startswith(\n",
      "                    (\n",
      "                        \"http://\",\n",
      "                        \"https://\",\n",
      "                        \"ftp://\",\n",
      "                        \"mailto:\",\n",
      "                        \"www.linkedin.com\",\n",
      "                        \"github.com/\",\n",
      "                        \"twitter.com\",\n",
      "                    )\n",
      "                ):\n",
      "                    links.append(link)\n",
      "        except Exception as e:\n",
      "            print(f\"Error extracting links: {str(e)}\")\n",
      "        return links\n",
      "\n",
      "    def extract_names(self):\n",
      "        \"\"\"Extracts and returns a list of names from the given\n",
      "        text using spaCy's named entity recognition.\n",
      "\n",
      "        Args:\n",
      "            text (str): The text to extract names from.\n",
      "\n",
      "        Returns:\n",
      "            list: A list of strings representing the names extracted from the text.\n",
      "        \"\"\"\n",
      "        names = [ent.text for ent in self.doc.ents if ent.label_ == \"PERSON\"]\n",
      "        return names\n",
      "\n",
      "    def extract_emails(self):\n",
      "        \"\"\"\n",
      "        Extract email addresses from a given string.\n",
      "\n",
      "        Args:\n",
      "            text (str): The string from which to extract email addresses.\n",
      "\n",
      "        Returns:\n",
      "            list: A list containing all the extracted email addresses.\n",
      "        \"\"\"\n",
      "        email_pattern = r\"\\b[A-Za-z0-9._%+-]+@[A-Za-z0-9.-]+\\.[A-Za-z]{2,}\\b\"\n",
      "        emails = re.findall(email_pattern, self.text)\n",
      "        return emails\n",
      "\n",
      "    def extract_phone_numbers(self):\n",
      "        \"\"\"\n",
      "        Extract phone numbers from a given string.\n",
      "\n",
      "        Args:\n",
      "            text (str): The string from which to extract phone numbers.\n",
      "\n",
      "        Returns:\n",
      "            list: A list containing all the extracted phone numbers.\n",
      "        \"\"\"\n",
      "        phone_number_pattern = (\n",
      "            r\"^(\\+\\d{1,3})?[-.\\s]?\\(?\\d{3}\\)?[-.\\s]?\\d{3}[-.\\s]?\\d{4}$\"\n",
      "        )\n",
      "        phone_numbers = re.findall(phone_number_pattern, self.text)\n",
      "        return phone_numbers\n",
      "\n",
      "    def extract_experience(self):\n",
      "        \"\"\"\n",
      "        Extract experience from a given string. It does so by using the Spacy module.\n",
      "\n",
      "        Args:\n",
      "            text (str): The string from which to extract experience.\n",
      "\n",
      "        Returns:\n",
      "            str: A string containing all the extracted experience.\n",
      "        \"\"\"\n",
      "        experience_section = []\n",
      "        in_experience_section = False\n",
      "\n",
      "        for token in self.doc:\n",
      "            if token.text in RESUME_SECTIONS:\n",
      "                if token.text == \"Experience\" or \"EXPERIENCE\" or \"experience\":\n",
      "                    in_experience_section = True\n",
      "                else:\n",
      "                    in_experience_section = False\n",
      "\n",
      "            if in_experience_section:\n",
      "                experience_section.append(token.text)\n",
      "\n",
      "        return \" \".join(experience_section)\n",
      "\n",
      "    def extract_position_year(self):\n",
      "        \"\"\"\n",
      "        Extract position and year from a given string.\n",
      "\n",
      "        Args:\n",
      "            text (str): The string from which to extract position and year.\n",
      "\n",
      "        Returns:\n",
      "            list: A list containing the extracted position and year.\n",
      "        \"\"\"\n",
      "        position_year_search_pattern = (\n",
      "            r\"(\\b\\w+\\b\\s+\\b\\w+\\b),\\s+(\\d{4})\\s*-\\s*(\\d{4}|\\bpresent\\b)\"\n",
      "        )\n",
      "        position_year = re.findall(position_year_search_pattern, self.text)\n",
      "        return position_year\n",
      "\n",
      "    def extract_particular_words(self):\n",
      "        \"\"\"\n",
      "        Extract nouns and proper nouns from the given text.\n",
      "\n",
      "        Args:\n",
      "            text (str): The input text to extract nouns from.\n",
      "\n",
      "        Returns:\n",
      "            list: A list of extracted nouns.\n",
      "        \"\"\"\n",
      "        pos_tags = [\"NOUN\", \"PROPN\"]\n",
      "        nouns = [token.text for token in self.doc if token.pos_ in pos_tags]\n",
      "        return nouns\n",
      "\n",
      "    def extract_entities(self):\n",
      "        \"\"\"\n",
      "        Extract named entities of types 'GPE' (geopolitical entity) and 'ORG' (organization) from the given text.\n",
      "\n",
      "        Args:\n",
      "            text (str): The input text to extract entities from.\n",
      "\n",
      "        Returns:\n",
      "            list: A list of extracted entities.\n",
      "        \"\"\"\n",
      "        entity_labels = [\"GPE\", \"ORG\"]\n",
      "        entities = [\n",
      "            token.text for token in self.doc.ents if token.label_ in entity_labels\n",
      "        ]\n",
      "        return list(set(entities))\n",
      "\n",
      "import json\n",
      "import os.path\n",
      "import pathlib\n",
      "\n",
      "from .parsers import ParseJobDesc, ParseResume\n",
      "from .ReadPdf import read_single_pdf\n",
      "\n",
      "READ_JOB_DESCRIPTION_FROM = \"Data/JobDescription/\"\n",
      "SAVE_DIRECTORY = \"Data/Processed/JobDescription\"\n",
      "\n",
      "\n",
      "class JobDescriptionProcessor:\n",
      "    def __init__(self, input_file):\n",
      "        self.input_file = input_file\n",
      "        self.input_file_name = os.path.join(READ_JOB_DESCRIPTION_FROM + self.input_file)\n",
      "\n",
      "    def process(self) -> bool:\n",
      "        try:\n",
      "            resume_dict = self._read_resumes()\n",
      "            self._write_json_file(resume_dict)\n",
      "            return True\n",
      "        except Exception as e:\n",
      "            print(f\"An error occurred: {str(e)}\")\n",
      "            return False\n",
      "\n",
      "    def _read_resumes(self) -> dict:\n",
      "        data = read_single_pdf(self.input_file_name)\n",
      "        output = ParseResume(data).get_JSON()\n",
      "        return output\n",
      "\n",
      "    def _read_job_desc(self) -> dict:\n",
      "        data = read_single_pdf(self.input_file_name)\n",
      "        output = ParseJobDesc(data).get_JSON()\n",
      "        return output\n",
      "\n",
      "    def _write_json_file(self, resume_dictionary: dict):\n",
      "        file_name = str(\n",
      "            \"JobDescription-\"\n",
      "            + self.input_file\n",
      "            + resume_dictionary[\"unique_id\"]\n",
      "            + \".json\"\n",
      "        )\n",
      "        save_directory_name = pathlib.Path(SAVE_DIRECTORY) / file_name\n",
      "        json_object = json.dumps(resume_dictionary, sort_keys=True, indent=14)\n",
      "        with open(save_directory_name, \"w+\") as outfile:\n",
      "            outfile.write(json_object)\n",
      "\n",
      "import textacy\n",
      "from textacy import extract\n",
      "\n",
      "\n",
      "class KeytermExtractor:\n",
      "    \"\"\"\n",
      "    A class for extracting keyterms from a given text using various algorithms.\n",
      "    \"\"\"\n",
      "\n",
      "    def __init__(self, raw_text: str, top_n_values: int = 20):\n",
      "        \"\"\"\n",
      "        Initialize the KeytermExtractor object.\n",
      "\n",
      "        Args:\n",
      "            raw_text (str): The raw input text.\n",
      "            top_n_values (int): The number of top keyterms to extract.\n",
      "        \"\"\"\n",
      "        self.raw_text = raw_text\n",
      "        self.text_doc = textacy.make_spacy_doc(self.raw_text, lang=\"en_core_web_md\")\n",
      "        self.top_n_values = top_n_values\n",
      "\n",
      "    def get_keyterms_based_on_textrank(self):\n",
      "        \"\"\"\n",
      "        Extract keyterms using the TextRank algorithm.\n",
      "\n",
      "        Returns:\n",
      "            List[str]: A list of top keyterms based on TextRank.\n",
      "        \"\"\"\n",
      "        return list(\n",
      "            extract.keyterms.textrank(\n",
      "                self.text_doc, normalize=\"lemma\", topn=self.top_n_values\n",
      "            )\n",
      "        )\n",
      "\n",
      "    def get_keyterms_based_on_sgrank(self):\n",
      "        \"\"\"\n",
      "        Extract keyterms using the SGRank algorithm.\n",
      "\n",
      "        Returns:\n",
      "            List[str]: A list of top keyterms based on SGRank.\n",
      "        \"\"\"\n",
      "        return list(\n",
      "            extract.keyterms.sgrank(\n",
      "                self.text_doc, normalize=\"lemma\", topn=self.top_n_values\n",
      "            )\n",
      "        )\n",
      "\n",
      "    def get_keyterms_based_on_scake(self):\n",
      "        \"\"\"\n",
      "        Extract keyterms using the sCAKE algorithm.\n",
      "\n",
      "        Returns:\n",
      "            List[str]: A list of top keyterms based on sCAKE.\n",
      "        \"\"\"\n",
      "        return list(\n",
      "            extract.keyterms.scake(\n",
      "                self.text_doc, normalize=\"lemma\", topn=self.top_n_values\n",
      "            )\n",
      "        )\n",
      "\n",
      "    def get_keyterms_based_on_yake(self):\n",
      "        \"\"\"\n",
      "        Extract keyterms using the YAKE algorithm.\n",
      "\n",
      "        Returns:\n",
      "            List[str]: A list of top keyterms based on YAKE.\n",
      "        \"\"\"\n",
      "        return list(\n",
      "            extract.keyterms.yake(\n",
      "                self.text_doc, normalize=\"lemma\", topn=self.top_n_values\n",
      "            )\n",
      "        )\n",
      "\n",
      "    def bi_gramchunker(self):\n",
      "        \"\"\"\n",
      "        Chunk the text into bigrams.\n",
      "\n",
      "        Returns:\n",
      "            List[str]: A list of bigrams.\n",
      "        \"\"\"\n",
      "        return list(\n",
      "            textacy.extract.basics.ngrams(\n",
      "                self.text_doc,\n",
      "                n=2,\n",
      "                filter_stops=True,\n",
      "                filter_nums=True,\n",
      "                filter_punct=True,\n",
      "            )\n",
      "        )\n",
      "\n",
      "    def tri_gramchunker(self):\n",
      "        \"\"\"\n",
      "        Chunk the text into trigrams.\n",
      "\n",
      "        Returns:\n",
      "            List[str]: A list of trigrams.\n",
      "        \"\"\"\n",
      "        return list(\n",
      "            textacy.extract.basics.ngrams(\n",
      "                self.text_doc,\n",
      "                n=3,\n",
      "                filter_stops=True,\n",
      "                filter_nums=True,\n",
      "                filter_punct=True,\n",
      "            )\n",
      "        )\n",
      "\n",
      "import logging\n",
      "from os import listdir\n",
      "from os.path import isfile, join\n",
      "\n",
      "import easygui\n",
      "import requests\n",
      "from bs4 import BeautifulSoup\n",
      "from pathvalidate import sanitize_filename\n",
      "from xhtml2pdf import pisa\n",
      "\n",
      "\"\"\"\n",
      "This script takes a LinkedIn job posting URL\n",
      "and converts the description to a PDF file.\n",
      "The PDF file is saved in the Data/JobDescription folder.\n",
      "The name will be OrgName__Job Title_X.pdf, where X is the number of files in the folder.\n",
      "\n",
      "IMPORTANT: Make sure the URL is to the actual job description,\n",
      "and not the job search page.\n",
      "\"\"\"\n",
      "\n",
      "\n",
      "def linkedin_to_pdf(job_url: str):\n",
      "\n",
      "    job_path = \"Data/JobDescription/\"\n",
      "    job_description = \"\"\n",
      "    files_number = len([f for f in listdir(job_path) if isfile(join(job_path, f))])\n",
      "\n",
      "    try:\n",
      "        page = requests.get(job_url)\n",
      "\n",
      "        if page.status_code != 200:\n",
      "            print(\n",
      "                f\"Failed to retrieve the job posting at {job_url}. Status code: {page.status_code}\"\n",
      "            )\n",
      "            return\n",
      "\n",
      "        # Parse the HTML content of the job posting using BeautifulSoup\n",
      "        soup = BeautifulSoup(page.text, \"html.parser\")\n",
      "\n",
      "        # Find the job title element and get the text\n",
      "        job_title = soup.find(\"h1\", {\"class\": \"topcard__title\"}).text.strip()\n",
      "\n",
      "        # Find the organization name element (try both selectors)\n",
      "        organization_element = soup.find(\"span\", {\"class\": \"topcard__flavor\"})\n",
      "\n",
      "        if not organization_element:\n",
      "            organization_element = soup.find(\"a\", {\"class\": \"topcard__org-name-link\"})\n",
      "\n",
      "        # Extract the organization name\n",
      "        organization = organization_element.text.strip()\n",
      "\n",
      "        # Find the job description element\n",
      "        job_description_element = soup.find(\n",
      "            \"div\", {\"class\": \"show-more-less-html__markup\"}\n",
      "        )\n",
      "\n",
      "        # Extract the job description and concatenate its elements\n",
      "        if job_description_element:\n",
      "            for element in job_description_element.contents:\n",
      "                job_description += str(element)\n",
      "\n",
      "        # Set file_path and sanitize organization name and job title\n",
      "        file_path = f\"{job_path}{sanitize_filename(organization + '__' + job_title)}_{files_number}.pdf\"\n",
      "\n",
      "        # Create a PDF file and write the job description to it\n",
      "        with open(file_path, \"wb\") as pdf_file:\n",
      "            pisa.CreatePDF(job_description, dest=pdf_file, encoding=\"utf-8\")\n",
      "\n",
      "        logging.info(\"PDF saved to \" + file_path)\n",
      "\n",
      "    except Exception as e:\n",
      "        logging.error(f\"Could not get the description from the URL: {job_url}\")\n",
      "        logging.error(e)\n",
      "        exit()\n",
      "\n",
      "\n",
      "if __name__ == \"__main__\":\n",
      "    url = easygui.enterbox(\"Enter the URL of the LinkedIn Job Posting:\").strip()\n",
      "    linkedin_to_pdf(url)\n",
      "\n",
      "import glob\n",
      "import os\n",
      "\n",
      "from pypdf import PdfReader\n",
      "\n",
      "\n",
      "def get_pdf_files(file_path):\n",
      "    \"\"\"\n",
      "    Get all PDF files from the specified file path.\n",
      "\n",
      "    Args:\n",
      "        file_path (str): The directory path containing the PDF files.\n",
      "\n",
      "    Returns:\n",
      "        list: A list containing the paths of all the PDF files in the directory.\n",
      "    \"\"\"\n",
      "    if os.path.exists(file_path):\n",
      "        return glob.glob(os.path.join(file_path, \"*.pdf\"))\n",
      "    else:\n",
      "        return []\n",
      "\n",
      "\n",
      "def read_multiple_pdf(file_path: str) -> list:\n",
      "    \"\"\"\n",
      "    Read multiple PDF files from the specified file path and extract the text from each page.\n",
      "\n",
      "    Args:\n",
      "        file_path (str): The directory path containing the PDF files.\n",
      "\n",
      "    Returns:\n",
      "        list: A list containing the extracted text from each page of the PDF files.\n",
      "    \"\"\"\n",
      "    pdf_files = get_pdf_files(file_path)\n",
      "    output = []\n",
      "    for file in pdf_files:\n",
      "        try:\n",
      "            with open(file, \"rb\") as f:\n",
      "                pdf_reader = PdfReader(f)\n",
      "                count = pdf_reader.getNumPages()\n",
      "                for i in range(count):\n",
      "                    page = pdf_reader.getPage(i)\n",
      "                    output.append(page.extractText())\n",
      "        except Exception as e:\n",
      "            print(f\"Error reading file '{file}': {str(e)}\")\n",
      "    return output\n",
      "\n",
      "\n",
      "def read_single_pdf(file_path: str) -> str:\n",
      "    \"\"\"\n",
      "    Read a single PDF file and extract the text from each page.\n",
      "\n",
      "    Args:\n",
      "        file_path (str): The path of the PDF file.\n",
      "\n",
      "    Returns:\n",
      "        list: A list containing the extracted text from each page of the PDF file.\n",
      "    \"\"\"\n",
      "    output = []\n",
      "    try:\n",
      "        with open(file_path, \"rb\") as f:\n",
      "            pdf_reader = PdfReader(f)\n",
      "            count = len(pdf_reader.pages)\n",
      "            for i in range(count):\n",
      "                page = pdf_reader.pages[i]\n",
      "                output.append(page.extract_text())\n",
      "    except Exception as e:\n",
      "        print(f\"Error reading file '{file_path}': {str(e)}\")\n",
      "    return str(\" \".join(output))\n",
      "\n",
      "\n",
      "def get_pdf_files(file_path: str) -> list:\n",
      "    \"\"\"\n",
      "    Get a list of PDF files from the specified directory path.\n",
      "\n",
      "    Args:\n",
      "        file_path (str): The directory path containing the PDF files.\n",
      "\n",
      "    Returns:\n",
      "        list: A list of PDF file paths.\n",
      "    \"\"\"\n",
      "    pdf_files = []\n",
      "    try:\n",
      "        pdf_files = glob.glob(os.path.join(file_path, \"*.pdf\"))\n",
      "    except Exception as e:\n",
      "        print(f\"Error getting PDF files from '{file_path}': {str(e)}\")\n",
      "    return pdf_files\n",
      "\n",
      "import json\n",
      "import os.path\n",
      "import pathlib\n",
      "\n",
      "from .parsers import ParseJobDesc, ParseResume\n",
      "from .ReadPdf import read_single_pdf\n",
      "\n",
      "READ_RESUME_FROM = \"Data/Resumes/\"\n",
      "SAVE_DIRECTORY = \"Data/Processed/Resumes\"\n",
      "\n",
      "\n",
      "class ResumeProcessor:\n",
      "    def __init__(self, input_file):\n",
      "        self.input_file = input_file\n",
      "        self.input_file_name = os.path.join(READ_RESUME_FROM + self.input_file)\n",
      "\n",
      "    def process(self) -> bool:\n",
      "        try:\n",
      "            resume_dict = self._read_resumes()\n",
      "            self._write_json_file(resume_dict)\n",
      "            return True\n",
      "        except Exception as e:\n",
      "            print(f\"An error occurred: {str(e)}\")\n",
      "            return False\n",
      "\n",
      "    def _read_resumes(self) -> dict:\n",
      "        data = read_single_pdf(self.input_file_name)\n",
      "        output = ParseResume(data).get_JSON()\n",
      "        return output\n",
      "\n",
      "    def _read_job_desc(self) -> dict:\n",
      "        data = read_single_pdf(self.input_file_name)\n",
      "        output = ParseJobDesc(data).get_JSON()\n",
      "        return output\n",
      "\n",
      "    def _write_json_file(self, resume_dictionary: dict):\n",
      "        file_name = str(\n",
      "            \"Resume-\" + self.input_file + resume_dictionary[\"unique_id\"] + \".json\"\n",
      "        )\n",
      "        save_directory_name = pathlib.Path(SAVE_DIRECTORY) / file_name\n",
      "        json_object = json.dumps(resume_dictionary, sort_keys=True, indent=14)\n",
      "        with open(save_directory_name, \"w+\") as outfile:\n",
      "            outfile.write(json_object)\n",
      "\n",
      "import string\n",
      "\n",
      "import nltk\n",
      "from nltk.corpus import stopwords\n",
      "from nltk.stem import WordNetLemmatizer\n",
      "from nltk.tokenize import word_tokenize\n",
      "\n",
      "\n",
      "class TextCleaner:\n",
      "\n",
      "    def __init__(self, raw_text):\n",
      "        self.stopwords_set = set(stopwords.words(\"english\") + list(string.punctuation))\n",
      "        self.lemmatizer = WordNetLemmatizer()\n",
      "        self.raw_input_text = raw_text\n",
      "\n",
      "    def clean_text(self) -> str:\n",
      "        tokens = word_tokenize(self.raw_input_text.lower())\n",
      "        tokens = [token for token in tokens if token not in self.stopwords_set]\n",
      "        tokens = [self.lemmatizer.lemmatize(token) for token in tokens]\n",
      "        cleaned_text = \" \".join(tokens)\n",
      "        return cleaned_text\n",
      "\n",
      "from . import ReadPdf\n",
      "from .JobDescriptionProcessor import JobDescriptionProcessor\n",
      "from .ResumeProcessor import ResumeProcessor\n",
      "\n",
      "import json\n",
      "import os\n",
      "import pathlib\n",
      "\n",
      "from scripts.Extractor import DataExtractor\n",
      "from scripts.KeytermsExtraction import KeytermExtractor\n",
      "from scripts.utils.Utils import CountFrequency, TextCleaner, generate_unique_id\n",
      "\n",
      "SAVE_DIRECTORY = \"../../Data/Processed/JobDescription\"\n",
      "\n",
      "\n",
      "class ParseJobDesc:\n",
      "\n",
      "    def __init__(self, job_desc: str):\n",
      "        self.job_desc_data = job_desc\n",
      "        self.clean_data = TextCleaner.clean_text(self.job_desc_data)\n",
      "        self.entities = DataExtractor(self.clean_data).extract_entities()\n",
      "        self.key_words = DataExtractor(self.clean_data).extract_particular_words()\n",
      "        self.pos_frequencies = CountFrequency(self.clean_data).count_frequency()\n",
      "        self.keyterms = KeytermExtractor(self.clean_data).get_keyterms_based_on_sgrank()\n",
      "        self.bi_grams = KeytermExtractor(self.clean_data).bi_gramchunker()\n",
      "        self.tri_grams = KeytermExtractor(self.clean_data).tri_gramchunker()\n",
      "\n",
      "    def get_JSON(self) -> dict:\n",
      "        \"\"\"\n",
      "        Returns a dictionary of job description data.\n",
      "        \"\"\"\n",
      "        job_desc_dictionary = {\n",
      "            \"unique_id\": generate_unique_id(),\n",
      "            \"job_desc_data\": self.job_desc_data,\n",
      "            \"clean_data\": self.clean_data,\n",
      "            \"entities\": self.entities,\n",
      "            \"extracted_keywords\": self.key_words,\n",
      "            \"keyterms\": self.keyterms,\n",
      "            \"bi_grams\": str(self.bi_grams),\n",
      "            \"tri_grams\": str(self.tri_grams),\n",
      "            \"pos_frequencies\": self.pos_frequencies,\n",
      "        }\n",
      "\n",
      "        return job_desc_dictionary\n",
      "\n",
      "import json\n",
      "import os\n",
      "import os.path\n",
      "import pathlib\n",
      "\n",
      "from scripts.Extractor import DataExtractor\n",
      "from scripts.KeytermsExtraction import KeytermExtractor\n",
      "from scripts.utils.Utils import CountFrequency, TextCleaner, generate_unique_id\n",
      "\n",
      "SAVE_DIRECTORY = \"../../Data/Processed/Resumes\"\n",
      "\n",
      "\n",
      "class ParseResume:\n",
      "\n",
      "    def __init__(self, resume: str):\n",
      "        self.resume_data = resume\n",
      "        self.clean_data = TextCleaner.clean_text(self.resume_data)\n",
      "        self.entities = DataExtractor(self.clean_data).extract_entities()\n",
      "        self.name = DataExtractor(self.clean_data[:30]).extract_names()\n",
      "        self.experience = DataExtractor(self.clean_data).extract_experience()\n",
      "        self.emails = DataExtractor(self.resume_data).extract_emails()\n",
      "        self.phones = DataExtractor(self.resume_data).extract_phone_numbers()\n",
      "        self.years = DataExtractor(self.clean_data).extract_position_year()\n",
      "        self.key_words = DataExtractor(self.clean_data).extract_particular_words()\n",
      "        self.pos_frequencies = CountFrequency(self.clean_data).count_frequency()\n",
      "        self.keyterms = KeytermExtractor(self.clean_data).get_keyterms_based_on_sgrank()\n",
      "        self.bi_grams = KeytermExtractor(self.clean_data).bi_gramchunker()\n",
      "        self.tri_grams = KeytermExtractor(self.clean_data).tri_gramchunker()\n",
      "\n",
      "    def get_JSON(self) -> dict:\n",
      "        \"\"\"\n",
      "        Returns a dictionary of resume data.\n",
      "        \"\"\"\n",
      "        resume_dictionary = {\n",
      "            \"unique_id\": generate_unique_id(),\n",
      "            \"resume_data\": self.resume_data,\n",
      "            \"clean_data\": self.clean_data,\n",
      "            \"entities\": self.entities,\n",
      "            \"extracted_keywords\": self.key_words,\n",
      "            \"keyterms\": self.keyterms,\n",
      "            \"name\": self.name,\n",
      "            \"experience\": self.experience,\n",
      "            \"emails\": self.emails,\n",
      "            \"phones\": self.phones,\n",
      "            \"years\": self.years,\n",
      "            \"bi_grams\": str(self.bi_grams),\n",
      "            \"tri_grams\": str(self.tri_grams),\n",
      "            \"pos_frequencies\": self.pos_frequencies,\n",
      "        }\n",
      "\n",
      "        return resume_dictionary\n",
      "\n",
      "from .ParseJobDescToJson import ParseJobDesc\n",
      "from .ParseResumeToJson import ParseResume\n",
      "\n",
      "from .get_similarity_score import find_path, get_similarity_score, read_config\n",
      "\n",
      "import json\n",
      "import logging\n",
      "import os\n",
      "from typing import List\n",
      "\n",
      "import yaml\n",
      "from qdrant_client import QdrantClient\n",
      "\n",
      "from scripts.utils.logger import init_logging_config\n",
      "\n",
      "init_logging_config(basic_log_level=logging.INFO)\n",
      "# Get the logger\n",
      "logger = logging.getLogger(__name__)\n",
      "\n",
      "# Set the logging level\n",
      "logger.setLevel(logging.INFO)\n",
      "\n",
      "\n",
      "def find_path(folder_name):\n",
      "    \"\"\"\n",
      "    The function `find_path` searches for a folder by name starting from the current directory and\n",
      "    traversing up the directory tree until the folder is found or the root directory is reached.\n",
      "\n",
      "    Args:\n",
      "      folder_name: The `find_path` function you provided is designed to search for a folder by name\n",
      "    starting from the current working directory and moving up the directory tree until it finds the\n",
      "    folder or reaches the root directory.\n",
      "\n",
      "    Returns:\n",
      "      The `find_path` function is designed to search for a folder with the given `folder_name` starting\n",
      "    from the current working directory (`os.getcwd()`). It iterates through the directory structure,\n",
      "    checking if the folder exists in the current directory or any of its parent directories. If the\n",
      "    folder is found, it returns the full path to that folder using `os.path.join(curr_dir, folder_name)`\n",
      "    \"\"\"\n",
      "    curr_dir = os.getcwd()\n",
      "    while True:\n",
      "        if folder_name in os.listdir(curr_dir):\n",
      "            return os.path.join(curr_dir, folder_name)\n",
      "        else:\n",
      "            parent_dir = os.path.dirname(curr_dir)\n",
      "            if parent_dir == \"/\":\n",
      "                break\n",
      "            curr_dir = parent_dir\n",
      "    raise ValueError(f\"Folder '{folder_name}' not found.\")\n",
      "\n",
      "\n",
      "cwd = find_path(\"Resume-Matcher\")\n",
      "READ_RESUME_FROM = os.path.join(cwd, \"Data\", \"Processed\", \"Resumes\")\n",
      "READ_JOB_DESCRIPTION_FROM = os.path.join(cwd, \"Data\", \"Processed\", \"JobDescription\")\n",
      "config_path = os.path.join(cwd, \"scripts\", \"similarity\")\n",
      "\n",
      "\n",
      "def read_config(filepath):\n",
      "    \"\"\"\n",
      "    The `read_config` function reads a configuration file in YAML format and handles exceptions related\n",
      "    to file not found or parsing errors.\n",
      "\n",
      "    Args:\n",
      "      filepath: The `filepath` parameter in the `read_config` function is a string that represents the\n",
      "    path to the configuration file that you want to read and parse. This function attempts to open the\n",
      "    file specified by `filepath`, load its contents as YAML, and return the parsed configuration. If any\n",
      "    errors occur during\n",
      "\n",
      "    Returns:\n",
      "      The function `read_config` will return the configuration loaded from the file if successful, or\n",
      "    `None` if there was an error during the process.\n",
      "    \"\"\"\n",
      "    try:\n",
      "        with open(filepath) as f:\n",
      "            config = yaml.safe_load(f)\n",
      "        return config\n",
      "    except FileNotFoundError as e:\n",
      "        logger.error(f\"Configuration file {filepath} not found: {e}\")\n",
      "    except yaml.YAMLError as e:\n",
      "        logger.error(\n",
      "            f\"Error parsing YAML in configuration file {filepath}: {e}\", exc_info=True\n",
      "        )\n",
      "    except Exception as e:\n",
      "        logger.error(f\"Error reading configuration file {filepath}: {e}\")\n",
      "    return None\n",
      "\n",
      "\n",
      "def read_doc(path):\n",
      "    \"\"\"\n",
      "    The `read_doc` function reads a JSON file from the specified path and returns its contents, handling\n",
      "    any exceptions that may occur during the process.\n",
      "\n",
      "    Args:\n",
      "      path: The `path` parameter in the `read_doc` function is a string that represents the file path to\n",
      "    the JSON document that you want to read and load. This function reads the JSON data from the file\n",
      "    located at the specified path.\n",
      "\n",
      "    Returns:\n",
      "      The function `read_doc(path)` reads a JSON file located at the specified `path`, and returns the\n",
      "    data loaded from the file. If there is an error reading the JSON file, it logs the error message and\n",
      "    returns an empty dictionary `{}`.\n",
      "    \"\"\"\n",
      "    with open(path) as f:\n",
      "        try:\n",
      "            data = json.load(f)\n",
      "        except Exception as e:\n",
      "            logger.error(f\"Error reading JSON file: {e}\")\n",
      "            data = {}\n",
      "    return data\n",
      "\n",
      "\n",
      "def get_score(resume_string, job_description_string):\n",
      "    \"\"\"\n",
      "    The function `get_score` uses QdrantClient to calculate the similarity score between a resume and a\n",
      "    job description.\n",
      "\n",
      "    Args:\n",
      "      resume_string: The `resume_string` parameter is a string containing the text of a resume. It\n",
      "    represents the content of a resume that you want to compare with a job description.\n",
      "      job_description_string: The `get_score` function you provided seems to be using a QdrantClient to\n",
      "    calculate the similarity score between a resume and a job description. The function takes in two\n",
      "    parameters: `resume_string` and `job_description_string`, where `resume_string` is the text content\n",
      "    of the resume and\n",
      "\n",
      "    Returns:\n",
      "      The function `get_score` returns the search result obtained by querying a QdrantClient with the\n",
      "    job description string against the resume string provided.\n",
      "    \"\"\"\n",
      "    logger.info(\"Started getting similarity score\")\n",
      "\n",
      "    documents: List[str] = [resume_string]\n",
      "    client = QdrantClient(\":memory:\")\n",
      "    client.set_model(\"BAAI/bge-base-en\")\n",
      "\n",
      "    client.add(\n",
      "        collection_name=\"demo_collection\",\n",
      "        documents=documents,\n",
      "    )\n",
      "\n",
      "    search_result = client.query(\n",
      "        collection_name=\"demo_collection\", query_text=job_description_string\n",
      "    )\n",
      "    logger.info(\"Finished getting similarity score\")\n",
      "    return search_result\n",
      "\n",
      "\n",
      "if __name__ == \"__main__\":\n",
      "    # To give your custom resume use this code\n",
      "    resume_dict = read_config(\n",
      "        READ_RESUME_FROM\n",
      "        + \"/Resume-alfred_pennyworth_pm.pdf83632b66-5cce-4322-a3c6-895ff7e3dd96.json\"\n",
      "    )\n",
      "    job_dict = read_config(\n",
      "        READ_JOB_DESCRIPTION_FROM\n",
      "        + \"/JobDescription-job_desc_product_manager.pdf6763dc68-12ff-4b32-b652-ccee195de071.json\"\n",
      "    )\n",
      "    resume_keywords = resume_dict[\"extracted_keywords\"]\n",
      "    job_description_keywords = job_dict[\"extracted_keywords\"]\n",
      "\n",
      "    resume_string = \" \".join(resume_keywords)\n",
      "    jd_string = \" \".join(job_description_keywords)\n",
      "    final_result = get_score(resume_string, jd_string)\n",
      "    for r in final_result:\n",
      "        print(r.score)\n",
      "\n",
      "import json\n",
      "import logging\n",
      "import os\n",
      "\n",
      "import cohere\n",
      "import yaml\n",
      "from qdrant_client import QdrantClient, models\n",
      "from qdrant_client.http.models import Batch\n",
      "\n",
      "from scripts.utils.logger import get_handlers, init_logging_config\n",
      "\n",
      "init_logging_config(basic_log_level=logging.INFO)\n",
      "# Get the logger\n",
      "logger = logging.getLogger(__name__)\n",
      "\n",
      "# Set the logging level\n",
      "logger.setLevel(logging.INFO)\n",
      "\n",
      "stderr_handler, file_handler = get_handlers()\n",
      "\n",
      "\n",
      "def find_path(folder_name):\n",
      "    \"\"\"\n",
      "    Find the path of a folder with the given name in the current directory or its parent directories.\n",
      "\n",
      "    Args:\n",
      "        folder_name (str): The name of the folder to search for.\n",
      "\n",
      "    Returns:\n",
      "        str: The path of the folder if found.\n",
      "\n",
      "    Raises:\n",
      "        ValueError: If the folder with the given name is not found in the current directory or its parent directories.\n",
      "    \"\"\"\n",
      "    curr_dir = os.getcwd()\n",
      "    while True:\n",
      "        if folder_name in os.listdir(curr_dir):\n",
      "            return os.path.join(curr_dir, folder_name)\n",
      "        else:\n",
      "            parent_dir = os.path.dirname(curr_dir)\n",
      "            if parent_dir == \"/\":\n",
      "                break\n",
      "            curr_dir = parent_dir\n",
      "    raise ValueError(f\"Folder '{folder_name}' not found.\")\n",
      "\n",
      "\n",
      "cwd = find_path(\"Resume-Matcher\")\n",
      "READ_RESUME_FROM = os.path.join(cwd, \"Data\", \"Processed\", \"Resumes\")\n",
      "READ_JOB_DESCRIPTION_FROM = os.path.join(cwd, \"Data\", \"Processed\", \"JobDescription\")\n",
      "config_path = os.path.join(cwd, \"scripts\", \"similarity\")\n",
      "\n",
      "\n",
      "def read_config(filepath):\n",
      "    \"\"\"\n",
      "    Reads a configuration file in YAML format and returns the parsed configuration.\n",
      "\n",
      "    Args:\n",
      "        filepath (str): The path to the configuration file.\n",
      "\n",
      "    Returns:\n",
      "        dict: The parsed configuration as a dictionary.\n",
      "\n",
      "    Raises:\n",
      "        FileNotFoundError: If the configuration file is not found.\n",
      "        yaml.YAMLError: If there is an error parsing the YAML in the configuration file.\n",
      "        Exception: If there is an error reading the configuration file.\n",
      "\n",
      "    \"\"\"\n",
      "    try:\n",
      "        with open(filepath) as f:\n",
      "            config = yaml.safe_load(f)\n",
      "        return config\n",
      "    except FileNotFoundError as e:\n",
      "        logger.error(f\"Configuration file {filepath} not found: {e}\")\n",
      "    except yaml.YAMLError as e:\n",
      "        logger.error(\n",
      "            f\"Error parsing YAML in configuration file {filepath}: {e}\", exc_info=True\n",
      "        )\n",
      "    except Exception as e:\n",
      "        logger.error(f\"Error reading configuration file {filepath}: {e}\")\n",
      "    return None\n",
      "\n",
      "\n",
      "def read_doc(path):\n",
      "    \"\"\"\n",
      "    Read a JSON file and return its contents as a dictionary.\n",
      "\n",
      "    Args:\n",
      "        path (str): The path to the JSON file.\n",
      "\n",
      "    Returns:\n",
      "        dict: The contents of the JSON file as a dictionary.\n",
      "\n",
      "    Raises:\n",
      "        Exception: If there is an error reading the JSON file.\n",
      "    \"\"\"\n",
      "    with open(path) as f:\n",
      "        try:\n",
      "            data = json.load(f)\n",
      "        except Exception as e:\n",
      "            logger.error(f\"Error reading JSON file: {e}\")\n",
      "            data = {}\n",
      "    return data\n",
      "\n",
      "\n",
      "# This class likely performs searches based on quadrants.\n",
      "class QdrantSearch:\n",
      "    def __init__(self, resumes, jd):\n",
      "        \"\"\"\n",
      "        The function initializes various parameters and clients for processing resumes and job\n",
      "        descriptions.\n",
      "\n",
      "        Args:\n",
      "          resumes: The `resumes` parameter in the `__init__` method seems to be a list of resumes that\n",
      "        is passed to the class constructor. It is likely used within the class for some processing or\n",
      "        analysis related to resumes. If you have any specific questions or need further assistance with\n",
      "        this parameter or any\n",
      "          jd: The `jd` parameter in the `__init__` method seems to represent a job description. It is\n",
      "        likely used as input to compare against the resumes provided in the `resumes` parameter. The job\n",
      "        description is probably used for matching and analyzing against the resumes in the system.\n",
      "        \"\"\"\n",
      "        config = read_config(config_path + \"/config.yml\")\n",
      "        self.cohere_key = config[\"cohere\"][\"api_key\"]\n",
      "        self.qdrant_key = config[\"qdrant\"][\"api_key\"]\n",
      "        self.qdrant_url = config[\"qdrant\"][\"url\"]\n",
      "        self.resumes = resumes\n",
      "        self.jd = jd\n",
      "        self.cohere = cohere.Client(self.cohere_key)\n",
      "        self.collection_name = \"resume_collection_name\"\n",
      "        self.qdrant = QdrantClient(\n",
      "            url=self.qdrant_url,\n",
      "            api_key=self.qdrant_key,\n",
      "        )\n",
      "\n",
      "        vector_size = 4096\n",
      "        print(f\"collection name={self.collection_name}\")\n",
      "        self.qdrant.recreate_collection(\n",
      "            collection_name=self.collection_name,\n",
      "            vectors_config=models.VectorParams(\n",
      "                size=vector_size, distance=models.Distance.COSINE\n",
      "            ),\n",
      "        )\n",
      "\n",
      "        self.logger = logging.getLogger(self.__class__.__name__)\n",
      "\n",
      "        self.logger.addHandler(stderr_handler)\n",
      "        self.logger.addHandler(file_handler)\n",
      "\n",
      "    def get_embedding(self, text):\n",
      "        \"\"\"\n",
      "        The function `get_embedding` takes a text input, generates embeddings using the Cohere API, and\n",
      "        returns the embeddings as a list of floats along with the length of the embeddings.\n",
      "\n",
      "        Args:\n",
      "          text: The `text` parameter in the `get_embedding` function is a string that represents the\n",
      "        text for which you want to generate embeddings. This text will be passed to the Cohere API to\n",
      "        retrieve the embeddings for further processing.\n",
      "\n",
      "        Returns:\n",
      "          The `get_embedding` function returns a tuple containing two elements:\n",
      "        1. A list of floating-point numbers representing the embeddings of the input text.\n",
      "        2. The length of the embeddings list.\n",
      "        \"\"\"\n",
      "        try:\n",
      "            embeddings = self.cohere.embed([text], \"large\").embeddings\n",
      "            return list(map(float, embeddings[0])), len(embeddings[0])\n",
      "        except Exception as e:\n",
      "            self.logger.error(f\"Error getting embeddings: {e}\", exc_info=True)\n",
      "\n",
      "    def update_qdrant(self):\n",
      "        \"\"\"\n",
      "        This Python function updates vectors and corresponding metadata in a Qdrant collection based on\n",
      "        resumes.\n",
      "        \"\"\"\n",
      "        vectors = []\n",
      "        ids = []\n",
      "        for i, resume in enumerate(self.resumes):\n",
      "            vector, size = self.get_embedding(resume)\n",
      "            vectors.append(vector)\n",
      "            ids.append(i)\n",
      "        try:\n",
      "            self.qdrant.upsert(\n",
      "                collection_name=self.collection_name,\n",
      "                points=Batch(\n",
      "                    ids=ids,\n",
      "                    vectors=vectors,\n",
      "                    payloads=[{\"text\": resume} for resume in self.resumes],\n",
      "                ),\n",
      "            )\n",
      "        except Exception as e:\n",
      "            self.logger.error(\n",
      "                f\"Error upserting the vectors to the qdrant collection: {e}\",\n",
      "                exc_info=True,\n",
      "            )\n",
      "\n",
      "    def search(self):\n",
      "        \"\"\"\n",
      "        The `search` function retrieves search results based on a query vector using a specified\n",
      "        collection in a search engine.\n",
      "\n",
      "        Returns:\n",
      "          A list of dictionaries containing the text and score of the search results.\n",
      "        \"\"\"\n",
      "        vector, _ = self.get_embedding(self.jd)\n",
      "\n",
      "        hits = self.qdrant.search(\n",
      "            collection_name=self.collection_name, query_vector=vector, limit=30\n",
      "        )\n",
      "        results = []\n",
      "        for hit in hits:\n",
      "            result = {\"text\": str(hit.payload)[:30], \"score\": hit.score}\n",
      "            results.append(result)\n",
      "\n",
      "        return results\n",
      "\n",
      "\n",
      "def get_similarity_score(resume_string, job_description_string):\n",
      "    \"\"\"\n",
      "    This Python function `get_similarity_score` calculates the similarity score between a resume and a\n",
      "    job description using QdrantSearch.\n",
      "\n",
      "    Args:\n",
      "      resume_string: The `get_similarity_score` function seems to be using a `QdrantSearch` class to\n",
      "    calculate the similarity score between a resume and a job description. The `resume_string` parameter\n",
      "    likely contains the text content of a resume, while the `job_description_string` parameter contains\n",
      "    the text content of\n",
      "      job_description_string: The `job_description_string` parameter is a string containing the job\n",
      "    description for which you want to calculate the similarity score with a given resume. This\n",
      "    description typically includes details about the job requirements, responsibilities, qualifications,\n",
      "    and skills needed for the position. The function `get_similarity_score` takes this job description\n",
      "\n",
      "    Returns:\n",
      "      The function `get_similarity_score` returns the search result obtained from comparing a resume\n",
      "    string with a job description string using a QdrantSearch object.\n",
      "    \"\"\"\n",
      "    logger.info(\"Started getting similarity score\")\n",
      "    qdrant_search = QdrantSearch([resume_string], job_description_string)\n",
      "    qdrant_search.update_qdrant()\n",
      "    search_result = qdrant_search.search()\n",
      "    logger.info(\"Finished getting similarity score\")\n",
      "    return search_result\n",
      "\n",
      "\n",
      "if __name__ == \"__main__\":\n",
      "    # To give your custom resume use this code\n",
      "    resume_dict = read_config(\n",
      "        READ_RESUME_FROM\n",
      "        + \"/Resume-bruce_wayne_fullstack.pdf4783d115-e6fc-462e-ae4d-479152884b28.json\"\n",
      "    )\n",
      "    job_dict = read_config(\n",
      "        READ_JOB_DESCRIPTION_FROM\n",
      "        + \"/JobDescription-job_desc_full_stack_engineer_pdf4de00846-a4fe-4fe5-a4d7\"\n",
      "        \"-2a8a1b9ad020.json\"\n",
      "    )\n",
      "    resume_keywords = resume_dict[\"extracted_keywords\"]\n",
      "    job_description_keywords = job_dict[\"extracted_keywords\"]\n",
      "\n",
      "    resume_string = \" \".join(resume_keywords)\n",
      "    jd_string = \" \".join(job_description_keywords)\n",
      "    final_result = get_similarity_score(resume_string, jd_string)\n",
      "    for r in final_result:\n",
      "        print(r)\n",
      "\n",
      "import os\n",
      "\n",
      "\n",
      "def get_filenames_from_dir(directory_path: str) -> list:\n",
      "    filenames = [\n",
      "        f\n",
      "        for f in os.listdir(directory_path)\n",
      "        if os.path.isfile(os.path.join(directory_path, f)) and f != \".DS_Store\"\n",
      "    ]\n",
      "    return filenames\n",
      "\n",
      "import textdistance as td\n",
      "\n",
      "\n",
      "def match(resume, job_des):\n",
      "    j = td.jaccard.similarity(resume, job_des)\n",
      "    s = td.sorensen_dice.similarity(resume, job_des)\n",
      "    c = td.cosine.similarity(resume, job_des)\n",
      "    o = td.overlap.normalized_similarity(resume, job_des)\n",
      "    total = (j + s + c + o) / 4\n",
      "    # total = (s+o)/2\n",
      "    return total * 100\n",
      "\n",
      "import re\n",
      "from uuid import uuid4\n",
      "\n",
      "import spacy\n",
      "\n",
      "# Load the English model\n",
      "nlp = spacy.load(\"en_core_web_md\")\n",
      "\n",
      "REGEX_PATTERNS = {\n",
      "    \"email_pattern\": r\"\\b[A-Za-z0-9._%+-]+@[A-Za-z0-9.-]+\\.[A-Za-z]{2,}\\b\",\n",
      "    \"phone_pattern\": r\"\\(?\\d{3}\\)?[-.\\s]?\\d{3}[-.\\s]?\\d{4}\",\n",
      "    \"link_pattern\": r\"\\b(?:https?://|www\\.)\\S+\\b\",\n",
      "}\n",
      "\n",
      "\n",
      "def generate_unique_id():\n",
      "    \"\"\"\n",
      "    Generate a unique ID and return it as a string.\n",
      "\n",
      "    Returns:\n",
      "        str: A string with a unique ID.\n",
      "    \"\"\"\n",
      "    return str(uuid4())\n",
      "\n",
      "\n",
      "class TextCleaner:\n",
      "    \"\"\"\n",
      "    A class for cleaning a text by removing specific patterns.\n",
      "    \"\"\"\n",
      "\n",
      "    def remove_emails_links(text):\n",
      "        \"\"\"\n",
      "        Clean the input text by removing specific patterns.\n",
      "\n",
      "        Args:\n",
      "            text (str): The input text to clean.\n",
      "\n",
      "        Returns:\n",
      "            str: The cleaned text.\n",
      "        \"\"\"\n",
      "        for pattern in REGEX_PATTERNS:\n",
      "            text = re.sub(REGEX_PATTERNS[pattern], \"\", text)\n",
      "        return text\n",
      "\n",
      "    def clean_text(text):\n",
      "        \"\"\"\n",
      "        Clean the input text by removing specific patterns.\n",
      "\n",
      "        Args:\n",
      "            text (str): The input text to clean.\n",
      "\n",
      "        Returns:\n",
      "            str: The cleaned text.\n",
      "        \"\"\"\n",
      "        text = TextCleaner.remove_emails_links(text)\n",
      "        doc = nlp(text)\n",
      "        for token in doc:\n",
      "            if token.pos_ == \"PUNCT\":\n",
      "                text = text.replace(token.text, \"\")\n",
      "        return str(text)\n",
      "\n",
      "    def remove_stopwords(text):\n",
      "        \"\"\"\n",
      "        Clean the input text by removing stopwords.\n",
      "\n",
      "        Args:\n",
      "            text (str): The input text to clean.\n",
      "\n",
      "        Returns:\n",
      "            str: The cleaned text.\n",
      "        \"\"\"\n",
      "        doc = nlp(text)\n",
      "        for token in doc:\n",
      "            if token.is_stop:\n",
      "                text = text.replace(token.text, \"\")\n",
      "        return text\n",
      "\n",
      "\n",
      "class CountFrequency:\n",
      "\n",
      "    def __init__(self, text):\n",
      "        self.text = text\n",
      "        self.doc = nlp(text)\n",
      "\n",
      "    def count_frequency(self):\n",
      "        \"\"\"\n",
      "        Count the frequency of words in the input text.\n",
      "\n",
      "        Returns:\n",
      "            dict: A dictionary with the words as keys and the frequency as values.\n",
      "        \"\"\"\n",
      "        pos_freq = {}\n",
      "        for token in self.doc:\n",
      "            if token.pos_ in pos_freq:\n",
      "                pos_freq[token.pos_] += 1\n",
      "            else:\n",
      "                pos_freq[token.pos_] = 1\n",
      "        return pos_freq\n",
      "\n",
      "from .logger import init_logging_config\n",
      "from .ReadFiles import get_filenames_from_dir\n",
      "from .Utils import TextCleaner\n",
      "\n",
      "import logging\n",
      "\n",
      "\n",
      "def get_handlers(\n",
      "    filename=\"app.log\", mode=\"w\", file_level=logging.DEBUG, stderr_level=logging.DEBUG\n",
      "):\n",
      "    \"\"\"\n",
      "    The function `get_handlers` returns a stream handler and a file handler with specified logging\n",
      "    levels and formatters.\n",
      "\n",
      "    Args:\n",
      "      filename: The `filename` parameter is the name of the log file where the log messages will be\n",
      "    written. In this case, the default filename is \"app.log\". Defaults to app.log\n",
      "      mode: The `mode` parameter in the `get_handlers` function specifies the mode in which the file\n",
      "    should be opened. In this case, the default mode is set to \"w\", which stands for write mode. This\n",
      "    means that if the file already exists, it will be truncated (i.e., its. Defaults to w\n",
      "      file_level: The `file_level` parameter in the `get_handlers` function is used to specify the\n",
      "    logging level for the file handler. In this case, it is set to `logging.DEBUG`, which means that the\n",
      "    file handler will log all messages at the DEBUG level and above.\n",
      "      stderr_level: The `stderr_level` parameter in the `get_handlers` function is used to specify the\n",
      "    logging level for the StreamHandler that outputs log messages to the standard error stream (stderr).\n",
      "    This level determines which log messages will be processed and output by the StreamHandler.\n",
      "\n",
      "    Returns:\n",
      "      The `get_handlers` function returns two logging handlers: `stderr_handler` which is a\n",
      "    StreamHandler for logging to stderr, and `file_handler` which is a FileHandler for logging to a file\n",
      "    specified by the `filename` parameter.\n",
      "    \"\"\"\n",
      "    # Stream handler\n",
      "    stderr_handler = logging.StreamHandler()\n",
      "    stderr_handler.setLevel(stderr_level)\n",
      "    stderr_handler.setFormatter(CustomFormatter())\n",
      "\n",
      "    # File handler\n",
      "    file_handler = logging.FileHandler(filename, mode=mode)\n",
      "    file_handler.setLevel(file_level)\n",
      "    file_handler.setFormatter(CustomFormatter(True))\n",
      "\n",
      "    # TODO: Add RotatingFileHandler\n",
      "\n",
      "    return stderr_handler, file_handler\n",
      "\n",
      "\n",
      "class CustomFormatter(logging.Formatter):\n",
      "    \"\"\"\n",
      "    A custom log formatter that adds color to log messages based on the log level.\n",
      "\n",
      "    Args:\n",
      "        file (bool): Indicates whether the log is being written to a file. Default is False.\n",
      "\n",
      "    Attributes:\n",
      "        FORMATS (dict): A dictionary mapping log levels to colorized log message formats.\n",
      "\n",
      "    Methods:\n",
      "        format(record): Formats the log record with the appropriate colorized log message format.\n",
      "\n",
      "    \"\"\"\n",
      "\n",
      "    def __init__(self, file=False):\n",
      "        \"\"\"\n",
      "        This function initializes logging formats with different colors and styles based on the log\n",
      "        level.\n",
      "\n",
      "        Args:\n",
      "          file: The `file` parameter in the `__init__` method is a boolean flag that determines whether\n",
      "        the logging output should be colored or not. If `file` is `True`, the colors will not be applied\n",
      "        to the log messages. Defaults to False\n",
      "        \"\"\"\n",
      "        super().__init__()\n",
      "        yellow = \"\\x1b[36;10m\" if not file else \"\"\n",
      "        blue = \"\\x1b[35;10m\" if not file else \"\"\n",
      "        green = \"\\x1b[32;10m\" if not file else \"\"\n",
      "        red = \"\\x1b[31;10m\" if not file else \"\"\n",
      "        bold_red = \"\\x1b[31;1m\" if not file else \"\"\n",
      "        reset = \"\\x1b[0m\" if not file else \"\"\n",
      "        log = \"%(asctime)s (%(filename)s:%(lineno)d) - %(levelname)s: \"\n",
      "        msg = reset + \"%(message)s\"\n",
      "\n",
      "        self.FORMATS = {\n",
      "            logging.DEBUG: blue + log + msg,\n",
      "            logging.INFO: green + log + msg,\n",
      "            logging.WARNING: yellow + log + msg,\n",
      "            logging.ERROR: red + log + msg,\n",
      "            logging.CRITICAL: bold_red + log + msg,\n",
      "        }\n",
      "\n",
      "    def format(self, record):\n",
      "        \"\"\"\n",
      "        Formats the log record with the appropriate colorized log message format.\n",
      "\n",
      "        Args:\n",
      "            record (LogRecord): The log record to be formatted.\n",
      "\n",
      "        Returns:\n",
      "            str: The formatted log message.\n",
      "\n",
      "        \"\"\"\n",
      "        log_fmt = self.FORMATS.get(record.levelno)\n",
      "        formatter = logging.Formatter(log_fmt)\n",
      "        return formatter.format(record)\n",
      "\n",
      "\n",
      "def init_logging_config(\n",
      "    basic_log_level=logging.INFO,\n",
      "    filename=\"app.log\",\n",
      "    mode=\"w\",\n",
      "    file_level=logging.DEBUG,\n",
      "    stderr_level=logging.DEBUG,\n",
      "):\n",
      "    \"\"\"\n",
      "    The function `init_logging_config` initializes logging configuration in Python by setting basic log\n",
      "    level, configuring handlers, and adding them to the logger.\n",
      "\n",
      "    Args:\n",
      "      basic_log_level: The `basic_log_level` parameter is used to set the logging level for the root\n",
      "    logger. In this function, it is set to `logging.INFO` by default, which means that log messages with\n",
      "    severity level INFO or higher will be processed.\n",
      "      filename: The `filename` parameter is a string that specifies the name of the log file where the\n",
      "    logs will be written. In the `init_logging_config` function you provided, the default value for\n",
      "    `filename` is \"app.log\". This means that if no filename is provided when calling the function, logs.\n",
      "    Defaults to app.log\n",
      "      mode: The `mode` parameter in the `init_logging_config` function specifies the mode in which the\n",
      "    log file will be opened. In this case, the default value is \"w\" which stands for write mode. This\n",
      "    means that the log file will be opened for writing, and if the file already exists. Defaults to w\n",
      "      file_level: The `file_level` parameter in the `init_logging_config` function is used to specify\n",
      "    the logging level for the file handler. This determines the severity level of log messages that will\n",
      "    be written to the log file specified by the `filename` parameter. In this case, the default value\n",
      "    for `file\n",
      "      stderr_level: The `stderr_level` parameter in the `init_logging_config` function is used to\n",
      "    specify the logging level for the stderr (standard error) handler. This handler is responsible for\n",
      "    directing log messages to the standard error stream. The logging level determines which severity of\n",
      "    log messages will be output to the stderr.\n",
      "    \"\"\"\n",
      "\n",
      "    logger = logging.getLogger()\n",
      "    logger.setLevel(basic_log_level)\n",
      "\n",
      "    # Get the handlers\n",
      "    stderr_handler, file_handler = get_handlers(\n",
      "        file_level=file_level, stderr_level=stderr_level, filename=filename, mode=mode\n",
      "    )\n",
      "\n",
      "    # Add the handlers\n",
      "    logger.addHandler(stderr_handler)\n",
      "    logger.addHandler(file_handler)\n",
      "\n",
      "import json\n",
      "import os\n",
      "from typing import List\n",
      "\n",
      "import networkx as nx\n",
      "import nltk\n",
      "import pandas as pd\n",
      "import plotly.express as px\n",
      "import plotly.graph_objects as go\n",
      "import streamlit as st\n",
      "from annotated_text import annotated_text, parameters\n",
      "from streamlit_extras import add_vertical_space as avs\n",
      "from streamlit_extras.badges import badge\n",
      "\n",
      "from scripts.similarity.get_score import *\n",
      "from scripts.utils import get_filenames_from_dir\n",
      "from scripts.utils.logger import init_logging_config\n",
      "\n",
      "# Set page configuration\n",
      "st.set_page_config(\n",
      "    page_title=\"Resume Matcher\",\n",
      "    page_icon=\"Assets/img/favicon.ico\",\n",
      "    initial_sidebar_state=\"auto\",\n",
      ")\n",
      "\n",
      "init_logging_config()\n",
      "cwd = find_path(\"Resume-Matcher\")\n",
      "config_path = os.path.join(cwd, \"scripts\", \"similarity\")\n",
      "\n",
      "try:\n",
      "    nltk.data.find(\"tokenizers/punkt\")\n",
      "except LookupError:\n",
      "    nltk.download(\"punkt\")\n",
      "\n",
      "parameters.SHOW_LABEL_SEPARATOR = False\n",
      "parameters.BORDER_RADIUS = 3\n",
      "parameters.PADDING = \"0.5 0.25rem\"\n",
      "\n",
      "\n",
      "def create_star_graph(nodes_and_weights, title):\n",
      "    # Create an empty graph\n",
      "    G = nx.Graph()\n",
      "\n",
      "    # Add the central node\n",
      "    central_node = \"resume\"\n",
      "    G.add_node(central_node)\n",
      "\n",
      "    # Add nodes and edges with weights to the graph\n",
      "    for node, weight in nodes_and_weights:\n",
      "        G.add_node(node)\n",
      "        G.add_edge(central_node, node, weight=weight * 100)\n",
      "\n",
      "    # Get position layout for nodes\n",
      "    pos = nx.spring_layout(G)\n",
      "\n",
      "    # Create edge trace\n",
      "    edge_x = []\n",
      "    edge_y = []\n",
      "    for edge in G.edges():\n",
      "        x0, y0 = pos[edge[0]]\n",
      "        x1, y1 = pos[edge[1]]\n",
      "        edge_x.extend([x0, x1, None])\n",
      "        edge_y.extend([y0, y1, None])\n",
      "\n",
      "    edge_trace = go.Scatter(\n",
      "        x=edge_x,\n",
      "        y=edge_y,\n",
      "        line=dict(width=0.5, color=\"#888\"),\n",
      "        hoverinfo=\"none\",\n",
      "        mode=\"lines\",\n",
      "    )\n",
      "\n",
      "    # Create node trace\n",
      "    node_x = []\n",
      "    node_y = []\n",
      "    for node in G.nodes():\n",
      "        x, y = pos[node]\n",
      "        node_x.append(x)\n",
      "        node_y.append(y)\n",
      "\n",
      "    node_trace = go.Scatter(\n",
      "        x=node_x,\n",
      "        y=node_y,\n",
      "        mode=\"markers\",\n",
      "        hoverinfo=\"text\",\n",
      "        marker=dict(\n",
      "            showscale=True,\n",
      "            colorscale=\"Rainbow\",\n",
      "            reversescale=True,\n",
      "            color=[],\n",
      "            size=10,\n",
      "            colorbar=dict(\n",
      "                thickness=15,\n",
      "                title=\"Node Connections\",\n",
      "                xanchor=\"left\",\n",
      "                titleside=\"right\",\n",
      "            ),\n",
      "            line_width=2,\n",
      "        ),\n",
      "    )\n",
      "\n",
      "    # Color node points by number of connections\n",
      "    node_adjacencies = []\n",
      "    node_text = []\n",
      "    for node in G.nodes():\n",
      "        adjacencies = list(G.adj[node])  # changes here\n",
      "        node_adjacencies.append(len(adjacencies))\n",
      "        node_text.append(f\"{node}<br># of connections: {len(adjacencies)}\")\n",
      "\n",
      "    node_trace.marker.color = node_adjacencies\n",
      "    node_trace.text = node_text\n",
      "\n",
      "    # Create the figure\n",
      "    fig = go.Figure(\n",
      "        data=[edge_trace, node_trace],\n",
      "        layout=go.Layout(\n",
      "            title=title,\n",
      "            titlefont_size=16,\n",
      "            showlegend=False,\n",
      "            hovermode=\"closest\",\n",
      "            margin=dict(b=20, l=5, r=5, t=40),\n",
      "            xaxis=dict(showgrid=False, zeroline=False, showticklabels=False),\n",
      "            yaxis=dict(showgrid=False, zeroline=False, showticklabels=False),\n",
      "        ),\n",
      "    )\n",
      "\n",
      "    # Show the figure\n",
      "    st.plotly_chart(fig)\n",
      "\n",
      "\n",
      "def create_annotated_text(\n",
      "    input_string: str, word_list: List[str], annotation: str, color_code: str\n",
      "):\n",
      "    # Tokenize the input string\n",
      "    tokens = nltk.word_tokenize(input_string)\n",
      "\n",
      "    # Convert the list to a set for quick lookups\n",
      "    word_set = set(word_list)\n",
      "\n",
      "    # Initialize an empty list to hold the annotated text\n",
      "    annotated_text = []\n",
      "\n",
      "    for token in tokens:\n",
      "        # Check if the token is in the set\n",
      "        if token in word_set:\n",
      "            # If it is, append a tuple with the token, annotation, and color code\n",
      "            annotated_text.append((token, annotation, color_code))\n",
      "        else:\n",
      "            # If it's not, just append the token as a string\n",
      "            annotated_text.append(token)\n",
      "\n",
      "    return annotated_text\n",
      "\n",
      "\n",
      "def read_json(filename):\n",
      "    with open(filename) as f:\n",
      "        data = json.load(f)\n",
      "    return data\n",
      "\n",
      "\n",
      "def tokenize_string(input_string):\n",
      "    tokens = nltk.word_tokenize(input_string)\n",
      "    return tokens\n",
      "\n",
      "\n",
      "# Display the main title and subheaders\n",
      "st.title(\":blue[Resume Matcher]\")\n",
      "with st.sidebar:\n",
      "    st.image(\"Assets/img/header_image.png\")\n",
      "    st.subheader(\n",
      "        \"Free and Open Source ATS to help your resume pass the screening stage.\"\n",
      "    )\n",
      "    st.markdown(\n",
      "        \"Check the website [www.resumematcher.fyi](https://www.resumematcher.fyi/)\"\n",
      "    )\n",
      "\n",
      "    st.markdown(\n",
      "        \"Give Resume Matcher a  on [GitHub](https://github.com/srbhr/resume-matcher)\"\n",
      "    )\n",
      "\n",
      "    badge(type=\"github\", name=\"srbhr/Resume-Matcher\")\n",
      "    st.markdown(\"For updates follow me on Twitter.\")\n",
      "    badge(type=\"twitter\", name=\"_srbhr_\")\n",
      "    st.markdown(\n",
      "        \"If you like the project and would like to further help in development please consider \"\n",
      "    )\n",
      "    badge(type=\"buymeacoffee\", name=\"srbhr\")\n",
      "\n",
      "st.divider()\n",
      "avs.add_vertical_space(1)\n",
      "\n",
      "resume_names = get_filenames_from_dir(\"Data/Processed/Resumes\")\n",
      "\n",
      "\n",
      "st.markdown(\n",
      "    f\"##### There are {len(resume_names)} resumes present. Please select one from the menu below:\"\n",
      ")\n",
      "output = st.selectbox(f\"\", resume_names)\n",
      "\n",
      "\n",
      "avs.add_vertical_space(5)\n",
      "\n",
      "# st.write(\"You have selected \", output, \" printing the resume\")\n",
      "selected_file = read_json(\"Data/Processed/Resumes/\" + output)\n",
      "\n",
      "avs.add_vertical_space(2)\n",
      "st.markdown(\"#### Parsed Resume Data\")\n",
      "st.caption(\n",
      "    \"This text is parsed from your resume. This is how it'll look like after getting parsed by an ATS.\"\n",
      ")\n",
      "st.caption(\"Utilize this to understand how to make your resume ATS friendly.\")\n",
      "avs.add_vertical_space(3)\n",
      "# st.json(selected_file)\n",
      "st.write(selected_file[\"clean_data\"])\n",
      "\n",
      "avs.add_vertical_space(3)\n",
      "st.write(\"Now let's take a look at the extracted keywords from the resume.\")\n",
      "\n",
      "annotated_text(\n",
      "    create_annotated_text(\n",
      "        selected_file[\"clean_data\"],\n",
      "        selected_file[\"extracted_keywords\"],\n",
      "        \"KW\",\n",
      "        \"#0B666A\",\n",
      "    )\n",
      ")\n",
      "\n",
      "avs.add_vertical_space(5)\n",
      "st.write(\"Now let's take a look at the extracted entities from the resume.\")\n",
      "\n",
      "# Call the function with your data\n",
      "create_star_graph(selected_file[\"keyterms\"], \"Entities from Resume\")\n",
      "\n",
      "df2 = pd.DataFrame(selected_file[\"keyterms\"], columns=[\"keyword\", \"value\"])\n",
      "\n",
      "# Create the dictionary\n",
      "keyword_dict = {}\n",
      "for keyword, value in selected_file[\"keyterms\"]:\n",
      "    keyword_dict[keyword] = value * 100\n",
      "\n",
      "fig = go.Figure(\n",
      "    data=[\n",
      "        go.Table(\n",
      "            header=dict(\n",
      "                values=[\"Keyword\", \"Value\"], font=dict(size=12), fill_color=\"#070A52\"\n",
      "            ),\n",
      "            cells=dict(\n",
      "                values=[list(keyword_dict.keys()), list(keyword_dict.values())],\n",
      "                line_color=\"darkslategray\",\n",
      "                fill_color=\"#6DA9E4\",\n",
      "            ),\n",
      "        )\n",
      "    ]\n",
      ")\n",
      "st.plotly_chart(fig)\n",
      "\n",
      "st.divider()\n",
      "\n",
      "fig = px.treemap(\n",
      "    df2,\n",
      "    path=[\"keyword\"],\n",
      "    values=\"value\",\n",
      "    color_continuous_scale=\"Rainbow\",\n",
      "    title=\"Key Terms/Topics Extracted from your Resume\",\n",
      ")\n",
      "st.write(fig)\n",
      "\n",
      "avs.add_vertical_space(5)\n",
      "\n",
      "job_descriptions = get_filenames_from_dir(\"Data/Processed/JobDescription\")\n",
      "\n",
      "\n",
      "st.markdown(\n",
      "    f\"##### There are {len(job_descriptions)} job descriptions present. Please select one from the menu below:\"\n",
      ")\n",
      "output = st.selectbox(\"\", job_descriptions)\n",
      "\n",
      "\n",
      "avs.add_vertical_space(5)\n",
      "\n",
      "selected_jd = read_json(\"Data/Processed/JobDescription/\" + output)\n",
      "\n",
      "avs.add_vertical_space(2)\n",
      "st.markdown(\"#### Job Description\")\n",
      "st.caption(\n",
      "    \"Currently in the pipeline I'm parsing this from PDF but it'll be from txt or copy paste.\"\n",
      ")\n",
      "avs.add_vertical_space(3)\n",
      "# st.json(selected_file)\n",
      "st.write(selected_jd[\"clean_data\"])\n",
      "\n",
      "st.markdown(\"#### Common Words between Job Description and Resumes Highlighted.\")\n",
      "\n",
      "annotated_text(\n",
      "    create_annotated_text(\n",
      "        selected_file[\"clean_data\"], selected_jd[\"extracted_keywords\"], \"JD\", \"#F24C3D\"\n",
      "    )\n",
      ")\n",
      "\n",
      "st.write(\"Now let's take a look at the extracted entities from the job description.\")\n",
      "\n",
      "# Call the function with your data\n",
      "create_star_graph(selected_jd[\"keyterms\"], \"Entities from Job Description\")\n",
      "\n",
      "df2 = pd.DataFrame(selected_jd[\"keyterms\"], columns=[\"keyword\", \"value\"])\n",
      "\n",
      "# Create the dictionary\n",
      "keyword_dict = {}\n",
      "for keyword, value in selected_jd[\"keyterms\"]:\n",
      "    keyword_dict[keyword] = value * 100\n",
      "\n",
      "fig = go.Figure(\n",
      "    data=[\n",
      "        go.Table(\n",
      "            header=dict(\n",
      "                values=[\"Keyword\", \"Value\"], font=dict(size=12), fill_color=\"#070A52\"\n",
      "            ),\n",
      "            cells=dict(\n",
      "                values=[list(keyword_dict.keys()), list(keyword_dict.values())],\n",
      "                line_color=\"darkslategray\",\n",
      "                fill_color=\"#6DA9E4\",\n",
      "            ),\n",
      "        )\n",
      "    ]\n",
      ")\n",
      "st.plotly_chart(fig)\n",
      "\n",
      "st.divider()\n",
      "\n",
      "fig = px.treemap(\n",
      "    df2,\n",
      "    path=[\"keyword\"],\n",
      "    values=\"value\",\n",
      "    color_continuous_scale=\"Rainbow\",\n",
      "    title=\"Key Terms/Topics Extracted from the selected Job Description\",\n",
      ")\n",
      "st.write(fig)\n",
      "\n",
      "avs.add_vertical_space(3)\n",
      "\n",
      "resume_string = \" \".join(selected_file[\"extracted_keywords\"])\n",
      "jd_string = \" \".join(selected_jd[\"extracted_keywords\"])\n",
      "result = get_score(resume_string, jd_string)\n",
      "similarity_score = round(result[0].score * 100, 2)\n",
      "score_color = \"green\"\n",
      "if similarity_score < 60:\n",
      "    score_color = \"red\"\n",
      "elif 60 <= similarity_score < 75:\n",
      "    score_color = \"orange\"\n",
      "st.markdown(\n",
      "    f\"Similarity Score obtained for the resume and job description is \"\n",
      "    f'<span style=\"color:{score_color};font-size:24px; font-weight:Bold\">{similarity_score}</span>',\n",
      "    unsafe_allow_html=True,\n",
      ")\n",
      "\n",
      "# Go back to top\n",
      "st.markdown(\"[:arrow_up: Back to Top](#resume-matcher)\")\n",
      "\n",
      "# Import necessary libraries\n",
      "import json\n",
      "import os\n",
      "from typing import List\n",
      "\n",
      "import networkx as nx\n",
      "import nltk\n",
      "import pandas as pd\n",
      "import plotly.express as px\n",
      "import plotly.graph_objects as go\n",
      "import streamlit as st\n",
      "from annotated_text import annotated_text, parameters\n",
      "from streamlit_extras import add_vertical_space as avs\n",
      "from streamlit_extras.badges import badge\n",
      "\n",
      "from scripts import JobDescriptionProcessor, ResumeProcessor\n",
      "from scripts.parsers import ParseJobDesc, ParseResume\n",
      "from scripts.ReadPdf import read_single_pdf\n",
      "from scripts.similarity.get_score import *\n",
      "from scripts.utils import get_filenames_from_dir\n",
      "\n",
      "# Set page configuration\n",
      "st.set_page_config(\n",
      "    page_title=\"Resume Matcher\",\n",
      "    page_icon=\"Assets/img/favicon.ico\",\n",
      "    initial_sidebar_state=\"auto\",\n",
      "    layout=\"wide\",\n",
      ")\n",
      "\n",
      "# Find the current working directory and configuration path\n",
      "cwd = find_path(\"Resume-Matcher\")\n",
      "config_path = os.path.join(cwd, \"scripts\", \"similarity\")\n",
      "\n",
      "# Check if NLTK punkt data is available, if not, download it\n",
      "try:\n",
      "    nltk.data.find(\"tokenizers/punkt\")\n",
      "except LookupError:\n",
      "    nltk.download(\"punkt\")\n",
      "\n",
      "# Set some visualization parameters using the annotated_text library\n",
      "parameters.SHOW_LABEL_SEPARATOR = False\n",
      "parameters.BORDER_RADIUS = 3\n",
      "parameters.PADDING = \"0.5 0.25rem\"\n",
      "\n",
      "\n",
      "# Function to set session state variables\n",
      "def update_session_state(key, val):\n",
      "    st.session_state[key] = val\n",
      "\n",
      "\n",
      "# Function to delete all files in a directory\n",
      "def delete_from_dir(filepath: str) -> bool:\n",
      "    try:\n",
      "        for file in os.scandir(filepath):\n",
      "            os.remove(file.path)\n",
      "\n",
      "        return True\n",
      "    except OSError as error:\n",
      "        print(f\"Exception: {error}\")\n",
      "        return False\n",
      "\n",
      "\n",
      "# Function to create a star-shaped graph visualization\n",
      "def create_star_graph(nodes_and_weights, title):\n",
      "    \"\"\"\n",
      "    Create a star-shaped graph visualization.\n",
      "\n",
      "    Args:\n",
      "        nodes_and_weights (list): List of tuples containing nodes and their weights.\n",
      "        title (str): Title for the graph.\n",
      "\n",
      "    Returns:\n",
      "        None\n",
      "    \"\"\"\n",
      "    # Create an empty graph\n",
      "    graph = nx.Graph()\n",
      "\n",
      "    # Add the central node\n",
      "    central_node = \"resume\"\n",
      "    graph.add_node(central_node)\n",
      "\n",
      "    # Add nodes and edges with weights to the graph\n",
      "    for node, weight in nodes_and_weights:\n",
      "        graph.add_node(node)\n",
      "        graph.add_edge(central_node, node, weight=weight * 100)\n",
      "\n",
      "    # Get position layout for nodes\n",
      "    pos = nx.spring_layout(graph)\n",
      "\n",
      "    # Create edge trace\n",
      "    edge_x = []\n",
      "    edge_y = []\n",
      "    for edge in graph.edges():\n",
      "        x0, y0 = pos[edge[0]]\n",
      "        x1, y1 = pos[edge[1]]\n",
      "        edge_x.extend([x0, x1, None])\n",
      "        edge_y.extend([y0, y1, None])\n",
      "\n",
      "    edge_trace = go.Scatter(\n",
      "        x=edge_x,\n",
      "        y=edge_y,\n",
      "        line=dict(width=0.5, color=\"#888\"),\n",
      "        hoverinfo=\"none\",\n",
      "        mode=\"lines\",\n",
      "    )\n",
      "\n",
      "    # Create node trace\n",
      "    node_x = []\n",
      "    node_y = []\n",
      "    for node in graph.nodes():\n",
      "        x, y = pos[node]\n",
      "        node_x.append(x)\n",
      "        node_y.append(y)\n",
      "\n",
      "    node_trace = go.Scatter(\n",
      "        x=node_x,\n",
      "        y=node_y,\n",
      "        mode=\"markers\",\n",
      "        hoverinfo=\"text\",\n",
      "        marker=dict(\n",
      "            showscale=True,\n",
      "            colorscale=\"Rainbow\",\n",
      "            reversescale=True,\n",
      "            color=[],\n",
      "            size=10,\n",
      "            colorbar=dict(\n",
      "                thickness=15,\n",
      "                title=\"Node Connections\",\n",
      "                xanchor=\"left\",\n",
      "                titleside=\"right\",\n",
      "            ),\n",
      "            line_width=2,\n",
      "        ),\n",
      "    )\n",
      "\n",
      "    # Color node points by number of connections\n",
      "    node_adjacencies = []\n",
      "    node_text = []\n",
      "    for node in graph.nodes():\n",
      "        adjacencies = list(graph.adj[node])  # Changes here\n",
      "        node_adjacencies.append(len(adjacencies))\n",
      "        node_text.append(f\"{node}<br># of connections: {len(adjacencies)}\")\n",
      "\n",
      "    node_trace.marker.color = node_adjacencies\n",
      "    node_trace.text = node_text\n",
      "\n",
      "    # Create the figure\n",
      "    figure = go.Figure(\n",
      "        data=[edge_trace, node_trace],\n",
      "        layout=go.Layout(\n",
      "            title=title,\n",
      "            titlefont=dict(size=16),\n",
      "            showlegend=False,\n",
      "            hovermode=\"closest\",\n",
      "            margin=dict(b=20, l=5, r=5, t=40),\n",
      "            xaxis=dict(showgrid=False, zeroline=False, showticklabels=False),\n",
      "            yaxis=dict(showgrid=False, zeroline=False, showticklabels=False),\n",
      "        ),\n",
      "    )\n",
      "\n",
      "    # Show the figure\n",
      "    st.plotly_chart(figure, use_container_width=True)\n",
      "\n",
      "\n",
      "# Function to create annotated text with highlighting\n",
      "def create_annotated_text(\n",
      "    input_string: str, word_list: List[str], annotation: str, color_code: str\n",
      "):\n",
      "    \"\"\"\n",
      "    Create annotated text with highlighted keywords.\n",
      "\n",
      "    Args:\n",
      "        input_string (str): The input text.\n",
      "        word_list (List[str]): List of keywords to be highlighted.\n",
      "        annotation (str): Annotation label for highlighted keywords.\n",
      "        color_code (str): Color code for highlighting.\n",
      "\n",
      "    Returns:\n",
      "        List: Annotated text with highlighted keywords.\n",
      "    \"\"\"\n",
      "    # Tokenize the input string\n",
      "    tokens = nltk.word_tokenize(input_string)\n",
      "\n",
      "    # Convert the list to a set for quick lookups\n",
      "    word_set = set(word_list)\n",
      "\n",
      "    # Initialize an empty list to hold the annotated text\n",
      "    ret_annotated_text = []\n",
      "\n",
      "    for token in tokens:\n",
      "        # Check if the token is in the set\n",
      "        if token in word_set:\n",
      "            # If it is, append a tuple with the token, annotation, and color code\n",
      "            ret_annotated_text.append((token, annotation, color_code))\n",
      "        else:\n",
      "            # If it's not, just append the token as a string\n",
      "            ret_annotated_text.append(token)\n",
      "\n",
      "    return ret_annotated_text\n",
      "\n",
      "\n",
      "# Function to read JSON data from a file\n",
      "def read_json(filename):\n",
      "    \"\"\"\n",
      "    Read JSON data from a file.\n",
      "\n",
      "    Args:\n",
      "        filename (str): The path to the JSON file.\n",
      "\n",
      "    Returns:\n",
      "        dict: The JSON data.\n",
      "    \"\"\"\n",
      "    with open(filename) as f:\n",
      "        data = json.load(f)\n",
      "    return data\n",
      "\n",
      "\n",
      "# Function to tokenize a string\n",
      "def tokenize_string(input_string):\n",
      "    \"\"\"\n",
      "    Tokenize a string into words.\n",
      "\n",
      "    Args:\n",
      "        input_string (str): The input string.\n",
      "\n",
      "    Returns:\n",
      "        List[str]: List of tokens.\n",
      "    \"\"\"\n",
      "    tokens = nltk.word_tokenize(input_string)\n",
      "    return tokens\n",
      "\n",
      "\n",
      "# Cleanup processed resume / job descriptions\n",
      "delete_from_dir(os.path.join(cwd, \"Data\", \"Processed\", \"Resumes\"))\n",
      "delete_from_dir(os.path.join(cwd, \"Data\", \"Processed\", \"JobDescription\"))\n",
      "\n",
      "# Set default session states for first run\n",
      "if \"resumeUploaded\" not in st.session_state.keys():\n",
      "    update_session_state(\"resumeUploaded\", \"Pending\")\n",
      "    update_session_state(\"resumePath\", \"\")\n",
      "if \"jobDescriptionUploaded\" not in st.session_state.keys():\n",
      "    update_session_state(\"jobDescriptionUploaded\", \"Pending\")\n",
      "    update_session_state(\"jobDescriptionPath\", \"\")\n",
      "\n",
      "# Display the main title and sub-headers\n",
      "st.title(\":blue[Resume Matcher]\")\n",
      "with st.sidebar:\n",
      "    st.image(\"Assets/img/header_image.png\")\n",
      "    st.subheader(\n",
      "        \"Free and Open Source ATS to help your resume pass the screening stage.\"\n",
      "    )\n",
      "    st.markdown(\n",
      "        \"Check the website [www.resumematcher.fyi](https://www.resumematcher.fyi/)\"\n",
      "    )\n",
      "    st.markdown(\n",
      "        \"Give Resume Matcher a  on [GitHub](https://github.com/srbhr/resume-matcher)\"\n",
      "    )\n",
      "    badge(type=\"github\", name=\"srbhr/Resume-Matcher\")\n",
      "    st.markdown(\"For updates follow me on Twitter.\")\n",
      "    badge(type=\"twitter\", name=\"_srbhr_\")\n",
      "    st.markdown(\n",
      "        \"If you like the project and would like to further help in development please consider \"\n",
      "    )\n",
      "    badge(type=\"buymeacoffee\", name=\"srbhr\")\n",
      "\n",
      "st.divider()\n",
      "avs.add_vertical_space(1)\n",
      "\n",
      "with st.container():\n",
      "    resumeCol, jobDescriptionCol = st.columns(2)\n",
      "    with resumeCol:\n",
      "        uploaded_Resume = st.file_uploader(\"Choose a Resume\", type=\"pdf\")\n",
      "        if uploaded_Resume is not None:\n",
      "            if st.session_state[\"resumeUploaded\"] == \"Pending\":\n",
      "                save_path_resume = os.path.join(\n",
      "                    cwd, \"Data\", \"Resumes\", uploaded_Resume.name\n",
      "                )\n",
      "\n",
      "                with open(save_path_resume, mode=\"wb\") as w:\n",
      "                    w.write(uploaded_Resume.getvalue())\n",
      "\n",
      "                if os.path.exists(save_path_resume):\n",
      "                    st.toast(\n",
      "                        f\"File {uploaded_Resume.name} is successfully saved!\", icon=\"\"\n",
      "                    )\n",
      "                    update_session_state(\"resumeUploaded\", \"Uploaded\")\n",
      "                    update_session_state(\"resumePath\", save_path_resume)\n",
      "        else:\n",
      "            update_session_state(\"resumeUploaded\", \"Pending\")\n",
      "            update_session_state(\"resumePath\", \"\")\n",
      "\n",
      "    with jobDescriptionCol:\n",
      "        uploaded_JobDescription = st.file_uploader(\n",
      "            \"Choose a Job Description\", type=\"pdf\"\n",
      "        )\n",
      "        if uploaded_JobDescription is not None:\n",
      "            if st.session_state[\"jobDescriptionUploaded\"] == \"Pending\":\n",
      "                save_path_jobDescription = os.path.join(\n",
      "                    cwd, \"Data\", \"JobDescription\", uploaded_JobDescription.name\n",
      "                )\n",
      "\n",
      "                with open(save_path_jobDescription, mode=\"wb\") as w:\n",
      "                    w.write(uploaded_JobDescription.getvalue())\n",
      "\n",
      "                if os.path.exists(save_path_jobDescription):\n",
      "                    st.toast(\n",
      "                        f\"File {uploaded_JobDescription.name} is successfully saved!\",\n",
      "                        icon=\"\",\n",
      "                    )\n",
      "                    update_session_state(\"jobDescriptionUploaded\", \"Uploaded\")\n",
      "                    update_session_state(\"jobDescriptionPath\", save_path_jobDescription)\n",
      "        else:\n",
      "            update_session_state(\"jobDescriptionUploaded\", \"Pending\")\n",
      "            update_session_state(\"jobDescriptionPath\", \"\")\n",
      "\n",
      "with st.spinner(\"Please wait...\"):\n",
      "    if (\n",
      "        uploaded_Resume is not None\n",
      "        and st.session_state[\"jobDescriptionUploaded\"] == \"Uploaded\"\n",
      "        and uploaded_JobDescription is not None\n",
      "        and st.session_state[\"jobDescriptionUploaded\"] == \"Uploaded\"\n",
      "    ):\n",
      "\n",
      "        resumeProcessor = ParseResume(read_single_pdf(st.session_state[\"resumePath\"]))\n",
      "        jobDescriptionProcessor = ParseJobDesc(\n",
      "            read_single_pdf(st.session_state[\"jobDescriptionPath\"])\n",
      "        )\n",
      "\n",
      "        # Resume / JD output\n",
      "        selected_file = resumeProcessor.get_JSON()\n",
      "        selected_jd = jobDescriptionProcessor.get_JSON()\n",
      "\n",
      "        # Add containers for each row to avoid overlap\n",
      "\n",
      "        # Parsed data\n",
      "        with st.container():\n",
      "            resumeCol, jobDescriptionCol = st.columns(2)\n",
      "            with resumeCol:\n",
      "                with st.expander(\"Parsed Resume Data\"):\n",
      "                    st.caption(\n",
      "                        \"This text is parsed from your resume. This is how it'll look like after getting parsed by an \"\n",
      "                        \"ATS.\"\n",
      "                    )\n",
      "                    st.caption(\n",
      "                        \"Utilize this to understand how to make your resume ATS friendly.\"\n",
      "                    )\n",
      "                    avs.add_vertical_space(3)\n",
      "                    st.write(selected_file[\"clean_data\"])\n",
      "\n",
      "            with jobDescriptionCol:\n",
      "                with st.expander(\"Parsed Job Description\"):\n",
      "                    st.caption(\n",
      "                        \"Currently in the pipeline I'm parsing this from PDF but it'll be from txt or copy paste.\"\n",
      "                    )\n",
      "                    avs.add_vertical_space(3)\n",
      "                    st.write(selected_jd[\"clean_data\"])\n",
      "\n",
      "        # Extracted keywords\n",
      "        with st.container():\n",
      "            resumeCol, jobDescriptionCol = st.columns(2)\n",
      "            with resumeCol:\n",
      "                with st.expander(\"Extracted Keywords\"):\n",
      "                    st.write(\n",
      "                        \"Now let's take a look at the extracted keywords from the resume.\"\n",
      "                    )\n",
      "                    annotated_text(\n",
      "                        create_annotated_text(\n",
      "                            selected_file[\"clean_data\"],\n",
      "                            selected_file[\"extracted_keywords\"],\n",
      "                            \"KW\",\n",
      "                            \"#0B666A\",\n",
      "                        )\n",
      "                    )\n",
      "            with jobDescriptionCol:\n",
      "                with st.expander(\"Extracted Keywords\"):\n",
      "                    st.write(\n",
      "                        \"Now let's take a look at the extracted keywords from the job description.\"\n",
      "                    )\n",
      "                    annotated_text(\n",
      "                        create_annotated_text(\n",
      "                            selected_jd[\"clean_data\"],\n",
      "                            selected_jd[\"extracted_keywords\"],\n",
      "                            \"KW\",\n",
      "                            \"#0B666A\",\n",
      "                        )\n",
      "                    )\n",
      "\n",
      "        # Star graph visualization\n",
      "        with st.container():\n",
      "            resumeCol, jobDescriptionCol = st.columns(2)\n",
      "            with resumeCol:\n",
      "                with st.expander(\"Extracted Entities\"):\n",
      "                    st.write(\n",
      "                        \"Now let's take a look at the extracted entities from the resume.\"\n",
      "                    )\n",
      "\n",
      "                    # Call the function with your data\n",
      "                    create_star_graph(selected_file[\"keyterms\"], \"Entities from Resume\")\n",
      "            with jobDescriptionCol:\n",
      "                with st.expander(\"Extracted Entities\"):\n",
      "                    st.write(\n",
      "                        \"Now let's take a look at the extracted entities from the job description.\"\n",
      "                    )\n",
      "\n",
      "                    # Call the function with your data\n",
      "                    create_star_graph(\n",
      "                        selected_jd[\"keyterms\"], \"Entities from Job Description\"\n",
      "                    )\n",
      "\n",
      "        # Keywords and values\n",
      "        with st.container():\n",
      "            resumeCol, jobDescriptionCol = st.columns(2)\n",
      "            with resumeCol:\n",
      "                with st.expander(\"Keywords & Values\"):\n",
      "                    df1 = pd.DataFrame(\n",
      "                        selected_file[\"keyterms\"], columns=[\"keyword\", \"value\"]\n",
      "                    )\n",
      "\n",
      "                    # Create the dictionary\n",
      "                    keyword_dict = {}\n",
      "                    for keyword, value in selected_file[\"keyterms\"]:\n",
      "                        keyword_dict[keyword] = value * 100\n",
      "\n",
      "                    fig = go.Figure(\n",
      "                        data=[\n",
      "                            go.Table(\n",
      "                                header=dict(\n",
      "                                    values=[\"Keyword\", \"Value\"],\n",
      "                                    font=dict(size=12, color=\"white\"),\n",
      "                                    fill_color=\"#1d2078\",\n",
      "                                ),\n",
      "                                cells=dict(\n",
      "                                    values=[\n",
      "                                        list(keyword_dict.keys()),\n",
      "                                        list(keyword_dict.values()),\n",
      "                                    ],\n",
      "                                    line_color=\"darkslategray\",\n",
      "                                    fill_color=\"#6DA9E4\",\n",
      "                                ),\n",
      "                            )\n",
      "                        ]\n",
      "                    )\n",
      "                    st.plotly_chart(fig, use_container_width=True)\n",
      "            with jobDescriptionCol:\n",
      "                with st.expander(\"Keywords & Values\"):\n",
      "                    df2 = pd.DataFrame(\n",
      "                        selected_jd[\"keyterms\"], columns=[\"keyword\", \"value\"]\n",
      "                    )\n",
      "\n",
      "                    # Create the dictionary\n",
      "                    keyword_dict = {}\n",
      "                    for keyword, value in selected_jd[\"keyterms\"]:\n",
      "                        keyword_dict[keyword] = value * 100\n",
      "\n",
      "                    fig = go.Figure(\n",
      "                        data=[\n",
      "                            go.Table(\n",
      "                                header=dict(\n",
      "                                    values=[\"Keyword\", \"Value\"],\n",
      "                                    font=dict(size=12, color=\"white\"),\n",
      "                                    fill_color=\"#1d2078\",\n",
      "                                ),\n",
      "                                cells=dict(\n",
      "                                    values=[\n",
      "                                        list(keyword_dict.keys()),\n",
      "                                        list(keyword_dict.values()),\n",
      "                                    ],\n",
      "                                    line_color=\"darkslategray\",\n",
      "                                    fill_color=\"#6DA9E4\",\n",
      "                                ),\n",
      "                            )\n",
      "                        ]\n",
      "                    )\n",
      "                    st.plotly_chart(fig, use_container_width=True)\n",
      "\n",
      "        # Treemaps\n",
      "        with st.container():\n",
      "            resumeCol, jobDescriptionCol = st.columns(2)\n",
      "            with resumeCol:\n",
      "                with st.expander(\"Key Topics\"):\n",
      "                    fig = px.treemap(\n",
      "                        df1,\n",
      "                        path=[\"keyword\"],\n",
      "                        values=\"value\",\n",
      "                        color_continuous_scale=\"Rainbow\",\n",
      "                        title=\"Key Terms/Topics Extracted from your Resume\",\n",
      "                    )\n",
      "                    st.plotly_chart(fig, use_container_width=True)\n",
      "\n",
      "            with jobDescriptionCol:\n",
      "                with st.expander(\"Key Topics\"):\n",
      "                    fig = px.treemap(\n",
      "                        df2,\n",
      "                        path=[\"keyword\"],\n",
      "                        values=\"value\",\n",
      "                        color_continuous_scale=\"Rainbow\",\n",
      "                        title=\"Key Terms/Topics Extracted from Job Description\",\n",
      "                    )\n",
      "                    st.plotly_chart(fig, use_container_width=True)\n",
      "\n",
      "        avs.add_vertical_space(2)\n",
      "        st.markdown(\"#### Similarity Score\")\n",
      "        print(\"Config file parsed successfully:\")\n",
      "        resume_string = \" \".join(selected_file[\"extracted_keywords\"])\n",
      "        jd_string = \" \".join(selected_jd[\"extracted_keywords\"])\n",
      "        result = get_score(resume_string, jd_string)\n",
      "        similarity_score = round(result[0].score * 100, 2)\n",
      "\n",
      "        # Default color to green\n",
      "        score_color = \"green\"\n",
      "        if similarity_score < 60:\n",
      "            score_color = \"red\"\n",
      "        elif 60 <= similarity_score < 75:\n",
      "            score_color = \"orange\"\n",
      "\n",
      "        st.markdown(\n",
      "            f\"Similarity Score obtained for the resume and job description is \"\n",
      "            f'<span style=\"color:{score_color};font-size:24px; font-weight:Bold\">{similarity_score}</span>',\n",
      "            unsafe_allow_html=True,\n",
      "        )\n",
      "\n",
      "        avs.add_vertical_space(2)\n",
      "        with st.expander(\"Common words between Resume and Job Description:\"):\n",
      "            annotated_text(\n",
      "                create_annotated_text(\n",
      "                    selected_file[\"clean_data\"],\n",
      "                    selected_jd[\"extracted_keywords\"],\n",
      "                    \"JD\",\n",
      "                    \"#F24C3D\",\n",
      "                )\n",
      "            )\n",
      "\n",
      "st.divider()\n",
      "\n",
      "# Go back to top\n",
      "st.markdown(\"[:arrow_up: Back to Top](#resume-matcher)\")\n",
      "\n",
      "import json\n",
      "from typing import List\n",
      "\n",
      "import networkx as nx\n",
      "import nltk\n",
      "import pandas as pd\n",
      "import plotly.express as px\n",
      "import plotly.graph_objects as go\n",
      "import streamlit as st\n",
      "from annotated_text import annotated_text, parameters\n",
      "from streamlit_extras import add_vertical_space as avs\n",
      "from streamlit_extras.badges import badge\n",
      "\n",
      "from scripts.utils import get_filenames_from_dir\n",
      "\n",
      "# Set page configuration\n",
      "st.set_page_config(\n",
      "    page_title=\"Resume Matcher\",\n",
      "    page_icon=\"Assets/img/favicon.ico\",\n",
      "    initial_sidebar_state=\"auto\",\n",
      ")\n",
      "\n",
      "nltk.download(\"punkt\")\n",
      "\n",
      "parameters.SHOW_LABEL_SEPARATOR = False\n",
      "parameters.BORDER_RADIUS = 3\n",
      "parameters.PADDING = \"0.5 0.25rem\"\n",
      "\n",
      "\n",
      "def create_star_graph(nodes_and_weights, title):\n",
      "    # Create an empty graph\n",
      "    G = nx.Graph()\n",
      "\n",
      "    # Add the central node\n",
      "    central_node = \"resume\"\n",
      "    G.add_node(central_node)\n",
      "\n",
      "    # Add nodes and edges with weights to the graph\n",
      "    for node, weight in nodes_and_weights:\n",
      "        G.add_node(node)\n",
      "        G.add_edge(central_node, node, weight=weight * 100)\n",
      "\n",
      "    # Get position layout for nodes\n",
      "    pos = nx.spring_layout(G)\n",
      "\n",
      "    # Create edge trace\n",
      "    edge_x = []\n",
      "    edge_y = []\n",
      "    for edge in G.edges():\n",
      "        x0, y0 = pos[edge[0]]\n",
      "        x1, y1 = pos[edge[1]]\n",
      "        edge_x.extend([x0, x1, None])\n",
      "        edge_y.extend([y0, y1, None])\n",
      "\n",
      "    edge_trace = go.Scatter(\n",
      "        x=edge_x,\n",
      "        y=edge_y,\n",
      "        line=dict(width=0.5, color=\"#888\"),\n",
      "        hoverinfo=\"none\",\n",
      "        mode=\"lines\",\n",
      "    )\n",
      "\n",
      "    # Create node trace\n",
      "    node_x = []\n",
      "    node_y = []\n",
      "    for node in G.nodes():\n",
      "        x, y = pos[node]\n",
      "        node_x.append(x)\n",
      "        node_y.append(y)\n",
      "\n",
      "    node_trace = go.Scatter(\n",
      "        x=node_x,\n",
      "        y=node_y,\n",
      "        mode=\"markers\",\n",
      "        hoverinfo=\"text\",\n",
      "        marker=dict(\n",
      "            showscale=True,\n",
      "            colorscale=\"Rainbow\",\n",
      "            reversescale=True,\n",
      "            color=[],\n",
      "            size=10,\n",
      "            colorbar=dict(\n",
      "                thickness=15,\n",
      "                title=\"Node Connections\",\n",
      "                xanchor=\"left\",\n",
      "                titleside=\"right\",\n",
      "            ),\n",
      "            line_width=2,\n",
      "        ),\n",
      "    )\n",
      "\n",
      "    # Color node points by number of connections\n",
      "    node_adjacencies = []\n",
      "    node_text = []\n",
      "    for node in G.nodes():\n",
      "        adjacencies = list(G.adj[node])  # changes here\n",
      "        node_adjacencies.append(len(adjacencies))\n",
      "        node_text.append(f\"{node}<br># of connections: {len(adjacencies)}\")\n",
      "\n",
      "    node_trace.marker.color = node_adjacencies\n",
      "    node_trace.text = node_text\n",
      "\n",
      "    # Create the figure\n",
      "    fig = go.Figure(\n",
      "        data=[edge_trace, node_trace],\n",
      "        layout=go.Layout(\n",
      "            title=title,\n",
      "            titlefont_size=16,\n",
      "            showlegend=False,\n",
      "            hovermode=\"closest\",\n",
      "            margin=dict(b=20, l=5, r=5, t=40),\n",
      "            xaxis=dict(showgrid=False, zeroline=False, showticklabels=False),\n",
      "            yaxis=dict(showgrid=False, zeroline=False, showticklabels=False),\n",
      "        ),\n",
      "    )\n",
      "\n",
      "    # Show the figure\n",
      "    st.plotly_chart(fig)\n",
      "\n",
      "\n",
      "def create_annotated_text(\n",
      "    input_string: str, word_list: List[str], annotation: str, color_code: str\n",
      "):\n",
      "    # Tokenize the input string\n",
      "    tokens = nltk.word_tokenize(input_string)\n",
      "\n",
      "    # Convert the list to a set for quick lookups\n",
      "    word_set = set(word_list)\n",
      "\n",
      "    # Initialize an empty list to hold the annotated text\n",
      "    annotated_text = []\n",
      "\n",
      "    for token in tokens:\n",
      "        # Check if the token is in the set\n",
      "        if token in word_set:\n",
      "            # If it is, append a tuple with the token, annotation, and color code\n",
      "            annotated_text.append((token, annotation, color_code))\n",
      "        else:\n",
      "            # If it's not, just append the token as a string\n",
      "            annotated_text.append(token)\n",
      "\n",
      "    return annotated_text\n",
      "\n",
      "\n",
      "def read_json(filename):\n",
      "    with open(filename) as f:\n",
      "        data = json.load(f)\n",
      "    return data\n",
      "\n",
      "\n",
      "def tokenize_string(input_string):\n",
      "    tokens = nltk.word_tokenize(input_string)\n",
      "    return tokens\n",
      "\n",
      "\n",
      "# Display the main title and subheaders\n",
      "st.title(\":blue[Resume Matcher]\")\n",
      "with st.sidebar:\n",
      "    st.image(\"Assets/img/header_image.png\")\n",
      "    st.subheader(\n",
      "        \"Free and Open Source ATS to help your resume pass the screening stage.\"\n",
      "    )\n",
      "    st.markdown(\n",
      "        \"Check the website [www.resumematcher.fyi](https://www.resumematcher.fyi/)\"\n",
      "    )\n",
      "\n",
      "    st.markdown(\n",
      "        \"Give Resume Matcher a  on [GitHub](https://github.com/srbhr/resume-matcher)\"\n",
      "    )\n",
      "\n",
      "    badge(type=\"github\", name=\"srbhr/Resume-Matcher\")\n",
      "    st.markdown(\"For updates follow me on Twitter.\")\n",
      "    badge(type=\"twitter\", name=\"_srbhr_\")\n",
      "    st.markdown(\n",
      "        \"If you like the project and would like to further help in development please consider \"\n",
      "    )\n",
      "    badge(type=\"buymeacoffee\", name=\"srbhr\")\n",
      "\n",
      "st.divider()\n",
      "avs.add_vertical_space(1)\n",
      "\n",
      "resume_names = get_filenames_from_dir(\"Data/Processed/Resumes\")\n",
      "\n",
      "output = st.selectbox(\n",
      "    f\"There are {len(resume_names)} resumes present. Please select one from the menu below:\",\n",
      "    resume_names,\n",
      ")\n",
      "\n",
      "avs.add_vertical_space(5)\n",
      "\n",
      "selected_file = read_json(\"Data/Processed/Resumes/\" + output)\n",
      "\n",
      "avs.add_vertical_space(2)\n",
      "st.markdown(\"#### Parsed Resume Data\")\n",
      "st.caption(\n",
      "    \"This text is parsed from your resume. This is how it'll look like after getting parsed by an ATS.\"\n",
      ")\n",
      "st.caption(\"Utilize this to understand how to make your resume ATS friendly.\")\n",
      "avs.add_vertical_space(3)\n",
      "# st.json(selected_file)\n",
      "st.write(selected_file[\"clean_data\"])\n",
      "\n",
      "avs.add_vertical_space(3)\n",
      "st.write(\"Now let's take a look at the extracted keywords from the resume.\")\n",
      "\n",
      "annotated_text(\n",
      "    create_annotated_text(\n",
      "        selected_file[\"clean_data\"],\n",
      "        selected_file[\"extracted_keywords\"],\n",
      "        \"KW\",\n",
      "        \"#0B666A\",\n",
      "    )\n",
      ")\n",
      "\n",
      "avs.add_vertical_space(5)\n",
      "st.write(\"Now let's take a look at the extracted entities from the resume.\")\n",
      "\n",
      "# Call the function with your data\n",
      "create_star_graph(selected_file[\"keyterms\"], \"Entities from Resume\")\n",
      "\n",
      "df2 = pd.DataFrame(selected_file[\"keyterms\"], columns=[\"keyword\", \"value\"])\n",
      "\n",
      "# Create the dictionary\n",
      "keyword_dict = {}\n",
      "for keyword, value in selected_file[\"keyterms\"]:\n",
      "    keyword_dict[keyword] = value * 100\n",
      "\n",
      "fig = go.Figure(\n",
      "    data=[\n",
      "        go.Table(\n",
      "            header=dict(\n",
      "                values=[\"Keyword\", \"Value\"], font=dict(size=12), fill_color=\"#070A52\"\n",
      "            ),\n",
      "            cells=dict(\n",
      "                values=[list(keyword_dict.keys()), list(keyword_dict.values())],\n",
      "                line_color=\"darkslategray\",\n",
      "                fill_color=\"#6DA9E4\",\n",
      "            ),\n",
      "        )\n",
      "    ]\n",
      ")\n",
      "st.plotly_chart(fig)\n",
      "\n",
      "st.divider()\n",
      "\n",
      "fig = px.treemap(\n",
      "    df2,\n",
      "    path=[\"keyword\"],\n",
      "    values=\"value\",\n",
      "    color_continuous_scale=\"Rainbow\",\n",
      "    title=\"Key Terms/Topics Extracted from your Resume\",\n",
      ")\n",
      "st.write(fig)\n",
      "\n",
      "avs.add_vertical_space(5)\n",
      "\n",
      "job_descriptions = get_filenames_from_dir(\"Data/Processed/JobDescription\")\n",
      "\n",
      "output = st.selectbox(\n",
      "    f\"There are {len(job_descriptions)} job descriptions present. Please select one from the menu below:\",\n",
      "    job_descriptions,\n",
      ")\n",
      "\n",
      "avs.add_vertical_space(5)\n",
      "\n",
      "selected_jd = read_json(\"Data/Processed/JobDescription/\" + output)\n",
      "\n",
      "avs.add_vertical_space(2)\n",
      "st.markdown(\"#### Job Description\")\n",
      "st.caption(\n",
      "    \"Currently in the pipeline I'm parsing this from PDF but it'll be from txt or copy paste.\"\n",
      ")\n",
      "avs.add_vertical_space(3)\n",
      "# st.json(selected_file)\n",
      "st.write(selected_jd[\"clean_data\"])\n",
      "\n",
      "st.markdown(\"#### Common Words between Job Description and Resumes Highlighted.\")\n",
      "\n",
      "annotated_text(\n",
      "    create_annotated_text(\n",
      "        selected_file[\"clean_data\"], selected_jd[\"extracted_keywords\"], \"JD\", \"#F24C3D\"\n",
      "    )\n",
      ")\n",
      "\n",
      "st.write(\"Now let's take a look at the extracted entities from the job description.\")\n",
      "\n",
      "# Call the function with your data\n",
      "create_star_graph(selected_jd[\"keyterms\"], \"Entities from Job Description\")\n",
      "\n",
      "df2 = pd.DataFrame(selected_jd[\"keyterms\"], columns=[\"keyword\", \"value\"])\n",
      "\n",
      "# Create the dictionary\n",
      "keyword_dict = {}\n",
      "for keyword, value in selected_jd[\"keyterms\"]:\n",
      "    keyword_dict[keyword] = value * 100\n",
      "\n",
      "fig = go.Figure(\n",
      "    data=[\n",
      "        go.Table(\n",
      "            header=dict(\n",
      "                values=[\"Keyword\", \"Value\"], font=dict(size=12), fill_color=\"#070A52\"\n",
      "            ),\n",
      "            cells=dict(\n",
      "                values=[list(keyword_dict.keys()), list(keyword_dict.values())],\n",
      "                line_color=\"darkslategray\",\n",
      "                fill_color=\"#6DA9E4\",\n",
      "            ),\n",
      "        )\n",
      "    ]\n",
      ")\n",
      "st.plotly_chart(fig)\n",
      "\n",
      "st.divider()\n",
      "\n",
      "fig = px.treemap(\n",
      "    df2,\n",
      "    path=[\"keyword\"],\n",
      "    values=\"value\",\n",
      "    color_continuous_scale=\"Rainbow\",\n",
      "    title=\"Key Terms/Topics Extracted from the selected Job Description\",\n",
      ")\n",
      "st.write(fig)\n",
      "\n",
      "avs.add_vertical_space(5)\n",
      "\n",
      "st.divider()\n",
      "\n",
      "st.markdown(\"## Vector Similarity Scores\")\n",
      "st.caption(\"Powered by Qdrant Vector Search\")\n",
      "st.info(\"These are pre-computed queries\", icon=\"\")\n",
      "st.warning(\n",
      "    \"Running Qdrant or Sentence Transformers without having capacity is not recommended\",\n",
      "    icon=\"\",\n",
      ")\n",
      "\n",
      "\n",
      "# Your data\n",
      "data = [\n",
      "    {\n",
      "        \"text\": \"{'resume': 'Alfred Pennyworth\",\n",
      "        \"query\": \"Job Description Product Manager\",\n",
      "        \"score\": 0.62658,\n",
      "    },\n",
      "    {\n",
      "        \"text\": \"{'resume': 'Barry Allen\",\n",
      "        \"query\": \"Job Description Product Manager\",\n",
      "        \"score\": 0.43777737,\n",
      "    },\n",
      "    {\n",
      "        \"text\": \"{'resume': 'Bruce Wayne \",\n",
      "        \"query\": \"Job Description Product Manager\",\n",
      "        \"score\": 0.39835533,\n",
      "    },\n",
      "    {\n",
      "        \"text\": \"{'resume': 'JOHN DOE\",\n",
      "        \"query\": \"Job Description Product Manager\",\n",
      "        \"score\": 0.3915512,\n",
      "    },\n",
      "    {\n",
      "        \"text\": \"{'resume': 'Harvey Dent\",\n",
      "        \"query\": \"Job Description Product Manager\",\n",
      "        \"score\": 0.3519544,\n",
      "    },\n",
      "    {\n",
      "        \"text\": \"{'resume': 'Barry Allen\",\n",
      "        \"query\": \"Job Description Senior Full Stack Engineer\",\n",
      "        \"score\": 0.6541866,\n",
      "    },\n",
      "    {\n",
      "        \"text\": \"{'resume': 'Alfred Pennyworth\",\n",
      "        \"query\": \"Job Description Senior Full Stack Engineer\",\n",
      "        \"score\": 0.59806436,\n",
      "    },\n",
      "    {\n",
      "        \"text\": \"{'resume': 'JOHN DOE\",\n",
      "        \"query\": \"Job Description Senior Full Stack Engineer\",\n",
      "        \"score\": 0.5951386,\n",
      "    },\n",
      "    {\n",
      "        \"text\": \"{'resume': 'Bruce Wayne \",\n",
      "        \"query\": \"Job Description Senior Full Stack Engineer\",\n",
      "        \"score\": 0.57700855,\n",
      "    },\n",
      "    {\n",
      "        \"text\": \"{'resume': 'Harvey Dent\",\n",
      "        \"query\": \"Job Description Senior Full Stack Engineer\",\n",
      "        \"score\": 0.38489106,\n",
      "    },\n",
      "    {\n",
      "        \"text\": \"{'resume': 'Barry Allen\",\n",
      "        \"query\": \"Job Description Front End Engineer\",\n",
      "        \"score\": 0.76813436,\n",
      "    },\n",
      "    {\n",
      "        \"text\": \"{'resume': 'Bruce Wayne'\",\n",
      "        \"query\": \"Job Description Front End Engineer\",\n",
      "        \"score\": 0.60440844,\n",
      "    },\n",
      "    {\n",
      "        \"text\": \"{'resume': 'JOHN DOE\",\n",
      "        \"query\": \"Job Description Front End Engineer\",\n",
      "        \"score\": 0.56080043,\n",
      "    },\n",
      "    {\n",
      "        \"text\": \"{'resume': 'Alfred Pennyworth\",\n",
      "        \"query\": \"Job Description Front End Engineer\",\n",
      "        \"score\": 0.5395049,\n",
      "    },\n",
      "    {\n",
      "        \"text\": \"{'resume': 'Harvey Dent\",\n",
      "        \"query\": \"Job Description Front End Engineer\",\n",
      "        \"score\": 0.3859515,\n",
      "    },\n",
      "    {\n",
      "        \"text\": \"{'resume': 'JOHN DOE\",\n",
      "        \"query\": \"Job Description Java Developer\",\n",
      "        \"score\": 0.5449441,\n",
      "    },\n",
      "    {\n",
      "        \"text\": \"{'resume': 'Alfred Pennyworth\",\n",
      "        \"query\": \"Job Description Java Developer\",\n",
      "        \"score\": 0.53476423,\n",
      "    },\n",
      "    {\n",
      "        \"text\": \"{'resume': 'Barry Allen\",\n",
      "        \"query\": \"Job Description Java Developer\",\n",
      "        \"score\": 0.5313871,\n",
      "    },\n",
      "    {\n",
      "        \"text\": \"{'resume': 'Bruce Wayne \",\n",
      "        \"query\": \"Job Description Java Developer\",\n",
      "        \"score\": 0.44446343,\n",
      "    },\n",
      "    {\n",
      "        \"text\": \"{'resume': 'Harvey Dent\",\n",
      "        \"query\": \"Job Description Java Developer\",\n",
      "        \"score\": 0.3616274,\n",
      "    },\n",
      "]\n",
      "\n",
      "# Create a DataFrame\n",
      "df = pd.DataFrame(data)\n",
      "\n",
      "# Create different DataFrames based on the query and sort by score\n",
      "df1 = df[df[\"query\"] == \"Job Description Product Manager\"].sort_values(\n",
      "    by=\"score\", ascending=False\n",
      ")\n",
      "df2 = df[df[\"query\"] == \"Job Description Senior Full Stack Engineer\"].sort_values(\n",
      "    by=\"score\", ascending=False\n",
      ")\n",
      "df3 = df[df[\"query\"] == \"Job Description Front End Engineer\"].sort_values(\n",
      "    by=\"score\", ascending=False\n",
      ")\n",
      "df4 = df[df[\"query\"] == \"Job Description Java Developer\"].sort_values(\n",
      "    by=\"score\", ascending=False\n",
      ")\n",
      "\n",
      "\n",
      "def plot_df(df, title):\n",
      "    fig = px.bar(df, x=\"text\", y=df[\"score\"] * 100, title=title)\n",
      "    st.plotly_chart(fig)\n",
      "\n",
      "\n",
      "st.markdown(\"### Bar plots of scores based on similarity to Job Description.\")\n",
      "\n",
      "st.subheader(\":blue[Legend]\")\n",
      "st.text(\"Alfred Pennyworth :  Product Manager\")\n",
      "st.text(\"Barry Allen :  Front End Developer\")\n",
      "st.text(\"Harvey Dent :  Machine Learning Engineer\")\n",
      "st.text(\"Bruce Wayne :  Fullstack Developer (MERN)\")\n",
      "st.text(\"John Doe :  Fullstack Developer (Java)\")\n",
      "\n",
      "\n",
      "plot_df(df1, \"Job Description Product Manager 10+ Years of Exper\")\n",
      "plot_df(df2, \"Job Description Senior Full Stack Engineer 5+ Year\")\n",
      "plot_df(df3, \"Job Description Front End Engineer 2 Years of Expe\")\n",
      "plot_df(df4, \"Job Description Java Developer 3 Years of Experien\")\n",
      "\n",
      "\n",
      "avs.add_vertical_space(3)\n",
      "\n",
      "# Go back to top\n",
      "st.markdown(\"[:arrow_up: Back to Top](#resume-matcher)\")\n",
      "\n",
      "File resume_matcher/__init__.py has no source code\n",
      "\n",
      "import re\n",
      "import urllib\n",
      "\n",
      "import spacy\n",
      "\n",
      "from resume_matcher.dataextractor.TextCleaner import TextCleaner\n",
      "\n",
      "# Load the English model\n",
      "nlp = spacy.load(\"en_core_web_md\")\n",
      "\n",
      "RESUME_SECTIONS = [\n",
      "    \"Contact Information\",\n",
      "    \"Objective\",\n",
      "    \"Summary\",\n",
      "    \"Education\",\n",
      "    \"Experience\",\n",
      "    \"Skills\",\n",
      "    \"Projects\",\n",
      "    \"Certifications\",\n",
      "    \"Licenses\",\n",
      "    \"Awards\",\n",
      "    \"Honors\",\n",
      "    \"Publications\",\n",
      "    \"References\",\n",
      "    \"Technical Skills\",\n",
      "    \"Computer Skills\",\n",
      "    \"Programming Languages\",\n",
      "    \"Software Skills\",\n",
      "    \"Soft Skills\",\n",
      "    \"Language Skills\",\n",
      "    \"Professional Skills\",\n",
      "    \"Transferable Skills\",\n",
      "    \"Work Experience\",\n",
      "    \"Professional Experience\",\n",
      "    \"Employment History\",\n",
      "    \"Internship Experience\",\n",
      "    \"Volunteer Experience\",\n",
      "    \"Leadership Experience\",\n",
      "    \"Research Experience\",\n",
      "    \"Teaching Experience\",\n",
      "]\n",
      "\n",
      "\n",
      "class DataExtractor:\n",
      "    \"\"\"\n",
      "    A class for extracting various types of data from text.\n",
      "    \"\"\"\n",
      "\n",
      "    def __init__(self, raw_text: str):\n",
      "        \"\"\"\n",
      "        Initialize the DataExtractor object.\n",
      "\n",
      "        Args:\n",
      "            raw_text (str): The raw input text.\n",
      "        \"\"\"\n",
      "\n",
      "        self.text = raw_text\n",
      "        self.clean_text = TextCleaner.clean_text(self.text)\n",
      "        self.doc = nlp(self.clean_text)\n",
      "\n",
      "    def extract_links(self):\n",
      "        \"\"\"\n",
      "        Find links of any type in a given string.\n",
      "\n",
      "        Args:\n",
      "            text (str): The string to search for links.\n",
      "\n",
      "        Returns:\n",
      "            list: A list containing all the found links.\n",
      "        \"\"\"\n",
      "        link_pattern = r\"\\b(?:https?://|www\\.)\\S+\\b\"\n",
      "        links = re.findall(link_pattern, self.text)\n",
      "        return links\n",
      "\n",
      "    def extract_links_extended(self):\n",
      "        \"\"\"\n",
      "        Extract links of all kinds (HTTP, HTTPS, FTP, email, www.linkedin.com,\n",
      "          and github.com/user_name) from a webpage.\n",
      "\n",
      "        Args:\n",
      "            url (str): The URL of the webpage.\n",
      "\n",
      "        Returns:\n",
      "            list: A list containing all the extracted links.\n",
      "        \"\"\"\n",
      "        links = []\n",
      "        try:\n",
      "            response = urllib.request.urlopen(self.text)\n",
      "            html_content = response.read().decode(\"utf-8\")\n",
      "            pattern = r'href=[\\'\"]?([^\\'\" >]+)'\n",
      "            raw_links = re.findall(pattern, html_content)\n",
      "            for link in raw_links:\n",
      "                if link.startswith(\n",
      "                    (\n",
      "                        \"http://\",\n",
      "                        \"https://\",\n",
      "                        \"ftp://\",\n",
      "                        \"mailto:\",\n",
      "                        \"www.linkedin.com\",\n",
      "                        \"github.com/\",\n",
      "                        \"twitter.com\",\n",
      "                    )\n",
      "                ):\n",
      "                    links.append(link)\n",
      "        except Exception as e:\n",
      "            print(f\"Error extracting links: {str(e)}\")\n",
      "        return links\n",
      "\n",
      "    def extract_names(self):\n",
      "        \"\"\"Extracts and returns a list of names from the given\n",
      "        text using spaCy's named entity recognition.\n",
      "\n",
      "        Args:\n",
      "            text (str): The text to extract names from.\n",
      "\n",
      "        Returns:\n",
      "            list: A list of strings representing the names extracted from the text.\n",
      "        \"\"\"\n",
      "        names = [ent.text for ent in self.doc.ents if ent.label_ == \"PERSON\"]\n",
      "        return names\n",
      "\n",
      "    def extract_emails(self):\n",
      "        \"\"\"\n",
      "        Extract email addresses from a given string.\n",
      "\n",
      "        Args:\n",
      "            text (str): The string from which to extract email addresses.\n",
      "\n",
      "        Returns:\n",
      "            list: A list containing all the extracted email addresses.\n",
      "        \"\"\"\n",
      "        email_pattern = r\"\\b[A-Za-z0-9._%+-]+@[A-Za-z0-9.-]+\\.[A-Za-z]{2,}\\b\"\n",
      "        emails = re.findall(email_pattern, self.text)\n",
      "        return emails\n",
      "\n",
      "    def extract_phone_numbers(self):\n",
      "        \"\"\"\n",
      "        Extract phone numbers from a given string.\n",
      "\n",
      "        Args:\n",
      "            text (str): The string from which to extract phone numbers.\n",
      "\n",
      "        Returns:\n",
      "            list: A list containing all the extracted phone numbers.\n",
      "        \"\"\"\n",
      "        phone_number_pattern = (\n",
      "            r\"^(\\+\\d{1,3})?[-.\\s]?\\(?\\d{3}\\)?[-.\\s]?\\d{3}[-.\\s]?\\d{4}$\"\n",
      "        )\n",
      "        phone_numbers = re.findall(phone_number_pattern, self.text)\n",
      "        return phone_numbers\n",
      "\n",
      "    def extract_experience(self):\n",
      "        \"\"\"\n",
      "        Extract experience from a given string. It does so by using the Spacy module.\n",
      "\n",
      "        Args:\n",
      "            text (str): The string from which to extract experience.\n",
      "\n",
      "        Returns:\n",
      "            str: A string containing all the extracted experience.\n",
      "        \"\"\"\n",
      "        experience_section = []\n",
      "        in_experience_section = False\n",
      "\n",
      "        for token in self.doc:\n",
      "            if token.text in RESUME_SECTIONS:\n",
      "                if token.text == \"Experience\" or \"EXPERIENCE\" or \"experience\":\n",
      "                    in_experience_section = True\n",
      "                else:\n",
      "                    in_experience_section = False\n",
      "\n",
      "            if in_experience_section:\n",
      "                experience_section.append(token.text)\n",
      "\n",
      "        return \" \".join(experience_section)\n",
      "\n",
      "    def extract_position_year(self):\n",
      "        \"\"\"\n",
      "        Extract position and year from a given string.\n",
      "\n",
      "        Args:\n",
      "            text (str): The string from which to extract position and year.\n",
      "\n",
      "        Returns:\n",
      "            list: A list containing the extracted position and year.\n",
      "        \"\"\"\n",
      "        position_year_search_pattern = (\n",
      "            r\"(\\b\\w+\\b\\s+\\b\\w+\\b),\\s+(\\d{4})\\s*-\\s*(\\d{4}|\\bpresent\\b)\"\n",
      "        )\n",
      "        position_year = re.findall(position_year_search_pattern, self.text)\n",
      "        return position_year\n",
      "\n",
      "    def extract_particular_words(self):\n",
      "        \"\"\"\n",
      "        Extract nouns and proper nouns from the given text.\n",
      "\n",
      "        Args:\n",
      "            text (str): The input text to extract nouns from.\n",
      "\n",
      "        Returns:\n",
      "            list: A list of extracted nouns.\n",
      "        \"\"\"\n",
      "        pos_tags = [\"NOUN\", \"PROPN\"]\n",
      "        nouns = [token.text for token in self.doc if token.pos_ in pos_tags]\n",
      "        return nouns\n",
      "\n",
      "    def extract_entities(self):\n",
      "        \"\"\"\n",
      "        Extract named entities of types 'GPE' (geopolitical entity) and 'ORG' (organization) from the given text.\n",
      "\n",
      "        Args:\n",
      "            text (str): The input text to extract entities from.\n",
      "\n",
      "        Returns:\n",
      "            list: A list of extracted entities.\n",
      "        \"\"\"\n",
      "        entity_labels = [\"GPE\", \"ORG\"]\n",
      "        entities = [\n",
      "            token.text for token in self.doc.ents if token.label_ in entity_labels\n",
      "        ]\n",
      "        return list(set(entities))\n",
      "\n",
      "import spacy\n",
      "import textacy\n",
      "from textacy import extract\n",
      "\n",
      "# Load the English model\n",
      "nlp = spacy.load(\"en_core_web_md\")\n",
      "\n",
      "RESUME_SECTIONS = [\n",
      "    \"Contact Information\",\n",
      "    \"Objective\",\n",
      "    \"Summary\",\n",
      "    \"Education\",\n",
      "    \"Experience\",\n",
      "    \"Skills\",\n",
      "    \"Projects\",\n",
      "    \"Certifications\",\n",
      "    \"Licenses\",\n",
      "    \"Awards\",\n",
      "    \"Honors\",\n",
      "    \"Publications\",\n",
      "    \"References\",\n",
      "    \"Technical Skills\",\n",
      "    \"Computer Skills\",\n",
      "    \"Programming Languages\",\n",
      "    \"Software Skills\",\n",
      "    \"Soft Skills\",\n",
      "    \"Language Skills\",\n",
      "    \"Professional Skills\",\n",
      "    \"Transferable Skills\",\n",
      "    \"Work Experience\",\n",
      "    \"Professional Experience\",\n",
      "    \"Employment History\",\n",
      "    \"Internship Experience\",\n",
      "    \"Volunteer Experience\",\n",
      "    \"Leadership Experience\",\n",
      "    \"Research Experience\",\n",
      "    \"Teaching Experience\",\n",
      "]\n",
      "\n",
      "REGEX_PATTERNS = {\n",
      "    \"email_pattern\": r\"\\b[A-Za-z0-9._%+-]+@[A-Za-z0-9.-]+\\.[A-Za-z]{2,}\\b\",\n",
      "    \"phone_pattern\": r\"\\(?\\d{3}\\)?[-.\\s]?\\d{3}[-.\\s]?\\d{4}\",\n",
      "    \"link_pattern\": r\"\\b(?:https?://|www\\.)\\S+\\b\",\n",
      "}\n",
      "\n",
      "READ_RESUME_FROM = \"Data/Resumes/\"\n",
      "SAVE_DIRECTORY_RESUME = \"Data/Processed/Resumes\"\n",
      "\n",
      "READ_JOB_DESCRIPTION_FROM = \"Data/JobDescription/\"\n",
      "SAVE_DIRECTORY_JOB_DESCRIPTION = \"Data/Processed/JobDescription\"\n",
      "\n",
      "\n",
      "class KeytermExtractor:\n",
      "    \"\"\"\n",
      "    A class for extracting keyterms from a given text using various algorithms.\n",
      "    \"\"\"\n",
      "\n",
      "    def __init__(self, raw_text: str, top_n_values: int = 20):\n",
      "        \"\"\"\n",
      "        Initialize the KeytermExtractor object.\n",
      "\n",
      "        Args:\n",
      "            raw_text (str): The raw input text.\n",
      "            top_n_values (int): The number of top keyterms to extract.\n",
      "        \"\"\"\n",
      "        self.raw_text = raw_text\n",
      "        self.text_doc = textacy.make_spacy_doc(self.raw_text, lang=\"en_core_web_md\")\n",
      "        self.top_n_values = top_n_values\n",
      "\n",
      "    def get_keyterms_based_on_textrank(self):\n",
      "        \"\"\"\n",
      "        Extract keyterms using the TextRank algorithm.\n",
      "\n",
      "        Returns:\n",
      "            List[str]: A list of top keyterms based on TextRank.\n",
      "        \"\"\"\n",
      "        return list(\n",
      "            extract.keyterms.textrank(\n",
      "                self.text_doc, normalize=\"lemma\", topn=self.top_n_values\n",
      "            )\n",
      "        )\n",
      "\n",
      "    def get_keyterms_based_on_sgrank(self):\n",
      "        \"\"\"\n",
      "        Extract keyterms using the SGRank algorithm.\n",
      "\n",
      "        Returns:\n",
      "            List[str]: A list of top keyterms based on SGRank.\n",
      "        \"\"\"\n",
      "        return list(\n",
      "            extract.keyterms.sgrank(\n",
      "                self.text_doc, normalize=\"lemma\", topn=self.top_n_values\n",
      "            )\n",
      "        )\n",
      "\n",
      "    def get_keyterms_based_on_scake(self):\n",
      "        \"\"\"\n",
      "        Extract keyterms using the sCAKE algorithm.\n",
      "\n",
      "        Returns:\n",
      "            List[str]: A list of top keyterms based on sCAKE.\n",
      "        \"\"\"\n",
      "        return list(\n",
      "            extract.keyterms.scake(\n",
      "                self.text_doc, normalize=\"lemma\", topn=self.top_n_values\n",
      "            )\n",
      "        )\n",
      "\n",
      "    def get_keyterms_based_on_yake(self):\n",
      "        \"\"\"\n",
      "        Extract keyterms using the YAKE algorithm.\n",
      "\n",
      "        Returns:\n",
      "            List[str]: A list of top keyterms based on YAKE.\n",
      "        \"\"\"\n",
      "        return list(\n",
      "            extract.keyterms.yake(\n",
      "                self.text_doc, normalize=\"lemma\", topn=self.top_n_values\n",
      "            )\n",
      "        )\n",
      "\n",
      "    def bi_gramchunker(self):\n",
      "        \"\"\"\n",
      "        Chunk the text into bigrams.\n",
      "\n",
      "        Returns:\n",
      "            List[str]: A list of bigrams.\n",
      "        \"\"\"\n",
      "        return list(\n",
      "            textacy.extract.basics.ngrams(\n",
      "                self.text_doc,\n",
      "                n=2,\n",
      "                filter_stops=True,\n",
      "                filter_nums=True,\n",
      "                filter_punct=True,\n",
      "            )\n",
      "        )\n",
      "\n",
      "    def tri_gramchunker(self):\n",
      "        \"\"\"\n",
      "        Chunk the text into trigrams.\n",
      "\n",
      "        Returns:\n",
      "            List[str]: A list of trigrams.\n",
      "        \"\"\"\n",
      "        return list(\n",
      "            textacy.extract.basics.ngrams(\n",
      "                self.text_doc,\n",
      "                n=3,\n",
      "                filter_stops=True,\n",
      "                filter_nums=True,\n",
      "                filter_punct=True,\n",
      "            )\n",
      "        )\n",
      "\n",
      "import re\n",
      "\n",
      "import spacy\n",
      "\n",
      "# Load the English model\n",
      "nlp = spacy.load(\"en_core_web_md\")\n",
      "\n",
      "RESUME_SECTIONS = [\n",
      "    \"Contact Information\",\n",
      "    \"Objective\",\n",
      "    \"Summary\",\n",
      "    \"Education\",\n",
      "    \"Experience\",\n",
      "    \"Skills\",\n",
      "    \"Projects\",\n",
      "    \"Certifications\",\n",
      "    \"Licenses\",\n",
      "    \"Awards\",\n",
      "    \"Honors\",\n",
      "    \"Publications\",\n",
      "    \"References\",\n",
      "    \"Technical Skills\",\n",
      "    \"Computer Skills\",\n",
      "    \"Programming Languages\",\n",
      "    \"Software Skills\",\n",
      "    \"Soft Skills\",\n",
      "    \"Language Skills\",\n",
      "    \"Professional Skills\",\n",
      "    \"Transferable Skills\",\n",
      "    \"Work Experience\",\n",
      "    \"Professional Experience\",\n",
      "    \"Employment History\",\n",
      "    \"Internship Experience\",\n",
      "    \"Volunteer Experience\",\n",
      "    \"Leadership Experience\",\n",
      "    \"Research Experience\",\n",
      "    \"Teaching Experience\",\n",
      "]\n",
      "\n",
      "REGEX_PATTERNS = {\n",
      "    \"email_pattern\": r\"\\b[A-Za-z0-9._%+-]+@[A-Za-z0-9.-]+\\.[A-Za-z]{2,}\\b\",\n",
      "    \"phone_pattern\": r\"\\(?\\d{3}\\)?[-.\\s]?\\d{3}[-.\\s]?\\d{4}\",\n",
      "    \"link_pattern\": r\"\\b(?:https?://|www\\.)\\S+\\b\",\n",
      "}\n",
      "\n",
      "READ_RESUME_FROM = \"Data/Resumes/\"\n",
      "SAVE_DIRECTORY_RESUME = \"Data/Processed/Resumes\"\n",
      "\n",
      "READ_JOB_DESCRIPTION_FROM = \"Data/JobDescription/\"\n",
      "SAVE_DIRECTORY_JOB_DESCRIPTION = \"Data/Processed/JobDescription\"\n",
      "\n",
      "\n",
      "class TextCleaner:\n",
      "    \"\"\"\n",
      "    A class for cleaning a text by removing specific patterns.\n",
      "    \"\"\"\n",
      "\n",
      "    def remove_emails_links(text):\n",
      "        \"\"\"\n",
      "        Clean the input text by removing specific patterns.\n",
      "\n",
      "        Args:\n",
      "            text (str): The input text to clean.\n",
      "\n",
      "        Returns:\n",
      "            str: The cleaned text.\n",
      "        \"\"\"\n",
      "        for pattern in REGEX_PATTERNS:\n",
      "            text = re.sub(REGEX_PATTERNS[pattern], \"\", text)\n",
      "        return text\n",
      "\n",
      "    def clean_text(text):\n",
      "        \"\"\"\n",
      "        Clean the input text by removing specific patterns.\n",
      "\n",
      "        Args:\n",
      "            text (str): The input text to clean.\n",
      "\n",
      "        Returns:\n",
      "            str: The cleaned text.\n",
      "        \"\"\"\n",
      "        text = TextCleaner.remove_emails_links(text)\n",
      "        doc = nlp(text)\n",
      "        for token in doc:\n",
      "            if token.pos_ == \"PUNCT\":\n",
      "                text = text.replace(token.text, \"\")\n",
      "        return str(text)\n",
      "\n",
      "    def remove_stopwords(text):\n",
      "        \"\"\"\n",
      "        Clean the input text by removing stopwords.\n",
      "\n",
      "        Args:\n",
      "            text (str): The input text to clean.\n",
      "\n",
      "        Returns:\n",
      "            str: The cleaned text.\n",
      "        \"\"\"\n",
      "        doc = nlp(text)\n",
      "        for token in doc:\n",
      "            if token.is_stop:\n",
      "                text = text.replace(token.text, \"\")\n",
      "        return text\n",
      "\n",
      "\n",
      "class CountFrequency:\n",
      "    def __init__(self, text):\n",
      "        self.text = text\n",
      "        self.doc = nlp(text)\n",
      "\n",
      "    def count_frequency(self):\n",
      "        \"\"\"\n",
      "        Count the frequency of words in the input text.\n",
      "\n",
      "        Returns:\n",
      "            dict: A dictionary with the words as keys and the frequency as values.\n",
      "        \"\"\"\n",
      "        pos_freq = {}\n",
      "        for token in self.doc:\n",
      "            if token.pos_ in pos_freq:\n",
      "                pos_freq[token.pos_] += 1\n",
      "            else:\n",
      "                pos_freq[token.pos_] = 1\n",
      "        return pos_freq\n",
      "\n",
      "File resume_matcher/dataextractor/__init__.py has no source code\n",
      "\n",
      "import os\n",
      "\n",
      "from resume_matcher.run_first import run_first\n",
      "from resume_matcher.scripts.get_score import get_score\n",
      "from resume_matcher.scripts.logger import init_logging_config\n",
      "from resume_matcher.scripts.utils import find_path, read_json\n",
      "\n",
      "init_logging_config()\n",
      "\n",
      "run_first()\n",
      "\n",
      "cwd = find_path(\"Resume-Matcher\")\n",
      "\n",
      "PROCESSED_RESUMES_PATH = os.path.join(cwd, \"Data\", \"Processed\", \"Resumes/\")\n",
      "PROCESSED_JOB_DESCRIPTIONS_PATH = os.path.join(\n",
      "    cwd, \"Data\", \"Processed\", \"JobDescription/\"\n",
      ")\n",
      "\n",
      "\n",
      "def get_filenames_from_dir(directory):\n",
      "    return [f for f in os.listdir(directory) if os.path.isfile(os.path.join(directory, f))]\n",
      "\n",
      "\n",
      "def process_files(resume, job_description):\n",
      "    resume_dict = read_json(PROCESSED_RESUMES_PATH + resume)\n",
      "    job_dict = read_json(PROCESSED_JOB_DESCRIPTIONS_PATH + job_description)\n",
      "    resume_keywords = resume_dict[\"extracted_keywords\"]\n",
      "    job_description_keywords = job_dict[\"extracted_keywords\"]\n",
      "\n",
      "    resume_string = \" \".join(resume_keywords)\n",
      "    jd_string = \" \".join(job_description_keywords)\n",
      "    final_result = get_score(resume_string, jd_string)\n",
      "    for r in final_result:\n",
      "        print(r.score)\n",
      "    print(f\"Processing resume: {resume}\")\n",
      "    print(f\"Processing job description: {job_description}\")\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "import logging\n",
      "import os\n",
      "\n",
      "from tqdm import tqdm\n",
      "from resume_matcher.scripts.processor import Processor\n",
      "from resume_matcher.scripts.utils import get_filenames_from_dir, find_path\n",
      "\n",
      "cwd = find_path(\"Resume-Matcher\")\n",
      "RESUMES_PATH = os.path.join(cwd, \"Data\", \"Resumes/\")\n",
      "JOB_DESCRIPTIONS_PATH = os.path.join(cwd, \"Data\", \"JobDescription/\")\n",
      "PROCESSED_RESUMES_PATH = os.path.join(cwd, \"Data\", \"Processed\", \"Resumes/\")\n",
      "PROCESSED_JOB_DESCRIPTIONS_PATH = os.path.join(\n",
      "    cwd, \"Data\", \"Processed\", \"JobDescription/\"\n",
      ")\n",
      "\n",
      "logger = logging.getLogger(__name__)\n",
      "\n",
      "\n",
      "def remove_old_files(files_path):\n",
      "    for filename in os.listdir(files_path):\n",
      "        try:\n",
      "            file_path = os.path.join(files_path, filename)\n",
      "\n",
      "            if os.path.isfile(file_path):\n",
      "                os.remove(file_path)\n",
      "        except Exception as e:\n",
      "            logging.error(f\"Error deleting {file_path}:\\n{e}\")\n",
      "\n",
      "    logging.info(\"Deleted old files from \" + files_path)\n",
      "\n",
      "\n",
      "def process_files(data_path, processed_path, file_type):\n",
      "    print(f\"Processing {file_type}s from {data_path}\")\n",
      "    logging.info(f\"Started to read from {data_path}\")\n",
      "    try:\n",
      "        remove_old_files(processed_path)\n",
      "        file_names = get_filenames_from_dir(data_path)\n",
      "        logging.info(f\"Reading from {data_path} is now complete.\")\n",
      "    except:\n",
      "        logging.error(f\"There are no {file_type}s present in the specified folder.\")\n",
      "        logging.error(\"Exiting from the program.\")\n",
      "        logging.error(\n",
      "            f\"Please add {file_type}s in the {data_path} folder and try again.\"\n",
      "        )\n",
      "        exit(1)\n",
      "\n",
      "    logging.info(f\"Started parsing the {file_type}s.\")\n",
      "    for file in tqdm(file_names):\n",
      "        processor_object = Processor(file, file_type)\n",
      "        success = processor_object.process()\n",
      "    print(f\"Processing of {file_type}s is now complete.\")\n",
      "    logging.info(f\"Parsing of the {file_type}s is now complete.\")\n",
      "\n",
      "\n",
      "def run_first():\n",
      "    process_files(RESUMES_PATH, PROCESSED_RESUMES_PATH, \"resume\")\n",
      "    process_files(\n",
      "        JOB_DESCRIPTIONS_PATH, PROCESSED_JOB_DESCRIPTIONS_PATH, \"job_description\"\n",
      "    )\n",
      "\n",
      "File resume_matcher/scripts/__init__.py has no source code\n",
      "\n",
      "import logging\n",
      "import os\n",
      "from typing import List\n",
      "\n",
      "from qdrant_client import QdrantClient\n",
      "\n",
      "from resume_matcher.scripts.utils import find_path, read_json\n",
      "\n",
      "# Get the logger\n",
      "logger = logging.getLogger(__name__)\n",
      "\n",
      "# Set the logging level\n",
      "logger.setLevel(logging.INFO)\n",
      "\n",
      "\n",
      "cwd = find_path(\"Resume-Matcher\")\n",
      "READ_RESUME_FROM = os.path.join(cwd, \"Data\", \"Processed\", \"Resumes/\")\n",
      "READ_JOB_DESCRIPTION_FROM = os.path.join(cwd, \"Data\", \"Processed\", \"JobDescription/\")\n",
      "\n",
      "\n",
      "def get_score(resume_string, job_description_string):\n",
      "    \"\"\"\n",
      "    The function `get_score` uses QdrantClient to calculate the similarity score between a resume and a\n",
      "    job description.\n",
      "\n",
      "    Args:\n",
      "      resume_string: The `resume_string` parameter is a string containing the text of a resume. It\n",
      "    represents the content of a resume that you want to compare with a job description.\n",
      "      job_description_string: The `get_score` function you provided seems to be using a QdrantClient to\n",
      "    calculate the similarity score between a resume and a job description. The function takes in two\n",
      "    parameters: `resume_string` and `job_description_string`, where `resume_string` is the text content\n",
      "    of the resume and\n",
      "\n",
      "    Returns:\n",
      "      The function `get_score` returns the search result obtained by querying a QdrantClient with the\n",
      "    job description string against the resume string provided.\n",
      "    \"\"\"\n",
      "    logger.info(\"Started getting similarity score\")\n",
      "\n",
      "    documents: List[str] = [resume_string]\n",
      "    client = QdrantClient(\":memory:\")\n",
      "    client.set_model(\"BAAI/bge-base-en\")\n",
      "\n",
      "    client.add(\n",
      "        collection_name=\"demo_collection\",\n",
      "        documents=documents,\n",
      "    )\n",
      "\n",
      "    search_result = client.query(\n",
      "        collection_name=\"demo_collection\", query_text=job_description_string\n",
      "    )\n",
      "    logger.info(\"Finished getting similarity score\")\n",
      "    return search_result\n",
      "\n",
      "\n",
      "def custom_test():\n",
      "    # To give your custom resume use this code\n",
      "    resume_dict = read_json(\n",
      "        READ_RESUME_FROM\n",
      "        + \"resume_barry_allen_fe.pdf44a91b3b-b553-4765-b6b8-bfe26135f87b.json\"\n",
      "    )\n",
      "    job_dict = read_json(\n",
      "        READ_JOB_DESCRIPTION_FROM\n",
      "        + \"job_description_job_desc_front_end_engineer.pdf947c72ae-7faf-45fa-86a4-92db51c07b45.json\"\n",
      "    )\n",
      "    resume_keywords = resume_dict[\"extracted_keywords\"]\n",
      "    job_description_keywords = job_dict[\"extracted_keywords\"]\n",
      "\n",
      "    resume_string = \" \".join(resume_keywords)\n",
      "    jd_string = \" \".join(job_description_keywords)\n",
      "    final_result = get_score(resume_string, jd_string)\n",
      "    for r in final_result:\n",
      "        print(r.score)\n",
      "\n",
      "\n",
      "if __name__ == \"__main__\":\n",
      "    custom_test()\n",
      "\n",
      "import logging\n",
      "\n",
      "\n",
      "def get_handlers(\n",
      "    filename=\"app.log\", mode=\"w\", file_level=logging.DEBUG, stderr_level=logging.DEBUG\n",
      "):\n",
      "    \"\"\"\n",
      "    The function `get_handlers` returns a stream handler and a file handler with specified logging\n",
      "    levels and formatters.\n",
      "\n",
      "    Args:\n",
      "      stderr_level:\n",
      "      file_level:\n",
      "      mode:\n",
      "      filename: The `filename` parameter is the name of the log file where the log messages will be\n",
      "    written. In this case, the default filename is \"app.log\". Defaults to app.log\n",
      "      mode: The `mode` parameter in the `get_handlers` function specifies the mode in which the file\n",
      "    should be opened. In this case, the default mode is set to \"w\", which stands for write mode. This\n",
      "    means that if the file already exists, it will be truncated (i.e., its. Defaults to w\n",
      "      file_level: The `file_level` parameter in the `get_handlers` function is used to specify the\n",
      "    logging level for the file handler. In this case, it is set to `logging.DEBUG`, which means that the\n",
      "    file handler will log all messages at the DEBUG level and above.\n",
      "      stderr_level: The `stderr_level` parameter in the `get_handlers` function is used to specify the\n",
      "    logging level for the StreamHandler that outputs log messages to the standard error stream (stderr).\n",
      "    This level determines which log messages will be processed and output by the StreamHandler.\n",
      "\n",
      "    Returns:\n",
      "      The `get_handlers` function returns two logging handlers: `stderr_handler` which is a\n",
      "    StreamHandler for logging to stderr, and `file_handler` which is a FileHandler for logging to a file\n",
      "    specified by the `filename` parameter.\n",
      "    \"\"\"\n",
      "    # Stream handler\n",
      "    stderr_handler = logging.StreamHandler()\n",
      "    stderr_handler.setLevel(stderr_level)\n",
      "    stderr_handler.setFormatter(CustomFormatter())\n",
      "\n",
      "    # File handler\n",
      "    file_handler = logging.FileHandler(filename, mode=mode)\n",
      "    file_handler.setLevel(file_level)\n",
      "    file_handler.setFormatter(CustomFormatter(True))\n",
      "\n",
      "    # TODO: Add RotatingFileHandler\n",
      "\n",
      "    return stderr_handler, file_handler\n",
      "\n",
      "\n",
      "class CustomFormatter(logging.Formatter):\n",
      "    \"\"\"\n",
      "    A custom log formatter that adds color to log messages based on the log level.\n",
      "\n",
      "    Args:\n",
      "        file (bool): Indicates whether the log is being written to a file. Default is False.\n",
      "\n",
      "    Attributes:\n",
      "        FORMATS (dict): A dictionary mapping log levels to colorized log message formats.\n",
      "\n",
      "    Methods:\n",
      "        format(record): Formats the log record with the appropriate colorized log message format.\n",
      "\n",
      "    \"\"\"\n",
      "\n",
      "    def __init__(self, file=False):\n",
      "        \"\"\"\n",
      "        This function initializes logging formats with different colors and styles based on the log\n",
      "        level.\n",
      "\n",
      "        Args:\n",
      "          file: The `file` parameter in the `__init__` method is a boolean flag that determines whether\n",
      "        the logging output should be colored or not. If `file` is `True`, the colors will not be applied\n",
      "        to the log messages. Defaults to False\n",
      "        \"\"\"\n",
      "        super().__init__()\n",
      "        yellow = \"\\x1b[36;10m\" if not file else \"\"\n",
      "        blue = \"\\x1b[35;10m\" if not file else \"\"\n",
      "        green = \"\\x1b[32;10m\" if not file else \"\"\n",
      "        red = \"\\x1b[31;10m\" if not file else \"\"\n",
      "        bold_red = \"\\x1b[31;1m\" if not file else \"\"\n",
      "        reset = \"\\x1b[0m\" if not file else \"\"\n",
      "        log = \"%(asctime)s (%(filename)s:%(lineno)d) - %(levelname)s: \"\n",
      "        msg = reset + \"%(message)s\"\n",
      "\n",
      "        self.FORMATS = {\n",
      "            logging.DEBUG: blue + log + msg,\n",
      "            logging.INFO: green + log + msg,\n",
      "            logging.WARNING: yellow + log + msg,\n",
      "            logging.ERROR: red + log + msg,\n",
      "            logging.CRITICAL: bold_red + log + msg,\n",
      "        }\n",
      "\n",
      "    def format(self, record):\n",
      "        \"\"\"\n",
      "        Formats the log record with the appropriate colorized log message format.\n",
      "\n",
      "        Args:\n",
      "            record (LogRecord): The log record to be formatted.\n",
      "\n",
      "        Returns:\n",
      "            str: The formatted log message.\n",
      "\n",
      "        \"\"\"\n",
      "        log_fmt = self.FORMATS.get(record.levelno)\n",
      "        formatter = logging.Formatter(log_fmt)\n",
      "        return formatter.format(record)\n",
      "\n",
      "\n",
      "def init_logging_config(\n",
      "    basic_log_level=logging.INFO,\n",
      "    filename=\"app.log\",\n",
      "    mode=\"w\",\n",
      "    file_level=logging.DEBUG,\n",
      "    stderr_level=logging.DEBUG,\n",
      "):\n",
      "    \"\"\"\n",
      "    The function `init_logging_config` initializes logging configuration in Python by setting basic log\n",
      "    level, configuring handlers, and adding them to the logger.\n",
      "\n",
      "    Args:\n",
      "      basic_log_level: The `basic_log_level` parameter is used to set the logging level for the root\n",
      "    logger. In this function, it is set to `logging.INFO` by default, which means that log messages with\n",
      "    severity level INFO or higher will be processed.\n",
      "      filename: The `filename` parameter is a string that specifies the name of the log file where the\n",
      "    logs will be written. In the `init_logging_config` function you provided, the default value for\n",
      "    `filename` is \"app.log\". This means that if no filename is provided when calling the function, logs.\n",
      "    Defaults to app.log\n",
      "      mode: The `mode` parameter in the `init_logging_config` function specifies the mode in which the\n",
      "    log file will be opened. In this case, the default value is \"w\" which stands for write mode. This\n",
      "    means that the log file will be opened for writing, and if the file already exists. Defaults to w\n",
      "      file_level: The `file_level` parameter in the `init_logging_config` function is used to specify\n",
      "    the logging level for the file handler. This determines the severity level of log messages that will\n",
      "    be written to the log file specified by the `filename` parameter. In this case, the default value\n",
      "    for `file\n",
      "      stderr_level: The `stderr_level` parameter in the `init_logging_config` function is used to\n",
      "    specify the logging level for the stderr (standard error) handler. This handler is responsible for\n",
      "    directing log messages to the standard error stream. The logging level determines which severity of\n",
      "    log messages will be output to the stderr.\n",
      "    \"\"\"\n",
      "\n",
      "    logger = logging.getLogger()\n",
      "    logger.setLevel(basic_log_level)\n",
      "\n",
      "    # Get the handlers\n",
      "    stderr_handler, file_handler = get_handlers(\n",
      "        file_level=file_level, stderr_level=stderr_level, filename=filename, mode=mode\n",
      "    )\n",
      "\n",
      "    # Add the handlers\n",
      "    logger.addHandler(stderr_handler)\n",
      "    logger.addHandler(file_handler)\n",
      "\n",
      "from resume_matcher.dataextractor.DataExtractor import DataExtractor\n",
      "from resume_matcher.dataextractor.KeyTermExtractor import KeytermExtractor\n",
      "from resume_matcher.dataextractor.TextCleaner import TextCleaner, CountFrequency\n",
      "from resume_matcher.scripts.utils import generate_unique_id\n",
      "\n",
      "\n",
      "class ParseDocumentToJson:\n",
      "    def __init__(self, doc: str, doc_type: str):\n",
      "        self.years = None\n",
      "        self.phones = None\n",
      "        self.emails = None\n",
      "        self.experience = None\n",
      "        self.name = None\n",
      "        self.doc_data = doc\n",
      "        self.doc_type = doc_type\n",
      "        self.clean_data = TextCleaner.clean_text(self.doc_data)\n",
      "        self.entities = DataExtractor(self.clean_data).extract_entities()\n",
      "        self.key_words = DataExtractor(self.clean_data).extract_particular_words()\n",
      "        self.pos_frequencies = CountFrequency(self.clean_data).count_frequency()\n",
      "        self.keyterms = KeytermExtractor(self.clean_data).get_keyterms_based_on_sgrank()\n",
      "        self.bi_grams = KeytermExtractor(self.clean_data).bi_gramchunker()\n",
      "        self.tri_grams = KeytermExtractor(self.clean_data).tri_gramchunker()\n",
      "        if self.doc_type == \"resume\":\n",
      "            self.get_additional_data()\n",
      "\n",
      "    def get_additional_data(self):\n",
      "        self.name = DataExtractor(self.clean_data[:30]).extract_names()\n",
      "        self.experience = DataExtractor(self.clean_data).extract_experience()\n",
      "        self.emails = DataExtractor(self.doc_data).extract_emails()\n",
      "        self.phones = DataExtractor(self.doc_data).extract_phone_numbers()\n",
      "        self.years = DataExtractor(self.clean_data).extract_position_year()\n",
      "\n",
      "    def get_JSON(self) -> dict:\n",
      "        doc_dictionary = {\n",
      "            \"unique_id\": generate_unique_id(),\n",
      "            \"doc_data\": self.doc_data,\n",
      "            \"clean_data\": self.clean_data,\n",
      "            \"entities\": self.entities,\n",
      "            \"extracted_keywords\": self.key_words,\n",
      "            \"keyterms\": self.keyterms,\n",
      "            \"bi_grams\": str(self.bi_grams),\n",
      "            \"tri_grams\": str(self.tri_grams),\n",
      "            \"pos_frequencies\": self.pos_frequencies,\n",
      "        }\n",
      "        if self.doc_type == \"resume\":\n",
      "            doc_dictionary.update(\n",
      "                {\n",
      "                    \"name\": self.name,\n",
      "                    \"experience\": self.experience,\n",
      "                    \"emails\": self.emails,\n",
      "                    \"phones\": self.phones,\n",
      "                    \"years\": self.years,\n",
      "                }\n",
      "            )\n",
      "        return doc_dictionary\n",
      "\n",
      "import json\n",
      "import os\n",
      "import os.path\n",
      "import pathlib\n",
      "\n",
      "from .parser import ParseDocumentToJson\n",
      "from .utils import read_single_pdf, find_path\n",
      "\n",
      "cwd = find_path(\"Resume-Matcher\")\n",
      "\n",
      "READ_RESUME_FROM = os.path.join(cwd, \"Data\", \"Resumes/\")\n",
      "SAVE_RESUME_TO = os.path.join(cwd, \"Data\", \"Processed\", \"Resumes/\")\n",
      "\n",
      "READ_JOB_DESCRIPTION_FROM = os.path.join(cwd, \"Data\", \"JobDescription/\")\n",
      "SAVE_JOB_DESCRIPTION_TO = os.path.join(cwd, \"Data\", \"Processed\", \"JobDescription/\")\n",
      "\n",
      "\n",
      "class Processor:\n",
      "    def __init__(self, input_file, file_type):\n",
      "        self.input_file = input_file\n",
      "        self.file_type = file_type\n",
      "        if file_type == \"resume\":\n",
      "            self.input_file_name = os.path.join(READ_RESUME_FROM + self.input_file)\n",
      "        elif file_type == \"job_description\":\n",
      "            self.input_file_name = os.path.join(\n",
      "                READ_JOB_DESCRIPTION_FROM + self.input_file\n",
      "            )\n",
      "\n",
      "    def process(self) -> bool:\n",
      "        try:\n",
      "            data_dict = self._read_data()\n",
      "            self._write_json_file(data_dict)\n",
      "            return True\n",
      "        except Exception as e:\n",
      "            print(f\"An error occurred: {str(e)}\")\n",
      "            return False\n",
      "\n",
      "    def _read_data(self) -> dict:\n",
      "        data = read_single_pdf(self.input_file_name)\n",
      "        output = ParseDocumentToJson(data, self.file_type).get_JSON()\n",
      "        return output\n",
      "\n",
      "    def _write_json_file(self, data_dict: dict):\n",
      "        file_name = str(\n",
      "            f\"{self.file_type}_\" + self.input_file + data_dict[\"unique_id\"] + \".json\"\n",
      "        )\n",
      "        save_directory_name = None\n",
      "        if self.file_type == \"resume\":\n",
      "            save_directory_name = pathlib.Path(SAVE_RESUME_TO) / file_name\n",
      "        elif self.file_type == \"job_description\":\n",
      "            save_directory_name = pathlib.Path(SAVE_JOB_DESCRIPTION_TO) / file_name\n",
      "        json_object = json.dumps(data_dict, sort_keys=True, indent=14)\n",
      "        with open(save_directory_name, \"w+\") as outfile:\n",
      "            outfile.write(json_object)\n",
      "\n",
      "import glob\n",
      "import json\n",
      "import logging\n",
      "import os\n",
      "import os.path\n",
      "from uuid import uuid4\n",
      "\n",
      "from pypdf import PdfReader\n",
      "\n",
      "logger = logging.getLogger(__name__)\n",
      "\n",
      "\n",
      "def find_path(folder_name):\n",
      "    \"\"\"\n",
      "    The function `find_path` searches for a folder by name starting from the current directory and\n",
      "    traversing up the directory tree until the folder is found or the root directory is reached.\n",
      "\n",
      "    Args:\n",
      "      folder_name: The `find_path` function you provided is designed to search for a folder by name\n",
      "    starting from the current working directory and moving up the directory tree until it finds the\n",
      "    folder or reaches the root directory.\n",
      "\n",
      "    Returns:\n",
      "      The `find_path` function is designed to search for a folder with the given `folder_name` starting\n",
      "    from the current working directory (`os.getcwd()`). It iterates through the directory structure,\n",
      "    checking if the folder exists in the current directory or any of its parent directories. If the\n",
      "    folder is found, it returns the full path to that folder using `os.path.join(curr_dir, folder_name)`\n",
      "    \"\"\"\n",
      "    curr_dir = os.getcwd()\n",
      "    while True:\n",
      "        if folder_name in os.listdir(curr_dir):\n",
      "            return os.path.join(curr_dir, folder_name)\n",
      "        else:\n",
      "            parent_dir = os.path.dirname(curr_dir)\n",
      "            if parent_dir == \"/\":\n",
      "                break\n",
      "            curr_dir = parent_dir\n",
      "    raise ValueError(f\"Folder '{folder_name}' not found.\")\n",
      "\n",
      "\n",
      "def read_json(path):\n",
      "    \"\"\"\n",
      "    The `read_json` function reads a JSON file from the specified path and returns its contents, handling\n",
      "    any exceptions that may occur during the process.\n",
      "\n",
      "    Args:\n",
      "      path: The `path` parameter in the `read_doc` function is a string that represents the file path to\n",
      "    the JSON document that you want to read and load. This function reads the JSON data from the file\n",
      "    located at the specified path.\n",
      "\n",
      "    Returns:\n",
      "      The function `read_doc(path)` reads a JSON file located at the specified `path`, and returns the\n",
      "    data loaded from the file. If there is an error reading the JSON file, it logs the error message and\n",
      "    returns an empty dictionary `{}`.\n",
      "    \"\"\"\n",
      "    with open(path) as f:\n",
      "        try:\n",
      "            data = json.load(f)\n",
      "        except Exception as e:\n",
      "            logger.error(f\"Error reading JSON file: {e}\")\n",
      "            data = {}\n",
      "    return data\n",
      "\n",
      "\n",
      "def read_multiple_pdf(file_path: str) -> list:\n",
      "    \"\"\"\n",
      "    Read multiple PDF files from the specified file path and extract the text from each page.\n",
      "\n",
      "    Args:\n",
      "        file_path (str): The directory path containing the PDF files.\n",
      "\n",
      "    Returns:\n",
      "        list: A list containing the extracted text from each page of the PDF files.\n",
      "    \"\"\"\n",
      "    pdf_files = get_pdf_files(file_path)\n",
      "    output = []\n",
      "    for file in pdf_files:\n",
      "        try:\n",
      "            with open(file, \"rb\") as f:\n",
      "                pdf_reader = PdfReader(f)\n",
      "                count = pdf_reader.getNumPages()\n",
      "                for i in range(count):\n",
      "                    page = pdf_reader.getPage(i)\n",
      "                    output.append(page.extractText())\n",
      "        except Exception as e:\n",
      "            print(f\"Error reading file '{file}': {str(e)}\")\n",
      "    return output\n",
      "\n",
      "\n",
      "def read_single_pdf(file_path: str) -> str:\n",
      "    \"\"\"\n",
      "    Read a single PDF file and extract the text from each page.\n",
      "\n",
      "    Args:\n",
      "        file_path (str): The path of the PDF file.\n",
      "\n",
      "    Returns:\n",
      "        list: A list containing the extracted text from each page of the PDF file.\n",
      "    \"\"\"\n",
      "    output = []\n",
      "    try:\n",
      "        with open(file_path, \"rb\") as f:\n",
      "            pdf_reader = PdfReader(f)\n",
      "            count = len(pdf_reader.pages)\n",
      "            for i in range(count):\n",
      "                page = pdf_reader.pages[i]\n",
      "                output.append(page.extract_text())\n",
      "    except Exception as e:\n",
      "        print(f\"Error reading file '{file_path}': {str(e)}\")\n",
      "    return str(\" \".join(output))\n",
      "\n",
      "\n",
      "def get_pdf_files(file_path: str) -> list:\n",
      "    \"\"\"\n",
      "    Get a list of PDF files from the specified directory path.\n",
      "\n",
      "    Args:\n",
      "        file_path (str): The directory path containing the PDF files.\n",
      "\n",
      "    Returns:\n",
      "        list: A list of PDF file paths.\n",
      "    \"\"\"\n",
      "    pdf_files = []\n",
      "    try:\n",
      "        pdf_files = glob.glob(os.path.join(file_path, \"*.pdf\"))\n",
      "    except Exception as e:\n",
      "        print(f\"Error getting PDF files from '{file_path}': {str(e)}\")\n",
      "    return pdf_files\n",
      "\n",
      "\n",
      "def generate_unique_id():\n",
      "    \"\"\"\n",
      "    Generate a unique ID and return it as a string.\n",
      "\n",
      "    Returns:\n",
      "        str: A string with a unique ID.\n",
      "    \"\"\"\n",
      "    return str(uuid4())\n",
      "\n",
      "\n",
      "def get_filenames_from_dir(directory_path: str) -> list:\n",
      "    filenames = [\n",
      "        f\n",
      "        for f in os.listdir(directory_path)\n",
      "        if os.path.isfile(os.path.join(directory_path, f)) and f != \".DS_Store\"\n",
      "    ]\n",
      "    return filenames\n",
      "\n",
      "import json\n",
      "import logging\n",
      "import os\n",
      "\n",
      "import yaml\n",
      "from qdrant_client import QdrantClient, models\n",
      "from qdrant_client.http.models import Batch\n",
      "\n",
      "from scripts.utils.logger import get_handlers, init_logging_config\n",
      "\n",
      "init_logging_config(basic_log_level=logging.INFO)\n",
      "# Get the logger\n",
      "logger = logging.getLogger(__name__)\n",
      "\n",
      "# Set the logging level\n",
      "logger.setLevel(logging.INFO)\n",
      "\n",
      "stderr_handler, file_handler = get_handlers()\n",
      "\n",
      "\n",
      "def find_path(folder_name):\n",
      "    \"\"\"\n",
      "    Find the path of a folder with the given name in the current directory or its parent directories.\n",
      "\n",
      "    Args:\n",
      "        folder_name (str): The name of the folder to search for.\n",
      "\n",
      "    Returns:\n",
      "        str: The path of the folder if found.\n",
      "\n",
      "    Raises:\n",
      "        ValueError: If the folder with the given name is not found in the current directory or its parent directories.\n",
      "    \"\"\"\n",
      "    curr_dir = os.getcwd()\n",
      "    while True:\n",
      "        if folder_name in os.listdir(curr_dir):\n",
      "            return os.path.join(curr_dir, folder_name)\n",
      "        else:\n",
      "            parent_dir = os.path.dirname(curr_dir)\n",
      "            if parent_dir == \"/\":\n",
      "                break\n",
      "            curr_dir = parent_dir\n",
      "    raise ValueError(f\"Folder '{folder_name}' not found.\")\n",
      "\n",
      "\n",
      "cwd = find_path(\"Resume-Matcher\")\n",
      "READ_RESUME_FROM = os.path.join(cwd, \"Data\", \"Processed\", \"Resumes\")\n",
      "READ_JOB_DESCRIPTION_FROM = os.path.join(cwd, \"Data\", \"Processed\", \"JobDescription\")\n",
      "config_path = os.path.join(cwd, \"scripts\", \"similarity\")\n",
      "\n",
      "\n",
      "def read_config(filepath):\n",
      "    \"\"\"\n",
      "    Reads a configuration file in YAML format and returns the parsed configuration.\n",
      "\n",
      "    Args:\n",
      "        filepath (str): The path to the configuration file.\n",
      "\n",
      "    Returns:\n",
      "        dict: The parsed configuration as a dictionary.\n",
      "\n",
      "    Raises:\n",
      "        FileNotFoundError: If the configuration file is not found.\n",
      "        yaml.YAMLError: If there is an error parsing the YAML in the configuration file.\n",
      "        Exception: If there is an error reading the configuration file.\n",
      "\n",
      "    \"\"\"\n",
      "    try:\n",
      "        with open(filepath) as f:\n",
      "            config = yaml.safe_load(f)\n",
      "        return config\n",
      "    except FileNotFoundError as e:\n",
      "        logger.error(f\"Configuration file {filepath} not found: {e}\")\n",
      "    except yaml.YAMLError as e:\n",
      "        logger.error(\n",
      "            f\"Error parsing YAML in configuration file {filepath}: {e}\", exc_info=True\n",
      "        )\n",
      "    except Exception as e:\n",
      "        logger.error(f\"Error reading configuration file {filepath}: {e}\")\n",
      "    return None\n",
      "\n",
      "\n",
      "def read_doc(path):\n",
      "    \"\"\"\n",
      "    Read a JSON file and return its contents as a dictionary.\n",
      "\n",
      "    Args:\n",
      "        path (str): The path to the JSON file.\n",
      "\n",
      "    Returns:\n",
      "        dict: The contents of the JSON file as a dictionary.\n",
      "\n",
      "    Raises:\n",
      "        Exception: If there is an error reading the JSON file.\n",
      "    \"\"\"\n",
      "    with open(path) as f:\n",
      "        try:\n",
      "            data = json.load(f)\n",
      "        except Exception as e:\n",
      "            logger.error(f\"Error reading JSON file: {e}\")\n",
      "            data = {}\n",
      "    return data\n",
      "\n",
      "\n",
      "# This class likely performs searches based on quadrants.\n",
      "class QdrantSearch:\n",
      "    def __init__(self, resumes, jd):\n",
      "        \"\"\"\n",
      "        The function initializes various parameters and clients for processing resumes and job\n",
      "        descriptions.\n",
      "\n",
      "        Args:\n",
      "          resumes: The `resumes` parameter in the `__init__` method seems to be a list of resumes that\n",
      "        is passed to the class constructor. It is likely used within the class for some processing or\n",
      "        analysis related to resumes. If you have any specific questions or need further assistance with\n",
      "        this parameter or any\n",
      "          jd: The `jd` parameter in the `__init__` method seems to represent a job description. It is\n",
      "        likely used as input to compare against the resumes provided in the `resumes` parameter. The job\n",
      "        description is probably used for matching and analyzing against the resumes in the system.\n",
      "        \"\"\"\n",
      "        config = read_config(config_path + \"/config.yml\")\n",
      "        self.cohere_key = config[\"cohere\"][\"api_key\"]\n",
      "        self.qdrant_key = config[\"qdrant\"][\"api_key\"]\n",
      "        self.qdrant_url = config[\"qdrant\"][\"url\"]\n",
      "        self.resumes = resumes\n",
      "        self.jd = jd\n",
      "        self.cohere = cohere.Client(self.cohere_key)\n",
      "        self.collection_name = \"resume_collection_name\"\n",
      "        self.qdrant = QdrantClient(\n",
      "            url=self.qdrant_url,\n",
      "            api_key=self.qdrant_key,\n",
      "        )\n",
      "\n",
      "        vector_size = 4096\n",
      "        print(f\"collection name={self.collection_name}\")\n",
      "        self.qdrant.recreate_collection(\n",
      "            collection_name=self.collection_name,\n",
      "            vectors_config=models.VectorParams(\n",
      "                size=vector_size, distance=models.Distance.COSINE\n",
      "            ),\n",
      "        )\n",
      "\n",
      "        self.logger = logging.getLogger(self.__class__.__name__)\n",
      "\n",
      "        self.logger.addHandler(stderr_handler)\n",
      "        self.logger.addHandler(file_handler)\n",
      "\n",
      "    def get_embedding(self, text):\n",
      "        \"\"\"\n",
      "        The function `get_embedding` takes a text input, generates embeddings using the Cohere API, and\n",
      "        returns the embeddings as a list of floats along with the length of the embeddings.\n",
      "\n",
      "        Args:\n",
      "          text: The `text` parameter in the `get_embedding` function is a string that represents the\n",
      "        text for which you want to generate embeddings. This text will be passed to the Cohere API to\n",
      "        retrieve the embeddings for further processing.\n",
      "\n",
      "        Returns:\n",
      "          The `get_embedding` function returns a tuple containing two elements:\n",
      "        1. A list of floating-point numbers representing the embeddings of the input text.\n",
      "        2. The length of the embeddings list.\n",
      "        \"\"\"\n",
      "        try:\n",
      "            embeddings = self.cohere.embed([text], \"large\").embeddings\n",
      "            return list(map(float, embeddings[0])), len(embeddings[0])\n",
      "        except Exception as e:\n",
      "            self.logger.error(f\"Error getting embeddings: {e}\", exc_info=True)\n",
      "\n",
      "    def update_qdrant(self):\n",
      "        \"\"\"\n",
      "        This Python function updates vectors and corresponding metadata in a Qdrant collection based on\n",
      "        resumes.\n",
      "        \"\"\"\n",
      "        vectors = []\n",
      "        ids = []\n",
      "        for i, resume in enumerate(self.resumes):\n",
      "            vector, size = self.get_embedding(resume)\n",
      "            vectors.append(vector)\n",
      "            ids.append(i)\n",
      "        try:\n",
      "            self.qdrant.upsert(\n",
      "                collection_name=self.collection_name,\n",
      "                points=Batch(\n",
      "                    ids=ids,\n",
      "                    vectors=vectors,\n",
      "                    payloads=[{\"text\": resume} for resume in self.resumes],\n",
      "                ),\n",
      "            )\n",
      "        except Exception as e:\n",
      "            self.logger.error(\n",
      "                f\"Error upserting the vectors to the qdrant collection: {e}\",\n",
      "                exc_info=True,\n",
      "            )\n",
      "\n",
      "    def search(self):\n",
      "        \"\"\"\n",
      "        The `search` function retrieves search results based on a query vector using a specified\n",
      "        collection in a search engine.\n",
      "\n",
      "        Returns:\n",
      "          A list of dictionaries containing the text and score of the search results.\n",
      "        \"\"\"\n",
      "        vector, _ = self.get_embedding(self.jd)\n",
      "\n",
      "        hits = self.qdrant.search(\n",
      "            collection_name=self.collection_name, query_vector=vector, limit=30\n",
      "        )\n",
      "        results = []\n",
      "        for hit in hits:\n",
      "            result = {\"text\": str(hit.payload)[:30], \"score\": hit.score}\n",
      "            results.append(result)\n",
      "\n",
      "        return results\n",
      "\n",
      "\n",
      "def get_similarity_score(resume_string, job_description_string):\n",
      "    \"\"\"\n",
      "    This Python function `get_similarity_score` calculates the similarity score between a resume and a\n",
      "    job description using QdrantSearch.\n",
      "\n",
      "    Args:\n",
      "      resume_string: The `get_similarity_score` function seems to be using a `QdrantSearch` class to\n",
      "    calculate the similarity score between a resume and a job description. The `resume_string` parameter\n",
      "    likely contains the text content of a resume, while the `job_description_string` parameter contains\n",
      "    the text content of\n",
      "      job_description_string: The `job_description_string` parameter is a string containing the job\n",
      "    description for which you want to calculate the similarity score with a given resume. This\n",
      "    description typically includes details about the job requirements, responsibilities, qualifications,\n",
      "    and skills needed for the position. The function `get_similarity_score` takes this job description\n",
      "\n",
      "    Returns:\n",
      "      The function `get_similarity_score` returns the search result obtained from comparing a resume\n",
      "    string with a job description string using a QdrantSearch object.\n",
      "    \"\"\"\n",
      "    logger.info(\"Started getting similarity score\")\n",
      "    qdrant_search = QdrantSearch([resume_string], job_description_string)\n",
      "    qdrant_search.update_qdrant()\n",
      "    search_result = qdrant_search.search()\n",
      "    logger.info(\"Finished getting similarity score\")\n",
      "    return search_result\n",
      "\n",
      "\n",
      "if __name__ == \"__main__\":\n",
      "    # To give your custom resume use this code\n",
      "    resume_dict = read_config(\n",
      "        READ_RESUME_FROM\n",
      "        + \"/Resume-bruce_wayne_fullstack.pdf4783d115-e6fc-462e-ae4d-479152884b28.json\"\n",
      "    )\n",
      "    job_dict = read_config(\n",
      "        READ_JOB_DESCRIPTION_FROM\n",
      "        + \"/JobDescription-job_desc_full_stack_engineer_pdf4de00846-a4fe-4fe5-a4d7\"\n",
      "        \"-2a8a1b9ad020.json\"\n",
      "    )\n",
      "    resume_keywords = resume_dict[\"extracted_keywords\"]\n",
      "    job_description_keywords = job_dict[\"extracted_keywords\"]\n",
      "\n",
      "    resume_string = \" \".join(resume_keywords)\n",
      "    jd_string = \" \".join(job_description_keywords)\n",
      "    final_result = get_similarity_score(resume_string, jd_string)\n",
      "    for r in final_result:\n",
      "        print(r)\n",
      "\n",
      "import json\n",
      "import logging\n",
      "import os\n",
      "\n",
      "from scripts import JobDescriptionProcessor, ResumeProcessor\n",
      "from scripts.utils import get_filenames_from_dir, init_logging_config\n",
      "\n",
      "init_logging_config()\n",
      "\n",
      "script_dir = os.path.dirname(os.path.abspath(__file__))\n",
      "PROCESSED_DATA_PATH = os.path.join(script_dir, \"Data\", \"Processed\")\n",
      "PROCESSED_RESUMES_PATH = os.path.join(PROCESSED_DATA_PATH, \"Resumes\")\n",
      "PROCESSED_JOB_DESCRIPTIONS_PATH = os.path.join(\n",
      "    PROCESSED_DATA_PATH, \"JobDescription\"\n",
      ")\n",
      "\n",
      "# check if processed data directory exists\n",
      "if not os.path.exists(PROCESSED_DATA_PATH):\n",
      "    os.makedirs(PROCESSED_DATA_PATH)\n",
      "    os.makedirs(PROCESSED_RESUMES_PATH)\n",
      "    os.makedirs(PROCESSED_JOB_DESCRIPTIONS_PATH)\n",
      "    logging.info(\"Created necessary directories.\")\n",
      "\n",
      "\n",
      "def read_json(filename):\n",
      "    with open(filename) as f:\n",
      "        data = json.load(f)\n",
      "    return data\n",
      "\n",
      "\n",
      "def remove_old_files(files_path):\n",
      "\n",
      "    for filename in os.listdir(files_path):\n",
      "        try:\n",
      "            file_path = os.path.join(files_path, filename)\n",
      "\n",
      "            if os.path.isfile(file_path):\n",
      "                os.remove(file_path)\n",
      "        except Exception as e:\n",
      "            logging.error(f\"Error deleting {file_path}:\\n{e}\")\n",
      "\n",
      "    logging.info(\"Deleted old files from \" + files_path)\n",
      "\n",
      "\n",
      "logging.info(\"Started to read from Data/Resumes\")\n",
      "try:\n",
      "    # Check if there are resumes present or not.\n",
      "    # If present then parse it.\n",
      "    remove_old_files(PROCESSED_RESUMES_PATH)\n",
      "\n",
      "    file_names = get_filenames_from_dir(\"Data/Resumes\")\n",
      "    logging.info(\"Reading from Data/Resumes is now complete.\")\n",
      "except Exception:\n",
      "    # Exit the program if there are no resumes.\n",
      "    logging.error(\"There are no resumes present in the specified folder.\")\n",
      "    logging.error(\"Exiting from the program.\")\n",
      "    logging.error(\n",
      "        \"Please add resumes in the Data/Resumes folder and try again.\"\n",
      "    )\n",
      "    exit(1)\n",
      "\n",
      "# Now after getting the file_names parse the resumes into a JSON Format.\n",
      "logging.info(\"Started parsing the resumes.\")\n",
      "for file in file_names:\n",
      "    processor = ResumeProcessor(file)\n",
      "    success = processor.process()\n",
      "logging.info(\"Parsing of the resumes is now complete.\")\n",
      "\n",
      "logging.info(\"Started to read from Data/JobDescription\")\n",
      "try:\n",
      "    # Check if there are resumes present or not.\n",
      "    # If present then parse it.\n",
      "    remove_old_files(PROCESSED_JOB_DESCRIPTIONS_PATH)\n",
      "\n",
      "    file_names = get_filenames_from_dir(\"Data/JobDescription\")\n",
      "    logging.info(\"Reading from Data/JobDescription is now complete.\")\n",
      "except Exception:\n",
      "    # Exit the program if there are no resumes.\n",
      "    logging.error(\n",
      "        \"There are no job-description present in the specified folder.\"\n",
      "    )\n",
      "    logging.error(\"Exiting from the program.\")\n",
      "    logging.error(\n",
      "        \"Please add resumes in the Data/JobDescription folder and try again.\"\n",
      "    )\n",
      "    exit(1)\n",
      "\n",
      "# Now after getting the file_names parse the resumes into a JSON Format.\n",
      "logging.info(\"Started parsing the Job Descriptions.\")\n",
      "for file in file_names:\n",
      "    processor = JobDescriptionProcessor(file)\n",
      "    success = processor.process()\n",
      "logging.info(\"Parsing of the Job Descriptions is now complete.\")\n",
      "logging.info(\"Success now run `streamlit run streamlit_second.py`\")\n",
      "\n",
      "import json\n",
      "import logging\n",
      "import os\n",
      "\n",
      "from scripts import JobDescriptionProcessor, ResumeProcessor\n",
      "from scripts.utils import get_filenames_from_dir, init_logging_config\n",
      "\n",
      "init_logging_config()\n",
      "\n",
      "PROCESSED_RESUMES_PATH = \"Data/Processed/Resumes\"\n",
      "PROCESSED_JOB_DESCRIPTIONS_PATH = \"Data/Processed/JobDescription\"\n",
      "\n",
      "\n",
      "def read_json(filename):\n",
      "    with open(filename) as f:\n",
      "        data = json.load(f)\n",
      "    return data\n",
      "\n",
      "\n",
      "def remove_old_files(files_path):\n",
      "    if not os.path.exists(files_path): # Check if the folder exists or not.\n",
      "        # Create the folder if it doesn't exist to avoid error in the next step.\n",
      "        os.makedirs(files_path)\n",
      "\n",
      "    for filename in os.listdir(files_path):\n",
      "        try:\n",
      "            file_path = os.path.join(files_path, filename)\n",
      "\n",
      "            if os.path.isfile(file_path):\n",
      "                os.remove(file_path)\n",
      "        except Exception as e:\n",
      "            logging.error(f\"Error deleting {file_path}:\\n{e}\")\n",
      "\n",
      "    logging.info(\"Deleted old files from \" + files_path)\n",
      "\n",
      "\n",
      "logging.info(\"Started to read from Data/Resumes\")\n",
      "try:\n",
      "    # Check if there are resumes present or not.\n",
      "    # If present then parse it.\n",
      "    remove_old_files(PROCESSED_RESUMES_PATH)\n",
      "\n",
      "    file_names = get_filenames_from_dir(\"Data/Resumes\")\n",
      "    logging.info(\"Reading from Data/Resumes is now complete.\")\n",
      "except:\n",
      "    # Exit the program if there are no resumes.\n",
      "    logging.error(\"There are no resumes present in the specified folder.\")\n",
      "    logging.error(\"Exiting from the program.\")\n",
      "    logging.error(\"Please add resumes in the Data/Resumes folder and try again.\")\n",
      "    exit(1)\n",
      "\n",
      "# Now after getting the file_names parse the resumes into a JSON Format.\n",
      "logging.info(\"Started parsing the resumes.\")\n",
      "for file in file_names:\n",
      "    processor = ResumeProcessor(file)\n",
      "    success = processor.process()\n",
      "logging.info(\"Parsing of the resumes is now complete.\")\n",
      "\n",
      "logging.info(\"Started to read from Data/JobDescription\")\n",
      "try:\n",
      "    # Check if there are resumes present or not.\n",
      "    # If present then parse it.\n",
      "    remove_old_files(PROCESSED_JOB_DESCRIPTIONS_PATH)\n",
      "\n",
      "    file_names = get_filenames_from_dir(\"Data/JobDescription\")\n",
      "    logging.info(\"Reading from Data/JobDescription is now complete.\")\n",
      "except:\n",
      "    # Exit the program if there are no resumes.\n",
      "    logging.error(\"There are no job-description present in the specified folder.\")\n",
      "    logging.error(\"Exiting from the program.\")\n",
      "    logging.error(\"Please add resumes in the Data/JobDescription folder and try again.\")\n",
      "    exit(1)\n",
      "\n",
      "# Now after getting the file_names parse the resumes into a JSON Format.\n",
      "logging.info(\"Started parsing the Job Descriptions.\")\n",
      "for file in file_names:\n",
      "    processor = JobDescriptionProcessor(file)\n",
      "    success = processor.process()\n",
      "logging.info(\"Parsing of the Job Descriptions is now complete.\")\n",
      "logging.info(\"Success now run `streamlit run streamlit_second.py`\")\n",
      "\n",
      "import json\n",
      "import logging\n",
      "import os\n",
      "\n",
      "from scripts import JobDescriptionProcessor, ResumeProcessor\n",
      "from scripts.utils import get_filenames_from_dir, init_logging_config\n",
      "\n",
      "init_logging_config()\n",
      "\n",
      "PROCESSED_RESUMES_PATH = \"Data/Processed/Resumes\"\n",
      "PROCESSED_JOB_DESCRIPTIONS_PATH = \"Data/Processed/JobDescription\"\n",
      "\n",
      "\n",
      "def read_json(filename):\n",
      "    with open(filename) as f:\n",
      "        data = json.load(f)\n",
      "    return data\n",
      "\n",
      "\n",
      "def remove_old_files(files_path):\n",
      "    if not os.path.exists(files_path): # Check if the folder exists or not.\n",
      "        # Create the folder if it doesn't exist to avoid error in the next step.\n",
      "        os.makedirs(files_path)\n",
      "\n",
      "    for filename in os.listdir(files_path):\n",
      "        try:\n",
      "            file_path = os.path.join(files_path, filename)\n",
      "\n",
      "            if os.path.isfile(file_path):\n",
      "                os.remove(file_path)\n",
      "        except Exception as e:\n",
      "            logging.error(f\"Error deleting {file_path}:\\n{e}\")\n",
      "\n",
      "    logging.info(\"Deleted old files from \" + files_path)\n",
      "\n",
      "\n",
      "logging.info(\"Started to read from Data/Resumes\")\n",
      "try:\n",
      "    # Check if there are resumes present or not.\n",
      "    if not os.path.exists(PROCESSED_RESUMES_PATH):\n",
      "        # If not present then create one.\n",
      "        os.makedirs(PROCESSED_RESUMES_PATH)\n",
      "    else:\n",
      "        # If present then parse it.\n",
      "        remove_old_files(PROCESSED_RESUMES_PATH)\n",
      "\n",
      "    file_names = get_filenames_from_dir(\"Data/Resumes\")\n",
      "    logging.info(\"Reading from Data/Resumes is now complete.\")\n",
      "except:\n",
      "    # Exit the program if there are no resumes.\n",
      "    logging.error(\"There are no resumes present in the specified folder.\")\n",
      "    logging.error(\"Exiting from the program.\")\n",
      "    logging.error(\"Please add resumes in the Data/Resumes folder and try again.\")\n",
      "    exit(1)\n",
      "\n",
      "# Now after getting the file_names parse the resumes into a JSON Format.\n",
      "logging.info(\"Started parsing the resumes.\")\n",
      "for file in file_names:\n",
      "    processor = ResumeProcessor(file)\n",
      "    success = processor.process()\n",
      "logging.info(\"Parsing of the resumes is now complete.\")\n",
      "\n",
      "logging.info(\"Started to read from Data/JobDescription\")\n",
      "try:\n",
      "    # Check if there are resumes present or not.\n",
      "    if not os.path.exists(PROCESSED_JOB_DESCRIPTIONS_PATH):\n",
      "        # If not present then create one.\n",
      "        os.makedirs(PROCESSED_JOB_DESCRIPTIONS_PATH)\n",
      "    else:  \n",
      "    # If present then parse it.\n",
      "        remove_old_files(PROCESSED_JOB_DESCRIPTIONS_PATH)\n",
      "\n",
      "    file_names = get_filenames_from_dir(\"Data/JobDescription\")\n",
      "    logging.info(\"Reading from Data/JobDescription is now complete.\")\n",
      "except:\n",
      "    # Exit the program if there are no resumes.\n",
      "    logging.error(\"There are no job-description present in the specified folder.\")\n",
      "    logging.error(\"Exiting from the program.\")\n",
      "    logging.error(\"Please add resumes in the Data/JobDescription folder and try again.\")\n",
      "    exit(1)\n",
      "\n",
      "# Now after getting the file_names parse the resumes into a JSON Format.\n",
      "logging.info(\"Started parsing the Job Descriptions.\")\n",
      "for file in file_names:\n",
      "    processor = JobDescriptionProcessor(file)\n",
      "    success = processor.process()\n",
      "logging.info(\"Parsing of the Job Descriptions is now complete.\")\n",
      "logging.info(\"Success now run `streamlit run streamlit_second.py`\")\n",
      "\n",
      "import json\n",
      "import os\n",
      "from typing import List\n",
      "\n",
      "import networkx as nx\n",
      "import nltk\n",
      "import pandas as pd\n",
      "import plotly.express as px\n",
      "import plotly.graph_objects as go\n",
      "import streamlit as st\n",
      "from annotated_text import annotated_text, parameters\n",
      "from streamlit_extras import add_vertical_space as avs\n",
      "from streamlit_extras.badges import badge\n",
      "\n",
      "from scripts.similarity.get_score import *\n",
      "from scripts.utils import get_filenames_from_dir\n",
      "from scripts.utils.logger import init_logging_config\n",
      "\n",
      "# Set page configuration\n",
      "st.set_page_config(\n",
      "    page_title=\"Resume Matcher\",\n",
      "    page_icon=\"Assets/img/favicon.ico\",\n",
      "    initial_sidebar_state=\"auto\",\n",
      ")\n",
      "\n",
      "init_logging_config()\n",
      "cwd = find_path(\"Resume-Matcher\")\n",
      "config_path = os.path.join(cwd, \"scripts\", \"similarity\")\n",
      "\n",
      "try:\n",
      "    nltk.data.find(\"tokenizers/punkt_tab\")\n",
      "except LookupError:\n",
      "    nltk.download(\"punkt_tab\")\n",
      "\n",
      "parameters.SHOW_LABEL_SEPARATOR = False\n",
      "parameters.BORDER_RADIUS = 3\n",
      "parameters.PADDING = \"0.5 0.25rem\"\n",
      "\n",
      "\n",
      "def create_star_graph(nodes_and_weights, title):\n",
      "    # Create an empty graph\n",
      "    G = nx.Graph()\n",
      "\n",
      "    # Add the central node\n",
      "    central_node = \"resume\"\n",
      "    G.add_node(central_node)\n",
      "\n",
      "    # Add nodes and edges with weights to the graph\n",
      "    for node, weight in nodes_and_weights:\n",
      "        G.add_node(node)\n",
      "        G.add_edge(central_node, node, weight=weight * 100)\n",
      "\n",
      "    # Get position layout for nodes\n",
      "    pos = nx.spring_layout(G)\n",
      "\n",
      "    # Create edge trace\n",
      "    edge_x = []\n",
      "    edge_y = []\n",
      "    for edge in G.edges():\n",
      "        x0, y0 = pos[edge[0]]\n",
      "        x1, y1 = pos[edge[1]]\n",
      "        edge_x.extend([x0, x1, None])\n",
      "        edge_y.extend([y0, y1, None])\n",
      "\n",
      "    edge_trace = go.Scatter(\n",
      "        x=edge_x,\n",
      "        y=edge_y,\n",
      "        line=dict(width=0.5, color=\"#888\"),\n",
      "        hoverinfo=\"none\",\n",
      "        mode=\"lines\",\n",
      "    )\n",
      "\n",
      "    # Create node trace\n",
      "    node_x = []\n",
      "    node_y = []\n",
      "    for node in G.nodes():\n",
      "        x, y = pos[node]\n",
      "        node_x.append(x)\n",
      "        node_y.append(y)\n",
      "\n",
      "    node_trace = go.Scatter(\n",
      "        x=node_x,\n",
      "        y=node_y,\n",
      "        mode=\"markers\",\n",
      "        hoverinfo=\"text\",\n",
      "        marker=dict(\n",
      "            showscale=True,\n",
      "            colorscale=\"Rainbow\",\n",
      "            reversescale=True,\n",
      "            color=[],\n",
      "            size=10,\n",
      "            colorbar=dict(\n",
      "                thickness=15,\n",
      "                title=\"Node Connections\",\n",
      "                xanchor=\"left\",\n",
      "                titleside=\"right\",\n",
      "            ),\n",
      "            line_width=2,\n",
      "        ),\n",
      "    )\n",
      "\n",
      "    # Color node points by number of connections\n",
      "    node_adjacencies = []\n",
      "    node_text = []\n",
      "    for node in G.nodes():\n",
      "        adjacencies = list(G.adj[node])  # changes here\n",
      "        node_adjacencies.append(len(adjacencies))\n",
      "        node_text.append(f\"{node}<br># of connections: {len(adjacencies)}\")\n",
      "\n",
      "    node_trace.marker.color = node_adjacencies\n",
      "    node_trace.text = node_text\n",
      "\n",
      "    # Create the figure\n",
      "    fig = go.Figure(\n",
      "        data=[edge_trace, node_trace],\n",
      "        layout=go.Layout(\n",
      "            title=title,\n",
      "            titlefont_size=16,\n",
      "            showlegend=False,\n",
      "            hovermode=\"closest\",\n",
      "            margin=dict(b=20, l=5, r=5, t=40),\n",
      "            xaxis=dict(showgrid=False, zeroline=False, showticklabels=False),\n",
      "            yaxis=dict(showgrid=False, zeroline=False, showticklabels=False),\n",
      "        ),\n",
      "    )\n",
      "\n",
      "    # Show the figure\n",
      "    st.plotly_chart(fig)\n",
      "\n",
      "\n",
      "def create_annotated_text(\n",
      "    input_string: str, word_list: List[str], annotation: str, color_code: str\n",
      "):\n",
      "    # Tokenize the input string\n",
      "    tokens = nltk.word_tokenize(input_string)\n",
      "\n",
      "    # Convert the list to a set for quick lookups\n",
      "    word_set = set(word_list)\n",
      "\n",
      "    # Initialize an empty list to hold the annotated text\n",
      "    annotated_text = []\n",
      "\n",
      "    for token in tokens:\n",
      "        # Check if the token is in the set\n",
      "        if token in word_set:\n",
      "            # If it is, append a tuple with the token, annotation, and color code\n",
      "            annotated_text.append((token, annotation, color_code))\n",
      "        else:\n",
      "            # If it's not, just append the token as a string\n",
      "            annotated_text.append(token)\n",
      "\n",
      "    return annotated_text\n",
      "\n",
      "\n",
      "def read_json(filename):\n",
      "    with open(filename) as f:\n",
      "        data = json.load(f)\n",
      "    return data\n",
      "\n",
      "\n",
      "def tokenize_string(input_string):\n",
      "    tokens = nltk.word_tokenize(input_string)\n",
      "    return tokens\n",
      "\n",
      "\n",
      "# Display the main title and subheaders\n",
      "st.title(\":blue[Resume Matcher]\")\n",
      "with st.sidebar:\n",
      "    st.image(\"Assets/img/header_image.png\")\n",
      "    st.subheader(\n",
      "        \"Free and Open Source ATS to help your resume pass the screening stage.\"\n",
      "    )\n",
      "    st.markdown(\n",
      "        \"Check the website [www.resumematcher.fyi](https://www.resumematcher.fyi/)\"\n",
      "    )\n",
      "\n",
      "    st.markdown(\n",
      "        \"Give Resume Matcher a  on [GitHub](https://github.com/srbhr/resume-matcher)\"\n",
      "    )\n",
      "\n",
      "    badge(type=\"github\", name=\"srbhr/Resume-Matcher\")\n",
      "    st.markdown(\"For updates follow me on Twitter.\")\n",
      "    badge(type=\"twitter\", name=\"_srbhr_\")\n",
      "    st.markdown(\n",
      "        \"If you like the project and would like to further help in development please consider \"\n",
      "    )\n",
      "    badge(type=\"buymeacoffee\", name=\"srbhr\")\n",
      "\n",
      "st.divider()\n",
      "avs.add_vertical_space(1)\n",
      "\n",
      "resume_names = get_filenames_from_dir(\"Data/Processed/Resumes\")\n",
      "\n",
      "\n",
      "st.markdown(\n",
      "    f\"##### There are {len(resume_names)} resumes present. Please select one from the menu below:\"\n",
      ")\n",
      "output = st.selectbox(f\"\", resume_names)\n",
      "\n",
      "\n",
      "avs.add_vertical_space(5)\n",
      "\n",
      "# st.write(\"You have selected \", output, \" printing the resume\")\n",
      "selected_file = read_json(\"Data/Processed/Resumes/\" + output)\n",
      "\n",
      "avs.add_vertical_space(2)\n",
      "st.markdown(\"#### Parsed Resume Data\")\n",
      "st.caption(\n",
      "    \"This text is parsed from your resume. This is how it'll look like after getting parsed by an ATS.\"\n",
      ")\n",
      "st.caption(\"Utilize this to understand how to make your resume ATS friendly.\")\n",
      "avs.add_vertical_space(3)\n",
      "# st.json(selected_file)\n",
      "st.write(selected_file[\"clean_data\"])\n",
      "\n",
      "avs.add_vertical_space(3)\n",
      "st.write(\"Now let's take a look at the extracted keywords from the resume.\")\n",
      "\n",
      "annotated_text(\n",
      "    create_annotated_text(\n",
      "        selected_file[\"clean_data\"],\n",
      "        selected_file[\"extracted_keywords\"],\n",
      "        \"KW\",\n",
      "        \"#0B666A\",\n",
      "    )\n",
      ")\n",
      "\n",
      "avs.add_vertical_space(5)\n",
      "st.write(\"Now let's take a look at the extracted entities from the resume.\")\n",
      "\n",
      "# Call the function with your data\n",
      "create_star_graph(selected_file[\"keyterms\"], \"Entities from Resume\")\n",
      "\n",
      "df2 = pd.DataFrame(selected_file[\"keyterms\"], columns=[\"keyword\", \"value\"])\n",
      "\n",
      "# Create the dictionary\n",
      "keyword_dict = {}\n",
      "for keyword, value in selected_file[\"keyterms\"]:\n",
      "    keyword_dict[keyword] = value * 100\n",
      "\n",
      "fig = go.Figure(\n",
      "    data=[\n",
      "        go.Table(\n",
      "            header=dict(\n",
      "                values=[\"Keyword\", \"Value\"], font=dict(size=12), fill_color=\"#070A52\"\n",
      "            ),\n",
      "            cells=dict(\n",
      "                values=[list(keyword_dict.keys()), list(keyword_dict.values())],\n",
      "                line_color=\"darkslategray\",\n",
      "                fill_color=\"#6DA9E4\",\n",
      "            ),\n",
      "        )\n",
      "    ]\n",
      ")\n",
      "st.plotly_chart(fig)\n",
      "\n",
      "st.divider()\n",
      "\n",
      "fig = px.treemap(\n",
      "    df2,\n",
      "    path=[\"keyword\"],\n",
      "    values=\"value\",\n",
      "    color_continuous_scale=\"Rainbow\",\n",
      "    title=\"Key Terms/Topics Extracted from your Resume\",\n",
      ")\n",
      "st.write(fig)\n",
      "\n",
      "avs.add_vertical_space(5)\n",
      "\n",
      "job_descriptions = get_filenames_from_dir(\"Data/Processed/JobDescription\")\n",
      "\n",
      "\n",
      "st.markdown(\n",
      "    f\"##### There are {len(job_descriptions)} job descriptions present. Please select one from the menu below:\"\n",
      ")\n",
      "output = st.selectbox(\"\", job_descriptions)\n",
      "\n",
      "\n",
      "avs.add_vertical_space(5)\n",
      "\n",
      "selected_jd = read_json(\"Data/Processed/JobDescription/\" + output)\n",
      "\n",
      "avs.add_vertical_space(2)\n",
      "st.markdown(\"#### Job Description\")\n",
      "st.caption(\n",
      "    \"Currently in the pipeline I'm parsing this from PDF but it'll be from txt or copy paste.\"\n",
      ")\n",
      "avs.add_vertical_space(3)\n",
      "# st.json(selected_file)\n",
      "st.write(selected_jd[\"clean_data\"])\n",
      "\n",
      "st.markdown(\"#### Common Words between Job Description and Resumes Highlighted.\")\n",
      "\n",
      "annotated_text(\n",
      "    create_annotated_text(\n",
      "        selected_file[\"clean_data\"], selected_jd[\"extracted_keywords\"], \"JD\", \"#F24C3D\"\n",
      "    )\n",
      ")\n",
      "\n",
      "st.write(\"Now let's take a look at the extracted entities from the job description.\")\n",
      "\n",
      "# Call the function with your data\n",
      "create_star_graph(selected_jd[\"keyterms\"], \"Entities from Job Description\")\n",
      "\n",
      "df2 = pd.DataFrame(selected_jd[\"keyterms\"], columns=[\"keyword\", \"value\"])\n",
      "\n",
      "# Create the dictionary\n",
      "keyword_dict = {}\n",
      "for keyword, value in selected_jd[\"keyterms\"]:\n",
      "    keyword_dict[keyword] = value * 100\n",
      "\n",
      "fig = go.Figure(\n",
      "    data=[\n",
      "        go.Table(\n",
      "            header=dict(\n",
      "                values=[\"Keyword\", \"Value\"], font=dict(size=12), fill_color=\"#070A52\"\n",
      "            ),\n",
      "            cells=dict(\n",
      "                values=[list(keyword_dict.keys()), list(keyword_dict.values())],\n",
      "                line_color=\"darkslategray\",\n",
      "                fill_color=\"#6DA9E4\",\n",
      "            ),\n",
      "        )\n",
      "    ]\n",
      ")\n",
      "st.plotly_chart(fig)\n",
      "\n",
      "st.divider()\n",
      "\n",
      "fig = px.treemap(\n",
      "    df2,\n",
      "    path=[\"keyword\"],\n",
      "    values=\"value\",\n",
      "    color_continuous_scale=\"Rainbow\",\n",
      "    title=\"Key Terms/Topics Extracted from the selected Job Description\",\n",
      ")\n",
      "st.write(fig)\n",
      "\n",
      "avs.add_vertical_space(3)\n",
      "\n",
      "resume_string = \" \".join(selected_file[\"extracted_keywords\"])\n",
      "jd_string = \" \".join(selected_jd[\"extracted_keywords\"])\n",
      "result = get_score(resume_string, jd_string)\n",
      "similarity_score = round(result[0].score * 100, 2)\n",
      "score_color = \"green\"\n",
      "if similarity_score < 60:\n",
      "    score_color = \"red\"\n",
      "elif 60 <= similarity_score < 75:\n",
      "    score_color = \"orange\"\n",
      "st.markdown(\n",
      "    f\"Similarity Score obtained for the resume and job description is \"\n",
      "    f'<span style=\"color:{score_color};font-size:24px; font-weight:Bold\">{similarity_score}</span>',\n",
      "    unsafe_allow_html=True,\n",
      ")\n",
      "\n",
      "# Go back to top\n",
      "st.markdown(\"[:arrow_up: Back to Top](#resume-matcher)\")\n",
      "\n",
      "# Import necessary libraries\n",
      "import json\n",
      "import os\n",
      "from typing import List\n",
      "\n",
      "import networkx as nx\n",
      "import nltk\n",
      "import pandas as pd\n",
      "import plotly.express as px\n",
      "import plotly.graph_objects as go\n",
      "import streamlit as st\n",
      "from annotated_text import annotated_text, parameters\n",
      "from streamlit_extras import add_vertical_space as avs\n",
      "from streamlit_extras.badges import badge\n",
      "\n",
      "from scripts import JobDescriptionProcessor, ResumeProcessor\n",
      "from scripts.parsers import ParseJobDesc, ParseResume\n",
      "from scripts.ReadPdf import read_single_pdf\n",
      "from scripts.similarity.get_score import *\n",
      "from scripts.utils import get_filenames_from_dir\n",
      "\n",
      "# Set page configuration\n",
      "st.set_page_config(\n",
      "    page_title=\"Resume Matcher\",\n",
      "    page_icon=\"Assets/img/favicon.ico\",\n",
      "    initial_sidebar_state=\"auto\",\n",
      "    layout=\"wide\",\n",
      ")\n",
      "\n",
      "# Find the current working directory and configuration path\n",
      "cwd = find_path(\"Resume-Matcher\")\n",
      "config_path = os.path.join(cwd, \"scripts\", \"similarity\")\n",
      "\n",
      "# Check if NLTK punkt_tab data is available, if not, download it\n",
      "try:\n",
      "    nltk.data.find(\"tokenizers/punkt_tab\")\n",
      "except LookupError:\n",
      "    nltk.download(\"punkt_tab\")\n",
      "\n",
      "# Set some visualization parameters using the annotated_text library\n",
      "parameters.SHOW_LABEL_SEPARATOR = False\n",
      "parameters.BORDER_RADIUS = 3\n",
      "parameters.PADDING = \"0.5 0.25rem\"\n",
      "\n",
      "\n",
      "# Function to set session state variables\n",
      "def update_session_state(key, val):\n",
      "    st.session_state[key] = val\n",
      "\n",
      "\n",
      "# Function to delete all files in a directory\n",
      "def delete_from_dir(filepath: str) -> bool:\n",
      "    try:\n",
      "        for file in os.scandir(filepath):\n",
      "            os.remove(file.path)\n",
      "\n",
      "        return True\n",
      "    except OSError as error:\n",
      "        print(f\"Exception: {error}\")\n",
      "        return False\n",
      "\n",
      "\n",
      "# Function to create a star-shaped graph visualization\n",
      "def create_star_graph(nodes_and_weights, title):\n",
      "    \"\"\"\n",
      "    Create a star-shaped graph visualization.\n",
      "\n",
      "    Args:\n",
      "        nodes_and_weights (list): List of tuples containing nodes and their weights.\n",
      "        title (str): Title for the graph.\n",
      "\n",
      "    Returns:\n",
      "        None\n",
      "    \"\"\"\n",
      "    # Create an empty graph\n",
      "    graph = nx.Graph()\n",
      "\n",
      "    # Add the central node\n",
      "    central_node = \"resume\"\n",
      "    graph.add_node(central_node)\n",
      "\n",
      "    # Add nodes and edges with weights to the graph\n",
      "    for node, weight in nodes_and_weights:\n",
      "        graph.add_node(node)\n",
      "        graph.add_edge(central_node, node, weight=weight * 100)\n",
      "\n",
      "    # Get position layout for nodes\n",
      "    pos = nx.spring_layout(graph)\n",
      "\n",
      "    # Create edge trace\n",
      "    edge_x = []\n",
      "    edge_y = []\n",
      "    for edge in graph.edges():\n",
      "        x0, y0 = pos[edge[0]]\n",
      "        x1, y1 = pos[edge[1]]\n",
      "        edge_x.extend([x0, x1, None])\n",
      "        edge_y.extend([y0, y1, None])\n",
      "\n",
      "    edge_trace = go.Scatter(\n",
      "        x=edge_x,\n",
      "        y=edge_y,\n",
      "        line=dict(width=0.5, color=\"#888\"),\n",
      "        hoverinfo=\"none\",\n",
      "        mode=\"lines\",\n",
      "    )\n",
      "\n",
      "    # Create node trace\n",
      "    node_x = []\n",
      "    node_y = []\n",
      "    for node in graph.nodes():\n",
      "        x, y = pos[node]\n",
      "        node_x.append(x)\n",
      "        node_y.append(y)\n",
      "\n",
      "    node_trace = go.Scatter(\n",
      "        x=node_x,\n",
      "        y=node_y,\n",
      "        mode=\"markers\",\n",
      "        hoverinfo=\"text\",\n",
      "        marker=dict(\n",
      "            showscale=True,\n",
      "            colorscale=\"Rainbow\",\n",
      "            reversescale=True,\n",
      "            color=[],\n",
      "            size=10,\n",
      "            colorbar=dict(\n",
      "                thickness=15,\n",
      "                title=\"Node Connections\",\n",
      "                xanchor=\"left\",\n",
      "                titleside=\"right\",\n",
      "            ),\n",
      "            line_width=2,\n",
      "        ),\n",
      "    )\n",
      "\n",
      "    # Color node points by number of connections\n",
      "    node_adjacencies = []\n",
      "    node_text = []\n",
      "    for node in graph.nodes():\n",
      "        adjacencies = list(graph.adj[node])  # Changes here\n",
      "        node_adjacencies.append(len(adjacencies))\n",
      "        node_text.append(f\"{node}<br># of connections: {len(adjacencies)}\")\n",
      "\n",
      "    node_trace.marker.color = node_adjacencies\n",
      "    node_trace.text = node_text\n",
      "\n",
      "    # Create the figure\n",
      "    figure = go.Figure(\n",
      "        data=[edge_trace, node_trace],\n",
      "        layout=go.Layout(\n",
      "            title=title,\n",
      "            titlefont=dict(size=16),\n",
      "            showlegend=False,\n",
      "            hovermode=\"closest\",\n",
      "            margin=dict(b=20, l=5, r=5, t=40),\n",
      "            xaxis=dict(showgrid=False, zeroline=False, showticklabels=False),\n",
      "            yaxis=dict(showgrid=False, zeroline=False, showticklabels=False),\n",
      "        ),\n",
      "    )\n",
      "\n",
      "    # Show the figure\n",
      "    st.plotly_chart(figure, use_container_width=True)\n",
      "\n",
      "\n",
      "# Function to create annotated text with highlighting\n",
      "def create_annotated_text(\n",
      "    input_string: str, word_list: List[str], annotation: str, color_code: str\n",
      "):\n",
      "    \"\"\"\n",
      "    Create annotated text with highlighted keywords.\n",
      "\n",
      "    Args:\n",
      "        input_string (str): The input text.\n",
      "        word_list (List[str]): List of keywords to be highlighted.\n",
      "        annotation (str): Annotation label for highlighted keywords.\n",
      "        color_code (str): Color code for highlighting.\n",
      "\n",
      "    Returns:\n",
      "        List: Annotated text with highlighted keywords.\n",
      "    \"\"\"\n",
      "    # Tokenize the input string\n",
      "    tokens = nltk.word_tokenize(input_string)\n",
      "\n",
      "    # Convert the list to a set for quick lookups\n",
      "    word_set = set(word_list)\n",
      "\n",
      "    # Initialize an empty list to hold the annotated text\n",
      "    ret_annotated_text = []\n",
      "\n",
      "    for token in tokens:\n",
      "        # Check if the token is in the set\n",
      "        if token in word_set:\n",
      "            # If it is, append a tuple with the token, annotation, and color code\n",
      "            ret_annotated_text.append((token, annotation, color_code))\n",
      "        else:\n",
      "            # If it's not, just append the token as a string\n",
      "            ret_annotated_text.append(token)\n",
      "\n",
      "    return ret_annotated_text\n",
      "\n",
      "\n",
      "# Function to read JSON data from a file\n",
      "def read_json(filename):\n",
      "    \"\"\"\n",
      "    Read JSON data from a file.\n",
      "\n",
      "    Args:\n",
      "        filename (str): The path to the JSON file.\n",
      "\n",
      "    Returns:\n",
      "        dict: The JSON data.\n",
      "    \"\"\"\n",
      "    with open(filename) as f:\n",
      "        data = json.load(f)\n",
      "    return data\n",
      "\n",
      "\n",
      "# Function to tokenize a string\n",
      "def tokenize_string(input_string):\n",
      "    \"\"\"\n",
      "    Tokenize a string into words.\n",
      "\n",
      "    Args:\n",
      "        input_string (str): The input string.\n",
      "\n",
      "    Returns:\n",
      "        List[str]: List of tokens.\n",
      "    \"\"\"\n",
      "    tokens = nltk.word_tokenize(input_string)\n",
      "    return tokens\n",
      "\n",
      "\n",
      "# Cleanup processed resume / job descriptions\n",
      "delete_from_dir(os.path.join(cwd, \"Data\", \"Processed\", \"Resumes\"))\n",
      "delete_from_dir(os.path.join(cwd, \"Data\", \"Processed\", \"JobDescription\"))\n",
      "\n",
      "# Set default session states for first run\n",
      "if \"resumeUploaded\" not in st.session_state.keys():\n",
      "    update_session_state(\"resumeUploaded\", \"Pending\")\n",
      "    update_session_state(\"resumePath\", \"\")\n",
      "if \"jobDescriptionUploaded\" not in st.session_state.keys():\n",
      "    update_session_state(\"jobDescriptionUploaded\", \"Pending\")\n",
      "    update_session_state(\"jobDescriptionPath\", \"\")\n",
      "\n",
      "# Display the main title and sub-headers\n",
      "st.title(\":blue[Resume Matcher]\")\n",
      "with st.sidebar:\n",
      "    st.image(\"Assets/img/header_image.png\")\n",
      "    st.subheader(\n",
      "        \"Free and Open Source ATS to help your resume pass the screening stage.\"\n",
      "    )\n",
      "    st.markdown(\n",
      "        \"Check the website [www.resumematcher.fyi](https://www.resumematcher.fyi/)\"\n",
      "    )\n",
      "    st.markdown(\n",
      "        \"Give Resume Matcher a  on [GitHub](https://github.com/srbhr/resume-matcher)\"\n",
      "    )\n",
      "    badge(type=\"github\", name=\"srbhr/Resume-Matcher\")\n",
      "    st.markdown(\"For updates follow me on Twitter.\")\n",
      "    badge(type=\"twitter\", name=\"_srbhr_\")\n",
      "    st.markdown(\n",
      "        \"If you like the project and would like to further help in development please consider \"\n",
      "    )\n",
      "    badge(type=\"buymeacoffee\", name=\"srbhr\")\n",
      "\n",
      "st.divider()\n",
      "avs.add_vertical_space(1)\n",
      "\n",
      "with st.container():\n",
      "    resumeCol, jobDescriptionCol = st.columns(2)\n",
      "    with resumeCol:\n",
      "        uploaded_Resume = st.file_uploader(\"Choose a Resume\", type=\"pdf\")\n",
      "        if uploaded_Resume is not None:\n",
      "            if st.session_state[\"resumeUploaded\"] == \"Pending\":\n",
      "                save_path_resume = os.path.join(\n",
      "                    cwd, \"Data\", \"Resumes\", uploaded_Resume.name\n",
      "                )\n",
      "\n",
      "                with open(save_path_resume, mode=\"wb\") as w:\n",
      "                    w.write(uploaded_Resume.getvalue())\n",
      "\n",
      "                if os.path.exists(save_path_resume):\n",
      "                    st.toast(\n",
      "                        f\"File {uploaded_Resume.name} is successfully saved!\", icon=\"\"\n",
      "                    )\n",
      "                    update_session_state(\"resumeUploaded\", \"Uploaded\")\n",
      "                    update_session_state(\"resumePath\", save_path_resume)\n",
      "        else:\n",
      "            update_session_state(\"resumeUploaded\", \"Pending\")\n",
      "            update_session_state(\"resumePath\", \"\")\n",
      "\n",
      "    with jobDescriptionCol:\n",
      "        uploaded_JobDescription = st.file_uploader(\n",
      "            \"Choose a Job Description\", type=\"pdf\"\n",
      "        )\n",
      "        if uploaded_JobDescription is not None:\n",
      "            if st.session_state[\"jobDescriptionUploaded\"] == \"Pending\":\n",
      "                save_path_jobDescription = os.path.join(\n",
      "                    cwd, \"Data\", \"JobDescription\", uploaded_JobDescription.name\n",
      "                )\n",
      "\n",
      "                with open(save_path_jobDescription, mode=\"wb\") as w:\n",
      "                    w.write(uploaded_JobDescription.getvalue())\n",
      "\n",
      "                if os.path.exists(save_path_jobDescription):\n",
      "                    st.toast(\n",
      "                        f\"File {uploaded_JobDescription.name} is successfully saved!\",\n",
      "                        icon=\"\",\n",
      "                    )\n",
      "                    update_session_state(\"jobDescriptionUploaded\", \"Uploaded\")\n",
      "                    update_session_state(\"jobDescriptionPath\", save_path_jobDescription)\n",
      "        else:\n",
      "            update_session_state(\"jobDescriptionUploaded\", \"Pending\")\n",
      "            update_session_state(\"jobDescriptionPath\", \"\")\n",
      "\n",
      "with st.spinner(\"Please wait...\"):\n",
      "    if (\n",
      "        uploaded_Resume is not None\n",
      "        and st.session_state[\"jobDescriptionUploaded\"] == \"Uploaded\"\n",
      "        and uploaded_JobDescription is not None\n",
      "        and st.session_state[\"jobDescriptionUploaded\"] == \"Uploaded\"\n",
      "    ):\n",
      "\n",
      "        resumeProcessor = ParseResume(read_single_pdf(st.session_state[\"resumePath\"]))\n",
      "        jobDescriptionProcessor = ParseJobDesc(\n",
      "            read_single_pdf(st.session_state[\"jobDescriptionPath\"])\n",
      "        )\n",
      "\n",
      "        # Resume / JD output\n",
      "        selected_file = resumeProcessor.get_JSON()\n",
      "        selected_jd = jobDescriptionProcessor.get_JSON()\n",
      "\n",
      "        # Add containers for each row to avoid overlap\n",
      "\n",
      "        # Parsed data\n",
      "        with st.container():\n",
      "            resumeCol, jobDescriptionCol = st.columns(2)\n",
      "            with resumeCol:\n",
      "                with st.expander(\"Parsed Resume Data\"):\n",
      "                    st.caption(\n",
      "                        \"This text is parsed from your resume. This is how it'll look like after getting parsed by an \"\n",
      "                        \"ATS.\"\n",
      "                    )\n",
      "                    st.caption(\n",
      "                        \"Utilize this to understand how to make your resume ATS friendly.\"\n",
      "                    )\n",
      "                    avs.add_vertical_space(3)\n",
      "                    st.write(selected_file[\"clean_data\"])\n",
      "\n",
      "            with jobDescriptionCol:\n",
      "                with st.expander(\"Parsed Job Description\"):\n",
      "                    st.caption(\n",
      "                        \"Currently in the pipeline I'm parsing this from PDF but it'll be from txt or copy paste.\"\n",
      "                    )\n",
      "                    avs.add_vertical_space(3)\n",
      "                    st.write(selected_jd[\"clean_data\"])\n",
      "\n",
      "        # Extracted keywords\n",
      "        with st.container():\n",
      "            resumeCol, jobDescriptionCol = st.columns(2)\n",
      "            with resumeCol:\n",
      "                with st.expander(\"Extracted Keywords\"):\n",
      "                    st.write(\n",
      "                        \"Now let's take a look at the extracted keywords from the resume.\"\n",
      "                    )\n",
      "                    annotated_text(\n",
      "                        create_annotated_text(\n",
      "                            selected_file[\"clean_data\"],\n",
      "                            selected_file[\"extracted_keywords\"],\n",
      "                            \"KW\",\n",
      "                            \"#0B666A\",\n",
      "                        )\n",
      "                    )\n",
      "            with jobDescriptionCol:\n",
      "                with st.expander(\"Extracted Keywords\"):\n",
      "                    st.write(\n",
      "                        \"Now let's take a look at the extracted keywords from the job description.\"\n",
      "                    )\n",
      "                    annotated_text(\n",
      "                        create_annotated_text(\n",
      "                            selected_jd[\"clean_data\"],\n",
      "                            selected_jd[\"extracted_keywords\"],\n",
      "                            \"KW\",\n",
      "                            \"#0B666A\",\n",
      "                        )\n",
      "                    )\n",
      "\n",
      "        # Star graph visualization\n",
      "        with st.container():\n",
      "            resumeCol, jobDescriptionCol = st.columns(2)\n",
      "            with resumeCol:\n",
      "                with st.expander(\"Extracted Entities\"):\n",
      "                    st.write(\n",
      "                        \"Now let's take a look at the extracted entities from the resume.\"\n",
      "                    )\n",
      "\n",
      "                    # Call the function with your data\n",
      "                    create_star_graph(selected_file[\"keyterms\"], \"Entities from Resume\")\n",
      "            with jobDescriptionCol:\n",
      "                with st.expander(\"Extracted Entities\"):\n",
      "                    st.write(\n",
      "                        \"Now let's take a look at the extracted entities from the job description.\"\n",
      "                    )\n",
      "\n",
      "                    # Call the function with your data\n",
      "                    create_star_graph(\n",
      "                        selected_jd[\"keyterms\"], \"Entities from Job Description\"\n",
      "                    )\n",
      "\n",
      "        # Keywords and values\n",
      "        with st.container():\n",
      "            resumeCol, jobDescriptionCol = st.columns(2)\n",
      "            with resumeCol:\n",
      "                with st.expander(\"Keywords & Values\"):\n",
      "                    df1 = pd.DataFrame(\n",
      "                        selected_file[\"keyterms\"], columns=[\"keyword\", \"value\"]\n",
      "                    )\n",
      "\n",
      "                    # Create the dictionary\n",
      "                    keyword_dict = {}\n",
      "                    for keyword, value in selected_file[\"keyterms\"]:\n",
      "                        keyword_dict[keyword] = value * 100\n",
      "\n",
      "                    fig = go.Figure(\n",
      "                        data=[\n",
      "                            go.Table(\n",
      "                                header=dict(\n",
      "                                    values=[\"Keyword\", \"Value\"],\n",
      "                                    font=dict(size=12, color=\"white\"),\n",
      "                                    fill_color=\"#1d2078\",\n",
      "                                ),\n",
      "                                cells=dict(\n",
      "                                    values=[\n",
      "                                        list(keyword_dict.keys()),\n",
      "                                        list(keyword_dict.values()),\n",
      "                                    ],\n",
      "                                    line_color=\"darkslategray\",\n",
      "                                    fill_color=\"#6DA9E4\",\n",
      "                                ),\n",
      "                            )\n",
      "                        ]\n",
      "                    )\n",
      "                    st.plotly_chart(fig, use_container_width=True)\n",
      "            with jobDescriptionCol:\n",
      "                with st.expander(\"Keywords & Values\"):\n",
      "                    df2 = pd.DataFrame(\n",
      "                        selected_jd[\"keyterms\"], columns=[\"keyword\", \"value\"]\n",
      "                    )\n",
      "\n",
      "                    # Create the dictionary\n",
      "                    keyword_dict = {}\n",
      "                    for keyword, value in selected_jd[\"keyterms\"]:\n",
      "                        keyword_dict[keyword] = value * 100\n",
      "\n",
      "                    fig = go.Figure(\n",
      "                        data=[\n",
      "                            go.Table(\n",
      "                                header=dict(\n",
      "                                    values=[\"Keyword\", \"Value\"],\n",
      "                                    font=dict(size=12, color=\"white\"),\n",
      "                                    fill_color=\"#1d2078\",\n",
      "                                ),\n",
      "                                cells=dict(\n",
      "                                    values=[\n",
      "                                        list(keyword_dict.keys()),\n",
      "                                        list(keyword_dict.values()),\n",
      "                                    ],\n",
      "                                    line_color=\"darkslategray\",\n",
      "                                    fill_color=\"#6DA9E4\",\n",
      "                                ),\n",
      "                            )\n",
      "                        ]\n",
      "                    )\n",
      "                    st.plotly_chart(fig, use_container_width=True)\n",
      "\n",
      "        # Treemaps\n",
      "        with st.container():\n",
      "            resumeCol, jobDescriptionCol = st.columns(2)\n",
      "            with resumeCol:\n",
      "                with st.expander(\"Key Topics\"):\n",
      "                    fig = px.treemap(\n",
      "                        df1,\n",
      "                        path=[\"keyword\"],\n",
      "                        values=\"value\",\n",
      "                        color_continuous_scale=\"Rainbow\",\n",
      "                        title=\"Key Terms/Topics Extracted from your Resume\",\n",
      "                    )\n",
      "                    st.plotly_chart(fig, use_container_width=True)\n",
      "\n",
      "            with jobDescriptionCol:\n",
      "                with st.expander(\"Key Topics\"):\n",
      "                    fig = px.treemap(\n",
      "                        df2,\n",
      "                        path=[\"keyword\"],\n",
      "                        values=\"value\",\n",
      "                        color_continuous_scale=\"Rainbow\",\n",
      "                        title=\"Key Terms/Topics Extracted from Job Description\",\n",
      "                    )\n",
      "                    st.plotly_chart(fig, use_container_width=True)\n",
      "\n",
      "        avs.add_vertical_space(2)\n",
      "        st.markdown(\"#### Similarity Score\")\n",
      "        print(\"Config file parsed successfully:\")\n",
      "        resume_string = \" \".join(selected_file[\"extracted_keywords\"])\n",
      "        jd_string = \" \".join(selected_jd[\"extracted_keywords\"])\n",
      "        result = get_score(resume_string, jd_string)\n",
      "        similarity_score = round(result[0].score * 100, 2)\n",
      "\n",
      "        # Default color to green\n",
      "        score_color = \"green\"\n",
      "        if similarity_score < 60:\n",
      "            score_color = \"red\"\n",
      "        elif 60 <= similarity_score < 75:\n",
      "            score_color = \"orange\"\n",
      "\n",
      "        st.markdown(\n",
      "            f\"Similarity Score obtained for the resume and job description is \"\n",
      "            f'<span style=\"color:{score_color};font-size:24px; font-weight:Bold\">{similarity_score}</span>',\n",
      "            unsafe_allow_html=True,\n",
      "        )\n",
      "\n",
      "        avs.add_vertical_space(2)\n",
      "        with st.expander(\"Common words between Resume and Job Description:\"):\n",
      "            annotated_text(\n",
      "                create_annotated_text(\n",
      "                    selected_file[\"clean_data\"],\n",
      "                    selected_jd[\"extracted_keywords\"],\n",
      "                    \"JD\",\n",
      "                    \"#F24C3D\",\n",
      "                )\n",
      "            )\n",
      "\n",
      "st.divider()\n",
      "\n",
      "# Go back to top\n",
      "st.markdown(\"[:arrow_up: Back to Top](#resume-matcher)\")\n",
      "\n",
      "import json\n",
      "from typing import List\n",
      "\n",
      "import networkx as nx\n",
      "import nltk\n",
      "import pandas as pd\n",
      "import plotly.express as px\n",
      "import plotly.graph_objects as go\n",
      "import streamlit as st\n",
      "from annotated_text import annotated_text, parameters\n",
      "from streamlit_extras import add_vertical_space as avs\n",
      "from streamlit_extras.badges import badge\n",
      "\n",
      "from scripts.utils import get_filenames_from_dir\n",
      "\n",
      "# Set page configuration\n",
      "st.set_page_config(\n",
      "    page_title=\"Resume Matcher\",\n",
      "    page_icon=\"Assets/img/favicon.ico\",\n",
      "    initial_sidebar_state=\"auto\",\n",
      ")\n",
      "\n",
      "# Check if NLTK punkt_tab data is available, if not, download it\n",
      "try:\n",
      "    nltk.data.find(\"tokenizers/punkt_tab\")\n",
      "except LookupError:\n",
      "    nltk.download(\"punkt_tab\")\n",
      "\n",
      "parameters.SHOW_LABEL_SEPARATOR = False\n",
      "parameters.BORDER_RADIUS = 3\n",
      "parameters.PADDING = \"0.5 0.25rem\"\n",
      "\n",
      "\n",
      "def create_star_graph(nodes_and_weights, title):\n",
      "    # Create an empty graph\n",
      "    G = nx.Graph()\n",
      "\n",
      "    # Add the central node\n",
      "    central_node = \"resume\"\n",
      "    G.add_node(central_node)\n",
      "\n",
      "    # Add nodes and edges with weights to the graph\n",
      "    for node, weight in nodes_and_weights:\n",
      "        G.add_node(node)\n",
      "        G.add_edge(central_node, node, weight=weight * 100)\n",
      "\n",
      "    # Get position layout for nodes\n",
      "    pos = nx.spring_layout(G)\n",
      "\n",
      "    # Create edge trace\n",
      "    edge_x = []\n",
      "    edge_y = []\n",
      "    for edge in G.edges():\n",
      "        x0, y0 = pos[edge[0]]\n",
      "        x1, y1 = pos[edge[1]]\n",
      "        edge_x.extend([x0, x1, None])\n",
      "        edge_y.extend([y0, y1, None])\n",
      "\n",
      "    edge_trace = go.Scatter(\n",
      "        x=edge_x,\n",
      "        y=edge_y,\n",
      "        line=dict(width=0.5, color=\"#888\"),\n",
      "        hoverinfo=\"none\",\n",
      "        mode=\"lines\",\n",
      "    )\n",
      "\n",
      "    # Create node trace\n",
      "    node_x = []\n",
      "    node_y = []\n",
      "    for node in G.nodes():\n",
      "        x, y = pos[node]\n",
      "        node_x.append(x)\n",
      "        node_y.append(y)\n",
      "\n",
      "    node_trace = go.Scatter(\n",
      "        x=node_x,\n",
      "        y=node_y,\n",
      "        mode=\"markers\",\n",
      "        hoverinfo=\"text\",\n",
      "        marker=dict(\n",
      "            showscale=True,\n",
      "            colorscale=\"Rainbow\",\n",
      "            reversescale=True,\n",
      "            color=[],\n",
      "            size=10,\n",
      "            colorbar=dict(\n",
      "                thickness=15,\n",
      "                title=\"Node Connections\",\n",
      "                xanchor=\"left\",\n",
      "                titleside=\"right\",\n",
      "            ),\n",
      "            line_width=2,\n",
      "        ),\n",
      "    )\n",
      "\n",
      "    # Color node points by number of connections\n",
      "    node_adjacencies = []\n",
      "    node_text = []\n",
      "    for node in G.nodes():\n",
      "        adjacencies = list(G.adj[node])  # changes here\n",
      "        node_adjacencies.append(len(adjacencies))\n",
      "        node_text.append(f\"{node}<br># of connections: {len(adjacencies)}\")\n",
      "\n",
      "    node_trace.marker.color = node_adjacencies\n",
      "    node_trace.text = node_text\n",
      "\n",
      "    # Create the figure\n",
      "    fig = go.Figure(\n",
      "        data=[edge_trace, node_trace],\n",
      "        layout=go.Layout(\n",
      "            title=title,\n",
      "            titlefont_size=16,\n",
      "            showlegend=False,\n",
      "            hovermode=\"closest\",\n",
      "            margin=dict(b=20, l=5, r=5, t=40),\n",
      "            xaxis=dict(showgrid=False, zeroline=False, showticklabels=False),\n",
      "            yaxis=dict(showgrid=False, zeroline=False, showticklabels=False),\n",
      "        ),\n",
      "    )\n",
      "\n",
      "    # Show the figure\n",
      "    st.plotly_chart(fig)\n",
      "\n",
      "\n",
      "def create_annotated_text(\n",
      "    input_string: str, word_list: List[str], annotation: str, color_code: str\n",
      "):\n",
      "    # Tokenize the input string\n",
      "    tokens = nltk.word_tokenize(input_string)\n",
      "\n",
      "    # Convert the list to a set for quick lookups\n",
      "    word_set = set(word_list)\n",
      "\n",
      "    # Initialize an empty list to hold the annotated text\n",
      "    annotated_text = []\n",
      "\n",
      "    for token in tokens:\n",
      "        # Check if the token is in the set\n",
      "        if token in word_set:\n",
      "            # If it is, append a tuple with the token, annotation, and color code\n",
      "            annotated_text.append((token, annotation, color_code))\n",
      "        else:\n",
      "            # If it's not, just append the token as a string\n",
      "            annotated_text.append(token)\n",
      "\n",
      "    return annotated_text\n",
      "\n",
      "\n",
      "def read_json(filename):\n",
      "    with open(filename) as f:\n",
      "        data = json.load(f)\n",
      "    return data\n",
      "\n",
      "\n",
      "def tokenize_string(input_string):\n",
      "    tokens = nltk.word_tokenize(input_string)\n",
      "    return tokens\n",
      "\n",
      "\n",
      "# Display the main title and subheaders\n",
      "st.title(\":blue[Resume Matcher]\")\n",
      "with st.sidebar:\n",
      "    st.image(\"Assets/img/header_image.png\")\n",
      "    st.subheader(\n",
      "        \"Free and Open Source ATS to help your resume pass the screening stage.\"\n",
      "    )\n",
      "    st.markdown(\n",
      "        \"Check the website [www.resumematcher.fyi](https://www.resumematcher.fyi/)\"\n",
      "    )\n",
      "\n",
      "    st.markdown(\n",
      "        \"Give Resume Matcher a  on [GitHub](https://github.com/srbhr/resume-matcher)\"\n",
      "    )\n",
      "\n",
      "    badge(type=\"github\", name=\"srbhr/Resume-Matcher\")\n",
      "    st.markdown(\"For updates follow me on Twitter.\")\n",
      "    badge(type=\"twitter\", name=\"_srbhr_\")\n",
      "    st.markdown(\n",
      "        \"If you like the project and would like to further help in development please consider \"\n",
      "    )\n",
      "    badge(type=\"buymeacoffee\", name=\"srbhr\")\n",
      "\n",
      "st.divider()\n",
      "avs.add_vertical_space(1)\n",
      "\n",
      "resume_names = get_filenames_from_dir(\"Data/Processed/Resumes\")\n",
      "\n",
      "output = st.selectbox(\n",
      "    f\"There are {len(resume_names)} resumes present. Please select one from the menu below:\",\n",
      "    resume_names,\n",
      ")\n",
      "\n",
      "avs.add_vertical_space(5)\n",
      "\n",
      "selected_file = read_json(\"Data/Processed/Resumes/\" + output)\n",
      "\n",
      "avs.add_vertical_space(2)\n",
      "st.markdown(\"#### Parsed Resume Data\")\n",
      "st.caption(\n",
      "    \"This text is parsed from your resume. This is how it'll look like after getting parsed by an ATS.\"\n",
      ")\n",
      "st.caption(\"Utilize this to understand how to make your resume ATS friendly.\")\n",
      "avs.add_vertical_space(3)\n",
      "# st.json(selected_file)\n",
      "st.write(selected_file[\"clean_data\"])\n",
      "\n",
      "avs.add_vertical_space(3)\n",
      "st.write(\"Now let's take a look at the extracted keywords from the resume.\")\n",
      "\n",
      "annotated_text(\n",
      "    create_annotated_text(\n",
      "        selected_file[\"clean_data\"],\n",
      "        selected_file[\"extracted_keywords\"],\n",
      "        \"KW\",\n",
      "        \"#0B666A\",\n",
      "    )\n",
      ")\n",
      "\n",
      "avs.add_vertical_space(5)\n",
      "st.write(\"Now let's take a look at the extracted entities from the resume.\")\n",
      "\n",
      "# Call the function with your data\n",
      "create_star_graph(selected_file[\"keyterms\"], \"Entities from Resume\")\n",
      "\n",
      "df2 = pd.DataFrame(selected_file[\"keyterms\"], columns=[\"keyword\", \"value\"])\n",
      "\n",
      "# Create the dictionary\n",
      "keyword_dict = {}\n",
      "for keyword, value in selected_file[\"keyterms\"]:\n",
      "    keyword_dict[keyword] = value * 100\n",
      "\n",
      "fig = go.Figure(\n",
      "    data=[\n",
      "        go.Table(\n",
      "            header=dict(\n",
      "                values=[\"Keyword\", \"Value\"], font=dict(size=12), fill_color=\"#070A52\"\n",
      "            ),\n",
      "            cells=dict(\n",
      "                values=[list(keyword_dict.keys()), list(keyword_dict.values())],\n",
      "                line_color=\"darkslategray\",\n",
      "                fill_color=\"#6DA9E4\",\n",
      "            ),\n",
      "        )\n",
      "    ]\n",
      ")\n",
      "st.plotly_chart(fig)\n",
      "\n",
      "st.divider()\n",
      "\n",
      "fig = px.treemap(\n",
      "    df2,\n",
      "    path=[\"keyword\"],\n",
      "    values=\"value\",\n",
      "    color_continuous_scale=\"Rainbow\",\n",
      "    title=\"Key Terms/Topics Extracted from your Resume\",\n",
      ")\n",
      "st.write(fig)\n",
      "\n",
      "avs.add_vertical_space(5)\n",
      "\n",
      "job_descriptions = get_filenames_from_dir(\"Data/Processed/JobDescription\")\n",
      "\n",
      "output = st.selectbox(\n",
      "    f\"There are {len(job_descriptions)} job descriptions present. Please select one from the menu below:\",\n",
      "    job_descriptions,\n",
      ")\n",
      "\n",
      "avs.add_vertical_space(5)\n",
      "\n",
      "selected_jd = read_json(\"Data/Processed/JobDescription/\" + output)\n",
      "\n",
      "avs.add_vertical_space(2)\n",
      "st.markdown(\"#### Job Description\")\n",
      "st.caption(\n",
      "    \"Currently in the pipeline I'm parsing this from PDF but it'll be from txt or copy paste.\"\n",
      ")\n",
      "avs.add_vertical_space(3)\n",
      "# st.json(selected_file)\n",
      "st.write(selected_jd[\"clean_data\"])\n",
      "\n",
      "st.markdown(\"#### Common Words between Job Description and Resumes Highlighted.\")\n",
      "\n",
      "annotated_text(\n",
      "    create_annotated_text(\n",
      "        selected_file[\"clean_data\"], selected_jd[\"extracted_keywords\"], \"JD\", \"#F24C3D\"\n",
      "    )\n",
      ")\n",
      "\n",
      "st.write(\"Now let's take a look at the extracted entities from the job description.\")\n",
      "\n",
      "# Call the function with your data\n",
      "create_star_graph(selected_jd[\"keyterms\"], \"Entities from Job Description\")\n",
      "\n",
      "df2 = pd.DataFrame(selected_jd[\"keyterms\"], columns=[\"keyword\", \"value\"])\n",
      "\n",
      "# Create the dictionary\n",
      "keyword_dict = {}\n",
      "for keyword, value in selected_jd[\"keyterms\"]:\n",
      "    keyword_dict[keyword] = value * 100\n",
      "\n",
      "fig = go.Figure(\n",
      "    data=[\n",
      "        go.Table(\n",
      "            header=dict(\n",
      "                values=[\"Keyword\", \"Value\"], font=dict(size=12), fill_color=\"#070A52\"\n",
      "            ),\n",
      "            cells=dict(\n",
      "                values=[list(keyword_dict.keys()), list(keyword_dict.values())],\n",
      "                line_color=\"darkslategray\",\n",
      "                fill_color=\"#6DA9E4\",\n",
      "            ),\n",
      "        )\n",
      "    ]\n",
      ")\n",
      "st.plotly_chart(fig)\n",
      "\n",
      "st.divider()\n",
      "\n",
      "fig = px.treemap(\n",
      "    df2,\n",
      "    path=[\"keyword\"],\n",
      "    values=\"value\",\n",
      "    color_continuous_scale=\"Rainbow\",\n",
      "    title=\"Key Terms/Topics Extracted from the selected Job Description\",\n",
      ")\n",
      "st.write(fig)\n",
      "\n",
      "avs.add_vertical_space(5)\n",
      "\n",
      "st.divider()\n",
      "\n",
      "st.markdown(\"## Vector Similarity Scores\")\n",
      "st.caption(\"Powered by Qdrant Vector Search\")\n",
      "st.info(\"These are pre-computed queries\", icon=\"\")\n",
      "st.warning(\n",
      "    \"Running Qdrant or Sentence Transformers without having capacity is not recommended\",\n",
      "    icon=\"\",\n",
      ")\n",
      "\n",
      "\n",
      "# Your data\n",
      "data = [\n",
      "    {\n",
      "        \"text\": \"{'resume': 'Alfred Pennyworth\",\n",
      "        \"query\": \"Job Description Product Manager\",\n",
      "        \"score\": 0.62658,\n",
      "    },\n",
      "    {\n",
      "        \"text\": \"{'resume': 'Barry Allen\",\n",
      "        \"query\": \"Job Description Product Manager\",\n",
      "        \"score\": 0.43777737,\n",
      "    },\n",
      "    {\n",
      "        \"text\": \"{'resume': 'Bruce Wayne \",\n",
      "        \"query\": \"Job Description Product Manager\",\n",
      "        \"score\": 0.39835533,\n",
      "    },\n",
      "    {\n",
      "        \"text\": \"{'resume': 'JOHN DOE\",\n",
      "        \"query\": \"Job Description Product Manager\",\n",
      "        \"score\": 0.3915512,\n",
      "    },\n",
      "    {\n",
      "        \"text\": \"{'resume': 'Harvey Dent\",\n",
      "        \"query\": \"Job Description Product Manager\",\n",
      "        \"score\": 0.3519544,\n",
      "    },\n",
      "    {\n",
      "        \"text\": \"{'resume': 'Barry Allen\",\n",
      "        \"query\": \"Job Description Senior Full Stack Engineer\",\n",
      "        \"score\": 0.6541866,\n",
      "    },\n",
      "    {\n",
      "        \"text\": \"{'resume': 'Alfred Pennyworth\",\n",
      "        \"query\": \"Job Description Senior Full Stack Engineer\",\n",
      "        \"score\": 0.59806436,\n",
      "    },\n",
      "    {\n",
      "        \"text\": \"{'resume': 'JOHN DOE\",\n",
      "        \"query\": \"Job Description Senior Full Stack Engineer\",\n",
      "        \"score\": 0.5951386,\n",
      "    },\n",
      "    {\n",
      "        \"text\": \"{'resume': 'Bruce Wayne \",\n",
      "        \"query\": \"Job Description Senior Full Stack Engineer\",\n",
      "        \"score\": 0.57700855,\n",
      "    },\n",
      "    {\n",
      "        \"text\": \"{'resume': 'Harvey Dent\",\n",
      "        \"query\": \"Job Description Senior Full Stack Engineer\",\n",
      "        \"score\": 0.38489106,\n",
      "    },\n",
      "    {\n",
      "        \"text\": \"{'resume': 'Barry Allen\",\n",
      "        \"query\": \"Job Description Front End Engineer\",\n",
      "        \"score\": 0.76813436,\n",
      "    },\n",
      "    {\n",
      "        \"text\": \"{'resume': 'Bruce Wayne'\",\n",
      "        \"query\": \"Job Description Front End Engineer\",\n",
      "        \"score\": 0.60440844,\n",
      "    },\n",
      "    {\n",
      "        \"text\": \"{'resume': 'JOHN DOE\",\n",
      "        \"query\": \"Job Description Front End Engineer\",\n",
      "        \"score\": 0.56080043,\n",
      "    },\n",
      "    {\n",
      "        \"text\": \"{'resume': 'Alfred Pennyworth\",\n",
      "        \"query\": \"Job Description Front End Engineer\",\n",
      "        \"score\": 0.5395049,\n",
      "    },\n",
      "    {\n",
      "        \"text\": \"{'resume': 'Harvey Dent\",\n",
      "        \"query\": \"Job Description Front End Engineer\",\n",
      "        \"score\": 0.3859515,\n",
      "    },\n",
      "    {\n",
      "        \"text\": \"{'resume': 'JOHN DOE\",\n",
      "        \"query\": \"Job Description Java Developer\",\n",
      "        \"score\": 0.5449441,\n",
      "    },\n",
      "    {\n",
      "        \"text\": \"{'resume': 'Alfred Pennyworth\",\n",
      "        \"query\": \"Job Description Java Developer\",\n",
      "        \"score\": 0.53476423,\n",
      "    },\n",
      "    {\n",
      "        \"text\": \"{'resume': 'Barry Allen\",\n",
      "        \"query\": \"Job Description Java Developer\",\n",
      "        \"score\": 0.5313871,\n",
      "    },\n",
      "    {\n",
      "        \"text\": \"{'resume': 'Bruce Wayne \",\n",
      "        \"query\": \"Job Description Java Developer\",\n",
      "        \"score\": 0.44446343,\n",
      "    },\n",
      "    {\n",
      "        \"text\": \"{'resume': 'Harvey Dent\",\n",
      "        \"query\": \"Job Description Java Developer\",\n",
      "        \"score\": 0.3616274,\n",
      "    },\n",
      "]\n",
      "\n",
      "# Create a DataFrame\n",
      "df = pd.DataFrame(data)\n",
      "\n",
      "# Create different DataFrames based on the query and sort by score\n",
      "df1 = df[df[\"query\"] == \"Job Description Product Manager\"].sort_values(\n",
      "    by=\"score\", ascending=False\n",
      ")\n",
      "df2 = df[df[\"query\"] == \"Job Description Senior Full Stack Engineer\"].sort_values(\n",
      "    by=\"score\", ascending=False\n",
      ")\n",
      "df3 = df[df[\"query\"] == \"Job Description Front End Engineer\"].sort_values(\n",
      "    by=\"score\", ascending=False\n",
      ")\n",
      "df4 = df[df[\"query\"] == \"Job Description Java Developer\"].sort_values(\n",
      "    by=\"score\", ascending=False\n",
      ")\n",
      "\n",
      "\n",
      "def plot_df(df, title):\n",
      "    fig = px.bar(df, x=\"text\", y=df[\"score\"] * 100, title=title)\n",
      "    st.plotly_chart(fig)\n",
      "\n",
      "\n",
      "st.markdown(\"### Bar plots of scores based on similarity to Job Description.\")\n",
      "\n",
      "st.subheader(\":blue[Legend]\")\n",
      "st.text(\"Alfred Pennyworth :  Product Manager\")\n",
      "st.text(\"Barry Allen :  Front End Developer\")\n",
      "st.text(\"Harvey Dent :  Machine Learning Engineer\")\n",
      "st.text(\"Bruce Wayne :  Fullstack Developer (MERN)\")\n",
      "st.text(\"John Doe :  Fullstack Developer (Java)\")\n",
      "\n",
      "\n",
      "plot_df(df1, \"Job Description Product Manager 10+ Years of Exper\")\n",
      "plot_df(df2, \"Job Description Senior Full Stack Engineer 5+ Year\")\n",
      "plot_df(df3, \"Job Description Front End Engineer 2 Years of Expe\")\n",
      "plot_df(df4, \"Job Description Java Developer 3 Years of Experien\")\n",
      "\n",
      "\n",
      "avs.add_vertical_space(3)\n",
      "\n",
      "# Go back to top\n",
      "st.markdown(\"[:arrow_up: Back to Top](#resume-matcher)\")\n",
      "\n",
      "# __init__ api\n",
      "# __init__ router\n",
      "import os\n",
      "from fastapi import FastAPI\n",
      "from fastapi.staticfiles import StaticFiles\n",
      "from fastapi.middleware.cors import CORSMiddleware\n",
      "from starlette.middleware.sessions import SessionMiddleware\n",
      "from app.api.v1 import api_router\n",
      "from app.core.config import settings\n",
      "from models.base import Base\n",
      "from core.database import DatabaseConnectionSingleton\n",
      "\n",
      "def create_app() -> FastAPI:\n",
      "    app = FastAPI(\n",
      "        title=settings.PROJECT_NAME,\n",
      "        docs_url=\"/api/docs\",\n",
      "        openapi_url=\"/api/openapi.json\"\n",
      "    )\n",
      "\n",
      "    app.add_middleware(SessionMiddleware, secret_key = settings.SESSION_SECRET_KEY, same_site=\"lax\")\n",
      "    app.add_middleware(\n",
      "        CORSMiddleware,\n",
      "        allow_origins=settings.ALLOWED_ORIGINS,\n",
      "        allow_credentials=True,\n",
      "        allow_methods=[\"*\"],\n",
      "        allow_headers=[\"*\"],\n",
      "    )\n",
      "\n",
      "    db_singleton = DatabaseConnectionSingleton(settings.DATABASE_URL)\n",
      "    Base.metadata.create_all(bind=db_singleton.engine)\n",
      "    \n",
      "    if os.path.exists(settings.FRONTEND_PATH): \n",
      "        app.mount(\"/app\", StaticFiles(directory=settings.FRONTEND_PATH, html=True), name=settings.PROJECT_NAME)\n",
      "\n",
      "    app.include_router(api_router, prefix=\"/api/v1\")\n",
      "\n",
      "    return app\n",
      "\n",
      "import os\n",
      "from pydantic_settings import BaseSettings\n",
      "\n",
      "class Settings(BaseSettings):\n",
      "    PROJECT_NAME=\"Resume Matcher\"\n",
      "    FRONTEND_PATH=os.path.join(os.path.dirname(__file__), \"frontend\", \"assets\")\n",
      "    ALLOWED_ORIGINS=[\"https://www.resume-matcher.fyi\"]\n",
      "    DATABASE_URL: str\n",
      "    SESSION_SECRET_KEY: str\n",
      "\n",
      "    class Config:\n",
      "        env_file = \".env\"\n",
      "\n",
      "settings = Settings()\n",
      "import threading\n",
      "from sqlalchemy import create_engine\n",
      "from sqlalchemy.orm import sessionmaker\n",
      "from typing import Optional\n",
      "\n",
      "\n",
      "class DatabaseConnectionSingleton:\n",
      "    _instance_lock = threading.Lock()\n",
      "    _instance = Optional[\"DatabaseConnectionSingleton\"] = None\n",
      "\n",
      "    def __new__(cls, *args, **kwargs):\n",
      "        if not cls._instance:\n",
      "            with cls._instance_lock:\n",
      "                if not cls._instance:\n",
      "                    cls._instance = super(DatabaseConnectionSingleton, cls).__new__(cls)\n",
      "        return cls._instance\n",
      "    \n",
      "    def __init__(self, db_url: str):\n",
      "        if not hasattr(self, \"engine\"):\n",
      "            self.engine = create_engine(db_url, connect_args={\"check_same_thread\": False})\n",
      "            self.session = sessionmaker(autoflush=False, bind=self.engine)\n",
      "\n",
      "    def get_session(self):\n",
      "        return self.session()\n",
      "import uvicorn\n",
      "from app.base import create_app\n",
      "\n",
      "app = create_app()\n",
      "\n",
      "if __name__ == \"__main__\":\n",
      "    uvicorn.run(\"app.main:app\", host=\"0.0.0.0\", port=8000, reload=True)\n",
      "from base import Base\n",
      "from resume import Resume\n",
      "from user import User\n",
      "from job import Job\n",
      "from association import job_resume_association\n",
      "from .base import Base\n",
      "from sqlalchemy import Column, String, Table, ForeignKey\n",
      "\n",
      "\n",
      "job_resume_association = Table(\n",
      "    \"job_resume\",\n",
      "    Base.metadata,\n",
      "    Column(\"job_id\", String, ForeignKey(\"job_descriptions.job_id\"), primary_key=True),\n",
      "    Column(\"resume_id\", String, ForeignKey(\"resumes.resume_id\"), primary_key = True)\n",
      ")\n",
      "from sqlalchemy.orm import DeclarativeBase\n",
      "\n",
      "class Base(DeclarativeBase):\n",
      "    pass\n",
      "from .base import Base \n",
      "from sqlalchemy import Column, String, Text, Date, Integer, ForeignKey\n",
      "from sqlalchemy.types import JSON\n",
      "from sqlalchemy.orm import relationship\n",
      "from .association import job_resume_association\n",
      "\n",
      "\n",
      "class Job(Base):\n",
      "    __tablename__ = \"jobs\"\n",
      "\n",
      "    job_id = Column(String, primary_key=True, index=True)  # uuid field\n",
      "\n",
      "    job_title = Column(String, nullable=False)\n",
      "    company_profile = Column(Text, nullable=True)\n",
      "    location = Column(String, nullable=True)\n",
      "    date_posted = Column(Date, nullable=True)\n",
      "    employment_type = Column(String, nullable=True)\n",
      "    job_summary = Column(Text, nullable=False)\n",
      "    key_responsibilities = Column(JSON, nullable=True)\n",
      "    qualifications = Column(JSON, nullable=True)\n",
      "    compensation_and_benfits = Column(JSON, nullable=True)\n",
      "    application_info = Column(JSON, nullable=True)\n",
      "    extracted_keywords = Column(JSON, nullable=True)\n",
      "\n",
      "    # one-to-many relation between user and jobs\n",
      "    owner_id = Column(Integer, ForeignKey(\"users.id\"), nullable=False)\n",
      "    owner = relationship(\"User\", back_populates=\"resumes\")\n",
      "\n",
      "    # many-to-many relationship in job and resume\n",
      "    jobs = relationship(\"JobDescription\", secondary=job_resume_association, back_populates=\"jobs\")\n",
      "from .base import Base \n",
      "from sqlalchemy import Column, String, Integer, ForeignKey\n",
      "from sqlalchemy.types import JSON\n",
      "from sqlalchemy.orm import relationship\n",
      "from .association import job_resume_association\n",
      "\n",
      "class Resume(Base):\n",
      "    __tablename__ = \"resumes\"\n",
      "\n",
      "    resume_id = Column(String, primary_key = True, index=True) # uuid field\n",
      "\n",
      "    personal_data = Column(JSON, nullable=False)\n",
      "    experiences = Column(JSON, nullable=True)\n",
      "    projects = Column(JSON, nullable=True)\n",
      "    skills = Column(JSON, nullable=True)\n",
      "    research_work = Column(JSON, nullable=True)\n",
      "    achievements = Column(JSON, nullable=True)\n",
      "    education = Column(JSON, nullable=True)\n",
      "    extracted_keywords = Column(JSON, nullable=True)\n",
      "\n",
      "    owner_id = Column(Integer, ForeignKey(\"users.id\"), nullable=False)\n",
      "    owner = relationship(\"User\", back_populates=\"resumes\")\n",
      "\n",
      "    jobs = relationship(\"JobDescription\", secondary=job_resume_association, back_populates=\"jobs\")\n",
      "from .base import Base \n",
      "from sqlalchemy import Column, String, Integer\n",
      "from sqlalchemy.orm import relationship\n",
      "\n",
      "class User(Base):\n",
      "    __tablename__ = \"users\"\n",
      "\n",
      "    id = Column(Integer, primary_key=True, index=True)\n",
      "\n",
      "    email = Column(String, unique=True, index=True, nullable=False)\n",
      "    name = Column(String, nullable=False)\n",
      "\n",
      "    resumes = relationship(\"Resume\", back_populates=\"owner\")\n",
      "    jobs = relationship(\"Job\", back_populates=\"owner\")\n",
      "from .router.health import health_check\n",
      "from fastapi import APIRouter, status, Depends\n",
      "from sqlalchemy.orm import Session\n",
      "from sqlalchemy import text\n",
      "from app.utils.utils import get_db_session\n",
      "\n",
      "health_check = APIRouter()\n",
      "\n",
      "@health_check.get(\"/ping\", tags=[\"health_check\"], status_code=status.HTTP_200_OK)\n",
      "def ping(db: Session = Depends(get_db_session)):\n",
      "    \"\"\"\n",
      "    health check endpoint\n",
      "    \"\"\"\n",
      "    try:\n",
      "        result = db.execute(text(\"SELECT 1\")).fetchone()\n",
      "        db_status = \"reachable\" if result is not None else \"not reachable\"\n",
      "    except Exception as e:\n",
      "        db_status = f\"error: {str(e)}\"\n",
      "    return {\"message\": \"pong\", \"database\": db_status}\n",
      "import os\n",
      "from fastapi import FastAPI\n",
      "from fastapi.staticfiles import StaticFiles\n",
      "from fastapi.middleware.cors import CORSMiddleware\n",
      "from starlette.middleware.sessions import SessionMiddleware\n",
      "from app.api import health_check\n",
      "from app.core.config import settings\n",
      "from app.models import Base\n",
      "from app.core.database import DatabaseConnectionSingleton\n",
      "\n",
      "def create_app() -> FastAPI:\n",
      "    app = FastAPI(\n",
      "        title=settings.PROJECT_NAME,\n",
      "        docs_url=\"/api/docs\",\n",
      "        openapi_url=\"/api/openapi.json\"\n",
      "    )\n",
      "\n",
      "    app.add_middleware(SessionMiddleware, secret_key = settings.SESSION_SECRET_KEY, same_site=\"lax\")\n",
      "    app.add_middleware(\n",
      "        CORSMiddleware,\n",
      "        allow_origins=settings.ALLOWED_ORIGINS,\n",
      "        allow_credentials=True,\n",
      "        allow_methods=[\"*\"],\n",
      "        allow_headers=[\"*\"],\n",
      "    )\n",
      "\n",
      "    db_singleton = DatabaseConnectionSingleton(settings.DATABASE_URL)\n",
      "    Base.metadata.create_all(bind=db_singleton.engine)\n",
      "    \n",
      "    if os.path.exists(settings.FRONTEND_PATH): \n",
      "        app.mount(\"/app\", StaticFiles(directory=settings.FRONTEND_PATH, html=True), name=settings.PROJECT_NAME)\n",
      "\n",
      "    app.include_router(health_check)\n",
      "\n",
      "    return app\n",
      "\n",
      "import os\n",
      "from pydantic_settings import BaseSettings\n",
      "from typing import List, Optional\n",
      "\n",
      "class Settings(BaseSettings):\n",
      "    PROJECT_NAME: str=\"Resume Matcher\"\n",
      "    FRONTEND_PATH: str=os.path.join(os.path.dirname(__file__), \"frontend\", \"assets\")\n",
      "    ALLOWED_ORIGINS: List[str]=[\"https://www.resume-matcher.fyi\"]\n",
      "    DATABASE_URL: Optional[str]\n",
      "    SESSION_SECRET_KEY: Optional[str]\n",
      "    PYTHONDONTWRITEBYTECODE: int=1\n",
      "\n",
      "    class Config:\n",
      "        env_file = \".env\"\n",
      "\n",
      "settings = Settings()\n",
      "import threading\n",
      "from sqlalchemy import create_engine\n",
      "from sqlalchemy.orm import sessionmaker\n",
      "from typing import Optional\n",
      "\n",
      "\n",
      "class DatabaseConnectionSingleton:\n",
      "    _instance_lock = threading.Lock()\n",
      "    _instance: Optional[\"DatabaseConnectionSingleton\"] = None\n",
      "\n",
      "    def __new__(cls, *args, **kwargs):\n",
      "        if not cls._instance:\n",
      "            with cls._instance_lock:\n",
      "                if not cls._instance:\n",
      "                    cls._instance = super(DatabaseConnectionSingleton, cls).__new__(cls)\n",
      "        return cls._instance\n",
      "    \n",
      "    def __init__(self, db_url: str):\n",
      "        if not hasattr(self, \"engine\"):\n",
      "            self.engine = create_engine(db_url, connect_args={\"check_same_thread\": False})\n",
      "            self.session = sessionmaker(autoflush=False, bind=self.engine)\n",
      "\n",
      "    def get_session(self):\n",
      "        return self.session()\n",
      "from .base import Base\n",
      "from .resume import Resume\n",
      "from .user import User\n",
      "from .job import Job\n",
      "from .association import job_resume_association\n",
      "from .base import Base\n",
      "from sqlalchemy import Column, String, Table, ForeignKey\n",
      "\n",
      "\n",
      "job_resume_association = Table(\n",
      "    \"job_resume\",\n",
      "    Base.metadata,\n",
      "    Column(\"job_id\", String, ForeignKey(\"jobs.job_id\"), primary_key=True),\n",
      "    Column(\"resume_id\", String, ForeignKey(\"resumes.resume_id\"), primary_key = True)\n",
      ")\n",
      "from .base import Base \n",
      "from sqlalchemy import Column, String, Text, Date, Integer, ForeignKey\n",
      "from sqlalchemy.types import JSON\n",
      "from sqlalchemy.orm import relationship\n",
      "from .association import job_resume_association\n",
      "\n",
      "\n",
      "class Job(Base):\n",
      "    __tablename__ = \"jobs\"\n",
      "\n",
      "    job_id = Column(String, primary_key=True, index=True)  # uuid field\n",
      "\n",
      "    job_title = Column(String, nullable=False)\n",
      "    company_profile = Column(Text, nullable=True)\n",
      "    location = Column(String, nullable=True)\n",
      "    date_posted = Column(Date, nullable=True)\n",
      "    employment_type = Column(String, nullable=True)\n",
      "    job_summary = Column(Text, nullable=False)\n",
      "    key_responsibilities = Column(JSON, nullable=True)\n",
      "    qualifications = Column(JSON, nullable=True)\n",
      "    compensation_and_benfits = Column(JSON, nullable=True)\n",
      "    application_info = Column(JSON, nullable=True)\n",
      "    extracted_keywords = Column(JSON, nullable=True)\n",
      "\n",
      "    # one-to-many relation between user and jobs\n",
      "    owner_id = Column(Integer, ForeignKey(\"users.id\"), nullable=False)\n",
      "    owner = relationship(\"User\", back_populates=\"resumes\")\n",
      "\n",
      "    # many-to-many relationship in job and resume\n",
      "    jobs = relationship(\"Resume\", secondary=job_resume_association, back_populates=\"resumes\")\n",
      "from app.core.config import settings\n",
      "from app.core.database import DatabaseConnectionSingleton\n",
      "\n",
      "\n",
      "def get_db_session():\n",
      "    db_singleton = DatabaseConnectionSingleton(settings.DATABASE_URL)\n",
      "    db = db_singleton.get_session()\n",
      "    try:\n",
      "        yield db\n",
      "    finally:\n",
      "        db.close()\n",
      "from .router.health import health_check\n",
      "\n",
      "from fastapi import APIRouter, status, Depends\n",
      "from sqlalchemy.orm import Session\n",
      "from sqlalchemy import text\n",
      "from app.utils.utils import get_db_session\n",
      "\n",
      "health_check = APIRouter()\n",
      "\n",
      "@health_check.get(\"/ping\", tags=[\"health_check\"], status_code=status.HTTP_200_OK)\n",
      "def ping(db: Session = Depends(get_db_session)):\n",
      "    \"\"\"\n",
      "    health check endpoint\n",
      "    \"\"\"\n",
      "    try:\n",
      "        result = db.execute(text(\"SELECT 1\")).fetchone()\n",
      "        db_status = \"reachable\" if result is not None else \"not reachable\"\n",
      "    except Exception as e:\n",
      "        db_status = f\"error: {str(e)}\"\n",
      "    return {\"message\": \"pong\", \"database\": db_status}\n",
      "\n",
      "import os\n",
      "from pydantic_settings import BaseSettings\n",
      "from typing import List, Optional\n",
      "\n",
      "class Settings(BaseSettings):\n",
      "    PROJECT_NAME: str=\"Resume Matcher\"\n",
      "    FRONTEND_PATH: str=os.path.join(os.path.dirname(__file__), \"frontend\", \"assets\")\n",
      "    ALLOWED_ORIGINS: List[str]=[\"https://www.resume-matcher.fyi\"]\n",
      "    DATABASE_URL: Optional[str]\n",
      "    SESSION_SECRET_KEY: Optional[str]\n",
      "    PYTHONDONTWRITEBYTECODE: int=1\n",
      "\n",
      "    class Config:\n",
      "        env_file = \".env\"\n",
      "\n",
      "settings = Settings()\n",
      "\n",
      "import uvicorn\n",
      "from app.base import create_app\n",
      "\n",
      "app = create_app()\n",
      "\n",
      "if __name__ == \"__main__\":\n",
      "    uvicorn.run(\"app.main:app\", host=\"0.0.0.0\", port=8000, reload=True)\n",
      "\n",
      "from .base import Base\n",
      "from .resume import Resume\n",
      "from .user import User\n",
      "from .job import Job\n",
      "from .association import job_resume_association\n",
      "\n",
      "from .base import Base\n",
      "from sqlalchemy import Column, String, Table, ForeignKey\n",
      "\n",
      "\n",
      "job_resume_association = Table(\n",
      "    \"job_resume\",\n",
      "    Base.metadata,\n",
      "    Column(\"job_id\", String, ForeignKey(\"jobs.job_id\"), primary_key=True),\n",
      "    Column(\"resume_id\", String, ForeignKey(\"resumes.resume_id\"), primary_key = True)\n",
      ")\n",
      "\n",
      "from sqlalchemy.orm import DeclarativeBase\n",
      "\n",
      "class Base(DeclarativeBase):\n",
      "    pass\n",
      "\n",
      "from .base import Base \n",
      "from sqlalchemy import Column, String, Text, Date, Integer, ForeignKey\n",
      "from sqlalchemy.types import JSON\n",
      "from sqlalchemy.orm import relationship\n",
      "from .association import job_resume_association\n",
      "\n",
      "\n",
      "class Job(Base):\n",
      "    __tablename__ = \"jobs\"\n",
      "\n",
      "    job_id = Column(String, primary_key=True, index=True)  # uuid field\n",
      "\n",
      "    job_title = Column(String, nullable=False)\n",
      "    company_profile = Column(Text, nullable=True)\n",
      "    location = Column(String, nullable=True)\n",
      "    date_posted = Column(Date, nullable=True)\n",
      "    employment_type = Column(String, nullable=True)\n",
      "    job_summary = Column(Text, nullable=False)\n",
      "    key_responsibilities = Column(JSON, nullable=True)\n",
      "    qualifications = Column(JSON, nullable=True)\n",
      "    compensation_and_benfits = Column(JSON, nullable=True)\n",
      "    application_info = Column(JSON, nullable=True)\n",
      "    extracted_keywords = Column(JSON, nullable=True)\n",
      "\n",
      "    # one-to-many relation between user and jobs\n",
      "    owner_id = Column(Integer, ForeignKey(\"users.id\"), nullable=False)\n",
      "    owner = relationship(\"User\", back_populates=\"jobs\")\n",
      "\n",
      "    # many-to-many relationship in job and resume\n",
      "    resumes = relationship(\"Resume\", secondary=job_resume_association, back_populates=\"jobs\")\n",
      "\n",
      "from .base import Base \n",
      "from sqlalchemy import Column, String, Integer, ForeignKey\n",
      "from sqlalchemy.types import JSON\n",
      "from sqlalchemy.orm import relationship\n",
      "from .association import job_resume_association\n",
      "\n",
      "class Resume(Base):\n",
      "    __tablename__ = \"resumes\"\n",
      "\n",
      "    resume_id = Column(String, primary_key = True, index=True) # uuid field\n",
      "\n",
      "    personal_data = Column(JSON, nullable=False)\n",
      "    experiences = Column(JSON, nullable=True)\n",
      "    projects = Column(JSON, nullable=True)\n",
      "    skills = Column(JSON, nullable=True)\n",
      "    research_work = Column(JSON, nullable=True)\n",
      "    achievements = Column(JSON, nullable=True)\n",
      "    education = Column(JSON, nullable=True)\n",
      "    extracted_keywords = Column(JSON, nullable=True)\n",
      "\n",
      "    owner_id = Column(Integer, ForeignKey(\"users.id\"), nullable=False)\n",
      "    owner = relationship(\"User\", back_populates=\"resumes\")\n",
      "\n",
      "    jobs = relationship(\"Job\", secondary=job_resume_association, back_populates=\"resumes\")\n",
      "\n",
      "from .base import Base \n",
      "from sqlalchemy import Column, String, Integer\n",
      "from sqlalchemy.orm import relationship\n",
      "\n",
      "class User(Base):\n",
      "    __tablename__ = \"users\"\n",
      "\n",
      "    id = Column(Integer, primary_key=True, index=True)\n",
      "\n",
      "    email = Column(String, unique=True, index=True, nullable=False)\n",
      "    name = Column(String, nullable=False)\n",
      "\n",
      "    resumes = relationship(\"Resume\", back_populates=\"owner\")\n",
      "    jobs = relationship(\"Job\", back_populates=\"owner\")\n",
      "\n",
      "import os\n",
      "from pydantic_settings import BaseSettings\n",
      "from typing import List, Optional\n",
      "\n",
      "class Settings(BaseSettings):\n",
      "    PROJECT_NAME: str=\"Resume Matcher\"\n",
      "    FRONTEND_PATH: str=os.path.join(os.path.dirname(__file__), \"frontend\", \"assets\")\n",
      "    ALLOWED_ORIGINS: List[str]=[\"https://www.resumematcher.fyi\"]\n",
      "    DATABASE_URL: Optional[str]\n",
      "    SESSION_SECRET_KEY: Optional[str]\n",
      "    PYTHONDONTWRITEBYTECODE: int=1\n",
      "\n",
      "    class Config:\n",
      "        env_file = \".env\"\n",
      "\n",
      "settings = Settings()\n",
      "\n",
      "from .router.health import health_check\n",
      "from .router.v1 import v1_router\n",
      "from .middleware import RequestIDMiddleware\n",
      "from starlette.middleware.base import BaseHTTPMiddleware\n",
      "from starlette.requests import Request\n",
      "from uuid import uuid4\n",
      "\n",
      "class RequestIDMiddleware(BaseHTTPMiddleware):\n",
      "    async def dispatch(self, request: Request, call_next):\n",
      "        path_parts = request.url.path.strip(\"/\").split(\"/\")\n",
      "        \n",
      "        # Safely grab the 3rd part: /api/v1/<service>\n",
      "        service_tag = f\"{path_parts[2]}:\" if len(path_parts) > 2 else \"\"\n",
      "\n",
      "        request_id = f\"{service_tag}{uuid4()}\"\n",
      "        request.state.request_id = request_id\n",
      "\n",
      "        response = await call_next(request)\n",
      "        return response\n",
      "from fastapi import APIRouter\n",
      "from .resume import resume_router\n",
      "\n",
      "v1_router = APIRouter(prefix=\"/api/v1\", tags=[\"v1\"])\n",
      "v1_router.include_router(resume_router, prefix=\"/resume\", tags=[\"resume\"])\n",
      "from fastapi import APIRouter, File, UploadFile, HTTPException, Depends, Request\n",
      "from sqlalchemy.orm import Session\n",
      "from uuid import uuid4\n",
      "from app.services.resume_service import ResumeService\n",
      "from app.core import get_db_session\n",
      "\n",
      "resume_router = APIRouter()\n",
      "\n",
      "@resume_router.post(\"/upload\", summary=\"Upload a resume in PDF format and store it into DB in HTML/Markdown format\")\n",
      "async def upload_resume(\n",
      "    request: Request,\n",
      "    file: UploadFile = File(...),\n",
      "    db: Session = Depends(get_db_session)\n",
      "):\n",
      "    \"\"\"\n",
      "    Accepts a PDF file, converts it to HTML/Markdown, and stores it in the database.\n",
      "    \n",
      "    - **file**: The PDF file to be uploaded.\n",
      "    - Returns a success message with the filename and content type.\n",
      "    - Raises HTTP 400 if the file is not a PDF.\n",
      "    - Raises HTTP 500 if there is an error during the conversion or storage process.\n",
      "    \"\"\"\n",
      "    request_id = getattr(request.state, \"request_id\", str(uuid4()))\n",
      "    \n",
      "    if file.content_type != \"application/pdf\":\n",
      "        raise HTTPException(status_code=400, detail=\"Invalid file type. Only PDF files are allowed.\")\n",
      "    \n",
      "    pdf_bytes = await file.read()\n",
      "    if not pdf_bytes:\n",
      "        raise HTTPException(status_code=400, detail=\"Empty file. Please upload a valid PDF file.\")\n",
      "    \n",
      "    try:\n",
      "        ResumeService(db).convert_and_store_resume(pdf_bytes=pdf_bytes, content_type=\"md\")\n",
      "    except Exception as e:\n",
      "        raise HTTPException(status_code=500, detail=f\"Error processing file: {str(e)}\")\n",
      "    \n",
      "    return {\"message\": f\"File {file.filename} successfully processed as MD and stored in the DB\", \"request_id\": request_id}\n",
      "import os\n",
      "from fastapi import FastAPI, HTTPException\n",
      "from fastapi.exceptions import RequestValidationError\n",
      "from fastapi.staticfiles import StaticFiles\n",
      "from fastapi.middleware.cors import CORSMiddleware\n",
      "from starlette.middleware.sessions import SessionMiddleware\n",
      "from app.api import health_check, v1_router, RequestIDMiddleware\n",
      "from app.core import settings, db_singleton, custom_http_exception_handler, validation_exception_handler, unhandled_exception_handler\n",
      "from app.models import Base\n",
      "\n",
      "def create_app() -> FastAPI:\n",
      "    app = FastAPI(\n",
      "        title=settings.PROJECT_NAME,\n",
      "        docs_url=\"/api/docs\",\n",
      "        openapi_url=\"/api/openapi.json\"\n",
      "    )\n",
      "\n",
      "    app.add_middleware(SessionMiddleware, secret_key = settings.SESSION_SECRET_KEY, same_site=\"lax\")\n",
      "    app.add_middleware(\n",
      "        CORSMiddleware,\n",
      "        allow_origins=settings.ALLOWED_ORIGINS,\n",
      "        allow_credentials=True,\n",
      "        allow_methods=[\"*\"],\n",
      "        allow_headers=[\"*\"],\n",
      "    )\n",
      "    app.add_middleware(RequestIDMiddleware)\n",
      "    \n",
      "    app.add_exception_handler(HTTPException, custom_http_exception_handler)\n",
      "    app.add_exception_handler(RequestValidationError, validation_exception_handler)\n",
      "    app.add_exception_handler(Exception, unhandled_exception_handler)\n",
      "\n",
      "    Base.metadata.create_all(bind=db_singleton.engine)\n",
      "    \n",
      "    if os.path.exists(settings.FRONTEND_PATH): \n",
      "        app.mount(\"/app\", StaticFiles(directory=settings.FRONTEND_PATH, html=True), name=settings.PROJECT_NAME)\n",
      "\n",
      "    app.include_router(health_check)\n",
      "    app.include_router(v1_router)\n",
      "\n",
      "    return app\n",
      "\n",
      "from .database import DatabaseConnectionSingleton\n",
      "from .config import settings\n",
      "from .exceptions import custom_http_exception_handler, validation_exception_handler, unhandled_exception_handler\n",
      "\n",
      "db_singleton = DatabaseConnectionSingleton(settings.DATABASE_URL)\n",
      " \n",
      "def get_db_session():\n",
      "    \"\"\"\n",
      "    Dependency that provides a database session for the request.\n",
      "    It ensures that the session is closed after the request is completed.\n",
      "\n",
      "    Yields:\n",
      "        db (Session): An active database session object.\n",
      "    \"\"\"\n",
      "    # Create a new session for each request\n",
      "    db = db_singleton.get_session()\n",
      "    try:\n",
      "        yield db\n",
      "    finally:\n",
      "        db.close()\n",
      "import threading\n",
      "from sqlalchemy import create_engine\n",
      "from sqlalchemy.orm import sessionmaker\n",
      "from typing import Optional\n",
      "\n",
      "\n",
      "class DatabaseConnectionSingleton:\n",
      "    _instance_lock = threading.Lock()\n",
      "    _instance: Optional[\"DatabaseConnectionSingleton\"] = None\n",
      "\n",
      "    def __new__(cls, *args, **kwargs):\n",
      "        if not cls._instance:\n",
      "            with cls._instance_lock:\n",
      "                if not cls._instance:\n",
      "                    cls._instance = super(DatabaseConnectionSingleton, cls).__new__(cls)\n",
      "        return cls._instance\n",
      "    \n",
      "    def __init__(self, db_url: str):\n",
      "        if not hasattr(self, \"engine\"):\n",
      "            self.engine = create_engine(db_url, connect_args={\"check_same_thread\": False})\n",
      "            self.session = sessionmaker(autoflush=False, bind=self.engine)\n",
      "\n",
      "    def get_session(self):\n",
      "        return self.session()\n",
      "\n",
      "from fastapi import Request, HTTPException\n",
      "from fastapi.responses import JSONResponse\n",
      "from fastapi.exceptions import RequestValidationError\n",
      "from starlette.status import HTTP_500_INTERNAL_SERVER_ERROR\n",
      "\n",
      "async def custom_http_exception_handler(request: Request, exc: HTTPException):\n",
      "    request_id = getattr(request.state, \"request_id\", \"\")\n",
      "    return JSONResponse(\n",
      "        status_code=exc.status_code,\n",
      "        content={\"detail\": exc.detail, \"request_id\": request_id},\n",
      "    )\n",
      "\n",
      "async def validation_exception_handler(request: Request, exc: RequestValidationError):\n",
      "    request_id = getattr(request.state, \"request_id\", \"\")\n",
      "    return JSONResponse(\n",
      "        status_code=422,\n",
      "        content={\"detail\": exc.errors(), \"request_id\": request_id},\n",
      "    )\n",
      "\n",
      "async def unhandled_exception_handler(request: Request, exc: Exception):\n",
      "    request_id = getattr(request.state, \"request_id\", \"\")\n",
      "    return JSONResponse(\n",
      "        status_code=HTTP_500_INTERNAL_SERVER_ERROR,\n",
      "        content={\"detail\": \"Internal Server Error\", \"request_id\": request_id},\n",
      "    )\n",
      "\n",
      "from .base import Base\n",
      "from .resume import ProcessedResume, Resume\n",
      "from .user import User\n",
      "from .job import Job\n",
      "from .association import job_resume_association\n",
      "\n",
      "from .base import Base \n",
      "from sqlalchemy import Column, String, Text, Date, Integer, ForeignKey\n",
      "from sqlalchemy.types import JSON\n",
      "from sqlalchemy.orm import relationship\n",
      "from .association import job_resume_association\n",
      "\n",
      "\n",
      "class Job(Base):\n",
      "    __tablename__ = \"jobs\"\n",
      "\n",
      "    job_id = Column(String, primary_key=True, index=True)  # uuid field\n",
      "\n",
      "    job_title = Column(String, nullable=False)\n",
      "    company_profile = Column(Text, nullable=True)\n",
      "    location = Column(String, nullable=True)\n",
      "    date_posted = Column(Date, nullable=True)\n",
      "    employment_type = Column(String, nullable=True)\n",
      "    job_summary = Column(Text, nullable=False)\n",
      "    key_responsibilities = Column(JSON, nullable=True)\n",
      "    qualifications = Column(JSON, nullable=True)\n",
      "    compensation_and_benfits = Column(JSON, nullable=True)\n",
      "    application_info = Column(JSON, nullable=True)\n",
      "    extracted_keywords = Column(JSON, nullable=True)\n",
      "\n",
      "    # one-to-many relation between user and jobs\n",
      "    owner_id = Column(Integer, ForeignKey(\"users.id\"), nullable=False)\n",
      "    owner = relationship(\"User\", back_populates=\"jobs\")\n",
      "\n",
      "    # many-to-many relationship in job and resume\n",
      "    resumes = relationship(\"ProcessedResume\", secondary=job_resume_association, back_populates=\"jobs\")\n",
      "\n",
      "from .base import Base \n",
      "from sqlalchemy import Column, String, Integer, ForeignKey, Text\n",
      "from sqlalchemy.types import JSON\n",
      "from sqlalchemy.orm import relationship\n",
      "from .association import job_resume_association\n",
      "\n",
      "class ProcessedResume(Base):\n",
      "    __tablename__ = \"processed_resumes\"\n",
      "\n",
      "    resume_id = Column(String, primary_key = True, index=True) # uuid field\n",
      "\n",
      "    personal_data = Column(JSON, nullable=False)\n",
      "    experiences = Column(JSON, nullable=True)\n",
      "    projects = Column(JSON, nullable=True)\n",
      "    skills = Column(JSON, nullable=True)\n",
      "    research_work = Column(JSON, nullable=True)\n",
      "    achievements = Column(JSON, nullable=True)\n",
      "    education = Column(JSON, nullable=True)\n",
      "    extracted_keywords = Column(JSON, nullable=True)\n",
      "\n",
      "    owner_id = Column(Integer, ForeignKey(\"users.id\"), nullable=False)\n",
      "    owner = relationship(\"User\", back_populates=\"processed_resumes\")\n",
      "\n",
      "    jobs = relationship(\"Job\", secondary=job_resume_association, back_populates=\"processed_resumes\")\n",
      "\n",
      "\n",
      "class Resume(Base):\n",
      "    __tablename__ = \"resumes\"\n",
      "\n",
      "    id = Column(Integer, primary_key=True, index=True)\n",
      "    resume_id = Column(String, unique=True, nullable=False)\n",
      "    content = Column(Text, nullable=False)\n",
      "    content_type = Column(String, nullable=False)\n",
      "\n",
      "from .base import Base \n",
      "from sqlalchemy import Column, String, Integer\n",
      "from sqlalchemy.orm import relationship\n",
      "\n",
      "class User(Base):\n",
      "    __tablename__ = \"users\"\n",
      "\n",
      "    id = Column(Integer, primary_key=True, index=True)\n",
      "\n",
      "    email = Column(String, unique=True, index=True, nullable=False)\n",
      "    name = Column(String, nullable=False)\n",
      "\n",
      "    resumes = relationship(\"ProcessedResume\", back_populates=\"owner\")\n",
      "    jobs = relationship(\"Job\", back_populates=\"owner\")\n",
      "\n",
      "from io import BytesIO\n",
      "from pdfminer.high_level import extract_text_to_fp\n",
      "from pdfminer.layout import LAParams\n",
      "import html2text\n",
      "from app.models import Resume\n",
      "\n",
      "class ResumeService:\n",
      "    \"\"\"\n",
      "    A service class for handling resume-related operations.\n",
      "    \"\"\"\n",
      "    def __init__(self, db_session):\n",
      "        \"\"\"\n",
      "        Initialize the ResumeService with a database session.\n",
      "        Args:\n",
      "            db_session (Session): The database session to be used.\n",
      "        \"\"\"\n",
      "        self.db = db_session\n",
      "\n",
      "\n",
      "    def convert_and_pdf_to_md(self, pdf_bytes: bytes, content_type='md') -> str:\n",
      "        \"\"\"\n",
      "        Convert PDF bytes to md using pdfminer and html2text.\n",
      "        \n",
      "        Args:\n",
      "            pdf_bytes (bytes): The PDF file content in bytes.\n",
      "            \n",
      "        Returns:\n",
      "            str: The converted md content.\n",
      "        \"\"\"\n",
      "        output = BytesIO()\n",
      "        laparams = LAParams()\n",
      "\n",
      "        extract_text_to_fp(BytesIO(pdf_bytes), output, laparams=laparams, output_type='hmtl', codec='utf-8')\n",
      "        html_content = output.getvalue().decode('utf-8')\n",
      "        if content_type == 'html':\n",
      "            return html_content\n",
      "        \n",
      "        converter = html2text.HTML2Text()\n",
      "        converter.ignore_links = False\n",
      "        converter.ignore_images = False\n",
      "        converter.body_width = 0\n",
      "\n",
      "        md_content = converter.handle(html_content)\n",
      "        \n",
      "        return md_content, content_type\n",
      "    \n",
      "    def store_resume(self, resume_id: str, content: str, content_type: str = \"md\") -> Resume:\n",
      "        \"\"\"\n",
      "        Store the resume content in the database.\n",
      "        \n",
      "        Args:\n",
      "            content (str): The resume content to be stored.\n",
      "            \n",
      "        Returns:\n",
      "            Resume: The created Resume object.\n",
      "        \"\"\"\n",
      "        resume = Resume(resume_id=resume_id, content=content, content_type=content_type)\n",
      "        self.db.add(resume)\n",
      "        self.db.commit()\n",
      "        self.db.refresh(resume)\n",
      "        \n",
      "        return resume\n",
      "    \n",
      "    def convert_and_store_resume(self, pdf_bytes: bytes, resume_id: str,content_type: str = \"md\") -> Resume:\n",
      "        \"\"\"\n",
      "        Convert PDF bytes to md and store it in the database.\n",
      "        \n",
      "        Args:\n",
      "            pdf_bytes (bytes): The PDF file content in bytes.\n",
      "            \n",
      "        Returns:\n",
      "            Resume: The created Resume object.\n",
      "        \"\"\"\n",
      "        md_content, content_type = self.convert_and_pdf_to_md(pdf_bytes, content_type)\n",
      "        \n",
      "        return self.store_resume(resume_id, md_content, content_type)\n",
      "from starlette.middleware.base import BaseHTTPMiddleware\n",
      "from starlette.requests import Request\n",
      "from uuid import uuid4\n",
      "\n",
      "\n",
      "class RequestIDMiddleware(BaseHTTPMiddleware):\n",
      "    async def dispatch(self, request: Request, call_next):\n",
      "        path_parts = request.url.path.strip(\"/\").split(\"/\")\n",
      "\n",
      "        # Safely grab the 3rd part: /api/v1/<service>\n",
      "        service_tag = f\"{path_parts[2]}:\" if len(path_parts) > 2 else \"\"\n",
      "\n",
      "        request_id = f\"{service_tag}{uuid4()}\"\n",
      "        request.state.request_id = request_id\n",
      "\n",
      "        response = await call_next(request)\n",
      "        return response\n",
      "\n",
      "from .base import Base\n",
      "from sqlalchemy import Column, String, Table, ForeignKey\n",
      "\n",
      "\n",
      "job_resume_association = Table(\n",
      "    \"job_resume\",\n",
      "    Base.metadata,\n",
      "    Column(\"job_id\", String, ForeignKey(\"jobs.job_id\"), primary_key=True),\n",
      "    Column(\"resume_id\", String, ForeignKey(\"resumes.resume_id\"), primary_key=True),\n",
      ")\n",
      "\n",
      "from sqlalchemy.orm import DeclarativeBase\n",
      "\n",
      "\n",
      "class Base(DeclarativeBase):\n",
      "    pass\n",
      "\n",
      "from .base import Base\n",
      "from sqlalchemy import Column, String, Text, Date, Integer, ForeignKey\n",
      "from sqlalchemy.types import JSON\n",
      "from sqlalchemy.orm import relationship\n",
      "from .association import job_resume_association\n",
      "\n",
      "\n",
      "class Job(Base):\n",
      "    __tablename__ = \"jobs\"\n",
      "\n",
      "    job_id = Column(String, primary_key=True, index=True)  # uuid field\n",
      "    job_title = Column(String, nullable=False)\n",
      "    company_profile = Column(Text, nullable=True)\n",
      "    location = Column(String, nullable=True)\n",
      "    date_posted = Column(Date, nullable=True)\n",
      "    employment_type = Column(String, nullable=True)\n",
      "    job_summary = Column(Text, nullable=False)\n",
      "    key_responsibilities = Column(JSON, nullable=True)\n",
      "    qualifications = Column(JSON, nullable=True)\n",
      "    compensation_and_benfits = Column(JSON, nullable=True)\n",
      "    application_info = Column(JSON, nullable=True)\n",
      "    extracted_keywords = Column(JSON, nullable=True)\n",
      "\n",
      "    # one-to-many relation between user and jobs\n",
      "    owner_id = Column(Integer, ForeignKey(\"users.id\"), nullable=False)\n",
      "    owner = relationship(\"User\", back_populates=\"jobs\")\n",
      "\n",
      "    # many-to-many relationship in job and resume\n",
      "    resumes = relationship(\n",
      "        \"ProcessedResume\", secondary=job_resume_association, back_populates=\"jobs\"\n",
      "    )\n",
      "\n",
      "from fastapi import APIRouter, File, UploadFile, HTTPException, Depends, Request\n",
      "from sqlalchemy.orm import Session\n",
      "from uuid import uuid4\n",
      "from app.services.resume_service import ResumeService\n",
      "from app.core import get_db_session\n",
      "\n",
      "resume_router = APIRouter()\n",
      "\n",
      "\n",
      "@resume_router.post(\n",
      "    \"/upload\",\n",
      "    summary=\"Upload a resume in PDF or DOCX format and store it into DB in HTML/Markdown format\",\n",
      ")\n",
      "async def upload_resume(\n",
      "    request: Request,\n",
      "    file: UploadFile = File(...),\n",
      "    db: Session = Depends(get_db_session),\n",
      "):\n",
      "    \"\"\"\n",
      "    Accepts a PDF or DOCX file, converts it to HTML/Markdown, and stores it in the database.\n",
      "\n",
      "    Raises:\n",
      "        HTTPException: If the file type is not supported or if the file is empty.\n",
      "    \"\"\"\n",
      "    request_id = getattr(request.state, \"request_id\", str(uuid4()))\n",
      "\n",
      "    allowed_content_types = [\n",
      "        \"application/pdf\",\n",
      "        \"application/vnd.openxmlformats-officedocument.wordprocessingml.document\",\n",
      "    ]\n",
      "    if file.content_type not in allowed_content_types:\n",
      "        raise HTTPException(\n",
      "            status_code=400,\n",
      "            detail=\"Invalid file type. Only PDF and DOCX files are allowed.\",\n",
      "        )\n",
      "\n",
      "    file_bytes = await file.read()\n",
      "    if not file_bytes:\n",
      "        raise HTTPException(\n",
      "            status_code=400, detail=\"Empty file. Please upload a valid file.\"\n",
      "        )\n",
      "\n",
      "    try:\n",
      "        resume_id = ResumeService(db).convert_and_store_resume(\n",
      "            file_bytes=file_bytes,\n",
      "            file_type=file.content_type,\n",
      "            filename=file.filename,\n",
      "            content_type=\"md\",\n",
      "        )\n",
      "    except Exception as e:\n",
      "        raise HTTPException(status_code=500, detail=f\"Error processing file: {str(e)}\")\n",
      "\n",
      "    return {\n",
      "        \"message\": f\"File {file.filename} successfully processed as MD and stored in the DB\",\n",
      "        \"request_id\": request_id,\n",
      "        \"resume_id\": resume_id,\n",
      "    }\n",
      "\n",
      "from .base import Base\n",
      "from sqlalchemy import Column, String, Integer, ForeignKey, Text\n",
      "from sqlalchemy.types import JSON\n",
      "from sqlalchemy.orm import relationship\n",
      "from .association import job_resume_association\n",
      "\n",
      "\n",
      "class ProcessedResume(Base):\n",
      "    __tablename__ = \"processed_resumes\"\n",
      "\n",
      "    resume_id = Column(String, primary_key=True, index=True)  # uuid field\n",
      "    personal_data = Column(JSON, nullable=False)\n",
      "    experiences = Column(JSON, nullable=True)\n",
      "    projects = Column(JSON, nullable=True)\n",
      "    skills = Column(JSON, nullable=True)\n",
      "    research_work = Column(JSON, nullable=True)\n",
      "    achievements = Column(JSON, nullable=True)\n",
      "    education = Column(JSON, nullable=True)\n",
      "    extracted_keywords = Column(JSON, nullable=True)\n",
      "\n",
      "    owner_id = Column(Integer, ForeignKey(\"users.id\"), nullable=False)\n",
      "    owner = relationship(\"User\", back_populates=\"processed_resumes\")\n",
      "\n",
      "    jobs = relationship(\n",
      "        \"Job\", secondary=job_resume_association, back_populates=\"processed_resumes\"\n",
      "    )\n",
      "\n",
      "\n",
      "class Resume(Base):\n",
      "    __tablename__ = \"resumes\"\n",
      "\n",
      "    id = Column(Integer, primary_key=True, index=True)\n",
      "    resume_id = Column(String, unique=True, nullable=False)\n",
      "    content = Column(Text, nullable=False)\n",
      "    content_type = Column(String, nullable=False)\n",
      "\n",
      "import tempfile\n",
      "import os\n",
      "import uuid\n",
      "from markitdown import MarkItDown\n",
      "from sqlalchemy.orm import Session\n",
      "from app.models.resume import Resume\n",
      "\n",
      "\n",
      "class ResumeService:\n",
      "    def __init__(self, db: Session):\n",
      "        self.db = db\n",
      "        self.md = MarkItDown(enable_plugins=False)\n",
      "\n",
      "    def convert_and_store_resume(\n",
      "        self, file_bytes: bytes, file_type: str, filename: str, content_type: str = \"md\"\n",
      "    ):\n",
      "        \"\"\"\n",
      "        Converts resume file (PDF/DOCX) to text using MarkItDown and stores it in the database.\n",
      "\n",
      "        Args:\n",
      "            file_bytes: Raw bytes of the uploaded file\n",
      "            file_type: MIME type of the file (\"application/pdf\" or \"application/vnd.openxmlformats-officedocument.wordprocessingml.document\")\n",
      "            filename: Original filename\n",
      "            content_type: Output format (\"md\" for markdown or \"html\")\n",
      "\n",
      "        Returns:\n",
      "            None\n",
      "        \"\"\"\n",
      "        with tempfile.NamedTemporaryFile(\n",
      "            delete=False, suffix=self._get_file_extension(file_type)\n",
      "        ) as temp_file:\n",
      "            temp_file.write(file_bytes)\n",
      "            temp_path = temp_file.name\n",
      "\n",
      "        try:\n",
      "            result = self.md.convert(temp_path)\n",
      "            text_content = result.text_content\n",
      "            resume_id = str(uuid.uuid4())\n",
      "            self._store_resume_in_db(resume_id, filename, text_content, content_type)\n",
      "\n",
      "            return resume_id\n",
      "\n",
      "        finally:\n",
      "            if os.path.exists(temp_path):\n",
      "                os.remove(temp_path)\n",
      "\n",
      "    def _get_file_extension(self, file_type: str) -> str:\n",
      "        \"\"\"Returns the appropriate file extension based on MIME type\"\"\"\n",
      "        if file_type == \"application/pdf\":\n",
      "            return \".pdf\"\n",
      "        elif (\n",
      "            file_type\n",
      "            == \"application/vnd.openxmlformats-officedocument.wordprocessingml.document\"\n",
      "        ):\n",
      "            return \".docx\"\n",
      "        return \"\"\n",
      "\n",
      "    def _store_resume_in_db(\n",
      "        self, resume_id: str, filename: str, text_content: str, content_type: str\n",
      "    ):\n",
      "        \"\"\"\n",
      "        Stores the parsed resume content in the database.\n",
      "        \"\"\"\n",
      "        resume = Resume(\n",
      "            resume_id=resume_id, content=text_content, content_type=content_type\n",
      "        )\n",
      "\n",
      "        self.db.add(resume)\n",
      "        self.db.commit()\n",
      "\n",
      "        return resume\n",
      "\n",
      "from .router.health import health_check\n",
      "from .router.v1 import v1_router\n",
      "from .middleware import RequestIDMiddleware\n",
      "\n",
      "from fastapi import APIRouter, status, Depends\n",
      "from sqlalchemy.orm import Session\n",
      "from sqlalchemy import text\n",
      "from app.utils.utils import get_db_session\n",
      "\n",
      "health_check = APIRouter()\n",
      "\n",
      "\n",
      "@health_check.get(\"/ping\", tags=[\"health_check\"], status_code=status.HTTP_200_OK)\n",
      "def ping(db: Session = Depends(get_db_session)):\n",
      "    \"\"\"\n",
      "    health check endpoint\n",
      "    \"\"\"\n",
      "    try:\n",
      "        result = db.execute(text(\"SELECT 1\")).fetchone()\n",
      "        db_status = \"reachable\" if result is not None else \"not reachable\"\n",
      "    except Exception as e:\n",
      "        db_status = f\"error: {str(e)}\"\n",
      "    return {\"message\": \"pong\", \"database\": db_status}\n",
      "\n",
      "from fastapi import APIRouter\n",
      "from .resume import resume_router\n",
      "\n",
      "v1_router = APIRouter(prefix=\"/api/v1\", tags=[\"v1\"])\n",
      "v1_router.include_router(resume_router, prefix=\"/resume\", tags=[\"resume\"])\n",
      "\n",
      "import os\n",
      "from fastapi import FastAPI, HTTPException\n",
      "from fastapi.exceptions import RequestValidationError\n",
      "from fastapi.staticfiles import StaticFiles\n",
      "from fastapi.middleware.cors import CORSMiddleware\n",
      "from starlette.middleware.sessions import SessionMiddleware\n",
      "from app.api import health_check, v1_router, RequestIDMiddleware\n",
      "from app.core import (\n",
      "    settings,\n",
      "    db_singleton,\n",
      "    custom_http_exception_handler,\n",
      "    validation_exception_handler,\n",
      "    unhandled_exception_handler,\n",
      ")\n",
      "from app.models import Base\n",
      "\n",
      "\n",
      "def create_app() -> FastAPI:\n",
      "    app = FastAPI(\n",
      "        title=settings.PROJECT_NAME,\n",
      "        docs_url=\"/api/docs\",\n",
      "        openapi_url=\"/api/openapi.json\",\n",
      "    )\n",
      "\n",
      "    app.add_middleware(\n",
      "        SessionMiddleware, secret_key=settings.SESSION_SECRET_KEY, same_site=\"lax\"\n",
      "    )\n",
      "    app.add_middleware(\n",
      "        CORSMiddleware,\n",
      "        allow_origins=settings.ALLOWED_ORIGINS,\n",
      "        allow_credentials=True,\n",
      "        allow_methods=[\"*\"],\n",
      "        allow_headers=[\"*\"],\n",
      "    )\n",
      "    app.add_middleware(RequestIDMiddleware)\n",
      "\n",
      "    app.add_exception_handler(HTTPException, custom_http_exception_handler)\n",
      "    app.add_exception_handler(RequestValidationError, validation_exception_handler)\n",
      "    app.add_exception_handler(Exception, unhandled_exception_handler)\n",
      "\n",
      "    Base.metadata.create_all(bind=db_singleton.engine)\n",
      "\n",
      "    if os.path.exists(settings.FRONTEND_PATH):\n",
      "        app.mount(\n",
      "            \"/app\",\n",
      "            StaticFiles(directory=settings.FRONTEND_PATH, html=True),\n",
      "            name=settings.PROJECT_NAME,\n",
      "        )\n",
      "\n",
      "    app.include_router(health_check)\n",
      "    app.include_router(v1_router)\n",
      "\n",
      "    return app\n",
      "\n",
      "from .database import DatabaseConnectionSingleton\n",
      "from .config import settings\n",
      "from .exceptions import (\n",
      "    custom_http_exception_handler,\n",
      "    validation_exception_handler,\n",
      "    unhandled_exception_handler,\n",
      ")\n",
      "\n",
      "db_singleton = DatabaseConnectionSingleton(settings.DATABASE_URL)\n",
      "\n",
      "\n",
      "def get_db_session():\n",
      "    \"\"\"\n",
      "    Dependency that provides a database session for the request.\n",
      "    It ensures that the session is closed after the request is completed.\n",
      "\n",
      "    Yields:\n",
      "        db (Session): An active database session object.\n",
      "    \"\"\"\n",
      "    # Create a new session for each request\n",
      "    db = db_singleton.get_session()\n",
      "    try:\n",
      "        yield db\n",
      "    finally:\n",
      "        db.close()\n",
      "\n",
      "import os\n",
      "from pydantic_settings import BaseSettings\n",
      "from typing import List, Optional\n",
      "\n",
      "\n",
      "class Settings(BaseSettings):\n",
      "    PROJECT_NAME: str = \"Resume Matcher\"\n",
      "    FRONTEND_PATH: str = os.path.join(os.path.dirname(__file__), \"frontend\", \"assets\")\n",
      "    ALLOWED_ORIGINS: List[str] = [\"https://www.resumematcher.fyi\"]\n",
      "    DATABASE_URL: Optional[str]\n",
      "    SESSION_SECRET_KEY: Optional[str]\n",
      "    PYTHONDONTWRITEBYTECODE: int = 1\n",
      "\n",
      "    class Config:\n",
      "        env_file = \".env\"\n",
      "\n",
      "\n",
      "settings = Settings()\n",
      "\n",
      "import threading\n",
      "from sqlalchemy import create_engine\n",
      "from sqlalchemy.orm import sessionmaker\n",
      "from typing import Optional\n",
      "\n",
      "\n",
      "class DatabaseConnectionSingleton:\n",
      "    _instance_lock = threading.Lock()\n",
      "    _instance: Optional[\"DatabaseConnectionSingleton\"] = None\n",
      "\n",
      "    def __new__(cls, *args, **kwargs):\n",
      "        if not cls._instance:\n",
      "            with cls._instance_lock:\n",
      "                if not cls._instance:\n",
      "                    cls._instance = super(DatabaseConnectionSingleton, cls).__new__(cls)\n",
      "        return cls._instance\n",
      "\n",
      "    def __init__(self, db_url: str):\n",
      "        if not hasattr(self, \"engine\"):\n",
      "            self.engine = create_engine(\n",
      "                db_url, connect_args={\"check_same_thread\": False}\n",
      "            )\n",
      "            self.session = sessionmaker(autoflush=False, bind=self.engine)\n",
      "\n",
      "    def get_session(self):\n",
      "        return self.session()\n",
      "\n",
      "from fastapi import Request, HTTPException\n",
      "from fastapi.responses import JSONResponse\n",
      "from fastapi.exceptions import RequestValidationError\n",
      "from starlette.status import HTTP_500_INTERNAL_SERVER_ERROR\n",
      "\n",
      "\n",
      "async def custom_http_exception_handler(request: Request, exc: HTTPException):\n",
      "    request_id = getattr(request.state, \"request_id\", \"\")\n",
      "    return JSONResponse(\n",
      "        status_code=exc.status_code,\n",
      "        content={\"detail\": exc.detail, \"request_id\": request_id},\n",
      "    )\n",
      "\n",
      "\n",
      "async def validation_exception_handler(request: Request, exc: RequestValidationError):\n",
      "    request_id = getattr(request.state, \"request_id\", \"\")\n",
      "    return JSONResponse(\n",
      "        status_code=422,\n",
      "        content={\"detail\": exc.errors(), \"request_id\": request_id},\n",
      "    )\n",
      "\n",
      "\n",
      "async def unhandled_exception_handler(request: Request, exc: Exception):\n",
      "    request_id = getattr(request.state, \"request_id\", \"\")\n",
      "    return JSONResponse(\n",
      "        status_code=HTTP_500_INTERNAL_SERVER_ERROR,\n",
      "        content={\"detail\": \"Internal Server Error\", \"request_id\": request_id},\n",
      "    )\n",
      "\n",
      "from .base import Base\n",
      "from .resume import ProcessedResume, Resume\n",
      "from .user import User\n",
      "from .job import Job\n",
      "from .association import job_resume_association\n",
      "\n",
      "__all__ = [\n",
      "    \"Base\",\n",
      "    \"Resume\",\n",
      "    \"ProcessedResume\",\n",
      "    \"User\",\n",
      "    \"Job\",\n",
      "    \"job_resume_association\",\n",
      "]\n",
      "\n",
      "from .base import Base\n",
      "from sqlalchemy import Column, String, Integer\n",
      "from sqlalchemy.orm import relationship\n",
      "\n",
      "\n",
      "class User(Base):\n",
      "    __tablename__ = \"users\"\n",
      "\n",
      "    id = Column(Integer, primary_key=True, index=True)\n",
      "\n",
      "    email = Column(String, unique=True, index=True, nullable=False)\n",
      "    name = Column(String, nullable=False)\n",
      "\n",
      "    resumes = relationship(\"ProcessedResume\", back_populates=\"owner\")\n",
      "    jobs = relationship(\"Job\", back_populates=\"owner\")\n",
      "\n",
      "from app.core.config import settings\n",
      "from app.core.database import DatabaseConnectionSingleton\n",
      "\n",
      "\n",
      "def get_db_session():\n",
      "    db_singleton = DatabaseConnectionSingleton(settings.DATABASE_URL)\n",
      "    db = db_singleton.get_session()\n",
      "    try:\n",
      "        yield db\n",
      "    finally:\n",
      "        db.close()\n",
      "\n",
      "from .router.health import health_check\n",
      "from .router.v1 import v1_router\n",
      "from .middleware import RequestIDMiddleware\n",
      "\n",
      "__all__ = [\"health_check\", \"v1_router\", \"RequestIDMiddleware\"]\n",
      "\n",
      "from fastapi import APIRouter\n",
      "from .resume import resume_router\n",
      "\n",
      "v1_router = APIRouter(prefix=\"/api/v1\", tags=[\"v1\"])\n",
      "v1_router.include_router(resume_router, prefix=\"/resume\", tags=[\"resume\"])\n",
      "\n",
      "\n",
      "__all__ = [\"v1_router\"]\n",
      "\n",
      "from .database import DatabaseConnectionSingleton\n",
      "from .config import settings\n",
      "from .exceptions import (\n",
      "    custom_http_exception_handler,\n",
      "    validation_exception_handler,\n",
      "    unhandled_exception_handler,\n",
      ")\n",
      "\n",
      "db_singleton = DatabaseConnectionSingleton(settings.DATABASE_URL)\n",
      "\n",
      "\n",
      "def get_db_session():\n",
      "    \"\"\"\n",
      "    Dependency that provides a database session for the request.\n",
      "    It ensures that the session is closed after the request is completed.\n",
      "\n",
      "    Yields:\n",
      "        db (Session): An active database session object.\n",
      "    \"\"\"\n",
      "    # Create a new session for each request\n",
      "    db = db_singleton.get_session()\n",
      "    try:\n",
      "        yield db\n",
      "    finally:\n",
      "        db.close()\n",
      "\n",
      "\n",
      "__all__ = [\n",
      "    \"settings\",\n",
      "    \"db_singleton\",\n",
      "    \"get_db_session\",\n",
      "    \"custom_http_exception_handler\",\n",
      "    \"validation_exception_handler\",\n",
      "    \"unhandled_exception_handler\",\n",
      "]\n",
      "\n",
      "from fastapi import APIRouter\n",
      "from .resume import resume_router\n",
      "from .job import job_router\n",
      "\n",
      "v1_router = APIRouter(prefix=\"/api/v1\", tags=[\"v1\"])\n",
      "v1_router.include_router(resume_router, prefix=\"/resumes\", tags=[\"resume\"])\n",
      "v1_router.include_router(job_router, prefix=\"/jobs\", tags=[\"job\"])\n",
      "\n",
      "\n",
      "__all__ = [\"v1_router\"]\n",
      "\n",
      "from fastapi import APIRouter, HTTPException, Depends, Request, status\n",
      "from sqlalchemy.orm import Session\n",
      "from uuid import uuid4\n",
      "from app.core import get_db_session\n",
      "from schemas.job import JobUploadRequest\n",
      "\n",
      "job_router = APIRouter()\n",
      "\n",
      "\n",
      "@job_router.post(\n",
      "    \"/upload\",\n",
      "    summary=\"stores the job posting in the database by parsing the JD into a structured format JSON\",\n",
      ")\n",
      "async def upload_job(\n",
      "    payload: JobUploadRequest,\n",
      "    request: Request,\n",
      "    db: Session = Depends(get_db_session),\n",
      "):\n",
      "    \"\"\"\n",
      "    Accepts a job description as a MarkDown text and stores it in the database.\n",
      "    \"\"\"\n",
      "    request_id = getattr(request.state, \"request_id\", str(uuid4()))\n",
      "\n",
      "    allowed_content_types = [\n",
      "        \"application/json\",\n",
      "    ]\n",
      "\n",
      "    content_type = request.headers.get(\"content-type\")\n",
      "    if not content_type:\n",
      "        raise HTTPException(\n",
      "            status_code=status.HTTP_400_BAD_REQUEST,\n",
      "            detail=\"Content-Type header is missing\",\n",
      "        )\n",
      "\n",
      "    if content_type not in allowed_content_types:\n",
      "        raise HTTPException(\n",
      "            status_code=400,\n",
      "            detail=f\"Invalid Content-Type. Only {', '.join(allowed_content_types)} is/are allowed.\",\n",
      "        )\n",
      "\n",
      "    try:\n",
      "        pass\n",
      "    except Exception as e:\n",
      "        raise HTTPException(\n",
      "            status_code=status.HTTP_500_INTERNAL_SERVER_ERROR,\n",
      "            detail=f\"Error processing file: {str(e)}\",\n",
      "        )\n",
      "\n",
      "    return {\n",
      "        \"message\": \"data successfully processed\",\n",
      "        \"request\": {\n",
      "            \"request_id\": request_id,\n",
      "            \"payload\": payload,\n",
      "        },\n",
      "    }\n",
      "\n",
      "from fastapi import APIRouter, File, UploadFile, HTTPException, Depends, Request, status\n",
      "from sqlalchemy.orm import Session\n",
      "from uuid import uuid4\n",
      "from app.services.resume_service import ResumeService\n",
      "from app.core import get_db_session\n",
      "\n",
      "resume_router = APIRouter()\n",
      "\n",
      "\n",
      "@resume_router.post(\n",
      "    \"/upload\",\n",
      "    summary=\"Upload a resume in PDF or DOCX format and store it into DB in HTML/Markdown format\",\n",
      ")\n",
      "async def upload_resume(\n",
      "    request: Request,\n",
      "    file: UploadFile = File(...),\n",
      "    db: Session = Depends(get_db_session),\n",
      "):\n",
      "    \"\"\"\n",
      "    Accepts a PDF or DOCX file, converts it to HTML/Markdown, and stores it in the database.\n",
      "\n",
      "    Raises:\n",
      "        HTTPException: If the file type is not supported or if the file is empty.\n",
      "    \"\"\"\n",
      "    request_id = getattr(request.state, \"request_id\", str(uuid4()))\n",
      "\n",
      "    allowed_content_types = [\n",
      "        \"application/pdf\",\n",
      "        \"application/vnd.openxmlformats-officedocument.wordprocessingml.document\",\n",
      "    ]\n",
      "\n",
      "    if file.content_type not in allowed_content_types:\n",
      "        raise HTTPException(\n",
      "            status_code=status.HTTP_400_BAD_REQUEST,\n",
      "            detail=\"Invalid file type. Only PDF and DOCX files are allowed.\",\n",
      "        )\n",
      "\n",
      "    file_bytes = await file.read()\n",
      "    if not file_bytes:\n",
      "        raise HTTPException(\n",
      "            status_code=status.HTTP_400_BAD_REQUEST,\n",
      "            detail=\"Empty file. Please upload a valid file.\",\n",
      "        )\n",
      "\n",
      "    try:\n",
      "        resume_id = ResumeService(db).convert_and_store_resume(\n",
      "            file_bytes=file_bytes,\n",
      "            file_type=file.content_type,\n",
      "            filename=file.filename,\n",
      "            content_type=\"md\",\n",
      "        )\n",
      "    except Exception as e:\n",
      "        raise HTTPException(\n",
      "            status_code=status.HTTP_500_INTERNAL_SERVER_ERROR,\n",
      "            detail=f\"Error processing file: {str(e)}\",\n",
      "        )\n",
      "\n",
      "    return {\n",
      "        \"message\": f\"File {file.filename} successfully processed as MD and stored in the DB\",\n",
      "        \"request_id\": request_id,\n",
      "        \"resume_id\": resume_id,\n",
      "    }\n",
      "\n",
      "from .base import Base\n",
      "from sqlalchemy import Column, String, Table, ForeignKey\n",
      "\n",
      "\n",
      "job_resume_association = Table(\n",
      "    \"job_resume\",\n",
      "    Base.metadata,\n",
      "    Column(\n",
      "        \"processed_job_id\",\n",
      "        String,\n",
      "        ForeignKey(\"processed_jobs.job_id\"),\n",
      "        primary_key=True,\n",
      "    ),\n",
      "    Column(\n",
      "        \"processed_resume_id\",\n",
      "        String,\n",
      "        ForeignKey(\"processed_resumes.resume_id\"),\n",
      "        primary_key=True,\n",
      "    ),\n",
      ")\n",
      "\n",
      "from .base import Base\n",
      "from sqlalchemy import Column, String, Text, Date, Integer, ForeignKey, DateTime\n",
      "from sqlalchemy.types import JSON\n",
      "from sqlalchemy.orm import relationship\n",
      "from .association import job_resume_association\n",
      "import datetime\n",
      "\n",
      "\n",
      "class ProcessedJob(Base):\n",
      "    __tablename__ = \"processed_jobs\"\n",
      "\n",
      "    job_id = Column(String, primary_key=True, index=True)  # uuid field\n",
      "    job_title = Column(String, nullable=False)\n",
      "    company_profile = Column(Text, nullable=True)\n",
      "    location = Column(String, nullable=True)\n",
      "    date_posted = Column(Date, nullable=True)\n",
      "    employment_type = Column(String, nullable=True)\n",
      "    job_summary = Column(Text, nullable=False)\n",
      "    key_responsibilities = Column(JSON, nullable=True)\n",
      "    qualifications = Column(JSON, nullable=True)\n",
      "    compensation_and_benfits = Column(JSON, nullable=True)\n",
      "    application_info = Column(JSON, nullable=True)\n",
      "    extracted_keywords = Column(JSON, nullable=True)\n",
      "    processed_at = Column(\n",
      "        DateTime, default=datetime.datetime.now(datetime.timezone.utc)\n",
      "    )\n",
      "\n",
      "    # one-to-many relation between user and jobs\n",
      "    owner_id = Column(Integer, ForeignKey(\"users.id\"), nullable=False)\n",
      "    owner = relationship(\"User\", back_populates=\"processed_jobs\")\n",
      "    raw_job = relationship(\"Job\", back_populates=\"processed_job\")\n",
      "\n",
      "    # many-to-many relationship in job and resume\n",
      "    resumes = relationship(\n",
      "        \"ProcessedResume\",\n",
      "        secondary=job_resume_association,\n",
      "        back_populates=\"jobs\",\n",
      "    )\n",
      "\n",
      "\n",
      "class Job(Base):\n",
      "    __tablename__ = \"jobs\"\n",
      "\n",
      "    id = Column(Integer, primary_key=True, index=True)\n",
      "    job_id = Column(String, unique=True, nullable=False)\n",
      "    content = Column(Text, nullable=False)\n",
      "    content_type = Column(String, nullable=False)\n",
      "    created_at = Column(DateTime, default=datetime.datetime.now(datetime.timezone.utc))\n",
      "\n",
      "    processed_job = relationship(\n",
      "        \"ProcessedJob\", back_populates=\"raw_job\", uselist=False\n",
      "    )\n",
      "\n",
      "from .base import Base\n",
      "from sqlalchemy import Column, String, Integer, ForeignKey, Text\n",
      "from sqlalchemy.types import JSON\n",
      "from sqlalchemy.orm import relationship\n",
      "from .association import job_resume_association\n",
      "\n",
      "\n",
      "class ProcessedResume(Base):\n",
      "    __tablename__ = \"processed_resumes\"\n",
      "\n",
      "    resume_id = Column(String, primary_key=True, index=True)  # uuid field\n",
      "    personal_data = Column(JSON, nullable=False)\n",
      "    experiences = Column(JSON, nullable=True)\n",
      "    projects = Column(JSON, nullable=True)\n",
      "    skills = Column(JSON, nullable=True)\n",
      "    research_work = Column(JSON, nullable=True)\n",
      "    achievements = Column(JSON, nullable=True)\n",
      "    education = Column(JSON, nullable=True)\n",
      "    extracted_keywords = Column(JSON, nullable=True)\n",
      "\n",
      "    owner_id = Column(Integer, ForeignKey(\"users.id\"), nullable=False)\n",
      "    owner = relationship(\"User\", back_populates=\"processed_resumes\")\n",
      "    raw_resume = relationship(\"Resume\", back_populates=\"processed_resume\")\n",
      "\n",
      "    jobs = relationship(\n",
      "        \"ProcessedJob\",\n",
      "        secondary=job_resume_association,\n",
      "        back_populates=\"processed_resumes\",\n",
      "    )\n",
      "\n",
      "\n",
      "class Resume(Base):\n",
      "    __tablename__ = \"resumes\"\n",
      "\n",
      "    id = Column(Integer, primary_key=True, index=True)\n",
      "    resume_id = Column(String, unique=True, nullable=False)\n",
      "    content = Column(Text, nullable=False)\n",
      "    content_type = Column(String, nullable=False)\n",
      "\n",
      "    processed_resume = relationship(\n",
      "        \"ProcessedResume\", back_populates=\"raw_resume\", uselist=False\n",
      "    )\n",
      "\n",
      "from .base import Base\n",
      "from sqlalchemy import Column, String, Integer\n",
      "from sqlalchemy.orm import relationship\n",
      "\n",
      "\n",
      "class User(Base):\n",
      "    __tablename__ = \"users\"\n",
      "\n",
      "    id = Column(Integer, primary_key=True, index=True)\n",
      "\n",
      "    email = Column(String, unique=True, index=True, nullable=False)\n",
      "    name = Column(String, nullable=False)\n",
      "\n",
      "    resumes = relationship(\"ProcessedResume\", back_populates=\"owner\")\n",
      "    jobs = relationship(\"ProcessedJob\", back_populates=\"owner\")\n",
      "\n",
      "from pydantic import BaseModel, Field\n",
      "from typing import List\n",
      "from uuid import UUID\n",
      "\n",
      "\n",
      "class JobUploadRequest(BaseModel):\n",
      "    job_descriptions: List[str] = Field(\n",
      "        ..., description=\"List of job descriptions in markdown format\"\n",
      "    )\n",
      "    resume_id: UUID = Field(..., description=\"UUID reference to the resume\")\n",
      "\n",
      "from fastapi import APIRouter, HTTPException, Depends, Request, status\n",
      "from sqlalchemy.orm import Session\n",
      "from uuid import uuid4\n",
      "from app.core import get_db_session\n",
      "from app.schemas.job import JobUploadRequest\n",
      "from app.services import JobService\n",
      "\n",
      "job_router = APIRouter()\n",
      "\n",
      "\n",
      "@job_router.post(\n",
      "    \"/upload\",\n",
      "    summary=\"stores the job posting in the database by parsing the JD into a structured format JSON\",\n",
      ")\n",
      "async def upload_job(\n",
      "    payload: JobUploadRequest,\n",
      "    request: Request,\n",
      "    db: Session = Depends(get_db_session),\n",
      "):\n",
      "    \"\"\"\n",
      "    Accepts a job description as a MarkDown text and stores it in the database.\n",
      "    \"\"\"\n",
      "    request_id = getattr(request.state, \"request_id\", str(uuid4()))\n",
      "\n",
      "    allowed_content_types = [\n",
      "        \"application/json\",\n",
      "    ]\n",
      "\n",
      "    content_type = request.headers.get(\"content-type\")\n",
      "    if not content_type:\n",
      "        raise HTTPException(\n",
      "            status_code=status.HTTP_400_BAD_REQUEST,\n",
      "            detail=\"Content-Type header is missing\",\n",
      "        )\n",
      "\n",
      "    if content_type not in allowed_content_types:\n",
      "        raise HTTPException(\n",
      "            status_code=400,\n",
      "            detail=f\"Invalid Content-Type. Only {', '.join(allowed_content_types)} is/are allowed.\",\n",
      "        )\n",
      "\n",
      "    try:\n",
      "        job_service = JobService(db)\n",
      "        job_ids = job_service.create_and_store_job(payload)\n",
      "    except Exception as e:\n",
      "        raise HTTPException(\n",
      "            status_code=status.HTTP_500_INTERNAL_SERVER_ERROR,\n",
      "            detail=f\"Error processing file: {str(e)}\",\n",
      "        )\n",
      "\n",
      "    return {\n",
      "        \"message\": \"data successfully processed\",\n",
      "        \"job_id\": job_ids,\n",
      "        \"request\": {\n",
      "            \"request_id\": request_id,\n",
      "            \"payload\": payload,\n",
      "        },\n",
      "    }\n",
      "\n",
      "from .base import Base\n",
      "from sqlalchemy import Column, String, Text, Date, Integer, ForeignKey, DateTime\n",
      "from sqlalchemy.types import JSON\n",
      "from sqlalchemy.orm import relationship\n",
      "from .association import job_resume_association\n",
      "import datetime\n",
      "\n",
      "\n",
      "class ProcessedJob(Base):\n",
      "    __tablename__ = \"processed_jobs\"\n",
      "\n",
      "    job_id = Column(String, primary_key=True, index=True)  # uuid field\n",
      "    job_title = Column(String, nullable=False)\n",
      "    company_profile = Column(Text, nullable=True)\n",
      "    location = Column(String, nullable=True)\n",
      "    date_posted = Column(Date, nullable=True)\n",
      "    employment_type = Column(String, nullable=True)\n",
      "    job_summary = Column(Text, nullable=False)\n",
      "    key_responsibilities = Column(JSON, nullable=True)\n",
      "    qualifications = Column(JSON, nullable=True)\n",
      "    compensation_and_benfits = Column(JSON, nullable=True)\n",
      "    application_info = Column(JSON, nullable=True)\n",
      "    extracted_keywords = Column(JSON, nullable=True)\n",
      "    processed_at = Column(\n",
      "        DateTime, default=datetime.datetime.now(datetime.timezone.utc)\n",
      "    )\n",
      "\n",
      "    # one-to-many relation between user and jobs\n",
      "    # owner_id = Column(Integer, ForeignKey(\"users.id\"), nullable=False)\n",
      "    # owner = relationship(\"User\", back_populates=\"processed_jobs\")\n",
      "    # raw_job = relationship(\"Job\", back_populates=\"processed_job\")\n",
      "\n",
      "    # many-to-many relationship in job and resume\n",
      "    # resumes = relationship(\n",
      "    #     \"ProcessedResume\",\n",
      "    #     secondary=job_resume_association,\n",
      "    #     back_populates=\"jobs\",\n",
      "    # )\n",
      "\n",
      "\n",
      "class Job(Base):\n",
      "    __tablename__ = \"jobs\"\n",
      "\n",
      "    id = Column(Integer, primary_key=True, index=True)\n",
      "    job_id = Column(String, unique=True, nullable=False)\n",
      "    resume_id = Column(String, ForeignKey(\"resumes.resume_id\"), nullable=False)\n",
      "    content = Column(Text, nullable=False)\n",
      "    created_at = Column(DateTime, default=datetime.datetime.now(datetime.timezone.utc))\n",
      "\n",
      "    # processed_job = relationship(\n",
      "    #     \"ProcessedJob\", back_populates=\"raw_job\", uselist=False\n",
      "    # )\n",
      "\n",
      "    resumes = relationship(\"Resume\", back_populates=\"jobs\")\n",
      "\n",
      "from .base import Base\n",
      "from sqlalchemy import Column, String, Integer, ForeignKey, Text\n",
      "from sqlalchemy.types import JSON\n",
      "from sqlalchemy.orm import relationship\n",
      "from .association import job_resume_association\n",
      "\n",
      "\n",
      "class ProcessedResume(Base):\n",
      "    __tablename__ = \"processed_resumes\"\n",
      "\n",
      "    resume_id = Column(String, primary_key=True, index=True)  # uuid field\n",
      "    personal_data = Column(JSON, nullable=False)\n",
      "    experiences = Column(JSON, nullable=True)\n",
      "    projects = Column(JSON, nullable=True)\n",
      "    skills = Column(JSON, nullable=True)\n",
      "    research_work = Column(JSON, nullable=True)\n",
      "    achievements = Column(JSON, nullable=True)\n",
      "    education = Column(JSON, nullable=True)\n",
      "    extracted_keywords = Column(JSON, nullable=True)\n",
      "\n",
      "    # owner_id = Column(Integer, ForeignKey(\"users.id\"), nullable=False)\n",
      "    # owner = relationship(\"User\", back_populates=\"processed_resumes\")\n",
      "    # raw_resume = relationship(\"Resume\", back_populates=\"processed_resume\")\n",
      "\n",
      "    # jobs = relationship(\n",
      "    #     \"ProcessedJob\",\n",
      "    #     secondary=job_resume_association,\n",
      "    #     back_populates=\"processed_resumes\",\n",
      "    # )\n",
      "\n",
      "\n",
      "class Resume(Base):\n",
      "    __tablename__ = \"resumes\"\n",
      "\n",
      "    id = Column(Integer, primary_key=True, index=True)\n",
      "    resume_id = Column(String, unique=True, nullable=False)\n",
      "    content = Column(Text, nullable=False)\n",
      "    content_type = Column(String, nullable=False)\n",
      "\n",
      "    # processed_resume = relationship(\n",
      "    #     \"ProcessedResume\", back_populates=\"raw_resume\", uselist=False\n",
      "    # )\n",
      "\n",
      "    jobs = relationship(\"Job\", back_populates=\"resumes\")\n",
      "\n",
      "from .base import Base\n",
      "from sqlalchemy import Column, String, Integer\n",
      "# from sqlalchemy.orm import relationship\n",
      "\n",
      "\n",
      "class User(Base):\n",
      "    __tablename__ = \"users\"\n",
      "\n",
      "    id = Column(Integer, primary_key=True, index=True)\n",
      "\n",
      "    email = Column(String, unique=True, index=True, nullable=False)\n",
      "    name = Column(String, nullable=False)\n",
      "\n",
      "    # commenting the associations out for now. TODO: uncomment when we are clear about how to associate tables\n",
      "    # processed_resumes = relationship(\n",
      "    #     \"ProcessedResume\", back_populates=\"owner\", cascade=\"all, delete-orphan\"\n",
      "    # )\n",
      "    # processed_jobs = relationship(\n",
      "    #     \"ProcessedJob\", back_populates=\"owner\", cascade=\"all, delete-orphan\"\n",
      "    # )\n",
      "\n",
      "from .resume_service import ResumeService\n",
      "from .job_service import JobService\n",
      "\n",
      "__all__ = [\"ResumeService\", \"JobService\"]\n",
      "\n",
      "import uuid\n",
      "from sqlalchemy.orm import Session\n",
      "from app.models import Job\n",
      "from typing import List\n",
      "from app.schemas.job import JobUploadRequest\n",
      "\n",
      "\n",
      "class JobService:\n",
      "    def __init__(self, db: Session):\n",
      "        self.db = db\n",
      "\n",
      "    def create_and_store_job(self, job_data: JobUploadRequest) -> List[str]:\n",
      "        \"\"\"\n",
      "        Stores job data in the database and returns a list of job IDs.\n",
      "\n",
      "        Args:\n",
      "            job_data: JobUploadRequest containing job details.\n",
      "\n",
      "        Returns:\n",
      "            List of job IDs.\n",
      "        \"\"\"\n",
      "        job_ids = []\n",
      "        for job_description in job_data.job_descriptions:\n",
      "            job_id = str(uuid.uuid4())\n",
      "            job = Job(\n",
      "                job_id=job_id,\n",
      "                resume_id=job_data.resume_id,\n",
      "                content=job_description,\n",
      "            )\n",
      "            self.db.add(job)\n",
      "            job_ids.append(job_id)\n",
      "\n",
      "        self.db.commit()\n",
      "        return job_ids\n",
      "\n",
      "import tempfile\n",
      "import os\n",
      "import uuid\n",
      "from markitdown import MarkItDown\n",
      "from sqlalchemy.orm import Session\n",
      "from app.models import Resume\n",
      "\n",
      "\n",
      "class ResumeService:\n",
      "    def __init__(self, db: Session):\n",
      "        self.db = db\n",
      "        self.md = MarkItDown(enable_plugins=False)\n",
      "\n",
      "    def convert_and_store_resume(\n",
      "        self, file_bytes: bytes, file_type: str, filename: str, content_type: str = \"md\"\n",
      "    ):\n",
      "        \"\"\"\n",
      "        Converts resume file (PDF/DOCX) to text using MarkItDown and stores it in the database.\n",
      "\n",
      "        Args:\n",
      "            file_bytes: Raw bytes of the uploaded file\n",
      "            file_type: MIME type of the file (\"application/pdf\" or \"application/vnd.openxmlformats-officedocument.wordprocessingml.document\")\n",
      "            filename: Original filename\n",
      "            content_type: Output format (\"md\" for markdown or \"html\")\n",
      "\n",
      "        Returns:\n",
      "            None\n",
      "        \"\"\"\n",
      "        with tempfile.NamedTemporaryFile(\n",
      "            delete=False, suffix=self._get_file_extension(file_type)\n",
      "        ) as temp_file:\n",
      "            temp_file.write(file_bytes)\n",
      "            temp_path = temp_file.name\n",
      "\n",
      "        try:\n",
      "            result = self.md.convert(temp_path)\n",
      "            text_content = result.text_content\n",
      "            resume_id = str(uuid.uuid4())\n",
      "            self._store_resume_in_db(resume_id, filename, text_content, content_type)\n",
      "\n",
      "            return resume_id\n",
      "\n",
      "        finally:\n",
      "            if os.path.exists(temp_path):\n",
      "                os.remove(temp_path)\n",
      "\n",
      "    def _get_file_extension(self, file_type: str) -> str:\n",
      "        \"\"\"Returns the appropriate file extension based on MIME type\"\"\"\n",
      "        if file_type == \"application/pdf\":\n",
      "            return \".pdf\"\n",
      "        elif (\n",
      "            file_type\n",
      "            == \"application/vnd.openxmlformats-officedocument.wordprocessingml.document\"\n",
      "        ):\n",
      "            return \".docx\"\n",
      "        return \"\"\n",
      "\n",
      "    def _store_resume_in_db(\n",
      "        self, resume_id: str, filename: str, text_content: str, content_type: str\n",
      "    ):\n",
      "        \"\"\"\n",
      "        Stores the parsed resume content in the database.\n",
      "        \"\"\"\n",
      "        resume = Resume(\n",
      "            resume_id=resume_id, content=text_content, content_type=content_type\n",
      "        )\n",
      "\n",
      "        self.db.add(resume)\n",
      "        self.db.commit()\n",
      "\n",
      "        return resume\n",
      "\n",
      "from fastapi import APIRouter, HTTPException, Depends, Request, status\n",
      "from sqlalchemy.orm import Session\n",
      "from uuid import uuid4\n",
      "from app.core import get_db_session\n",
      "from app.schemas.job import JobUploadRequest\n",
      "from app.services import JobService\n",
      "\n",
      "job_router = APIRouter()\n",
      "\n",
      "\n",
      "@job_router.post(\n",
      "    \"/upload\",\n",
      "    summary=\"stores the job posting in the database by parsing the JD into a structured format JSON\",\n",
      ")\n",
      "async def upload_job(\n",
      "    payload: JobUploadRequest,\n",
      "    request: Request,\n",
      "    db: Session = Depends(get_db_session),\n",
      "):\n",
      "    \"\"\"\n",
      "    Accepts a job description as a MarkDown text and stores it in the database.\n",
      "    \"\"\"\n",
      "    request_id = getattr(request.state, \"request_id\", str(uuid4()))\n",
      "\n",
      "    allowed_content_types = [\n",
      "        \"application/json\",\n",
      "    ]\n",
      "\n",
      "    content_type = request.headers.get(\"content-type\")\n",
      "    if not content_type:\n",
      "        raise HTTPException(\n",
      "            status_code=status.HTTP_400_BAD_REQUEST,\n",
      "            detail=\"Content-Type header is missing\",\n",
      "        )\n",
      "\n",
      "    if content_type not in allowed_content_types:\n",
      "        raise HTTPException(\n",
      "            status_code=400,\n",
      "            detail=f\"Invalid Content-Type. Only {', '.join(allowed_content_types)} is/are allowed.\",\n",
      "        )\n",
      "\n",
      "    try:\n",
      "        job_service = JobService(db)\n",
      "        job_ids = job_service.create_and_store_job(payload.model_dump())\n",
      "\n",
      "    except AssertionError as e:\n",
      "        raise HTTPException(\n",
      "            status_code=status.HTTP_400_BAD_REQUEST,\n",
      "            detail=str(e),\n",
      "        )\n",
      "\n",
      "    except Exception as e:\n",
      "        raise HTTPException(\n",
      "            status_code=status.HTTP_500_INTERNAL_SERVER_ERROR,\n",
      "            detail=f\"{str(e)}\",\n",
      "        )\n",
      "\n",
      "    return {\n",
      "        \"message\": \"data successfully processed\",\n",
      "        \"job_id\": job_ids,\n",
      "        \"request\": {\n",
      "            \"request_id\": request_id,\n",
      "            \"payload\": payload,\n",
      "        },\n",
      "    }\n",
      "\n",
      "from .base import Base\n",
      "from sqlalchemy import Column, String, Text, Date, Integer, ForeignKey, DateTime\n",
      "from sqlalchemy.types import JSON\n",
      "from sqlalchemy.orm import relationship\n",
      "from .association import job_resume_association\n",
      "import datetime\n",
      "\n",
      "\n",
      "class ProcessedJob(Base):\n",
      "    __tablename__ = \"processed_jobs\"\n",
      "\n",
      "    job_id = Column(\n",
      "        String,\n",
      "        ForeignKey(\"jobs.job_id\", ondelete=\"CASCADE\"),\n",
      "        primary_key=True,\n",
      "        index=True,\n",
      "    )\n",
      "    job_title = Column(String, nullable=False)\n",
      "    company_profile = Column(Text, nullable=True)\n",
      "    location = Column(String, nullable=True)\n",
      "    date_posted = Column(Date, nullable=True)\n",
      "    employment_type = Column(String, nullable=True)\n",
      "    job_summary = Column(Text, nullable=False)\n",
      "    key_responsibilities = Column(JSON, nullable=True)\n",
      "    qualifications = Column(JSON, nullable=True)\n",
      "    compensation_and_benfits = Column(JSON, nullable=True)\n",
      "    application_info = Column(JSON, nullable=True)\n",
      "    extracted_keywords = Column(JSON, nullable=True)\n",
      "    processed_at = Column(\n",
      "        DateTime, default=datetime.datetime.now(datetime.timezone.utc)\n",
      "    )\n",
      "\n",
      "    # one-to-many relation between user and jobs\n",
      "    owner_id = Column(Integer, ForeignKey(\"users.id\"), nullable=False)\n",
      "    owner = relationship(\"User\", back_populates=\"processed_jobs\")\n",
      "    raw_job = relationship(\"Job\", back_populates=\"raw_job_association\")\n",
      "\n",
      "    # many-to-many relationship in job and resume\n",
      "    processed_resumes = relationship(\n",
      "        \"ProcessedResume\",\n",
      "        secondary=job_resume_association,\n",
      "        back_populates=\"processed_jobs\",\n",
      "    )\n",
      "\n",
      "\n",
      "class Job(Base):\n",
      "    __tablename__ = \"jobs\"\n",
      "\n",
      "    id = Column(Integer, primary_key=True, index=True)\n",
      "    job_id = Column(String, unique=True, nullable=False)\n",
      "    resume_id = Column(String, ForeignKey(\"resumes.resume_id\"), nullable=False)\n",
      "    content = Column(Text, nullable=False)\n",
      "    created_at = Column(DateTime, default=datetime.datetime.now(datetime.timezone.utc))\n",
      "\n",
      "    raw_job_association = relationship(\n",
      "        \"ProcessedJob\", back_populates=\"raw_job\", uselist=False\n",
      "    )\n",
      "\n",
      "    resumes = relationship(\"Resume\", back_populates=\"jobs\")\n",
      "\n",
      "from .base import Base\n",
      "from sqlalchemy import Column, String, Integer, ForeignKey, Text\n",
      "from sqlalchemy.types import JSON\n",
      "from sqlalchemy.orm import relationship\n",
      "from .association import job_resume_association\n",
      "\n",
      "\n",
      "class ProcessedResume(Base):\n",
      "    __tablename__ = \"processed_resumes\"\n",
      "\n",
      "    resume_id = Column(\n",
      "        String,\n",
      "        ForeignKey(\"resumes.resume_id\", ondelete=\"CASCADE\"),\n",
      "        primary_key=True,\n",
      "        index=True,\n",
      "    )\n",
      "    personal_data = Column(JSON, nullable=False)\n",
      "    experiences = Column(JSON, nullable=True)\n",
      "    projects = Column(JSON, nullable=True)\n",
      "    skills = Column(JSON, nullable=True)\n",
      "    research_work = Column(JSON, nullable=True)\n",
      "    achievements = Column(JSON, nullable=True)\n",
      "    education = Column(JSON, nullable=True)\n",
      "    extracted_keywords = Column(JSON, nullable=True)\n",
      "\n",
      "    owner_id = Column(Integer, ForeignKey(\"users.id\"), nullable=False)\n",
      "    owner = relationship(\"User\", back_populates=\"processed_resumes\")\n",
      "    raw_resume = relationship(\"Resume\", back_populates=\"raw_resume_association\")\n",
      "\n",
      "    processed_jobs = relationship(\n",
      "        \"ProcessedJob\",\n",
      "        secondary=job_resume_association,\n",
      "        back_populates=\"processed_resumes\",\n",
      "    )\n",
      "\n",
      "\n",
      "class Resume(Base):\n",
      "    __tablename__ = \"resumes\"\n",
      "\n",
      "    id = Column(Integer, primary_key=True, index=True)\n",
      "    resume_id = Column(String, unique=True, nullable=False)\n",
      "    content = Column(Text, nullable=False)\n",
      "    content_type = Column(String, nullable=False)\n",
      "\n",
      "    raw_resume_association = relationship(\n",
      "        \"ProcessedResume\", back_populates=\"raw_resume\", uselist=False\n",
      "    )\n",
      "\n",
      "    jobs = relationship(\"Job\", back_populates=\"resumes\")\n",
      "\n",
      "from .base import Base\n",
      "from sqlalchemy import Column, String, Integer\n",
      "from sqlalchemy.orm import relationship\n",
      "\n",
      "\n",
      "class User(Base):\n",
      "    __tablename__ = \"users\"\n",
      "\n",
      "    id = Column(Integer, primary_key=True, index=True)\n",
      "\n",
      "    email = Column(String, unique=True, index=True, nullable=False)\n",
      "    name = Column(String, nullable=False)\n",
      "\n",
      "    processed_resumes = relationship(\n",
      "        \"ProcessedResume\", back_populates=\"owner\", cascade=\"all, delete-orphan\"\n",
      "    )\n",
      "    processed_jobs = relationship(\n",
      "        \"ProcessedJob\", back_populates=\"owner\", cascade=\"all, delete-orphan\"\n",
      "    )\n",
      "\n",
      "import uuid\n",
      "from sqlalchemy.orm import Session\n",
      "from app.models import Job, Resume\n",
      "from typing import List\n",
      "\n",
      "\n",
      "class JobService:\n",
      "    def __init__(self, db: Session):\n",
      "        self.db = db\n",
      "\n",
      "    def create_and_store_job(self, job_data: dict) -> List[str]:\n",
      "        \"\"\"\n",
      "        Stores job data in the database and returns a list of job IDs.\n",
      "\n",
      "        Args:\n",
      "            job_data: JobUploadRequest containing job details.\n",
      "\n",
      "        Returns:\n",
      "            List of job IDs.\n",
      "        \"\"\"\n",
      "        resume_id = job_data.get(\"resume_id\")\n",
      "        print(f\"resume available? {self._is_resume_available(resume_id)}\")\n",
      "        if not self._is_resume_available(resume_id):\n",
      "            raise AssertionError(\n",
      "                f\"resume corresponding to resume_id: {resume_id} not found\"\n",
      "            )\n",
      "\n",
      "        job_ids = []\n",
      "        for job_description in job_data.get(\"job_descriptions\", []):\n",
      "            job_id = str(uuid.uuid4())\n",
      "            job = Job(\n",
      "                job_id=job_id,\n",
      "                resume_id=str(resume_id),\n",
      "                content=job_description,\n",
      "            )\n",
      "            self.db.add(job)\n",
      "            job_ids.append(job_id)\n",
      "\n",
      "        self.db.commit()\n",
      "        return job_ids\n",
      "\n",
      "    def _is_resume_available(self, resume_id: str) -> bool:\n",
      "        \"\"\"\n",
      "        Checks if a resume exists in the database.\n",
      "\n",
      "        Args:\n",
      "            resume_id: ID of the resume to check.\n",
      "\n",
      "        Returns:\n",
      "            True if the resume exists, False otherwise.\n",
      "        \"\"\"\n",
      "        return (\n",
      "            self.db.query(Resume).filter(Resume.resume_id == resume_id).first()\n",
      "            is not None\n",
      "        )\n",
      "\n",
      "import uuid\n",
      "from sqlalchemy.orm import Session\n",
      "from app.models import Job, Resume\n",
      "from typing import List\n",
      "\n",
      "\n",
      "class JobService:\n",
      "    def __init__(self, db: Session):\n",
      "        self.db = db\n",
      "\n",
      "    def create_and_store_job(self, job_data: dict) -> List[str]:\n",
      "        \"\"\"\n",
      "        Stores job data in the database and returns a list of job IDs.\n",
      "\n",
      "        Args:\n",
      "            job_data: JobUploadRequest containing job details.\n",
      "\n",
      "        Returns:\n",
      "            List of job IDs.\n",
      "        \"\"\"\n",
      "        resume_id = str(job_data.get(\"resume_id\"))\n",
      "\n",
      "        if not self._is_resume_available(resume_id):\n",
      "            raise AssertionError(\n",
      "                f\"resume corresponding to resume_id: {resume_id} not found\"\n",
      "            )\n",
      "\n",
      "        job_ids = []\n",
      "        for job_description in job_data.get(\"job_descriptions\", []):\n",
      "            job_id = str(uuid.uuid4())\n",
      "            job = Job(\n",
      "                job_id=job_id,\n",
      "                resume_id=str(resume_id),\n",
      "                content=job_description,\n",
      "            )\n",
      "            self.db.add(job)\n",
      "            job_ids.append(job_id)\n",
      "\n",
      "        self.db.commit()\n",
      "        return job_ids\n",
      "\n",
      "    def _is_resume_available(self, resume_id: str) -> bool:\n",
      "        \"\"\"\n",
      "        Checks if a resume exists in the database.\n",
      "\n",
      "        Args:\n",
      "            resume_id: ID of the resume to check.\n",
      "\n",
      "        Returns:\n",
      "            True if the resume exists, False otherwise.\n",
      "        \"\"\"\n",
      "        return (\n",
      "            self.db.query(Resume).filter(Resume.resume_id == resume_id).first()\n",
      "            is not None\n",
      "        )\n",
      "\n",
      "# Generic async LLM-agent - automatic provider selection\n",
      "\n",
      "# * If caller supplies `openai_api_key` (arg or ENV), we use OpenAIProvider.\n",
      "# * Else we fallback to a local Ollama model.\n",
      "# * If neither is available, we raise -> ProviderError.\n",
      "\n",
      "from .manager import AgentManager\n",
      "\n",
      "__all__ = [\"AgentManager\"]\n",
      "\n",
      "class ProviderError(RuntimeError):\n",
      "    \"\"\"Raised when the underlying LLM provider fails\"\"\"\n",
      "\n",
      "\n",
      "class StrategyError(RuntimeError):\n",
      "    \"\"\"Raised when a Strategy cannot parse/return expected output\"\"\"\n",
      "\n",
      "import os\n",
      "from typing import Dict, Any\n",
      "\n",
      "from .providers.ollama import OllamaProvider\n",
      "from .providers.openai import OpenAIProvider\n",
      "from .strategies.base import Strategy\n",
      "from .strategies.wrapper import JSONWrapper\n",
      "from .exceptions import ProviderError\n",
      "\n",
      "\n",
      "class AgentManager:\n",
      "    def __init__(\n",
      "        self, strategy: Strategy | None = None, model: str = \"gemma3:4b\"\n",
      "    ) -> None:\n",
      "        self.strategy = strategy or JSONWrapper()\n",
      "        self.model = model\n",
      "\n",
      "    async def _get_provider(self, **kwargs: Any) -> OllamaProvider | OpenAIProvider:\n",
      "        api_key = kwargs.get(\"openai_api_key\", os.getenv(\"OPENAI_API_KEY\"))\n",
      "        if api_key:\n",
      "            return OpenAIProvider(api_key=api_key)\n",
      "\n",
      "        model = kwargs.get(\"model\", self.model)\n",
      "        installed_ollama_models = await OllamaProvider.get_installed_models()\n",
      "        if model not in installed_ollama_models:\n",
      "            raise ProviderError(\n",
      "                f\"Ollama Model '{model}' is not found. Run `ollama pull {model} or pick from any available models {installed_ollama_models}\"\n",
      "            )\n",
      "        return OllamaProvider(model_name=model)\n",
      "\n",
      "    async def run(self, prompt: str, **kwargs: Any) -> Dict[str, Any]:\n",
      "        \"\"\"\n",
      "        Run the agent with the given prompt and generation arguments.\n",
      "        \"\"\"\n",
      "        provider = await self._get_provider(**kwargs)\n",
      "        return await self.strategy(prompt, provider, **kwargs)\n",
      "\n",
      "from abc import ABC, abstractmethod\n",
      "from typing import Any\n",
      "\n",
      "\n",
      "class Provider(ABC):\n",
      "    \"\"\"\n",
      "    Abstract base class for providers.\n",
      "    \"\"\"\n",
      "\n",
      "    @abstractmethod\n",
      "    async def __call__(self, prompt: str, **generation_args: Any) -> str: ...\n",
      "\n",
      "import asyncio\n",
      "import logging\n",
      "import ollama\n",
      "from typing import Any, Dict, List, Optional\n",
      "\n",
      "from .base import Provider\n",
      "from ..exceptions import ProviderError\n",
      "\n",
      "logger = logging.getLogger(__name__)\n",
      "\n",
      "\n",
      "class OllamaProvider(Provider):\n",
      "    def __init__(self, model_name: str = \"gemma3:4b\", host: Optional[str] = None):\n",
      "        self.model = model_name\n",
      "        self._client = ollama.Client(host=host) if host else ollama.Client()\n",
      "\n",
      "    @staticmethod\n",
      "    async def get_installed_models(host: Optional[str] = None) -> List[str]:\n",
      "        \"\"\"\n",
      "        List all installed models.\n",
      "        \"\"\"\n",
      "\n",
      "        def _list_sync() -> List[str]:\n",
      "            client = ollama.Client(host=host) if host else ollama.Client()\n",
      "            return [model[\"name\"] for model in client.list()]\n",
      "\n",
      "        return await asyncio.to_thread(_list_sync)\n",
      "\n",
      "    def _generate_sync(self, prompt: str, options: Dict[str, Any]) -> str:\n",
      "        \"\"\"\n",
      "        Generate a response from the model.\n",
      "        \"\"\"\n",
      "        try:\n",
      "            response = self._client.generate(\n",
      "                prompt=prompt,\n",
      "                model=self.model,\n",
      "                options=options,\n",
      "            )\n",
      "            return response[\"response\"].strip()\n",
      "        except Exception as e:\n",
      "            logger.error(f\"ollama sync error: {e}\")\n",
      "            raise ProviderError(f\"Ollama - Error generating response: {e}\")\n",
      "\n",
      "    async def __call__(self, prompt: str, **generation_args: Any) -> str:\n",
      "        opts = {\n",
      "            \"temperature\": generation_args.get(\"temperature\", 0),\n",
      "            \"top_p\": generation_args.get(\"top_p\", 0.9),\n",
      "            \"top_k\": generation_args.get(\"top_k\", 40),\n",
      "            \"num_ctx\": generation_args.get(\"max_length\", 20000),\n",
      "        }\n",
      "        return await asyncio.to_thread(self._generate_sync, prompt, opts)\n",
      "\n",
      "import os\n",
      "import asyncio\n",
      "import logging\n",
      "from openai import OpenAI\n",
      "from typing import Any, Dict\n",
      "\n",
      "from .base import Provider\n",
      "from ..exceptions import ProviderError\n",
      "\n",
      "logger = logging.getLogger(__name__)\n",
      "\n",
      "\n",
      "class OpenAIProvider(Provider):\n",
      "    def __init__(self, api_key: str | None = None, model: str = \"gpt-4o\"):\n",
      "        api_key = api_key or os.getenv(\"OPENAI_API_KEY\")\n",
      "        if not api_key:\n",
      "            raise ProviderError(\"OpenAI API key is missing\")\n",
      "        self._client = OpenAI(api_key=api_key)\n",
      "        self.model = model\n",
      "        self.instructions = \"\"\n",
      "\n",
      "    def _generate_sync(self, prompt: str, options: Dict[str, Any]) -> str:\n",
      "        try:\n",
      "            response = self._client.responses.create(\n",
      "                model=self.model,\n",
      "                instructions=self.instructions,\n",
      "                input=prompt,\n",
      "                **options,\n",
      "            )\n",
      "            return response.output_text\n",
      "        except Exception as e:\n",
      "            raise ProviderError(f\"OpenAI - error generating response: {e}\") from e\n",
      "\n",
      "    async def __call__(self, prompt: str, **generation_args: Any) -> str:\n",
      "        opts = {\n",
      "            \"temperature\": generation_args.get(\"temperature\", 0),\n",
      "            \"top_p\": generation_args.get(\"top_p\", 0.9),\n",
      "            \"top_k\": generation_args.get(\"top_k\", 40),\n",
      "            \"max_tokens\": generation_args.get(\"max_length\", 20000),\n",
      "        }\n",
      "        return await asyncio.to_thread(self._generate_sync, prompt, opts)\n",
      "\n",
      "from abc import ABC, abstractmethod\n",
      "from typing import Any, Dict\n",
      "\n",
      "from ..providers.base import Provider\n",
      "\n",
      "\n",
      "class Strategy(ABC):\n",
      "    @abstractmethod\n",
      "    async def __call__(\n",
      "        self, prompt: str, provider: Provider, **generation_args: Any\n",
      "    ) -> Dict[str, Any]:\n",
      "        \"\"\"\n",
      "        Abstract method which should be used to define the strategy for generating a response from LLM.\n",
      "\n",
      "        Args:\n",
      "            prompt (str): The input prompt for the provider.\n",
      "            provider (Provider): The provider instance to use for generation.\n",
      "            **generation_args (Any): Additional arguments for generation.\n",
      "\n",
      "        Returns:\n",
      "            Dict[str, Any]: The generated response and any additional information.\n",
      "        \"\"\"\n",
      "        ...\n",
      "\n",
      "import json\n",
      "import logging\n",
      "from typing import Any, Dict\n",
      "\n",
      "from ..providers.base import Provider\n",
      "from ..exceptions import StrategyError\n",
      "from .base import Strategy\n",
      "\n",
      "logger = logging.getLogger(__name__)\n",
      "\n",
      "\n",
      "class JSONWrapper(Strategy):\n",
      "    async def __call__(\n",
      "        self, prompt: str, provider: Provider, **generation_args: Any\n",
      "    ) -> Dict[str, Any]:\n",
      "        \"\"\"\n",
      "        Wrapper strategy to format the prompt as JSON with the help of LLM.\n",
      "        \"\"\"\n",
      "        response = await provider(prompt, **generation_args)\n",
      "        try:\n",
      "            return json.loads(response)\n",
      "        except json.JSONDecodeError as e:\n",
      "            logger.error(\n",
      "                f\"provider returned non-JSON. parsing error: {e} - response: {response}\"\n",
      "            )\n",
      "            raise StrategyError(f\"JSON parsing error: {e}\") from e\n",
      "\n",
      "from fastapi import APIRouter, HTTPException, Depends, Request, status\n",
      "from sqlalchemy.orm import Session\n",
      "from uuid import uuid4\n",
      "from app.core import get_db_session\n",
      "from app.schemas.pydantic.job import JobUploadRequest\n",
      "from app.services import JobService\n",
      "\n",
      "job_router = APIRouter()\n",
      "\n",
      "\n",
      "@job_router.post(\n",
      "    \"/upload\",\n",
      "    summary=\"stores the job posting in the database by parsing the JD into a structured format JSON\",\n",
      ")\n",
      "async def upload_job(\n",
      "    payload: JobUploadRequest,\n",
      "    request: Request,\n",
      "    db: Session = Depends(get_db_session),\n",
      "):\n",
      "    \"\"\"\n",
      "    Accepts a job description as a MarkDown text and stores it in the database.\n",
      "    \"\"\"\n",
      "    request_id = getattr(request.state, \"request_id\", str(uuid4()))\n",
      "\n",
      "    allowed_content_types = [\n",
      "        \"application/json\",\n",
      "    ]\n",
      "\n",
      "    content_type = request.headers.get(\"content-type\")\n",
      "    if not content_type:\n",
      "        raise HTTPException(\n",
      "            status_code=status.HTTP_400_BAD_REQUEST,\n",
      "            detail=\"Content-Type header is missing\",\n",
      "        )\n",
      "\n",
      "    if content_type not in allowed_content_types:\n",
      "        raise HTTPException(\n",
      "            status_code=400,\n",
      "            detail=f\"Invalid Content-Type. Only {', '.join(allowed_content_types)} is/are allowed.\",\n",
      "        )\n",
      "\n",
      "    try:\n",
      "        job_service = JobService(db)\n",
      "        job_ids = job_service.create_and_store_job(payload.model_dump())\n",
      "\n",
      "    except AssertionError as e:\n",
      "        raise HTTPException(\n",
      "            status_code=status.HTTP_400_BAD_REQUEST,\n",
      "            detail=str(e),\n",
      "        )\n",
      "\n",
      "    except Exception as e:\n",
      "        raise HTTPException(\n",
      "            status_code=status.HTTP_500_INTERNAL_SERVER_ERROR,\n",
      "            detail=f\"{str(e)}\",\n",
      "        )\n",
      "\n",
      "    return {\n",
      "        \"message\": \"data successfully processed\",\n",
      "        \"job_id\": job_ids,\n",
      "        \"request\": {\n",
      "            \"request_id\": request_id,\n",
      "            \"payload\": payload,\n",
      "        },\n",
      "    }\n",
      "\n",
      "import os\n",
      "from fastapi import FastAPI, HTTPException\n",
      "from fastapi.exceptions import RequestValidationError\n",
      "from fastapi.staticfiles import StaticFiles\n",
      "from fastapi.middleware.cors import CORSMiddleware\n",
      "from starlette.middleware.sessions import SessionMiddleware\n",
      "from app.api import health_check, v1_router, RequestIDMiddleware\n",
      "from app.core import (\n",
      "    settings,\n",
      "    db_singleton,\n",
      "    setup_logging,\n",
      "    custom_http_exception_handler,\n",
      "    validation_exception_handler,\n",
      "    unhandled_exception_handler,\n",
      ")\n",
      "from app.models import Base\n",
      "\n",
      "\n",
      "def create_app() -> FastAPI:\n",
      "    \"\"\"\n",
      "    configure and create the FastAPI application instance.\n",
      "    \"\"\"\n",
      "    setup_logging()\n",
      "\n",
      "    app = FastAPI(\n",
      "        title=settings.PROJECT_NAME,\n",
      "        docs_url=\"/api/docs\",\n",
      "        openapi_url=\"/api/openapi.json\",\n",
      "    )\n",
      "\n",
      "    app.add_middleware(\n",
      "        SessionMiddleware, secret_key=settings.SESSION_SECRET_KEY, same_site=\"lax\"\n",
      "    )\n",
      "    app.add_middleware(\n",
      "        CORSMiddleware,\n",
      "        allow_origins=settings.ALLOWED_ORIGINS,\n",
      "        allow_credentials=True,\n",
      "        allow_methods=[\"*\"],\n",
      "        allow_headers=[\"*\"],\n",
      "    )\n",
      "    app.add_middleware(RequestIDMiddleware)\n",
      "\n",
      "    app.add_exception_handler(HTTPException, custom_http_exception_handler)\n",
      "    app.add_exception_handler(RequestValidationError, validation_exception_handler)\n",
      "    app.add_exception_handler(Exception, unhandled_exception_handler)\n",
      "\n",
      "    Base.metadata.create_all(bind=db_singleton.engine)\n",
      "\n",
      "    if os.path.exists(settings.FRONTEND_PATH):\n",
      "        app.mount(\n",
      "            \"/app\",\n",
      "            StaticFiles(directory=settings.FRONTEND_PATH, html=True),\n",
      "            name=settings.PROJECT_NAME,\n",
      "        )\n",
      "\n",
      "    app.include_router(health_check)\n",
      "    app.include_router(v1_router)\n",
      "\n",
      "    return app\n",
      "\n",
      "from .database import DatabaseConnectionSingleton\n",
      "from .config import settings, setup_logging\n",
      "from .exceptions import (\n",
      "    custom_http_exception_handler,\n",
      "    validation_exception_handler,\n",
      "    unhandled_exception_handler,\n",
      ")\n",
      "\n",
      "db_singleton = DatabaseConnectionSingleton(settings.DATABASE_URL)\n",
      "\n",
      "\n",
      "def get_db_session():\n",
      "    \"\"\"\n",
      "    Dependency that provides a database session for the request.\n",
      "    It ensures that the session is closed after the request is completed.\n",
      "\n",
      "    Yields:\n",
      "        db (Session): An active database session object.\n",
      "    \"\"\"\n",
      "    # Create a new session for each request\n",
      "    db = db_singleton.get_session()\n",
      "    try:\n",
      "        yield db\n",
      "    finally:\n",
      "        db.close()\n",
      "\n",
      "\n",
      "__all__ = [\n",
      "    \"settings\",\n",
      "    \"db_singleton\",\n",
      "    \"setup_logging\",\n",
      "    \"get_db_session\",\n",
      "    \"custom_http_exception_handler\",\n",
      "    \"validation_exception_handler\",\n",
      "    \"unhandled_exception_handler\",\n",
      "]\n",
      "\n",
      "import os\n",
      "import sys\n",
      "import logging\n",
      "from pydantic_settings import BaseSettings\n",
      "from typing import List, Optional, Literal\n",
      "\n",
      "\n",
      "class Settings(BaseSettings):\n",
      "    PROJECT_NAME: str = \"Resume Matcher\"\n",
      "    FRONTEND_PATH: str = os.path.join(os.path.dirname(__file__), \"frontend\", \"assets\")\n",
      "    ALLOWED_ORIGINS: List[str] = [\"https://www.resumematcher.fyi\"]\n",
      "    DATABASE_URL: Optional[str]\n",
      "    SESSION_SECRET_KEY: Optional[str]\n",
      "    PYTHONDONTWRITEBYTECODE: int = 1\n",
      "\n",
      "    class Config:\n",
      "        env_file = \".env\"\n",
      "\n",
      "\n",
      "settings = Settings()\n",
      "\n",
      "\n",
      "_LEVEL_BY_ENV: dict[Literal[\"production\", \"staging\", \"local\"], int] = {\n",
      "    \"production\": logging.INFO,\n",
      "    \"staging\": logging.DEBUG,\n",
      "    \"local\": logging.DEBUG,\n",
      "}\n",
      "\n",
      "\n",
      "def setup_logging() -> None:\n",
      "    \"\"\"\n",
      "    Configure the root logger exactly once,\n",
      "\n",
      "    * Console only (StreamHandler -> stderr)\n",
      "    * ISO - 8601 timestamps\n",
      "    * Env - based log level: production -> INFO, else DEBUG\n",
      "    * Prevents duplicate handler creation if called twice\n",
      "    \"\"\"\n",
      "    root = logging.getLogger()\n",
      "    if root.handlers:\n",
      "        return\n",
      "\n",
      "    env = settings.ENV.lower() if hasattr(settings, \"ENV\") else \"production\"\n",
      "    level = _LEVEL_BY_ENV.get(env, logging.INFO)\n",
      "\n",
      "    formatter = logging.Formatter(\n",
      "        fmt=\"[%(asctime)s - %(name)s - %(levelname)s] %(message)s\",\n",
      "        datefmt=\"%Y-%m-%dT%H:%M:%S%z\",\n",
      "    )\n",
      "\n",
      "    handler = logging.StreamHandler(sys.stderr)\n",
      "    handler.setFormatter(formatter)\n",
      "\n",
      "    root.setLevel(level)\n",
      "    root.addHandler(handler)\n",
      "\n",
      "    for noisy in (\"sqlalchemy.engine\", \"uvicorn.access\"):\n",
      "        logging.getLogger(noisy).setLevel(logging.WARNING)\n",
      "\n",
      "from .base import PromptFactory\n",
      "\n",
      "prompt_factory = PromptFactory()\n",
      "__all__ = [\"prompt_factory\"]\n",
      "\n",
      "import pkgutil\n",
      "import importlib\n",
      "from typing import Dict\n",
      "\n",
      "from app.prompt import __path__ as prompt_pkg_path\n",
      "\n",
      "\n",
      "class PromptFactory:\n",
      "    def __init__(self) -> None:\n",
      "        self._prompts: Dict[str, str] = {}\n",
      "        self._discover()\n",
      "\n",
      "    def _discover(self) -> None:\n",
      "        for finder, module_name, ispkg in pkgutil.iter_modules(prompt_pkg_path):\n",
      "            if module_name.startswith(\"_\") or module_name == \"base\":\n",
      "                continue\n",
      "\n",
      "            module = importlib.import_module(f\"app.prompt.{module_name}\")\n",
      "            if hasattr(module, \"PROMPT\"):\n",
      "                self._prompts[module_name] = getattr(module, \"PROMPT\")\n",
      "\n",
      "    def list_prompts(self) -> Dict[str, str]:\n",
      "        return self._prompts\n",
      "\n",
      "    def get(self, name: str) -> str:\n",
      "        try:\n",
      "            return self._prompts[name]\n",
      "        except KeyError:\n",
      "            raise KeyError(\n",
      "                f\"Prompt '{name}' not found. Available prompts: {list(self._prompts.keys())}\"\n",
      "            )\n",
      "\n",
      "PROMPT = f\"\"\"\n",
      "You are a JSON extraction engine. Convert the following resume text into precisely the JSON schema specified below.\n",
      "- Do not compose any extra fields or commentary.\n",
      "- Do not make up values for any fields.\n",
      "- User \"Present\" if an end date is ongoing.\n",
      "- Make sure dates are in YYYY-MM-DD.\n",
      "\n",
      "Schema:\n",
      "```json\n",
      "{0}\n",
      "```\n",
      "\n",
      "Resume:\n",
      "```text\n",
      "{1}\n",
      "```\n",
      "\n",
      "NOTE: Please output only a valid JSON matching the EXACT schema.\n",
      "\"\"\"\n",
      "\n",
      "from .base import JSONSchemaFactory\n",
      "\n",
      "json_schema_factory = JSONSchemaFactory()\n",
      "__all__ = [\"json_schema_factory\"]\n",
      "\n",
      "import pkgutil\n",
      "import importlib\n",
      "from typing import Dict\n",
      "\n",
      "from app.schemas.json import __path__ as schema_pkg_path\n",
      "\n",
      "\n",
      "class JSONSchemaFactory:\n",
      "    def __init__(self) -> None:\n",
      "        self._schema: Dict[str, str] = {}\n",
      "        self._discover()\n",
      "\n",
      "    def _discover(self) -> None:\n",
      "        for finder, module_name, ispkg in pkgutil.iter_modules(schema_pkg_path):\n",
      "            if module_name.startswith(\"_\") or module_name == \"base\":\n",
      "                continue\n",
      "\n",
      "            module = importlib.import_module(f\"app.prompt.{module_name}\")\n",
      "            if hasattr(module, \"PROMPT\"):\n",
      "                self._schema[module_name] = getattr(module, \"PROMPT\")\n",
      "\n",
      "    def list_prompts(self) -> Dict[str, str]:\n",
      "        return self._schema\n",
      "\n",
      "    def get(self, name: str) -> str:\n",
      "        try:\n",
      "            return self._schema[name]\n",
      "        except KeyError:\n",
      "            raise KeyError(\n",
      "                f\"Prompt '{name}' not found. Available prompts: {list(self._schema.keys())}\"\n",
      "            )\n",
      "\n",
      "SCHEMA = {\n",
      "    \"UUID\": \"string\",\n",
      "    \"Personal Data\": {\n",
      "        \"firstName\": \"string\",\n",
      "        \"lastName\": \"string\",\n",
      "        \"email\": \"string\",\n",
      "        \"phone\": \"string\",\n",
      "        \"linkedin\": \"string\",\n",
      "        \"portfolio\": \"string\",\n",
      "        \"location\": {\"city\": \"string\", \"country\": \"string\"},\n",
      "    },\n",
      "    \"Experiences\": [\n",
      "        {\n",
      "            \"jobTitle\": \"string\",\n",
      "            \"company\": \"string\",\n",
      "            \"location\": \"string\",\n",
      "            \"startDate\": \"YYYY-MM-DD\",\n",
      "            \"endDate\": \"YYYY-MM-DD or Present\",\n",
      "            \"description\": [\"string\", \"...\"],\n",
      "            \"technologiesUsed\": [\"string\", \"...\"],\n",
      "        }\n",
      "    ],\n",
      "    \"Projects\": [\n",
      "        {\n",
      "            \"projectName\": \"string\",\n",
      "            \"description\": \"string\",\n",
      "            \"technologiesUsed\": [\"string\", \"...\"],\n",
      "            \"link\": \"string\",\n",
      "            \"startDate\": \"YYYY-MM-DD\",\n",
      "            \"endDate\": \"YYYY-MM-DD\",\n",
      "        }\n",
      "    ],\n",
      "    \"Skills\": [{\"category\": \"string\", \"skillName\": \"string\"}],\n",
      "    \"Research Work\": [\n",
      "        {\n",
      "            \"title\": \"string\",\n",
      "            \"publication\": \"string\",\n",
      "            \"date\": \"YYYY-MM-DD\",\n",
      "            \"link\": \"string\",\n",
      "            \"description\": \"string\",\n",
      "        }\n",
      "    ],\n",
      "    \"Achievements\": [\"string\", \"...\"],\n",
      "    \"Education\": [\n",
      "        {\n",
      "            \"institution\": \"string\",\n",
      "            \"degree\": \"string\",\n",
      "            \"fieldOfStudy\": \"string\",\n",
      "            \"startDate\": \"YYYY-MM-DD\",\n",
      "            \"endDate\": \"YYYY-MM-DD\",\n",
      "            \"grade\": \"string\",\n",
      "            \"description\": \"string\",\n",
      "        }\n",
      "    ],\n",
      "    \"Extracted Keywords\": [\"string\", \"...\"],\n",
      "}\n",
      "\n",
      "File app/schemas/pydantic/job.py has no source code\n",
      "\n",
      "import os\n",
      "import uuid\n",
      "import json\n",
      "import tempfile\n",
      "\n",
      "from typing import Dict, Any\n",
      "from markitdown import MarkItDown\n",
      "from sqlalchemy.orm import Session\n",
      "\n",
      "from app.models import Resume\n",
      "from app.agent import AgentManager\n",
      "from app.prompt import prompt_factory\n",
      "from app.schemas.json import json_schema_factory\n",
      "\n",
      "\n",
      "class ResumeService:\n",
      "    def __init__(self, db: Session):\n",
      "        self.db = db\n",
      "        self.md = MarkItDown(enable_plugins=False)\n",
      "        self.json_agent_manager = AgentManager(model=\"gemma3:12b\")\n",
      "\n",
      "    def convert_and_store_resume(\n",
      "        self, file_bytes: bytes, file_type: str, filename: str, content_type: str = \"md\"\n",
      "    ):\n",
      "        \"\"\"\n",
      "        Converts resume file (PDF/DOCX) to text using MarkItDown and stores it in the database.\n",
      "\n",
      "        Args:\n",
      "            file_bytes: Raw bytes of the uploaded file\n",
      "            file_type: MIME type of the file (\"application/pdf\" or \"application/vnd.openxmlformats-officedocument.wordprocessingml.document\")\n",
      "            filename: Original filename\n",
      "            content_type: Output format (\"md\" for markdown or \"html\")\n",
      "\n",
      "        Returns:\n",
      "            None\n",
      "        \"\"\"\n",
      "        with tempfile.NamedTemporaryFile(\n",
      "            delete=False, suffix=self._get_file_extension(file_type)\n",
      "        ) as temp_file:\n",
      "            temp_file.write(file_bytes)\n",
      "            temp_path = temp_file.name\n",
      "\n",
      "        try:\n",
      "            result = self.md.convert(temp_path)\n",
      "            text_content = result.text_content\n",
      "            resume_id = str(uuid.uuid4())\n",
      "            self._store_resume_in_db(resume_id, filename, text_content, content_type)\n",
      "\n",
      "            return resume_id\n",
      "\n",
      "        finally:\n",
      "            if os.path.exists(temp_path):\n",
      "                os.remove(temp_path)\n",
      "\n",
      "    def _get_file_extension(self, file_type: str) -> str:\n",
      "        \"\"\"Returns the appropriate file extension based on MIME type\"\"\"\n",
      "        if file_type == \"application/pdf\":\n",
      "            return \".pdf\"\n",
      "        elif (\n",
      "            file_type\n",
      "            == \"application/vnd.openxmlformats-officedocument.wordprocessingml.document\"\n",
      "        ):\n",
      "            return \".docx\"\n",
      "        return \"\"\n",
      "\n",
      "    def _store_resume_in_db(\n",
      "        self, resume_id: str, filename: str, text_content: str, content_type: str\n",
      "    ):\n",
      "        \"\"\"\n",
      "        Stores the parsed resume content in the database.\n",
      "        \"\"\"\n",
      "        resume = Resume(\n",
      "            resume_id=resume_id, content=text_content, content_type=content_type\n",
      "        )\n",
      "\n",
      "        self.db.add(resume)\n",
      "        self.db.commit()\n",
      "\n",
      "        return resume\n",
      "\n",
      "    async def _extract_structured_json(self, resume_text: str) -> Dict[str, Any]:\n",
      "        \"\"\"\n",
      "        Uses the AgentManager+JSONWrapper to ask the LLM to\n",
      "        return the data in exact JSON schema we need.\n",
      "        \"\"\"\n",
      "        prompt_template = prompt_factory.get(\"structured_resume\")\n",
      "        prompt = prompt_template.format(\n",
      "            schema=json.dumps(json_schema_factory.get(\"structured_resume\"), indent=2),\n",
      "            resume_text=resume_text,\n",
      "        )\n",
      "\n",
      "        return await self.json_agent_manager.run(prompt=prompt)\n",
      "\n",
      "import asyncio\n",
      "import logging\n",
      "import ollama\n",
      "from typing import Any, Dict, List, Optional\n",
      "\n",
      "from .base import Provider\n",
      "from ..exceptions import ProviderError\n",
      "\n",
      "logger = logging.getLogger(__name__)\n",
      "\n",
      "\n",
      "class OllamaProvider(Provider):\n",
      "    def __init__(self, model_name: str = \"gemma3:4b\", host: Optional[str] = None):\n",
      "        self.model = model_name\n",
      "        self._client = ollama.Client(host=host) if host else ollama.Client()\n",
      "\n",
      "    @staticmethod\n",
      "    async def get_installed_models(host: Optional[str] = None) -> List[str]:\n",
      "        \"\"\"\n",
      "        List all installed models.\n",
      "        \"\"\"\n",
      "\n",
      "        def _list_sync() -> List[str]:\n",
      "            client = ollama.Client(host=host) if host else ollama.Client()\n",
      "            return [model_class.model for model_class in client.list().models]\n",
      "\n",
      "        return await asyncio.to_thread(_list_sync)\n",
      "\n",
      "    def _generate_sync(self, prompt: str, options: Dict[str, Any]) -> str:\n",
      "        \"\"\"\n",
      "        Generate a response from the model.\n",
      "        \"\"\"\n",
      "        try:\n",
      "            response = self._client.generate(\n",
      "                prompt=prompt,\n",
      "                model=self.model,\n",
      "                options=options,\n",
      "            )\n",
      "            return response[\"response\"].strip()\n",
      "        except Exception as e:\n",
      "            logger.error(f\"ollama sync error: {e}\")\n",
      "            raise ProviderError(f\"Ollama - Error generating response: {e}\")\n",
      "\n",
      "    async def __call__(self, prompt: str, **generation_args: Any) -> str:\n",
      "        opts = {\n",
      "            \"temperature\": generation_args.get(\"temperature\", 0),\n",
      "            \"top_p\": generation_args.get(\"top_p\", 0.9),\n",
      "            \"top_k\": generation_args.get(\"top_k\", 40),\n",
      "            \"num_ctx\": generation_args.get(\"max_length\", 20000),\n",
      "        }\n",
      "        return await asyncio.to_thread(self._generate_sync, prompt, opts)\n",
      "\n",
      "import json\n",
      "import logging\n",
      "from typing import Any, Dict\n",
      "\n",
      "from ..providers.base import Provider\n",
      "from ..exceptions import StrategyError\n",
      "from .base import Strategy\n",
      "\n",
      "logger = logging.getLogger(__name__)\n",
      "\n",
      "\n",
      "class JSONWrapper(Strategy):\n",
      "    async def __call__(\n",
      "        self, prompt: str, provider: Provider, **generation_args: Any\n",
      "    ) -> Dict[str, Any]:\n",
      "        \"\"\"\n",
      "        Wrapper strategy to format the prompt as JSON with the help of LLM.\n",
      "        \"\"\"\n",
      "        response = await provider(prompt, **generation_args)\n",
      "        response = response.replace(\"```\", \"\").replace(\"json\", \"\").strip()\n",
      "        logger.info(f\"provider response: {response}\")\n",
      "        try:\n",
      "            return json.loads(response)\n",
      "        except json.JSONDecodeError as e:\n",
      "            logger.error(\n",
      "                f\"provider returned non-JSON. parsing error: {e} - response: {response}\"\n",
      "            )\n",
      "            raise StrategyError(f\"JSON parsing error: {e}\") from e\n",
      "\n",
      "from fastapi import APIRouter, File, UploadFile, HTTPException, Depends, Request, status\n",
      "from sqlalchemy.orm import Session\n",
      "import logging\n",
      "import traceback\n",
      "from uuid import uuid4\n",
      "from app.services.resume_service import ResumeService\n",
      "from app.core import get_db_session\n",
      "\n",
      "resume_router = APIRouter()\n",
      "logger = logging.getLogger(__name__)\n",
      "\n",
      "\n",
      "@resume_router.post(\n",
      "    \"/upload\",\n",
      "    summary=\"Upload a resume in PDF or DOCX format and store it into DB in HTML/Markdown format\",\n",
      ")\n",
      "async def upload_resume(\n",
      "    request: Request,\n",
      "    file: UploadFile = File(...),\n",
      "    db: Session = Depends(get_db_session),\n",
      "):\n",
      "    \"\"\"\n",
      "    Accepts a PDF or DOCX file, converts it to HTML/Markdown, and stores it in the database.\n",
      "\n",
      "    Raises:\n",
      "        HTTPException: If the file type is not supported or if the file is empty.\n",
      "    \"\"\"\n",
      "    request_id = getattr(request.state, \"request_id\", str(uuid4()))\n",
      "\n",
      "    allowed_content_types = [\n",
      "        \"application/pdf\",\n",
      "        \"application/vnd.openxmlformats-officedocument.wordprocessingml.document\",\n",
      "    ]\n",
      "\n",
      "    if file.content_type not in allowed_content_types:\n",
      "        raise HTTPException(\n",
      "            status_code=status.HTTP_400_BAD_REQUEST,\n",
      "            detail=\"Invalid file type. Only PDF and DOCX files are allowed.\",\n",
      "        )\n",
      "\n",
      "    file_bytes = await file.read()\n",
      "    if not file_bytes:\n",
      "        raise HTTPException(\n",
      "            status_code=status.HTTP_400_BAD_REQUEST,\n",
      "            detail=\"Empty file. Please upload a valid file.\",\n",
      "        )\n",
      "\n",
      "    try:\n",
      "        resume_service = ResumeService(db)\n",
      "        resume_id = await resume_service.convert_and_store_resume(\n",
      "            file_bytes=file_bytes,\n",
      "            file_type=file.content_type,\n",
      "            filename=file.filename,\n",
      "            content_type=\"md\",\n",
      "        )\n",
      "    except Exception as e:\n",
      "        logger.error(\n",
      "            f\"Error processing file: {str(e)} - traceback: {traceback.format_exc()}\"\n",
      "        )\n",
      "        raise HTTPException(\n",
      "            status_code=status.HTTP_500_INTERNAL_SERVER_ERROR,\n",
      "            detail=f\"Error processing file: {str(e)}\",\n",
      "        )\n",
      "\n",
      "    return {\n",
      "        \"message\": f\"File {file.filename} successfully processed as MD and stored in the DB\",\n",
      "        \"request_id\": request_id,\n",
      "        \"resume_id\": resume_id,\n",
      "    }\n",
      "\n",
      "from .base import Base\n",
      "from sqlalchemy import Column, String, Text, Date, Integer, ForeignKey, DateTime\n",
      "from sqlalchemy.types import JSON\n",
      "from sqlalchemy.orm import relationship\n",
      "from .association import job_resume_association\n",
      "import datetime\n",
      "\n",
      "\n",
      "class ProcessedJob(Base):\n",
      "    __tablename__ = \"processed_jobs\"\n",
      "\n",
      "    job_id = Column(\n",
      "        String,\n",
      "        ForeignKey(\"jobs.job_id\", ondelete=\"CASCADE\"),\n",
      "        primary_key=True,\n",
      "        index=True,\n",
      "    )\n",
      "    job_title = Column(String, nullable=False)\n",
      "    company_profile = Column(Text, nullable=True)\n",
      "    location = Column(String, nullable=True)\n",
      "    date_posted = Column(Date, nullable=True)\n",
      "    employment_type = Column(String, nullable=True)\n",
      "    job_summary = Column(Text, nullable=False)\n",
      "    key_responsibilities = Column(JSON, nullable=True)\n",
      "    qualifications = Column(JSON, nullable=True)\n",
      "    compensation_and_benfits = Column(JSON, nullable=True)\n",
      "    application_info = Column(JSON, nullable=True)\n",
      "    extracted_keywords = Column(JSON, nullable=True)\n",
      "    processed_at = Column(\n",
      "        DateTime, default=datetime.datetime.now(datetime.timezone.utc)\n",
      "    )\n",
      "\n",
      "    # one-to-many relation between user and jobs\n",
      "    # owner_id = Column(Integer, ForeignKey(\"users.id\"), nullable=False)\n",
      "    # owner = relationship(\"User\", back_populates=\"processed_jobs\")\n",
      "    raw_job = relationship(\"Job\", back_populates=\"raw_job_association\")\n",
      "\n",
      "    # many-to-many relationship in job and resume\n",
      "    processed_resumes = relationship(\n",
      "        \"ProcessedResume\",\n",
      "        secondary=job_resume_association,\n",
      "        back_populates=\"processed_jobs\",\n",
      "    )\n",
      "\n",
      "\n",
      "class Job(Base):\n",
      "    __tablename__ = \"jobs\"\n",
      "\n",
      "    id = Column(Integer, primary_key=True, index=True)\n",
      "    job_id = Column(String, unique=True, nullable=False)\n",
      "    resume_id = Column(String, ForeignKey(\"resumes.resume_id\"), nullable=False)\n",
      "    content = Column(Text, nullable=False)\n",
      "    created_at = Column(DateTime, default=datetime.datetime.now(datetime.timezone.utc))\n",
      "\n",
      "    raw_job_association = relationship(\n",
      "        \"ProcessedJob\", back_populates=\"raw_job\", uselist=False\n",
      "    )\n",
      "\n",
      "    resumes = relationship(\"Resume\", back_populates=\"jobs\")\n",
      "\n",
      "from .base import Base\n",
      "from sqlalchemy import Column, String, Integer, ForeignKey, Text\n",
      "from sqlalchemy.types import JSON\n",
      "from sqlalchemy.orm import relationship\n",
      "from .association import job_resume_association\n",
      "\n",
      "\n",
      "class ProcessedResume(Base):\n",
      "    __tablename__ = \"processed_resumes\"\n",
      "\n",
      "    resume_id = Column(\n",
      "        String,\n",
      "        ForeignKey(\"resumes.resume_id\", ondelete=\"CASCADE\"),\n",
      "        primary_key=True,\n",
      "        index=True,\n",
      "    )\n",
      "    personal_data = Column(JSON, nullable=False)\n",
      "    experiences = Column(JSON, nullable=True)\n",
      "    projects = Column(JSON, nullable=True)\n",
      "    skills = Column(JSON, nullable=True)\n",
      "    research_work = Column(JSON, nullable=True)\n",
      "    achievements = Column(JSON, nullable=True)\n",
      "    education = Column(JSON, nullable=True)\n",
      "    extracted_keywords = Column(JSON, nullable=True)\n",
      "\n",
      "    # owner_id = Column(Integer, ForeignKey(\"users.id\"), nullable=False)\n",
      "    # owner = relationship(\"User\", back_populates=\"processed_resumes\")\n",
      "    raw_resume = relationship(\"Resume\", back_populates=\"raw_resume_association\")\n",
      "\n",
      "    processed_jobs = relationship(\n",
      "        \"ProcessedJob\",\n",
      "        secondary=job_resume_association,\n",
      "        back_populates=\"processed_resumes\",\n",
      "    )\n",
      "\n",
      "\n",
      "class Resume(Base):\n",
      "    __tablename__ = \"resumes\"\n",
      "\n",
      "    id = Column(Integer, primary_key=True, index=True)\n",
      "    resume_id = Column(String, unique=True, nullable=False)\n",
      "    content = Column(Text, nullable=False)\n",
      "    content_type = Column(String, nullable=False)\n",
      "\n",
      "    raw_resume_association = relationship(\n",
      "        \"ProcessedResume\", back_populates=\"raw_resume\", uselist=False\n",
      "    )\n",
      "\n",
      "    jobs = relationship(\"Job\", back_populates=\"resumes\")\n",
      "\n",
      "from .base import Base\n",
      "from sqlalchemy import Column, String, Integer\n",
      "# from sqlalchemy.orm import relationship\n",
      "\n",
      "\n",
      "class User(Base):\n",
      "    __tablename__ = \"users\"\n",
      "\n",
      "    id = Column(Integer, primary_key=True, index=True)\n",
      "\n",
      "    email = Column(String, unique=True, index=True, nullable=False)\n",
      "    name = Column(String, nullable=False)\n",
      "\n",
      "    # processed_resumes = relationship(\n",
      "    #     \"ProcessedResume\", back_populates=\"owner\", cascade=\"all, delete-orphan\"\n",
      "    # )\n",
      "    # processed_jobs = relationship(\n",
      "    #     \"ProcessedJob\", back_populates=\"owner\", cascade=\"all, delete-orphan\"\n",
      "    # )\n",
      "\n",
      "PROMPT = \"\"\"\n",
      "You are a JSON extraction engine. Convert the following resume text into precisely the JSON schema specified below.\n",
      "- Do not compose any extra fields or commentary.\n",
      "- Do not make up values for any fields.\n",
      "- User \"Present\" if an end date is ongoing.\n",
      "- Make sure dates are in YYYY-MM-DD.\n",
      "- Do not format the response in Markdown or any other format. Just output raw JSON.\n",
      "\n",
      "Schema:\n",
      "```json\n",
      "{0}\n",
      "```\n",
      "\n",
      "Resume:\n",
      "```text\n",
      "{1}\n",
      "```\n",
      "\n",
      "NOTE: Please output only a valid JSON matching the EXACT schema.\n",
      "\"\"\"\n",
      "\n",
      "import pkgutil\n",
      "import importlib\n",
      "from typing import Dict\n",
      "\n",
      "from app.schemas.json import __path__ as schema_pkg_path\n",
      "\n",
      "\n",
      "class JSONSchemaFactory:\n",
      "    def __init__(self) -> None:\n",
      "        self._schema: Dict[str, str] = {}\n",
      "        self._discover()\n",
      "\n",
      "    def _discover(self) -> None:\n",
      "        for finder, module_name, ispkg in pkgutil.iter_modules(schema_pkg_path):\n",
      "            if module_name.startswith(\"_\") or module_name == \"base\":\n",
      "                continue\n",
      "\n",
      "            module = importlib.import_module(f\"app.schemas.json.{module_name}\")\n",
      "            if hasattr(module, \"SCHEMA\"):\n",
      "                self._schema[module_name] = getattr(module, \"SCHEMA\")\n",
      "\n",
      "    def list_prompts(self) -> Dict[str, str]:\n",
      "        return self._schema\n",
      "\n",
      "    def get(self, name: str) -> str:\n",
      "        try:\n",
      "            return self._schema[name]\n",
      "        except KeyError:\n",
      "            raise KeyError(\n",
      "                f\"SCHEMA '{name}' not found. Available schemas: {list(self._schema.keys())}\"\n",
      "            )\n",
      "\n",
      "from .job import JobUploadRequest\n",
      "from .structured_resume import StructuredResumeModel\n",
      "\n",
      "__all__ = [\"JobUploadRequest\", \"StructuredResumeModel\"]\n",
      "\n",
      "from typing import List, Optional\n",
      "from pydantic import BaseModel, Field\n",
      "\n",
      "\n",
      "class Location(BaseModel):\n",
      "    city: str\n",
      "    country: str\n",
      "\n",
      "\n",
      "class PersonalData(BaseModel):\n",
      "    firstName: str = Field(..., alias=\"firstName\")\n",
      "    lastName: Optional[str] = Field(..., alias=\"lastName\")\n",
      "    email: str\n",
      "    phone: str\n",
      "    linkedin: Optional[str] = None\n",
      "    portfolio: Optional[str] = None\n",
      "    location: Location\n",
      "\n",
      "\n",
      "class Experience(BaseModel):\n",
      "    job_title: str = Field(..., alias=\"jobTitle\")\n",
      "    company: str\n",
      "    location: str\n",
      "    start_date: str = Field(..., alias=\"startDate\")\n",
      "    end_date: str = Field(..., alias=\"endDate\")\n",
      "    description: List[str]\n",
      "    technologies_used: Optional[List[str]] = Field(\n",
      "        default_factory=list, alias=\"technologiesUsed\"\n",
      "    )\n",
      "\n",
      "\n",
      "class Project(BaseModel):\n",
      "    project_name: str = Field(..., alias=\"projectName\")\n",
      "    description: str\n",
      "    technologies_used: List[str] = Field(..., alias=\"technologiesUsed\")\n",
      "    link: Optional[str] = None\n",
      "    start_date: Optional[str] = Field(None, alias=\"startDate\")\n",
      "    end_date: Optional[str] = Field(None, alias=\"endDate\")\n",
      "\n",
      "\n",
      "class Skill(BaseModel):\n",
      "    category: str\n",
      "    skill_name: str = Field(..., alias=\"skillName\")\n",
      "\n",
      "\n",
      "class ResearchWork(BaseModel):\n",
      "    title: str\n",
      "    publication: str\n",
      "    date: str\n",
      "    link: Optional[str] = None\n",
      "    description: Optional[str] = None\n",
      "\n",
      "\n",
      "class Education(BaseModel):\n",
      "    institution: str\n",
      "    degree: str\n",
      "    field_of_study: str = Field(..., alias=\"fieldOfStudy\")\n",
      "    start_date: str = Field(..., alias=\"startDate\")\n",
      "    end_date: str = Field(..., alias=\"endDate\")\n",
      "    grade: Optional[str] = None\n",
      "    description: Optional[str] = None\n",
      "\n",
      "\n",
      "class StructuredResumeModel(BaseModel):\n",
      "    personal_data: PersonalData = Field(..., alias=\"Personal Data\")\n",
      "    experiences: List[Experience] = Field(..., alias=\"Experiences\")\n",
      "    projects: List[Project] = Field(..., alias=\"Projects\")\n",
      "    skills: List[Skill] = Field(..., alias=\"Skills\")\n",
      "    research_work: List[ResearchWork] = Field(\n",
      "        default_factory=list, alias=\"Research Work\"\n",
      "    )\n",
      "    achievements: List[str] = Field(default_factory=list, alias=\"Achievements\")\n",
      "    education: List[Education] = Field(..., alias=\"Education\")\n",
      "    extracted_keywords: List[str] = Field(\n",
      "        default_factory=list, alias=\"Extracted Keywords\"\n",
      "    )\n",
      "\n",
      "    class ConfigDict:\n",
      "        validate_by_name = True\n",
      "        str_strip_whitespace = True\n",
      "\n",
      "import os\n",
      "import uuid\n",
      "import json\n",
      "import tempfile\n",
      "import logging\n",
      "\n",
      "from markitdown import MarkItDown\n",
      "from sqlalchemy.orm import Session\n",
      "from pydantic import ValidationError\n",
      "\n",
      "from app.models import Resume, ProcessedResume\n",
      "from app.agent import AgentManager\n",
      "from app.prompt import prompt_factory\n",
      "from app.schemas.json import json_schema_factory\n",
      "from app.schemas.pydantic import StructuredResumeModel\n",
      "\n",
      "logger = logging.getLogger(__name__)\n",
      "\n",
      "\n",
      "class ResumeService:\n",
      "    def __init__(self, db: Session):\n",
      "        self.db = db\n",
      "        self.md = MarkItDown(enable_plugins=False)\n",
      "        self.json_agent_manager = AgentManager(model=\"gemma3:4b\")\n",
      "\n",
      "    async def convert_and_store_resume(\n",
      "        self, file_bytes: bytes, file_type: str, filename: str, content_type: str = \"md\"\n",
      "    ):\n",
      "        \"\"\"\n",
      "        Converts resume file (PDF/DOCX) to text using MarkItDown and stores it in the database.\n",
      "\n",
      "        Args:\n",
      "            file_bytes: Raw bytes of the uploaded file\n",
      "            file_type: MIME type of the file (\"application/pdf\" or \"application/vnd.openxmlformats-officedocument.wordprocessingml.document\")\n",
      "            filename: Original filename\n",
      "            content_type: Output format (\"md\" for markdown or \"html\")\n",
      "\n",
      "        Returns:\n",
      "            None\n",
      "        \"\"\"\n",
      "        with tempfile.NamedTemporaryFile(\n",
      "            delete=False, suffix=self._get_file_extension(file_type)\n",
      "        ) as temp_file:\n",
      "            temp_file.write(file_bytes)\n",
      "            temp_path = temp_file.name\n",
      "\n",
      "        try:\n",
      "            result = self.md.convert(temp_path)\n",
      "            text_content = result.text_content\n",
      "            resume_id = str(uuid.uuid4())\n",
      "            self._store_resume_in_db(resume_id, filename, text_content, content_type)\n",
      "            await self._extract_and_store_structured_resume(resume_text=text_content)\n",
      "            return resume_id\n",
      "\n",
      "        finally:\n",
      "            if os.path.exists(temp_path):\n",
      "                os.remove(temp_path)\n",
      "\n",
      "    def _get_file_extension(self, file_type: str) -> str:\n",
      "        \"\"\"Returns the appropriate file extension based on MIME type\"\"\"\n",
      "        if file_type == \"application/pdf\":\n",
      "            return \".pdf\"\n",
      "        elif (\n",
      "            file_type\n",
      "            == \"application/vnd.openxmlformats-officedocument.wordprocessingml.document\"\n",
      "        ):\n",
      "            return \".docx\"\n",
      "        return \"\"\n",
      "\n",
      "    def _store_resume_in_db(\n",
      "        self, resume_id: str, filename: str, text_content: str, content_type: str\n",
      "    ):\n",
      "        \"\"\"\n",
      "        Stores the parsed resume content in the database.\n",
      "        \"\"\"\n",
      "        resume = Resume(\n",
      "            resume_id=resume_id, content=text_content, content_type=content_type\n",
      "        )\n",
      "\n",
      "        self.db.add(resume)\n",
      "        self.db.commit()\n",
      "\n",
      "        return resume\n",
      "\n",
      "    async def _extract_and_store_structured_resume(self, resume_text: str):\n",
      "        \"\"\"\n",
      "        extract and store structured resume data in the database\n",
      "        \"\"\"\n",
      "        structured_resume = await self._extract_structured_json(resume_text)\n",
      "        if not structured_resume:\n",
      "            logger.info(\"Structured resume extraction failed.\")\n",
      "            return None\n",
      "\n",
      "        resume_id = str(uuid.uuid4())\n",
      "\n",
      "        self.db.add(\n",
      "            ProcessedResume(\n",
      "                resume_id=resume_id,\n",
      "                personal_data=json.dumps(structured_resume.get(\"personal_data\", {}))\n",
      "                if structured_resume.get(\"personal_data\")\n",
      "                else None,\n",
      "                experiences=json.dumps(\n",
      "                    {\"experiences\": structured_resume.get(\"experiences\", [])}\n",
      "                )\n",
      "                if structured_resume.get(\"experiences\")\n",
      "                else None,\n",
      "                projects=json.dumps({\"projects\": structured_resume.get(\"projects\", [])})\n",
      "                if structured_resume.get(\"projects\")\n",
      "                else None,\n",
      "                skills=json.dumps({\"skills\": structured_resume.get(\"skills\", [])})\n",
      "                if structured_resume.get(\"skills\")\n",
      "                else None,\n",
      "                research_work=json.dumps(\n",
      "                    {\"research_work\": structured_resume.get(\"research_work\", [])}\n",
      "                )\n",
      "                if structured_resume.get(\"research_work\")\n",
      "                else None,\n",
      "                achievements=json.dumps(\n",
      "                    {\"achievements\": structured_resume.get(\"achievements\", [])}\n",
      "                )\n",
      "                if structured_resume.get(\"achievements\")\n",
      "                else None,\n",
      "                education=json.dumps(\n",
      "                    {\"education\": structured_resume.get(\"education\", [])}\n",
      "                )\n",
      "                if structured_resume.get(\"education\")\n",
      "                else None,\n",
      "                extracted_keywords=json.dumps(\n",
      "                    {\n",
      "                        \"extracted_keywords\": structured_resume.get(\n",
      "                            \"extracted_keywords\", []\n",
      "                        )\n",
      "                    }\n",
      "                    if structured_resume.get(\"extracted_keywords\")\n",
      "                    else None\n",
      "                ),\n",
      "            )\n",
      "        )\n",
      "        self.db.commit()\n",
      "\n",
      "        return resume_id\n",
      "\n",
      "    async def _extract_structured_json(\n",
      "        self, resume_text: str\n",
      "    ) -> StructuredResumeModel | None:\n",
      "        \"\"\"\n",
      "        Uses the AgentManager+JSONWrapper to ask the LLM to\n",
      "        return the data in exact JSON schema we need.\n",
      "        \"\"\"\n",
      "        prompt_template = prompt_factory.get(\"structured_resume\")\n",
      "        prompt = prompt_template.format(\n",
      "            json.dumps(json_schema_factory.get(\"structured_resume\"), indent=2),\n",
      "            resume_text,\n",
      "        )\n",
      "        logger.info(f\"Structured Resume Prompt: {prompt}\")\n",
      "        raw_output = await self.json_agent_manager.run(prompt=prompt)\n",
      "\n",
      "        try:\n",
      "            structured_resume: StructuredResumeModel = (\n",
      "                StructuredResumeModel.model_validate(raw_output)\n",
      "            )\n",
      "        except ValidationError as e:\n",
      "            logger.info(f\"Validation error: {e}\")\n",
      "            return None\n",
      "        return structured_resume.model_dump()\n",
      "\n",
      "from fastapi import APIRouter, HTTPException, Depends, Request, status\n",
      "from sqlalchemy.orm import Session\n",
      "from uuid import uuid4\n",
      "from app.core import get_db_session\n",
      "from app.schemas.pydantic.job import JobUploadRequest\n",
      "from app.services import JobService\n",
      "\n",
      "job_router = APIRouter()\n",
      "\n",
      "\n",
      "@job_router.post(\n",
      "    \"/upload\",\n",
      "    summary=\"stores the job posting in the database by parsing the JD into a structured format JSON\",\n",
      ")\n",
      "async def upload_job(\n",
      "    payload: JobUploadRequest,\n",
      "    request: Request,\n",
      "    db: Session = Depends(get_db_session),\n",
      "):\n",
      "    \"\"\"\n",
      "    Accepts a job description as a MarkDown text and stores it in the database.\n",
      "    \"\"\"\n",
      "    request_id = getattr(request.state, \"request_id\", str(uuid4()))\n",
      "\n",
      "    allowed_content_types = [\n",
      "        \"application/json\",\n",
      "    ]\n",
      "\n",
      "    content_type = request.headers.get(\"content-type\")\n",
      "    if not content_type:\n",
      "        raise HTTPException(\n",
      "            status_code=status.HTTP_400_BAD_REQUEST,\n",
      "            detail=\"Content-Type header is missing\",\n",
      "        )\n",
      "\n",
      "    if content_type not in allowed_content_types:\n",
      "        raise HTTPException(\n",
      "            status_code=400,\n",
      "            detail=f\"Invalid Content-Type. Only {', '.join(allowed_content_types)} is/are allowed.\",\n",
      "        )\n",
      "\n",
      "    try:\n",
      "        job_service = JobService(db)\n",
      "        job_ids = await job_service.create_and_store_job(payload.model_dump())\n",
      "\n",
      "    except AssertionError as e:\n",
      "        raise HTTPException(\n",
      "            status_code=status.HTTP_400_BAD_REQUEST,\n",
      "            detail=str(e),\n",
      "        )\n",
      "\n",
      "    except Exception as e:\n",
      "        raise HTTPException(\n",
      "            status_code=status.HTTP_500_INTERNAL_SERVER_ERROR,\n",
      "            detail=f\"{str(e)}\",\n",
      "        )\n",
      "\n",
      "    return {\n",
      "        \"message\": \"data successfully processed\",\n",
      "        \"job_id\": job_ids,\n",
      "        \"request\": {\n",
      "            \"request_id\": request_id,\n",
      "            \"payload\": payload,\n",
      "        },\n",
      "    }\n",
      "\n",
      "from .base import Base\n",
      "from .resume import ProcessedResume, Resume\n",
      "from .user import User\n",
      "from .job import ProcessedJob, Job\n",
      "from .association import job_resume_association\n",
      "\n",
      "__all__ = [\n",
      "    \"Base\",\n",
      "    \"Resume\",\n",
      "    \"ProcessedResume\",\n",
      "    \"ProcessedJob\",\n",
      "    \"User\",\n",
      "    \"Job\",\n",
      "    \"job_resume_association\",\n",
      "]\n",
      "\n",
      "from .base import Base\n",
      "from sqlalchemy import Column, String, Text, Integer, ForeignKey, DateTime\n",
      "from sqlalchemy.types import JSON\n",
      "from sqlalchemy.orm import relationship\n",
      "from .association import job_resume_association\n",
      "import datetime\n",
      "\n",
      "\n",
      "class ProcessedJob(Base):\n",
      "    __tablename__ = \"processed_jobs\"\n",
      "\n",
      "    job_id = Column(\n",
      "        String,\n",
      "        ForeignKey(\"jobs.job_id\", ondelete=\"CASCADE\"),\n",
      "        primary_key=True,\n",
      "        index=True,\n",
      "    )\n",
      "    job_title = Column(String, nullable=False)\n",
      "    company_profile = Column(Text, nullable=True)\n",
      "    location = Column(String, nullable=True)\n",
      "    date_posted = Column(String, nullable=True)\n",
      "    employment_type = Column(String, nullable=True)\n",
      "    job_summary = Column(Text, nullable=False)\n",
      "    key_responsibilities = Column(JSON, nullable=True)\n",
      "    qualifications = Column(JSON, nullable=True)\n",
      "    compensation_and_benfits = Column(JSON, nullable=True)\n",
      "    application_info = Column(JSON, nullable=True)\n",
      "    extracted_keywords = Column(JSON, nullable=True)\n",
      "    processed_at = Column(\n",
      "        DateTime, default=datetime.datetime.now(datetime.timezone.utc)\n",
      "    )\n",
      "\n",
      "    # one-to-many relation between user and jobs\n",
      "    # owner_id = Column(Integer, ForeignKey(\"users.id\"), nullable=False)\n",
      "    # owner = relationship(\"User\", back_populates=\"processed_jobs\")\n",
      "    raw_job = relationship(\"Job\", back_populates=\"raw_job_association\")\n",
      "\n",
      "    # many-to-many relationship in job and resume\n",
      "    processed_resumes = relationship(\n",
      "        \"ProcessedResume\",\n",
      "        secondary=job_resume_association,\n",
      "        back_populates=\"processed_jobs\",\n",
      "    )\n",
      "\n",
      "\n",
      "class Job(Base):\n",
      "    __tablename__ = \"jobs\"\n",
      "\n",
      "    id = Column(Integer, primary_key=True, index=True)\n",
      "    job_id = Column(String, unique=True, nullable=False)\n",
      "    resume_id = Column(String, ForeignKey(\"resumes.resume_id\"), nullable=False)\n",
      "    content = Column(Text, nullable=False)\n",
      "    created_at = Column(DateTime, default=datetime.datetime.now(datetime.timezone.utc))\n",
      "\n",
      "    raw_job_association = relationship(\n",
      "        \"ProcessedJob\", back_populates=\"raw_job\", uselist=False\n",
      "    )\n",
      "\n",
      "    resumes = relationship(\"Resume\", back_populates=\"jobs\")\n",
      "\n",
      "PROMPT = \"\"\"\n",
      "You are a JSON-extraction engine. Convert the following raw job posting text into exactly the JSON schema below:\n",
      " Do not add any extra fields or prose.\n",
      " Use YYYY-MM-DD for all dates.\n",
      " Ensure any URLs (website, applyLink) conform to URI format.\n",
      " Do not change the structure or key names; output only valid JSON matching the schema.\n",
      "- Do not format the response in Markdown or any other format. Just output raw JSON.\n",
      "\n",
      "Schema:\n",
      "```json\n",
      "{0}\n",
      "```\n",
      "\n",
      "Job Posting:\n",
      "{1}\n",
      "\n",
      "Note: Please output only a valid JSON matching the EXACT schema with no surrounding commentary.\n",
      "\"\"\"\n",
      "\n",
      "SCHEMA = {\n",
      "    \"jobId\": \"string\",\n",
      "    \"jobTitle\": \"string\",\n",
      "    \"companyProfile\": {\n",
      "        \"companyName\": \"string\",\n",
      "        \"industry\": \"Optional[string]\",\n",
      "        \"website\": \"Optional[string]\",\n",
      "        \"description\": \"Optional[string]\",\n",
      "    },\n",
      "    \"location\": {\n",
      "        \"city\": \"string\",\n",
      "        \"state\": \"string\",\n",
      "        \"country\": \"string\",\n",
      "        \"remoteStatus\": \"string\",\n",
      "    },\n",
      "    \"datePosted\": \"YYYY-MM-DD\",\n",
      "    \"employmentType\": \"string\",\n",
      "    \"jobSummary\": \"string\",\n",
      "    \"keyResponsibilities\": [\n",
      "        \"string\",\n",
      "        \"...\",\n",
      "    ],\n",
      "    \"qualifications\": {\n",
      "        \"required\": [\n",
      "            \"string\",\n",
      "            \"...\",\n",
      "        ],\n",
      "        \"preferred\": [\n",
      "            \"string\",\n",
      "            \"...\",\n",
      "        ],\n",
      "    },\n",
      "    \"compensationAndBenefits\": {\n",
      "        \"salaryRange\": \"string\",\n",
      "        \"benefits\": [\n",
      "            \"string\",\n",
      "            \"...\",\n",
      "        ],\n",
      "    },\n",
      "    \"applicationInfo\": {\n",
      "        \"howToApply\": \"string\",\n",
      "        \"applyLink\": \"string\",\n",
      "        \"contactEmail\": \"Optional[string]\",\n",
      "    },\n",
      "    \"extractedKeywords\": [\n",
      "        \"string\",\n",
      "        \"...\",\n",
      "    ],\n",
      "}\n",
      "\n",
      "from .job import JobUploadRequest\n",
      "from .structured_job import StructuredJobModel\n",
      "from .structured_resume import StructuredResumeModel\n",
      "\n",
      "__all__ = [\"JobUploadRequest\", \"StructuredResumeModel\", \"StructuredJobModel\"]\n",
      "\n",
      "from typing import Optional, List\n",
      "from pydantic import BaseModel, Field, HttpUrl, EmailStr, field_validator\n",
      "from typing_extensions import Literal\n",
      "\n",
      "\n",
      "class CompanyProfile(BaseModel):\n",
      "    company_name: str = Field(..., alias=\"companyName\")\n",
      "    industry: Optional[str] = None\n",
      "    website: Optional[HttpUrl] = None\n",
      "    description: Optional[str] = None\n",
      "\n",
      "\n",
      "class Location(BaseModel):\n",
      "    city: str\n",
      "    state: Optional[str] = None\n",
      "    country: Optional[str] = None\n",
      "    remote_status: Literal[\"Fully Remote\", \"Hybrid\", \"On-site\", \"Remote\"] = Field(\n",
      "        ..., alias=\"remoteStatus\"\n",
      "    )\n",
      "\n",
      "    @field_validator(\"remote_status\", mode=\"before\")\n",
      "    def validate_remote_status(cls, value):\n",
      "        if isinstance(value, str):\n",
      "            v_lower = value.lower()\n",
      "            mapping = {\n",
      "                \"fully remote\": \"Fully Remote\",\n",
      "                \"hybrid\": \"Hybrid\",\n",
      "                \"on-site\": \"On-site\",\n",
      "                \"remote\": \"Remote\",\n",
      "            }\n",
      "            if v_lower in mapping:\n",
      "                return mapping[v_lower]\n",
      "        raise ValueError(\n",
      "            \"remote_status must be one of: Fully Remote, Hybrid, On-site, Remote (case insensitive)\"\n",
      "        )\n",
      "\n",
      "\n",
      "class Qualifications(BaseModel):\n",
      "    required: List[str]\n",
      "    preferred: Optional[List[str]] = None\n",
      "\n",
      "\n",
      "class CompensationAndBenefits(BaseModel):\n",
      "    salary_range: Optional[str] = Field(..., alias=\"salaryRange\")\n",
      "    benefits: Optional[List[str]] = None\n",
      "\n",
      "\n",
      "class ApplicationInfo(BaseModel):\n",
      "    how_to_apply: Optional[str] = Field(..., alias=\"howToApply\")\n",
      "    apply_link: Optional[str] = Field(..., alias=\"applyLink\")\n",
      "    contact_email: Optional[EmailStr] = Field(..., alias=\"contactEmail\")\n",
      "\n",
      "\n",
      "class StructuredJobModel(BaseModel):\n",
      "    job_title: str = Field(..., alias=\"jobTitle\")\n",
      "    company_profile: CompanyProfile = Field(..., alias=\"companyProfile\")\n",
      "    location: Location\n",
      "    date_posted: str = Field(..., alias=\"datePosted\")\n",
      "    employment_type: Literal[\n",
      "        \"Full-time\", \"Part-time\", \"Contract\", \"Internship\", \"Temporary\"\n",
      "    ] = Field(..., alias=\"employmentType\")\n",
      "    job_summary: str = Field(..., alias=\"jobSummary\")\n",
      "    key_responsibilities: List[str] = Field(..., alias=\"keyResponsibilities\")\n",
      "    qualifications: Qualifications\n",
      "    compensation_and_benefits: Optional[CompensationAndBenefits] = Field(\n",
      "        None, alias=\"compensationAndBenefits\"\n",
      "    )\n",
      "    application_info: Optional[ApplicationInfo] = Field(None, alias=\"applicationInfo\")\n",
      "    extracted_keywords: List[str] = Field(..., alias=\"extractedKeywords\")\n",
      "\n",
      "    class ConfigDict:\n",
      "        validate_by_name = True\n",
      "        str_strip_whitespace = True\n",
      "\n",
      "import uuid\n",
      "import json\n",
      "import logging\n",
      "\n",
      "from typing import List\n",
      "from sqlalchemy.orm import Session\n",
      "from pydantic import ValidationError\n",
      "\n",
      "from app.agent import AgentManager\n",
      "from app.prompt import prompt_factory\n",
      "from app.schemas.json import json_schema_factory\n",
      "from app.models import Job, Resume, ProcessedJob\n",
      "from app.schemas.pydantic import StructuredJobModel\n",
      "\n",
      "logger = logging.getLogger(__name__)\n",
      "\n",
      "\n",
      "class JobService:\n",
      "    def __init__(self, db: Session):\n",
      "        self.db = db\n",
      "        self.json_agent_manager = AgentManager(model=\"gemma3:4b\")\n",
      "\n",
      "    async def create_and_store_job(self, job_data: dict) -> List[str]:\n",
      "        \"\"\"\n",
      "        Stores job data in the database and returns a list of job IDs.\n",
      "        \"\"\"\n",
      "        resume_id = str(job_data.get(\"resume_id\"))\n",
      "\n",
      "        if not self._is_resume_available(resume_id):\n",
      "            raise AssertionError(\n",
      "                f\"resume corresponding to resume_id: {resume_id} not found\"\n",
      "            )\n",
      "\n",
      "        job_ids = []\n",
      "        for job_description in job_data.get(\"job_descriptions\", []):\n",
      "            job_id = str(uuid.uuid4())\n",
      "            job = Job(\n",
      "                job_id=job_id,\n",
      "                resume_id=str(resume_id),\n",
      "                content=job_description,\n",
      "            )\n",
      "\n",
      "            jb_id = await self._extract_and_store_structured_resume(\n",
      "                job_id=job_id, job_description_text=job_description\n",
      "            )\n",
      "            logger.info(f\"Job ID: {jb_id}\")\n",
      "            self.db.add(job)\n",
      "            job_ids.append(job_id)\n",
      "\n",
      "        self.db.commit()\n",
      "        return job_ids\n",
      "\n",
      "    def _is_resume_available(self, resume_id: str) -> bool:\n",
      "        \"\"\"\n",
      "        Checks if a resume exists in the database.\n",
      "        \"\"\"\n",
      "        return (\n",
      "            self.db.query(Resume).filter(Resume.resume_id == resume_id).first()\n",
      "            is not None\n",
      "        )\n",
      "\n",
      "    async def _extract_and_store_structured_resume(\n",
      "        self, job_id, job_description_text: str\n",
      "    ):\n",
      "        \"\"\"\n",
      "        extract and store structured job data in the database\n",
      "        \"\"\"\n",
      "        structured_job = await self._extract_structured_json(job_description_text)\n",
      "        if not structured_job:\n",
      "            logger.info(\"Structured job extraction failed.\")\n",
      "            return None\n",
      "\n",
      "        self.db.add(\n",
      "            ProcessedJob(\n",
      "                job_id=job_id,\n",
      "                job_title=structured_job.get(\"job_title\"),\n",
      "                company_profile=structured_job.get(\"company_profile\"),\n",
      "                location=structured_job.get(\"location\"),\n",
      "                date_posted=structured_job.get(\"date_posted\"),\n",
      "                employment_type=structured_job.get(\"employment_type\"),\n",
      "                job_summary=structured_job.get(\"job_summary\"),\n",
      "                key_responsibilities=json.dumps(\n",
      "                    {\n",
      "                        \"key_responsibilities\": structured_job.get(\n",
      "                            \"key_responsibilities\", []\n",
      "                        )\n",
      "                    }\n",
      "                )\n",
      "                if structured_job.get(\"key_responsibilities\")\n",
      "                else None,\n",
      "                qualifications=json.dumps(\n",
      "                    {\"qualifications\": structured_job.get(\"qualifications\", [])}\n",
      "                )\n",
      "                if structured_job.get(\"qualifications\")\n",
      "                else None,\n",
      "                compensation_and_benfits=json.dumps(\n",
      "                    {\n",
      "                        \"compensation_and_benfits\": structured_job.get(\n",
      "                            \"compensation_and_benfits\", []\n",
      "                        )\n",
      "                    }\n",
      "                )\n",
      "                if structured_job.get(\"compensation_and_benfits\")\n",
      "                else None,\n",
      "                application_info=json.dumps(\n",
      "                    {\"application_info\": structured_job.get(\"application_info\", [])}\n",
      "                )\n",
      "                if structured_job.get(\"application_info\")\n",
      "                else None,\n",
      "                extracted_keywords=json.dumps(\n",
      "                    {\"extracted_keywords\": structured_job.get(\"extracted_keywords\", [])}\n",
      "                )\n",
      "                if structured_job.get(\"extracted_keywords\")\n",
      "                else None,\n",
      "            )\n",
      "        )\n",
      "        self.db.commit()\n",
      "\n",
      "        return job_id\n",
      "\n",
      "    async def _extract_structured_json(\n",
      "        self, job_description_text: str\n",
      "    ) -> StructuredJobModel | None:\n",
      "        \"\"\"\n",
      "        Uses the AgentManager+JSONWrapper to ask the LLM to\n",
      "        return the data in exact JSON schema we need.\n",
      "        \"\"\"\n",
      "        prompt_template = prompt_factory.get(\"structured_job\")\n",
      "        prompt = prompt_template.format(\n",
      "            json.dumps(json_schema_factory.get(\"structured_job\"), indent=2),\n",
      "            job_description_text,\n",
      "        )\n",
      "        logger.info(f\"Structured Job Prompt: {prompt}\")\n",
      "        raw_output = await self.json_agent_manager.run(prompt=prompt)\n",
      "\n",
      "        try:\n",
      "            structured_job: StructuredJobModel = StructuredJobModel.model_validate(\n",
      "                raw_output\n",
      "            )\n",
      "        except ValidationError as e:\n",
      "            logger.info(f\"Validation error: {e}\")\n",
      "            return None\n",
      "        return structured_job.model_dump()\n",
      "\n",
      "import json\n",
      "import logging\n",
      "from typing import Any, Dict\n",
      "\n",
      "from .base import Strategy\n",
      "from ..providers.base import Provider\n",
      "from ..exceptions import StrategyError\n",
      "\n",
      "\n",
      "logger = logging.getLogger(__name__)\n",
      "\n",
      "\n",
      "class JSONWrapper(Strategy):\n",
      "    async def __call__(\n",
      "        self, prompt: str, provider: Provider, **generation_args: Any\n",
      "    ) -> Dict[str, Any]:\n",
      "        \"\"\"\n",
      "        Wrapper strategy to format the prompt as JSON with the help of LLM.\n",
      "        \"\"\"\n",
      "        response = await provider(prompt, **generation_args)\n",
      "        response = response.replace(\"```\", \"\").replace(\"json\", \"\").strip()\n",
      "        logger.info(f\"provider response: {response}\")\n",
      "        try:\n",
      "            return json.loads(response)\n",
      "        except json.JSONDecodeError as e:\n",
      "            logger.error(\n",
      "                f\"provider returned non-JSON. parsing error: {e} - response: {response}\"\n",
      "            )\n",
      "            raise StrategyError(f\"JSON parsing error: {e}\") from e\n",
      "\n",
      "from .router.v1 import v1_router\n",
      "from .router.health import health_check\n",
      "from .middleware import RequestIDMiddleware\n",
      "\n",
      "__all__ = [\"health_check\", \"v1_router\", \"RequestIDMiddleware\"]\n",
      "\n",
      "from uuid import uuid4\n",
      "from starlette.requests import Request\n",
      "from starlette.middleware.base import BaseHTTPMiddleware\n",
      "\n",
      "\n",
      "class RequestIDMiddleware(BaseHTTPMiddleware):\n",
      "    async def dispatch(self, request: Request, call_next):\n",
      "        path_parts = request.url.path.strip(\"/\").split(\"/\")\n",
      "\n",
      "        # Safely grab the 3rd part: /api/v1/<service>\n",
      "        service_tag = f\"{path_parts[2]}:\" if len(path_parts) > 2 else \"\"\n",
      "\n",
      "        request_id = f\"{service_tag}{uuid4()}\"\n",
      "        request.state.request_id = request_id\n",
      "\n",
      "        response = await call_next(request)\n",
      "        return response\n",
      "\n",
      "from sqlalchemy import text\n",
      "from sqlalchemy.orm import Session\n",
      "from fastapi import APIRouter, status, Depends\n",
      "\n",
      "from app.core import get_sync_db_session\n",
      "\n",
      "health_check = APIRouter()\n",
      "\n",
      "\n",
      "@health_check.get(\"/ping\", tags=[\"health_check\"], status_code=status.HTTP_200_OK)\n",
      "def ping(db: Session = Depends(get_sync_db_session)):\n",
      "    \"\"\"\n",
      "    health check endpoint\n",
      "    \"\"\"\n",
      "    try:\n",
      "        result = db.execute(text(\"SELECT 1\")).fetchone()\n",
      "        db_status = \"reachable\" if result is not None else \"not reachable\"\n",
      "    except Exception as e:\n",
      "        db_status = f\"error: {str(e)}\"\n",
      "    return {\"message\": \"pong\", \"database\": db_status}\n",
      "\n",
      "from fastapi import APIRouter\n",
      "\n",
      "from .job import job_router\n",
      "from .resume import resume_router\n",
      "\n",
      "v1_router = APIRouter(prefix=\"/api/v1\", tags=[\"v1\"])\n",
      "v1_router.include_router(resume_router, prefix=\"/resumes\")\n",
      "v1_router.include_router(job_router, prefix=\"/jobs\")\n",
      "\n",
      "\n",
      "__all__ = [\"v1_router\"]\n",
      "\n",
      "from uuid import uuid4\n",
      "\n",
      "from sqlalchemy.ext.asyncio import AsyncSession\n",
      "from fastapi import APIRouter, HTTPException, Depends, Request, status\n",
      "\n",
      "from app.core import get_db_session\n",
      "from app.services import JobService\n",
      "from app.schemas.pydantic.job import JobUploadRequest\n",
      "\n",
      "job_router = APIRouter()\n",
      "\n",
      "\n",
      "@job_router.post(\n",
      "    \"/upload\",\n",
      "    summary=\"stores the job posting in the database by parsing the JD into a structured format JSON\",\n",
      ")\n",
      "async def upload_job(\n",
      "    payload: JobUploadRequest,\n",
      "    request: Request,\n",
      "    db: AsyncSession = Depends(get_db_session),\n",
      "):\n",
      "    \"\"\"\n",
      "    Accepts a job description as a MarkDown text and stores it in the database.\n",
      "    \"\"\"\n",
      "    request_id = getattr(request.state, \"request_id\", str(uuid4()))\n",
      "\n",
      "    allowed_content_types = [\n",
      "        \"application/json\",\n",
      "    ]\n",
      "\n",
      "    content_type = request.headers.get(\"content-type\")\n",
      "    if not content_type:\n",
      "        raise HTTPException(\n",
      "            status_code=status.HTTP_400_BAD_REQUEST,\n",
      "            detail=\"Content-Type header is missing\",\n",
      "        )\n",
      "\n",
      "    if content_type not in allowed_content_types:\n",
      "        raise HTTPException(\n",
      "            status_code=400,\n",
      "            detail=f\"Invalid Content-Type. Only {', '.join(allowed_content_types)} is/are allowed.\",\n",
      "        )\n",
      "\n",
      "    try:\n",
      "        job_service = JobService(db)\n",
      "        job_ids = await job_service.create_and_store_job(payload.model_dump())\n",
      "\n",
      "    except AssertionError as e:\n",
      "        raise HTTPException(\n",
      "            status_code=status.HTTP_400_BAD_REQUEST,\n",
      "            detail=str(e),\n",
      "        )\n",
      "\n",
      "    except Exception as e:\n",
      "        raise HTTPException(\n",
      "            status_code=status.HTTP_500_INTERNAL_SERVER_ERROR,\n",
      "            detail=f\"{str(e)}\",\n",
      "        )\n",
      "\n",
      "    return {\n",
      "        \"message\": \"data successfully processed\",\n",
      "        \"job_id\": job_ids,\n",
      "        \"request\": {\n",
      "            \"request_id\": request_id,\n",
      "            \"payload\": payload,\n",
      "        },\n",
      "    }\n",
      "\n",
      "import logging\n",
      "import traceback\n",
      "\n",
      "from uuid import uuid4\n",
      "from sqlalchemy.ext.asyncio import AsyncSession\n",
      "from fastapi import APIRouter, File, UploadFile, HTTPException, Depends, Request, status\n",
      "\n",
      "from app.core import get_db_session\n",
      "from app.services.resume_service import ResumeService\n",
      "\n",
      "resume_router = APIRouter()\n",
      "logger = logging.getLogger(__name__)\n",
      "\n",
      "\n",
      "@resume_router.post(\n",
      "    \"/upload\",\n",
      "    summary=\"Upload a resume in PDF or DOCX format and store it into DB in HTML/Markdown format\",\n",
      ")\n",
      "async def upload_resume(\n",
      "    request: Request,\n",
      "    file: UploadFile = File(...),\n",
      "    db: AsyncSession = Depends(get_db_session),\n",
      "):\n",
      "    \"\"\"\n",
      "    Accepts a PDF or DOCX file, converts it to HTML/Markdown, and stores it in the database.\n",
      "\n",
      "    Raises:\n",
      "        HTTPException: If the file type is not supported or if the file is empty.\n",
      "    \"\"\"\n",
      "    request_id = getattr(request.state, \"request_id\", str(uuid4()))\n",
      "\n",
      "    allowed_content_types = [\n",
      "        \"application/pdf\",\n",
      "        \"application/vnd.openxmlformats-officedocument.wordprocessingml.document\",\n",
      "    ]\n",
      "\n",
      "    if file.content_type not in allowed_content_types:\n",
      "        raise HTTPException(\n",
      "            status_code=status.HTTP_400_BAD_REQUEST,\n",
      "            detail=\"Invalid file type. Only PDF and DOCX files are allowed.\",\n",
      "        )\n",
      "\n",
      "    file_bytes = await file.read()\n",
      "    if not file_bytes:\n",
      "        raise HTTPException(\n",
      "            status_code=status.HTTP_400_BAD_REQUEST,\n",
      "            detail=\"Empty file. Please upload a valid file.\",\n",
      "        )\n",
      "\n",
      "    try:\n",
      "        resume_service = ResumeService(db)\n",
      "        resume_id = await resume_service.convert_and_store_resume(\n",
      "            file_bytes=file_bytes,\n",
      "            file_type=file.content_type,\n",
      "            filename=file.filename,\n",
      "            content_type=\"md\",\n",
      "        )\n",
      "    except Exception as e:\n",
      "        logger.error(\n",
      "            f\"Error processing file: {str(e)} - traceback: {traceback.format_exc()}\"\n",
      "        )\n",
      "        raise HTTPException(\n",
      "            status_code=status.HTTP_500_INTERNAL_SERVER_ERROR,\n",
      "            detail=f\"Error processing file: {str(e)}\",\n",
      "        )\n",
      "\n",
      "    return {\n",
      "        \"message\": f\"File {file.filename} successfully processed as MD and stored in the DB\",\n",
      "        \"request_id\": request_id,\n",
      "        \"resume_id\": resume_id,\n",
      "    }\n",
      "\n",
      "import os\n",
      "\n",
      "from contextlib import asynccontextmanager\n",
      "from fastapi import FastAPI, HTTPException\n",
      "from fastapi.exceptions import RequestValidationError\n",
      "from fastapi.staticfiles import StaticFiles\n",
      "from fastapi.middleware.cors import CORSMiddleware\n",
      "from starlette.middleware.sessions import SessionMiddleware\n",
      "\n",
      "from .api import health_check, v1_router, RequestIDMiddleware\n",
      "from .core import (\n",
      "    settings,\n",
      "    async_engine,\n",
      "    setup_logging,\n",
      "    custom_http_exception_handler,\n",
      "    validation_exception_handler,\n",
      "    unhandled_exception_handler,\n",
      ")\n",
      "from .models import Base\n",
      "\n",
      "\n",
      "@asynccontextmanager\n",
      "async def lifespan(app: FastAPI):\n",
      "    async with async_engine.begin() as conn:\n",
      "        await conn.run_sync(Base.metadata.create_all)\n",
      "    yield\n",
      "    await async_engine.dispose()\n",
      "\n",
      "\n",
      "def create_app() -> FastAPI:\n",
      "    \"\"\"\n",
      "    configure and create the FastAPI application instance.\n",
      "    \"\"\"\n",
      "    setup_logging()\n",
      "\n",
      "    app = FastAPI(\n",
      "        title=settings.PROJECT_NAME,\n",
      "        docs_url=\"/api/docs\",\n",
      "        openapi_url=\"/api/openapi.json\",\n",
      "        lifespan=lifespan,\n",
      "    )\n",
      "\n",
      "    app.add_middleware(\n",
      "        SessionMiddleware, secret_key=settings.SESSION_SECRET_KEY, same_site=\"lax\"\n",
      "    )\n",
      "    app.add_middleware(\n",
      "        CORSMiddleware,\n",
      "        allow_origins=settings.ALLOWED_ORIGINS,\n",
      "        allow_credentials=True,\n",
      "        allow_methods=[\"*\"],\n",
      "        allow_headers=[\"*\"],\n",
      "    )\n",
      "    app.add_middleware(RequestIDMiddleware)\n",
      "\n",
      "    app.add_exception_handler(HTTPException, custom_http_exception_handler)\n",
      "    app.add_exception_handler(RequestValidationError, validation_exception_handler)\n",
      "    app.add_exception_handler(Exception, unhandled_exception_handler)\n",
      "\n",
      "    if os.path.exists(settings.FRONTEND_PATH):\n",
      "        app.mount(\n",
      "            \"/app\",\n",
      "            StaticFiles(directory=settings.FRONTEND_PATH, html=True),\n",
      "            name=settings.PROJECT_NAME,\n",
      "        )\n",
      "\n",
      "    app.include_router(health_check)\n",
      "    app.include_router(v1_router)\n",
      "\n",
      "    return app\n",
      "\n",
      "from .database import init_models, async_engine, get_db_session, get_sync_db_session\n",
      "from .config import settings, setup_logging\n",
      "from .exceptions import (\n",
      "    custom_http_exception_handler,\n",
      "    validation_exception_handler,\n",
      "    unhandled_exception_handler,\n",
      ")\n",
      "\n",
      "\n",
      "__all__ = [\n",
      "    \"settings\",\n",
      "    \"init_models\",\n",
      "    \"async_engine\",\n",
      "    \"setup_logging\",\n",
      "    \"get_db_session\",\n",
      "    \"get_sync_db_session\",\n",
      "    \"custom_http_exception_handler\",\n",
      "    \"validation_exception_handler\",\n",
      "    \"unhandled_exception_handler\",\n",
      "]\n",
      "\n",
      "import os\n",
      "import sys\n",
      "import logging\n",
      "from pydantic_settings import BaseSettings\n",
      "from typing import List, Optional, Literal\n",
      "\n",
      "\n",
      "class Settings(BaseSettings):\n",
      "    PROJECT_NAME: str = \"Resume Matcher\"\n",
      "    FRONTEND_PATH: str = os.path.join(os.path.dirname(__file__), \"frontend\", \"assets\")\n",
      "    ALLOWED_ORIGINS: List[str] = [\"https://www.resumematcher.fyi\"]\n",
      "    SYNC_DATABASE_URL: Optional[str]\n",
      "    ASYNC_DATABASE_URL: Optional[str]\n",
      "    SESSION_SECRET_KEY: Optional[str]\n",
      "    DB_ECHO: bool = False\n",
      "    PYTHONDONTWRITEBYTECODE: int = 1\n",
      "\n",
      "    class Config:\n",
      "        env_file = \".env\"\n",
      "\n",
      "\n",
      "settings = Settings()\n",
      "\n",
      "\n",
      "_LEVEL_BY_ENV: dict[Literal[\"production\", \"staging\", \"local\"], int] = {\n",
      "    \"production\": logging.INFO,\n",
      "    \"staging\": logging.DEBUG,\n",
      "    \"local\": logging.DEBUG,\n",
      "}\n",
      "\n",
      "\n",
      "def setup_logging() -> None:\n",
      "    \"\"\"\n",
      "    Configure the root logger exactly once,\n",
      "\n",
      "    * Console only (StreamHandler -> stderr)\n",
      "    * ISO - 8601 timestamps\n",
      "    * Env - based log level: production -> INFO, else DEBUG\n",
      "    * Prevents duplicate handler creation if called twice\n",
      "    \"\"\"\n",
      "    root = logging.getLogger()\n",
      "    if root.handlers:\n",
      "        return\n",
      "\n",
      "    env = settings.ENV.lower() if hasattr(settings, \"ENV\") else \"production\"\n",
      "    level = _LEVEL_BY_ENV.get(env, logging.INFO)\n",
      "\n",
      "    formatter = logging.Formatter(\n",
      "        fmt=\"[%(asctime)s - %(name)s - %(levelname)s] %(message)s\",\n",
      "        datefmt=\"%Y-%m-%dT%H:%M:%S%z\",\n",
      "    )\n",
      "\n",
      "    handler = logging.StreamHandler(sys.stderr)\n",
      "    handler.setFormatter(formatter)\n",
      "\n",
      "    root.setLevel(level)\n",
      "    root.addHandler(handler)\n",
      "\n",
      "    for noisy in (\"sqlalchemy.engine\", \"uvicorn.access\"):\n",
      "        logging.getLogger(noisy).setLevel(logging.WARNING)\n",
      "\n",
      "from __future__ import annotations\n",
      "\n",
      "from functools import lru_cache\n",
      "from typing import AsyncGenerator, Generator, Optional\n",
      "\n",
      "from sqlalchemy import event, create_engine\n",
      "from sqlalchemy.engine import Engine\n",
      "from sqlalchemy.orm import Session, sessionmaker\n",
      "from sqlalchemy.ext.asyncio import (\n",
      "    AsyncEngine,\n",
      "    AsyncSession,\n",
      "    async_sessionmaker,\n",
      "    create_async_engine,\n",
      ")\n",
      "\n",
      "from .config import settings\n",
      "from ..models.base import Base\n",
      "\n",
      "\n",
      "class _DatabaseSettings:\n",
      "    \"\"\"Pulled from environment once at import-time.\"\"\"\n",
      "\n",
      "    SYNC_DATABASE_URL: str = settings.SYNC_DATABASE_URL\n",
      "    ASYNC_DATABASE_URL: str = settings.ASYNC_DATABASE_URL\n",
      "    DB_ECHO: bool = settings.DB_ECHO\n",
      "\n",
      "    DB_CONNECT_ARGS = (\n",
      "        {\"check_same_thread\": False} if SYNC_DATABASE_URL.startswith(\"sqlite\") else {}\n",
      "    )\n",
      "\n",
      "\n",
      "settings = _DatabaseSettings()\n",
      "\n",
      "\n",
      "def _configure_sqlite(engine: Engine) -> None:\n",
      "    \"\"\"\n",
      "    For SQLite:\n",
      "\n",
      "    * Enable WAL mode (better concurrent writes).\n",
      "    * Enforce foreign-key constraints.\n",
      "    * Safe noop for non-SQLite engines.\n",
      "    \"\"\"\n",
      "    if engine.dialect.name != \"sqlite\":\n",
      "        return\n",
      "\n",
      "    @event.listens_for(engine, \"connect\", once=True)\n",
      "    def _set_sqlite_pragma(dbapi_conn, _):\n",
      "        cursor = dbapi_conn.cursor()\n",
      "        cursor.execute(\"PRAGMA journal_mode=WAL;\")\n",
      "        cursor.execute(\"PRAGMA foreign_keys=ON;\")\n",
      "        cursor.close()\n",
      "\n",
      "\n",
      "@lru_cache(maxsize=1)\n",
      "def _make_sync_engine() -> Engine:\n",
      "    \"\"\"Create (or return) the global synchronous Engine.\"\"\"\n",
      "    engine = create_engine(\n",
      "        settings.SYNC_DATABASE_URL,\n",
      "        echo=settings.DB_ECHO,\n",
      "        pool_pre_ping=True,\n",
      "        connect_args=settings.DB_CONNECT_ARGS,\n",
      "        future=True,\n",
      "    )\n",
      "    _configure_sqlite(engine)\n",
      "    return engine\n",
      "\n",
      "\n",
      "@lru_cache(maxsize=1)\n",
      "def _make_async_engine() -> AsyncEngine:\n",
      "    \"\"\"Create (or return) the global asynchronous Engine.\"\"\"\n",
      "    engine = create_async_engine(\n",
      "        settings.ASYNC_DATABASE_URL,\n",
      "        echo=settings.DB_ECHO,\n",
      "        pool_pre_ping=True,\n",
      "        connect_args=settings.DB_CONNECT_ARGS,\n",
      "        future=True,\n",
      "    )\n",
      "    _configure_sqlite(engine.sync_engine)\n",
      "    return engine\n",
      "\n",
      "\n",
      "# \n",
      "# Session factories\n",
      "# \n",
      "\n",
      "sync_engine: Engine = _make_sync_engine()\n",
      "async_engine: AsyncEngine = _make_async_engine()\n",
      "\n",
      "SessionLocal: sessionmaker[Session] = sessionmaker(\n",
      "    bind=sync_engine,\n",
      "    autoflush=False,\n",
      "    autocommit=False,\n",
      "    expire_on_commit=False,\n",
      ")\n",
      "\n",
      "AsyncSessionLocal: async_sessionmaker[AsyncSession] = async_sessionmaker(\n",
      "    bind=async_engine,\n",
      "    expire_on_commit=False,\n",
      ")\n",
      "\n",
      "\n",
      "def get_sync_db_session() -> Generator[Session, None, None]:\n",
      "    \"\"\"\n",
      "    Yield a *transactional* synchronous ``Session``.\n",
      "\n",
      "    Commits if no exception was raised, otherwise rolls back. Always closes.\n",
      "    Useful for CLI scripts or rare sync paths.\n",
      "    \"\"\"\n",
      "    db = SessionLocal()\n",
      "    try:\n",
      "        yield db\n",
      "        db.commit()\n",
      "    except Exception:\n",
      "        db.rollback()\n",
      "        raise\n",
      "    finally:\n",
      "        db.close()\n",
      "\n",
      "\n",
      "async def get_db_session() -> AsyncGenerator[AsyncSession, None]:\n",
      "    async with AsyncSessionLocal() as session:\n",
      "        try:\n",
      "            yield session\n",
      "            await session.commit()\n",
      "        except Exception:\n",
      "            await session.rollback()\n",
      "            raise\n",
      "\n",
      "\n",
      "async def init_models(Base: Base) -> None:\n",
      "    async with async_engine.begin() as conn:\n",
      "        await conn.run_sync(Base.metadata.create_all)\n",
      "\n",
      "import uvicorn\n",
      "from .base import create_app\n",
      "\n",
      "app = create_app()\n",
      "\n",
      "if __name__ == \"__main__\":\n",
      "    uvicorn.run(\"app.main:app\", host=\"0.0.0.0\", port=8000, reload=True)\n",
      "\n",
      "from sqlalchemy.types import JSON\n",
      "from sqlalchemy.orm import relationship\n",
      "from sqlalchemy import Column, String, Text, Integer, ForeignKey, DateTime, text\n",
      "\n",
      "from .base import Base\n",
      "from .association import job_resume_association\n",
      "\n",
      "\n",
      "class ProcessedJob(Base):\n",
      "    __tablename__ = \"processed_jobs\"\n",
      "\n",
      "    job_id = Column(\n",
      "        String,\n",
      "        ForeignKey(\"jobs.job_id\", ondelete=\"CASCADE\"),\n",
      "        primary_key=True,\n",
      "        index=True,\n",
      "    )\n",
      "    job_title = Column(String, nullable=False)\n",
      "    company_profile = Column(Text, nullable=True)\n",
      "    location = Column(String, nullable=True)\n",
      "    date_posted = Column(String, nullable=True)\n",
      "    employment_type = Column(String, nullable=True)\n",
      "    job_summary = Column(Text, nullable=False)\n",
      "    key_responsibilities = Column(JSON, nullable=True)\n",
      "    qualifications = Column(JSON, nullable=True)\n",
      "    compensation_and_benfits = Column(JSON, nullable=True)\n",
      "    application_info = Column(JSON, nullable=True)\n",
      "    extracted_keywords = Column(JSON, nullable=True)\n",
      "    processed_at = Column(\n",
      "        DateTime(timezone=True),\n",
      "        server_default=text(\"CURRENT_TIMESTAMP\"),\n",
      "        nullable=False,\n",
      "        index=True,\n",
      "    )\n",
      "\n",
      "    # one-to-many relation between user and jobs\n",
      "    # owner_id = Column(Integer, ForeignKey(\"users.id\"), nullable=False)\n",
      "    # owner = relationship(\"User\", back_populates=\"processed_jobs\")\n",
      "    raw_job = relationship(\"Job\", back_populates=\"raw_job_association\")\n",
      "\n",
      "    # many-to-many relationship in job and resume\n",
      "    processed_resumes = relationship(\n",
      "        \"ProcessedResume\",\n",
      "        secondary=job_resume_association,\n",
      "        back_populates=\"processed_jobs\",\n",
      "    )\n",
      "\n",
      "\n",
      "class Job(Base):\n",
      "    __tablename__ = \"jobs\"\n",
      "\n",
      "    id = Column(Integer, primary_key=True, index=True)\n",
      "    job_id = Column(String, unique=True, nullable=False)\n",
      "    resume_id = Column(String, ForeignKey(\"resumes.resume_id\"), nullable=False)\n",
      "    content = Column(Text, nullable=False)\n",
      "    created_at = Column(\n",
      "        DateTime(timezone=True),\n",
      "        server_default=text(\"CURRENT_TIMESTAMP\"),\n",
      "        nullable=False,\n",
      "        index=True,\n",
      "    )\n",
      "\n",
      "    raw_job_association = relationship(\n",
      "        \"ProcessedJob\", back_populates=\"raw_job\", uselist=False\n",
      "    )\n",
      "\n",
      "    resumes = relationship(\"Resume\", back_populates=\"jobs\")\n",
      "\n",
      "from sqlalchemy.types import JSON\n",
      "from sqlalchemy.orm import relationship\n",
      "from sqlalchemy import Column, String, Integer, ForeignKey, Text, DateTime, text\n",
      "\n",
      "from .base import Base\n",
      "from .association import job_resume_association\n",
      "\n",
      "\n",
      "class ProcessedResume(Base):\n",
      "    __tablename__ = \"processed_resumes\"\n",
      "\n",
      "    resume_id = Column(\n",
      "        String,\n",
      "        ForeignKey(\"resumes.resume_id\", ondelete=\"CASCADE\"),\n",
      "        primary_key=True,\n",
      "        index=True,\n",
      "    )\n",
      "    personal_data = Column(JSON, nullable=False)\n",
      "    experiences = Column(JSON, nullable=True)\n",
      "    projects = Column(JSON, nullable=True)\n",
      "    skills = Column(JSON, nullable=True)\n",
      "    research_work = Column(JSON, nullable=True)\n",
      "    achievements = Column(JSON, nullable=True)\n",
      "    education = Column(JSON, nullable=True)\n",
      "    extracted_keywords = Column(JSON, nullable=True)\n",
      "    processed_at = Column(\n",
      "        DateTime(timezone=True),\n",
      "        server_default=text(\"CURRENT_TIMESTAMP\"),\n",
      "        nullable=False,\n",
      "        index=True,\n",
      "    )\n",
      "\n",
      "    # owner_id = Column(Integer, ForeignKey(\"users.id\"), nullable=False)\n",
      "    # owner = relationship(\"User\", back_populates=\"processed_resumes\")\n",
      "    raw_resume = relationship(\"Resume\", back_populates=\"raw_resume_association\")\n",
      "\n",
      "    processed_jobs = relationship(\n",
      "        \"ProcessedJob\",\n",
      "        secondary=job_resume_association,\n",
      "        back_populates=\"processed_resumes\",\n",
      "    )\n",
      "\n",
      "\n",
      "class Resume(Base):\n",
      "    __tablename__ = \"resumes\"\n",
      "\n",
      "    id = Column(Integer, primary_key=True, index=True)\n",
      "    resume_id = Column(String, unique=True, nullable=False)\n",
      "    content = Column(Text, nullable=False)\n",
      "    content_type = Column(String, nullable=False)\n",
      "    created_at = Column(\n",
      "        DateTime(timezone=True),\n",
      "        server_default=text(\"CURRENT_TIMESTAMP\"),\n",
      "        nullable=False,\n",
      "        index=True,\n",
      "    )\n",
      "\n",
      "    raw_resume_association = relationship(\n",
      "        \"ProcessedResume\", back_populates=\"raw_resume\", uselist=False\n",
      "    )\n",
      "\n",
      "    jobs = relationship(\"Job\", back_populates=\"resumes\")\n",
      "\n",
      "from uuid import UUID\n",
      "from typing import List\n",
      "from pydantic import BaseModel, Field\n",
      "\n",
      "\n",
      "class JobUploadRequest(BaseModel):\n",
      "    job_descriptions: List[str] = Field(\n",
      "        ..., description=\"List of job descriptions in markdown format\"\n",
      "    )\n",
      "    resume_id: UUID = Field(..., description=\"UUID reference to the resume\")\n",
      "\n",
      "from typing import Optional, List\n",
      "from typing_extensions import Literal\n",
      "from pydantic import BaseModel, Field, HttpUrl, EmailStr, field_validator\n",
      "\n",
      "\n",
      "class CompanyProfile(BaseModel):\n",
      "    company_name: str = Field(..., alias=\"companyName\")\n",
      "    industry: Optional[str] = None\n",
      "    website: Optional[HttpUrl] = None\n",
      "    description: Optional[str] = None\n",
      "\n",
      "\n",
      "class Location(BaseModel):\n",
      "    city: str\n",
      "    state: Optional[str] = None\n",
      "    country: Optional[str] = None\n",
      "    remote_status: Literal[\"Fully Remote\", \"Hybrid\", \"On-site\", \"Remote\"] = Field(\n",
      "        ..., alias=\"remoteStatus\"\n",
      "    )\n",
      "\n",
      "    @field_validator(\"remote_status\", mode=\"before\")\n",
      "    def validate_remote_status(cls, value):\n",
      "        if isinstance(value, str):\n",
      "            v_lower = value.lower()\n",
      "            mapping = {\n",
      "                \"fully remote\": \"Fully Remote\",\n",
      "                \"hybrid\": \"Hybrid\",\n",
      "                \"on-site\": \"On-site\",\n",
      "                \"remote\": \"Remote\",\n",
      "            }\n",
      "            if v_lower in mapping:\n",
      "                return mapping[v_lower]\n",
      "        raise ValueError(\n",
      "            \"remote_status must be one of: Fully Remote, Hybrid, On-site, Remote (case insensitive)\"\n",
      "        )\n",
      "\n",
      "\n",
      "class Qualifications(BaseModel):\n",
      "    required: List[str]\n",
      "    preferred: Optional[List[str]] = None\n",
      "\n",
      "\n",
      "class CompensationAndBenefits(BaseModel):\n",
      "    salary_range: Optional[str] = Field(..., alias=\"salaryRange\")\n",
      "    benefits: Optional[List[str]] = None\n",
      "\n",
      "\n",
      "class ApplicationInfo(BaseModel):\n",
      "    how_to_apply: Optional[str] = Field(..., alias=\"howToApply\")\n",
      "    apply_link: Optional[str] = Field(..., alias=\"applyLink\")\n",
      "    contact_email: Optional[EmailStr] = Field(..., alias=\"contactEmail\")\n",
      "\n",
      "\n",
      "class StructuredJobModel(BaseModel):\n",
      "    job_title: str = Field(..., alias=\"jobTitle\")\n",
      "    company_profile: CompanyProfile = Field(..., alias=\"companyProfile\")\n",
      "    location: Location\n",
      "    date_posted: str = Field(..., alias=\"datePosted\")\n",
      "    employment_type: Literal[\n",
      "        \"Full-time\", \"Part-time\", \"Contract\", \"Internship\", \"Temporary\"\n",
      "    ] = Field(..., alias=\"employmentType\")\n",
      "    job_summary: str = Field(..., alias=\"jobSummary\")\n",
      "    key_responsibilities: List[str] = Field(..., alias=\"keyResponsibilities\")\n",
      "    qualifications: Qualifications\n",
      "    compensation_and_benefits: Optional[CompensationAndBenefits] = Field(\n",
      "        None, alias=\"compensationAndBenefits\"\n",
      "    )\n",
      "    application_info: Optional[ApplicationInfo] = Field(None, alias=\"applicationInfo\")\n",
      "    extracted_keywords: List[str] = Field(..., alias=\"extractedKeywords\")\n",
      "\n",
      "    class ConfigDict:\n",
      "        validate_by_name = True\n",
      "        str_strip_whitespace = True\n",
      "\n",
      "import uuid\n",
      "import json\n",
      "import logging\n",
      "\n",
      "from typing import List\n",
      "from pydantic import ValidationError\n",
      "from sqlalchemy import select\n",
      "from sqlalchemy.ext.asyncio import AsyncSession\n",
      "\n",
      "from app.agent import AgentManager\n",
      "from app.prompt import prompt_factory\n",
      "from app.schemas.json import json_schema_factory\n",
      "from app.models import Job, Resume, ProcessedJob\n",
      "from app.schemas.pydantic import StructuredJobModel\n",
      "\n",
      "logger = logging.getLogger(__name__)\n",
      "\n",
      "\n",
      "class JobService:\n",
      "    def __init__(self, db: AsyncSession):\n",
      "        self.db = db\n",
      "        self.json_agent_manager = AgentManager(model=\"gemma3:4b\")\n",
      "\n",
      "    async def create_and_store_job(self, job_data: dict) -> List[str]:\n",
      "        \"\"\"\n",
      "        Stores job data in the database and returns a list of job IDs.\n",
      "        \"\"\"\n",
      "        resume_id = str(job_data.get(\"resume_id\"))\n",
      "\n",
      "        if not await self._is_resume_available(resume_id):\n",
      "            raise AssertionError(\n",
      "                f\"resume corresponding to resume_id: {resume_id} not found\"\n",
      "            )\n",
      "\n",
      "        job_ids = []\n",
      "        for job_description in job_data.get(\"job_descriptions\", []):\n",
      "            job_id = str(uuid.uuid4())\n",
      "            job = Job(\n",
      "                job_id=job_id,\n",
      "                resume_id=str(resume_id),\n",
      "                content=job_description,\n",
      "            )\n",
      "\n",
      "            jb_id = await self._extract_and_store_structured_resume(\n",
      "                job_id=job_id, job_description_text=job_description\n",
      "            )\n",
      "            logger.info(f\"Job ID: {jb_id}\")\n",
      "            self.db.add(job)\n",
      "            job_ids.append(job_id)\n",
      "\n",
      "        self.db.commit()\n",
      "        return job_ids\n",
      "\n",
      "    async def _is_resume_available(self, resume_id: str) -> bool:\n",
      "        \"\"\"\n",
      "        Checks if a resume exists in the database.\n",
      "        \"\"\"\n",
      "        query = select(Resume).where(Resume.resume_id == resume_id)\n",
      "        result = await self.db.scalar(query)\n",
      "        return result is not None\n",
      "\n",
      "    async def _extract_and_store_structured_resume(\n",
      "        self, job_id, job_description_text: str\n",
      "    ):\n",
      "        \"\"\"\n",
      "        extract and store structured job data in the database\n",
      "        \"\"\"\n",
      "        structured_job = await self._extract_structured_json(job_description_text)\n",
      "        if not structured_job:\n",
      "            logger.info(\"Structured job extraction failed.\")\n",
      "            return None\n",
      "\n",
      "        processed_job = ProcessedJob(\n",
      "            job_id=job_id,\n",
      "            job_title=structured_job.get(\"job_title\"),\n",
      "            company_profile=structured_job.get(\"company_profile\"),\n",
      "            location=structured_job.get(\"location\"),\n",
      "            date_posted=structured_job.get(\"date_posted\"),\n",
      "            employment_type=structured_job.get(\"employment_type\"),\n",
      "            job_summary=structured_job.get(\"job_summary\"),\n",
      "            key_responsibilities=json.dumps(\n",
      "                {\"key_responsibilities\": structured_job.get(\"key_responsibilities\", [])}\n",
      "            )\n",
      "            if structured_job.get(\"key_responsibilities\")\n",
      "            else None,\n",
      "            qualifications=json.dumps(\n",
      "                {\"qualifications\": structured_job.get(\"qualifications\", [])}\n",
      "            )\n",
      "            if structured_job.get(\"qualifications\")\n",
      "            else None,\n",
      "            compensation_and_benfits=json.dumps(\n",
      "                {\n",
      "                    \"compensation_and_benfits\": structured_job.get(\n",
      "                        \"compensation_and_benfits\", []\n",
      "                    )\n",
      "                }\n",
      "            )\n",
      "            if structured_job.get(\"compensation_and_benfits\")\n",
      "            else None,\n",
      "            application_info=json.dumps(\n",
      "                {\"application_info\": structured_job.get(\"application_info\", [])}\n",
      "            )\n",
      "            if structured_job.get(\"application_info\")\n",
      "            else None,\n",
      "            extracted_keywords=json.dumps(\n",
      "                {\"extracted_keywords\": structured_job.get(\"extracted_keywords\", [])}\n",
      "            )\n",
      "            if structured_job.get(\"extracted_keywords\")\n",
      "            else None,\n",
      "        )\n",
      "\n",
      "        self.db.add(processed_job)\n",
      "        await self.db.flush()\n",
      "        await self.db.commit()\n",
      "\n",
      "        return job_id\n",
      "\n",
      "    async def _extract_structured_json(\n",
      "        self, job_description_text: str\n",
      "    ) -> StructuredJobModel | None:\n",
      "        \"\"\"\n",
      "        Uses the AgentManager+JSONWrapper to ask the LLM to\n",
      "        return the data in exact JSON schema we need.\n",
      "        \"\"\"\n",
      "        prompt_template = prompt_factory.get(\"structured_job\")\n",
      "        prompt = prompt_template.format(\n",
      "            json.dumps(json_schema_factory.get(\"structured_job\"), indent=2),\n",
      "            job_description_text,\n",
      "        )\n",
      "        logger.info(f\"Structured Job Prompt: {prompt}\")\n",
      "        raw_output = await self.json_agent_manager.run(prompt=prompt)\n",
      "\n",
      "        try:\n",
      "            structured_job: StructuredJobModel = StructuredJobModel.model_validate(\n",
      "                raw_output\n",
      "            )\n",
      "        except ValidationError as e:\n",
      "            logger.info(f\"Validation error: {e}\")\n",
      "            return None\n",
      "        return structured_job.model_dump()\n",
      "\n",
      "import os\n",
      "import uuid\n",
      "import json\n",
      "import tempfile\n",
      "import logging\n",
      "\n",
      "from markitdown import MarkItDown\n",
      "from sqlalchemy.ext.asyncio import AsyncSession\n",
      "from pydantic import ValidationError\n",
      "\n",
      "from app.models import Resume, ProcessedResume\n",
      "from app.agent import AgentManager\n",
      "from app.prompt import prompt_factory\n",
      "from app.schemas.json import json_schema_factory\n",
      "from app.schemas.pydantic import StructuredResumeModel\n",
      "\n",
      "logger = logging.getLogger(__name__)\n",
      "\n",
      "\n",
      "class ResumeService:\n",
      "    def __init__(self, db: AsyncSession):\n",
      "        self.db = db\n",
      "        self.md = MarkItDown(enable_plugins=False)\n",
      "        self.json_agent_manager = AgentManager(model=\"gemma3:4b\")\n",
      "\n",
      "    async def convert_and_store_resume(\n",
      "        self, file_bytes: bytes, file_type: str, filename: str, content_type: str = \"md\"\n",
      "    ):\n",
      "        \"\"\"\n",
      "        Converts resume file (PDF/DOCX) to text using MarkItDown and stores it in the database.\n",
      "\n",
      "        Args:\n",
      "            file_bytes: Raw bytes of the uploaded file\n",
      "            file_type: MIME type of the file (\"application/pdf\" or \"application/vnd.openxmlformats-officedocument.wordprocessingml.document\")\n",
      "            filename: Original filename\n",
      "            content_type: Output format (\"md\" for markdown or \"html\")\n",
      "\n",
      "        Returns:\n",
      "            None\n",
      "        \"\"\"\n",
      "        with tempfile.NamedTemporaryFile(\n",
      "            delete=False, suffix=self._get_file_extension(file_type)\n",
      "        ) as temp_file:\n",
      "            temp_file.write(file_bytes)\n",
      "            temp_path = temp_file.name\n",
      "\n",
      "        try:\n",
      "            result = self.md.convert(temp_path)\n",
      "            text_content = result.text_content\n",
      "            resume_id = await self._store_resume_in_db(text_content, content_type)\n",
      "\n",
      "            await self._extract_and_store_structured_resume(\n",
      "                resume_id=resume_id, resume_text=text_content\n",
      "            )\n",
      "\n",
      "            return resume_id\n",
      "        finally:\n",
      "            if os.path.exists(temp_path):\n",
      "                os.remove(temp_path)\n",
      "\n",
      "    def _get_file_extension(self, file_type: str) -> str:\n",
      "        \"\"\"Returns the appropriate file extension based on MIME type\"\"\"\n",
      "        if file_type == \"application/pdf\":\n",
      "            return \".pdf\"\n",
      "        elif (\n",
      "            file_type\n",
      "            == \"application/vnd.openxmlformats-officedocument.wordprocessingml.document\"\n",
      "        ):\n",
      "            return \".docx\"\n",
      "        return \"\"\n",
      "\n",
      "    async def _store_resume_in_db(self, text_content: str, content_type: str):\n",
      "        \"\"\"\n",
      "        Stores the parsed resume content in the database.\n",
      "        \"\"\"\n",
      "        resume_id = str(uuid.uuid4())\n",
      "        resume = Resume(\n",
      "            resume_id=resume_id, content=text_content, content_type=content_type\n",
      "        )\n",
      "\n",
      "        self.db.add(resume)\n",
      "        await self.db.flush()\n",
      "        await self.db.commit()\n",
      "\n",
      "        return resume_id\n",
      "\n",
      "    async def _extract_and_store_structured_resume(\n",
      "        self, resume_id, resume_text: str\n",
      "    ) -> None:\n",
      "        \"\"\"\n",
      "        extract and store structured resume data in the database\n",
      "        \"\"\"\n",
      "        structured_resume = await self._extract_structured_json(resume_text)\n",
      "        if not structured_resume:\n",
      "            logger.info(\"Structured resume extraction failed.\")\n",
      "            return None\n",
      "\n",
      "        processed_resume = ProcessedResume(\n",
      "            resume_id=resume_id,\n",
      "            personal_data=json.dumps(structured_resume.get(\"personal_data\", {}))\n",
      "            if structured_resume.get(\"personal_data\")\n",
      "            else None,\n",
      "            experiences=json.dumps(\n",
      "                {\"experiences\": structured_resume.get(\"experiences\", [])}\n",
      "            )\n",
      "            if structured_resume.get(\"experiences\")\n",
      "            else None,\n",
      "            projects=json.dumps({\"projects\": structured_resume.get(\"projects\", [])})\n",
      "            if structured_resume.get(\"projects\")\n",
      "            else None,\n",
      "            skills=json.dumps({\"skills\": structured_resume.get(\"skills\", [])})\n",
      "            if structured_resume.get(\"skills\")\n",
      "            else None,\n",
      "            research_work=json.dumps(\n",
      "                {\"research_work\": structured_resume.get(\"research_work\", [])}\n",
      "            )\n",
      "            if structured_resume.get(\"research_work\")\n",
      "            else None,\n",
      "            achievements=json.dumps(\n",
      "                {\"achievements\": structured_resume.get(\"achievements\", [])}\n",
      "            )\n",
      "            if structured_resume.get(\"achievements\")\n",
      "            else None,\n",
      "            education=json.dumps({\"education\": structured_resume.get(\"education\", [])})\n",
      "            if structured_resume.get(\"education\")\n",
      "            else None,\n",
      "            extracted_keywords=json.dumps(\n",
      "                {\"extracted_keywords\": structured_resume.get(\"extracted_keywords\", [])}\n",
      "                if structured_resume.get(\"extracted_keywords\")\n",
      "                else None\n",
      "            ),\n",
      "        )\n",
      "\n",
      "        self.db.add(processed_resume)\n",
      "        await self.db.commit()\n",
      "\n",
      "    async def _extract_structured_json(\n",
      "        self, resume_text: str\n",
      "    ) -> StructuredResumeModel | None:\n",
      "        \"\"\"\n",
      "        Uses the AgentManager+JSONWrapper to ask the LLM to\n",
      "        return the data in exact JSON schema we need.\n",
      "        \"\"\"\n",
      "        prompt_template = prompt_factory.get(\"structured_resume\")\n",
      "        prompt = prompt_template.format(\n",
      "            json.dumps(json_schema_factory.get(\"structured_resume\"), indent=2),\n",
      "            resume_text,\n",
      "        )\n",
      "        logger.info(f\"Structured Resume Prompt: {prompt}\")\n",
      "        raw_output = await self.json_agent_manager.run(prompt=prompt)\n",
      "\n",
      "        try:\n",
      "            structured_resume: StructuredResumeModel = (\n",
      "                StructuredResumeModel.model_validate(raw_output)\n",
      "            )\n",
      "        except ValidationError as e:\n",
      "            logger.info(f\"Validation error: {e}\")\n",
      "            return None\n",
      "        return structured_resume.model_dump()\n",
      "\n",
      "from sqlalchemy import text\n",
      "from sqlalchemy.ext.asyncio import AsyncSession\n",
      "from fastapi import APIRouter, status, Depends\n",
      "\n",
      "from app.core import get_db_session\n",
      "\n",
      "health_check = APIRouter()\n",
      "\n",
      "\n",
      "@health_check.get(\"/ping\", tags=[\"Health check\"], status_code=status.HTTP_200_OK)\n",
      "async def ping(db: AsyncSession = Depends(get_db_session)):\n",
      "    \"\"\"\n",
      "    health check endpoint\n",
      "    \"\"\"\n",
      "    try:\n",
      "        result = await db.execute(text(\"SELECT 1\"))\n",
      "        db_status = \"reachable\" if result.fetchone() is not None else \"not reachable\"\n",
      "    except Exception as e:\n",
      "        db_status = f\"error: {str(e)}\"\n",
      "    return {\"message\": \"pong\", \"database\": db_status}\n",
      "\n",
      "# Generic async LLM-agent - automatic provider selection\n",
      "\n",
      "# * If caller supplies `openai_api_key` (arg or ENV), we use OpenAIProvider.\n",
      "# * Else we fallback to a local Ollama model.\n",
      "# * If neither is available, we raise -> ProviderError.\n",
      "\n",
      "from .manager import AgentManager, EmbeddingManager\n",
      "\n",
      "__all__ = [\"AgentManager\", \"EmbeddingManager\"]\n",
      "\n",
      "import os\n",
      "from typing import Dict, Any\n",
      "\n",
      "from .exceptions import ProviderError\n",
      "from .strategies.base import Strategy\n",
      "from .strategies.wrapper import JSONWrapper\n",
      "from .providers.ollama import OllamaProvider, OllamaEmbeddingProvider\n",
      "from .providers.openai import OpenAIProvider, OpenAIEmbeddingProvider\n",
      "\n",
      "\n",
      "class AgentManager:\n",
      "    def __init__(\n",
      "        self, strategy: Strategy | None = None, model: str = \"gemma3:4b\"\n",
      "    ) -> None:\n",
      "        self.strategy = strategy or JSONWrapper()\n",
      "        self.model = model\n",
      "\n",
      "    async def _get_provider(self, **kwargs: Any) -> OllamaProvider | OpenAIProvider:\n",
      "        api_key = kwargs.get(\"openai_api_key\", os.getenv(\"OPENAI_API_KEY\"))\n",
      "        if api_key:\n",
      "            return OpenAIProvider(api_key=api_key)\n",
      "\n",
      "        model = kwargs.get(\"model\", self.model)\n",
      "        installed_ollama_models = await OllamaProvider.get_installed_models()\n",
      "        if model not in installed_ollama_models:\n",
      "            raise ProviderError(\n",
      "                f\"Ollama Model '{model}' is not found. Run `ollama pull {model} or pick from any available models {installed_ollama_models}\"\n",
      "            )\n",
      "        return OllamaProvider(model_name=model)\n",
      "\n",
      "    async def run(self, prompt: str, **kwargs: Any) -> Dict[str, Any]:\n",
      "        \"\"\"\n",
      "        Run the agent with the given prompt and generation arguments.\n",
      "        \"\"\"\n",
      "        provider = await self._get_provider(**kwargs)\n",
      "        return await self.strategy(prompt, provider, **kwargs)\n",
      "\n",
      "\n",
      "class EmbeddingManager:\n",
      "    def __init__(self, model: str = \"nomic-embed-text:137m-v1.5-fp16\") -> None:\n",
      "        self._model = model\n",
      "\n",
      "    async def _get_embedding_provider(\n",
      "        self, **kwargs: Any\n",
      "    ) -> OllamaEmbeddingProvider | OpenAIEmbeddingProvider:\n",
      "        api_key = kwargs.get(\"openai_api_key\", os.getenv(\"OPENAI_API_KEY\"))\n",
      "        if api_key:\n",
      "            return OpenAIEmbeddingProvider(api_key=api_key)\n",
      "        model = kwargs.get(\"embedding_model\", self._model)\n",
      "        installed_ollama_models = await OllamaProvider.get_installed_models()\n",
      "        if model not in installed_ollama_models:\n",
      "            raise ProviderError(\n",
      "                f\"Ollama Model '{model}' is not found. Run `ollama pull {model} or pick from any available models {installed_ollama_models}\"\n",
      "            )\n",
      "        return OllamaEmbeddingProvider(model_name=model)\n",
      "\n",
      "    async def embed(self, text: str, **kwargs: Any) -> list[float]:\n",
      "        \"\"\"\n",
      "        Get the embedding for the given text.\n",
      "        \"\"\"\n",
      "        provider = await self._get_embedding_provider(**kwargs)\n",
      "        return await provider.embed(text)\n",
      "\n",
      "from typing import Any\n",
      "from abc import ABC, abstractmethod\n",
      "\n",
      "\n",
      "class Provider(ABC):\n",
      "    \"\"\"\n",
      "    Abstract base class for providers.\n",
      "    \"\"\"\n",
      "\n",
      "    @abstractmethod\n",
      "    async def __call__(self, prompt: str, **generation_args: Any) -> str: ...\n",
      "\n",
      "\n",
      "class EmbeddingProvider(ABC):\n",
      "    \"\"\"\n",
      "    Abstract base class for embedding providers.\n",
      "    \"\"\"\n",
      "\n",
      "    @abstractmethod\n",
      "    async def embed(self, text: str) -> list[float]: ...\n",
      "\n",
      "import logging\n",
      "import ollama\n",
      "\n",
      "from typing import Any, Dict, List, Optional\n",
      "from fastapi.concurrency import run_in_threadpool\n",
      "\n",
      "from ..exceptions import ProviderError\n",
      "from .base import Provider, EmbeddingProvider\n",
      "\n",
      "logger = logging.getLogger(__name__)\n",
      "\n",
      "\n",
      "class OllamaProvider(Provider):\n",
      "    def __init__(self, model_name: str = \"gemma3:4b\", host: Optional[str] = None):\n",
      "        self.model = model_name\n",
      "        self._client = ollama.Client(host=host) if host else ollama.Client()\n",
      "\n",
      "    @staticmethod\n",
      "    async def get_installed_models(host: Optional[str] = None) -> List[str]:\n",
      "        \"\"\"\n",
      "        List all installed models.\n",
      "        \"\"\"\n",
      "\n",
      "        def _list_sync() -> List[str]:\n",
      "            client = ollama.Client(host=host) if host else ollama.Client()\n",
      "            return [model_class.model for model_class in client.list().models]\n",
      "\n",
      "        return await run_in_threadpool(_list_sync)\n",
      "\n",
      "    def _generate_sync(self, prompt: str, options: Dict[str, Any]) -> str:\n",
      "        \"\"\"\n",
      "        Generate a response from the model.\n",
      "        \"\"\"\n",
      "        try:\n",
      "            response = self._client.generate(\n",
      "                prompt=prompt,\n",
      "                model=self.model,\n",
      "                options=options,\n",
      "            )\n",
      "            return response[\"response\"].strip()\n",
      "        except Exception as e:\n",
      "            logger.error(f\"ollama sync error: {e}\")\n",
      "            raise ProviderError(f\"Ollama - Error generating response: {e}\")\n",
      "\n",
      "    async def __call__(self, prompt: str, **generation_args: Any) -> str:\n",
      "        opts = {\n",
      "            \"temperature\": generation_args.get(\"temperature\", 0),\n",
      "            \"top_p\": generation_args.get(\"top_p\", 0.9),\n",
      "            \"top_k\": generation_args.get(\"top_k\", 40),\n",
      "            \"num_ctx\": generation_args.get(\"max_length\", 20000),\n",
      "        }\n",
      "        return await run_in_threadpool(self._generate_sync, prompt, opts)\n",
      "\n",
      "\n",
      "class OllamaEmbeddingProvider(EmbeddingProvider):\n",
      "    def __init__(\n",
      "        self,\n",
      "        embedding_model: str = \"nomic-embed-text:137m-v1.5-fp16\",\n",
      "        host: Optional[str] = None,\n",
      "    ):\n",
      "        self._model = embedding_model\n",
      "        self._client = ollama.Client(host=host) if host else ollama.Client()\n",
      "\n",
      "    async def embed(self, text: str) -> List[float]:\n",
      "        \"\"\"\n",
      "        Generate an embedding for the given text.\n",
      "        \"\"\"\n",
      "        try:\n",
      "            response = run_in_threadpool(\n",
      "                self._client.embed,\n",
      "                input=text,\n",
      "                model=self._model,\n",
      "            )\n",
      "            return response[\"embedding\"]\n",
      "        except Exception as e:\n",
      "            logger.error(f\"ollama embedding error: {e}\")\n",
      "            raise ProviderError(f\"Ollama - Error generating embedding: {e}\")\n",
      "\n",
      "import os\n",
      "import logging\n",
      "\n",
      "from openai import OpenAI\n",
      "from typing import Any, Dict\n",
      "from fastapi.concurrency import run_in_threadpool\n",
      "\n",
      "from ..exceptions import ProviderError\n",
      "from .base import Provider, EmbeddingProvider\n",
      "\n",
      "logger = logging.getLogger(__name__)\n",
      "\n",
      "\n",
      "class OpenAIProvider(Provider):\n",
      "    def __init__(self, api_key: str | None = None, model: str = \"gpt-4o\"):\n",
      "        api_key = api_key or os.getenv(\"OPENAI_API_KEY\")\n",
      "        if not api_key:\n",
      "            raise ProviderError(\"OpenAI API key is missing\")\n",
      "        self._client = OpenAI(api_key=api_key)\n",
      "        self.model = model\n",
      "        self.instructions = \"\"\n",
      "\n",
      "    def _generate_sync(self, prompt: str, options: Dict[str, Any]) -> str:\n",
      "        try:\n",
      "            response = self._client.responses.create(\n",
      "                model=self.model,\n",
      "                instructions=self.instructions,\n",
      "                input=prompt,\n",
      "                **options,\n",
      "            )\n",
      "            return response.output_text\n",
      "        except Exception as e:\n",
      "            raise ProviderError(f\"OpenAI - error generating response: {e}\") from e\n",
      "\n",
      "    async def __call__(self, prompt: str, **generation_args: Any) -> str:\n",
      "        opts = {\n",
      "            \"temperature\": generation_args.get(\"temperature\", 0),\n",
      "            \"top_p\": generation_args.get(\"top_p\", 0.9),\n",
      "            \"top_k\": generation_args.get(\"top_k\", 40),\n",
      "            \"max_tokens\": generation_args.get(\"max_length\", 20000),\n",
      "        }\n",
      "        return await run_in_threadpool(self._generate_sync, prompt, opts)\n",
      "\n",
      "\n",
      "class OpenAIEmbeddingProvider(EmbeddingProvider):\n",
      "    def __init__(\n",
      "        self,\n",
      "        api_key: str | None = None,\n",
      "        embedding_model: str = \"text-embedding-ada-002\",\n",
      "    ):\n",
      "        api_key = api_key or os.getenv(\"OPENAI_API_KEY\")\n",
      "        if not api_key:\n",
      "            raise ProviderError(\"OpenAI API key is missing\")\n",
      "        self._client = OpenAI(api_key=api_key)\n",
      "        self._model = embedding_model\n",
      "\n",
      "    async def embed(self, text: str) -> list[float]:\n",
      "        try:\n",
      "            response = await run_in_threadpool(\n",
      "                self._client.embeddings.create, input=text, model=self._model\n",
      "            )\n",
      "            return response[\"data\"][0][\"embedding\"]\n",
      "        except Exception as e:\n",
      "            raise ProviderError(f\"OpenAI - error generating embedding: {e}\") from e\n",
      "\n",
      "from .job_service import JobService\n",
      "from .resume_service import ResumeService\n",
      "from .scoring_service import ScoringService\n",
      "\n",
      "__all__ = [\"ResumeService\", \"JobService\", \"ScoringService\"]\n",
      "\n",
      "import logging\n",
      "import numpy as np\n",
      "\n",
      "from typing import Dict, Optional\n",
      "from sqlalchemy.future import select\n",
      "from sqlalchemy.ext.asyncio import AsyncSession\n",
      "from fastapi.concurrency import run_in_threadpool\n",
      "\n",
      "from app.models import Resume, Job\n",
      "from app.agent import EmbeddingManager, AgentManager\n",
      "\n",
      "logger = logging.getLogger(__name__)\n",
      "\n",
      "\n",
      "class ScoringService:\n",
      "    \"\"\"\n",
      "    Service to handle scoring of resumes and jobs using embeddings.\n",
      "    Fetches Resume and Job data from the database, computes embeddings,\n",
      "    and calculates cosine similarity scores. Uses LLM for iteratively improving\n",
      "    the scoring process.\n",
      "    \"\"\"\n",
      "\n",
      "    def __init__(\n",
      "        self, resume_id: str, job_id: str, db: AsyncSession, max_retries: int = 5\n",
      "    ):\n",
      "        self.db = db\n",
      "        self.job_id = job_id\n",
      "        self.resume_id = resume_id\n",
      "        self.max_retries = max_retries\n",
      "        self.agent_manager = AgentManager()\n",
      "        self.embedding_manager = EmbeddingManager()\n",
      "\n",
      "    async def get_resume_embedding(self, resume_id: str) -> Optional[np.ndarray]:\n",
      "        \"\"\"\n",
      "        Fetches the resume from the database and computes its embedding.\n",
      "        \"\"\"\n",
      "        query = select(Resume).where(Resume.resume_id == resume_id)\n",
      "        result = await self.db.execute(query)\n",
      "        resume = result.scalars().first()\n",
      "\n",
      "        if not resume:\n",
      "            return None\n",
      "\n",
      "        return await run_in_threadpool(\n",
      "            self.embedding_manager.get_embedding,\n",
      "            resume.content,\n",
      "            model=self.embedding_model,\n",
      "        )\n",
      "\n",
      "PROMPT = \"\"\"\n",
      "You are an expert resume editor and talent acquisition specialist. Your task is to revise the following resume so that it aligns as closely as possible with the provided job description and extracted job keywords, in order to maximize the cosine similarity between the resume and the job keywords.\n",
      "\n",
      "Instructions:\n",
      "- Carefully review the job description and the list of extracted job keywords.\n",
      "- Update the candidate's resume by:\n",
      "  - Emphasizing and incorporating relevant skills, experiences, and keywords from the job description and keyword list.\n",
      "  - Rewriting, adding, or removing resume content as needed to better match the job requirements.\n",
      "  - Maintaining a natural, professional tone and avoiding keyword stuffing.\n",
      "- ONLY output the updated resume. Do not include any explanations, commentary, or formatting outside of the resume itself.\n",
      "\n",
      "Job Description:\n",
      "{raw_job_description}\n",
      "\n",
      "Extracted Job Keywords:\n",
      "{extracted_job_keywords}\n",
      "\n",
      "Original Resume:\n",
      "{raw_resume}\n",
      "\n",
      "[Optional: Extracted Resume Keywords:\n",
      "{extracted_resume_keywords}]\n",
      "\n",
      "ONLY OUTPUT THE UPDATED RESUME.\n",
      "\n",
      "\"\"\"\n",
      "\n",
      "from typing import Optional\n",
      "\n",
      "\n",
      "class ResumeNotFoundError(Exception):\n",
      "    \"\"\"\n",
      "    Exception raised when a resume is not found in the database.\n",
      "    \"\"\"\n",
      "\n",
      "    def __init__(self, resume_id: Optional[str] = None, message: Optional[str] = None):\n",
      "        if resume_id and not message:\n",
      "            message = f\"Resume with ID {resume_id} not found.\"\n",
      "        elif not message:\n",
      "            message = \"Resume not found.\"\n",
      "        super().__init__(message)\n",
      "        self.resume_id = resume_id\n",
      "\n",
      "\n",
      "class JobNotFoundError(Exception):\n",
      "    \"\"\"\n",
      "    Exception raised when a job is not found in the database.\n",
      "    \"\"\"\n",
      "\n",
      "    def __init__(self, job_id: Optional[str] = None, message: Optional[str] = None):\n",
      "        if job_id and not message:\n",
      "            message = f\"Job with ID {job_id} not found.\"\n",
      "        elif not message:\n",
      "            message = \"Job not found.\"\n",
      "        super().__init__(message)\n",
      "        self.job_id = job_id\n",
      "\n",
      "\n",
      "class EmbeddingError(Exception):\n",
      "    \"\"\"\n",
      "    Exception raised when there is an error in embedding.\n",
      "    \"\"\"\n",
      "\n",
      "    def __init__(self, message: str = None):\n",
      "        super().__init__(message)\n",
      "        self.message = message\n",
      "\n",
      "\n",
      "class ResumeParsingError(Exception):\n",
      "    \"\"\"\n",
      "    Exception raised when a resume processing and storing in the database failed.\n",
      "    \"\"\"\n",
      "\n",
      "    def __init__(self, resume_id: Optional[str] = None, message: Optional[str] = None):\n",
      "        if resume_id and not message:\n",
      "            message = f\"Parsing of resume with ID {resume_id} failed.\"\n",
      "        elif not message:\n",
      "            message = \"Parsed resume not found.\"\n",
      "        super().__init__(message)\n",
      "        self.resume_id = resume_id\n",
      "\n",
      "import json\n",
      "import logging\n",
      "import numpy as np\n",
      "\n",
      "from sqlalchemy.future import select\n",
      "from typing import Dict, Optional, Tuple\n",
      "from sqlalchemy.ext.asyncio import AsyncSession\n",
      "from fastapi.concurrency import run_in_threadpool\n",
      "\n",
      "from app.agent import EmbeddingManager, AgentManager\n",
      "from app.models import Resume, Job, ProcessedResume, ProcessedJob\n",
      "from .exceptions import (\n",
      "    ResumeNotFoundError,\n",
      "    JobNotFoundError,\n",
      "    ResumeParsingError,\n",
      ")\n",
      "\n",
      "logger = logging.getLogger(__name__)\n",
      "\n",
      "\n",
      "class ScoringService:\n",
      "    \"\"\"\n",
      "    Service to handle scoring of resumes and jobs using embeddings.\n",
      "    Fetches Resume and Job data from the database, computes embeddings,\n",
      "    and calculates cosine similarity scores. Uses LLM for iteratively improving\n",
      "    the scoring process.\n",
      "    \"\"\"\n",
      "\n",
      "    def __init__(\n",
      "        self, resume_id: str, job_id: str, db: AsyncSession, max_retries: int = 5\n",
      "    ):\n",
      "        self.db = db\n",
      "        self.job_id = job_id\n",
      "        self.resume_id = resume_id\n",
      "        self.max_retries = max_retries\n",
      "        self.agent_manager = AgentManager()\n",
      "        self.embedding_manager = EmbeddingManager()\n",
      "\n",
      "    async def _get_resume(\n",
      "        self, resume_id: str\n",
      "    ) -> Tuple[Resume | None, ProcessedResume | None]:\n",
      "        \"\"\"\n",
      "        Fetches the resume from the database.\n",
      "        \"\"\"\n",
      "        query = select(Resume).where(Resume.resume_id == resume_id)\n",
      "        result = await self.db.execute(query)\n",
      "        resume = result.scalars().first()\n",
      "\n",
      "        if not resume:\n",
      "            raise ResumeNotFoundError(self.resume_id)\n",
      "\n",
      "        query = select(ProcessedResume).where(ProcessedResume.resume_id == resume_id)\n",
      "        result = await self.db.execute(query)\n",
      "        processed_resume = result.scalars().first()\n",
      "\n",
      "        if not processed_resume:\n",
      "            ResumeParsingError(self.resume_id)\n",
      "\n",
      "        return resume, processed_resume\n",
      "\n",
      "    async def _get_job(self, job_id: str) -> Tuple[Job | None, ProcessedJob | None]:\n",
      "        \"\"\"\n",
      "        Fetches the job from the database.\n",
      "        \"\"\"\n",
      "        query = select(Job).where(Job.job_id == job_id)\n",
      "        result = await self.db.execute(query)\n",
      "        job = result.scalars().first()\n",
      "\n",
      "        if not job:\n",
      "            raise JobNotFoundError(self.job_id)\n",
      "\n",
      "        query = select(ProcessedJob).where(ProcessedJob.job_id == job_id)\n",
      "        result = await self.db.execute(query)\n",
      "        processed_job = result.scalars().first()\n",
      "\n",
      "        if not processed_job:\n",
      "            ResumeParsingError(self.resume_id)\n",
      "\n",
      "        return job, processed_job\n",
      "\n",
      "    def calculate_cosine_similarity(\n",
      "        self,\n",
      "        extracted_job_keywords_embedding: np.ndarray,\n",
      "        resume_embedding: np.ndarray,\n",
      "    ) -> float:\n",
      "        \"\"\"\n",
      "        Calculates the cosine similarity between two embeddings.\n",
      "        \"\"\"\n",
      "        if resume_embedding is None or extracted_job_keywords_embedding is None:\n",
      "            return 0.0\n",
      "        return np.dot(extracted_job_keywords_embedding, resume_embedding) / (\n",
      "            np.linalg.norm(extracted_job_keywords_embedding)\n",
      "            * np.linalg.norm(resume_embedding)\n",
      "        )\n",
      "\n",
      "    async def improve_score_with_llm(\n",
      "        self, resume: Resume, job: Job, cosine_similarity_score: float\n",
      "    ) -> str:\n",
      "        \"\"\"\n",
      "        Uses LLM to improve the score based on resume and job description.\n",
      "        \"\"\"\n",
      "        prompt = f\"Resume: {resume.content}\\nJob Description: {job.content}\\nCurrent Score: Improve the score.\"\n",
      "        response = await self.agent_manager(prompt)\n",
      "        return response\n",
      "\n",
      "    async def run(self) -> Dict:\n",
      "        \"\"\"\n",
      "        Main method to run the scoring process.\n",
      "        \"\"\"\n",
      "        resume, processed_resume = await self._get_resume(self.resume_id)\n",
      "        job, processed_job = await self._get_job(self.job_id)\n",
      "\n",
      "        extracted_job_keywords = json.loads(processed_job.extracted_keywords)\n",
      "\n",
      "        resume_embedding = await self.embedding_manager(text=resume.content)\n",
      "        extracted_job_keywords_embedding = await self.embedding_manager(\n",
      "            text=\", \".join(extracted_job_keywords.get(\"extracted_keywords\", []))\n",
      "        )\n",
      "\n",
      "        cosine_similarity_score = self.calculate_cosine_similarity(\n",
      "            extracted_job_keywords_embedding, resume_embedding\n",
      "        )\n",
      "        improved_score = await self.improve_score_with_llm(\n",
      "            resume, job, cosine_similarity_score\n",
      "        )\n",
      "\n",
      "        return {\n",
      "            \"resume_id\": self.resume_id,\n",
      "            \"job_id\": self.job_id,\n",
      "            \"score\": cosine_similarity_score,\n",
      "            \"improved_score\": improved_score,\n",
      "        }\n",
      "\n",
      "import os\n",
      "from typing import Dict, Any\n",
      "\n",
      "from .exceptions import ProviderError\n",
      "from .strategies.wrapper import JSONWrapper, MDWrapper\n",
      "from .providers.ollama import OllamaProvider, OllamaEmbeddingProvider\n",
      "from .providers.openai import OpenAIProvider, OpenAIEmbeddingProvider\n",
      "\n",
      "\n",
      "class AgentManager:\n",
      "    def __init__(self, strategy: str | None = None, model: str = \"gemma3:4b\") -> None:\n",
      "        match strategy:\n",
      "            case \"md\":\n",
      "                self.strategy = MDWrapper()\n",
      "            case \"json\":\n",
      "                self.strategy = JSONWrapper()\n",
      "            case _:\n",
      "                self.strategy = JSONWrapper()\n",
      "        self.model = model\n",
      "\n",
      "    async def _get_provider(self, **kwargs: Any) -> OllamaProvider | OpenAIProvider:\n",
      "        api_key = kwargs.get(\"openai_api_key\", os.getenv(\"OPENAI_API_KEY\"))\n",
      "        if api_key:\n",
      "            return OpenAIProvider(api_key=api_key)\n",
      "\n",
      "        model = kwargs.get(\"model\", self.model)\n",
      "        installed_ollama_models = await OllamaProvider.get_installed_models()\n",
      "        if model not in installed_ollama_models:\n",
      "            raise ProviderError(\n",
      "                f\"Ollama Model '{model}' is not found. Run `ollama pull {model} or pick from any available models {installed_ollama_models}\"\n",
      "            )\n",
      "        return OllamaProvider(model_name=model)\n",
      "\n",
      "    async def run(self, prompt: str, **kwargs: Any) -> Dict[str, Any]:\n",
      "        \"\"\"\n",
      "        Run the agent with the given prompt and generation arguments.\n",
      "        \"\"\"\n",
      "        provider = await self._get_provider(**kwargs)\n",
      "        return await self.strategy(prompt, provider, **kwargs)\n",
      "\n",
      "\n",
      "class EmbeddingManager:\n",
      "    def __init__(self, model: str = \"nomic-embed-text:137m-v1.5-fp16\") -> None:\n",
      "        self._model = model\n",
      "\n",
      "    async def _get_embedding_provider(\n",
      "        self, **kwargs: Any\n",
      "    ) -> OllamaEmbeddingProvider | OpenAIEmbeddingProvider:\n",
      "        api_key = kwargs.get(\"openai_api_key\", os.getenv(\"OPENAI_API_KEY\"))\n",
      "        if api_key:\n",
      "            return OpenAIEmbeddingProvider(api_key=api_key)\n",
      "        model = kwargs.get(\"embedding_model\", self._model)\n",
      "        installed_ollama_models = await OllamaProvider.get_installed_models()\n",
      "        if model not in installed_ollama_models:\n",
      "            raise ProviderError(\n",
      "                f\"Ollama Model '{model}' is not found. Run `ollama pull {model} or pick from any available models {installed_ollama_models}\"\n",
      "            )\n",
      "        return OllamaEmbeddingProvider(model_name=model)\n",
      "\n",
      "    async def embed(self, text: str, **kwargs: Any) -> list[float]:\n",
      "        \"\"\"\n",
      "        Get the embedding for the given text.\n",
      "        \"\"\"\n",
      "        provider = await self._get_embedding_provider(**kwargs)\n",
      "        return await provider.embed(text)\n",
      "\n",
      "import json\n",
      "import logging\n",
      "from typing import Any, Dict\n",
      "\n",
      "from .base import Strategy\n",
      "from ..providers.base import Provider\n",
      "from ..exceptions import StrategyError\n",
      "\n",
      "\n",
      "logger = logging.getLogger(__name__)\n",
      "\n",
      "\n",
      "class JSONWrapper(Strategy):\n",
      "    async def __call__(\n",
      "        self, prompt: str, provider: Provider, **generation_args: Any\n",
      "    ) -> Dict[str, Any]:\n",
      "        \"\"\"\n",
      "        Wrapper strategy to format the prompt as JSON with the help of LLM.\n",
      "        \"\"\"\n",
      "        response = await provider(prompt, **generation_args)\n",
      "        response = response.replace(\"```\", \"\").replace(\"json\", \"\").strip()\n",
      "        logger.info(f\"provider response: {response}\")\n",
      "        try:\n",
      "            return json.loads(response)\n",
      "        except json.JSONDecodeError as e:\n",
      "            logger.error(\n",
      "                f\"provider returned non-JSON. parsing error: {e} - response: {response}\"\n",
      "            )\n",
      "            raise StrategyError(f\"JSON parsing error: {e}\") from e\n",
      "\n",
      "\n",
      "class MDWrapper(Strategy):\n",
      "    async def __call__(\n",
      "        self, prompt: str, provider: Provider, **generation_args: Any\n",
      "    ) -> Dict[str, Any]:\n",
      "        \"\"\"\n",
      "        Wrapper strategy to format the prompt as Markdown with the help of LLM.\n",
      "        \"\"\"\n",
      "        response = await provider(prompt, **generation_args)\n",
      "        response = response.replace(\"```md\", \"\").replace(\"```\", \"\").strip()\n",
      "        logger.info(f\"provider response: {response}\")\n",
      "        try:\n",
      "            return json.loads(response)\n",
      "        except json.JSONDecodeError as e:\n",
      "            logger.error(\n",
      "                f\"provider returned non-JSON. parsing error: {e} - response: {response}\"\n",
      "            )\n",
      "            raise StrategyError(f\"JSON parsing error: {e}\") from e\n",
      "\n",
      "import logging\n",
      "import traceback\n",
      "\n",
      "from uuid import uuid4\n",
      "from sqlalchemy.ext.asyncio import AsyncSession\n",
      "from fastapi import APIRouter, File, UploadFile, HTTPException, Depends, Request, status\n",
      "\n",
      "from app.core import get_db_session\n",
      "from app.services import (\n",
      "    ResumeService,\n",
      "    ScoreImprovementService,\n",
      "    ResumeNotFoundError,\n",
      "    ResumeParsingError,\n",
      "    JobNotFoundError,\n",
      ")\n",
      "\n",
      "resume_router = APIRouter()\n",
      "logger = logging.getLogger(__name__)\n",
      "\n",
      "\n",
      "@resume_router.post(\n",
      "    \"/upload\",\n",
      "    summary=\"Upload a resume in PDF or DOCX format and store it into DB in HTML/Markdown format\",\n",
      ")\n",
      "async def upload_resume(\n",
      "    request: Request,\n",
      "    file: UploadFile = File(...),\n",
      "    db: AsyncSession = Depends(get_db_session),\n",
      "):\n",
      "    \"\"\"\n",
      "    Accepts a PDF or DOCX file, converts it to HTML/Markdown, and stores it in the database.\n",
      "\n",
      "    Raises:\n",
      "        HTTPException: If the file type is not supported or if the file is empty.\n",
      "    \"\"\"\n",
      "    request_id = getattr(request.state, \"request_id\", str(uuid4()))\n",
      "\n",
      "    allowed_content_types = [\n",
      "        \"application/pdf\",\n",
      "        \"application/vnd.openxmlformats-officedocument.wordprocessingml.document\",\n",
      "    ]\n",
      "\n",
      "    if file.content_type not in allowed_content_types:\n",
      "        raise HTTPException(\n",
      "            status_code=status.HTTP_400_BAD_REQUEST,\n",
      "            detail=\"Invalid file type. Only PDF and DOCX files are allowed.\",\n",
      "        )\n",
      "\n",
      "    file_bytes = await file.read()\n",
      "    if not file_bytes:\n",
      "        raise HTTPException(\n",
      "            status_code=status.HTTP_400_BAD_REQUEST,\n",
      "            detail=\"Empty file. Please upload a valid file.\",\n",
      "        )\n",
      "\n",
      "    try:\n",
      "        resume_service = ResumeService(db)\n",
      "        resume_id = await resume_service.convert_and_store_resume(\n",
      "            file_bytes=file_bytes,\n",
      "            file_type=file.content_type,\n",
      "            filename=file.filename,\n",
      "            content_type=\"md\",\n",
      "        )\n",
      "    except Exception as e:\n",
      "        logger.error(\n",
      "            f\"Error processing file: {str(e)} - traceback: {traceback.format_exc()}\"\n",
      "        )\n",
      "        raise HTTPException(\n",
      "            status_code=status.HTTP_500_INTERNAL_SERVER_ERROR,\n",
      "            detail=f\"Error processing file: {str(e)}\",\n",
      "        )\n",
      "\n",
      "    return {\n",
      "        \"message\": f\"File {file.filename} successfully processed as MD and stored in the DB\",\n",
      "        \"request_id\": request_id,\n",
      "        \"resume_id\": resume_id,\n",
      "    }\n",
      "\n",
      "\n",
      "@resume_router.post(\n",
      "    \"/improvements\",\n",
      "    summary=\"Score and improve a resume against a job description\",\n",
      ")\n",
      "async def score_and_improve(\n",
      "    request: Request,\n",
      "    resume_id: str,\n",
      "    job_id: str,\n",
      "    db: AsyncSession = Depends(get_db_session),\n",
      "):\n",
      "    \"\"\"\n",
      "    Scores and improves a resume against a job description.\n",
      "\n",
      "    Raises:\n",
      "        HTTPException: If the resume or job is not found.\n",
      "    \"\"\"\n",
      "    request_id = getattr(request.state, \"request_id\", str(uuid4()))\n",
      "\n",
      "    try:\n",
      "        score_improvement_service = ScoreImprovementService(db=db)\n",
      "        improvements = await score_improvement_service.run(\n",
      "            resume_id=resume_id, job_id=job_id\n",
      "        )\n",
      "    except ResumeNotFoundError as e:\n",
      "        logger.error(str(e))\n",
      "        raise HTTPException(\n",
      "            status_code=status.HTTP_500_INTERNAL_SERVER_ERROR,\n",
      "            detail=str(e),\n",
      "        )\n",
      "    except JobNotFoundError as e:\n",
      "        logger.error(str(e))\n",
      "        raise HTTPException(\n",
      "            status_code=status.HTTP_500_INTERNAL_SERVER_ERROR,\n",
      "            detail=str(e),\n",
      "        )\n",
      "    except ResumeParsingError as e:\n",
      "        logger.error(str(e))\n",
      "        raise HTTPException(\n",
      "            status_code=status.HTTP_500_INTERNAL_SERVER_ERROR,\n",
      "            detail=str(e),\n",
      "        )\n",
      "    except Exception as e:\n",
      "        logger.error(f\"Error: {str(e)} - traceback: {traceback.format_exc()}\")\n",
      "        raise HTTPException(\n",
      "            status_code=status.HTTP_500_INTERNAL_SERVER_ERROR,\n",
      "            detail=\"sorry, something went wrong!\",\n",
      "        )\n",
      "\n",
      "    return {\n",
      "        \"request_id\": request_id,\n",
      "        \"data\": improvements,\n",
      "    }\n",
      "\n",
      "PROMPT = \"\"\"\n",
      "You are an expert resume editor and talent acquisition specialist. Your task is to revise the following resume so that it aligns as closely as possible with the provided job description and extracted job keywords, in order to maximize the cosine similarity between the resume and the job keywords.\n",
      "\n",
      "Instructions:\n",
      "- Carefully review the job description and the list of extracted job keywords.\n",
      "- Update the candidate's resume by:\n",
      "  - Emphasizing and naturally incorporating relevant skills, experiences, and keywords from the job description and keyword list.\n",
      "  - Where appropriate, naturally weave the extracted job keywords into the resume content.\n",
      "  - Rewriting, adding, or removing resume content as needed to better match the job requirements.\n",
      "  - Maintaining a natural, professional tone and avoiding keyword stuffing.\n",
      "  - Where possible, use quantifiable achievements and action verbs.\n",
      "  - The current cosine similarity score is {current_cosine_similarity:.4f}. Revise the resume to further increase this score.\n",
      "- ONLY output the improved updated resume. Do not include any explanations, commentary, or formatting outside of the resume itself.\n",
      "\n",
      "Job Description:\n",
      "```md\n",
      "{raw_job_description}\n",
      "```\n",
      "\n",
      "Extracted Job Keywords:\n",
      "```md\n",
      "{extracted_job_keywords}\n",
      "```\n",
      "\n",
      "Original Resume:\n",
      "```md\n",
      "{raw_resume}\n",
      "```\n",
      "\n",
      "Extracted Resume Keywords:\n",
      "```md\n",
      "{extracted_resume_keywords}\n",
      "```\n",
      "\n",
      "NOTE: ONLY OUTPUT THE IMPROVED UPDATED RESUME IN MARKDOWN FORMAT.\n",
      "\"\"\"\n",
      "\n",
      "from .job_service import JobService\n",
      "from .resume_service import ResumeService\n",
      "from .scoring_improvement_service import ScoreImprovementService\n",
      "from .exceptions import ResumeNotFoundError, ResumeParsingError, JobNotFoundError\n",
      "\n",
      "__all__ = [\n",
      "    \"JobService\",\n",
      "    \"ResumeService\",\n",
      "    \"JobNotFoundError\",\n",
      "    \"ResumeParsingError\",\n",
      "    \"ResumeNotFoundError\",\n",
      "    \"ScoreImprovementService\",\n",
      "]\n",
      "\n",
      "File LOC changed from 133 to 169\n",
      "import json\n",
      "import logging\n",
      "import numpy as np\n",
      "\n",
      "from sqlalchemy.future import select\n",
      "from typing import Dict, Optional, Tuple\n",
      "from sqlalchemy.ext.asyncio import AsyncSession\n",
      "\n",
      "from app.prompt import prompt_factory\n",
      "from app.agent import EmbeddingManager, AgentManager\n",
      "from app.models import Resume, Job, ProcessedResume, ProcessedJob\n",
      "from .exceptions import (\n",
      "    ResumeNotFoundError,\n",
      "    JobNotFoundError,\n",
      "    ResumeParsingError,\n",
      ")\n",
      "\n",
      "logger = logging.getLogger(__name__)\n",
      "\n",
      "\n",
      "class ScoreImprovementService:\n",
      "    \"\"\"\n",
      "    Service to handle scoring of resumes and jobs using embeddings.\n",
      "    Fetches Resume and Job data from the database, computes embeddings,\n",
      "    and calculates cosine similarity scores. Uses LLM for iteratively improving\n",
      "    the scoring process.\n",
      "    \"\"\"\n",
      "\n",
      "    def __init__(self, db: AsyncSession, max_retries: int = 5):\n",
      "        self.db = db\n",
      "        self.max_retries = max_retries\n",
      "        self.agent_manager = AgentManager(strategy=\"md\")\n",
      "        self.embedding_manager = EmbeddingManager()\n",
      "\n",
      "    async def _get_resume(\n",
      "        self, resume_id: str\n",
      "    ) -> Tuple[Resume | None, ProcessedResume | None]:\n",
      "        \"\"\"\n",
      "        Fetches the resume from the database.\n",
      "        \"\"\"\n",
      "        query = select(Resume).where(Resume.resume_id == resume_id)\n",
      "        result = await self.db.execute(query)\n",
      "        resume = result.scalars().first()\n",
      "\n",
      "        if not resume:\n",
      "            raise ResumeNotFoundError(self.resume_id)\n",
      "\n",
      "        query = select(ProcessedResume).where(ProcessedResume.resume_id == resume_id)\n",
      "        result = await self.db.execute(query)\n",
      "        processed_resume = result.scalars().first()\n",
      "\n",
      "        if not processed_resume:\n",
      "            ResumeParsingError(self.resume_id)\n",
      "\n",
      "        return resume, processed_resume\n",
      "\n",
      "    async def _get_job(self, job_id: str) -> Tuple[Job | None, ProcessedJob | None]:\n",
      "        \"\"\"\n",
      "        Fetches the job from the database.\n",
      "        \"\"\"\n",
      "        query = select(Job).where(Job.job_id == job_id)\n",
      "        result = await self.db.execute(query)\n",
      "        job = result.scalars().first()\n",
      "\n",
      "        if not job:\n",
      "            raise JobNotFoundError(self.job_id)\n",
      "\n",
      "        query = select(ProcessedJob).where(ProcessedJob.job_id == job_id)\n",
      "        result = await self.db.execute(query)\n",
      "        processed_job = result.scalars().first()\n",
      "\n",
      "        if not processed_job:\n",
      "            ResumeParsingError(self.resume_id)\n",
      "\n",
      "        return job, processed_job\n",
      "\n",
      "    def calculate_cosine_similarity(\n",
      "        self,\n",
      "        extracted_job_keywords_embedding: np.ndarray,\n",
      "        resume_embedding: np.ndarray,\n",
      "    ) -> float:\n",
      "        \"\"\"\n",
      "        Calculates the cosine similarity between two embeddings.\n",
      "        \"\"\"\n",
      "        if resume_embedding is None or extracted_job_keywords_embedding is None:\n",
      "            return 0.0\n",
      "        return np.dot(extracted_job_keywords_embedding, resume_embedding) / (\n",
      "            np.linalg.norm(extracted_job_keywords_embedding)\n",
      "            * np.linalg.norm(resume_embedding)\n",
      "        )\n",
      "\n",
      "    async def improve_score_with_llm(\n",
      "        self,\n",
      "        resume: str,\n",
      "        extracted_resume_keywords: str,\n",
      "        job: str,\n",
      "        extracted_job_keywords: str,\n",
      "        previous_cosine_similarity_score: float,\n",
      "        attempt: Optional[int] = 1,\n",
      "    ) -> str:\n",
      "        \"\"\"\n",
      "        Uses LLM to improve the score based on resume and job description.\n",
      "        \"\"\"\n",
      "        prompt_template = prompt_factory.get(\"resume_improvement\")\n",
      "        init_prompt = prompt_template.format(\n",
      "            raw_job_description=job,\n",
      "            extracted_job_keywords=extracted_job_keywords,\n",
      "            raw_resume=resume,\n",
      "            extracted_resume_keywords=extracted_resume_keywords,\n",
      "            cosine_similarity_score=previous_cosine_similarity_score,\n",
      "        )\n",
      "        improved_resume = await self.agent_manager(init_prompt)\n",
      "\n",
      "        new_score = self.calculate_cosine_similarity(\n",
      "            improved_resume, extracted_job_keywords\n",
      "        )\n",
      "        if new_score > previous_cosine_similarity_score or attempt >= self.max_retries:\n",
      "            return improved_resume, new_score\n",
      "\n",
      "        return await self.improve_score_with_llm(\n",
      "            resume=improved_resume,\n",
      "            extracted_resume_keywords=extracted_resume_keywords,\n",
      "            job=job,\n",
      "            extracted_job_keywords=extracted_job_keywords,\n",
      "            previous_cosine_similarity_score=new_score,\n",
      "            attempt=attempt + 1,\n",
      "        )\n",
      "\n",
      "    async def run(self, resume_id: str, job_id: str) -> Dict:\n",
      "        \"\"\"\n",
      "        Main method to run the scoring process.\n",
      "        \"\"\"\n",
      "        resume, processed_resume = await self._get_resume(resume_id)\n",
      "        job, processed_job = await self._get_job(job_id)\n",
      "\n",
      "        extracted_job_keywords = \", \".join(\n",
      "            json.loads(processed_job.extracted_keywords).get(\"extracted_keywords\", [])\n",
      "        )\n",
      "\n",
      "        extracted_resume_keywords = \", \".join(\n",
      "            json.loads(processed_resume.extracted_keywords).get(\n",
      "                \"extracted_keywords\", []\n",
      "            )\n",
      "        )\n",
      "\n",
      "        resume_embedding = await self.embedding_manager(text=resume.content)\n",
      "        extracted_job_keywords_embedding = await self.embedding_manager(\n",
      "            text=extracted_job_keywords\n",
      "        )\n",
      "\n",
      "        cosine_similarity_score = self.calculate_cosine_similarity(\n",
      "            extracted_job_keywords_embedding, resume_embedding\n",
      "        )\n",
      "        updated_resume, updated_score = await self.improve_score_with_llm(\n",
      "            resume=resume.content,\n",
      "            extracted_resume_keywords=extracted_resume_keywords,\n",
      "            job=job.content,\n",
      "            extracted_job_keywords=extracted_job_keywords,\n",
      "            cosine_similarity_score=cosine_similarity_score,\n",
      "        )\n",
      "\n",
      "        return {\n",
      "            \"resume_id\": resume_id,\n",
      "            \"job_id\": job_id,\n",
      "            \"original_score\": cosine_similarity_score,\n",
      "            \"new_score\": updated_score,\n",
      "            \"updated_resume\": updated_resume,\n",
      "        }\n",
      "\n",
      "import os\n",
      "from typing import Dict, Any\n",
      "\n",
      "from .exceptions import ProviderError\n",
      "from .strategies.wrapper import JSONWrapper, MDWrapper\n",
      "from .providers.ollama import OllamaProvider, OllamaEmbeddingProvider\n",
      "from .providers.openai import OpenAIProvider, OpenAIEmbeddingProvider\n",
      "\n",
      "\n",
      "class AgentManager:\n",
      "    def __init__(self, strategy: str | None = None, model: str = \"gemma3:4b\") -> None:\n",
      "        match strategy:\n",
      "            case \"md\":\n",
      "                self.strategy = MDWrapper()\n",
      "            case \"json\":\n",
      "                self.strategy = JSONWrapper()\n",
      "            case _:\n",
      "                self.strategy = JSONWrapper()\n",
      "        self.model = model\n",
      "\n",
      "    async def _get_provider(self, **kwargs: Any) -> OllamaProvider | OpenAIProvider:\n",
      "        api_key = kwargs.get(\"openai_api_key\", os.getenv(\"OPENAI_API_KEY\"))\n",
      "        if api_key:\n",
      "            return OpenAIProvider(api_key=api_key)\n",
      "\n",
      "        model = kwargs.get(\"model\", self.model)\n",
      "        installed_ollama_models = await OllamaProvider.get_installed_models()\n",
      "        if model not in installed_ollama_models:\n",
      "            raise ProviderError(\n",
      "                f\"Ollama Model '{model}' is not found. Run `ollama pull {model} or pick from any available models {installed_ollama_models}\"\n",
      "            )\n",
      "        return OllamaProvider(model_name=model)\n",
      "\n",
      "    async def run(self, prompt: str, **kwargs: Any) -> Dict[str, Any]:\n",
      "        \"\"\"\n",
      "        Run the agent with the given prompt and generation arguments.\n",
      "        \"\"\"\n",
      "        provider = await self._get_provider(**kwargs)\n",
      "        return await self.strategy(prompt, provider, **kwargs)\n",
      "\n",
      "\n",
      "class EmbeddingManager:\n",
      "    def __init__(self, model: str = \"nomic-embed-text:137m-v1.5-fp16\") -> None:\n",
      "        self._model = model\n",
      "\n",
      "    async def _get_embedding_provider(\n",
      "        self, **kwargs: Any\n",
      "    ) -> OllamaEmbeddingProvider | OpenAIEmbeddingProvider:\n",
      "        api_key = kwargs.get(\"openai_api_key\", os.getenv(\"OPENAI_API_KEY\"))\n",
      "        if api_key:\n",
      "            return OpenAIEmbeddingProvider(api_key=api_key)\n",
      "        model = kwargs.get(\"embedding_model\", self._model)\n",
      "        installed_ollama_models = await OllamaProvider.get_installed_models()\n",
      "        if model not in installed_ollama_models:\n",
      "            raise ProviderError(\n",
      "                f\"Ollama Model '{model}' is not found. Run `ollama pull {model} or pick from any available models {installed_ollama_models}\"\n",
      "            )\n",
      "        return OllamaEmbeddingProvider(embedding_model=model)\n",
      "\n",
      "    async def embed(self, text: str, **kwargs: Any) -> list[float]:\n",
      "        \"\"\"\n",
      "        Get the embedding for the given text.\n",
      "        \"\"\"\n",
      "        provider = await self._get_embedding_provider(**kwargs)\n",
      "        return await provider.embed(text)\n",
      "\n",
      "import logging\n",
      "import ollama\n",
      "\n",
      "from typing import Any, Dict, List, Optional\n",
      "from fastapi.concurrency import run_in_threadpool\n",
      "\n",
      "from ..exceptions import ProviderError\n",
      "from .base import Provider, EmbeddingProvider\n",
      "\n",
      "logger = logging.getLogger(__name__)\n",
      "\n",
      "\n",
      "class OllamaProvider(Provider):\n",
      "    def __init__(self, model_name: str = \"gemma3:4b\", host: Optional[str] = None):\n",
      "        self.model = model_name\n",
      "        self._client = ollama.Client(host=host) if host else ollama.Client()\n",
      "\n",
      "    @staticmethod\n",
      "    async def get_installed_models(host: Optional[str] = None) -> List[str]:\n",
      "        \"\"\"\n",
      "        List all installed models.\n",
      "        \"\"\"\n",
      "\n",
      "        def _list_sync() -> List[str]:\n",
      "            client = ollama.Client(host=host) if host else ollama.Client()\n",
      "            return [model_class.model for model_class in client.list().models]\n",
      "\n",
      "        return await run_in_threadpool(_list_sync)\n",
      "\n",
      "    def _generate_sync(self, prompt: str, options: Dict[str, Any]) -> str:\n",
      "        \"\"\"\n",
      "        Generate a response from the model.\n",
      "        \"\"\"\n",
      "        try:\n",
      "            response = self._client.generate(\n",
      "                prompt=prompt,\n",
      "                model=self.model,\n",
      "                options=options,\n",
      "            )\n",
      "            return response[\"response\"].strip()\n",
      "        except Exception as e:\n",
      "            logger.error(f\"ollama sync error: {e}\")\n",
      "            raise ProviderError(f\"Ollama - Error generating response: {e}\")\n",
      "\n",
      "    async def __call__(self, prompt: str, **generation_args: Any) -> str:\n",
      "        opts = {\n",
      "            \"temperature\": generation_args.get(\"temperature\", 0),\n",
      "            \"top_p\": generation_args.get(\"top_p\", 0.9),\n",
      "            \"top_k\": generation_args.get(\"top_k\", 40),\n",
      "            \"num_ctx\": generation_args.get(\"max_length\", 20000),\n",
      "        }\n",
      "        return await run_in_threadpool(self._generate_sync, prompt, opts)\n",
      "\n",
      "\n",
      "class OllamaEmbeddingProvider(EmbeddingProvider):\n",
      "    def __init__(\n",
      "        self,\n",
      "        embedding_model: str = \"nomic-embed-text:137m-v1.5-fp16\",\n",
      "        host: Optional[str] = None,\n",
      "    ):\n",
      "        self._model = embedding_model\n",
      "        self._client = ollama.Client(host=host) if host else ollama.Client()\n",
      "\n",
      "    async def embed(self, text: str) -> List[float]:\n",
      "        \"\"\"\n",
      "        Generate an embedding for the given text.\n",
      "        \"\"\"\n",
      "        try:\n",
      "            response = await run_in_threadpool(\n",
      "                self._client.embed,\n",
      "                input=text,\n",
      "                model=self._model,\n",
      "            )\n",
      "            return response.embeddings\n",
      "        except Exception as e:\n",
      "            logger.error(f\"ollama embedding error: {e}\")\n",
      "            raise ProviderError(f\"Ollama - Error generating embedding: {e}\")\n",
      "\n",
      "import json\n",
      "import logging\n",
      "from typing import Any, Dict\n",
      "\n",
      "from .base import Strategy\n",
      "from ..providers.base import Provider\n",
      "from ..exceptions import StrategyError\n",
      "\n",
      "\n",
      "logger = logging.getLogger(__name__)\n",
      "\n",
      "\n",
      "class JSONWrapper(Strategy):\n",
      "    async def __call__(\n",
      "        self, prompt: str, provider: Provider, **generation_args: Any\n",
      "    ) -> Dict[str, Any]:\n",
      "        \"\"\"\n",
      "        Wrapper strategy to format the prompt as JSON with the help of LLM.\n",
      "        \"\"\"\n",
      "        response = await provider(prompt, **generation_args)\n",
      "        response = response.replace(\"```\", \"\").replace(\"json\", \"\").strip()\n",
      "        logger.info(f\"provider response: {response}\")\n",
      "        try:\n",
      "            return json.loads(response)\n",
      "        except json.JSONDecodeError as e:\n",
      "            logger.error(\n",
      "                f\"provider returned non-JSON. parsing error: {e} - response: {response}\"\n",
      "            )\n",
      "            raise StrategyError(f\"JSON parsing error: {e}\") from e\n",
      "\n",
      "\n",
      "class MDWrapper(Strategy):\n",
      "    async def __call__(\n",
      "        self, prompt: str, provider: Provider, **generation_args: Any\n",
      "    ) -> Dict[str, Any]:\n",
      "        \"\"\"\n",
      "        Wrapper strategy to format the prompt as Markdown with the help of LLM.\n",
      "        \"\"\"\n",
      "        logger.info(f\"prompt given to provider: \\n{prompt}\")\n",
      "        response = await provider(prompt, **generation_args)\n",
      "        logger.info(f\"provider response: {response}\")\n",
      "        try:\n",
      "            response = (\n",
      "                \"```md\\n\" + response + \"```\" if \"```md\" not in response else response\n",
      "            )\n",
      "            return response\n",
      "        except Exception as e:\n",
      "            logger.error(\n",
      "                f\"provider returned non-md. parsing error: {e} - response: {response}\"\n",
      "            )\n",
      "            raise StrategyError(f\"Markdown parsing error: {e}\") from e\n",
      "\n",
      "import logging\n",
      "import traceback\n",
      "\n",
      "from uuid import uuid4\n",
      "from sqlalchemy.ext.asyncio import AsyncSession\n",
      "from fastapi import APIRouter, File, UploadFile, HTTPException, Depends, Request, status\n",
      "\n",
      "from app.core import get_db_session\n",
      "from app.services import (\n",
      "    ResumeService,\n",
      "    ScoreImprovementService,\n",
      "    ResumeNotFoundError,\n",
      "    ResumeParsingError,\n",
      "    JobNotFoundError,\n",
      ")\n",
      "from app.schemas.pydantic import ResumeImprovementRequest\n",
      "\n",
      "resume_router = APIRouter()\n",
      "logger = logging.getLogger(__name__)\n",
      "\n",
      "\n",
      "@resume_router.post(\n",
      "    \"/upload\",\n",
      "    summary=\"Upload a resume in PDF or DOCX format and store it into DB in HTML/Markdown format\",\n",
      ")\n",
      "async def upload_resume(\n",
      "    request: Request,\n",
      "    file: UploadFile = File(...),\n",
      "    db: AsyncSession = Depends(get_db_session),\n",
      "):\n",
      "    \"\"\"\n",
      "    Accepts a PDF or DOCX file, converts it to HTML/Markdown, and stores it in the database.\n",
      "\n",
      "    Raises:\n",
      "        HTTPException: If the file type is not supported or if the file is empty.\n",
      "    \"\"\"\n",
      "    request_id = getattr(request.state, \"request_id\", str(uuid4()))\n",
      "\n",
      "    allowed_content_types = [\n",
      "        \"application/pdf\",\n",
      "        \"application/vnd.openxmlformats-officedocument.wordprocessingml.document\",\n",
      "    ]\n",
      "\n",
      "    if file.content_type not in allowed_content_types:\n",
      "        raise HTTPException(\n",
      "            status_code=status.HTTP_400_BAD_REQUEST,\n",
      "            detail=\"Invalid file type. Only PDF and DOCX files are allowed.\",\n",
      "        )\n",
      "\n",
      "    file_bytes = await file.read()\n",
      "    if not file_bytes:\n",
      "        raise HTTPException(\n",
      "            status_code=status.HTTP_400_BAD_REQUEST,\n",
      "            detail=\"Empty file. Please upload a valid file.\",\n",
      "        )\n",
      "\n",
      "    try:\n",
      "        resume_service = ResumeService(db)\n",
      "        resume_id = await resume_service.convert_and_store_resume(\n",
      "            file_bytes=file_bytes,\n",
      "            file_type=file.content_type,\n",
      "            filename=file.filename,\n",
      "            content_type=\"md\",\n",
      "        )\n",
      "    except Exception as e:\n",
      "        logger.error(\n",
      "            f\"Error processing file: {str(e)} - traceback: {traceback.format_exc()}\"\n",
      "        )\n",
      "        raise HTTPException(\n",
      "            status_code=status.HTTP_500_INTERNAL_SERVER_ERROR,\n",
      "            detail=f\"Error processing file: {str(e)}\",\n",
      "        )\n",
      "\n",
      "    return {\n",
      "        \"message\": f\"File {file.filename} successfully processed as MD and stored in the DB\",\n",
      "        \"request_id\": request_id,\n",
      "        \"resume_id\": resume_id,\n",
      "    }\n",
      "\n",
      "\n",
      "@resume_router.post(\n",
      "    \"/improvements\",\n",
      "    summary=\"Score and improve a resume against a job description\",\n",
      ")\n",
      "async def score_and_improve(\n",
      "    request: Request,\n",
      "    payload: ResumeImprovementRequest,\n",
      "    db: AsyncSession = Depends(get_db_session),\n",
      "):\n",
      "    \"\"\"\n",
      "    Scores and improves a resume against a job description.\n",
      "\n",
      "    Raises:\n",
      "        HTTPException: If the resume or job is not found.\n",
      "    \"\"\"\n",
      "    request_id = getattr(request.state, \"request_id\", str(uuid4()))\n",
      "    request_payload = payload.model_dump()\n",
      "\n",
      "    try:\n",
      "        resume_id = str(request_payload.get(\"resume_id\", \"\"))\n",
      "        if not resume_id:\n",
      "            raise ResumeNotFoundError(\n",
      "                message=\"invalid value passed in `resume_id` field, please try again with valid resume_id.\"\n",
      "            )\n",
      "        job_id = str(request_payload.get(\"job_id\", \"\"))\n",
      "        if not job_id:\n",
      "            raise JobNotFoundError(\n",
      "                message=\"invalid value passed in `job_id` field, please try again with valid job_id.\"\n",
      "            )\n",
      "        score_improvement_service = ScoreImprovementService(db=db)\n",
      "        improvements = await score_improvement_service.run(\n",
      "            resume_id=resume_id,\n",
      "            job_id=job_id,\n",
      "        )\n",
      "    except ResumeNotFoundError as e:\n",
      "        logger.error(str(e))\n",
      "        raise HTTPException(\n",
      "            status_code=status.HTTP_500_INTERNAL_SERVER_ERROR,\n",
      "            detail=str(e),\n",
      "        )\n",
      "    except JobNotFoundError as e:\n",
      "        logger.error(str(e))\n",
      "        raise HTTPException(\n",
      "            status_code=status.HTTP_500_INTERNAL_SERVER_ERROR,\n",
      "            detail=str(e),\n",
      "        )\n",
      "    except ResumeParsingError as e:\n",
      "        logger.error(str(e))\n",
      "        raise HTTPException(\n",
      "            status_code=status.HTTP_500_INTERNAL_SERVER_ERROR,\n",
      "            detail=str(e),\n",
      "        )\n",
      "    except Exception as e:\n",
      "        logger.error(f\"Error: {str(e)} - traceback: {traceback.format_exc()}\")\n",
      "        raise HTTPException(\n",
      "            status_code=status.HTTP_500_INTERNAL_SERVER_ERROR,\n",
      "            detail=\"sorry, something went wrong!\",\n",
      "        )\n",
      "\n",
      "    return {\n",
      "        \"request_id\": request_id,\n",
      "        \"data\": improvements,\n",
      "    }\n",
      "\n",
      "import logging\n",
      "\n",
      "from sqlalchemy.exc import SQLAlchemyError\n",
      "from fastapi import Request, HTTPException\n",
      "from fastapi.responses import JSONResponse\n",
      "from fastapi.exceptions import RequestValidationError\n",
      "from starlette.status import HTTP_500_INTERNAL_SERVER_ERROR\n",
      "\n",
      "logger = logging.getLogger(__name__)\n",
      "\n",
      "\n",
      "async def custom_http_exception_handler(request: Request, exc: HTTPException):\n",
      "    request_id = getattr(request.state, \"request_id\", \"\")\n",
      "    return JSONResponse(\n",
      "        status_code=exc.status_code,\n",
      "        content={\"detail\": exc.detail, \"request_id\": request_id},\n",
      "    )\n",
      "\n",
      "\n",
      "async def validation_exception_handler(request: Request, exc: RequestValidationError):\n",
      "    request_id = getattr(request.state, \"request_id\", \"\")\n",
      "    return JSONResponse(\n",
      "        status_code=422,\n",
      "        content={\"detail\": exc.errors(), \"request_id\": request_id},\n",
      "    )\n",
      "\n",
      "\n",
      "async def unhandled_exception_handler(request: Request, exc: Exception):\n",
      "    request_id = getattr(request.state, \"request_id\", \"\")\n",
      "    return JSONResponse(\n",
      "        status_code=HTTP_500_INTERNAL_SERVER_ERROR,\n",
      "        content={\"detail\": \"Internal Server Error\", \"request_id\": request_id},\n",
      "    )\n",
      "\n",
      "\n",
      "async def sqlalchemy_exception_handler(request: Request, exc: SQLAlchemyError):\n",
      "    logger.error(f\"DB error on {request.url}: {exc} - {exc.with_traceback()}\")\n",
      "    return JSONResponse(\n",
      "        status_code=500,\n",
      "        content={\n",
      "            \"request_id\": getattr(request.state, \"request_id\", None),\n",
      "            \"detail\": \"A database error occurred. Please try again later.\",\n",
      "        },\n",
      "    )\n",
      "\n",
      "from .job import JobUploadRequest\n",
      "from .structured_job import StructuredJobModel\n",
      "from .structured_resume import StructuredResumeModel\n",
      "from .resume_improvement import ResumeImprovementRequest\n",
      "\n",
      "__all__ = [\n",
      "    \"JobUploadRequest\",\n",
      "    \"StructuredResumeModel\",\n",
      "    \"StructuredJobModel\",\n",
      "    \"ResumeImprovementRequest\",\n",
      "]\n",
      "\n",
      "from uuid import UUID\n",
      "from pydantic import BaseModel, Field\n",
      "\n",
      "\n",
      "class ResumeImprovementRequest(BaseModel):\n",
      "    job_id: UUID = Field(..., description=\"DB UUID reference to the job\")\n",
      "    resume_id: UUID = Field(..., description=\"DB UUID reference to the resume\")\n",
      "\n",
      "from typing import Optional, List\n",
      "from typing_extensions import Literal\n",
      "from pydantic import BaseModel, Field, EmailStr, field_validator\n",
      "\n",
      "\n",
      "class CompanyProfile(BaseModel):\n",
      "    company_name: str = Field(..., alias=\"companyName\")\n",
      "    industry: Optional[str] = None\n",
      "    website: Optional[str] = None\n",
      "    description: Optional[str] = None\n",
      "\n",
      "\n",
      "class Location(BaseModel):\n",
      "    city: str\n",
      "    state: Optional[str] = None\n",
      "    country: Optional[str] = None\n",
      "    remote_status: Literal[\"Fully Remote\", \"Hybrid\", \"On-site\", \"Remote\"] = Field(\n",
      "        ..., alias=\"remoteStatus\"\n",
      "    )\n",
      "\n",
      "    @field_validator(\"remote_status\", mode=\"before\")\n",
      "    def validate_remote_status(cls, value):\n",
      "        if isinstance(value, str):\n",
      "            v_lower = value.lower()\n",
      "            mapping = {\n",
      "                \"fully remote\": \"Fully Remote\",\n",
      "                \"hybrid\": \"Hybrid\",\n",
      "                \"on-site\": \"On-site\",\n",
      "                \"remote\": \"Remote\",\n",
      "            }\n",
      "            if v_lower in mapping:\n",
      "                return mapping[v_lower]\n",
      "        raise ValueError(\n",
      "            \"remote_status must be one of: Fully Remote, Hybrid, On-site, Remote (case insensitive)\"\n",
      "        )\n",
      "\n",
      "\n",
      "class Qualifications(BaseModel):\n",
      "    required: List[str]\n",
      "    preferred: Optional[List[str]] = None\n",
      "\n",
      "\n",
      "class CompensationAndBenefits(BaseModel):\n",
      "    salary_range: Optional[str] = Field(..., alias=\"salaryRange\")\n",
      "    benefits: Optional[List[str]] = None\n",
      "\n",
      "\n",
      "class ApplicationInfo(BaseModel):\n",
      "    how_to_apply: Optional[str] = Field(..., alias=\"howToApply\")\n",
      "    apply_link: Optional[str] = Field(..., alias=\"applyLink\")\n",
      "    contact_email: Optional[EmailStr] = Field(..., alias=\"contactEmail\")\n",
      "\n",
      "\n",
      "class StructuredJobModel(BaseModel):\n",
      "    job_title: str = Field(..., alias=\"jobTitle\")\n",
      "    company_profile: CompanyProfile = Field(..., alias=\"companyProfile\")\n",
      "    location: Location\n",
      "    date_posted: str = Field(..., alias=\"datePosted\")\n",
      "    employment_type: Literal[\n",
      "        \"Full-time\", \"Part-time\", \"Contract\", \"Internship\", \"Temporary\"\n",
      "    ] = Field(..., alias=\"employmentType\")\n",
      "    job_summary: str = Field(..., alias=\"jobSummary\")\n",
      "    key_responsibilities: List[str] = Field(..., alias=\"keyResponsibilities\")\n",
      "    qualifications: Qualifications\n",
      "    compensation_and_benefits: Optional[CompensationAndBenefits] = Field(\n",
      "        None, alias=\"compensationAndBenefits\"\n",
      "    )\n",
      "    application_info: Optional[ApplicationInfo] = Field(None, alias=\"applicationInfo\")\n",
      "    extracted_keywords: List[str] = Field(..., alias=\"extractedKeywords\")\n",
      "\n",
      "    class ConfigDict:\n",
      "        validate_by_name = True\n",
      "        str_strip_whitespace = True\n",
      "\n",
      "from .job_service import JobService\n",
      "from .resume_service import ResumeService\n",
      "from .scoring_improvement_service import ScoreImprovementService\n",
      "from .exceptions import (\n",
      "    ResumeNotFoundError,\n",
      "    ResumeParsingError,\n",
      "    JobNotFoundError,\n",
      "    JobParsingError,\n",
      ")\n",
      "\n",
      "__all__ = [\n",
      "    \"JobService\",\n",
      "    \"ResumeService\",\n",
      "    \"JobParsingError\",\n",
      "    \"JobNotFoundError\",\n",
      "    \"ResumeParsingError\",\n",
      "    \"ResumeNotFoundError\",\n",
      "    \"ScoreImprovementService\",\n",
      "]\n",
      "\n",
      "from typing import Optional\n",
      "\n",
      "\n",
      "class ResumeNotFoundError(Exception):\n",
      "    \"\"\"\n",
      "    Exception raised when a resume is not found in the database.\n",
      "    \"\"\"\n",
      "\n",
      "    def __init__(self, resume_id: Optional[str] = None, message: Optional[str] = None):\n",
      "        if resume_id and not message:\n",
      "            message = f\"Resume with ID {resume_id} not found.\"\n",
      "        elif not message:\n",
      "            message = \"Resume not found.\"\n",
      "        super().__init__(message)\n",
      "        self.resume_id = resume_id\n",
      "\n",
      "\n",
      "class JobNotFoundError(Exception):\n",
      "    \"\"\"\n",
      "    Exception raised when a job is not found in the database.\n",
      "    \"\"\"\n",
      "\n",
      "    def __init__(self, job_id: Optional[str] = None, message: Optional[str] = None):\n",
      "        if job_id and not message:\n",
      "            message = f\"Job with ID {job_id} not found.\"\n",
      "        elif not message:\n",
      "            message = \"Job not found.\"\n",
      "        super().__init__(message)\n",
      "        self.job_id = job_id\n",
      "\n",
      "\n",
      "class EmbeddingError(Exception):\n",
      "    \"\"\"\n",
      "    Exception raised when there is an error in embedding.\n",
      "    \"\"\"\n",
      "\n",
      "    def __init__(self, message: str = None):\n",
      "        super().__init__(message)\n",
      "        self.message = message\n",
      "\n",
      "\n",
      "class ResumeParsingError(Exception):\n",
      "    \"\"\"\n",
      "    Exception raised when a resume processing and storing in the database failed.\n",
      "    \"\"\"\n",
      "\n",
      "    def __init__(self, resume_id: Optional[str] = None, message: Optional[str] = None):\n",
      "        if resume_id and not message:\n",
      "            message = f\"Parsing of resume with ID {resume_id} failed.\"\n",
      "        elif not message:\n",
      "            message = \"Parsed resume not found.\"\n",
      "        super().__init__(message)\n",
      "        self.resume_id = resume_id\n",
      "\n",
      "\n",
      "class JobParsingError(Exception):\n",
      "    \"\"\"\n",
      "    Exception raised when a resume processing and storing in the database failed.\n",
      "    \"\"\"\n",
      "\n",
      "    def __init__(self, job_id: Optional[str] = None, message: Optional[str] = None):\n",
      "        if job_id and not message:\n",
      "            message = f\"Parsing of job with ID {job_id} failed.\"\n",
      "        elif not message:\n",
      "            message = \"Parsed job not found.\"\n",
      "        super().__init__(message)\n",
      "        self.resume_id = job_id\n",
      "\n",
      "import uuid\n",
      "import json\n",
      "import logging\n",
      "\n",
      "from typing import List\n",
      "from pydantic import ValidationError\n",
      "from sqlalchemy import select\n",
      "from sqlalchemy.ext.asyncio import AsyncSession\n",
      "\n",
      "from app.agent import AgentManager\n",
      "from app.prompt import prompt_factory\n",
      "from app.schemas.json import json_schema_factory\n",
      "from app.models import Job, Resume, ProcessedJob\n",
      "from app.schemas.pydantic import StructuredJobModel\n",
      "\n",
      "logger = logging.getLogger(__name__)\n",
      "\n",
      "\n",
      "class JobService:\n",
      "    def __init__(self, db: AsyncSession):\n",
      "        self.db = db\n",
      "        self.json_agent_manager = AgentManager(model=\"gemma3:4b\")\n",
      "\n",
      "    async def create_and_store_job(self, job_data: dict) -> List[str]:\n",
      "        \"\"\"\n",
      "        Stores job data in the database and returns a list of job IDs.\n",
      "        \"\"\"\n",
      "        resume_id = str(job_data.get(\"resume_id\"))\n",
      "\n",
      "        if not await self._is_resume_available(resume_id):\n",
      "            raise AssertionError(\n",
      "                f\"resume corresponding to resume_id: {resume_id} not found\"\n",
      "            )\n",
      "\n",
      "        job_ids = []\n",
      "        for job_description in job_data.get(\"job_descriptions\", []):\n",
      "            job_id = str(uuid.uuid4())\n",
      "            job = Job(\n",
      "                job_id=job_id,\n",
      "                resume_id=str(resume_id),\n",
      "                content=job_description,\n",
      "            )\n",
      "            self.db.add(job)\n",
      "\n",
      "            await self._extract_and_store_structured_resume(\n",
      "                job_id=job_id, job_description_text=job_description\n",
      "            )\n",
      "            logger.info(f\"Job ID: {job_id}\")\n",
      "            job_ids.append(job_id)\n",
      "\n",
      "        self.db.commit()\n",
      "        return job_ids\n",
      "\n",
      "    async def _is_resume_available(self, resume_id: str) -> bool:\n",
      "        \"\"\"\n",
      "        Checks if a resume exists in the database.\n",
      "        \"\"\"\n",
      "        query = select(Resume).where(Resume.resume_id == resume_id)\n",
      "        result = await self.db.scalar(query)\n",
      "        return result is not None\n",
      "\n",
      "    async def _extract_and_store_structured_resume(\n",
      "        self, job_id, job_description_text: str\n",
      "    ):\n",
      "        \"\"\"\n",
      "        extract and store structured job data in the database\n",
      "        \"\"\"\n",
      "        structured_job = await self._extract_structured_json(job_description_text)\n",
      "        if not structured_job:\n",
      "            logger.info(\"Structured job extraction failed.\")\n",
      "            return None\n",
      "\n",
      "        processed_job = ProcessedJob(\n",
      "            job_id=job_id,\n",
      "            job_title=structured_job.get(\"job_title\"),\n",
      "            company_profile=json.dumps(structured_job.get(\"company_profile\"))\n",
      "            if structured_job.get(\"company_profile\")\n",
      "            else None,\n",
      "            location=json.dumps(structured_job.get(\"location\"))\n",
      "            if structured_job.get(\"location\")\n",
      "            else None,\n",
      "            date_posted=structured_job.get(\"date_posted\"),\n",
      "            employment_type=structured_job.get(\"employment_type\"),\n",
      "            job_summary=structured_job.get(\"job_summary\"),\n",
      "            key_responsibilities=json.dumps(\n",
      "                {\"key_responsibilities\": structured_job.get(\"key_responsibilities\", [])}\n",
      "            )\n",
      "            if structured_job.get(\"key_responsibilities\")\n",
      "            else None,\n",
      "            qualifications=json.dumps(structured_job.get(\"qualifications\", []))\n",
      "            if structured_job.get(\"qualifications\")\n",
      "            else None,\n",
      "            compensation_and_benfits=json.dumps(\n",
      "                structured_job.get(\"compensation_and_benfits\", [])\n",
      "            )\n",
      "            if structured_job.get(\"compensation_and_benfits\")\n",
      "            else None,\n",
      "            application_info=json.dumps(structured_job.get(\"application_info\", []))\n",
      "            if structured_job.get(\"application_info\")\n",
      "            else None,\n",
      "            extracted_keywords=json.dumps(\n",
      "                {\"extracted_keywords\": structured_job.get(\"extracted_keywords\", [])}\n",
      "            )\n",
      "            if structured_job.get(\"extracted_keywords\")\n",
      "            else None,\n",
      "        )\n",
      "\n",
      "        self.db.add(processed_job)\n",
      "        await self.db.flush()\n",
      "        await self.db.commit()\n",
      "\n",
      "        return job_id\n",
      "\n",
      "    async def _extract_structured_json(\n",
      "        self, job_description_text: str\n",
      "    ) -> StructuredJobModel | None:\n",
      "        \"\"\"\n",
      "        Uses the AgentManager+JSONWrapper to ask the LLM to\n",
      "        return the data in exact JSON schema we need.\n",
      "        \"\"\"\n",
      "        prompt_template = prompt_factory.get(\"structured_job\")\n",
      "        prompt = prompt_template.format(\n",
      "            json.dumps(json_schema_factory.get(\"structured_job\"), indent=2),\n",
      "            job_description_text,\n",
      "        )\n",
      "        logger.info(f\"Structured Job Prompt: {prompt}\")\n",
      "        raw_output = await self.json_agent_manager.run(prompt=prompt)\n",
      "\n",
      "        try:\n",
      "            structured_job: StructuredJobModel = StructuredJobModel.model_validate(\n",
      "                raw_output\n",
      "            )\n",
      "        except ValidationError as e:\n",
      "            logger.info(f\"Validation error: {e}\")\n",
      "            return None\n",
      "        return structured_job.model_dump()\n",
      "\n",
      "import json\n",
      "import logging\n",
      "import markdown\n",
      "import numpy as np\n",
      "\n",
      "from sqlalchemy.future import select\n",
      "from typing import Dict, Optional, Tuple\n",
      "from sqlalchemy.ext.asyncio import AsyncSession\n",
      "\n",
      "from app.prompt import prompt_factory\n",
      "from app.agent import EmbeddingManager, AgentManager\n",
      "from app.models import Resume, Job, ProcessedResume, ProcessedJob\n",
      "from .exceptions import (\n",
      "    ResumeNotFoundError,\n",
      "    JobNotFoundError,\n",
      "    ResumeParsingError,\n",
      "    JobParsingError,\n",
      ")\n",
      "\n",
      "logger = logging.getLogger(__name__)\n",
      "\n",
      "\n",
      "class ScoreImprovementService:\n",
      "    \"\"\"\n",
      "    Service to handle scoring of resumes and jobs using embeddings.\n",
      "    Fetches Resume and Job data from the database, computes embeddings,\n",
      "    and calculates cosine similarity scores. Uses LLM for iteratively improving\n",
      "    the scoring process.\n",
      "    \"\"\"\n",
      "\n",
      "    def __init__(self, db: AsyncSession, max_retries: int = 5):\n",
      "        self.db = db\n",
      "        self.max_retries = max_retries\n",
      "        self.agent_manager = AgentManager(strategy=\"md\")\n",
      "        self.embedding_manager = EmbeddingManager()\n",
      "\n",
      "    async def _get_resume(\n",
      "        self, resume_id: str\n",
      "    ) -> Tuple[Resume | None, ProcessedResume | None]:\n",
      "        \"\"\"\n",
      "        Fetches the resume from the database.\n",
      "        \"\"\"\n",
      "        query = select(Resume).where(Resume.resume_id == resume_id)\n",
      "        result = await self.db.execute(query)\n",
      "        resume = result.scalars().first()\n",
      "\n",
      "        if not resume:\n",
      "            raise ResumeNotFoundError(resume_id=resume_id)\n",
      "\n",
      "        query = select(ProcessedResume).where(ProcessedResume.resume_id == resume_id)\n",
      "        result = await self.db.execute(query)\n",
      "        processed_resume = result.scalars().first()\n",
      "\n",
      "        if not processed_resume:\n",
      "            ResumeParsingError(resume_id=resume_id)\n",
      "\n",
      "        return resume, processed_resume\n",
      "\n",
      "    async def _get_job(self, job_id: str) -> Tuple[Job | None, ProcessedJob | None]:\n",
      "        \"\"\"\n",
      "        Fetches the job from the database.\n",
      "        \"\"\"\n",
      "        query = select(Job).where(Job.job_id == job_id)\n",
      "        result = await self.db.execute(query)\n",
      "        job = result.scalars().first()\n",
      "\n",
      "        if not job:\n",
      "            raise JobNotFoundError(job_id=job_id)\n",
      "\n",
      "        query = select(ProcessedJob).where(ProcessedJob.job_id == job_id)\n",
      "        result = await self.db.execute(query)\n",
      "        processed_job = result.scalars().first()\n",
      "\n",
      "        if not processed_job:\n",
      "            JobParsingError(job_id=job_id)\n",
      "\n",
      "        return job, processed_job\n",
      "\n",
      "    def calculate_cosine_similarity(\n",
      "        self,\n",
      "        extracted_job_keywords_embedding: np.ndarray,\n",
      "        resume_embedding: np.ndarray,\n",
      "    ) -> float:\n",
      "        \"\"\"\n",
      "        Calculates the cosine similarity between two embeddings.\n",
      "        \"\"\"\n",
      "        if resume_embedding is None or extracted_job_keywords_embedding is None:\n",
      "            return 0.0\n",
      "\n",
      "        ejk = np.asarray(extracted_job_keywords_embedding).squeeze()\n",
      "        re = np.asarray(resume_embedding).squeeze()\n",
      "\n",
      "        return float(np.dot(ejk, re) / (np.linalg.norm(ejk) * np.linalg.norm(re)))\n",
      "\n",
      "    async def improve_score_with_llm(\n",
      "        self,\n",
      "        resume: str,\n",
      "        extracted_resume_keywords: str,\n",
      "        job: str,\n",
      "        extracted_job_keywords: str,\n",
      "        previous_cosine_similarity_score: float,\n",
      "        extracted_job_keywords_embedding: float,\n",
      "        attempt: Optional[int] = 1,\n",
      "    ) -> str:\n",
      "        \"\"\"\n",
      "        Uses LLM to improve the score based on resume and job description.\n",
      "        \"\"\"\n",
      "        prompt_template = prompt_factory.get(\"resume_improvement\")\n",
      "        init_prompt = prompt_template.format(\n",
      "            raw_job_description=job,\n",
      "            extracted_job_keywords=extracted_job_keywords,\n",
      "            raw_resume=resume,\n",
      "            extracted_resume_keywords=extracted_resume_keywords,\n",
      "            current_cosine_similarity=previous_cosine_similarity_score,\n",
      "        )\n",
      "        improved_resume = await self.agent_manager.run(init_prompt)\n",
      "\n",
      "        improved_resume_embedding = await self.embedding_manager.embed(\n",
      "            text=improved_resume\n",
      "        )\n",
      "\n",
      "        new_score = self.calculate_cosine_similarity(\n",
      "            improved_resume_embedding, extracted_job_keywords_embedding\n",
      "        )\n",
      "        if new_score > previous_cosine_similarity_score or attempt >= self.max_retries:\n",
      "            return improved_resume, new_score\n",
      "\n",
      "        return await self.improve_score_with_llm(\n",
      "            resume=improved_resume,\n",
      "            extracted_resume_keywords=extracted_resume_keywords,\n",
      "            job=job,\n",
      "            extracted_job_keywords=extracted_job_keywords,\n",
      "            previous_cosine_similarity_score=new_score,\n",
      "            attempt=attempt + 1,\n",
      "        )\n",
      "\n",
      "    async def run(self, resume_id: str, job_id: str) -> Dict:\n",
      "        \"\"\"\n",
      "        Main method to run the scoring process.\n",
      "        \"\"\"\n",
      "        resume, processed_resume = await self._get_resume(resume_id)\n",
      "        job, processed_job = await self._get_job(job_id)\n",
      "\n",
      "        extracted_job_keywords = \", \".join(\n",
      "            json.loads(processed_job.extracted_keywords).get(\"extracted_keywords\", [])\n",
      "        )\n",
      "\n",
      "        extracted_resume_keywords = \", \".join(\n",
      "            json.loads(processed_resume.extracted_keywords).get(\n",
      "                \"extracted_keywords\", []\n",
      "            )\n",
      "        )\n",
      "\n",
      "        resume_embedding = await self.embedding_manager.embed(text=resume.content)\n",
      "        extracted_job_keywords_embedding = await self.embedding_manager.embed(\n",
      "            text=extracted_job_keywords\n",
      "        )\n",
      "\n",
      "        cosine_similarity_score = self.calculate_cosine_similarity(\n",
      "            extracted_job_keywords_embedding, resume_embedding\n",
      "        )\n",
      "        updated_resume, updated_score = await self.improve_score_with_llm(\n",
      "            resume=resume.content,\n",
      "            extracted_resume_keywords=extracted_resume_keywords,\n",
      "            job=job.content,\n",
      "            extracted_job_keywords=extracted_job_keywords,\n",
      "            previous_cosine_similarity_score=cosine_similarity_score,\n",
      "            extracted_job_keywords_embedding=extracted_job_keywords_embedding,\n",
      "        )\n",
      "\n",
      "        return {\n",
      "            \"resume_id\": resume_id,\n",
      "            \"job_id\": job_id,\n",
      "            \"original_score\": cosine_similarity_score,\n",
      "            \"new_score\": updated_score,\n",
      "            \"updated_resume\": markdown.markdown(text=updated_resume),\n",
      "        }\n",
      "\n",
      "from .job_service import JobService\n",
      "from .resume_service import ResumeService\n",
      "from .score_improvement_service import ScoreImprovementService\n",
      "from .exceptions import (\n",
      "    ResumeNotFoundError,\n",
      "    ResumeParsingError,\n",
      "    JobNotFoundError,\n",
      "    JobParsingError,\n",
      ")\n",
      "\n",
      "__all__ = [\n",
      "    \"JobService\",\n",
      "    \"ResumeService\",\n",
      "    \"JobParsingError\",\n",
      "    \"JobNotFoundError\",\n",
      "    \"ResumeParsingError\",\n",
      "    \"ResumeNotFoundError\",\n",
      "    \"ScoreImprovementService\",\n",
      "]\n",
      "\n",
      "File app/services/score_improvement_service.py has no source code\n",
      "\n",
      "import uuid\n",
      "import json\n",
      "import logging\n",
      "\n",
      "from typing import List\n",
      "from pydantic import ValidationError\n",
      "from sqlalchemy import select\n",
      "from sqlalchemy.ext.asyncio import AsyncSession\n",
      "\n",
      "from app.agent import AgentManager\n",
      "from app.prompt import prompt_factory\n",
      "from app.schemas.json import json_schema_factory\n",
      "from app.models import Job, Resume, ProcessedJob\n",
      "from app.schemas.pydantic import StructuredJobModel\n",
      "\n",
      "logger = logging.getLogger(__name__)\n",
      "\n",
      "\n",
      "class JobService:\n",
      "    def __init__(self, db: AsyncSession):\n",
      "        self.db = db\n",
      "        self.json_agent_manager = AgentManager(model=\"gemma3:4b\")\n",
      "\n",
      "    async def create_and_store_job(self, job_data: dict) -> List[str]:\n",
      "        \"\"\"\n",
      "        Stores job data in the database and returns a list of job IDs.\n",
      "        \"\"\"\n",
      "        resume_id = str(job_data.get(\"resume_id\"))\n",
      "\n",
      "        if not await self._is_resume_available(resume_id):\n",
      "            raise AssertionError(\n",
      "                f\"resume corresponding to resume_id: {resume_id} not found\"\n",
      "            )\n",
      "\n",
      "        job_ids = []\n",
      "        for job_description in job_data.get(\"job_descriptions\", []):\n",
      "            job_id = str(uuid.uuid4())\n",
      "            job = Job(\n",
      "                job_id=job_id,\n",
      "                resume_id=str(resume_id),\n",
      "                content=job_description,\n",
      "            )\n",
      "            self.db.add(job)\n",
      "\n",
      "            await self._extract_and_store_structured_job(\n",
      "                job_id=job_id, job_description_text=job_description\n",
      "            )\n",
      "            logger.info(f\"Job ID: {job_id}\")\n",
      "            job_ids.append(job_id)\n",
      "\n",
      "        await self.db.commit()\n",
      "        return job_ids\n",
      "\n",
      "    async def _is_resume_available(self, resume_id: str) -> bool:\n",
      "        \"\"\"\n",
      "        Checks if a resume exists in the database.\n",
      "        \"\"\"\n",
      "        query = select(Resume).where(Resume.resume_id == resume_id)\n",
      "        result = await self.db.scalar(query)\n",
      "        return result is not None\n",
      "\n",
      "    async def _extract_and_store_structured_job(\n",
      "        self, job_id, job_description_text: str\n",
      "    ):\n",
      "        \"\"\"\n",
      "        extract and store structured job data in the database\n",
      "        \"\"\"\n",
      "        structured_job = await self._extract_structured_json(job_description_text)\n",
      "        if not structured_job:\n",
      "            logger.info(\"Structured job extraction failed.\")\n",
      "            return None\n",
      "\n",
      "        processed_job = ProcessedJob(\n",
      "            job_id=job_id,\n",
      "            job_title=structured_job.get(\"job_title\"),\n",
      "            company_profile=json.dumps(structured_job.get(\"company_profile\"))\n",
      "            if structured_job.get(\"company_profile\")\n",
      "            else None,\n",
      "            location=json.dumps(structured_job.get(\"location\"))\n",
      "            if structured_job.get(\"location\")\n",
      "            else None,\n",
      "            date_posted=structured_job.get(\"date_posted\"),\n",
      "            employment_type=structured_job.get(\"employment_type\"),\n",
      "            job_summary=structured_job.get(\"job_summary\"),\n",
      "            key_responsibilities=json.dumps(\n",
      "                {\"key_responsibilities\": structured_job.get(\"key_responsibilities\", [])}\n",
      "            )\n",
      "            if structured_job.get(\"key_responsibilities\")\n",
      "            else None,\n",
      "            qualifications=json.dumps(structured_job.get(\"qualifications\", []))\n",
      "            if structured_job.get(\"qualifications\")\n",
      "            else None,\n",
      "            compensation_and_benfits=json.dumps(\n",
      "                structured_job.get(\"compensation_and_benfits\", [])\n",
      "            )\n",
      "            if structured_job.get(\"compensation_and_benfits\")\n",
      "            else None,\n",
      "            application_info=json.dumps(structured_job.get(\"application_info\", []))\n",
      "            if structured_job.get(\"application_info\")\n",
      "            else None,\n",
      "            extracted_keywords=json.dumps(\n",
      "                {\"extracted_keywords\": structured_job.get(\"extracted_keywords\", [])}\n",
      "            )\n",
      "            if structured_job.get(\"extracted_keywords\")\n",
      "            else None,\n",
      "        )\n",
      "\n",
      "        self.db.add(processed_job)\n",
      "        await self.db.flush()\n",
      "        await self.db.commit()\n",
      "\n",
      "        return job_id\n",
      "\n",
      "    async def _extract_structured_json(\n",
      "        self, job_description_text: str\n",
      "    ) -> StructuredJobModel | None:\n",
      "        \"\"\"\n",
      "        Uses the AgentManager+JSONWrapper to ask the LLM to\n",
      "        return the data in exact JSON schema we need.\n",
      "        \"\"\"\n",
      "        prompt_template = prompt_factory.get(\"structured_job\")\n",
      "        prompt = prompt_template.format(\n",
      "            json.dumps(json_schema_factory.get(\"structured_job\"), indent=2),\n",
      "            job_description_text,\n",
      "        )\n",
      "        logger.info(f\"Structured Job Prompt: {prompt}\")\n",
      "        raw_output = await self.json_agent_manager.run(prompt=prompt)\n",
      "\n",
      "        try:\n",
      "            structured_job: StructuredJobModel = StructuredJobModel.model_validate(\n",
      "                raw_output\n",
      "            )\n",
      "        except ValidationError as e:\n",
      "            logger.info(f\"Validation error: {e}\")\n",
      "            return None\n",
      "        return structured_job.model_dump()\n",
      "\n",
      "import enum\n",
      "\n",
      "from typing import Optional, List\n",
      "from pydantic import BaseModel, Field, EmailStr\n",
      "\n",
      "\n",
      "class EmploymentTypeEnum(str, enum.Enum):\n",
      "    \"\"\"Case-insensitive Enum for employment types.\"\"\"\n",
      "\n",
      "    FULL_TIME = \"Full-time\"\n",
      "    PART_TIME = \"Part-time\"\n",
      "    CONTRACT = \"Contract\"\n",
      "    INTERNSHIP = \"Internship\"\n",
      "    TEMPORARY = \"Temporary\"\n",
      "    NOT_SPECIFIED = \"Not Specified\"\n",
      "\n",
      "    @classmethod\n",
      "    def _missing_(cls, value: object):\n",
      "        \"\"\"Handles case-insensitive lookup.\"\"\"\n",
      "        if isinstance(value, str):\n",
      "            value_lower = value.lower()\n",
      "            mapping = {member.value.lower(): member for member in cls}\n",
      "            if value_lower in mapping:\n",
      "                return mapping[value_lower]\n",
      "\n",
      "        raise ValueError(\n",
      "            \"employment type must be one of: Full-time, Part-time, Contract, Internship, Temporary, Not Specified (case insensitive)\"\n",
      "        )\n",
      "\n",
      "\n",
      "class RemoteStatusEnum(str, enum.Enum):\n",
      "    \"\"\"Case-insensitive Enum for remote work status.\"\"\"\n",
      "\n",
      "    FULLY_REMOTE = \"Fully Remote\"\n",
      "    HYBRID = \"Hybrid\"\n",
      "    ON_SITE = \"On-site\"\n",
      "    REMOTE = \"Remote\"\n",
      "    NOT_SPECIFIED = \"Not Specified\"\n",
      "\n",
      "    @classmethod\n",
      "    def _missing_(cls, value: object):\n",
      "        \"\"\"Handles case-insensitive lookup.\"\"\"\n",
      "        if isinstance(value, str):\n",
      "            value_lower = value.lower()\n",
      "            mapping = {member.value.lower(): member for member in cls}\n",
      "            if value_lower in mapping:\n",
      "                return mapping[value_lower]\n",
      "\n",
      "        raise ValueError(\n",
      "            \"remote_status must be one of: Fully Remote, Hybrid, On-site, Remote, Not Specified (case insensitive)\"\n",
      "        )\n",
      "\n",
      "\n",
      "class CompanyProfile(BaseModel):\n",
      "    company_name: str = Field(..., alias=\"companyName\")\n",
      "    industry: Optional[str] = None\n",
      "    website: Optional[str] = None\n",
      "    description: Optional[str] = None\n",
      "\n",
      "\n",
      "class Location(BaseModel):\n",
      "    city: str\n",
      "    state: Optional[str] = None\n",
      "    country: Optional[str] = None\n",
      "    remote_status: RemoteStatusEnum = Field(..., alias=\"remoteStatus\")\n",
      "\n",
      "\n",
      "class Qualifications(BaseModel):\n",
      "    required: List[str]\n",
      "    preferred: Optional[List[str]] = None\n",
      "\n",
      "\n",
      "class CompensationAndBenefits(BaseModel):\n",
      "    salary_range: Optional[str] = Field(..., alias=\"salaryRange\")\n",
      "    benefits: Optional[List[str]] = None\n",
      "\n",
      "\n",
      "class ApplicationInfo(BaseModel):\n",
      "    how_to_apply: Optional[str] = Field(..., alias=\"howToApply\")\n",
      "    apply_link: Optional[str] = Field(..., alias=\"applyLink\")\n",
      "    contact_email: Optional[EmailStr] = Field(..., alias=\"contactEmail\")\n",
      "\n",
      "\n",
      "class StructuredJobModel(BaseModel):\n",
      "    job_title: str = Field(..., alias=\"jobTitle\")\n",
      "    company_profile: CompanyProfile = Field(..., alias=\"companyProfile\")\n",
      "    location: Location\n",
      "    date_posted: str = Field(..., alias=\"datePosted\")\n",
      "    employment_type: EmploymentTypeEnum = Field(..., alias=\"employmentType\")\n",
      "    job_summary: str = Field(..., alias=\"jobSummary\")\n",
      "    key_responsibilities: List[str] = Field(..., alias=\"keyResponsibilities\")\n",
      "    qualifications: Qualifications\n",
      "    compensation_and_benefits: Optional[CompensationAndBenefits] = Field(\n",
      "        None, alias=\"compensationAndBenefits\"\n",
      "    )\n",
      "    application_info: Optional[ApplicationInfo] = Field(None, alias=\"applicationInfo\")\n",
      "    extracted_keywords: List[str] = Field(..., alias=\"extractedKeywords\")\n",
      "\n",
      "    class ConfigDict:\n",
      "        validate_by_name = True\n",
      "        str_strip_whitespace = True\n",
      "\n",
      "import uuid\n",
      "import json\n",
      "import logging\n",
      "\n",
      "from typing import List, Dict, Any\n",
      "from pydantic import ValidationError\n",
      "from sqlalchemy import select\n",
      "from sqlalchemy.ext.asyncio import AsyncSession\n",
      "\n",
      "from app.agent import AgentManager\n",
      "from app.prompt import prompt_factory\n",
      "from app.schemas.json import json_schema_factory\n",
      "from app.models import Job, Resume, ProcessedJob\n",
      "from app.schemas.pydantic import StructuredJobModel\n",
      "\n",
      "logger = logging.getLogger(__name__)\n",
      "\n",
      "\n",
      "class JobService:\n",
      "    def __init__(self, db: AsyncSession):\n",
      "        self.db = db\n",
      "        self.json_agent_manager = AgentManager(model=\"gemma3:4b\")\n",
      "\n",
      "    async def create_and_store_job(self, job_data: dict) -> List[str]:\n",
      "        \"\"\"\n",
      "        Stores job data in the database and returns a list of job IDs.\n",
      "        \"\"\"\n",
      "        resume_id = str(job_data.get(\"resume_id\"))\n",
      "\n",
      "        if not await self._is_resume_available(resume_id):\n",
      "            raise AssertionError(\n",
      "                f\"resume corresponding to resume_id: {resume_id} not found\"\n",
      "            )\n",
      "\n",
      "        job_ids = []\n",
      "        for job_description in job_data.get(\"job_descriptions\", []):\n",
      "            job_id = str(uuid.uuid4())\n",
      "            job = Job(\n",
      "                job_id=job_id,\n",
      "                resume_id=str(resume_id),\n",
      "                content=job_description,\n",
      "            )\n",
      "            self.db.add(job)\n",
      "\n",
      "            await self._extract_and_store_structured_job(\n",
      "                job_id=job_id, job_description_text=job_description\n",
      "            )\n",
      "            logger.info(f\"Job ID: {job_id}\")\n",
      "            job_ids.append(job_id)\n",
      "\n",
      "        await self.db.commit()\n",
      "        return job_ids\n",
      "\n",
      "    async def _is_resume_available(self, resume_id: str) -> bool:\n",
      "        \"\"\"\n",
      "        Checks if a resume exists in the database.\n",
      "        \"\"\"\n",
      "        query = select(Resume).where(Resume.resume_id == resume_id)\n",
      "        result = await self.db.scalar(query)\n",
      "        return result is not None\n",
      "\n",
      "    async def _extract_and_store_structured_job(\n",
      "        self, job_id, job_description_text: str\n",
      "    ):\n",
      "        \"\"\"\n",
      "        extract and store structured job data in the database\n",
      "        \"\"\"\n",
      "        structured_job = await self._extract_structured_json(job_description_text)\n",
      "        if not structured_job:\n",
      "            logger.info(\"Structured job extraction failed.\")\n",
      "            return None\n",
      "\n",
      "        processed_job = ProcessedJob(\n",
      "            job_id=job_id,\n",
      "            job_title=structured_job.get(\"job_title\"),\n",
      "            company_profile=json.dumps(structured_job.get(\"company_profile\"))\n",
      "            if structured_job.get(\"company_profile\")\n",
      "            else None,\n",
      "            location=json.dumps(structured_job.get(\"location\"))\n",
      "            if structured_job.get(\"location\")\n",
      "            else None,\n",
      "            date_posted=structured_job.get(\"date_posted\"),\n",
      "            employment_type=structured_job.get(\"employment_type\"),\n",
      "            job_summary=structured_job.get(\"job_summary\"),\n",
      "            key_responsibilities=json.dumps(\n",
      "                {\"key_responsibilities\": structured_job.get(\"key_responsibilities\", [])}\n",
      "            )\n",
      "            if structured_job.get(\"key_responsibilities\")\n",
      "            else None,\n",
      "            qualifications=json.dumps(structured_job.get(\"qualifications\", []))\n",
      "            if structured_job.get(\"qualifications\")\n",
      "            else None,\n",
      "            compensation_and_benfits=json.dumps(\n",
      "                structured_job.get(\"compensation_and_benfits\", [])\n",
      "            )\n",
      "            if structured_job.get(\"compensation_and_benfits\")\n",
      "            else None,\n",
      "            application_info=json.dumps(structured_job.get(\"application_info\", []))\n",
      "            if structured_job.get(\"application_info\")\n",
      "            else None,\n",
      "            extracted_keywords=json.dumps(\n",
      "                {\"extracted_keywords\": structured_job.get(\"extracted_keywords\", [])}\n",
      "            )\n",
      "            if structured_job.get(\"extracted_keywords\")\n",
      "            else None,\n",
      "        )\n",
      "\n",
      "        self.db.add(processed_job)\n",
      "        await self.db.flush()\n",
      "        await self.db.commit()\n",
      "\n",
      "        return job_id\n",
      "\n",
      "    async def _extract_structured_json(\n",
      "        self, job_description_text: str\n",
      "    ) -> Dict[str, Any] | None:\n",
      "        \"\"\"\n",
      "        Uses the AgentManager+JSONWrapper to ask the LLM to\n",
      "        return the data in exact JSON schema we need.\n",
      "        \"\"\"\n",
      "        prompt_template = prompt_factory.get(\"structured_job\")\n",
      "        prompt = prompt_template.format(\n",
      "            json.dumps(json_schema_factory.get(\"structured_job\"), indent=2),\n",
      "            job_description_text,\n",
      "        )\n",
      "        logger.info(f\"Structured Job Prompt: {prompt}\")\n",
      "        raw_output = await self.json_agent_manager.run(prompt=prompt)\n",
      "\n",
      "        try:\n",
      "            structured_job: StructuredJobModel = StructuredJobModel.model_validate(\n",
      "                raw_output\n",
      "            )\n",
      "        except ValidationError as e:\n",
      "            logger.info(f\"Validation error: {e}\")\n",
      "            return None\n",
      "        return structured_job.model_dump(mode=\"json\")\n",
      "\n",
      "import enum\n",
      "\n",
      "from typing import Optional, List\n",
      "from pydantic import BaseModel, Field\n",
      "\n",
      "\n",
      "class EmploymentTypeEnum(str, enum.Enum):\n",
      "    \"\"\"Case-insensitive Enum for employment types.\"\"\"\n",
      "\n",
      "    FULL_TIME = \"Full-time\"\n",
      "    PART_TIME = \"Part-time\"\n",
      "    CONTRACT = \"Contract\"\n",
      "    INTERNSHIP = \"Internship\"\n",
      "    TEMPORARY = \"Temporary\"\n",
      "    NOT_SPECIFIED = \"Not Specified\"\n",
      "\n",
      "    @classmethod\n",
      "    def _missing_(cls, value: object):\n",
      "        \"\"\"Handles case-insensitive lookup.\"\"\"\n",
      "        if isinstance(value, str):\n",
      "            value_lower = value.lower()\n",
      "            mapping = {member.value.lower(): member for member in cls}\n",
      "            if value_lower in mapping:\n",
      "                return mapping[value_lower]\n",
      "\n",
      "        raise ValueError(\n",
      "            \"employment type must be one of: Full-time, Part-time, Contract, Internship, Temporary, Not Specified (case insensitive)\"\n",
      "        )\n",
      "\n",
      "\n",
      "class RemoteStatusEnum(str, enum.Enum):\n",
      "    \"\"\"Case-insensitive Enum for remote work status.\"\"\"\n",
      "\n",
      "    FULLY_REMOTE = \"Fully Remote\"\n",
      "    HYBRID = \"Hybrid\"\n",
      "    ON_SITE = \"On-site\"\n",
      "    REMOTE = \"Remote\"\n",
      "    NOT_SPECIFIED = \"Not Specified\"\n",
      "\n",
      "    @classmethod\n",
      "    def _missing_(cls, value: object):\n",
      "        \"\"\"Handles case-insensitive lookup.\"\"\"\n",
      "        if isinstance(value, str):\n",
      "            value_lower = value.lower()\n",
      "            mapping = {member.value.lower(): member for member in cls}\n",
      "            if value_lower in mapping:\n",
      "                return mapping[value_lower]\n",
      "\n",
      "        raise ValueError(\n",
      "            \"remote_status must be one of: Fully Remote, Hybrid, On-site, Remote, Not Specified (case insensitive)\"\n",
      "        )\n",
      "\n",
      "\n",
      "class CompanyProfile(BaseModel):\n",
      "    company_name: str = Field(..., alias=\"companyName\")\n",
      "    industry: Optional[str] = None\n",
      "    website: Optional[str] = None\n",
      "    description: Optional[str] = None\n",
      "\n",
      "\n",
      "class Location(BaseModel):\n",
      "    city: str\n",
      "    state: Optional[str] = None\n",
      "    country: Optional[str] = None\n",
      "    remote_status: RemoteStatusEnum = Field(..., alias=\"remoteStatus\")\n",
      "\n",
      "\n",
      "class Qualifications(BaseModel):\n",
      "    required: List[str]\n",
      "    preferred: Optional[List[str]] = None\n",
      "\n",
      "\n",
      "class CompensationAndBenefits(BaseModel):\n",
      "    salary_range: Optional[str] = Field(..., alias=\"salaryRange\")\n",
      "    benefits: Optional[List[str]] = None\n",
      "\n",
      "\n",
      "class ApplicationInfo(BaseModel):\n",
      "    how_to_apply: Optional[str] = Field(..., alias=\"howToApply\")\n",
      "    apply_link: Optional[str] = Field(..., alias=\"applyLink\")\n",
      "    contact_email: Optional[str] = Field(..., alias=\"contactEmail\")\n",
      "\n",
      "\n",
      "class StructuredJobModel(BaseModel):\n",
      "    job_title: str = Field(..., alias=\"jobTitle\")\n",
      "    company_profile: CompanyProfile = Field(..., alias=\"companyProfile\")\n",
      "    location: Location\n",
      "    date_posted: str = Field(..., alias=\"datePosted\")\n",
      "    employment_type: EmploymentTypeEnum = Field(..., alias=\"employmentType\")\n",
      "    job_summary: str = Field(..., alias=\"jobSummary\")\n",
      "    key_responsibilities: List[str] = Field(..., alias=\"keyResponsibilities\")\n",
      "    qualifications: Qualifications\n",
      "    compensation_and_benefits: Optional[CompensationAndBenefits] = Field(\n",
      "        None, alias=\"compensationAndBenefits\"\n",
      "    )\n",
      "    application_info: Optional[ApplicationInfo] = Field(None, alias=\"applicationInfo\")\n",
      "    extracted_keywords: List[str] = Field(..., alias=\"extractedKeywords\")\n",
      "\n",
      "    class ConfigDict:\n",
      "        validate_by_name = True\n",
      "        str_strip_whitespace = True\n",
      "\n",
      "import logging\n",
      "import traceback\n",
      "\n",
      "from uuid import uuid4\n",
      "from sqlalchemy.ext.asyncio import AsyncSession\n",
      "from fastapi.responses import JSONResponse, StreamingResponse\n",
      "from fastapi import (\n",
      "    APIRouter,\n",
      "    File,\n",
      "    UploadFile,\n",
      "    HTTPException,\n",
      "    Depends,\n",
      "    Request,\n",
      "    status,\n",
      "    Query,\n",
      ")\n",
      "\n",
      "from app.core import get_db_session\n",
      "from app.services import (\n",
      "    ResumeService,\n",
      "    ScoreImprovementService,\n",
      "    ResumeNotFoundError,\n",
      "    ResumeParsingError,\n",
      "    JobNotFoundError,\n",
      ")\n",
      "from app.schemas.pydantic import ResumeImprovementRequest\n",
      "\n",
      "resume_router = APIRouter()\n",
      "logger = logging.getLogger(__name__)\n",
      "\n",
      "\n",
      "@resume_router.post(\n",
      "    \"/upload\",\n",
      "    summary=\"Upload a resume in PDF or DOCX format and store it into DB in HTML/Markdown format\",\n",
      ")\n",
      "async def upload_resume(\n",
      "    request: Request,\n",
      "    file: UploadFile = File(...),\n",
      "    db: AsyncSession = Depends(get_db_session),\n",
      "):\n",
      "    \"\"\"\n",
      "    Accepts a PDF or DOCX file, converts it to HTML/Markdown, and stores it in the database.\n",
      "\n",
      "    Raises:\n",
      "        HTTPException: If the file type is not supported or if the file is empty.\n",
      "    \"\"\"\n",
      "    request_id = getattr(request.state, \"request_id\", str(uuid4()))\n",
      "\n",
      "    allowed_content_types = [\n",
      "        \"application/pdf\",\n",
      "        \"application/vnd.openxmlformats-officedocument.wordprocessingml.document\",\n",
      "    ]\n",
      "\n",
      "    if file.content_type not in allowed_content_types:\n",
      "        raise HTTPException(\n",
      "            status_code=status.HTTP_400_BAD_REQUEST,\n",
      "            detail=\"Invalid file type. Only PDF and DOCX files are allowed.\",\n",
      "        )\n",
      "\n",
      "    file_bytes = await file.read()\n",
      "    if not file_bytes:\n",
      "        raise HTTPException(\n",
      "            status_code=status.HTTP_400_BAD_REQUEST,\n",
      "            detail=\"Empty file. Please upload a valid file.\",\n",
      "        )\n",
      "\n",
      "    try:\n",
      "        resume_service = ResumeService(db)\n",
      "        resume_id = await resume_service.convert_and_store_resume(\n",
      "            file_bytes=file_bytes,\n",
      "            file_type=file.content_type,\n",
      "            filename=file.filename,\n",
      "            content_type=\"md\",\n",
      "        )\n",
      "    except Exception as e:\n",
      "        logger.error(\n",
      "            f\"Error processing file: {str(e)} - traceback: {traceback.format_exc()}\"\n",
      "        )\n",
      "        raise HTTPException(\n",
      "            status_code=status.HTTP_500_INTERNAL_SERVER_ERROR,\n",
      "            detail=f\"Error processing file: {str(e)}\",\n",
      "        )\n",
      "\n",
      "    return {\n",
      "        \"message\": f\"File {file.filename} successfully processed as MD and stored in the DB\",\n",
      "        \"request_id\": request_id,\n",
      "        \"resume_id\": resume_id,\n",
      "    }\n",
      "\n",
      "\n",
      "@resume_router.post(\n",
      "    \"/improve\",\n",
      "    summary=\"Score and improve a resume against a job description\",\n",
      ")\n",
      "async def score_and_improve(\n",
      "    request: Request,\n",
      "    payload: ResumeImprovementRequest,\n",
      "    db: AsyncSession = Depends(get_db_session),\n",
      "    stream: bool = Query(\n",
      "        False, description=\"Enable streaming response using Server-Sent Events\"\n",
      "    ),\n",
      "):\n",
      "    \"\"\"\n",
      "    Scores and improves a resume against a job description.\n",
      "\n",
      "    Raises:\n",
      "        HTTPException: If the resume or job is not found.\n",
      "    \"\"\"\n",
      "    request_id = getattr(request.state, \"request_id\", str(uuid4()))\n",
      "    headers = {\"X-Request-ID\": request_id}\n",
      "\n",
      "    request_payload = payload.model_dump()\n",
      "\n",
      "    try:\n",
      "        resume_id = str(request_payload.get(\"resume_id\", \"\"))\n",
      "        if not resume_id:\n",
      "            raise ResumeNotFoundError(\n",
      "                message=\"invalid value passed in `resume_id` field, please try again with valid resume_id.\"\n",
      "            )\n",
      "        job_id = str(request_payload.get(\"job_id\", \"\"))\n",
      "        if not job_id:\n",
      "            raise JobNotFoundError(\n",
      "                message=\"invalid value passed in `job_id` field, please try again with valid job_id.\"\n",
      "            )\n",
      "        score_improvement_service = ScoreImprovementService(db=db)\n",
      "\n",
      "        if stream:\n",
      "            return StreamingResponse(\n",
      "                content=score_improvement_service.run_and_stream(\n",
      "                    resume_id=resume_id,\n",
      "                    job_id=job_id,\n",
      "                ),\n",
      "                media_type=\"text/event-stream\",\n",
      "                headers=headers,\n",
      "            )\n",
      "        else:\n",
      "            improvements = await score_improvement_service.run(\n",
      "                resume_id=resume_id,\n",
      "                job_id=job_id,\n",
      "            )\n",
      "            return JSONResponse(\n",
      "                content={\n",
      "                    \"request_id\": request_id,\n",
      "                    \"data\": improvements,\n",
      "                },\n",
      "                headers=headers,\n",
      "            )\n",
      "    except ResumeNotFoundError as e:\n",
      "        logger.error(str(e))\n",
      "        raise HTTPException(\n",
      "            status_code=status.HTTP_500_INTERNAL_SERVER_ERROR,\n",
      "            detail=str(e),\n",
      "        )\n",
      "    except JobNotFoundError as e:\n",
      "        logger.error(str(e))\n",
      "        raise HTTPException(\n",
      "            status_code=status.HTTP_500_INTERNAL_SERVER_ERROR,\n",
      "            detail=str(e),\n",
      "        )\n",
      "    except ResumeParsingError as e:\n",
      "        logger.error(str(e))\n",
      "        raise HTTPException(\n",
      "            status_code=status.HTTP_500_INTERNAL_SERVER_ERROR,\n",
      "            detail=str(e),\n",
      "        )\n",
      "    except Exception as e:\n",
      "        logger.error(f\"Error: {str(e)} - traceback: {traceback.format_exc()}\")\n",
      "        raise HTTPException(\n",
      "            status_code=status.HTTP_500_INTERNAL_SERVER_ERROR,\n",
      "            detail=\"sorry, something went wrong!\",\n",
      "        )\n",
      "\n",
      "import json\n",
      "import asyncio\n",
      "import logging\n",
      "import markdown\n",
      "import numpy as np\n",
      "\n",
      "from sqlalchemy.future import select\n",
      "from sqlalchemy.ext.asyncio import AsyncSession\n",
      "from typing import Dict, Optional, Tuple, AsyncGenerator\n",
      "\n",
      "from app.prompt import prompt_factory\n",
      "from app.agent import EmbeddingManager, AgentManager\n",
      "from app.models import Resume, Job, ProcessedResume, ProcessedJob\n",
      "from .exceptions import (\n",
      "    ResumeNotFoundError,\n",
      "    JobNotFoundError,\n",
      "    ResumeParsingError,\n",
      "    JobParsingError,\n",
      ")\n",
      "\n",
      "logger = logging.getLogger(__name__)\n",
      "\n",
      "\n",
      "class ScoreImprovementService:\n",
      "    \"\"\"\n",
      "    Service to handle scoring of resumes and jobs using embeddings.\n",
      "    Fetches Resume and Job data from the database, computes embeddings,\n",
      "    and calculates cosine similarity scores. Uses LLM for iteratively improving\n",
      "    the scoring process.\n",
      "    \"\"\"\n",
      "\n",
      "    def __init__(self, db: AsyncSession, max_retries: int = 5):\n",
      "        self.db = db\n",
      "        self.max_retries = max_retries\n",
      "        self.agent_manager = AgentManager(strategy=\"md\")\n",
      "        self.embedding_manager = EmbeddingManager()\n",
      "\n",
      "    async def _get_resume(\n",
      "        self, resume_id: str\n",
      "    ) -> Tuple[Resume | None, ProcessedResume | None]:\n",
      "        \"\"\"\n",
      "        Fetches the resume from the database.\n",
      "        \"\"\"\n",
      "        query = select(Resume).where(Resume.resume_id == resume_id)\n",
      "        result = await self.db.execute(query)\n",
      "        resume = result.scalars().first()\n",
      "\n",
      "        if not resume:\n",
      "            raise ResumeNotFoundError(resume_id=resume_id)\n",
      "\n",
      "        query = select(ProcessedResume).where(ProcessedResume.resume_id == resume_id)\n",
      "        result = await self.db.execute(query)\n",
      "        processed_resume = result.scalars().first()\n",
      "\n",
      "        if not processed_resume:\n",
      "            ResumeParsingError(resume_id=resume_id)\n",
      "\n",
      "        return resume, processed_resume\n",
      "\n",
      "    async def _get_job(self, job_id: str) -> Tuple[Job | None, ProcessedJob | None]:\n",
      "        \"\"\"\n",
      "        Fetches the job from the database.\n",
      "        \"\"\"\n",
      "        query = select(Job).where(Job.job_id == job_id)\n",
      "        result = await self.db.execute(query)\n",
      "        job = result.scalars().first()\n",
      "\n",
      "        if not job:\n",
      "            raise JobNotFoundError(job_id=job_id)\n",
      "\n",
      "        query = select(ProcessedJob).where(ProcessedJob.job_id == job_id)\n",
      "        result = await self.db.execute(query)\n",
      "        processed_job = result.scalars().first()\n",
      "\n",
      "        if not processed_job:\n",
      "            JobParsingError(job_id=job_id)\n",
      "\n",
      "        return job, processed_job\n",
      "\n",
      "    def calculate_cosine_similarity(\n",
      "        self,\n",
      "        extracted_job_keywords_embedding: np.ndarray,\n",
      "        resume_embedding: np.ndarray,\n",
      "    ) -> float:\n",
      "        \"\"\"\n",
      "        Calculates the cosine similarity between two embeddings.\n",
      "        \"\"\"\n",
      "        if resume_embedding is None or extracted_job_keywords_embedding is None:\n",
      "            return 0.0\n",
      "\n",
      "        ejk = np.asarray(extracted_job_keywords_embedding).squeeze()\n",
      "        re = np.asarray(resume_embedding).squeeze()\n",
      "\n",
      "        return float(np.dot(ejk, re) / (np.linalg.norm(ejk) * np.linalg.norm(re)))\n",
      "\n",
      "    async def improve_score_with_llm(\n",
      "        self,\n",
      "        resume: str,\n",
      "        extracted_resume_keywords: str,\n",
      "        job: str,\n",
      "        extracted_job_keywords: str,\n",
      "        previous_cosine_similarity_score: float,\n",
      "        extracted_job_keywords_embedding: float,\n",
      "        attempt: Optional[int] = 1,\n",
      "    ) -> Tuple[str, float]:\n",
      "        \"\"\"\n",
      "        Uses LLM to improve the score based on resume and job description.\n",
      "        \"\"\"\n",
      "        prompt_template = prompt_factory.get(\"resume_improvement\")\n",
      "        init_prompt = prompt_template.format(\n",
      "            raw_job_description=job,\n",
      "            extracted_job_keywords=extracted_job_keywords,\n",
      "            raw_resume=resume,\n",
      "            extracted_resume_keywords=extracted_resume_keywords,\n",
      "            current_cosine_similarity=previous_cosine_similarity_score,\n",
      "        )\n",
      "        improved_resume = await self.agent_manager.run(init_prompt)\n",
      "\n",
      "        improved_resume_embedding = await self.embedding_manager.embed(\n",
      "            text=improved_resume\n",
      "        )\n",
      "\n",
      "        new_score = self.calculate_cosine_similarity(\n",
      "            improved_resume_embedding, extracted_job_keywords_embedding\n",
      "        )\n",
      "        if new_score > previous_cosine_similarity_score or attempt >= self.max_retries:\n",
      "            return improved_resume, new_score\n",
      "\n",
      "        return await self.improve_score_with_llm(\n",
      "            resume=improved_resume,\n",
      "            extracted_resume_keywords=extracted_resume_keywords,\n",
      "            job=job,\n",
      "            extracted_job_keywords=extracted_job_keywords,\n",
      "            previous_cosine_similarity_score=new_score,\n",
      "            attempt=attempt + 1,\n",
      "        )\n",
      "\n",
      "    async def run(self, resume_id: str, job_id: str) -> Dict:\n",
      "        \"\"\"\n",
      "        Main method to run the scoring and improving process and return dict.\n",
      "        \"\"\"\n",
      "\n",
      "        resume, processed_resume = await self._get_resume(resume_id)\n",
      "        job, processed_job = await self._get_job(job_id)\n",
      "\n",
      "        extracted_job_keywords = \", \".join(\n",
      "            json.loads(processed_job.extracted_keywords).get(\"extracted_keywords\", [])\n",
      "        )\n",
      "\n",
      "        extracted_resume_keywords = \", \".join(\n",
      "            json.loads(processed_resume.extracted_keywords).get(\n",
      "                \"extracted_keywords\", []\n",
      "            )\n",
      "        )\n",
      "\n",
      "        resume_embedding = await self.embedding_manager.embed(text=resume.content)\n",
      "        extracted_job_keywords_embedding = await self.embedding_manager.embed(\n",
      "            text=extracted_job_keywords\n",
      "        )\n",
      "\n",
      "        cosine_similarity_score = self.calculate_cosine_similarity(\n",
      "            extracted_job_keywords_embedding, resume_embedding\n",
      "        )\n",
      "        updated_resume, updated_score = await self.improve_score_with_llm(\n",
      "            resume=resume.content,\n",
      "            extracted_resume_keywords=extracted_resume_keywords,\n",
      "            job=job.content,\n",
      "            extracted_job_keywords=extracted_job_keywords,\n",
      "            previous_cosine_similarity_score=cosine_similarity_score,\n",
      "            extracted_job_keywords_embedding=extracted_job_keywords_embedding,\n",
      "        )\n",
      "\n",
      "        return {\n",
      "            \"resume_id\": resume_id,\n",
      "            \"job_id\": job_id,\n",
      "            \"original_score\": cosine_similarity_score,\n",
      "            \"new_score\": updated_score,\n",
      "            \"updated_resume\": markdown.markdown(text=updated_resume),\n",
      "        }\n",
      "\n",
      "    async def run_and_stream(self, resume_id: str, job_id: str) -> AsyncGenerator:\n",
      "        \"\"\"\n",
      "        Main method to run the scoring and improving process and return dict.\n",
      "        \"\"\"\n",
      "\n",
      "        yield f\"data: {json.dumps({'status': 'starting', 'message': 'Analyzing resume and job description...'})}\\n\\n\"\n",
      "        await asyncio.sleep(2)\n",
      "\n",
      "        resume, processed_resume = await self._get_resume(resume_id)\n",
      "        job, processed_job = await self._get_job(job_id)\n",
      "\n",
      "        yield f\"data: {json.dumps({'status': 'parsing', 'message': 'Parsing resume content...'})}\\n\\n\"\n",
      "        await asyncio.sleep(2)\n",
      "\n",
      "        extracted_job_keywords = \", \".join(\n",
      "            json.loads(processed_job.extracted_keywords).get(\"extracted_keywords\", [])\n",
      "        )\n",
      "\n",
      "        extracted_resume_keywords = \", \".join(\n",
      "            json.loads(processed_resume.extracted_keywords).get(\n",
      "                \"extracted_keywords\", []\n",
      "            )\n",
      "        )\n",
      "\n",
      "        resume_embedding = await self.embedding_manager.embed(text=resume.content)\n",
      "        extracted_job_keywords_embedding = await self.embedding_manager.embed(\n",
      "            text=extracted_job_keywords\n",
      "        )\n",
      "\n",
      "        yield f\"data: {json.dumps({'status': 'scoring', 'message': 'Calculating compatibility score...'})}\\n\\n\"\n",
      "        await asyncio.sleep(3)\n",
      "\n",
      "        cosine_similarity_score = self.calculate_cosine_similarity(\n",
      "            extracted_job_keywords_embedding, resume_embedding\n",
      "        )\n",
      "\n",
      "        yield f\"data: {json.dumps({'status': 'scored', 'score': cosine_similarity_score})}\\n\\n\"\n",
      "\n",
      "        yield f\"data: {json.dumps({'status': 'improving', 'message': 'Generating improvement suggestions...'})}\\n\\n\"\n",
      "        await asyncio.sleep(3)\n",
      "\n",
      "        updated_resume, updated_score = await self.improve_score_with_llm(\n",
      "            resume=resume.content,\n",
      "            extracted_resume_keywords=extracted_resume_keywords,\n",
      "            job=job.content,\n",
      "            extracted_job_keywords=extracted_job_keywords,\n",
      "            previous_cosine_similarity_score=cosine_similarity_score,\n",
      "            extracted_job_keywords_embedding=extracted_job_keywords_embedding,\n",
      "        )\n",
      "\n",
      "        for i, suggestion in enumerate(updated_resume):\n",
      "            yield f\"data: {json.dumps({'status': 'suggestion', 'index': i, 'text': suggestion})}\\n\\n\"\n",
      "            await asyncio.sleep(0.2)\n",
      "\n",
      "        final_result = {\n",
      "            \"resume_id\": resume_id,\n",
      "            \"job_id\": job_id,\n",
      "            \"original_score\": cosine_similarity_score,\n",
      "            \"new_score\": updated_score,\n",
      "            \"updated_resume\": markdown.markdown(text=updated_resume),\n",
      "        }\n",
      "\n",
      "        yield f\"data: {json.dumps({'status': 'completed', 'result': final_result})}\\n\\n\"\n",
      "\n",
      "File apps/backend/app/agent/__init__.py has no source code\n",
      "\n",
      "File apps/backend/app/agent/exceptions.py has no source code\n",
      "\n",
      "File apps/backend/app/agent/manager.py has no source code\n",
      "\n",
      "File apps/backend/app/agent/providers/base.py has no source code\n",
      "\n",
      "File apps/backend/app/agent/providers/ollama.py has no source code\n",
      "\n",
      "File apps/backend/app/agent/providers/openai.py has no source code\n",
      "\n",
      "File apps/backend/app/agent/strategies/base.py has no source code\n",
      "\n",
      "File apps/backend/app/agent/strategies/wrapper.py has no source code\n",
      "\n",
      "File apps/backend/app/api/__init__.py has no source code\n",
      "\n",
      "File apps/backend/app/api/middleware.py has no source code\n",
      "\n",
      "File apps/backend/app/api/router/health.py has no source code\n",
      "\n",
      "File apps/backend/app/api/router/v1/__init__.py has no source code\n",
      "\n",
      "File apps/backend/app/api/router/v1/job.py has no source code\n",
      "\n",
      "File apps/backend/app/api/router/v1/resume.py has no source code\n",
      "\n",
      "File apps/backend/app/base.py has no source code\n",
      "\n",
      "File apps/backend/app/core/__init__.py has no source code\n",
      "\n",
      "File apps/backend/app/core/config.py has no source code\n",
      "\n",
      "File apps/backend/app/core/database.py has no source code\n",
      "\n",
      "File apps/backend/app/core/exceptions.py has no source code\n",
      "\n",
      "File apps/backend/app/main.py has no source code\n",
      "\n",
      "File apps/backend/app/models/__init__.py has no source code\n",
      "\n",
      "File apps/backend/app/models/association.py has no source code\n",
      "\n",
      "File apps/backend/app/models/base.py has no source code\n",
      "\n",
      "File apps/backend/app/models/job.py has no source code\n",
      "\n",
      "File apps/backend/app/models/resume.py has no source code\n",
      "\n",
      "File apps/backend/app/models/user.py has no source code\n",
      "\n",
      "File apps/backend/app/prompt/__init__.py has no source code\n",
      "\n",
      "File apps/backend/app/prompt/base.py has no source code\n",
      "\n",
      "File apps/backend/app/prompt/resume_improvement.py has no source code\n",
      "\n",
      "File apps/backend/app/prompt/structured_job.py has no source code\n",
      "\n",
      "File apps/backend/app/prompt/structured_resume.py has no source code\n",
      "\n",
      "File apps/backend/app/schemas/json/__init__.py has no source code\n",
      "\n",
      "File apps/backend/app/schemas/json/base.py has no source code\n",
      "\n",
      "File apps/backend/app/schemas/json/structured_job.py has no source code\n",
      "\n",
      "File apps/backend/app/schemas/json/structured_resume.py has no source code\n",
      "\n",
      "File apps/backend/app/schemas/pydantic/__init__.py has no source code\n",
      "\n",
      "File apps/backend/app/schemas/pydantic/job.py has no source code\n",
      "\n",
      "File apps/backend/app/schemas/pydantic/resume_improvement.py has no source code\n",
      "\n",
      "File apps/backend/app/schemas/pydantic/structured_job.py has no source code\n",
      "\n",
      "File apps/backend/app/schemas/pydantic/structured_resume.py has no source code\n",
      "\n",
      "File apps/backend/app/services/__init__.py has no source code\n",
      "\n",
      "File apps/backend/app/services/exceptions.py has no source code\n",
      "\n",
      "File apps/backend/app/services/job_service.py has no source code\n",
      "\n",
      "File apps/backend/app/services/resume_service.py has no source code\n",
      "\n",
      "File apps/backend/app/services/score_improvement_service.py has no source code\n",
      "\n",
      "import os\n",
      "import sys\n",
      "import logging\n",
      "from pydantic_settings import BaseSettings\n",
      "from typing import List, Optional, Literal\n",
      "\n",
      "\n",
      "class Settings(BaseSettings):\n",
      "    PROJECT_NAME: str = \"Resume Matcher\"\n",
      "    FRONTEND_PATH: str = os.path.join(os.path.dirname(__file__), \"frontend\", \"assets\")\n",
      "    ALLOWED_ORIGINS: List[str] = [\"http://localhost:3000\", \"http://127.0.0.1:3000\"]\n",
      "    SYNC_DATABASE_URL: Optional[str]\n",
      "    ASYNC_DATABASE_URL: Optional[str]\n",
      "    SESSION_SECRET_KEY: Optional[str]\n",
      "    DB_ECHO: bool = False\n",
      "    PYTHONDONTWRITEBYTECODE: int = 1\n",
      "\n",
      "    class Config:\n",
      "        env_file = \"app/backend/.env\"\n",
      "\n",
      "\n",
      "settings = Settings()\n",
      "\n",
      "\n",
      "_LEVEL_BY_ENV: dict[Literal[\"production\", \"staging\", \"local\"], int] = {\n",
      "    \"production\": logging.INFO,\n",
      "    \"staging\": logging.DEBUG,\n",
      "    \"local\": logging.DEBUG,\n",
      "}\n",
      "\n",
      "\n",
      "def setup_logging() -> None:\n",
      "    \"\"\"\n",
      "    Configure the root logger exactly once,\n",
      "\n",
      "    * Console only (StreamHandler -> stderr)\n",
      "    * ISO - 8601 timestamps\n",
      "    * Env - based log level: production -> INFO, else DEBUG\n",
      "    * Prevents duplicate handler creation if called twice\n",
      "    \"\"\"\n",
      "    root = logging.getLogger()\n",
      "    if root.handlers:\n",
      "        return\n",
      "\n",
      "    env = settings.ENV.lower() if hasattr(settings, \"ENV\") else \"production\"\n",
      "    level = _LEVEL_BY_ENV.get(env, logging.INFO)\n",
      "\n",
      "    formatter = logging.Formatter(\n",
      "        fmt=\"[%(asctime)s - %(name)s - %(levelname)s] %(message)s\",\n",
      "        datefmt=\"%Y-%m-%dT%H:%M:%S%z\",\n",
      "    )\n",
      "\n",
      "    handler = logging.StreamHandler(sys.stderr)\n",
      "    handler.setFormatter(formatter)\n",
      "\n",
      "    root.setLevel(level)\n",
      "    root.addHandler(handler)\n",
      "\n",
      "    for noisy in (\"sqlalchemy.engine\", \"uvicorn.access\"):\n",
      "        logging.getLogger(noisy).setLevel(logging.WARNING)\n",
      "\n",
      "import os\n",
      "import sys\n",
      "import logging\n",
      "from pydantic_settings import BaseSettings, SettingsConfigDict\n",
      "from typing import List, Optional, Literal\n",
      "\n",
      "\n",
      "class Settings(BaseSettings):\n",
      "    PROJECT_NAME: str = \"Resume Matcher\"\n",
      "    FRONTEND_PATH: str = os.path.join(os.path.dirname(__file__), \"frontend\", \"assets\")\n",
      "    ALLOWED_ORIGINS: List[str] = [\"http://localhost:3000\", \"http://127.0.0.1:3000\"]\n",
      "    SYNC_DATABASE_URL: Optional[str]\n",
      "    ASYNC_DATABASE_URL: Optional[str]\n",
      "    SESSION_SECRET_KEY: Optional[str]\n",
      "    DB_ECHO: bool = False\n",
      "    PYTHONDONTWRITEBYTECODE: int = 1\n",
      "\n",
      "    model_config = SettingsConfigDict(\n",
      "        env_file=os.path.join(os.path.dirname(__file__), os.pardir, os.pardir, \".env\"),\n",
      "        env_file_encoding=\"utf-8\",\n",
      "    )\n",
      "\n",
      "\n",
      "settings = Settings()\n",
      "\n",
      "\n",
      "_LEVEL_BY_ENV: dict[Literal[\"production\", \"staging\", \"local\"], int] = {\n",
      "    \"production\": logging.INFO,\n",
      "    \"staging\": logging.DEBUG,\n",
      "    \"local\": logging.DEBUG,\n",
      "}\n",
      "\n",
      "\n",
      "def setup_logging() -> None:\n",
      "    \"\"\"\n",
      "    Configure the root logger exactly once,\n",
      "\n",
      "    * Console only (StreamHandler -> stderr)\n",
      "    * ISO - 8601 timestamps\n",
      "    * Env - based log level: production -> INFO, else DEBUG\n",
      "    * Prevents duplicate handler creation if called twice\n",
      "    \"\"\"\n",
      "    root = logging.getLogger()\n",
      "    if root.handlers:\n",
      "        return\n",
      "\n",
      "    env = settings.ENV.lower() if hasattr(settings, \"ENV\") else \"production\"\n",
      "    level = _LEVEL_BY_ENV.get(env, logging.INFO)\n",
      "\n",
      "    formatter = logging.Formatter(\n",
      "        fmt=\"[%(asctime)s - %(name)s - %(levelname)s] %(message)s\",\n",
      "        datefmt=\"%Y-%m-%dT%H:%M:%S%z\",\n",
      "    )\n",
      "\n",
      "    handler = logging.StreamHandler(sys.stderr)\n",
      "    handler.setFormatter(formatter)\n",
      "\n",
      "    root.setLevel(level)\n",
      "    root.addHandler(handler)\n",
      "\n",
      "    for noisy in (\"sqlalchemy.engine\", \"uvicorn.access\"):\n",
      "        logging.getLogger(noisy).setLevel(logging.WARNING)\n",
      "\n",
      "from sqlalchemy import text\n",
      "from sqlalchemy.ext.asyncio import AsyncSession\n",
      "from fastapi import APIRouter, status, Depends\n",
      "\n",
      "from app.core import get_db_session\n",
      "\n",
      "health_check = APIRouter()\n",
      "\n",
      "\n",
      "@health_check.get(\"/ping\", tags=[\"Health check\"], status_code=status.HTTP_200_OK)\n",
      "async def ping(db: AsyncSession = Depends(get_db_session)):\n",
      "    \"\"\"\n",
      "    health check endpoint\n",
      "    \"\"\"\n",
      "    try:\n",
      "        result = await db.execute(text(\"SELECT 1\"))\n",
      "        db_status = \"reachable\" if result.fetchone() is not None else \"not reachable\"\n",
      "    except Exception as e:\n",
      "        import logging\n",
      "        logging.error(\"Database health check failed\", exc_info=True)\n",
      "        db_status = \"unreachable\"\n",
      "    return {\"message\": \"pong\", \"database\": db_status}\n",
      "\n",
      "SCHEMA = {\n",
      "    \"personalInfo\": {\n",
      "        \"name\": \"string\",\n",
      "        \"title\": \"string\",\n",
      "        \"email\": \"string\",\n",
      "        \"phone\": \"string\",\n",
      "        \"location\": \"string | null\",\n",
      "        \"website\": \"string | null\",\n",
      "        \"linkedin\": \"string | null\",\n",
      "        \"github\": \"string | null\",\n",
      "    },\n",
      "    \"summary\": \"string\",\n",
      "    \"experience\": [\n",
      "        {\n",
      "            \"id\": 0,\n",
      "            \"title\": \"string\",\n",
      "            \"company\": \"string\",\n",
      "            \"location\": \"string\",\n",
      "            \"years\": \"string\",\n",
      "            \"description\": [\"string\"],\n",
      "        }\n",
      "    ],\n",
      "    \"education\": [\n",
      "        {\n",
      "            \"id\": 0,\n",
      "            \"institution\": \"string\",\n",
      "            \"degree\": \"string\",\n",
      "            \"years\": \"string\",\n",
      "            \"description\": \"string\",\n",
      "        }\n",
      "    ],\n",
      "    \"skills\": [\"string\"],\n",
      "}\n",
      "\n",
      "from .job import JobUploadRequest\n",
      "from .structured_job import StructuredJobModel\n",
      "from .resume_preview import ResumePreviewerModel\n",
      "from .structured_resume import StructuredResumeModel\n",
      "from .resume_improvement import ResumeImprovementRequest\n",
      "\n",
      "__all__ = [\n",
      "    \"JobUploadRequest\",\n",
      "    \"ResumePreviewerModel\",\n",
      "    \"StructuredResumeModel\",\n",
      "    \"StructuredJobModel\",\n",
      "    \"ResumeImprovementRequest\",\n",
      "]\n",
      "\n",
      "from typing import List, Optional\n",
      "from pydantic import BaseModel\n",
      "\n",
      "\n",
      "class PersonalInfo(BaseModel):\n",
      "    name: str\n",
      "    title: Optional[str] = None\n",
      "    email: str\n",
      "    phone: str\n",
      "    location: Optional[str] = None\n",
      "    website: Optional[str] = None\n",
      "    linkedin: Optional[str] = None\n",
      "    github: Optional[str] = None\n",
      "\n",
      "\n",
      "class ExperienceItem(BaseModel):\n",
      "    id: int\n",
      "    title: str\n",
      "    company: str\n",
      "    location: Optional[str] = None\n",
      "    years: str\n",
      "    description: List[str]\n",
      "\n",
      "\n",
      "class EducationItem(BaseModel):\n",
      "    id: int\n",
      "    institution: str\n",
      "    degree: str\n",
      "    years: str\n",
      "    description: str\n",
      "\n",
      "\n",
      "class ResumePreviewerModel(BaseModel):\n",
      "    personalInfo: PersonalInfo\n",
      "    summary: str\n",
      "    experience: List[ExperienceItem]\n",
      "    education: List[EducationItem]\n",
      "    skills: List[str]\n",
      "\n",
      "import json\n",
      "import asyncio\n",
      "import logging\n",
      "import markdown\n",
      "import numpy as np\n",
      "\n",
      "from sqlalchemy.future import select\n",
      "from pydantic import ValidationError\n",
      "from sqlalchemy.ext.asyncio import AsyncSession\n",
      "from typing import Dict, Optional, Tuple, AsyncGenerator\n",
      "\n",
      "from app.prompt import prompt_factory\n",
      "from app.schemas.json import json_schema_factory\n",
      "from app.schemas.pydantic import ResumePreviewerModel\n",
      "from app.agent import EmbeddingManager, AgentManager\n",
      "from app.models import Resume, Job, ProcessedResume, ProcessedJob\n",
      "from .exceptions import (\n",
      "    ResumeNotFoundError,\n",
      "    JobNotFoundError,\n",
      "    ResumeParsingError,\n",
      "    JobParsingError,\n",
      ")\n",
      "\n",
      "logger = logging.getLogger(__name__)\n",
      "\n",
      "\n",
      "class ScoreImprovementService:\n",
      "    \"\"\"\n",
      "    Service to handle scoring of resumes and jobs using embeddings.\n",
      "    Fetches Resume and Job data from the database, computes embeddings,\n",
      "    and calculates cosine similarity scores. Uses LLM for iteratively improving\n",
      "    the scoring process.\n",
      "    \"\"\"\n",
      "\n",
      "    def __init__(self, db: AsyncSession, max_retries: int = 5):\n",
      "        self.db = db\n",
      "        self.max_retries = max_retries\n",
      "        self.md_agent_manager = AgentManager(strategy=\"md\")\n",
      "        self.json_agent_manager = AgentManager()\n",
      "        self.embedding_manager = EmbeddingManager()\n",
      "\n",
      "    async def _get_resume(\n",
      "        self, resume_id: str\n",
      "    ) -> Tuple[Resume | None, ProcessedResume | None]:\n",
      "        \"\"\"\n",
      "        Fetches the resume from the database.\n",
      "        \"\"\"\n",
      "        query = select(Resume).where(Resume.resume_id == resume_id)\n",
      "        result = await self.db.execute(query)\n",
      "        resume = result.scalars().first()\n",
      "\n",
      "        if not resume:\n",
      "            raise ResumeNotFoundError(resume_id=resume_id)\n",
      "\n",
      "        query = select(ProcessedResume).where(ProcessedResume.resume_id == resume_id)\n",
      "        result = await self.db.execute(query)\n",
      "        processed_resume = result.scalars().first()\n",
      "\n",
      "        if not processed_resume:\n",
      "            ResumeParsingError(resume_id=resume_id)\n",
      "\n",
      "        return resume, processed_resume\n",
      "\n",
      "    async def _get_job(self, job_id: str) -> Tuple[Job | None, ProcessedJob | None]:\n",
      "        \"\"\"\n",
      "        Fetches the job from the database.\n",
      "        \"\"\"\n",
      "        query = select(Job).where(Job.job_id == job_id)\n",
      "        result = await self.db.execute(query)\n",
      "        job = result.scalars().first()\n",
      "\n",
      "        if not job:\n",
      "            raise JobNotFoundError(job_id=job_id)\n",
      "\n",
      "        query = select(ProcessedJob).where(ProcessedJob.job_id == job_id)\n",
      "        result = await self.db.execute(query)\n",
      "        processed_job = result.scalars().first()\n",
      "\n",
      "        if not processed_job:\n",
      "            JobParsingError(job_id=job_id)\n",
      "\n",
      "        return job, processed_job\n",
      "\n",
      "    def calculate_cosine_similarity(\n",
      "        self,\n",
      "        extracted_job_keywords_embedding: np.ndarray,\n",
      "        resume_embedding: np.ndarray,\n",
      "    ) -> float:\n",
      "        \"\"\"\n",
      "        Calculates the cosine similarity between two embeddings.\n",
      "        \"\"\"\n",
      "        if resume_embedding is None or extracted_job_keywords_embedding is None:\n",
      "            return 0.0\n",
      "\n",
      "        ejk = np.asarray(extracted_job_keywords_embedding).squeeze()\n",
      "        re = np.asarray(resume_embedding).squeeze()\n",
      "\n",
      "        return float(np.dot(ejk, re) / (np.linalg.norm(ejk) * np.linalg.norm(re)))\n",
      "\n",
      "    async def improve_score_with_llm(\n",
      "        self,\n",
      "        resume: str,\n",
      "        extracted_resume_keywords: str,\n",
      "        job: str,\n",
      "        extracted_job_keywords: str,\n",
      "        previous_cosine_similarity_score: float,\n",
      "        extracted_job_keywords_embedding: float,\n",
      "        attempt: Optional[int] = 1,\n",
      "    ) -> Tuple[str, float]:\n",
      "        \"\"\"\n",
      "        Uses LLM to improve the score based on resume and job description.\n",
      "        \"\"\"\n",
      "        prompt_template = prompt_factory.get(\"resume_improvement\")\n",
      "        init_prompt = prompt_template.format(\n",
      "            raw_job_description=job,\n",
      "            extracted_job_keywords=extracted_job_keywords,\n",
      "            raw_resume=resume,\n",
      "            extracted_resume_keywords=extracted_resume_keywords,\n",
      "            current_cosine_similarity=previous_cosine_similarity_score,\n",
      "        )\n",
      "        improved_resume = await self.md_agent_manager.run(init_prompt)\n",
      "\n",
      "        improved_resume_embedding = await self.embedding_manager.embed(\n",
      "            text=improved_resume\n",
      "        )\n",
      "\n",
      "        new_score = self.calculate_cosine_similarity(\n",
      "            improved_resume_embedding, extracted_job_keywords_embedding\n",
      "        )\n",
      "        if new_score > previous_cosine_similarity_score or attempt >= self.max_retries:\n",
      "            return improved_resume, new_score\n",
      "\n",
      "        return await self.improve_score_with_llm(\n",
      "            resume=improved_resume,\n",
      "            extracted_resume_keywords=extracted_resume_keywords,\n",
      "            job=job,\n",
      "            extracted_job_keywords=extracted_job_keywords,\n",
      "            previous_cosine_similarity_score=new_score,\n",
      "            attempt=attempt + 1,\n",
      "        )\n",
      "\n",
      "    async def get_resume_for_previewer(self, updated_resume: str) -> Dict:\n",
      "        \"\"\"\n",
      "        Returns the updated resume in a format suitable for the dashboard.\n",
      "        \"\"\"\n",
      "        prompt_template = prompt_factory.get(\"structured_resume\")\n",
      "        prompt = prompt_template.format(\n",
      "            json.dumps(json_schema_factory.get(\"structured_resume\"), indent=2),\n",
      "            updated_resume,\n",
      "        )\n",
      "        logger.info(f\"Structured Resume Prompt: {prompt}\")\n",
      "        raw_output = await self.json_agent_manager.run(prompt=prompt)\n",
      "\n",
      "        try:\n",
      "            resume_preview: ResumePreviewerModel = ResumePreviewerModel.model_validate(\n",
      "                raw_output\n",
      "            )\n",
      "        except ValidationError as e:\n",
      "            logger.info(f\"Validation error: {e}\")\n",
      "            return None\n",
      "        return resume_preview.model_dump()\n",
      "\n",
      "    async def run(self, resume_id: str, job_id: str) -> Dict:\n",
      "        \"\"\"\n",
      "        Main method to run the scoring and improving process and return dict.\n",
      "        \"\"\"\n",
      "\n",
      "        resume, processed_resume = await self._get_resume(resume_id)\n",
      "        job, processed_job = await self._get_job(job_id)\n",
      "\n",
      "        extracted_job_keywords = \", \".join(\n",
      "            json.loads(processed_job.extracted_keywords).get(\"extracted_keywords\", [])\n",
      "        )\n",
      "\n",
      "        extracted_resume_keywords = \", \".join(\n",
      "            json.loads(processed_resume.extracted_keywords).get(\n",
      "                \"extracted_keywords\", []\n",
      "            )\n",
      "        )\n",
      "\n",
      "        resume_embedding = await self.embedding_manager.embed(text=resume.content)\n",
      "        extracted_job_keywords_embedding = await self.embedding_manager.embed(\n",
      "            text=extracted_job_keywords\n",
      "        )\n",
      "\n",
      "        cosine_similarity_score = self.calculate_cosine_similarity(\n",
      "            extracted_job_keywords_embedding, resume_embedding\n",
      "        )\n",
      "        updated_resume, updated_score = await self.improve_score_with_llm(\n",
      "            resume=resume.content,\n",
      "            extracted_resume_keywords=extracted_resume_keywords,\n",
      "            job=job.content,\n",
      "            extracted_job_keywords=extracted_job_keywords,\n",
      "            previous_cosine_similarity_score=cosine_similarity_score,\n",
      "            extracted_job_keywords_embedding=extracted_job_keywords_embedding,\n",
      "        )\n",
      "\n",
      "        resume_preview = await self.get_resume_for_previewer(\n",
      "            updated_resume=updated_resume\n",
      "        )\n",
      "\n",
      "        return {\n",
      "            \"resume_id\": resume_id,\n",
      "            \"job_id\": job_id,\n",
      "            \"original_score\": cosine_similarity_score,\n",
      "            \"new_score\": updated_score,\n",
      "            \"updated_resume\": markdown.markdown(text=updated_resume),\n",
      "            \"resume_preview\": resume_preview,\n",
      "        }\n",
      "\n",
      "    async def run_and_stream(self, resume_id: str, job_id: str) -> AsyncGenerator:\n",
      "        \"\"\"\n",
      "        Main method to run the scoring and improving process and return dict.\n",
      "        \"\"\"\n",
      "\n",
      "        yield f\"data: {json.dumps({'status': 'starting', 'message': 'Analyzing resume and job description...'})}\\n\\n\"\n",
      "        await asyncio.sleep(2)\n",
      "\n",
      "        resume, processed_resume = await self._get_resume(resume_id)\n",
      "        job, processed_job = await self._get_job(job_id)\n",
      "\n",
      "        yield f\"data: {json.dumps({'status': 'parsing', 'message': 'Parsing resume content...'})}\\n\\n\"\n",
      "        await asyncio.sleep(2)\n",
      "\n",
      "        extracted_job_keywords = \", \".join(\n",
      "            json.loads(processed_job.extracted_keywords).get(\"extracted_keywords\", [])\n",
      "        )\n",
      "\n",
      "        extracted_resume_keywords = \", \".join(\n",
      "            json.loads(processed_resume.extracted_keywords).get(\n",
      "                \"extracted_keywords\", []\n",
      "            )\n",
      "        )\n",
      "\n",
      "        resume_embedding = await self.embedding_manager.embed(text=resume.content)\n",
      "        extracted_job_keywords_embedding = await self.embedding_manager.embed(\n",
      "            text=extracted_job_keywords\n",
      "        )\n",
      "\n",
      "        yield f\"data: {json.dumps({'status': 'scoring', 'message': 'Calculating compatibility score...'})}\\n\\n\"\n",
      "        await asyncio.sleep(3)\n",
      "\n",
      "        cosine_similarity_score = self.calculate_cosine_similarity(\n",
      "            extracted_job_keywords_embedding, resume_embedding\n",
      "        )\n",
      "\n",
      "        yield f\"data: {json.dumps({'status': 'scored', 'score': cosine_similarity_score})}\\n\\n\"\n",
      "\n",
      "        yield f\"data: {json.dumps({'status': 'improving', 'message': 'Generating improvement suggestions...'})}\\n\\n\"\n",
      "        await asyncio.sleep(3)\n",
      "\n",
      "        updated_resume, updated_score = await self.improve_score_with_llm(\n",
      "            resume=resume.content,\n",
      "            extracted_resume_keywords=extracted_resume_keywords,\n",
      "            job=job.content,\n",
      "            extracted_job_keywords=extracted_job_keywords,\n",
      "            previous_cosine_similarity_score=cosine_similarity_score,\n",
      "            extracted_job_keywords_embedding=extracted_job_keywords_embedding,\n",
      "        )\n",
      "\n",
      "        for i, suggestion in enumerate(updated_resume):\n",
      "            yield f\"data: {json.dumps({'status': 'suggestion', 'index': i, 'text': suggestion})}\\n\\n\"\n",
      "            await asyncio.sleep(0.2)\n",
      "\n",
      "        final_result = {\n",
      "            \"resume_id\": resume_id,\n",
      "            \"job_id\": job_id,\n",
      "            \"original_score\": cosine_similarity_score,\n",
      "            \"new_score\": updated_score,\n",
      "            \"updated_resume\": markdown.markdown(text=updated_resume),\n",
      "        }\n",
      "\n",
      "        yield f\"data: {json.dumps({'status': 'completed', 'result': final_result})}\\n\\n\"\n",
      "\n",
      "import enum\n",
      "\n",
      "from typing import Optional, List\n",
      "from pydantic import BaseModel, Field\n",
      "\n",
      "\n",
      "class EmploymentTypeEnum(str, enum.Enum):\n",
      "    \"\"\"Case-insensitive Enum for employment types.\"\"\"\n",
      "\n",
      "    FULL_TIME = \"Full-time\"\n",
      "    PART_TIME = \"Part-time\"\n",
      "    CONTRACT = \"Contract\"\n",
      "    INTERNSHIP = \"Internship\"\n",
      "    TEMPORARY = \"Temporary\"\n",
      "    NOT_SPECIFIED = \"Not Specified\"\n",
      "\n",
      "    @classmethod\n",
      "    def _missing_(cls, value: object):\n",
      "        \"\"\"Handles case-insensitive lookup.\"\"\"\n",
      "        if isinstance(value, str):\n",
      "            value_lower = value.lower()\n",
      "            mapping = {member.value.lower(): member for member in cls}\n",
      "            if value_lower in mapping:\n",
      "                return mapping[value_lower]\n",
      "\n",
      "        raise ValueError(\n",
      "            \"employment type must be one of: Full-time, Part-time, Contract, Internship, Temporary, Not Specified (case insensitive)\"\n",
      "        )\n",
      "\n",
      "\n",
      "class RemoteStatusEnum(str, enum.Enum):\n",
      "    \"\"\"Case-insensitive Enum for remote work status.\"\"\"\n",
      "\n",
      "    FULLY_REMOTE = \"Fully Remote\"\n",
      "    HYBRID = \"Hybrid\"\n",
      "    ON_SITE = \"On-site\"\n",
      "    REMOTE = \"Remote\"\n",
      "    NOT_SPECIFIED = \"Not Specified\"\n",
      "\n",
      "    @classmethod\n",
      "    def _missing_(cls, value: object):\n",
      "        \"\"\"Handles case-insensitive lookup.\"\"\"\n",
      "        if isinstance(value, str):\n",
      "            value_lower = value.lower()\n",
      "            mapping = {member.value.lower(): member for member in cls}\n",
      "            if value_lower in mapping:\n",
      "                return mapping[value_lower]\n",
      "\n",
      "        raise ValueError(\n",
      "            \"remote_status must be one of: Fully Remote, Hybrid, On-site, Remote, Not Specified (case insensitive)\"\n",
      "        )\n",
      "\n",
      "\n",
      "class CompanyProfile(BaseModel):\n",
      "    company_name: str = Field(..., alias=\"companyName\")\n",
      "    industry: Optional[str] = None\n",
      "    website: Optional[str] = None\n",
      "    description: Optional[str] = None\n",
      "\n",
      "\n",
      "class Location(BaseModel):\n",
      "    city: Optional[str] = None\n",
      "    state: Optional[str] = None\n",
      "    country: Optional[str] = None\n",
      "    remote_status: RemoteStatusEnum = Field(..., alias=\"remoteStatus\")\n",
      "\n",
      "\n",
      "class Qualifications(BaseModel):\n",
      "    required: List[str]\n",
      "    preferred: Optional[List[str]] = None\n",
      "\n",
      "\n",
      "class CompensationAndBenefits(BaseModel):\n",
      "    salary_range: Optional[str] = Field(..., alias=\"salaryRange\")\n",
      "    benefits: Optional[List[str]] = None\n",
      "\n",
      "\n",
      "class ApplicationInfo(BaseModel):\n",
      "    how_to_apply: Optional[str] = Field(..., alias=\"howToApply\")\n",
      "    apply_link: Optional[str] = Field(..., alias=\"applyLink\")\n",
      "    contact_email: Optional[str] = Field(..., alias=\"contactEmail\")\n",
      "\n",
      "\n",
      "class StructuredJobModel(BaseModel):\n",
      "    job_title: str = Field(..., alias=\"jobTitle\")\n",
      "    company_profile: CompanyProfile = Field(..., alias=\"companyProfile\")\n",
      "    location: Location\n",
      "    date_posted: str = Field(..., alias=\"datePosted\")\n",
      "    employment_type: EmploymentTypeEnum = Field(..., alias=\"employmentType\")\n",
      "    job_summary: str = Field(..., alias=\"jobSummary\")\n",
      "    key_responsibilities: List[str] = Field(..., alias=\"keyResponsibilities\")\n",
      "    qualifications: Qualifications\n",
      "    compensation_and_benefits: Optional[CompensationAndBenefits] = Field(\n",
      "        None, alias=\"compensationAndBenefits\"\n",
      "    )\n",
      "    application_info: Optional[ApplicationInfo] = Field(None, alias=\"applicationInfo\")\n",
      "    extracted_keywords: List[str] = Field(..., alias=\"extractedKeywords\")\n",
      "\n",
      "    class ConfigDict:\n",
      "        validate_by_name = True\n",
      "        str_strip_whitespace = True\n",
      "\n",
      "import gc\n",
      "import json\n",
      "import asyncio\n",
      "import logging\n",
      "import markdown\n",
      "import numpy as np\n",
      "\n",
      "from sqlalchemy.future import select\n",
      "from pydantic import ValidationError\n",
      "from sqlalchemy.ext.asyncio import AsyncSession\n",
      "from typing import Dict, Optional, Tuple, AsyncGenerator\n",
      "\n",
      "from app.prompt import prompt_factory\n",
      "from app.schemas.json import json_schema_factory\n",
      "from app.schemas.pydantic import ResumePreviewerModel\n",
      "from app.agent import EmbeddingManager, AgentManager\n",
      "from app.models import Resume, Job, ProcessedResume, ProcessedJob\n",
      "from .exceptions import (\n",
      "    ResumeNotFoundError,\n",
      "    JobNotFoundError,\n",
      "    ResumeParsingError,\n",
      "    JobParsingError,\n",
      ")\n",
      "\n",
      "logger = logging.getLogger(__name__)\n",
      "\n",
      "\n",
      "class ScoreImprovementService:\n",
      "    \"\"\"\n",
      "    Service to handle scoring of resumes and jobs using embeddings.\n",
      "    Fetches Resume and Job data from the database, computes embeddings,\n",
      "    and calculates cosine similarity scores. Uses LLM for iteratively improving\n",
      "    the scoring process.\n",
      "    \"\"\"\n",
      "\n",
      "    def __init__(self, db: AsyncSession, max_retries: int = 5):\n",
      "        self.db = db\n",
      "        self.max_retries = max_retries\n",
      "        self.md_agent_manager = AgentManager(strategy=\"md\")\n",
      "        self.json_agent_manager = AgentManager()\n",
      "        self.embedding_manager = EmbeddingManager()\n",
      "\n",
      "    async def _get_resume(\n",
      "        self, resume_id: str\n",
      "    ) -> Tuple[Resume | None, ProcessedResume | None]:\n",
      "        \"\"\"\n",
      "        Fetches the resume from the database.\n",
      "        \"\"\"\n",
      "        query = select(Resume).where(Resume.resume_id == resume_id)\n",
      "        result = await self.db.execute(query)\n",
      "        resume = result.scalars().first()\n",
      "\n",
      "        if not resume:\n",
      "            raise ResumeNotFoundError(resume_id=resume_id)\n",
      "\n",
      "        query = select(ProcessedResume).where(ProcessedResume.resume_id == resume_id)\n",
      "        result = await self.db.execute(query)\n",
      "        processed_resume = result.scalars().first()\n",
      "\n",
      "        if not processed_resume:\n",
      "            ResumeParsingError(resume_id=resume_id)\n",
      "\n",
      "        return resume, processed_resume\n",
      "\n",
      "    async def _get_job(self, job_id: str) -> Tuple[Job | None, ProcessedJob | None]:\n",
      "        \"\"\"\n",
      "        Fetches the job from the database.\n",
      "        \"\"\"\n",
      "        query = select(Job).where(Job.job_id == job_id)\n",
      "        result = await self.db.execute(query)\n",
      "        job = result.scalars().first()\n",
      "\n",
      "        if not job:\n",
      "            raise JobNotFoundError(job_id=job_id)\n",
      "\n",
      "        query = select(ProcessedJob).where(ProcessedJob.job_id == job_id)\n",
      "        result = await self.db.execute(query)\n",
      "        processed_job = result.scalars().first()\n",
      "\n",
      "        if not processed_job:\n",
      "            JobParsingError(job_id=job_id)\n",
      "\n",
      "        return job, processed_job\n",
      "\n",
      "    def calculate_cosine_similarity(\n",
      "        self,\n",
      "        extracted_job_keywords_embedding: np.ndarray,\n",
      "        resume_embedding: np.ndarray,\n",
      "    ) -> float:\n",
      "        \"\"\"\n",
      "        Calculates the cosine similarity between two embeddings.\n",
      "        \"\"\"\n",
      "        if resume_embedding is None or extracted_job_keywords_embedding is None:\n",
      "            return 0.0\n",
      "\n",
      "        ejk = np.asarray(extracted_job_keywords_embedding).squeeze()\n",
      "        re = np.asarray(resume_embedding).squeeze()\n",
      "\n",
      "        return float(np.dot(ejk, re) / (np.linalg.norm(ejk) * np.linalg.norm(re)))\n",
      "\n",
      "    async def improve_score_with_llm(\n",
      "        self,\n",
      "        resume: str,\n",
      "        extracted_resume_keywords: str,\n",
      "        job: str,\n",
      "        extracted_job_keywords: str,\n",
      "        previous_cosine_similarity_score: float,\n",
      "        extracted_job_keywords_embedding: np.ndarray,\n",
      "    ) -> Tuple[str, float]:\n",
      "        prompt_template = prompt_factory.get(\"resume_improvement\")\n",
      "        best_resume, best_score = resume, previous_cosine_similarity_score\n",
      "\n",
      "        for attempt in range(1, self.max_retries + 1):\n",
      "            logger.info(\n",
      "                f\"Attempt {attempt}/{self.max_retries} to improve resume score.\"\n",
      "            )\n",
      "            prompt = prompt_template.format(\n",
      "                raw_job_description=job,\n",
      "                extracted_job_keywords=extracted_job_keywords,\n",
      "                raw_resume=best_resume,\n",
      "                extracted_resume_keywords=extracted_resume_keywords,\n",
      "                current_cosine_similarity=best_score,\n",
      "            )\n",
      "            improved = await self.md_agent_manager.run(prompt)\n",
      "            emb = await self.embedding_manager.embed(text=improved)\n",
      "            score = self.calculate_cosine_similarity(\n",
      "                emb, extracted_job_keywords_embedding\n",
      "            )\n",
      "\n",
      "            if score > best_score:\n",
      "                return improved, score\n",
      "\n",
      "            logger.info(\n",
      "                f\"Attempt {attempt} resulted in score: {score}, best score so far: {best_score}\"\n",
      "            )\n",
      "\n",
      "        return best_resume, best_score\n",
      "\n",
      "    async def get_resume_for_previewer(self, updated_resume: str) -> Dict:\n",
      "        \"\"\"\n",
      "        Returns the updated resume in a format suitable for the dashboard.\n",
      "        \"\"\"\n",
      "        prompt_template = prompt_factory.get(\"structured_resume\")\n",
      "        prompt = prompt_template.format(\n",
      "            json.dumps(json_schema_factory.get(\"resume_preview\"), indent=2),\n",
      "            updated_resume,\n",
      "        )\n",
      "        logger.info(f\"Structured Resume Prompt: {prompt}\")\n",
      "        raw_output = await self.json_agent_manager.run(prompt=prompt)\n",
      "\n",
      "        try:\n",
      "            resume_preview: ResumePreviewerModel = ResumePreviewerModel.model_validate(\n",
      "                raw_output\n",
      "            )\n",
      "        except ValidationError as e:\n",
      "            logger.info(f\"Validation error: {e}\")\n",
      "            return None\n",
      "        return resume_preview.model_dump()\n",
      "\n",
      "    async def run(self, resume_id: str, job_id: str) -> Dict:\n",
      "        \"\"\"\n",
      "        Main method to run the scoring and improving process and return dict.\n",
      "        \"\"\"\n",
      "\n",
      "        resume, processed_resume = await self._get_resume(resume_id)\n",
      "        job, processed_job = await self._get_job(job_id)\n",
      "\n",
      "        extracted_job_keywords = \", \".join(\n",
      "            json.loads(processed_job.extracted_keywords).get(\"extracted_keywords\", [])\n",
      "        )\n",
      "\n",
      "        extracted_resume_keywords = \", \".join(\n",
      "            json.loads(processed_resume.extracted_keywords).get(\n",
      "                \"extracted_keywords\", []\n",
      "            )\n",
      "        )\n",
      "\n",
      "        resume_embedding_task = asyncio.create_task(\n",
      "            self.embedding_manager.embed(resume.content)\n",
      "        )\n",
      "        job_kw_embedding_task = asyncio.create_task(\n",
      "            self.embedding_manager.embed(extracted_job_keywords)\n",
      "        )\n",
      "        resume_embedding, extracted_job_keywords_embedding = await asyncio.gather(\n",
      "            resume_embedding_task, job_kw_embedding_task\n",
      "        )\n",
      "\n",
      "        cosine_similarity_score = self.calculate_cosine_similarity(\n",
      "            extracted_job_keywords_embedding, resume_embedding\n",
      "        )\n",
      "        updated_resume, updated_score = await self.improve_score_with_llm(\n",
      "            resume=resume.content,\n",
      "            extracted_resume_keywords=extracted_resume_keywords,\n",
      "            job=job.content,\n",
      "            extracted_job_keywords=extracted_job_keywords,\n",
      "            previous_cosine_similarity_score=cosine_similarity_score,\n",
      "            extracted_job_keywords_embedding=extracted_job_keywords_embedding,\n",
      "        )\n",
      "\n",
      "        resume_preview = await self.get_resume_for_previewer(\n",
      "            updated_resume=updated_resume\n",
      "        )\n",
      "\n",
      "        logger.info(f\"Resume Preview: {resume_preview}\")\n",
      "\n",
      "        execution = {\n",
      "            \"resume_id\": resume_id,\n",
      "            \"job_id\": job_id,\n",
      "            \"original_score\": cosine_similarity_score,\n",
      "            \"new_score\": updated_score,\n",
      "            \"updated_resume\": markdown.markdown(text=updated_resume),\n",
      "            \"resume_preview\": resume_preview,\n",
      "        }\n",
      "\n",
      "        gc.collect()\n",
      "\n",
      "        return execution\n",
      "\n",
      "    async def run_and_stream(self, resume_id: str, job_id: str) -> AsyncGenerator:\n",
      "        \"\"\"\n",
      "        Main method to run the scoring and improving process and return dict.\n",
      "        \"\"\"\n",
      "\n",
      "        yield f\"data: {json.dumps({'status': 'starting', 'message': 'Analyzing resume and job description...'})}\\n\\n\"\n",
      "        await asyncio.sleep(2)\n",
      "\n",
      "        resume, processed_resume = await self._get_resume(resume_id)\n",
      "        job, processed_job = await self._get_job(job_id)\n",
      "\n",
      "        yield f\"data: {json.dumps({'status': 'parsing', 'message': 'Parsing resume content...'})}\\n\\n\"\n",
      "        await asyncio.sleep(2)\n",
      "\n",
      "        extracted_job_keywords = \", \".join(\n",
      "            json.loads(processed_job.extracted_keywords).get(\"extracted_keywords\", [])\n",
      "        )\n",
      "\n",
      "        extracted_resume_keywords = \", \".join(\n",
      "            json.loads(processed_resume.extracted_keywords).get(\n",
      "                \"extracted_keywords\", []\n",
      "            )\n",
      "        )\n",
      "\n",
      "        resume_embedding = await self.embedding_manager.embed(text=resume.content)\n",
      "        extracted_job_keywords_embedding = await self.embedding_manager.embed(\n",
      "            text=extracted_job_keywords\n",
      "        )\n",
      "\n",
      "        yield f\"data: {json.dumps({'status': 'scoring', 'message': 'Calculating compatibility score...'})}\\n\\n\"\n",
      "        await asyncio.sleep(3)\n",
      "\n",
      "        cosine_similarity_score = self.calculate_cosine_similarity(\n",
      "            extracted_job_keywords_embedding, resume_embedding\n",
      "        )\n",
      "\n",
      "        yield f\"data: {json.dumps({'status': 'scored', 'score': cosine_similarity_score})}\\n\\n\"\n",
      "\n",
      "        yield f\"data: {json.dumps({'status': 'improving', 'message': 'Generating improvement suggestions...'})}\\n\\n\"\n",
      "        await asyncio.sleep(3)\n",
      "\n",
      "        updated_resume, updated_score = await self.improve_score_with_llm(\n",
      "            resume=resume.content,\n",
      "            extracted_resume_keywords=extracted_resume_keywords,\n",
      "            job=job.content,\n",
      "            extracted_job_keywords=extracted_job_keywords,\n",
      "            previous_cosine_similarity_score=cosine_similarity_score,\n",
      "            extracted_job_keywords_embedding=extracted_job_keywords_embedding,\n",
      "        )\n",
      "\n",
      "        for i, suggestion in enumerate(updated_resume):\n",
      "            yield f\"data: {json.dumps({'status': 'suggestion', 'index': i, 'text': suggestion})}\\n\\n\"\n",
      "            await asyncio.sleep(0.2)\n",
      "\n",
      "        final_result = {\n",
      "            \"resume_id\": resume_id,\n",
      "            \"job_id\": job_id,\n",
      "            \"original_score\": cosine_similarity_score,\n",
      "            \"new_score\": updated_score,\n",
      "            \"updated_resume\": markdown.markdown(text=updated_resume),\n",
      "        }\n",
      "\n",
      "        yield f\"data: {json.dumps({'status': 'completed', 'result': final_result})}\\n\\n\"\n",
      "\n"
     ]
    }
   ],
   "source": [
    "from pydriller import Repository\n",
    "from datetime import datetime, timedelta\n",
    "import pymongo\n",
    "\n",
    "# Connect to MongoDB\n",
    "myclient = pymongo.MongoClient(\"mongodb://localhost:27017/\")\n",
    "\n",
    "# Define the repository parameters\n",
    "repo_name = \"Resume-Matcher\"\n",
    "repo_path = f\"/Users/as/Library/Mobile Documents/com~apple~CloudDocs/Dokumente/Studium/Bachelor-Thesis/tmp/{repo_name}\"\n",
    "end_date = datetime.now()\n",
    "start_date = end_date - timedelta(days=6*365)\n",
    "\n",
    "ocdb = myclient[\"OCEL\"]\n",
    "\n",
    "# Go through commits in the repository\n",
    "# FIXME Instead of using snapshot, start with inital commit\n",
    "python_files = set()\n",
    "repository_code_metrics = dict()\n",
    "\n",
    "# Snapshot of code quality for start of analysis\n",
    "for commit in Repository(repo_path, since=start_date, only_modifications_with_file_types=[\".py\"], num_workers=1).traverse_commits():\n",
    "    initial_commit = subprocess.run(['git', 'rev-parse', 'HEAD'], cwd=repo_path, stdout=subprocess.PIPE, text=True).stdout.strip()\n",
    "    \n",
    "    subprocess.run(['git', 'checkout', commit.hash], cwd=repo_path, check=True)\n",
    "    result = subprocess.run(\n",
    "        [\"git\", \"ls-files\", \"*.py\", \"**/*.py\"],\n",
    "        cwd=repo_path,\n",
    "        stdout=subprocess.PIPE,\n",
    "        text=True,\n",
    "        check=True\n",
    "    )\n",
    "    py_files = result.stdout.strip().split('\\n')\n",
    "    # Variant 1, check if simply passing the path is faster\n",
    "    for file in py_files:\n",
    "        try:\n",
    "            with open(f\"{repo_path}/{file}\", 'r') as f:\n",
    "                source_code = f.read()\n",
    "        except Exception as e:\n",
    "            print(f\"Error reading file {file}: {e}\")\n",
    "        mi = get_maintainability_index(source_code)/100\n",
    "        pl = get_pylint_score(source_code)/10\n",
    "        repository_code_metrics[file] = [mi, pl]\n",
    "\n",
    "    subprocess.run(['git', 'checkout', initial_commit], cwd=repo_path, check=True)\n",
    "    break\n",
    "    \n",
    "for commit in Repository(repo_path, since=start_date, to=end_date, only_modifications_with_file_types=[\".py\"], num_workers=1).traverse_commits():\n",
    "    file_mis = []\n",
    "    file_pylints = []\n",
    "    line_metrics = []\n",
    "    helstead_metrics = []\n",
    "\n",
    "    commit_mis = []\n",
    "    commit_pylints = []\n",
    "\n",
    "    # Go through modified files in the commit\n",
    "    for modified_file in commit.modified_files:\n",
    "        if modified_file.change_type.name == \"ADD\" and modified_file.new_path and modified_file.new_path.endswith(\".py\"):\n",
    "            repository_code_metrics[modified_file.new_path] = [0, 0]\n",
    "        elif modified_file.change_type.name == \"DELETE\" and modified_file.old_path and modified_file.old_path.endswith(\".py\"):\n",
    "            repository_code_metrics.pop(modified_file.old_path, None)\n",
    "            continue\n",
    "        elif modified_file.change_type.name == \"RENAME\":\n",
    "            if modified_file.old_path and modified_file.old_path.endswith(\".py\"):\n",
    "                repository_code_metrics.pop(modified_file.old_path, None)\n",
    "            if modified_file.new_path and modified_file.new_path.endswith(\".py\"):\n",
    "                repository_code_metrics[modified_file.new_path] = [0, 0]\n",
    "            if modified_file.source_code and modified_file.source_code_before:\n",
    "                print(f\"File LOC changed from {len(modified_file.source_code_before.split(\"\\n\"))} to {len(modified_file.source_code.split(\"\\n\"))}\")\n",
    "        elif modified_file.change_type.name == \"MODIFY\" and modified_file.new_path and modified_file.new_path.endswith(\".py\"):\n",
    "            repository_code_metrics[modified_file.new_path] = [0, 0]\n",
    "        else:\n",
    "            continue\n",
    "        \n",
    "        if modified_file.new_path is None or modified_file.new_path.endswith(\".py\") == False:\n",
    "            continue\n",
    "        # Calculate code quality metrics\n",
    "        # TODO Optimize by not analyzing files that are renamed\n",
    "        if modified_file.source_code is None:\n",
    "            source = \"\"\n",
    "            print(f\"File {modified_file.new_path} has no source code\")\n",
    "        else:\n",
    "            source = modified_file.source_code\n",
    "        mi = get_maintainability_index(source)/100\n",
    "        pl = get_pylint_score(source)/10\n",
    "        file_mis.append(mi)\n",
    "        file_pylints.append(pl)\n",
    "\n",
    "        repository_code_metrics[modified_file.new_path] = [mi, pl]\n",
    "\n",
    "        line_metrics.append(get_line_metrics(source))\n",
    "        helstead_metrics.append(get_halstead_metrics(source))\n",
    "\n",
    "        # Create a dictionary for the commit\n",
    "        type = \"commit\"\n",
    "    for k,v in repository_code_metrics.items():\n",
    "        commit_mis.append(v[0])\n",
    "        commit_pylints.append(v[1])\n",
    "    commit_mi = sum(commit_mis)/len(commit_mis) if commit_mis else 0\n",
    "    commit_pylint = sum(commit_pylints)/len(commit_pylints) if commit_pylints else 0\n",
    "    commit_dict = {\n",
    "        \"commit_date\": commit.committer_date,\n",
    "        \"commit_author\": commit.author.name,\n",
    "        \"commit_message\": commit.msg,\n",
    "        \"commit_mi\": commit_mi,\n",
    "        \"commit_pylint\": commit_pylint,\n",
    "        \"commit_loc\": sum([lm.loc for lm in line_metrics])/len(line_metrics) if line_metrics else 0,\n",
    "        \"commit_lloc\": sum([lm.lloc for lm in line_metrics])/len(line_metrics) if line_metrics else 0,\n",
    "        \"commit_sloc\": sum([lm.sloc for lm in line_metrics])/len(line_metrics) if line_metrics else 0,\n",
    "        \"commit_comments\": sum([lm.comments for lm in line_metrics])/len(line_metrics) if line_metrics else 0,\n",
    "        \"commit_single_comments\": sum([lm.single_comments for lm in line_metrics])/len(line_metrics) if line_metrics else 0,\n",
    "        \"commit_multi\": sum([lm.multi for lm in line_metrics])/len(line_metrics) if line_metrics else 0,\n",
    "        \"commit_blank\": sum([lm.blank for lm in line_metrics])/len(line_metrics) if line_metrics else 0,\n",
    "        \"commit_h1\": sum([hm.total.h1 for hm in helstead_metrics])/len(helstead_metrics) if helstead_metrics else 0,\n",
    "        \"commit_h2\": sum([hm.total.h2 for hm in helstead_metrics])/len(helstead_metrics) if helstead_metrics else 0,\n",
    "        \"commit_N1\": sum([hm.total.N1 for hm in helstead_metrics])/len(helstead_metrics) if helstead_metrics else 0,\n",
    "        \"commit_N2\": sum([hm.total.N2 for hm in helstead_metrics])/len(helstead_metrics) if helstead_metrics else 0,\n",
    "    }\n",
    "    ocdb[repo_name].replace_one({\"_id\": commit.hash}, {\"type\": type, \"attributes\": commit_dict}, upsert=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Missing attributes in for record: {'_id': '3071e17d3625214bcaec418b7e1e005186d80597', 'relationships': {'qualifier': 'commit_pylint'}}\n",
      "Missing attributes in for record: {'_id': '01d86e10a06c1d9f35238e6557bc85dda8d6fc1a', 'relationships': {'qualifier': 'commit_pylint'}}\n",
      "Missing attributes in for record: {'_id': 'b5f93a1765d4b545846165699babe8b4f7010cc1', 'relationships': {'qualifier': 'commit_pylint'}}\n",
      "Missing attributes in for record: {'_id': '807e81fe23ccfc32497c1bd43bf2fb5e429d06eb', 'relationships': {'qualifier': 'commit_pylint'}}\n",
      "Missing attributes in for record: {'_id': '2c6791a73fcbb385c4596c8a545880fd54a525f7', 'relationships': {'qualifier': 'commit_pylint'}}\n",
      "Missing attributes in for record: {'_id': '41f940bbe877608fb04a81a4e6469c4f3a1e203a', 'relationships': {'qualifier': 'commit_pylint'}}\n",
      "Missing attributes in for record: {'_id': 'bdd3e9e46b3a91cf6e73b709691c5ff7340b96df', 'relationships': {'qualifier': 'commit_pylint'}}\n",
      "Missing attributes in for record: {'_id': 'f14d939da33414b108ed69fcc5be9768426f469b', 'relationships': {'qualifier': 'commit_pylint'}}\n",
      "Missing attributes in for record: {'_id': '082a012e6fa999da52c329ae777c96c9874be736', 'relationships': {'qualifier': 'commit_pylint'}}\n",
      "Missing attributes in for record: {'_id': 'dcda5f00f20bc7a6916eefbad6a99c0d900545b3', 'relationships': {'qualifier': 'commit_pylint'}}\n",
      "Missing attributes in for record: {'_id': '688ce8f5afc967c2b558d3b406a8d54c0c183c9b', 'relationships': {'qualifier': 'commit_pylint'}}\n",
      "Missing attributes in for record: {'_id': '4052fd8e0fc7ae9f5b43b49b94b463a634bb6247', 'relationships': {'qualifier': 'commit_pylint'}}\n",
      "Missing attributes in for record: {'_id': '715c51b89e9245ee48d1367c05c55b899fcf00c7', 'relationships': {'qualifier': 'commit_pylint'}}\n",
      "Missing attributes in for record: {'_id': '91ea665f0e8fcde3ccf1fe9a1ffd44460c92805a', 'relationships': {'qualifier': 'commit_pylint'}}\n",
      "Missing attributes in for record: {'_id': '1e15f522c9e77cd20a5773c2dcbb9b790bcb71a7', 'relationships': {'qualifier': 'commit_pylint'}}\n",
      "Missing attributes in for record: {'_id': 'f1afc0e0a2d77ab9f0fa34fb445a6ae32a2f0631', 'relationships': {'qualifier': 'commit_pylint'}}\n",
      "Missing attributes in for record: {'_id': '0921e280eb047f7f24c8d3f31ea3e98cb8c32b20', 'relationships': {'qualifier': 'commit_pylint'}}\n",
      "Missing attributes in for record: {'_id': 'b2c8d7cdb7f618a9306c0c4e154c21dddb6c364d', 'relationships': {'qualifier': 'commit_pylint'}}\n",
      "Missing attributes in for record: {'_id': 'e46c65b48ab995981dac429747b927ae68723bd6', 'relationships': {'qualifier': 'commit_pylint'}}\n",
      "Missing attributes in for record: {'_id': '4d8148749c2dbc88891be6d697ec287eca8b4b72', 'relationships': {'qualifier': 'commit_pylint'}}\n",
      "Missing attributes in for record: {'_id': '0d002b4e79a6658fc7d3dbfe9a9cf9b25098352e', 'relationships': {'qualifier': 'commit_pylint'}}\n",
      "Missing attributes in for record: {'_id': '7fc73d0a9320eaa889a25d79d5c823c9c0b6a39f', 'relationships': {'qualifier': 'commit_pylint'}}\n",
      "Missing attributes in for record: {'_id': '9b2080e8165b3e2b1e1236586ee8b1e7e53ee5ba', 'relationships': {'qualifier': 'commit_pylint'}}\n",
      "Missing attributes in for record: {'_id': 'a66f953df1a52e8d13be92e4a5b3f4040495713d', 'relationships': {'qualifier': 'commit_pylint'}}\n",
      "Missing attributes in for record: {'_id': '93281ddbda230a599b409db98ea6050cca87e7a3', 'relationships': {'qualifier': 'commit_pylint'}}\n",
      "Missing attributes in for record: {'_id': '8f2f1c28f8b9d3ca50889218e6dd798fba55a9f3', 'relationships': {'qualifier': 'commit_pylint'}}\n",
      "Missing attributes in for record: {'_id': '61a3c478fc25d71f09b1f866565296c854b95e02', 'relationships': {'qualifier': 'commit_pylint'}}\n",
      "Missing attributes in for record: {'_id': '87dfab8065149b603270f67bb7e7f7f6a1b18ff9', 'relationships': {'qualifier': 'commit_pylint'}}\n",
      "Missing attributes in for record: {'_id': 'f2a3aa9543ccf9c865a35f71ae14e6ed4fec800f', 'relationships': {'qualifier': 'commit_pylint'}}\n",
      "Missing attributes in for record: {'_id': '7bd17fdfd1b8ae6f8ef65e3074ccba41baa1f7af', 'relationships': {'qualifier': 'commit_pylint'}}\n",
      "Missing attributes in for record: {'_id': '657cdfe81b018eaa5560cb8ff5b12a2ac7858986', 'relationships': {'qualifier': 'commit_pylint'}}\n",
      "Missing attributes in for record: {'_id': 'a2e4b4a1af11d36d3aad818b01d0dd24aeef0e53', 'relationships': {'qualifier': 'commit_pylint'}}\n",
      "Missing attributes in for record: {'_id': '6033225be22d0110aa8464e94c9942c5fb0ee771', 'relationships': {'qualifier': 'commit_pylint'}}\n",
      "Missing attributes in for record: {'_id': '0062ef6ef991210816b6ee54132d4eb0442b4236', 'relationships': {'qualifier': 'commit_pylint'}}\n",
      "Missing attributes in for record: {'_id': 'e48233c1f450e1e37fa402f3d3b55e2b64cb280c', 'relationships': {'qualifier': 'commit_pylint'}}\n",
      "Missing attributes in for record: {'_id': 'bb6e0810c614b3ff0264e8eb07b416513f05b653', 'relationships': {'qualifier': 'commit_pylint'}}\n",
      "Missing attributes in for record: {'_id': 'ce4bef9e0fa0ae80f6ffc73360c1924cf9aaa9c1', 'relationships': {'qualifier': 'commit_pylint'}}\n",
      "Missing attributes in for record: {'_id': '4099a531abbc4323a081451b24bf261326c4e709', 'relationships': {'qualifier': 'commit_pylint'}}\n",
      "Missing attributes in for record: {'_id': 'ab3f8f40faa315021d3e207abd8b6b64f999f1d5', 'relationships': {'qualifier': 'commit_pylint'}}\n",
      "Missing attributes in for record: {'_id': '5aa6accb8e4cac3a76c96ce38f0b7cf1f447e420', 'relationships': {'qualifier': 'commit_pylint'}}\n",
      "Missing attributes in for record: {'_id': '14ebb3647eefbf2e860a36cb040e0b582fe0f5ed', 'relationships': {'qualifier': 'commit_pylint'}}\n",
      "Missing attributes in for record: {'_id': 'e96767930e6ca605337f92fecad7f58abe57bf1e', 'relationships': {'qualifier': 'commit_pylint'}}\n",
      "Missing attributes in for record: {'_id': '0dd3af3cc87dd49bb90900f7e861d72c1f3a1683', 'relationships': {'qualifier': 'commit_pylint'}}\n",
      "Missing attributes in for record: {'_id': '67ee0df96055e0a5588fad31ca12642279b91075', 'relationships': {'qualifier': 'commit_pylint'}}\n",
      "Missing attributes in for record: {'_id': 'fb0d7fd6ac8aa82f4698f2301ffd9f825c5a40b7', 'relationships': {'qualifier': 'commit_pylint'}}\n",
      "Missing attributes in for record: {'_id': 'f1164c3ff993ef2e16cdb000c0622fa7cefaacb5', 'relationships': {'qualifier': 'commit_pylint'}}\n",
      "Missing attributes in for record: {'_id': 'ad4977b652e401d78aa08048f98393c5eceba494', 'relationships': {'qualifier': 'commit_pylint'}}\n",
      "Missing attributes in for record: {'_id': '0c27815fb06721d1b6560488beda3e8307c9a4d7', 'relationships': {'qualifier': 'commit_pylint'}}\n",
      "Missing attributes in for record: {'_id': '0e85c1e852859e6a7bf667bb26dbf9d51dbbf261', 'relationships': {'qualifier': 'commit_pylint'}}\n",
      "Missing attributes in for record: {'_id': '65ceb4e822c339a29f7a68c1063dde306f1bd5aa', 'relationships': {'qualifier': 'commit_pylint'}}\n",
      "Missing attributes in for record: {'_id': '537af644a5cd9f2625b132c370a88c8475fd741f', 'relationships': {'qualifier': 'commit_pylint'}}\n",
      "Missing attributes in for record: {'_id': '931ae8948cd8aecc3e1e2b7e7bb4ce8feeeb34a2', 'relationships': {'qualifier': 'commit_pylint'}}\n",
      "Missing attributes in for record: {'_id': 'a6ba5522fb2e32d04dac8044218f12132032e00a', 'relationships': {'qualifier': 'commit_pylint'}}\n",
      "Missing attributes in for record: {'_id': 'e7880e98ca46133fbeec79994c0c8f807cf29ba6', 'relationships': {'qualifier': 'commit_pylint'}}\n",
      "Missing attributes in for record: {'_id': '6f38fbfccef4de03aac2c1d7e35893185d018b36', 'relationships': {'qualifier': 'commit_pylint'}}\n",
      "Missing attributes in for record: {'_id': '65dd1b912704783aec6918f7d802dc80007d47c9', 'relationships': {'qualifier': 'commit_pylint'}}\n",
      "Missing attributes in for record: {'_id': 'b4077df149369e36f411f00eacbe212492e45bb3', 'relationships': {'qualifier': 'commit_pylint'}}\n",
      "Missing attributes in for record: {'_id': 'd5a8a9b24204b81cf173b3fdec85a2f8255d9baa', 'relationships': {'qualifier': 'commit_pylint'}}\n",
      "Missing attributes in for record: {'_id': 'ba7523ea1d0ea3a30d207fece7d9d3f81ef8060b', 'relationships': {'qualifier': 'commit_pylint'}}\n",
      "Missing attributes in for record: {'_id': '4064ac0ac3063b58e8392df984840df8253b6fbc', 'relationships': {'qualifier': 'commit_pylint'}}\n",
      "Missing attributes in for record: {'_id': 'f1a7ff4d8918894309d5ec9073eb01d7a8cd1ab6', 'relationships': {'qualifier': 'commit_pylint'}}\n",
      "Missing attributes in for record: {'_id': 'c74e73e3eda6f99f3cce396a308d285f6f068fa6', 'relationships': {'qualifier': 'commit_pylint'}}\n",
      "Missing attributes in for record: {'_id': '29105edc1d7ce3808b292e53ea1d426f3484cd2e', 'relationships': {'qualifier': 'commit_pylint'}}\n",
      "Missing attributes in for record: {'_id': 'bf5cfd574d1b2bcd3224dd93b83b38bc66007348', 'relationships': {'qualifier': 'commit_pylint'}}\n",
      "Missing attributes in for record: {'_id': '2b9cb6a5570e4c8d5165281595be78f470b3e3f3', 'relationships': {'qualifier': 'commit_pylint'}}\n",
      "Missing attributes in for record: {'_id': '1ddb88222ea30d4e4d01f99f4df810c8a7db26d3', 'relationships': {'qualifier': 'commit_pylint'}}\n",
      "Missing attributes in for record: {'_id': 'dc572fde263a51e4b05a2ea26cfc55a744cb63a2', 'relationships': {'qualifier': 'commit_pylint'}}\n",
      "Missing attributes in for record: {'_id': '362b83c002bfac8b789e2f82d5a6b27853f08209', 'relationships': {'qualifier': 'commit_pylint'}}\n",
      "Missing attributes in for record: {'_id': 'bf6296f6f39472bc0faa253fc151934299832177', 'relationships': {'qualifier': 'commit_pylint'}}\n",
      "Missing attributes in for record: {'_id': 'e359e0adaf44d1f582e75d933db252c3692b0598', 'relationships': {'qualifier': 'commit_pylint'}}\n",
      "Missing attributes in for record: {'_id': '73be66d126a59dbdbc85f4d1679663b1542a19c6', 'relationships': {'qualifier': 'commit_pylint'}}\n",
      "Missing attributes in for record: {'_id': 'f2c28b4b4bff4070582fdd1c87563e4a68601a69', 'relationships': {'qualifier': 'commit_pylint'}}\n",
      "Missing attributes in for record: {'_id': 'f4db040fffaee03e153c95b5c54430bc30da9b7e', 'relationships': {'qualifier': 'commit_pylint'}}\n",
      "Missing attributes in for record: {'_id': '76062e92fddf37c0daa0aee3f06ef4b05409ae78', 'relationships': {'qualifier': 'commit_pylint'}}\n",
      "Missing attributes in for record: {'_id': '71dcaef35f288c04e083f59f3df2fa7814579363', 'relationships': {'qualifier': 'commit_pylint'}}\n",
      "Missing attributes in for record: {'_id': '0b39f8c616f8cfb149572a3f15e470b313415271', 'relationships': {'qualifier': 'commit_pylint'}}\n",
      "Missing attributes in for record: {'_id': '70722b57a9bd13193593b37f53242936d94431cc', 'relationships': {'qualifier': 'commit_pylint'}}\n",
      "Missing attributes in for record: {'_id': '973e3674daab1ccb6dfbd94d5b13c131f8a258bc', 'relationships': {'qualifier': 'commit_pylint'}}\n",
      "Missing attributes in for record: {'_id': '2e426346ad523ec9af6dbf75a1d74be30d3908ca', 'relationships': {'qualifier': 'commit_pylint'}}\n",
      "Missing attributes in for record: {'_id': '731201ab4fd1c4ff30b0010460f9979ab5505a29', 'relationships': {'qualifier': 'commit_pylint'}}\n",
      "Missing attributes in for record: {'_id': '88f14b8b121337a82c5c70d68e03d28bb3ddac4d', 'relationships': {'qualifier': 'commit_pylint'}}\n",
      "Missing attributes in for record: {'_id': '7e60d1a1fae2b8387a90c7474209a61e83503376', 'relationships': {'qualifier': 'commit_pylint'}}\n",
      "Missing attributes in for record: {'_id': 'ff4d7e9f78f7a74538d3ade7e850f7416a770ce9', 'relationships': {'qualifier': 'commit_pylint'}}\n",
      "Missing attributes in for record: {'_id': 'cebefe928fd23e0461fcf998dca749f2aec25d8c', 'relationships': {'qualifier': 'commit_pylint'}}\n",
      "Missing attributes in for record: {'_id': 'f415013aeeac86ddd5fb97c7a8e39b3a77ba12ec', 'relationships': {'qualifier': 'commit_pylint'}}\n",
      "Missing attributes in for record: {'_id': '48d058d29c7b6db98e196832137c69cd61e98968', 'relationships': {'qualifier': 'commit_pylint'}}\n",
      "Missing attributes in for record: {'_id': 'fd104cc26df1420ac2707e5db27f93cd950bb8ef', 'relationships': {'qualifier': 'commit_pylint'}}\n",
      "Missing attributes in for record: {'_id': '078b93cae58db0da361f33d6a4e62c61477f0fef', 'relationships': {'qualifier': 'commit_pylint'}}\n",
      "Missing attributes in for record: {'_id': '3670c6ce485967fd099a5862a5d7eca0ab5bde01', 'relationships': {'qualifier': 'commit_pylint'}}\n",
      "Missing attributes in for record: {'_id': '2b60122a85fe8fa29514882e12af006f6cc4ceb5', 'relationships': {'qualifier': 'commit_pylint'}}\n",
      "Missing attributes in for record: {'_id': '12848699494a28f76c34982ef872a824fb32036d', 'relationships': {'qualifier': 'commit_pylint'}}\n",
      "Missing attributes in for record: {'_id': '8eea5e0838e87f843ee2617b39917e06e0050d26', 'relationships': {'qualifier': 'commit_pylint'}}\n",
      "Missing attributes in for record: {'_id': 'b8b1bb497e266f586e8b0ad50a324d9704b0be9f', 'relationships': {'qualifier': 'commit_pylint'}}\n",
      "Missing attributes in for record: {'_id': 'fa2c02df0acf2d4fb807161b683970c2c9d38d26', 'relationships': {'qualifier': 'commit_pylint'}}\n",
      "Missing attributes in for record: {'_id': 'b3af62273e0a9bb9fa1dfcdd5132006a22dbd36e', 'relationships': {'qualifier': 'commit_pylint'}}\n",
      "Missing attributes in for record: {'_id': '4b483740be309b68e842397b0cbb69753a0dea7b', 'relationships': {'qualifier': 'commit_pylint'}}\n",
      "Missing attributes in for record: {'_id': '7ea5b50aa41ef40d111fc9260e11d50e4b5e8930', 'relationships': {'qualifier': 'commit_pylint'}}\n",
      "Missing attributes in for record: {'_id': 'f72b58eb67965d5b4610c5b21f1a4ae9d59c1efb', 'relationships': {'qualifier': 'commit_pylint'}}\n",
      "Missing attributes in for record: {'_id': 'bc37099b67868a928132f63b6e2ad0e56e5c9b47', 'relationships': {'qualifier': 'commit_pylint'}}\n",
      "Missing attributes in for record: {'_id': 'bc5004c96a45b00850aad311678665e7b7576427', 'relationships': {'qualifier': 'commit_pylint'}}\n",
      "Missing attributes in for record: {'_id': 'a8a0935bb1bc93441fd8153cbb8960a0e79a400a', 'relationships': {'qualifier': 'commit_pylint'}}\n",
      "Missing attributes in for record: {'_id': '282aff8daa3dd6e509ea6592c2eef87376801ef7', 'relationships': {'qualifier': 'commit_pylint'}}\n",
      "Missing attributes in for record: {'_id': 'aaca47af7534970c1d20e7b19a5ec493172e5a2d', 'relationships': {'qualifier': 'commit_pylint'}}\n",
      "Missing attributes in for record: {'_id': '1b09afcf548e2783fe0635093c39f0e675e684a8', 'relationships': {'qualifier': 'commit_pylint'}}\n",
      "Missing attributes in for record: {'_id': '0224dec3a44db45105035a2cf48cf7302a138d89', 'relationships': {'qualifier': 'commit_pylint'}}\n",
      "Missing attributes in for record: {'_id': '8ac9c1298baf3508dbf99126c3e27d1e3b7e45da', 'relationships': {'qualifier': 'commit_pylint'}}\n",
      "Missing attributes in for record: {'_id': 'a6c58e0dc4daeb36bc2d410cfc26ad2786a591bd', 'relationships': {'qualifier': 'commit_pylint'}}\n",
      "Missing attributes in for record: {'_id': 'cc6ef2a0d469f840109183e5d2454a24949cafb5', 'relationships': {'qualifier': 'commit_pylint'}}\n",
      "Missing attributes in for record: {'_id': 'cb963e61d6f17f72f928267632be78933e29c039', 'relationships': {'qualifier': 'commit_pylint'}}\n",
      "Missing attributes in for record: {'_id': '615354ecada74e48925ea7f89bc1652e7d06b008', 'relationships': {'qualifier': 'commit_pylint'}}\n",
      "Missing attributes in for record: {'_id': '6a4161d1cd73ac976c7542dc97997a7cef6beef0', 'relationships': {'qualifier': 'commit_pylint'}}\n",
      "Missing attributes in for record: {'_id': '3621cf8f138b26adf8d28ad8326b148017ce701c', 'relationships': {'qualifier': 'commit_pylint'}}\n",
      "Missing attributes in for record: {'_id': '2311e9e5120602821baa535ec58becde914b256e', 'relationships': {'qualifier': 'commit_pylint'}}\n",
      "Missing attributes in for record: {'_id': '10201b889a2282e57f59678263ca57b45b552f8a', 'relationships': {'qualifier': 'commit_pylint'}}\n",
      "Missing attributes in for record: {'_id': 'b5566d0ddecf6981b76aeb838b2a3921b06be352', 'relationships': {'qualifier': 'commit_pylint'}}\n",
      "Missing attributes in for record: {'_id': '946492bdeaebc9223b71fec4fd416891747867e2', 'relationships': {'qualifier': 'commit_pylint'}}\n",
      "Missing attributes in for record: {'_id': '732a920c7e900d90355f99f1add2d55173819e41', 'relationships': {'qualifier': 'commit_pylint'}}\n",
      "Missing attributes in for record: {'_id': 'cab845e52531a83a4c6c36a95444436f2500cd53', 'relationships': {'qualifier': 'commit_pylint'}}\n",
      "[]\n"
     ]
    },
    {
     "ename": "ValueError",
     "evalue": "not enough values to unpack (expected 2, got 0)",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mValueError\u001b[0m                                Traceback (most recent call last)",
      "Cell \u001b[0;32mIn[6], line 40\u001b[0m\n\u001b[1;32m     38\u001b[0m \u001b[38;5;28mprint\u001b[39m(commit_pylints)\n\u001b[1;32m     39\u001b[0m sorted_pylints \u001b[38;5;241m=\u001b[39m \u001b[38;5;28msorted\u001b[39m(\u001b[38;5;28mzip\u001b[39m(commit_dates, commit_pylints), key\u001b[38;5;241m=\u001b[39m\u001b[38;5;28;01mlambda\u001b[39;00m x: x[\u001b[38;5;241m0\u001b[39m])\n\u001b[0;32m---> 40\u001b[0m commit_dates, commit_pylints \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mzip\u001b[39m(\u001b[38;5;241m*\u001b[39msorted_pylints)\n\u001b[1;32m     42\u001b[0m date_str \u001b[38;5;241m=\u001b[39m \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124m2020-06-02T19:45:54.000+00:00\u001b[39m\u001b[38;5;124m\"\u001b[39m\n\u001b[1;32m     43\u001b[0m date_obj_1 \u001b[38;5;241m=\u001b[39m datetime\u001b[38;5;241m.\u001b[39mfromisoformat(date_str\u001b[38;5;241m.\u001b[39mreplace(\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mZ\u001b[39m\u001b[38;5;124m\"\u001b[39m, \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124m+00:00\u001b[39m\u001b[38;5;124m\"\u001b[39m))\n",
      "\u001b[0;31mValueError\u001b[0m: not enough values to unpack (expected 2, got 0)"
     ]
    }
   ],
   "source": [
    "import pymongo\n",
    "# Connect to MongoDB\n",
    "myclient = pymongo.MongoClient(\"mongodb://localhost:27017/\")\n",
    "\n",
    "# Define the repository parameters\n",
    "repo_path = f\"/Users/as/Library/Mobile Documents/com~apple~CloudDocs/Dokumente/Studium/Bachelor-Thesis/tmp/{repo_name}\"\n",
    "\n",
    "\n",
    "ocdb = myclient[\"OCEL\"]\n",
    "from matplotlib import pyplot as plt\n",
    "import numpy as np\n",
    "collection = ocdb[repo_name]\n",
    "# Query the data\n",
    "data = collection.find({\"type\": \"commit\"}, {\"relationships.qualifier\": \"commit_pylint\" }).sort(\"attributes.time\", pymongo.ASCENDING)\n",
    "\n",
    "# Extract commit_dates and maintainability indices\n",
    "commit_dates = []\n",
    "commit_pylints = []\n",
    "\n",
    "def get_attribute_value(attributes, name):\n",
    "    for attr in attributes:\n",
    "        if attr.get(\"name\") == name:\n",
    "            return attr.get(\"value\")\n",
    "\n",
    "def get_related_objects(relationships, qualifier):\n",
    "    related_object_ids = []\n",
    "    for rel in relationships:\n",
    "        if rel.get(\"qualifier\") == qualifier:\n",
    "            related_object_ids.append(rel.get(\"object\")) \n",
    "    return related_object_ids if related_object_ids != [] else None\n",
    "\n",
    "for record in data:\n",
    "    if \"attributes\" in record and \"relationships\" in record:\n",
    "        commit_dates.append(get_attribute_value(record[\"attributes\"], \"message\"))\n",
    "        commit_pylints.append(get_related_objects(record[\"relationships\"], \"commit_pylint\"))\n",
    "    else:\n",
    "        print(f\"Missing attributes in for record: {record}\")\n",
    "print(commit_pylints)\n",
    "sorted_pylints = sorted(zip(commit_dates, commit_pylints), key=lambda x: x[0])\n",
    "commit_dates, commit_pylints = zip(*sorted_pylints)\n",
    "\n",
    "date_str = \"2020-06-02T19:45:54.000+00:00\"\n",
    "date_obj_1 = datetime.fromisoformat(date_str.replace(\"Z\", \"+00:00\"))\n",
    "date_obj_2 = datetime.fromisoformat(date_str.replace(\"020\", \"024\"))\n",
    "dt = np.array([date_obj_1, date_obj_2])\n",
    "# Plot the graph\n",
    "plt.figure(figsize=(12, 6))\n",
    "plt.plot(dt, [0, 1], label=\"Start\", color=\"green\", marker=\"o\")\n",
    "plt.plot(commit_dates, commit_pylints, label=\"Pylint Score\", color=\"red\", marker=\"x\", linestyle=\"-\")\n",
    "plt.xlabel(\"Commit Date\")\n",
    "plt.ylabel(\"Code Quality Score\")\n",
    "plt.title(f\"Code Quality Metrics Over Time for {repo_name}\")\n",
    "plt.rcParams['axes.spines.top'] = False\n",
    "plt.rcParams['axes.spines.right'] = False\n",
    "plt.ylim(0, 1.1)\n",
    "plt.grid()\n",
    "plt.legend()\n",
    "plt.xticks(rotation=90)\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "from os import read\n",
    "import pymongo\n",
    "import ast\n",
    "from build.utils import write_to_file\n",
    "# Connect to MongoDB\n",
    "myclient = pymongo.MongoClient(\"mongodb://localhost:27017/\")\n",
    "ocdb = myclient[\"OCEL\"]\n",
    "from build.database_handler import get_ocel_data\n",
    "\n",
    "ocel_path = get_ocel_data()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [
    {
     "ename": "KeyError",
     "evalue": "'ocel:timestamp'",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mKeyError\u001b[0m                                  Traceback (most recent call last)",
      "\u001b[0;32m/var/folders/f_/ghr23kc12zz2478m3gb_wcq00000gn/T/ipykernel_3090/3127420010.py\u001b[0m in \u001b[0;36m?\u001b[0;34m()\u001b[0m\n\u001b[1;32m     22\u001b[0m \u001b[0;32mimport\u001b[0m \u001b[0mpm4py\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     23\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     24\u001b[0m \u001b[0;31m# 1. Load OCEL from JSON file\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     25\u001b[0m \u001b[0;32mfrom\u001b[0m \u001b[0mpm4py\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mobjects\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mocel\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mimporter\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mjsonocel\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mvariants\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mocel20_standard\u001b[0m \u001b[0;32mimport\u001b[0m \u001b[0mapply\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 26\u001b[0;31m \u001b[0mocel\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mapply\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m\"Exports/OCEL-Data-Part1.json\"\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     27\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     28\u001b[0m \u001b[0;31m# 2. Discover the object-centric Petri net\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     29\u001b[0m \u001b[0;32mfrom\u001b[0m \u001b[0mpm4py\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0malgo\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mdiscovery\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mocel\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mocpn\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0malgorithm\u001b[0m \u001b[0;32mimport\u001b[0m \u001b[0mapply\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m/Library/Frameworks/Python.framework/Versions/3.12/lib/python3.12/site-packages/pm4py/objects/ocel/importer/jsonocel/variants/ocel20_standard.py\u001b[0m in \u001b[0;36m?\u001b[0;34m(file_path, parameters)\u001b[0m\n\u001b[1;32m     91\u001b[0m     \u001b[0mlegacy_obj\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;34m\"ocel:global-log\"\u001b[0m\u001b[0;34m]\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0;34m{\u001b[0m\u001b[0;34m}\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     92\u001b[0m     \u001b[0mlegacy_obj\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;34m\"ocel:global-event\"\u001b[0m\u001b[0;34m]\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0;34m{\u001b[0m\u001b[0;34m}\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     93\u001b[0m     \u001b[0mlegacy_obj\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;34m\"ocel:global-object\"\u001b[0m\u001b[0;34m]\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0;34m{\u001b[0m\u001b[0;34m}\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     94\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 95\u001b[0;31m     \u001b[0mlog\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mclassic\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mget_base_ocel\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mlegacy_obj\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mparameters\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mparameters\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     96\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     97\u001b[0m     \u001b[0mlog\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mocel_consistency\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mapply\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mlog\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mparameters\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mparameters\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     98\u001b[0m     \u001b[0mlog\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mfiltering_utils\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mpropagate_relations_filtering\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mlog\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mparameters\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mparameters\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m/Library/Frameworks/Python.framework/Versions/3.12/lib/python3.12/site-packages/pm4py/objects/ocel/importer/jsonocel/variants/classic.py\u001b[0m in \u001b[0;36m?\u001b[0;34m(json_obj, parameters)\u001b[0m\n\u001b[1;32m    102\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    103\u001b[0m     \u001b[0mevents\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mpandas_utils\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0minsert_index\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mevents\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0minternal_index\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mreset_index\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;32mFalse\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mcopy_dataframe\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;32mFalse\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    104\u001b[0m     \u001b[0mrelations\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mpandas_utils\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0minsert_index\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mrelations\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0minternal_index\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mreset_index\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;32mFalse\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mcopy_dataframe\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;32mFalse\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    105\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 106\u001b[0;31m     \u001b[0mevents\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mevents\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0msort_values\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0mevent_timestamp\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0minternal_index\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    107\u001b[0m     \u001b[0mrelations\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mrelations\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0msort_values\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0mevent_timestamp\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0minternal_index\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    108\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    109\u001b[0m     \u001b[0;32mdel\u001b[0m \u001b[0mevents\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0minternal_index\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m/Library/Frameworks/Python.framework/Versions/3.12/lib/python3.12/site-packages/pandas/core/frame.py\u001b[0m in \u001b[0;36m?\u001b[0;34m(self, by, axis, ascending, inplace, kind, na_position, ignore_index, key)\u001b[0m\n\u001b[1;32m   6923\u001b[0m                 \u001b[0;34mf\"\u001b[0m\u001b[0;34mLength of ascending (\u001b[0m\u001b[0;34m{\u001b[0m\u001b[0mlen\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mascending\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m}\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\"\u001b[0m  \u001b[0;31m# type: ignore[arg-type]\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   6924\u001b[0m                 \u001b[0;34mf\"\u001b[0m\u001b[0;34m != length of by (\u001b[0m\u001b[0;34m{\u001b[0m\u001b[0mlen\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mby\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m}\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\"\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   6925\u001b[0m             \u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   6926\u001b[0m         \u001b[0;32mif\u001b[0m \u001b[0mlen\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mby\u001b[0m\u001b[0;34m)\u001b[0m \u001b[0;34m>\u001b[0m \u001b[0;36m1\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m-> 6927\u001b[0;31m             \u001b[0mkeys\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0;34m[\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_get_label_or_level_values\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mx\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0maxis\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0maxis\u001b[0m\u001b[0;34m)\u001b[0m \u001b[0;32mfor\u001b[0m \u001b[0mx\u001b[0m \u001b[0;32min\u001b[0m \u001b[0mby\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m   6928\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   6929\u001b[0m             \u001b[0;31m# need to rewrap columns in Series to apply key function\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   6930\u001b[0m             \u001b[0;32mif\u001b[0m \u001b[0mkey\u001b[0m \u001b[0;32mis\u001b[0m \u001b[0;32mnot\u001b[0m \u001b[0;32mNone\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m/Library/Frameworks/Python.framework/Versions/3.12/lib/python3.12/site-packages/pandas/core/generic.py\u001b[0m in \u001b[0;36m?\u001b[0;34m(self, key, axis)\u001b[0m\n\u001b[1;32m   1840\u001b[0m             \u001b[0mvalues\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mxs\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mkey\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0maxis\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mother_axes\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;36m0\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_values\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1841\u001b[0m         \u001b[0;32melif\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_is_level_reference\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mkey\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0maxis\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0maxis\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1842\u001b[0m             \u001b[0mvalues\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0maxes\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0maxis\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mget_level_values\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mkey\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_values\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1843\u001b[0m         \u001b[0;32melse\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m-> 1844\u001b[0;31m             \u001b[0;32mraise\u001b[0m \u001b[0mKeyError\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mkey\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m   1845\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1846\u001b[0m         \u001b[0;31m# Check for duplicates\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1847\u001b[0m         \u001b[0;32mif\u001b[0m \u001b[0mvalues\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mndim\u001b[0m \u001b[0;34m>\u001b[0m \u001b[0;36m1\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;31mKeyError\u001b[0m: 'ocel:timestamp'"
     ]
    }
   ],
   "source": [
    "import json\n",
    "\n",
    "# Attempt to fix the JSON file if it uses single quotes\n",
    "with open(\"Exports/OCEL-Data-Part1.json\", \"r\") as f:\n",
    "\tcontent = f.read()\n",
    "\n",
    "try:\n",
    "\t# Try to load as JSON directly\n",
    "\tdata = json.loads(content)\n",
    "except json.JSONDecodeError:\n",
    "\t# If it fails, try to eval as Python dict and dump as JSON\n",
    "\timport ast\n",
    "\tdata = ast.literal_eval(content)\n",
    "\twith open(\"Exports/OCEL-Data-Part1.json\", \"w\") as f:\n",
    "\t\tjson.dump(data, f, indent=2)\n",
    "\n",
    "from build.utils import validate_json\n",
    "validate_json(\"Exports/OCEL-Data-Part1.json\", \"Data-Schemes/OCEL-Schema.json\")\n",
    "\n",
    "\n",
    "# filepath: test.ipynb\n",
    "import pm4py\n",
    "\n",
    "# 1. Load OCEL from JSON file\n",
    "from pm4py.objects.ocel.importer.jsonocel.variants.ocel20_standard import apply\n",
    "ocel = apply(\"Exports/OCEL-Data-Part1.json\")\n",
    "\n",
    "# 2. Discover the object-centric Petri net\n",
    "from pm4py.algo.discovery.ocel.ocpn.algorithm import apply \n",
    "ocpn = apply(ocel)\n",
    "\n",
    "# 3. Visualize the object-centric Petri net\n",
    "from pm4py.visualization.ocel.ocpn import visualizer\n",
    "gviz = visualizer.apply(ocpn)\n",
    "visualizer.view(gviz)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import json\n",
    "from datetime import datetime\n",
    "from os import write\n",
    "\n",
    "with open(\"Exports/OCEL-Data.json\") as f:\n",
    "    ocel = json.load(f)\n",
    "\n",
    "# Define your split date(s)\n",
    "split_date = datetime.fromisoformat(\"2023-01-01T00:00:00\").replace(tzinfo=None)  # Ensure timezone-naive datetime\n",
    "\n",
    "# Split events\n",
    "events_part1 = [e for e in ocel[\"events\"] if datetime.fromisoformat(e[\"time\"]).replace(tzinfo=None) < split_date]\n",
    "events_part2 = [e for e in ocel[\"events\"] if datetime.fromisoformat(e[\"time\"]).replace(tzinfo=None) >= split_date]\n",
    "\n",
    "# (Optional) Get referenced object IDs for each part\n",
    "def get_referenced_objects(events):\n",
    "    obj_ids = set()\n",
    "    for e in events:\n",
    "        # Each relationship is a dict, e.g. {\"object\": \"id\", ...}\n",
    "        for rel in e[\"relationships\"]:\n",
    "            obj_ids.add(rel[\"objectId\"])  # or the correct key for the object ID\n",
    "    return obj_ids\n",
    "\n",
    "obj_ids_part1 = get_referenced_objects(events_part1)\n",
    "obj_ids_part2 = get_referenced_objects(events_part2)\n",
    "\n",
    "# (Optional) Filter objects for each part\n",
    "objects_part1 = [o for o in ocel[\"objects\"] if o[\"id\"] in obj_ids_part1]\n",
    "objects_part2 = [o for o in ocel[\"objects\"] if o[\"id\"] in obj_ids_part2]\n",
    "\n",
    "# Build new OCEL logs\n",
    "ocel_part1 = {**ocel, \"events\": events_part1, \"objects\": objects_part1}\n",
    "ocel_part2 = {**ocel, \"events\": events_part2, \"objects\": objects_part2}\n",
    "\n",
    "write_to_file(\"Exports/OCEL-Data-Part1.json\", json.dumps(ocel_part1, indent=2))\n",
    "write_to_file(\"Exports/OCEL-Data-Part2.json\", json.dumps(ocel_part2, indent=2))\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [
    {
     "ename": "AttributeError",
     "evalue": "'dict' object has no attribute 'event_activity'",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mAttributeError\u001b[0m                            Traceback (most recent call last)",
      "Cell \u001b[0;32mIn[17], line 4\u001b[0m\n\u001b[1;32m      1\u001b[0m \u001b[38;5;28;01mfrom\u001b[39;00m \u001b[38;5;21;01mpm4py\u001b[39;00m \u001b[38;5;28;01mimport\u001b[39;00m ocel_flattening \u001b[38;5;28;01mas\u001b[39;00m flatten\n\u001b[1;32m      3\u001b[0m \u001b[38;5;66;03m# ocel: your loaded OCEL log\u001b[39;00m\n\u001b[0;32m----> 4\u001b[0m user_log \u001b[38;5;241m=\u001b[39m \u001b[43mflatten\u001b[49m\u001b[43m(\u001b[49m\u001b[43mocel\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[38;5;124;43muser\u001b[39;49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[43m)\u001b[49m\n\u001b[1;32m      5\u001b[0m write_to_file(\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mExports/OCEL-User-Log.json\u001b[39m\u001b[38;5;124m\"\u001b[39m, json\u001b[38;5;241m.\u001b[39mdumps(user_log, indent\u001b[38;5;241m=\u001b[39m\u001b[38;5;241m2\u001b[39m))\n",
      "File \u001b[0;32m/Library/Frameworks/Python.framework/Versions/3.12/lib/python3.12/site-packages/pm4py/ocel.py:87\u001b[0m, in \u001b[0;36mocel_flattening\u001b[0;34m(ocel, object_type)\u001b[0m\n\u001b[1;32m     67\u001b[0m \u001b[38;5;250m\u001b[39m\u001b[38;5;124;03m\"\"\"\u001b[39;00m\n\u001b[1;32m     68\u001b[0m \u001b[38;5;124;03mFlattens the object-centric event log to a traditional event log with the choice of an object type.\u001b[39;00m\n\u001b[1;32m     69\u001b[0m \u001b[38;5;124;03mIn the flattened log, the objects of a given object type are the cases, and each case\u001b[39;00m\n\u001b[0;32m   (...)\u001b[0m\n\u001b[1;32m     84\u001b[0m \u001b[38;5;124;03m    event_log = pm4py.ocel_flattening(ocel, 'items')\u001b[39;00m\n\u001b[1;32m     85\u001b[0m \u001b[38;5;124;03m\"\"\"\u001b[39;00m\n\u001b[1;32m     86\u001b[0m \u001b[38;5;28;01mfrom\u001b[39;00m \u001b[38;5;21;01mpm4py\u001b[39;00m\u001b[38;5;21;01m.\u001b[39;00m\u001b[38;5;21;01mobjects\u001b[39;00m\u001b[38;5;21;01m.\u001b[39;00m\u001b[38;5;21;01mocel\u001b[39;00m\u001b[38;5;21;01m.\u001b[39;00m\u001b[38;5;21;01mutil\u001b[39;00m \u001b[38;5;28;01mimport\u001b[39;00m flattening\n\u001b[0;32m---> 87\u001b[0m \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[43mflattening\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mflatten\u001b[49m\u001b[43m(\u001b[49m\u001b[43mocel\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mobject_type\u001b[49m\u001b[43m)\u001b[49m\n",
      "File \u001b[0;32m/Library/Frameworks/Python.framework/Versions/3.12/lib/python3.12/site-packages/pm4py/objects/ocel/util/flattening.py:59\u001b[0m, in \u001b[0;36mflatten\u001b[0;34m(ocel, ot, parameters)\u001b[0m\n\u001b[1;32m     55\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m parameters \u001b[38;5;129;01mis\u001b[39;00m \u001b[38;5;28;01mNone\u001b[39;00m:\n\u001b[1;32m     56\u001b[0m     parameters \u001b[38;5;241m=\u001b[39m {}\n\u001b[1;32m     58\u001b[0m event_activity \u001b[38;5;241m=\u001b[39m exec_utils\u001b[38;5;241m.\u001b[39mget_param_value(Parameters\u001b[38;5;241m.\u001b[39mEVENT_ACTIVITY, parameters,\n\u001b[0;32m---> 59\u001b[0m                                             \u001b[43mocel\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mevent_activity\u001b[49m)\n\u001b[1;32m     60\u001b[0m event_timestamp \u001b[38;5;241m=\u001b[39m exec_utils\u001b[38;5;241m.\u001b[39mget_param_value(Parameters\u001b[38;5;241m.\u001b[39mEVENT_TIMESTAMP, parameters,\n\u001b[1;32m     61\u001b[0m                                              ocel\u001b[38;5;241m.\u001b[39mevent_timestamp)\n\u001b[1;32m     63\u001b[0m objects \u001b[38;5;241m=\u001b[39m ocel\u001b[38;5;241m.\u001b[39mobjects[ocel\u001b[38;5;241m.\u001b[39mobjects[ocel\u001b[38;5;241m.\u001b[39mobject_type_column] \u001b[38;5;241m==\u001b[39m ot]\n",
      "\u001b[0;31mAttributeError\u001b[0m: 'dict' object has no attribute 'event_activity'"
     ]
    }
   ],
   "source": [
    "from pm4py import ocel_flattening as flatten\n",
    "\n",
    "# ocel: your loaded OCEL log\n",
    "user_log = flatten(ocel, \"user\")\n",
    "write_to_file(\"Exports/OCEL-User-Log.json\", json.dumps(user_log, indent=2))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "print(\"Hello Pylint\")\n",
      "def hello_pylint():\n",
      "    # A simple function to demonstrate Pylint functionality.\n",
      "    \n",
      "    print(\"Hello, Pylint!\")\n",
      "    return \"Hello, Pylint!\"\n",
      "\n",
      "hello_pylint().replace(\"Pylint\", \"Code Quality\")  # Example of a string operation\n",
      "\n",
      "PL Score: 4.0\n"
     ]
    }
   ],
   "source": [
    "f = get_pylint_score(\"\"\"\n",
    "print(\"Hello Pylint\")\n",
    "def hello_pylint():\n",
    "    # A simple function to demonstrate Pylint functionality.\n",
    "    \n",
    "    print(\"Hello, Pylint!\")\n",
    "    return \"Hello, Pylint!\"\n",
    "\n",
    "hello_pylint().replace(\"Pylint\", \"Code Quality\")  # Example of a string operation\n",
    "\"\"\")\n",
    "print(f\"PL Score: {f}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "image/png": "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",
      "text/plain": [
       "<Figure size 1200x600 with 1 Axes>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "import pymongo\n",
    "import matplotlib.pyplot as plt\n",
    "import datetime\n",
    "\n",
    "myclient = pymongo.MongoClient(\"mongodb://localhost:27017/\")\n",
    "ocdb = myclient[\"OCEL\"]\n",
    "\n",
    "def get_object(id: str):\n",
    "    \"\"\"\n",
    "    Get an object from the database by its ID.\n",
    "    Args:\n",
    "        id (str): The ID of the object to retrieve.\n",
    "    Returns:\n",
    "        dict: The object data if found, otherwise None.\n",
    "    \"\"\"\n",
    "    return ocdb[\"ocel:objects\"].find({\"_id\": id})[0]\n",
    "\n",
    "def plot_file_code_quality(file_id):\n",
    "    \"\"\"\n",
    "    Plot the code quality metrics for a specific file in the repository\n",
    "    Args:\n",
    "        repo_name (str): The name of the repository of the file file belongs to\n",
    "        file_id (str): The ID of the file to plot the code quality metrics for\n",
    "    \"\"\"\n",
    "    file = get_object(file_id)\n",
    "    mis = {}\n",
    "    pylint_scores = {}\n",
    "    for attribute in file[\"attributes\"]:\n",
    "        if attribute[\"name\"] == \"pylint_score\":\n",
    "            pylint_scores[datetime.datetime.fromisoformat(attribute[\"time\"]).replace(tzinfo=None)] = float(attribute[\"value\"])\n",
    "        if attribute[\"name\"] == \"loc\":\n",
    "            mis[datetime.datetime.fromisoformat(attribute[\"time\"]).replace(tzinfo=None)] = int(attribute[\"value\"])\n",
    "    commit_dates = sorted(mis.keys())\n",
    "    maintainability_indices = [mis[date] for date in commit_dates]\n",
    "    plt.figure(figsize=(12, 6))\n",
    "    # plt.plot(commit_dates, maintainability_indices, label=\"Maintainability Index\", color=\"blue\", marker=\"o\", linestyle=\"-\")\n",
    "    plt.plot(commit_dates, [pylint_scores.get(date, 0) for date in commit_dates], label=\"Pylint Score\", color=\"red\", marker=\"x\", linestyle=\"--\")\n",
    "    plt.xlabel(\"Commit Date\")\n",
    "    plt.ylabel(\"Code Quality Score\")\n",
    "    plt.title(f\"Code Quality Metrics Over Time for {file['_id']}\")\n",
    "    plt.xticks(rotation=90)\n",
    "    plt.rcParams['axes.spines.top'] = False\n",
    "    plt.rcParams['axes.spines.right'] = False\n",
    "    plt.ylim(0, 1.1)\n",
    "    plt.legend()\n",
    "    plt.savefig(f\"Exports/{file['_id']}_code_quality.png\")\n",
    "\n",
    "plot_file_code_quality(\"streamlit_app.py\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [],
   "source": [
    "from re import search\n",
    "from attr import attrib\n",
    "\n",
    "\n",
    "def get_attribute_value_at_time(file_id, attribute_name, searched_time):\n",
    "    \"\"\"\n",
    "    Get the value of an attribute at a specific time.\n",
    "    Args:\n",
    "        file_id (str): The ID of the file to get the attribute value for.\n",
    "        attribute_name (str): The name of the attribute to get the value for.\n",
    "        time (datetime): The time to get the attribute value at.\n",
    "    Returns:\n",
    "        str: The value of the attribute at the specified time, or None if not found.\n",
    "    \"\"\"\n",
    "    file = get_object(file_id)\n",
    "    time = datetime.datetime.fromisoformat(searched_time).replace(tzinfo=None)\n",
    "    attributes = {}\n",
    "    for attribute in file[\"attributes\"]:\n",
    "        attr_time = datetime.datetime.fromisoformat(attribute[\"time\"]).replace(tzinfo=None)\n",
    "        if attribute[\"name\"] == attribute_name and attr_time <= time:\n",
    "            attributes[attr_time] = attribute[\"value\"]\n",
    "    return attributes.get(max(attributes.keys(), default=None), None) if attributes else None\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_related_objects(file_id, qualifier):\n",
    "    \"\"\"\n",
    "    Get the related objects of a file based on a qualifier.\n",
    "    Args:\n",
    "        file_id (str): The ID of the file to get the related objects for.\n",
    "        qualifier (str): The qualifier to filter the related objects by.\n",
    "    Returns:\n",
    "        list: A list of related object IDs.\n",
    "    \"\"\"\n",
    "    file = get_object(file_id)\n",
    "    related_objects = []\n",
    "    for relation in file[\"relationships\"]:\n",
    "        if relation[\"qualifier\"] == qualifier:\n",
    "            related_objects.append(relation[\"objectId\"])\n",
    "    return related_objects "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [
    {
     "ename": "IndexError",
     "evalue": "no such item for Cursor instance",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mIndexError\u001b[0m                                Traceback (most recent call last)",
      "Cell \u001b[0;32mIn[13], line 53\u001b[0m\n\u001b[1;32m     50\u001b[0m     plt\u001b[38;5;241m.\u001b[39mlegend()\n\u001b[1;32m     51\u001b[0m     plt\u001b[38;5;241m.\u001b[39msavefig(\u001b[38;5;124mf\u001b[39m\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mExports/CQ-\u001b[39m\u001b[38;5;132;01m{\u001b[39;00mpull_request[\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124m_id\u001b[39m\u001b[38;5;124m\"\u001b[39m]\u001b[38;5;132;01m}\u001b[39;00m\u001b[38;5;124m.png\u001b[39m\u001b[38;5;124m\"\u001b[39m)\n\u001b[0;32m---> 53\u001b[0m \u001b[43mplot_pull_request_code_quality\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[38;5;124;43m373\u001b[39;49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[43m)\u001b[49m\n",
      "Cell \u001b[0;32mIn[13], line 22\u001b[0m, in \u001b[0;36mplot_pull_request_code_quality\u001b[0;34m(pull_request_id)\u001b[0m\n\u001b[1;32m     20\u001b[0m     commit \u001b[38;5;241m=\u001b[39m get_object(relationship[\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mqualifier\u001b[39m\u001b[38;5;124m\"\u001b[39m])\n\u001b[1;32m     21\u001b[0m \u001b[38;5;28;01melif\u001b[39;00m relationship[\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mqualifier\u001b[39m\u001b[38;5;124m\"\u001b[39m] \u001b[38;5;241m==\u001b[39m \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124maggregates\u001b[39m\u001b[38;5;124m\"\u001b[39m:\n\u001b[0;32m---> 22\u001b[0m     file_object \u001b[38;5;241m=\u001b[39m \u001b[43mget_object\u001b[49m\u001b[43m(\u001b[49m\u001b[43mrelationship\u001b[49m\u001b[43m[\u001b[49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[38;5;124;43mqualifier\u001b[39;49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[43m]\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m     23\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m commit \u001b[38;5;129;01mis\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m \u001b[38;5;28;01mNone\u001b[39;00m \u001b[38;5;129;01mand\u001b[39;00m \u001b[38;5;28mlen\u001b[39m(commit[\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mattributes\u001b[39m\u001b[38;5;124m\"\u001b[39m]) \u001b[38;5;241m>\u001b[39m \u001b[38;5;241m0\u001b[39m:\n\u001b[1;32m     24\u001b[0m     commit_time \u001b[38;5;241m=\u001b[39m datetime\u001b[38;5;241m.\u001b[39mdatetime\u001b[38;5;241m.\u001b[39mfromisoformat(commit[\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mattributes\u001b[39m\u001b[38;5;124m\"\u001b[39m][\u001b[38;5;241m0\u001b[39m][\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mtime\u001b[39m\u001b[38;5;124m\"\u001b[39m])\u001b[38;5;241m.\u001b[39mreplace(tzinfo\u001b[38;5;241m=\u001b[39m\u001b[38;5;28;01mNone\u001b[39;00m)\n",
      "Cell \u001b[0;32mIn[10], line 16\u001b[0m, in \u001b[0;36mget_object\u001b[0;34m(id)\u001b[0m\n\u001b[1;32m      8\u001b[0m \u001b[38;5;28;01mdef\u001b[39;00m \u001b[38;5;21mget_object\u001b[39m(\u001b[38;5;28mid\u001b[39m: \u001b[38;5;28mstr\u001b[39m):\n\u001b[1;32m      9\u001b[0m \u001b[38;5;250m    \u001b[39m\u001b[38;5;124;03m\"\"\"\u001b[39;00m\n\u001b[1;32m     10\u001b[0m \u001b[38;5;124;03m    Get an object from the database by its ID.\u001b[39;00m\n\u001b[1;32m     11\u001b[0m \u001b[38;5;124;03m    Args:\u001b[39;00m\n\u001b[0;32m   (...)\u001b[0m\n\u001b[1;32m     14\u001b[0m \u001b[38;5;124;03m        dict: The object data if found, otherwise None.\u001b[39;00m\n\u001b[1;32m     15\u001b[0m \u001b[38;5;124;03m    \"\"\"\u001b[39;00m\n\u001b[0;32m---> 16\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[43mocdb\u001b[49m\u001b[43m[\u001b[49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[38;5;124;43mocel:objects\u001b[39;49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[43m]\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mfind\u001b[49m\u001b[43m(\u001b[49m\u001b[43m{\u001b[49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[38;5;124;43m_id\u001b[39;49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[43m:\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;28;43mid\u001b[39;49m\u001b[43m}\u001b[49m\u001b[43m)\u001b[49m\u001b[43m[\u001b[49m\u001b[38;5;241;43m0\u001b[39;49m\u001b[43m]\u001b[49m\n",
      "File \u001b[0;32m/Library/Frameworks/Python.framework/Versions/3.12/lib/python3.12/site-packages/pymongo/synchronous/cursor.py:640\u001b[0m, in \u001b[0;36mCursor.__getitem__\u001b[0;34m(self, index)\u001b[0m\n\u001b[1;32m    638\u001b[0m         \u001b[38;5;28;01mfor\u001b[39;00m doc \u001b[38;5;129;01min\u001b[39;00m clone:  \u001b[38;5;66;03m# type: ignore[attr-defined]\u001b[39;00m\n\u001b[1;32m    639\u001b[0m             \u001b[38;5;28;01mreturn\u001b[39;00m doc\n\u001b[0;32m--> 640\u001b[0m         \u001b[38;5;28;01mraise\u001b[39;00m \u001b[38;5;167;01mIndexError\u001b[39;00m(\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mno such item for Cursor instance\u001b[39m\u001b[38;5;124m\"\u001b[39m)\n\u001b[1;32m    641\u001b[0m     \u001b[38;5;28;01mraise\u001b[39;00m \u001b[38;5;167;01mTypeError\u001b[39;00m(\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mindex \u001b[39m\u001b[38;5;132;01m%r\u001b[39;00m\u001b[38;5;124m cannot be applied to Cursor instances\u001b[39m\u001b[38;5;124m\"\u001b[39m \u001b[38;5;241m%\u001b[39m index)\n\u001b[1;32m    642\u001b[0m \u001b[38;5;28;01melse\u001b[39;00m:\n",
      "\u001b[0;31mIndexError\u001b[0m: no such item for Cursor instance"
     ]
    }
   ],
   "source": [
    "from matplotlib.colors import hex2color\n",
    "from build.code_quality_analyzer import calculate_maintainability_index\n",
    "from build.database_handler import get_event\n",
    "\n",
    "\n",
    "def plot_pull_request_code_quality(pull_request_id):\n",
    "    \"\"\"\n",
    "    Plot the code quality metrics for a specific pull request in the repository\n",
    "    Args:\n",
    "        pull_request_id (str): The ID of the pull request to plot the code quality metrics for\n",
    "    \"\"\"\n",
    "    pull_request = get_object(pull_request_id)\n",
    "    file_code_quality = {}\n",
    "    mis = {}\n",
    "    pylint_scores = {}\n",
    "    commit_dates = []\n",
    "    for relationship in pull_request[\"relationships\"]:\n",
    "        commit, file_object = None, None\n",
    "        if relationship[\"qualifier\"] == \"formalises\":\n",
    "            commit = get_object(relationship[\"qualifier\"])\n",
    "        elif relationship[\"qualifier\"] == \"aggregates\":\n",
    "            file_object = get_object(relationship[\"qualifier\"])\n",
    "        if commit is not None and len(commit[\"attributes\"]) > 0:\n",
    "            commit_time = datetime.datetime.fromisoformat(commit[\"attributes\"][0][\"time\"]).replace(tzinfo=None)\n",
    "            commit_dates.append(commit_time)\n",
    "        if file_object is not None and file_object[\"_id\"].find(\".py\") != -1:\n",
    "            h1 = get_attribute_value_at_time(file_object[\"_id\"], \"h1\", commit_time) \n",
    "            h2 = get_attribute_value_at_time(file_object[\"_id\"], \"h2\", commit_time)\n",
    "            N1 = get_attribute_value_at_time(file_object[\"_id\"], \"N1\", commit_time)\n",
    "            N2 = get_attribute_value_at_time(file_object[\"_id\"], \"N2\", commit_time)\n",
    "            cyclomatic_complexity = get_attribute_value_at_time(file_object[\"_id\"], \"cyclomatic_complexity\", commit_time)\n",
    "            loc = get_attribute_value_at_time(file_object[\"_id\"], \"loc\", commit_time)\n",
    "            mi = calculate_maintainability_index(N1, N2, h1, h2, cyclomatic_complexity, loc)\n",
    "            pl = get_attribute_value_at_time(file_object[\"_id\"], \"pylint_score\", commit_time)\n",
    "            file_code_quality[file_object[\"_id\"]] = {\n",
    "                \"maintainability_index\": mi,\n",
    "                \"pylint_score\": pl\n",
    "            }\n",
    "            # Get all files modified in all commits and then create average of metrics for files per commit\n",
    "    maintainability_indices = [mis[date] for date in commit_dates]\n",
    "    plt.figure(figsize=(12, 6))\n",
    "    plt.plot(commit_dates, maintainability_indices, label=\"Maintainability Index\", color=\"blue\", marker=\"o\", linestyle=\"-\")\n",
    "    plt.xlabel(\"Commit Date\")\n",
    "    plt.ylabel(\"Code Quality Score\")\n",
    "    plt.title(f\"Code Quality Metrics Over Time for PR#{pull_request['_id']}\")\n",
    "    plt.xticks(rotation=90)\n",
    "    plt.rcParams['axes.spines.top'] = False\n",
    "    plt.rcParams['axes.spines.right'] = False\n",
    "    plt.ylim(0, 1.1)\n",
    "    plt.legend()\n",
    "    plt.savefig(f\"Exports/CQ-{pull_request[\"_id\"]}.png\")\n",
    "\n",
    "plot_pull_request_code_quality(\"373\")"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "3.12",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.1"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
