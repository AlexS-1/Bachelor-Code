\chapter{Introduction}
\label{chap:intro}

Modern software engineering is shaped by distributed collaboration, version control, and continuous integration. As development processes become increasingly complex and team-based, effective coordination between contributors is essential for ensuring code quality, consistency, and maintainability. Version control systems, particularly Git, have emerged as critical infrastructure to support collaborative software development. Git enables developers to track changes, experiment with new features, and manage parallel development streams through branching and merging mechanisms. On top of Git, platforms such as GitHub provide user-friendly interfaces, hosting, and social collaboration features. One of GitHub’s central mechanisms is the \ac{pr}, which allows contributors to propose changes to a repository while enabling project maintainers to review, discuss, and control their integration. This mechanism plays a pivotal role in shaping how software contributions are evaluated, discussed, and merged in modern open-source projects \autocite{DBLP:conf/icse/TsayDH14}.

\begin{figure}[htbp]
    \centering
    \includegraphics[width=\textwidth]{figures/Collaboration_Illustration.pdf}
    \caption{Overview of collaboration process in open-source GitHub Respository}
    \label{fig:your-label}
\end{figure}

Despite its centrality, the pull request process is often opaque and difficult to navigate, particularly for new contributors. Many repositories include contribution guidelines, but enforcement is limited and the guidelines may lack specificity. Contributors must determine, for example, how to find suitable reviewers, what tests must be passed, or how long reviews might take. Furthermore, \ac{cicd} pipelines, while automating quality assurance steps, can introduce additional complexity and variability in the process. These factors contribute to a lack of transparency in contribution workflows, impeding effective participation and complicating efforts to assess the impact of contributions on code quality. Therefore, repository owners should focus on making following contribution guidelines logical and easy to understand. 

To address these challenges, this thesis proposes a method to extract both local (Git) and remote (GitHub) repository data and combine them into an \ac{ocel}. The \ac{ocel} serves as a unified representation of the contribution process, capturing events such as commits, reviews, comments, and merges, and linking them to relevant objects such as pull requests, files, and users. This structure enables the application of \ac{ocpm} techniques to analyze workflows from multiple viewpoints. Using this approach, we visualize the pull request process alongside code quality metrics over time, allowing both maintainers and contributors to better understand how contributions evolve and affect the codebase. A case study on three large Python repositories is conducted to demonstrate the feasibility and utility of this method. The evaluation focuses on detecting process changes, comparing workflows across projects, and analyzing the relationship between contribution guideline adherence and code quality evolution.

\section{Motivation}
Effective contribution to a software repository requires following the project’s contribution guidelines, which may include naming conventions, review processes, and expected merge timelines. Adhering to these guidelines ensures that contributions integrate smoothly into the codebase, which is crucial for maintaining project quality and consistency. When guidelines are not followed, the integration of changes can be delayed or result in conflicts.

However, official documentation and contribution guideldines rarely capture every possible scenario a contributor might encounter. As a result, developers often face uncertainty when their specific situation is not addressed, forcing them to guess how to proceed. This gap in documentation can lead to confusion and inconsistent contributions.

For example, consider a scenario where a developer discovers a bug while using a program and wants to contribute a fix to the repository without any prior open-source experience. This newcomer must figure out how to clone the repository, understand the coding standards, create a pull request, and find an appropriate reviewer — all without clear instructions. Such scenarios highlight the need for tools or visualizations that clarify the contribution process for inexperienced developers.

\paragraph{Problem Relevance.} Open-source software projects have grown in popularity, but there is a shortage of new developers to maintain them \autocite{DBLP:journals/corr/abs-2208-04895}\autocite{DBLP:journals/ese/RehmanWKIM22}. This imbalance emphasizes the importance of making the contribution process as clear and accessible as possible to attract and retain contributors. Currently, there is no known implementation that automatically extracts pull request information from a Git repository and organizes it into an OCEL. Developing such a dataset would provide a unified view of the project’s history and contribution workflow, serving as a valuable resource for analysis. Raw Git commit and pull request data are difficult to overview in their native form, especially for new developers. Visualizing this data using an OCEL can make the contribution workflow more comprehensible, which is particularly valuable for those unfamiliar with the project’s history and processes.

Contribution guidelines are typically the first point of contact for potential contributors, but they often lack a detailed, step-by-step walkthrough of the process \autocite{DBLP:conf/icsm/ElazharySEZ19}. Contributors frequently have to learn the process by trial and error rather than by following clear instructions, which can lead to inefficiencies and errors. Moreover, when individual developers — especially newcomers — pursue their own goals without coordination, the overall code quality can suffer. For example, inconsistent coding styles or missed review steps may accumulate over time, further highlighting the need for better guidance and process transparency.

\paragraph{Existing Solutions.} For developers of all experience levels, understanding the contribution process is essential. A variety of tools exist to assist in related tasks: some focus on measuring code quality, others on analyzing parts of the contribution process, and others on helping reviewers evaluate pull requests. However, these tools generally address only one aspect at a time. Our goal is to integrate process visualization and code quality measurement to reveal their relationship.

Some existing data extraction methods use only the local Git repository or only the GitHub REST API, but we found no approach that combines both sources. By integrating data from both local and remote repositories, we can create a more complete picture of the project’s history and workflow.

Code quality metrics have been studied and refined for many years, and new metrics continue to be proposed \autocite{DBLP:journals/smr/CodabuxSC24}. However, these metrics are typically applied to snapshots of code at a single point in time, with little attention to how quality evolves over the lifetime of a project. We propose using OCELs to visualize changes in code quality across the project history. Although prior research has examined how code review influences code quality \autocite{DBLP:journals/ese/McIntoshKAH16}, the broader contribution process has not been analyzed together in terms of its impact on quality.

Elazhary et al. (2019) \autocite{DBLP:conf/icsm/ElazharySEZ19} analyze whether pull requests adhere to a project’s contribution guidelines and detect anomalies in the process. However, they do not consider how those process characteristics affect the resulting code quality. Our work extends this by linking process conformance and deviations to the measured code quality outcomes.

\section{Problem Statement}
Most existing datasets used in Mining Software Repositories (MSR) research focus on a single dimension of project history, such as commits or issues. This narrow focus prevents analysis of the interdependencies between different aspects of the contribution process. Furthermore, these datasets often rely on older event log formats rather than modern object-centric logs, limiting their ability to capture complex workflows and relationships among artifacts.

Existing code quality analysis tools generally emphasize static snapshots or single-perspective analysis (e.g., file-level or project-level metrics) and pay little attention to how code quality changes over the repository’s history. As a result, opportunities to identify trends or anomalies in code quality evolution are often missed.

Finally, it remains unclear whether strict adherence to contribution guidelines actually leads to improved code quality. The effect of guideline compliance on metrics like maintainability or defect rates has not been thoroughly studied, leaving an important question unanswered.

\section{Research Questions}
Our first research question asks how to standardize and combine data from a project’s local Git repository and the remote GitHub repository into an object-centric event log (OCEL). Specifically, we will determine how to extract the relevant data (commits, pull request events, review comments, etc.), what additional benefits an OCEL provides compared to traditional event logs in this domain, and why a particular OCEL definition and format is chosen for modeling the workflow.

The second research question examines how code quality evolves when viewed from different perspectives. We will investigate whether code quality generally improves as the repository matures, whether different metrics (for example, the maintainability index and Pylint score) follow similar or divergent trends, and how code quality changes when examined at the level of individual files, authors, pull requests, and the entire repository.

The third research question asks whether adhering to contribution guidelines results in the desired code quality outcomes. We will look for noticeable changes in code quality evolution that coincide with changes in the guidelines, determine how deviations from the guidelines affect code quality, and identify which specific guideline violations have the greatest impact on code quality.

\section{Research Goals}
The first goal of this research is to create an approach for building datasets of GitHub contribution processes that capture multiple code quality viewpoints in an \ac{ocel} format. The second goal is to develop visualization tools that use the \ac{ocel} to show how code quality metrics evolve over time from different perspectives (repository, pull request, author, file). The third goal is to use conformance checking on the discovered process models to pinpoint the most common deviations from the documented contribution guidelines. The fourth goal is to compare the discovered pull request process models across different projects and analyze how these variations impact code quality.

\section{Contributions}
This thesis makes three main contributions. First, we present a general approach for constructing datasets from GitHub contribution data in an object-centric event log format. Second, we provide a visualization tool that generates code quality graphs from these OCELs, showing how quality metrics change over time. Third, we conduct a case study on three large Python repositories and their contribution guidelines to demonstrate the usefulness and applicability of our approach.

\section{Thesis Structure}
In Chapter 2, we discuss fundamental principles of code quality and object-centric process mining (OCPM), explaining key concepts and metrics used in this work. Chapter 3 reviews related work on code quality metrics and contribution process mining, summarizing existing tools and studies relevant to our research. Chapter 4 details our design decisions and overall approach, from data extraction to analysis, including how we construct the OCEL and compute quality metrics. Chapter 5 presents a case study evaluation of our method on three Python repositories, demonstrating the approach and analyzing the results. Finally, Chapter 6 concludes the thesis and outlines directions for future work.
