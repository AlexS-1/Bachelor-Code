\chapter{Introduction}
\label{chap:intro}

\paragraph{Domain.} With ever more connected supply chains process mining has become widely adopted as a tool to analyze e.g. procurement or order management processes. Due to many involved parties and the lack of the possibility to model different participants, the limitations of traditional process mining are often hindering industry wide adoption. Therefore, \ac{ocpm} has been proposed and quickly became very popular for above examples. Due to more interconnected business models,  software products are also becoming more complex. Most software being developed uses some form of \ac{vcs}. The predominant tool to manage changes for a software project is \emph{git}. Therefore, understanding the intricate details of git repositories provides valuable insights into how the code base changes over time. Not only does process mining allow developers to form a better understanding of how code contributions are integrated into the project, but also \acp{ocel}  allow querying. This helps developers decide whether to approve or deny merging changes into a certain branch of a project not only based on their own knowledge, but also by utilizing the experience of others. 
	
\paragraph{Problem.} Many repositories use \ac{cicd} pipe-lines that check whether changes made to parts of the code cause issues in the already existing code base. Often times developers do not question why a test fails or passes, but instead just focus on building code that passes the needed tests. Therefore, the quality of the tests, but also the quality of the code are vital to reduce the possibility of future bugs. Although, code quality metrics are included in many \ac{cicd} pipelines, the outcomes besides passing or not are hardly checked. Therefore, gaining insights into how code quality evolves within one pull request or the code base and where to request changes are vital steps towards understanding the code base better and thus delivering higher quality code.

\paragraph{Method.} On the one hand, using GitHub's REST \ac{api} provides insights into what steps were carried out, on the other hand generalizations over multiple pull requests are not directly extractable. \Ac{ocpm} can be used to generate process models for multiple viewpoints in one event log. Compared to traditional process mining, this enables to use it as a single source of truth for building process models. In this fashion, multiple viewpoints i.e. activities form the perspective of one user, one pull request or one file can be conceited. Through examining the event log for details of one object type, reviewers can utilise historical data about similar instances to help them decide whether to merge a pull request or request additional changes. Furthermore, visualising processes e.g., for certain development teams, enables all developers to apply efficient variations into their workflow.

\paragraph{Evaluation.} Verify code quality metrics against data from similar tool: \emph{wiley}.

\section{Motivation}
\label{sec:intro_ssec:motiv}
\paragraph{Problem Relevance.} 
\begin{itemize}
	\item No known implementation of data extraction of pull request information from GitHub repositories to form event log or \ac{ocel}
	\item Novel combination of code quality analysis with process mining
	\item Lack of user-friendly tools to analyse OCEL not just for process mining, but also general querying for information e.g., about pull request author
\end{itemize}

\paragraph{Existing Solutions.}
\begin{itemize}
	\item Data extraction with data only from git (local repository) and REST API exist separately, but to the author's knowledge no combined approach exists
	\item Code quality metrics have existed for long time, were refined, but too little focus of code quality changes $\Rightarrow$ Use process mining to visualise the change of code quality along the process
	\item Easy to use web-based tools exist to visualise an \ac{ocel} and show general statistics, but lack of possibility to query event log to answer specific questions
\end{itemize}


\section{Problem Statement}
\label{sec:intro_ssec:probs}
\begin{itemize}
	\item Git commit and pull request data is hard to overview
	\item Existing tools focus too little on the changes of code quality during the history of a repository
	\item Different data sources are hard to unify
\end{itemize}


\section{Research Questions}
\label{sec:intro_ssec:rqs}
\begin{itemize}
	\item Which data sources exist for extracting data from git repositories
	\item How does code quality change, when introducing adapted processes
	\item Which steps need to be taken to unify data from different sources
	\item How can the insights gathered through process mining be visualised in a user-friendly way
	\item How far do planned processes listed in contribution guides differ from real processes?
\end{itemize}

\section{Research Goals}
\label{sec:intro_ssec:rgs}
\begin{itemize}
	\item Combine data of a local repository and online data from GitHub to form an \ac{ocel}
	\item Analyze \ac{ocel} to discover process model of contributing to a repository through pull requests from the pull request viewpoint
	\item Visualize changes in code quality from the pull request, user and commit viewpoints
	\item Check whether carried out process match with proposed processes in guidelines using conformance-checking
	\item Present analysis results in a user-friendly interface
\end{itemize}


\section{Contributions}
\label{sec:intro_ssec:c}
\begin{itemize}
	\item \textbf{To be written later, when implementation and other relevant parts are finished}
\end{itemize}


\section{Thesis Structure}
\label{sec:intro_ssec:ts}
The remainder of the thesis is structured as follows. In \cref{chap:prelim}, the author formally defines the terms used in the latter chapters and explains how the chosen code quality metrics are calculated, by introducing a running example. In \cref{chap:related_work}, he presents related work on other approaches to measure code quality, how data from software repositories is commonly analyzed and which process mining concepts have been applied in the context of git repositories. In \cref{chap:method}, the extraction process is explained referring to the running example, introduced in \cref{chap:prelim}. Next the approach is applied to three large Python repositories and the discovered process models together with their \acp{ocel} are compared to the planned processes for contribution of these repositories in \cref{chap:eval}. In \cref{chap:discussion}, design choices regarding the data model are discussed and the limitations of code quality metrics and time complexity are explained. Lastly, in \cref{chap:conclusion} the discoveries made are summarized and possibilities for future work are lined out.