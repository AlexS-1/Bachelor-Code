[
    {
        "96c6276dc1ea0aad50b96cf844406a8641fa1d37---XESConversion.py": {
            "1": "from pydriller import Repository",
            "2": "import json",
            "3": "import pm4py",
            "4": "from datetime import datetime",
            "5": "from dateutil.relativedelta import relativedelta",
            "6": "from pm4py.objects.log.obj import EventLog, Trace, Event",
            "7": "from pm4py.objects.log.exporter.xes import exporter as xes_exporter",
            "8": "",
            "9": "def analyze_commits(repo_url, language_file_extension, dt1, dt2, single_comment_symbol, multi_comment_symbols=[]):",
            "10": "    files_data = {}",
            "11": "    # Traverse through the commits in the repository",
            "12": "    # Only save commits, that contain at least one file of the format {language_file_extension}",
            "13": "    for commit in Repository(repo_url, ",
            "14": "    only_modifications_with_file_types=[f\".{language_file_extension}\"],",
            "15": "    since=dt1,",
            "16": "    to=dt2).traverse_commits():",
            "17": "        if len(multi_comment_symbols) >= 2:",
            "18": "            multi_comments_enabled = True",
            "19": "        else:",
            "20": "            multi_comments_enabled = False",
            "21": "        # Analyze each file modified in the commit",
            "22": "        for modified_file in commit.modified_files:",
            "23": "            # only store file data for Rust files",
            "24": "            if modified_file.filename not in files_data:",
            "25": "                files_data[modified_file.filename] = []",
            "26": "            if len(modified_file.filename.split(\".\")) == 2 and modified_file.filename.split(\".\")[1] == language_file_extension:",
            "27": "                file_data = {",
            "28": "                    \"commit\": commit.hash,",
            "29": "                    \"timestamp\": commit.committer_date.isoformat(),",
            "30": "                    \"author\": commit.author.name,",
            "31": "                    \"commit_message\": commit.msg,",
            "32": "                    \"additions\": modified_file.added_lines,",
            "33": "                    \"deletions\": modified_file.deleted_lines,",
            "34": "                    \"change_type\": modified_file.change_type.name,",
            "35": "                    \"diff\": modified_file.diff_parsed",
            "36": "                }",
            "37": "                diff_added = {}",
            "38": "                diff_deleted = {}",
            "39": "                diff_modified = {}",
            "40": "                following_multi_comment = False",
            "41": "                # For added diff ispect lines filter out comments",
            "42": "                for line in modified_file.diff_parsed[\"added\"]:",
            "43": "                    if line[1].find(single_comment_symbol) != -1 or following_multi_comment:",
            "44": "                        diff_added[line[0]] = line[1]",
            "45": "                    if multi_comments_enabled and line[1].find(multi_comment_symbols[0]) != -1:",
            "46": "                        diff_added[line[0]] = line[1]",
            "47": "                        following_multi_comment = True",
            "48": "                    if multi_comments_enabled and line[1].find(multi_comment_symbols[1]) != -1:",
            "49": "                        diff_added[line[0]] = line[1]",
            "50": "                        following_multi_comment = False",
            "51": "                file_data[\"comment_added_diff\"] = diff_added",
            "52": "                # For deleted diff ispect lines filter out comments",
            "53": "                for line in modified_file.diff_parsed[\"deleted\"]:",
            "54": "                    if line[1].find(single_comment_symbol) != -1 or following_multi_comment:",
            "55": "                        diff_deleted[line[0]] = line[1]",
            "56": "                        if line[0] in diff_added.keys():",
            "57": "                            diff_modified[line[0]] = line[1]",
            "58": "                    if multi_comments_enabled and line[1].find(multi_comment_symbols[0]) != -1:",
            "59": "                        diff_added[line[0]] = line[1]",
            "60": "                        following_multi_comment = True",
            "61": "                        if line[0] in diff_added.keys():",
            "62": "                            diff_modified[line[0]] = line[1]",
            "63": "                    if multi_comments_enabled and line[1].find(multi_comment_symbols[1]) != -1:",
            "64": "                        diff_added[line[0]] = line[1]",
            "65": "                        if line[0] in diff_added.keys():",
            "66": "                            diff_modified[line[0]] = line[1]",
            "67": "                file_data[\"comment_deleted_diff\"] = diff_deleted",
            "68": "                file_data[\"comment_modified_diff\"] = diff_modified",
            "69": "                # Generate keywords based on the commit message and type of changes",
            "70": "                # file_data[\"keywords\"] = extract_keywords(commit.msg, modified_file)",
            "71": "                # Extract type of commit from commit message",
            "72": "                # file_data[\"activity\"] = extract_activity(commit.msg)",
            "73": "                if len(diff_added) + len(diff_deleted) != 0:",
            "74": "                    files_data[modified_file.filename].append(file_data)",
            "75": "    return files_data",
            "76": "",
            "77": "def extract_keywords(commit_message, modified_file):",
            "78": "    # Determine basic keywords based on the commit message",
            "79": "    keywords = []",
            "80": "    if \"performance\" in commit_message.lower():",
            "81": "        keywords.append(\"performance\")",
            "82": "    if \"security\" in commit_message.lower():",
            "83": "        keywords.append(\"security\")",
            "84": "    if modified_file.added_lines > modified_file.deleted_lines:",
            "85": "        keywords.append(\"expansion\")",
            "86": "    else:",
            "87": "        keywords.append(\"optimization\")",
            "88": "    return keywords",
            "89": "",
            "90": "def extract_activity(commit_message):",
            "91": "    # Use commit message keywords to determine activity type",
            "92": "    activity = \"\"",
            "93": "    if \"bug\" in commit.msg.lower() or \"fix\" in commit.msg.lower():",
            "94": "        activity = \"Bug Fix\"",
            "95": "    elif \"feature\" in commit.msg.lower() or \"add\" in commit.msg.lower():",
            "96": "        activity = \"Feature Development\"",
            "97": "    elif \"refactor\" in commit.msg.lower():",
            "98": "        activity = \"Refactoring\"",
            "99": "    else:",
            "100": "        activity = \"Other\"",
            "101": "    return activity",
            "102": "",
            "103": "def pretty_diff(commits_data, type, single_comment_symbol, multi_comment_symbols=[]):",
            "104": "    following_multi_comment = False",
            "105": "    if len(multi_comment_symbols) >= 2:",
            "106": "        multi_comments_enabled = True",
            "107": "    else:",
            "108": "        multi_comments_enabled = False",
            "109": "    for file, commits in commits_data.items():",
            "110": "        if len(file) > 0:",
            "111": "            for commit in commits:",
            "112": "                diff_edited = []",
            "113": "                # Set current line for each analysis",
            "114": "                for i in range(len(commit[\"diff\"][type])):",
            "115": "                    curr_line = commit[\"diff\"][type][i][0]",
            "116": "                    curr_content = commit[\"diff\"][type][i][1]",
            "117": "                    # In case of a starting multiline comment start adding future lines without comment symbol ",
            "118": "                    if multi_comments_enabled and curr_content.find(multi_comment_symbols[0]) != -1:",
            "119": "                            following_multi_comment = True",
            "120": "                    # In case of comment add them to existing dict if they directly follow",
            "121": "                    if curr_content.find(single_comment_symbol) == 0 or curr_content.find(single_comment_symbol + \" \") != -1 or following_multi_comment:",
            "122": "                        if len(diff_edited) > 0:",
            "123": "                            if len(diff_edited[-1][\"line_numbers\"]) == 0 or curr_line == diff_edited[-1][\"line_numbers\"][-1] + 1:",
            "124": "                                if len(diff_edited[-1][\"comments\"].keys()) > 0 and list(diff_edited[-1][\"comments\"].keys())[-1] + 1 == curr_line:",
            "125": "                                    diff_edited[-1][\"comments\"][curr_line] = curr_content",
            "126": "                                else:",
            "127": "                                    diff_edited.append({",
            "128": "                                        \"line_numbers\": [],",
            "129": "                                        \"comments\": {curr_line: curr_content},",
            "130": "                                        \"lines\": []})",
            "131": "                        else:",
            "132": "                    # or create new one",
            "133": "                            diff_edited.append({",
            "134": "                                \"line_numbers\": [],",
            "135": "                                \"comments\": {curr_line: curr_content},",
            "136": "                                \"lines\": []})",
            "137": "                    # In case of no comment add lines to existing dict if line number directly follows",
            "138": "                    else:    ",
            "139": "                        if len(diff_edited) > 0:",
            "140": "                            if len(diff_edited[-1][\"line_numbers\"]) == 0 or curr_line == diff_edited[-1][\"line_numbers\"][-1] + 1:",
            "141": "                                diff_edited[-1][\"line_numbers\"].append(curr_line)",
            "142": "                                diff_edited[-1][\"lines\"].append(curr_content)",
            "143": "                            else:",
            "144": "                    # Or create new one",
            "145": "                                diff_edited.append({",
            "146": "                                    \"line_numbers\": [curr_line],",
            "147": "                                    \"comments\": {},",
            "148": "                                    \"lines\": [curr_content]})",
            "149": "                    # Disable multiline comments when symbol found",
            "150": "                    if multi_comments_enabled and curr_content.find(multi_comment_symbols[1]) != -1:",
            "151": "                        following_multi_comment = False",
            "152": "                commit[\"diff\"][type] = diff_edited",
            "153": "    return commits_data",
            "154": "",
            "155": "def analyze_diffs(data):",
            "156": "    analysis_results = []",
            "157": "",
            "158": "    for file, commits in data.items():",
            "159": "        # Store last modified timestamps for each line",
            "160": "        last_modified = {}",
            "161": "        for commit in commits:",
            "162": "            # print(\"Starting to analyse commit: \", commit[\"commit\"])",
            "163": "            commit_time = datetime.fromisoformat(commit[\"timestamp\"])",
            "164": "            # Track modified lines",
            "165": "            for block in commit[\"diff\"][\"added\"]:",
            "166": "                for line in block[\"line_numbers\"]:",
            "167": "                    line_number = line",
            "168": "                    last_modified[line_number] = commit_time",
            "169": "            # Compare with comments",
            "170": "            for line in commit[\"comment_added_diff\"]:",
            "171": "                comment_time = datetime.fromisoformat(commit[\"timestamp\"])",
            "172": "                last_modified_lines = list(last_modified.keys())",
            "173": "                if int(line) in last_modified_lines:",
            "174": "                    for block in commit[\"diff\"][\"added\"]:",
            "175": "                        if line in block[\"comments\"] and len(block[\"line_numbers\"]) == 0:",
            "176": "                            if(comment_time > last_modified[int(line)]):",
            "177": "                                analysis_results.append({",
            "178": "                                    \"file\": file,",
            "179": "                                    \"line\": int(line),",
            "180": "                                    \"comment\": commit[\"comment_added_diff\"][line],",
            "181": "                                    \"comment_time\": str(comment_time),",
            "182": "                                    \"last_code_change_time\": str(last_modified[int(line)])",
            "183": "                                })",
            "184": "    return analysis_results",
            "185": "",
            "186": "def save_to_json(commits_data, path):",
            "187": "    # Save the processed commit data to a JSON file",
            "188": "    with open(path, 'w') as json_file:",
            "189": "        json.dump(commits_data, json_file, indent=4)",
            "190": "    print(\"Data has been saved to\", path)",
            "191": "",
            "192": "def create_xes_log(data):",
            "193": "    # Create a new EventLog object",
            "194": "    log = EventLog()",
            "195": "",
            "196": "    # Iterate over each commit entry in the data",
            "197": "    for file, commits in data.items():",
            "198": "        # Create a trace for the file",
            "199": "        trace = Trace()",
            "200": "        trace.attributes[\"file\"] = file",
            "201": "",
            "202": "        for commit in commits:",
            "203": "            # Extract event attributes",
            "204": "            event = Event()",
            "205": "            event[\"timestamp\"] = commit.get(\"timestamp\")",
            "206": "            event[\"author\"] = commit.get(\"author\")",
            "207": "            event[\"change_type\"] = commit.get(\"change_type\")",
            "208": "            event[\"commit_message\"] = commit.get(\"commit_message\")",
            "209": "            event[\"additions\"] = commit.get(\"additions\")",
            "210": "            event[\"deletions\"] = commit.get(\"deletions\")",
            "211": "            event[\"diff\"] = commit.get(\"diff\")",
            "212": "            if commit.get(\"comment_added_diff\"):",
            "213": "                event[\"comment_change\"] = \"True\"",
            "214": "            else:",
            "215": "                event[\"comment_change\"] = \"False\"",
            "216": "",
            "217": "            # Add the event to the trace",
            "218": "            trace.append(event)",
            "219": "",
            "220": "        # Add the trace to the log",
            "221": "        log.append(trace)",
            "222": "",
            "223": "    return log",
            "224": "",
            "225": "def save_xes_log(log, filename):",
            "226": "    # Export the log to an XES file",
            "227": "    xes_exporter.apply(log, filename)",
            "228": "",
            "229": "def save_to_xes(log, path):",
            "230": "    # Create the XES log from the commit data",
            "231": "    xes_log = create_xes_log(log)",
            "232": "",
            "233": "    # Save the XES log to a file",
            "234": "    save_xes_log(xes_log, path)",
            "235": "    print(\"XES log has been saved to\", path)",
            "236": "",
            "237": "if __name__ == \"__main__\":",
            "238": "    repo_url = \"https://github.com/apache/accumulo.git\"  # Example repository URL",
            "239": "    commits_data = analyze_commits(repo_url, \"java\", datetime.today() - relativedelta(years=1), datetime.today(), \"//\", [\"/*\", \"*/\"])",
            "240": "    save_to_json(commits_data, \"Data/commits_data.json\")",
            "241": "    # save_to_xes(commits_data, \"Data/commits_data.xes\")",
            "242": "    with open(\"Data/commits_data.json\", \"r\") as json_file: ",
            "243": "       commits_data = json.load(json_file)",
            "244": "    analyzed_data = pretty_diff(commits_data, \"added\", \"//\", [\"/*\", \"*/\"])",
            "245": "    save_to_json(analyzed_data, \"Exports/analyzed_data.json\")",
            "246": "    analyzed_data = pretty_diff(commits_data, \"deleted\", \"#\")",
            "247": "    save_to_json(analyzed_data, \"Exports/analyzed_data.json\")",
            "248": "    ",
            "249": "    # Test case",
            "250": "    with open(\"Exports/analyzed_data.json\", \"r\") as json_file:",
            "251": "        data = json.load(json_file)",
            "252": "    analyzed_data = analyze_diffs(data)",
            "253": "",
            "254": "    save_to_json(analyzed_data, \"Exports/analysis_results.json\")",
            "255": ""
        }
    },
    {
        "4210b185c8b05071e5e33b726c1d537cba181c6c---comment_lister.py": {
            "1": "import subprocess",
            "2": "import json",
            "3": "import os",
            "4": "import shutil",
            "5": "from datetime import datetime, timezone",
            "6": "",
            "7": "def run_comment_lister(repo_path, jar_path, tag=\"-target=HEAD\"):",
            "8": "    try:",
            "9": "        result = subprocess.run(",
            "10": "            ['java', '-jar', jar_path, repo_path, tag],",
            "11": "            stdout=subprocess.PIPE,",
            "12": "            stderr=subprocess.PIPE,",
            "13": "            text=True,",
            "14": "            check=True",
            "15": "        )",
            "16": "        return result.stdout",
            "17": "    except subprocess.CalledProcessError as e:",
            "18": "        print(f\"Error running CommentLister: {e.stderr}\")",
            "19": "        return None",
            "20": "",
            "21": "def filter_comments_by_time(commit_data, start_time, end_time):",
            "22": "    filtered_comments = []",
            "23": "    commit_time = datetime.fromisoformat(commit_data[\"CommitTime\"])",
            "24": "    if start_time <= end_time:",
            "25": "        for filename, contents in commit_data[\"Files\"].items():",
            "26": "            i = 0",
            "27": "            error = False",
            "28": "            while not error:",
            "29": "                try:",
            "30": "                    comment_data = {",
            "31": "                        \"line\": contents[str(i)][\"Line\"],",
            "32": "                        \"comment\": contents[str(i)][\"Text\"]",
            "33": "                    }",
            "34": "                except KeyError as e:",
            "35": "                    error = True",
            "36": "                if not error:",
            "37": "                    filtered_comments.append(comment_data)",
            "38": "                i += 1",
            "39": "    return filtered_comments"
        }
    },
    {
        "4210b185c8b05071e5e33b726c1d537cba181c6c---pydriller.py": {
            "1": "from pydriller import Repository",
            "2": "import json",
            "3": "import pm4py",
            "4": "from datetime import datetime",
            "5": "from dateutil.relativedelta import relativedelta",
            "6": "from pm4py.objects.log.obj import EventLog, Trace, Event",
            "7": "from pm4py.objects.log.exporter.xes import exporter as xes_exporter",
            "8": "",
            "9": "def get_commits_data(repo_path, from_date, to_date):",
            "10": "    files_data = {}",
            "11": "    for commit in Repository(repo_path, since=from_date, to=to_date).traverse_commits():",
            "12": "        for file in commit.modified_files:",
            "13": "            if file not in files_data:",
            "14": "                files_data[file.filename] = []",
            "15": "            file_data = {",
            "16": "                \"commit\": commit.hash,",
            "17": "                \"timestamp\": commit.committer_date.isoformat(),",
            "18": "                \"author\": commit.author.name,",
            "19": "                \"diff\": file.diff_parsed",
            "20": "            }",
            "21": "            if len(file.diff_parsed) != 0:",
            "22": "                files_data[file.filename].append(file_data)",
            "23": "    return files_data",
            "24": "",
            "25": "def analyze_commits(repo_url, language_file_extension, dt1, dt2, single_comment_symbol, multi_comment_symbols=[]):",
            "26": "    files_data = {}",
            "27": "    # Traverse through the commits in the repository",
            "28": "    # Only save commits, that contain at least one file of the format {language_file_extension}",
            "29": "    for commit in Repository(repo_url, ",
            "30": "    only_modifications_with_file_types=[f\".{language_file_extension}\"],",
            "31": "    since=dt1,",
            "32": "    to=dt2).traverse_commits():",
            "33": "        if len(multi_comment_symbols) >= 2:",
            "34": "            multi_comments_enabled = True",
            "35": "        else:",
            "36": "            multi_comments_enabled = False",
            "37": "        # Analyze each file modified in the commit",
            "38": "        for modified_file in commit.modified_files:",
            "39": "            # only store file data for Rust files",
            "40": "            if modified_file.filename not in files_data:",
            "41": "                files_data[modified_file.filename] = []",
            "42": "            if len(modified_file.filename.split(\".\")) == 2 and modified_file.filename.split(\".\")[1] == language_file_extension:",
            "43": "                file_data = {",
            "44": "                    \"commit\": commit.hash,",
            "45": "                    \"timestamp\": commit.committer_date.isoformat(),",
            "46": "                    \"author\": commit.author.name,",
            "47": "                    \"commit_message\": commit.msg,",
            "48": "                    \"additions\": modified_file.added_lines,",
            "49": "                    \"deletions\": modified_file.deleted_lines,",
            "50": "                    \"change_type\": modified_file.change_type.name,",
            "51": "                    \"diff\": modified_file.diff_parsed",
            "52": "                }",
            "53": "                diff_added = {}",
            "54": "                diff_deleted = {}",
            "55": "                diff_modified = {}",
            "56": "                following_multi_comment = False",
            "57": "                # For added diff ispect lines filter out comments",
            "58": "                for line in modified_file.diff_parsed[\"added\"]:",
            "59": "                    if line[1].find(single_comment_symbol) != -1 or following_multi_comment:",
            "60": "                        diff_added[line[0]] = line[1]",
            "61": "                    if multi_comments_enabled and line[1].find(multi_comment_symbols[0]) != -1:",
            "62": "                        diff_added[line[0]] = line[1]",
            "63": "                        following_multi_comment = True",
            "64": "                    if multi_comments_enabled and line[1].find(multi_comment_symbols[1]) != -1:",
            "65": "                        diff_added[line[0]] = line[1]",
            "66": "                        following_multi_comment = False",
            "67": "                file_data[\"comment_added_diff\"] = diff_added",
            "68": "                # For deleted diff ispect lines filter out comments",
            "69": "                for line in modified_file.diff_parsed[\"deleted\"]:",
            "70": "                    if line[1].find(single_comment_symbol) != -1 or following_multi_comment:",
            "71": "                        diff_deleted[line[0]] = line[1]",
            "72": "                        if line[0] in diff_added.keys():",
            "73": "                            diff_modified[line[0]] = line[1]",
            "74": "                    if multi_comments_enabled and line[1].find(multi_comment_symbols[0]) != -1:",
            "75": "                        diff_added[line[0]] = line[1]",
            "76": "                        following_multi_comment = True",
            "77": "                        if line[0] in diff_added.keys():",
            "78": "                            diff_modified[line[0]] = line[1]",
            "79": "                    if multi_comments_enabled and line[1].find(multi_comment_symbols[1]) != -1:",
            "80": "                        diff_added[line[0]] = line[1]",
            "81": "                        if line[0] in diff_added.keys():",
            "82": "                            diff_modified[line[0]] = line[1]",
            "83": "                file_data[\"comment_deleted_diff\"] = diff_deleted",
            "84": "                file_data[\"comment_modified_diff\"] = diff_modified",
            "85": "                # Generate keywords based on the commit message and type of changes",
            "86": "                # file_data[\"keywords\"] = extract_keywords(commit.msg, modified_file)",
            "87": "                # Extract type of commit from commit message",
            "88": "                # file_data[\"activity\"] = extract_activity(commit.msg)",
            "89": "                if len(diff_added) + len(diff_deleted) != 0:",
            "90": "                    files_data[modified_file.filename].append(file_data)",
            "91": "    return files_data",
            "92": "",
            "93": "def extract_keywords(commit_message, modified_file):",
            "94": "    # Determine basic keywords based on the commit message",
            "95": "    keywords = []",
            "96": "    if \"performance\" in commit_message.lower():",
            "97": "        keywords.append(\"performance\")",
            "98": "    if \"security\" in commit_message.lower():",
            "99": "        keywords.append(\"security\")",
            "100": "    if modified_file.added_lines > modified_file.deleted_lines:",
            "101": "        keywords.append(\"expansion\")",
            "102": "    else:",
            "103": "        keywords.append(\"optimization\")",
            "104": "    return keywords",
            "105": "",
            "106": "def extract_activity(commit_message):",
            "107": "    # Use commit message keywords to determine activity type",
            "108": "    activity = \"\"",
            "109": "    if \"bug\" in commit.msg.lower() or \"fix\" in commit.msg.lower():",
            "110": "        activity = \"Bug Fix\"",
            "111": "    elif \"feature\" in commit.msg.lower() or \"add\" in commit.msg.lower():",
            "112": "        activity = \"Feature Development\"",
            "113": "    elif \"refactor\" in commit.msg.lower():",
            "114": "        activity = \"Refactoring\"",
            "115": "    else:",
            "116": "        activity = \"Other\"",
            "117": "    return activity",
            "118": "",
            "119": "def pretty_diff(commits_data, type, single_comment_symbol, multi_comment_symbols=[]):",
            "120": "    following_multi_comment = False",
            "121": "    if len(multi_comment_symbols) >= 2:",
            "122": "        multi_comments_enabled = True",
            "123": "    else:",
            "124": "        multi_comments_enabled = False",
            "125": "    for file, commits in commits_data.items():",
            "126": "        if len(file) > 0:",
            "127": "            for commit in commits:",
            "128": "                diff_edited = []",
            "129": "                # Set current line for each analysis",
            "130": "                for i in range(len(commit[\"diff\"][type])):",
            "131": "                    curr_line = commit[\"diff\"][type][i][0]",
            "132": "                    curr_content = commit[\"diff\"][type][i][1]",
            "133": "                    # In case of a starting multiline comment start adding future lines without comment symbol ",
            "134": "                    if multi_comments_enabled and curr_content.find(multi_comment_symbols[0]) != -1:",
            "135": "                            following_multi_comment = True",
            "136": "                    # In case of comment add them to existing dict if they directly follow",
            "137": "                    if curr_content.find(single_comment_symbol) == 0 or curr_content.find(single_comment_symbol + \" \") != -1 or following_multi_comment:",
            "138": "                        if len(diff_edited) > 0:",
            "139": "                            if len(diff_edited[-1][\"line_numbers\"]) == 0 or curr_line == diff_edited[-1][\"line_numbers\"][-1] + 1:",
            "140": "                                if len(diff_edited[-1][\"comments\"].keys()) > 0 and list(diff_edited[-1][\"comments\"].keys())[-1] + 1 == curr_line:",
            "141": "                                    diff_edited[-1][\"comments\"][curr_line] = curr_content",
            "142": "                                else:",
            "143": "                                    diff_edited.append({",
            "144": "                                        \"line_numbers\": [],",
            "145": "                                        \"comments\": {curr_line: curr_content},",
            "146": "                                        \"lines\": []})",
            "147": "                        else:",
            "148": "                    # or create new one",
            "149": "                            diff_edited.append({",
            "150": "                                \"line_numbers\": [],",
            "151": "                                \"comments\": {curr_line: curr_content},",
            "152": "                                \"lines\": []})",
            "153": "                    # In case of no comment add lines to existing dict if line number directly follows",
            "154": "                    else:    ",
            "155": "                        if len(diff_edited) > 0:",
            "156": "                            if len(diff_edited[-1][\"line_numbers\"]) == 0 or curr_line == diff_edited[-1][\"line_numbers\"][-1] + 1:",
            "157": "                                diff_edited[-1][\"line_numbers\"].append(curr_line)",
            "158": "                                diff_edited[-1][\"lines\"].append(curr_content)",
            "159": "                            else:",
            "160": "                    # Or create new one",
            "161": "                                diff_edited.append({",
            "162": "                                    \"line_numbers\": [curr_line],",
            "163": "                                    \"comments\": {},",
            "164": "                                    \"lines\": [curr_content]})",
            "165": "                    # Disable multiline comments when symbol found",
            "166": "                    if multi_comments_enabled and curr_content.find(multi_comment_symbols[1]) != -1:",
            "167": "                        following_multi_comment = False",
            "168": "                commit[\"diff\"][type] = diff_edited",
            "169": "    return commits_data",
            "170": "",
            "171": "def analyze_diffs(data):",
            "172": "    analysis_results = []",
            "173": "",
            "174": "    for file, commits in data.items():",
            "175": "        # Store last modified timestamps for each line",
            "176": "        last_modified = {}",
            "177": "        for commit in commits:",
            "178": "            # print(\"Starting to analyse commit: \", commit[\"commit\"])",
            "179": "            commit_time = datetime.fromisoformat(commit[\"timestamp\"])",
            "180": "            # Track modified lines",
            "181": "            for block in commit[\"diff\"][\"added\"]:",
            "182": "                for line in block[\"line_numbers\"]:",
            "183": "                    line_number = line",
            "184": "                    last_modified[line_number] = commit_time",
            "185": "            # Compare with comments",
            "186": "            for line in commit[\"comment_added_diff\"]:",
            "187": "                comment_time = datetime.fromisoformat(commit[\"timestamp\"])",
            "188": "                last_modified_lines = list(last_modified.keys())",
            "189": "                if int(line) in last_modified_lines:",
            "190": "                    for block in commit[\"diff\"][\"added\"]:",
            "191": "                        if line in block[\"comments\"] and len(block[\"line_numbers\"]) == 0:",
            "192": "                            if(comment_time > last_modified[int(line)]):",
            "193": "                                analysis_results.append({",
            "194": "                                    \"file\": file,",
            "195": "                                    \"line\": int(line),",
            "196": "                                    \"comment\": commit[\"comment_added_diff\"][line],",
            "197": "                                    \"comment_time\": str(comment_time),",
            "198": "                                    \"last_code_change_time\": str(last_modified[int(line)])",
            "199": "                                })",
            "200": "    return analysis_results",
            "201": "",
            "202": "def save_to_json(commits_data, path):",
            "203": "    # Save the processed commit data to a JSON file",
            "204": "    with open(path, 'w') as json_file:",
            "205": "        json.dump(commits_data, json_file, indent=4)",
            "206": "    print(\"Data has been saved to\", path)",
            "207": "",
            "208": "def create_xes_log(data):",
            "209": "    # Create a new EventLog object",
            "210": "    log = EventLog()",
            "211": "",
            "212": "    # Iterate over each commit entry in the data",
            "213": "    for file, commits in data.items():",
            "214": "        # Create a trace for the file",
            "215": "        trace = Trace()",
            "216": "        trace.attributes[\"file\"] = file",
            "217": "",
            "218": "        for commit in commits:",
            "219": "            # Extract event attributes",
            "220": "            event = Event()",
            "221": "            event[\"timestamp\"] = commit.get(\"timestamp\")",
            "222": "            event[\"author\"] = commit.get(\"author\")",
            "223": "            event[\"change_type\"] = commit.get(\"change_type\")",
            "224": "            event[\"commit_message\"] = commit.get(\"commit_message\")",
            "225": "            event[\"additions\"] = commit.get(\"additions\")",
            "226": "            event[\"deletions\"] = commit.get(\"deletions\")",
            "227": "            event[\"diff\"] = commit.get(\"diff\")",
            "228": "            if commit.get(\"comment_added_diff\"):",
            "229": "                event[\"comment_change\"] = \"True\"",
            "230": "            else:",
            "231": "                event[\"comment_change\"] = \"False\"",
            "232": "",
            "233": "            # Add the event to the trace",
            "234": "            trace.append(event)",
            "235": "",
            "236": "        # Add the trace to the log",
            "237": "        log.append(trace)",
            "238": "",
            "239": "    return log",
            "240": "",
            "241": "def save_xes_log(log, filename):",
            "242": "    # Export the log to an XES file",
            "243": "    xes_exporter.apply(log, filename)",
            "244": "",
            "245": "def save_to_xes(log, path):",
            "246": "    # Create the XES log from the commit data",
            "247": "    xes_log = create_xes_log(log)",
            "248": "",
            "249": "    # Save the XES log to a file",
            "250": "    save_xes_log(xes_log, path)",
            "251": "    print(\"XES log has been saved to\", path)",
            "252": "",
            "253": "if __name__ == \"__main__\":",
            "254": "    repo_url = \"https://github.com/apache/accumulo.git\"  # Example repository URL",
            "255": "    commits_data = analyze_commits(repo_url, \"java\", datetime.today() - relativedelta(years=1), datetime.today(), \"//\", [\"/*\", \"*/\"])",
            "256": "    save_to_json(commits_data, \"Data/commits_data.json\")",
            "257": "    # save_to_xes(commits_data, \"Data/commits_data.xes\")",
            "258": "    with open(\"Data/commits_data.json\", \"r\") as json_file: ",
            "259": "       commits_data = json.load(json_file)",
            "260": "    analyzed_data = pretty_diff(commits_data, \"added\", \"//\", [\"/*\", \"*/\"])",
            "261": "    save_to_json(analyzed_data, \"Exports/analyzed_data.json\")",
            "262": "    analyzed_data = pretty_diff(commits_data, \"deleted\", \"#\")",
            "263": "    save_to_json(analyzed_data, \"Exports/analyzed_data.json\")",
            "264": "    ",
            "265": "    # Test case",
            "266": "    with open(\"Exports/analyzed_data.json\", \"r\") as json_file:",
            "267": "        data = json.load(json_file)",
            "268": "    analyzed_data = analyze_diffs(data)",
            "269": "",
            "270": "    save_to_json(analyzed_data, \"Exports/analysis_results.json\")",
            "271": ""
        }
    },
    {
        "4210b185c8b05071e5e33b726c1d537cba181c6c---main.py": {
            "1": "# Import modules",
            "2": "from build.pydriller import get_commits_data",
            "3": "from build.comment_lister import run_comment_lister, filter_comments_by_time",
            "4": "",
            "5": "# Import packages",
            "6": "import os",
            "7": "import json",
            "8": "import subprocess",
            "9": "import shutil",
            "10": "from datetime import datetime, timezone",
            "11": "",
            "12": "def main():",
            "13": "     # Convert repo URL to path by cloning repo",
            "14": "    repo_url = \"https://github.com/AlexS-1/Bachelor-Code.git\"",
            "15": "",
            "16": "    repo_name = os.path.basename(repo_url).replace(\".git\", \"\")",
            "17": "    temp_dir = \"/Users/as/Library/Mobile Documents/com~apple~CloudDocs/Dokumente/Studium/Bachelor-Thesis/tmp\"",
            "18": "    clone_path = os.path.join(temp_dir, repo_name)",
            "19": "",
            "20": "    subprocess.run(['git', 'clone', repo_url, clone_path], check=True)",
            "21": "",
            "22": "    # Paths",
            "23": "    repo_path = clone_path",
            "24": "    jar_path = \"/Users/as/Library/Mobile Documents/com~apple~CloudDocs/Dokumente/Studium/Bachelor-Thesis/CommentLister/target/CommentLister.jar\"",
            "25": "    ",
            "26": "    # Setting different timeperiod",
            "27": "    start_time = datetime.today().replace(year = datetime.today().year - 1, tzinfo=None, microsecond=0)",
            "28": "    end_time = datetime.today().replace(microsecond=0)",
            "29": "",
            "30": "    commits_data = get_commits_data(repo_path, start_time, end_time)",
            "31": "",
            "32": "    for file, commits in commits_data.items():",
            "33": "        for commit in commits:",
            "34": "            tag = \"-target=\" + commit[\"commit\"]",
            "35": "            output = run_comment_lister(repo_path, jar_path, tag)",
            "36": "            if output is None:",
            "37": "                return",
            "38": "",
            "39": "            # Parse output as JSON",
            "40": "            try:",
            "41": "                comment_data = json.loads(output)",
            "42": "            except json.JSONDecodeError as e:",
            "43": "                print(f\"Failed to parse CommentLister output: {e}\")",
            "44": "                return",
            "45": "",
            "46": "            # Filter comments by time",
            "47": "            filtered_comments = filter_comments_by_time(comment_data, start_time, end_time)",
            "48": "            commit[\"comments\"] = filtered_comments",
            "49": "            ",
            "50": "    # Save filtered comments",
            "51": "    with open('Data/filtered_commits_data.json', 'w') as f:",
            "52": "        json.dump(commits, f, indent=4)",
            "53": "",
            "54": "    ",
            "55": "    ",
            "56": "    shutil.rmtree(clone_path)",
            "57": "",
            "58": "if __name__ == \"__main__\":",
            "59": "    main()"
        }
    }
]